<Articles>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>MutCombinator: identification of mutated peptides allowing combinatorial mutations using nucleotide-based graph search</Title>
    <Doi>10.1093/bioinformatics/btaa504</Doi>
    <Authors>Choi Seunghyuk, Paek Eunok</Authors>
    <Abstract/>
    <Body>1 Introduction With the advances in genomics and proteomics technologies such as next-generation sequencing and tandem mass spectrometry, we can better identify sample-specific and/or novel peptides. Construction of protein sequence database plays an important role in identifying novel peptides because reliable peptide identification depends heavily on the database being searched. In terms of mutated peptide identification, many methods have been proposed to construct a proteogenomic database, such as CustomizedDB ( Park  et al. , 2014 ), CustomProDB ( Wang and Zhang, 2013 ), CanProVar ( Zhang  et al. , 2017 ) and a variant graph ( Woo  et al. , 2014a ). CustomizedDB and CustomProDB assumed that all mutations occur simultaneously in a gene. On the other hand, CanProVar assumed that all digested peptides can have no more than a single mutation. These approaches reduce the search time by avoiding the exhaustive search for all possibilities, but naturally preclude covering all possible mutated peptides at the same time. Woo and colleagues proposed a ‘variant graph’, which represents a given transcriptome model as a direct acyclic graph, where each node is a nucleotide sequence representing a part of an exon or a variant call and edges connect neighboring exons indicating splice sites or point mutation occurrences. They also provided a ‘variant graph to FASTA’ enumeration package because most database search engines only take a FASTA formatted database as an input. The enumerated variant graph can represent almost all possible combinations of mutated peptides depending on a user specific parameter. However, the parameter is not intuitive because it is an internal parameter that controls the algorithm behavior and does not directly describe the desired output. The parameter value has to do with the density of mutation calls, which may vary widely depending on genes, making it very difficult to set the value properly. Furthermore, the variant graph method does not allow a user to set the translation frame: a user may want only in-frame translation, or all three-frame translations. To overcome such limitations, we developed MutCombinator, which enables us to identify combinatorially mutated peptides by searching a variant graph directly ( Fig. 1 ) without enumerating them into amino acid sequences off-line. A variant graph is built by taking reference genome sequences in FASTA, transcriptome model in GTF and variant calls in VCF format as input. MutCombinator can identify peptides with combinations of maximum  n  mutations ( n  is a user-specified parameter) in a peptide. The possible combinations of mutations grow exponentially as  n  increases; therefore, the search time can grow exponentially. To keep the search time under control, we adopted the two existing techniques: (i) extract short amino acids sequence tags (of length 3) from a spectrum, and search paths containing at least one tag to avoid traversing the whole variant graph [this approach was suggested elsewhere ( Mann and Wilm, 1994 ), but they used FASTA database instead of a variant graph] and (ii) indexing variant graph using three amino acids long sequences to directly access nucleotide sequences in the graph. For convenience of a user, MutCombinator supports (i) multi-threading, which enables parallel processing of spectra and (ii) both in-frame coding region search as well as three-frame search that encompasses non-coding regions as well.  Fig. 1. Overview of MutCombinator. A variant graph is built from the reference genome sequences, a transcriptome model and variant calls. Positions of each nine nucleotide sequence in the variant graph are indexed by corresponding three amino acids and stored in a pre-compiled index table. Positions of sequence tags (e.g. PRE and TTY) deduced from a spectrum are directly recognized by looking up the index table. PSM is processed by traversing the flanking paths of tag positions. The black box represents an exon model in the given transcriptome model. The gray box represents a variant model annotated in the given variant calls We designed a multistage search ( Madar  et al. , 2018 ) with MutCombinator to effectively identify mutated peptides using proteogenomics data from a previous study ( Mun  et al. , 2019 ). First, we used unidentified tandem mass (MS/MS) spectra from the previous study as an input for the second stage search using MutCombinator under the conditions: (i) use of 12 688 mutations from sample-specific variant calls and 83 873 mutations from COSMIC database (a total of 96 287 mutations), (ii) 28 843 expressed protein coding transcripts (supported by FPKM &gt; 1) and (iii) allowing up to three mutations per a peptide. As a result, we additionally identified 80 mutated peptides than the previous report. From this result, we could find 10 additional KEGG-pathways and 70 combinations of mutations. Furthermore, we also identified four mutated peptides harboring exclusively expressed mutations. At the third stage, we further searched unidentified MS/MS spectra from the first and second stage search against the same database with the same search conditions as the second stage except for one thing: use of 12 516 expressed non-coding transcripts (also supported by FPKM &gt; 1). We identified 14 aberrantly translated peptides—five pseudogenes, four frameshifts, two exon extensions, two 5′ UTRs, and one 3′ UTR peptides. 2 Materials and methods Peptide identification in MutCombinator consists of four parts: (i) construction of a variant graph with frame information, (ii) indexing the variant graph to directly access nucleotide sequences in the graph, (iii) candidate peptides generation by traversing the variant graph from indexed positions and (iv) scoring peptide spectrum matches (PSMs). 2.1 Construction of variant graph with frame-awareness Originally, a variant graph was not designed to limit the search only to in-frame translation because it assumed that the graph would be built directly from RNA-Seq results. This assumption was made for discovery of potential novel coding regions; however, it is not suitable for identifying mutations in known protein coding regions, because it triples the search space, resulting in increased false positives and execution time. Recent proteogenomic research have focused more on identifying mutated peptides in the coding region because their relation with the disease can be significant ( Mertins  et al. , 2016 ;  Mun  et al. , 2019 ;  Zhang  et al. , 2014 ). To facilitate comprehensive mutation identification in a proteogenomic search, we augmented variant graphs with frame information in each node whenever it is available. When constructing a variant graph with frame-awareness, each transcript model is initially represented as a linear graph structure (list) where nodes represent nucleotide sequences of exons and edges represent junction sites between two exons ( Fig. 2a ). When multiple transcripts share a common region (the same genome positions and the same nucleotide sequences, shown in gray in  Fig. 2a ), it is represented as a single node in the merged transcript as shown in  Figure 2b  while the remaining parts of the original transcript, i.e. distinct parts, are split from the original node and the split sites are connected by an edge. When splitting nodes, each node inherits its frame information from the original transcript. The frame information is recorded as binary vectors where each row represents each of the three frames and each column represents a transcript of a given gene. For example, if a gene has two transcripts, then the first transcript is represented with a bit value 0b00000001 and second one 0b00000010. Thus, only one of the three rows in the binary vectors has non-zero value in the original transcript, if the transcript model represents a coding sequence ( Fig. 2a ). We used only seven bits because the sign-bit (the most significant bit) is not suitable for index value. When a gene has more than 7 transcripts, PABPC1 has 18 protein coding transcripts for example and the column size of the binary vectors is determined as {1 + quotient of dividing the number of transcripts by 7} bytes.  Fig. 2. Construction of a variant graph augmented with frame information. Nodes represent nucleotide sequences and edges connect neighboring nodes. Edges generated by splitting nodes in each step are represented as dotted lines. The letter boxes indicate coding sequences. ( a ) Each transcript is represented as a linear graph structure. Frame information is recorded in each node as binary vectors. Gray nodes represent common regions among multiple transcripts of the same gene. ( b ) The common regions are merged into a single node, and their original frame information is coalesced by bit-OR operation. Each gene is represented as a single directed acyclic graph after this step. ( c ) SNVs and insertions are added, and frames are re-calculated To compact the transcripts into a single merged transcript, nodes in the transcripts are split into common and distinct parts based on both genomic positions and nucleotide sequences. When nodes are split into two or three nodes, each frame information of the split nodes is recalculated in order to keep track of the transcript structures. In the example shown in  Figure 2b  (focusing on how the frame information of the second transcript changes) frames of the first node do not change. Frames of the remaining nodes are calculated based on their predecessor nodes in a topological order. If the nucleotide sequence length of a predecessor node is a multiple of three, then the current node is assigned the same frame of the predecessor node. Otherwise, the frames of the current node are set as the union of up-rotating each predecessor’s frame by the remainder of dividing its nucleotide sequence length by 3. Union operation is actually performed by bit-OR operation. This way, all the transcript models of a gene can be merged into a single variant graph. In case there are SNVs and insertions given as input (in VCF format), a node containing mutation site is split based on its mutation position. A new node representing the mutation is created and added to the graph, and their frames must be recalculated. The recalculation method is the same as above. In the example shown in  Figure 2c , there are two mutations: insertion ‘G &gt; GTT’ and SNV ‘G &gt; T’. While there is a single predecessor node for the last node SNV ‘G &gt; T’ (the split node and new node), the last node ‘GCA’, which is caused by the insertion, has two predecessors such as node ‘G’ and node ‘GTT’. In this case, we coalesce both up-rotated frame information by bit-OR operation, and apply the coalesced frame information as that of the last node ‘GCA’. In the case of deletion, we simply make a new transcript model in the gene before the merge step. As an concrete example, the first node in  Figure 2b , representing a nucleotide sequence ‘ATGGCA’, is split into three nodes such as ‘AT’, ‘G’ and ‘GCA’ shown in  Figure 2c . Node ‘AT’ inherited the same frame as the original node ‘ATGGCA’ because it is the first node among the three. The successor of node ‘AT’ is node ‘G’; therefore, the frames of node ‘G’ is set as up-rotating that of node ‘AT’ twice. Similarly, the frame information of the last node ‘GCA’ is set as bit-OR operation of both up-rotating that of node ‘G’ once and up-rotating that of node ‘GTT’ three times (thus no rotation operation). Notice that node ‘GTT’ as the insertion ‘G &gt; GTT’ just copies the same frame information with node ‘G’ because they have the same predecessor node ‘AT’. 2.2 Indexing variant graph In a typical database search approach to peptide identification, each spectrum is compared with candidate peptide sequences in a sequence database. There have been three major methods to avoid searching the whole database: (i) limit candidate peptides only to those that match the precursor mass of a given spectrum, (ii) select candidate peptides using fragment ion matches ( Kong  et al. , 2017 ) and (iii) select candidate peptides that contain sequence tags derived by applying de novo sequencing to a given spectrum ( Mann and Wilm, 1994 ;  Na  et al. , 2012 ;  Tabb  et al. , 2003 ). The first and second approaches enumerate all possible enzymatic peptides and find the best match for each spectrum. Enumerating all possible peptides of a given variant graph is not practical once we start to consider combinatorial mutation. MutCombinator adopted the third approach: extracting three amino acids sequence tag from an input spectrum and directly accessing the tag positions in variant graph by pre-compiled index. Each index is generated by traversing the whole variant graph and recording the following information: (i) a start node, (ii) an end node, (it must be noted that a sequence tag may span over multiple nodes), (iii) a tag start position within the start node, (iv) a tag end position within the end node, (v) a gene id, (vi) the number of mutation sites included in the nine nucleotide sequence of a tag and (vii) the frame information of a tag obtained by bit-AND operation of all the nodes in a path spanning the nine nucleotide sequence. During index generation, there are two cases when an index of tag should be discarded: (i) the number of mutation occurrences exceeds the maximum allowable mutations  n , specified as a user parameter, or (ii) all three frame information is 0b00000000, meaning that there is no proper path denoted by the tag. 2.3 Generating candidate peptides Sequence tags are inferred from a spectrum by  de novo  sequencing. The positions of these tag occurrences in a variant graph can be retrieved by looking up the pre-compiled index table. Candidate peptides, the masses of which match to a precursor mass of the spectrum, are generated by traversing the flanking nodes neighboring the tag positions. When traversing the flanking nodes, nucleotide sequences in a node is virtually translated to amino acids for all the valid frames and the peptide mass is calculated while extending the tag sequence into the neighboring nodes until the peptide mass just exceeds the precursor mass given the tolerance. The frame information in the tag is updated during the traversal by bit-AND operation among the visited nodes to confirm the validity of a path, i.e. a variant graph merged all the transcript models of a gene into a single graph, thus an integrity check is necessary to confirm that a path actually corresponds to some transcript model ( Supplementary Fig. S1 ). The traversal stops extending into the neighboring nodes whenever the frame information becomes 0b00000000 (i.e. there is no valid transcript that matches the nucleotide sequence of the current path), or when a path contains more mutations than the number of maximum allowable mutation  n . 2.4 Evaluating PSM quality MODa ( Na  et al. , 2012 ) evaluated PSMs using a logistic regression of four component scores such as: (i) prefix residue mass (PRM) score, (ii) mass error of matched fragment ions, (iii) the fractions of b and y ions found and (iv) the propensity to a particular ion type. We used the same scoring method to evaluate PSMs. Briefly, an experimental spectrum is first converted into a PRM spectrum, and the PRM spectrum is used to match against the candidate peptides using an alignment based on dynamic programming. 3 Results 3.1 Multistage search to further identify mutated peptides A huge number of disease-related mutations could be obtained by several resources such as ClinVar ( Landrum  et al. , 2018 ) and COSMIC databases ( Tate  et al. , 2019 ). To use these resources for proteogenomic study, we usually make a mutated peptide sequence database by considering all possible combinations of mutations because we cannot be sure which mutations might be observed in our samples. Under the assumption that some of these mutated peptides, derived from the public mutation resources, could also be observed by MS/MS spectra for the sample of our interest, we designed multistage search ( Madar  et al. , 2018 ) using MutCombinator ( Fig. 3 ). We chose N33T34 dataset, obtained from a tissue sample of a microsatellite instability (MSI) high cancer patient from a previous proteogenomics study on EOGC (early onset gastric cancer) ( Mun  et al. , 2019 ). N33T34 dataset included three types of data—4 215 882 MS/MS spectra [the spectra were processed by PE-MMR ( Shin  et al. , 2008 )] labeled with 4-plex iTRAQ, mRNA-seq and whole exome-seq. We used a preprocessed dataset provided by Mun and colleagues. There were a total of 41 359 expressed transcripts annotated in Ensembl transcriptome model v71. Among them, 28 843 and 12 516 transcripts were protein coding and non-coding, respectively. As for mutations, there were a total of 12 688 mutations matched to the expressed transcripts.  Fig. 3. Multistage search using MutCombinator. Unidentified MS/MS spectra from the previous result (EOGC second-stage dataset) are subjected to MutCombinator as an input. MutCombinator identifies mutated peptides considering combinations of mutations of both sample-specific and COSMIC mutations in the expressed coding transcripts. Unidentified MS/MS spectra from the expressed coding transcripts are subjected to identification of aberrantly translated peptides in the expressed non-coding transcripts. The identified PSMs are filtered out if there is the same sequence in UniProt proteome or contaminants. Note that the result of conventional search was provided by Mun and colleagues Among 12 688 sample-specific mutations, only 254 mutations (247 SNVs and 7 insertions) were found among 83 873 stomach cancer-related mutations of COSMIC database (version 87) with the following conditions: (i) available genomic positions and mutated nucleotide sequences, (ii) categorized as SNV, insertion, or deletion and (iii) matched to the expressed transcripts. Assuming that the two mutation sources can be complementary to each other, we constructed CnSSVG (Cosmic and sample-specific variant graph) using 96 307 unified mutations (83 873 stomach cancer-related mutations as well as 12 688 sample-specific mutations). EOGC group identified 588 483 PSMs by searching N33T34 spectra against CustomizedDB ( Park  et al. , 2014 ) using MS-GF+ search ( Kim and Pevzner, 2014 ) and we denoted this search strategy as a conventional search in  Figure 3 . To further identify mutated peptides considering combinations of mutations of both sample-specific and COSMIC mutations, we used 2 792 596 unidentified MS/MS spectra. Note that PE-MMR generates multiple spectra per scan by correcting precursor m/z and charge state; therefore, we filtered out 1 423 286 MS/MS spectra corresponding to 588 483 identified MS/MS scans in the previous result. We searched 2 792 596 MS/MS spectra using MutCombinator against CnSSVG. The search parameters were set as follows: 10 ppm for precursor tolerance, 0.025 Da for fragment tolerance, three fixed modifications (carbamidomethylation at cysteine and iTRAQ label at peptide N-terminal and lysine), semi-tryptic for enzyme specificity allowing up to two miscleavages and eight for minimum peptide length. We also set  n  to three, allowing up to three mutations per peptide. After the search, the same scans could appear more than one time in the PSM list because the spectra were processed by PE-MMR; therefore, we selected a PSM having the highest score among PSMs with the same scan number. And then, we applied separate false discovery rate (FDR) strategy ( Woo  et al. , 2014b ) so that mutated peptides and wild-type peptides could fairly compete with each other. We divided the search results into two: (i) PSMs with wild-type peptide match including both target or decoy and (ii) PSMs with mutated peptide match including both target or decoy. If a sequence of mutated peptide is equivalent to a sequence of wild-type peptide, we assigned the PSM as a wild-type PSM. The result was estimated at 1% local-FDR at PSM level. We identified 8778 wild-type PSMs and 231 mutated PSMs. From 231 mutated PSMs, we filtered out those sequences of which were found in UniProt proteome (release 2019-11) or contaminants. Repeating a similar workflow, we further identified aberrantly translated peptides from 12 516 non-coding transcripts using non-coding search mode in MutCombinator as the last stage. We also compared MutCombinator with a conventional search (MS-GF+ applied against CustomizedDB including mutation) when executed in a single stage search mode ( Supplementary Fig. S2 ). It must be noted that the search space of the two can be tremendously different. 3.2 Mutated peptides in coding regions With a multistage search using MutCombinator, we further identified 211 mutated PSMs in the coding regions ( Fig. 4a ). This result amounts to additional identification of 80 mutated peptides, 52 genes and 70 combinations of mutations. We compared two KEGG-pathways ( P -value &lt; 0.05) from (i) genes from the conventional search and (ii) genes from the conventional search together with MutCombinator, using DAVID ( Huang da  et al. , 2009 ) to see whether the additional gain in peptide identification could lead to different interpretation in terms of pathways ( Fig. 4b ). In the original conventional search results, two pathways were strongly enriched in ECM-receptor interaction and focal adhesion, showing significantly negative mRNA-survival correlation ( Mun  et al. , 2019 ). Our approach resulted in 10 additional significantly enriched pathways. To make sure that such additional enriched pathways are not random, possibly due to high proportions of such genes in CnSSVG, we further calculated  P -values using Fisher exact test. We used all genes harboring mutations in CnSSVG, as a background population and then calculated  P -value of each pathway using the genes found by MutCombinator only. All of the pathways showed  P -value below 0.05, showing that the pathways are significantly enriched in the search results (details in  Supplementary Table S1 ). Proteoglycans in cancer, one of the additional pathways, also showed significant negative mRNA-survival correlation in the previous report. Intriguingly, inflammation related pathways such as phagosome, leukocyte transendothelial migration, bacterial invasion of epithelial cells and viral myocarditis were enriched and this result is consistent with the already known relationship between inflammation and cancers ( Coussens and Werb, 2002 ).  Fig. 4. The identification of mutated peptides in coding regions. ( a ) Peptides, genes and combinations of mutations corresponding to a total of 211 mutated PSMs are described. ( b ) KEGG-pathways of two gene groups––results of conventional search with/without MutCombinator analysis—show different patterns. Pathways with  P -value &lt; 0.05 are used. ( c ) Combinations of mutations observed in MS/MS are categorized into three groups––conventional search, MutCombinator and commonly observed by both. The combinations of mutations in MutCombinator group are further classified into sample-specific and COSMIC mutations. ( d ) Mutated peptides harboring exclusively expressed mutations in LSP1 protein. Identified peptides and corresponding gene model are shown, and amino acid changes are indicated by red underline Owing to MutCombinator, we can further identify 70 combinations of mutations including 60 SNVs, 6 INDELs, 3 double SNVs and 1 double INDEL ( Fig. 4c ). Nine SNVs were derived from the sample-specific mutations, and the rest were derived from the COSMIC mutations. On the other hand, MutCombinator enables considering combinations of mutations allowing up to three mutations per peptide. We could identify four mutated peptides harboring exclusively expressed mutations in RHOA, RRBP1, HIST1H3H and LSP1. For example, we identified two mutated peptides resided in a genomic region from 1 902 744 to 1 902 800 in chromosome 11 ( Fig. 4d ). One of them had Alanine changed into Threonine at position 9 because of SNV (G &gt; A) at locus chr11:1 902 768. The other peptide had Glutamine changed into Leucine at position 17 because of SNV (A &gt; T) at locus chr11:1 902 793. Although these mutations originated from the same sample-specific mutations, they were expressed exclusively at the protein level. Next-generation sequencing analyses bulk of cells simultaneously, thus the actual combinations of mutations are not distinguishable at the genomic level. Certain conventional proteogenomic analyses could have missed identifying these two exclusively mutated peptides, but our approach could successfully resolve mutational ambiguities at the protein level by considering mutations combinatorially during the second stage search. 3.3 Aberrantly translated peptides in non-coding regions Proteogenomic approach can be useful in identifying peptides deduced not only from mutations but also from aberrant expression of non-coding RNAs and pseudogenes ( Kim  et al. , 2014 ;  Nesvizhskii, 2014 ). Protein sequence database built from three frame translation of genes of interest such as non-coding RNAs and/or pseudogenes is used to identify aberrantly expressed peptides from MS/MS spectra. Such an approach could be useful to correct gene annotation or, perhaps, analyze disease-specific patterns ( Stewart  et al. , 2019 ). We added three frame translation mode for non-coding RNAs and pseudogenes to MutCombinator so that a user can easily identify aberrantly expressed peptides with/without combinatorial mutations. We applied third stage search to identify aberrantly expressed peptides, after the two-stage search in the coding regions. We searched 2 773 632 unidentified spectra against 12 516 expressed non-coding/pseudogene transcripts, considering sample-specific and COSMIC mutations. We estimated at 1% local FDR and identified 122 PSMs. We removed 86 PSMs, peptide sequences of which exactly match UniProt (release 2019-11) or common contaminant sequences. We obtained genomic loci of 27 peptides corresponding to the remaining 36 PSMs by applying ACTG tool ( Choi  et al. , 2017 ). Thirteen mutated peptides could be matched to coding regions in Ensembl v71 so we further discarded the mutated PSMs in the identifications of aberrantly translated peptides. The summary of non-coding search result is described in  Figure 5 .  Fig. 5. Aberrantly translated peptides supported by MS/MS assay. ( a ) Mutated and wild peptides are categorized into variant types such as (1) protein coding, (2) frameshift, (3) UTRs, (4) exon extension and (5) pseudogene. ( b ) Details of novel peptides supported by MS/MS assay. The first peptide in the list is matched to two genes 4 Discussion  Proteogenomics has improved understandings of biology, via integration of genomics and proteomics. The baseline results of the integration depend on identifications of expressed and mutated peptides; however, there is no practically available software tool to identify mutated peptides considering all possible combinations of mutations in coding regions. We designed MutCombinator so that it can be applied to identify mutated peptides allowing combinatorial mutations using a reasonable amount of computational resources. A total of 2 792 596 spectra were processed using 75 GB of memory and 16 threads, taking 42 h on workstation computers. We demonstrated the usefulness of MutCombinator in two aspects: (i) identifications of mutated peptides with combinatorial mutations and (ii) incorporation of large-scale mutation database such as COSMIC. By considering combinations of mutations, MutCombinator facilitates identifying mutated peptides regardless of where the mutations really come from. In other words, we now can decode the combinations of mutations even when mutations from different sources are aggregated in a single database. Funding This work was supported by the National Research Foundation of Korea [NRF-2017M3C9A5031597, NRF-2017R1E1A1A01077412 and NRF-2019M3E5D3073568]; and the BK21 plus program through the National Research Foundation funded by the Ministry of Education of Korea. 
 Conflict of Interest : none declared.  Supplementary Material btaa504_Supplementary_Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>OMSim: a simulator for optical map data</Title>
    <Doi>10.1093/bioinformatics/btx293</Doi>
    <Authors>Miclotte Giles, Plaisance Stéphane, Rombauts Stephane, Van de Peer Yves, Audenaert Pieter, Fostier Jan, Hancock John</Authors>
    <Abstract/>
    <Body>1 Introduction The Bionano Genomics platform is able to visualize occurrences of specific, short sequence motifs (e.g. 7 bp) along very long stretches of linearized DNA molecules (up to 2.5 Mbp), thus forming a unique, sequence-specific pattern per molecule, sometimes referred to as a ‘barcode’. By using those signature patterns, the molecules can be assembled in a complete consensus genome map. This view of the genome can be used to validate or improve de novo genome assembly, by providing a scaffold on which the contigs can be anchored ( Shi  et al. , 2016 ), or to detect large-scale structural variation in genomes ( Mak  et al. , 2015 ). Bionano Genomics optical map data is generated in several steps. First, DNA molecules of up to 2.5 Mbp are labeled using nicking restriction endonucleases, cutting one strand of the DNA near specific recognition nucleotide sequences. At these nicking sites fluorescent nucleotides are introduced into the DNA to highlight the position where the DNA motif occurs. The labeled DNA is then linearized using nanochannel arrays and imaged, such that the fluorescent labels along each molecule can be detected. For each DNA molecule, its size as well as the positions of the labels on the molecule are estimated and stored in BNX format. These data can be visualized as beads on a cord, and assembling these into optical consensus maps involves the alignment of the molecules such that the label positions match. The Irys Software System ( http://bionanogenomics.com/wp-content/uploads/2015/01/datasheet-web.pdf ) and the Irys-scaffolding scripts ( Shelton  et al. , 2015 ) can be used to automate this procedure. We developed OMSim to generate synthetic optical map data. This serves two purposes. First, OMSim can assist in the development and benchmarking of tools that operate on Bionano Genomics optical map data such as alignment and assembly software. Simulated optical map data were used to this end in ( Muggli  et al. , 2014 ,  2015 ;  Li  et al. , 2016 ;  Leung  et al. , 2017 ), but the simulation tools were not publicly available and only took into account a limited subset of the noise factors present in real data. Second, OMSim can assist in designing the optimal experimental setup: given a genome of interest, OMSim-generated data can help to select the right nicking enzyme or combinations thereof, to identify local label-depleted areas with low information content, to evaluate the distribution of nicking sites, to identify fragile sites due to nearby occurring labels, etc. This information can then be used to optimize the parameters of an Irys run, to ultimately generate an optimal amount of useful real data. A concrete example of this assistance in experimental design is the simulation of data corresponding to a structurally altered genome and evaluating the ability of the Bionano platform to identify the structural variations. The use of simulated data for this second application significantly improves upon the use of nicker software, e.g. BioNano Genomics Knickers, which provide overal statistics on label density based on a reference genome analysis. These global statistics provide only limited insights in the problem at hand, while simulated data allows to actually test the performance of the assembly or variant detection. OMSim simulates the Bionano Genomics process using statistical models for which the parameters were derived from real data (see  Supplementary Material  data S1 for the parameter description), and generates output in BNX format. It is implemented in Python, and relies on the Scipy library to sample from the required distributions. A graphical user interface has been developed to facilitate the setup of the simulation process. OMSim requires a reference assembly as the ground truth for the simulation. Each map is simulated from a single contig, hence the contiguity of the reference assembly limits the lengths of the simulated optical maps, i.e. it is impossible to simulate an optical map that is longer than the contig from which it is simulated. 2 Methods and results OMSim was designed to accurately mimic all sources of variation that occur in the Bionano Genomics data. First, false positive and false negative labels are taken into account, where labels are either erroneously placed or not placed. Second, there is the occurrence of fragile sites, where labels that occur very close to each other cause systematic breaks in the molecules. Third, each molecule has a stretch factor, which quantifies how the migration of the DNA molecule through a nanochannel causes the molecule to stretch or shrink. Fourth, there is some additional variability in the position of the labels due to local stretching. Fifth, due to the limited optical resolution, nearby labels may appear as one label in the image. Finally, also due to the optical resolution, there is the possibility of chimeric maps, which occur when distinct molecules are close together in a nanochannel such that they appear as a single molecule in the image. The OMSim process consists of two steps. First, the locations of the sequence recognition sites in the genome are indexed using the computationally efficient Knuth-Morris-Pratt algorithm ( Knuth  et al. , 1977 ). This index can be reused for future runs. Second, using this index, OMSim simulates the actual optical map data. Molecule lengths are generated from a negative binomial distribution and for each molecule a start location is uniformly chosen on the provided reference genome. Then, labels and noise are introduced in each molecule. False positive (resp. negative) labels are uniformly distributed along the molecules (resp. labels). The molecules are broken at fragile sites, based on the proximity of neighbouring labels. Stretch factor variations are normally distributed. Labels that occur close together are collapsed into a single label. After simulating the molecules, chimeras are introduced by concatenating molecules. Optical map data was simulated from the human genome reference Hg19, and anchored using the Bionano Genomics RefAligner. The resulting alignments were compared to the alignments of real data from NA24385 (Ashkenazim Trio son, public data from  http://bionanogenomics.com/science/public-datasets/ ). A portion of these alignments and the coverage and the size distribution of both simulated maps and real maps are shown in  Figure 1 . This figure shows that the simulated data can be aligned to the reference, that similarly as in real data missing labels are present due to false positives or collapsing labels, and that the size distributions of the simulated and real data are nearly identical. A peak memory usage of 478 MB was measured while indexing the human reference Hg19 and simulating optical map data from this index. Peak memory usage depends on the number of nicking sites in the reference. The indexing run time is linear in the size of the reference, while the simulation run time is linear in the size of the output. In our tests for genomes with sizes ranging from 4 Mbp up to 30 Gbp, this corresponds to a throughput of 30 Mbp per minute for indexing and 12.5 Gbp per minute for the actual simulation. Loading the index in subsequent runs took less than 30 seconds for all data sets. From these results we conclude that OMSim efficiently simulates data that resemble the real Bionano Genomics data.
 Fig. 1 ( A ) Alignments of real and simulated data on a section of chromosome 10 of the human genome. The alignments were obtained with the Bionano Genomics RefAligner. The tracks from top to bottom are: (1) the consensus map, (2) real optical maps from data set NA24385 and (3) optical maps simulated with OMSim. Markers on each track correspond to the anchored labels. Only a fraction of the actual coverage is shown. ( B ) Comparison of the label coverage in 100 bins along chromosome 10 in Hg19, for both simulated and real data. ( C ) Comparison of the size distribution of simulated and real data over the entire genome. Molecules shorter than 150 kbp were filtered out  Funding This work was supported by The Research Foundation–Flanders (FWO) [G0C3914N]. 
 Conflict of Interest : none declared. Supplementary Material Supplementary Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Seed-based IntaRNA prediction combined with GFP-reporter system identifies mRNA targets of the small RNA Yfr1</Title>
    <Doi>10.1093/bioinformatics/btp609</Doi>
    <Authors>Richter Andreas S., Schleberger Christian, Backofen Rolf, Steglich Claudia</Authors>
    <Abstract>Motivation: Prochlorococcus possesses the smallest genome of all sequenced photoautotrophs. Although the number of regulatory proteins in the genome is very small, the relative number of small regulatory RNAs is comparable with that of other bacteria. The compact genome size of Prochlorococcus offers an ideal system to search for targets of small RNAs (sRNAs) and to refine existing target prediction algorithms.</Abstract>
    <Body>1 INTRODUCTION Bacterial small RNAs (sRNAs) are regulatory RNAs that often act as post-transcriptional regulators by base pairing to  trans -encoded target mRNAs. The sRNA–mRNA interaction can result in translational repression and/or mRNA degradation, as well as translational activation, mostly in response to changing environmental conditions (Waters and Storz,  2009 ). The few sRNA–mRNA interactions experimentally characterized so far have been particularly studied in the two model organisms  Escherichia coli  ( E.coli ) and  Salmonella typhimurium  LT2 ( Salmonella ) (Gottesman,  2005 ; Vogel,  2009 ). However, sRNA regulators are not restricted to model bacteria, but occur ubiquitously in bacteria. In this study, we investigated the ecologically important cyanobacterium  Prochlorococcus . This photoautotrophically dwelling organism often accounts for up to 50% of the organic biomass in the oligotrophic areas of the open oceans, and is thus a crucial component of the food web (Goericke and Welschmeyer,  1993 ; Vaulot  et al. ,  1995 ). A recent systematic survey of sRNAs in  Prochlorococcus  MED4 revealed a large number of potential regulatory RNAs comparable with those found in other bacteria (Steglich  et al. ,  2008 ). This finding was very surprising, as  Prochlorococcus  has experienced an evolutionary streamlining of its genome, leading to very compact genomes between 1.64 and 2.68 Mb, which notably results in a small number of regulatory proteins (Kettler  et al. ,  2007 ). The identification of sRNA targets in  Prochlorococcus  constitutes a big challenge, since common experimental approaches such as knockouts of these sRNAs cannot be applied. Instead, the only possible approach is a combination of  in silico  target prediction, followed by  in vivo  experimental validation (in a heterologous expression system). An interesting sRNA candidate to study is Yfr1, which is an abundant RNA with ubiquitous appearance in all lineages of cyanobacteria except for two  Prochlorococcus  strains (Voss  et al. ,  2007 ). Recent studies have shown that Yfr1 is constitutively expressed and accumulates up to 18 000 copies per cell in  Synechococcus elangatus  PCC6301 (Nakamura  et al. ,  2007 ). The high copy numbers of Yfr1 raise the question of whether this RNA acts as a  trans -encoded sRNA through base pairing with its targets, or whether it modulates protein activity. An example of such modulation activity is the 6S RNA, which downregulates mRNA transcription by mimicking an open promoter complex (Wassarman,  2007 ). However, a prominent feature of Yfr1 is the ultraconserved 11 nt long sequence motif located in an unpaired sequence stretch flanked by two stem–loops ( Fig. 1 A). Similar to Yfr1, the two  Salmonella  sRNAs GcvB and RybB show a conserved single-stranded region. In both the GcvB and RybB sRNAs, these regions are involved in the binding of multiple targets, which results in reduced translation of the targets (Vogel,  2009 ). To verify whether Yfr1 analogously regulates  trans -encoded mRNAs via base pairing, we predicted putative interaction partners of Yfr1 in the cyanobacterium  Prochlorococcus  MED4 and experimentally validated these candidates by a reporter system based on green fluorescent protein (GFP).
 Fig. 1. ( A ) Secondary structure of  Prochlorococcus  MED4 Yfr1, as predicted by RNA fold  (Hofacker  et al. ,  1994 ). The ultraconserved region is set in bold. The arrow indicates the introduced mutation M2 (dark grey). ( B ) Secondary structure resulting from mutation M1 (substituted positions highlighted in light grey). 2 METHODS 2.1 Computational prediction of Yfr1 targets For the target prediction, a 400 nt subsequence including 250 nt upstream and 150 nt downstream of the start codon was extracted for all annotated genes of the  Prochlorococcus  MED4 genome [GenBank accession number BX548174 (Rocap  et al. ,  2003 ) using the updated annotation by Kettler  et al.  ( 2007 )]. In total, we obtained 1964 sequences covering the full 5′ untranslated region (5′ UTR) (if not &gt;250 nt) and the beginning of the coding sequence of each gene to search for interactions with Yfr1. Putative interactions with Yfr1 were predicted with I nta RNA based on hybridization energy and accessibility of the interaction sites (Busch  et al. ,  2008 ). The I nta RNA approach also incorporates interaction seeds, i.e. short regions of (nearly) perfect sequence complementarity. Accessibility is defined as the energy required to unfold the region of interaction in each molecule. In the calculation of these unfolding energies, we assumed global folding of Yfr1. In contrast, the mRNA does not fold globally due to helicase activity of the ribosome (Takyar  et al. ,  2005 ). Hence, the mRNA subsequence was locally folded in a 200 nt window with a maximal base pair distance of 100 nt. For each gene, the optimal interaction and up to five suboptimal interactions were computed. In  Prochlorococcus  MED4, the ultraconserved motif 5′-ACUCCUCACAC−3′ covers positions 17–27 of Yfr1 RNA ( Fig. 1 A). This motif was predicted to be single-stranded in the consensus secondary structure of Yfr1 orthologs from 31 cyanobacteria (Voss  et al. ,  2007 ). In order to search for interactions with this motif as seed region, we extended the I nta RNA program by adding optional constraints that allow to fix the seed position to a given interval of the sRNA sequence. For the target search, we defined an interaction seed of eight paired bases and at most one unpaired base within the aforementioned conserved Yfr1 motif (I nta RNA parameters -p 8 -u 1 -f 17,27). To investigate the influence of interaction seeds, the target prediction was additionally conducted without requiring a seed region (I nta RNA parameter -p 2 for at least 2 bp). We also tested a modified energy score that weights the accessibility against the hybridization energy with factor α:
 
where  E hybrid  denotes the hybridization energy of the interaction and ED x  denotes the energy required to make the interaction site accessible in sequence  x . The original I nta RNA scoring does not weight the unfolding energy of the interaction sites, i.e. α=1. In addition to the I nta RNA energy score, the location of the interaction in the mRNA is used as a further criterion to evaluate the quality of prediction. The majority of characterized  trans- encoded sRNAs downregulate their targets by base pairing to the 5′ UTR in the vicinity of the ribosome binding site (RBS) (reviewed in Aiba,  2007 ). Therefore, the predicted target candidates were filtered for interactions that involve the mRNA region from −39 to +19 relative to the start codon, which is the maximal region covered by ribosomes (Hüttenhofer and Noller,  1994 ). The Yfr1-target interactions predicted with fixed seed and full accessibility scoring are provided in  Supplementary Material 1 . Target candidates resulting from each parameter setting are listed in  Supplementary Table 1 . 2.2 Experimental validation of Yfr1 targets 2.2.1 E.coli growth conditions and plasmid constructions E.coli  strain Top10F′ was used for cloning of all target- gfp  fusions in plasmid pXG-10 or of Yfr1 gene in plasmid pZE12- luc . All interaction studies were carried out in  E.coli  strain Top10.  E.coli  cells were grown in Luria–Bertani broth at 37°C in the presence of 100 μg/ml ampicillin and/or 25 μg/ml chloramphenicol. Plasmids used in this work were obtained from Dr Jörg Vogel (MPI, Berlin). Plasmid constructions of the respective 5′ UTRs and of Yfr1 are described in detail in Urban and Vogel ( 2007 ). In brief, full-length 5′ UTRs and the first coding residues of the targets of interest were ligated in pXG-10 plasmid using two complementary oligonucleotides with an Mph1103I restriction site at the 5′ terminus and an NheI restriction site at the 3′ terminus, which were annealed to each other prior to ligation. In the case of the 5′ UTR of PMM0494, a PCR-generated fragment (containing an Mph1103I and an NheI restriction site) was digested and ligated into Mph1103I- and NheI-digested pXG-10 plasmid. The Yfr1 gene was amplified by PCR containing an XbaI restriction site and ligated in pZE12- luc  plasmid containing an XbaI restriction site for insertion. Yfr1 mutants (Yfr1 M1: CC at positions 20 and 21 substituted by GG leading to the formation of a stem–loop structure in the normally unpaired region, Yfr1 M2: UCCU at positions 19–22 substituted by AAAA without changing the structure, see  Figure 1 ) were generated by annealing two complementary oligonucleotides containing an XbaI restriction site. The complete list of oligonucleotides used for cloning is provided in  Supplementary Table 2 . 2.2.2 Analysis of Yfr1-mediated target regulation We tested potential interactions of Yfr1 sRNA with the 5′ UTRs of the putative targets PMM0050 ( argJ , bifunctional ornithine acetyltransferase/ N -acetylglutamate synthase), PMM0494 ( ppa , putative inorganic pyrophosphatase), PMM0538 (unknown function), PMM1119 ( som , outer membrane protein), PMM1121 ( som , outer membrane protein) or PMM1697 (type II alternative σ factor). For fluorescence measurement, overnight cultures were grown in 96-well plates (Nunc, Roskilde, Denmark) at 37°C with gentle agitation in an air humidity saturated environment to prevent evaporation. Cells were diluted 1:100, fixed in 1% Histofix (Roth, Karlsruhe, Germany) and kept in darkness until measurements were conducted. Single cell fluorescence was determined by flow cytometry with the flow cytometer LSR II (BD Bioscience, New Jersey, USA). Cell fluorescence was measured with an excitation wavelength of 488nm and the emission was detected at 513/17nm. Target- gfp  fusions as well as control plasmids pXG-0 (negative control) and pXG-1 (positive control) were tested in the presence of a nonsense RNA and Yfr1 sRNA, respectively. The mean fluorescence per plasmid combination was calculated from 10 000 events (cells) of six individual clones. 3 RESULTS AND DISCUSSION 3.1 Experimental validation of predicted Yfr1 targets Table 1  lists the 10 highest scoring candidates of the Yfr1 target prediction. Out of these, we experimentally tested the six monocistronic target candidates with known transcriptional start sites and interaction sites predicted in the 5′ UTR or at the start codon. The predicted interactions for targets with a GFP fluorescence signal above background (indicating measurable expression) are shown in  Figure 2 . Two of the six tested target candidates are translationally repressed by Yfr1, as shown by a reduced GFP fluorescence signal ( Fig. 3 ). The first clusters of the bar chart in  Figure 3  constitute the negative controls ( E.coli  strain Top10 without plasmid or with plasmid pXG-0 devoid of  gfp , respectively) and the positive control ( E.coli  strain Top10 with plasmid pXG-1 carrying  gfp ). The remaining clusters represent the 5′ UTR- gfp  fusions for the targets of interest. Each  gfp  fusion plasmid was tested in the presence of a second plasmid containing a nonsense RNA (white bars), Yfr1 sRNA (red bars) and the two mutated Yfr1 sRNAs M1 and M2 (light and dark blue bars) ( Fig. 3 ).
 Table 1. Highest scoring Yfr1 target candidates and their ranks under different I nta RNA parameter settings Target Fixed seed No seed α 1 0.5 0 1 0.5 0 PMM1119 ( som ) 1 4 22 1 11 173 PMM0494 ( ppa ) 2 3 25 2 10 195 PMM1121 ( som ) 3 6 32 4 32 184 PMM1697 4 7 12 12 19 10 PMED4_09051 5 1 8 52 2 46 PMM0538 6 16 14 26 84 91 PMM0130 7 13 72 13 60 238 PMM1021 8 22 69 5 51 830 PMM1405 9 12 26 37 61 115 PMM0050 ( argJ ) 10 11 7 40 8 11 Only interactions at RBS [−39 to +19 relative to start codon, see Hüttenhofer and Noller ( 1994 )] were considered. All ranks are given according to I nta RNA energy score. α is a weighting factor for the accessibility in the energy score. 
 Fig. 2. Interactions between Yfr1 and target mRNA 5′ UTRs predicted by I nta RNA. Additionally, a putative interaction between Yfr1 and the positive control pXG-1 is presented. The 5′ ends of the mRNAs were experimentally mapped by deep sequencing (C.Steglich, unpublished data). Yfr1 RNA and coding sequences of the mRNAs are set in bold. Start codons are underlined. Shine-Dalgarno (SD) sequences are marked with a box. Asterisks denote start codons that are presumably misannotated in the  Prochlorococcus  MED4 genome sequence. The arrows indicate mutations M1 (light grey) and M2 (dark grey) introduced in Yfr1. 
 Fig. 3. Experimental validation of Yfr1 target predictions. The relative decrease in GFP fluorescence as determined by flow cytometry indicates the strength of Yfr1-mediated regulation. The dashed line indicates background fluorescence (i.e. cellular autofluorescence), determined as the mean GFP signal of the negative controls. Fold changes of reduced GFP signal for PMM1119 (3.0-fold), PMM1121 (2.7-fold) and pXG1 (1.5-fold) were calculated after background subtraction from absolute fluorescence values (Urban and Vogel,  2007 ). In the presence of the nonsense RNA, no regulation of the 5′ UTR- gfp  fusions by an interaction is expected ( Fig. 3 , white bars), and the fluorescence measured here represents the 5′ UTR-specific translation efficiency. The different GFP fluorescence intensities can be explained by differences in the affinities of the ribosomes for the translation initiation region. The strongest inhibition by Yfr1 was detected for the 5′ UTRs of the two  som  genes PMM1119 and PMM1121 (3.0- and 2.7-fold reduced GFP signal, red bars in  Fig. 3 ). No change in GFP fluorescence was observed for PMM1697 and PMM0538 5′ UTRs in the presence of Yfr1. For PMM0494 and PMM0050, no fluorescence above the background level (dashed line in  Fig. 3 ) could be detected for any tested plasmid combination. Translation inhibition of the two  som s was abolished by the introduction of a mutation in the conserved Yfr1 motif exchanging CC by GG (Yfr1 M1, light blue bars in  Fig. 3 ). These two substitutions involve the region predicted to base pair with the RBS of the two  som  mRNAs. Furthermore, mutation M1 led to a structural change by introducing a stem–loop in the single-stranded region of wild-type Yfr1 ( Fig. 1 B). Thus, mutation M1 results in both a sequential and structural change at the interaction site. To test whether the destruction of the antisense complementarity alone (without structural change) abolishes regulation by Yfr1, we constructed another Yfr1 mutant. In the Yfr1 mutant M2, nucleotides UCCU were substituted by AAAA without changing the secondary structure of wild-type Yfr1 ( Fig. 1 A). Again, translation of PMM1119 and PMM1121 was restored ( Fig. 3 , dark blue bars). These results indicate that Yfr1 inhibits translation of the two  som  mRNAs by direct base pairing at the RBS. Furthermore, the results strongly indicate that both sequence and structure are important for Yfr1 regulation. Surprisingly, we also observed a 1.5-fold reduction in GFP fluorescence for the positive control pXG-1 in the presence of Yfr1 and restored translation under the control of Yfr1 M1 and M2. However, the strong RBS in the 5′ UTR of  gfp  in pXG-1 (Urban and Vogel,  2007 ) shows a perfect complementarity to part of the conserved Yfr1 motif. Thus, Yfr1 can form a perfect 6 nt duplex with the 5′ UTR ( Fig. 2 ), which can explain the observation of a reduction in translation. 3.2 Influence of seed requirement and accessibility on Yfr1 target prediction The prediction of sRNA targets with I nta RNA is based on two assumptions: (i) a seed region is required to initiate the interaction [in analogy to the 5′ seed region of miRNAs (Bartel,  2009 )] and (ii) the accessibility of the interaction sites is important for target recognition. A previous study on a dataset of 18 different sRNA–mRNA interactions presented evidence that the incorporation of these two requirements improves the prediction quality of I nta RNA (Busch  et al. ,  2008 ). Here, we investigated the importance of accessibility and of a seed region in a practical application, namely the identification of new targets for the Yfr1 sRNA. Therefore, we computed lists of putative targets without enforcing a seed region and with enforcing a seed at the conserved Yfr1 motif. When requiring the fixed seed position, we obtained a short list of only 29 target candidates with the experimentally validated Yfr1 targets PMM1119 and PMM1121 ranked at positions 1 and 3, respectively ( Table 1 ). Without the seed requirement, 1418 target candidates were obtained with the two true positives ranked at positions 1 and 4. Even without using a seed constraint, the interactions predicted for the true positives include the conserved single-stranded region of Yfr1. Thus, the combination of complementarity and accessibility alone resulted in interactions with an implicit seed. In addition to the effect of a seed requirement, we studied the influence of accessibility on the Yfr1 target prediction. In the original I nta RNA scoring, hybridization energy and interaction site accessibilities contribute equally to the energy score. Here, we tested a modified energy score, where the interaction site accessibility of both sequences was weighted by factor α with the values 0, 0.5 and 1. For both seed requirements studied, the true positives PMM1119 and PMM1121 were ranked best with the original scoring ( Table 1 ). One interesting observation was that in the case of Yfr1, a full weighting of the interaction site accessibility, i.e. α=1, was required for a correct target site prediction. When both the seed region and accessibility were neglected, the two verified Yfr1 targets were not found within the top 150 predictions. When the seed position was fixed to the conserved region but accessibility was not included in the scoring, the validated targets were ranked at positions 22 and 32. However, in this case, predicted interactions involved almost the entire Yfr1 sequence (data not shown). This observation is consistent with the findings of Tjaden  et al.  ( 2006 ) and Busch  et al.  ( 2008 ), who showed that an energy model based solely on hybridization energy tends to maximize the length of hybridization, resulting in a small fraction of correctly predicted base pairs (i.e. low positive predictive value). 4 CONCLUSIONS In this study, we show that Yfr1 sRNA modulates the translation of two high-scoring predicted targets by an antisense interaction. Both target mRNAs code for outer membrane proteins (Hansel  et al. ,  1998 ). This class of proteins constitutes a major functional class that is regulated by bacterial sRNAs in  E.coli  and  Salmonella  (Waters and Storz,  2009 ). The result was surprising as, until now, no highly abundant sRNAs have been shown to act via base pair interaction. However, both mRNA targets identified herein are also highly abundant [among the 10 most expressed mRNAs and with long half-lives of about 30 min (C.Steglich, unpublished data)], which may require a high copy number of Yfr1 for efficient regulation. Furthermore, an mRNA with a long half-life can be regulated more efficiently by translational control than by transcriptional control. Additionally, we assessed the influence of seed regions and interaction site accessibility on the prediction quality of Yfr1 targets. As with the  Salmonella  sRNAs GcvB and RybB, Yfr1 contains a conserved single-stranded region, which seems to constitute a perfect interaction seed. When requiring this region as seed for the target prediction, the number of putative Yfr1 targets was remarkably smaller without seed requirement (29 versus 1418 candidates), although the two true positives were under the highest ranking candidates in both settings. When neglecting both accessibility and a seed region, the true Yfr1 targets could not be found amongst the top 150 predictions. In conclusion, the combination of computational and experimental methods, as presented in this study, proved to be an appropriate approach for the identification of sRNA targets in organisms where genetic manipulation constitutes a great challenge. Funding : German Research Foundation DFG Priority Program SPP1258 Sensory and Regulatory RNAs in Prokaryotes (grant number BA 2168/2-1 to R.B., Ste 1119/2-1 to C.S.]; German Federal Ministry of Education and Research FRISYS - Freiburg Initiative for Systems Biology (grant number 0313921 to R.B.). Conflict of Interest : none declared. 
          Supplementary Material 
          
             
                [Supplementary Data] 
             
             
             
             
             
          
       </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Optimization strategies for fast detection of positive selection on phylogenetic trees</Title>
    <Doi>10.1093/bioinformatics/btt760</Doi>
    <Authors>Valle Mario, Schabauer Hannes, Pacher Christoph, Stockinger Heinz, Stamatakis Alexandros, Robinson-Rechavi Marc, Salamin Nicolas</Authors>
    <Abstract>Motivation: The detection of positive selection is widely used to study gene and genome evolution, but its application remains limited by the high computational cost of existing implementations. We present a series of computational optimizations for more efficient estimation of the likelihood function on large-scale phylogenetic problems. We illustrate our approach using the branch-site model of codon evolution.</Abstract>
    <Body>1 INTRODUCTION The development of evolutionary models has a long tradition in phylogenetics, and recent advances have enhanced our understanding of the molecular mechanisms involved. At the heart of these advances is the democratization of the use of the likelihood framework, which was made possible by algorithmic developments ( Felsenstein, 1981 ) and the wide availability of powerful computing platforms. The surge of genomic data is, however, pushing the limits of current implementations [e.g. ( Rannala and Yang, 2008 )] and demands for the developments of better and more efficient ways to compute the phylogenetic likelihood function (PLF). The development of codon models is a good example to illustrate these current challenges and the benefits that can be reached by improving the efficiency of current likelihood calculations ( Gil  et al. , 2013 ). There are clear advantages to use codon models in phylogenetics ( Seo and Kishino, 2008 ), but these are currently not widely used because of the large computational burdens involved ( Anisimova and Kosiol, 2009 ). Further, the detection of positive selection has been facilitated by the development of new codon models. However, their application to genome-scale data comprising a large number of species, or individuals in the case of population genomic studies, remains challenging. Thus, there exists an urgent need for improved implementations and novel optimization techniques to analyse emerging genomic datasets ( Lemey  et al. , 2012 ;  Murrell  et al. , 2012 ;  Schabauer  et al. , 2012 ). The prevalent approach for detecting positive selection in protein-coding genes is to use Markov models of codon substitution to estimate the ratio of non-synonymous to synonymous changes along the branches of a phylogenetic tree ( Yang, 2006 ). The branch-site model (BSM) [ Yang, 2006  (Section 8.4);  Zhang  et al. , 2005 ] allows to detect positive selection that affects a subset of codon sites for a subset of branches in a phylogenetic tree. This model is particularly useful to perform interspecific comparisons and is probably the most widely used approach for this specific purpose. The test compares a model that assumes positive selection on one branch or on a set of a priori specified branches (hypothesis  H 1 ) with a null model that does not incorporate positive selection (hypothesis  H 0 ). If the test is significant, the Bayes Empirical Bayes (BEB) method is used to compute the posterior probability of each particular codon to evolve under positive selection along the specified branches ( Yang  et al. , 2005 ). In CodeML, the test is usually applied iteratively and independently to each branch of a given phylogenetic tree ( Anisimova and Yang, 2007 ;  Studer  et al. , 2008 ). This approach is compute bound, and although alternatives have recently been proposed, the limiting factor of such analyses still lies with the repeated calls to compute the PLF. For example, the estimation of positive selection on a large genomic vertebrate dataset ( Proux  et al. , 2009 ) shows the enormous computational requirements of such analyses [approx. 100 CPU years for  each  release of the Selectome database ( Kraut  et al. , 2010 )]. As a consequence, large gene trees, comprising more than 100 sequences, are usually excluded and faster implementations of the BSM are urgently needed. This clearly illustrates the need to further optimize current software and to develop more efficient computational approaches for maximum likelihood inference on phylogenetic trees. Several recent studies introduced techniques for efficiently computing positive selection on the branches of a phylogenetic tree. One idea is to use stochastic mapping to count substitutions along the branches of a tree and thereby derive dN/dS ratios ( Dutheil  et al. , 2012 ;  Lemey  et al. , 2012 ). While this approach is fast, it is computationally distinct. Alternatively, new models have been proposed to avoid the likelihood ratio test (LRT) estimation of positive selection for all branches of the tree. Instead, branch assignments are considered as a random effect within a mixed effect framework ( Murrell  et al. , 2012 ). Their model notably differs from the BSM ( Zhang  et al. , 2005 ) in that putative positive selection is not optimized on a priori defined branches, but over a subset of branches which are determined by the software. This technique reduces the computational cost of the test, but the accuracy and robustness of this new model is not yet fully characterized. Moreover, the authors introduced solutions for parallelizing BSM computations, but the parallel approach is not discussed in their article. The bottleneck in efficiency of phylogenetic software is commonly the PLF, as the majority of runtime is spent here. In ( Stamatakis, 2011 , p.2), the PLF is reported to consume &gt;95% of total execution time in maximum likelihood and Bayesian tools for phylogenetic tree reconstruction. Although this was estimated when searching for the best tree topology, which is a key component of phylogenetic computations but not the focus of this article, the PLF is still the core element in  all  phylogenetic applications using maximum likelihood. All these areas would therefore benefit from an optimized PLF. Recent discussions have proposed to use data augmentation strategies to speed up the likelihood calculations by using heuristics to simplify the estimation of the conditional vectors at each node ( Rodrigue and Aris-Brosou, 2011 ). However, there are still opportunities for improving the PLF with respect to sequential efficiency and parallelization techniques. Our main objective is therefore to propose methodological and algorithmic improvements and parallelization strategies to compute the PLF  without  modifying the underlying evolutionary model. Our optimizations and parallelizations yield substantial speedups in the likelihood computations. Hence, we can apply the BSM to large trees of several hundreds of sequences and obtain results in feasible times. These computational optimizations are thus of broad applicability to further likelihood-based phylogenetic software, including but not limited to nucleotide- and amino acid-based phylogenetic analyses in both the maximum likelihood and Bayesian frameworks ( Nielsen, 2005 ). 1.1 Number of elementary tree operations In the BSM framework, four site classes 0, 1, 2a and 2b are applied to model combinations of purifying selection, neutral evolution, and positive selection on foreground and background branches. When computing hypotheses  H 0  and  H 1 , each site class has its distinct proportion according to its contribution to the overall likelihood (cf. the  supplementary material  for an introduction to the BSM). These proportions only depend on the two parameters  p 0  and  p 1 ; each site class has a specific ω value for its selective pressure in the foreground and in the background.   is in the interval (0,1),   and  either 
  (foreground for  H 1 )  or 
  (foreground for  H 0 ).   corresponds to  , respectively. Computing the likelihood requires computing the transition probabilities for a given branch length  t  by computing the matrix exponential  , where  Q  is the instantaneous substitution rate matrix,  S  is the symmetric codon substitution matrix and Π is the diagonal matrix of codon frequencies. The resulting probability matrix  P t  is used to update the corresponding conditional probability vector (CPV)  w , that is,  . Each CPV models the site-wise transition between 61 codon states (universal genetic code) along each branch of the phylogenetic tree. This operation is applied to all sites of the multiple sequence alignment (MSA) and to all nodes of the tree by means of a post-order tree traversal. The CPU-intensive computation of the CPV entails the following three computational kernels that operate on real dense matrices (similar to SlimCodeML, see  Section 2.1.2 ): (i) eigendecomposition of a symmetric matrix [see, e.g. ( Bai  et al. , 2000 )], (ii) multiplication of a matrix by its transpose (resulting in a symmetric matrix) and (iii) multiplication of a symmetric matrix by a vector. 1.1.1 How many decompositions? To compute  e Qt  we need to decompose  Q  for each distinct combination of parameters κ (transition to transversion rate),   and ω. The   are constant over site classes and parameter optimization steps; κ may change at each parameter optimization step (but is constant over site classes); ω varies among optimization steps  and  site classes. For each distinct value of ω,  Q  is distinct and therefore needs to be decomposed separately. There are three distinct ω values over all site classes; hence, we need to decompose three  Q  matrices in the first parameter optimization step. For subsequent steps,   remains constant, but  Q 1  may change because of a new κ value. The total number of  Q  decompositions does not depend on the number of branches in the tree nor on the number of sites in the MSA. In the general case, the number of  Q  matrices depends on the number of unique substitution matrices in the model, which can be large in mixture models [e.g. ( Lartillot and Philippe, 2004 ;  Venditti  et al. , 2008 )]. With respect to other evolutionary models, similar optimizations may be applicable. 1.1.2 How many matrix–matrix multiplications? P t  has to be computed for each combination of  Q  and  t . For our case of binary trees, the number of branches in the phylogeny equals   where  n  is the number of extant taxa. For each distinct  Q , branches have to be computed separately. The BSM applies  Q 0  and  Q 1  to each branch, but  Q 2  only to foreground branches. In other words,  P t  has to be computed for all branches using  Q 0  and  Q 1  (site classes 0 and 1),  and  in addition on the foreground branch(es) by using  Q 2  (site classes 2a and 2b). Therefore, we need to compute  P t 
  times for  m  branches in the phylogeny and  l  foreground branches; this yields   branches when using a single foreground branch. Overall, we need to compute 17 distinct  P  matrices in our example 1. This matrix–matrix multiplication is also applied in further evolutionary models based on substitution matrices. 1.1.3 How many matrix–vector computations? In a straightforward approach, each CPV is computed along each branch for all sites and all site classes. In our example this makes   CPV computations. If a CPV connected to a leaf is computed on ‘clean’ data [no ambiguity symbols in MSA ( Comnish-Bowden, 1985 )], the CPV at the leaf only contains a single 1 (0 elsewhere). In this case, computing the resulting CPV simplifies to selecting the corresponding column of the P matrix. In the general case, an upper limit of the number of involved matrix–vector multiplications per site class is the number of branches in the phylogeny × the number of sites in the MSA. Certainly, this number can be decreased depending on similarities in the codons as discussed in  Section 2.1.1  (‘subtrees reuse’). Likewise, this step is important to all other evolutionary models based on substitution matrices. Further computational savings are possible. In this context, we refer to a ‘subtree’ as a connected part of the phylogeny where at least one node is a leaf. Whenever a particular branch of a single site applies the same  P  and all other CPVs of its subtree match, the particular CPV has a ‘twin’ in another site class and needs to be computed only once. In  Figure 1 , such matching CPVs are identified by matching indexes. For example, CPV23 appears in site class 1 and in site class 2b, as also CPV20 and CPV21 have twins, and they pairwise apply matching  P  matrices (here, all based on  Q 1 ). These redundancies are caused by matching   values for site classes 0 and 2a and by matching   values for site classes 1 and 2b. In our example, this means that only 40 out of 64 (62.5%) CPVs have distinct values and will hence have to be computed. CPVs are computed recursively via a postorder traversal propagating from the leaves towards the root ( Felsenstein, 1981 ). Hence, for the BSM in general, the number of distinct CPVs depends on the location of the foreground branch in the tree (the closer to the root, the less CPV computations are required).
 Fig. 1. Analysis on how many elementary subtree computations are necessary in the branch-site model; CPV m  correspond to  m  distinct conditional probability vectors, where matching  m  need to be computed only once; Q  identify three distinct Q matrices for distinct   values 2 IMPROVEMENTS Here we discuss optimization techniques that we propose. Note that we have not added any heuristics, and each of the following improvements is supposed to be beneficial independent of the number of species and independent of the number of alignment sites. Specific implementation issues are described along with each optimization technique. 2.1 Sequential improvements 2.1.1 Subtrees reuse The per-site likelihoods for a MSA are independent of each other and can thus be computed in an arbitrary order. If two or more sites of the MSA are identical, it suffices to only compute the logarithmic likelihood (lnL) on one site and multiply it by the number of identical sites to obtain the total lnL. This technique is used in most likelihood-based software, but there are further redundant computations caused by re-occurring patterns in the MSA. In each subtree, there is a potential to economize CPV computations for different sites of the MSA. If the same state appears at two or more sites of a sequence, all occurrences yield identical CPVs at the particular leaf. If the patterns of the sub-alignment induced by a subtree match are identical for two or more sites, the corresponding CPVs for the two sites are also identical. However, identical patterns in the sub-alignments induced by a subtree need to be identified first. The identification of such identical patterns in sub-alignments can be done, e.g. by searching (i) sequentially or (ii) using a symbol table ( Sedgewick and Wayne, 2011 , p.361). In the latter case, the  key  is the index of the CPV within the tree, and the  value  associated with the key is its CPV. In the straightforward approach (i), there are no costs on storing values, but up to  m  – 1 lookups for a matching subpattern, where  m  is the length of the MSA. For huge MSAs, it may be advantageous to implement the second approach, where the additional cost for storing or linking site patterns is compensated by a faster lookup. In FastCodeML, we identify reusable subtree patterns in a preprocessing step and tag each node with the codon sequence identified by the subtree rooted in this node. Subsequently, a lookup of these tags for all sites with identical subtrees is done. Once identified, the CPV that can be re-used is linked via a pointer in the reusing tree, that is, this saves the costs of computing this particular CPV. The unused subtree can be freed to reduce memory consumption. In the example of  Figure 2 , computing the two CPVs incident to two leaves in box ① and the CPV at  ②  are redundant, because both codon sites feature an identical subtree: all involved CPVs match. Thus three CPV computations can be saved.
 Fig. 2. Subtrees reuse strategy depicted for two (not necessarily neighboring) sites in the MSA; in ( a ) subtree (1) contains identical codons for both sites; consequently, in ( b ) the CPVs for both sites are identical and need to be computed only once (dotted line) Related techniques for extending pattern detection and re-use in the MSA to the subtree level have already been proposed ( Izquierdo-Carrasco  et al. , 2011 ;  Stamatakis  et al. , 2002 ;  Sumner and Charleston, 2010 ). However, they focus on detecting patterns and avoiding redundant likelihood computations on trees whose topologies change in the course of ML tree search. For dynamically changing trees, a trade-off between the pattern detection and memory storage costs and the amount of saved computations needs to be achieved. To reduce the cost of pattern detection, the initial implementation of the Subtree Equality Vector (SEV) technique ( Stamatakis  et al. , 2002 ) only considered subtree patterns that contained a single identical character. The book keeping was subsequently further simplified to sites consisting entirely of gaps ( Izquierdo-Carrasco  et al. , 2011 ). In  Kosakovsky Pond and Muse (2004) , the authors suggest to sort nucleotide-based MSAs by site similarity to avoid redundant computations. This approach minimizes memory consumption, as only a subset of sites needs to be kept in memory. However, this incurs additional costs for rearranging the sites in order to maximize the number of lookups from neighboring sites. The memory consumption for our application scenario (Selectome database updates) does not represent a limiting factor. Hence, all CPVs can be kept in memory, avoiding the expensive reordering of sites. However, especially for memory-intensive approaches, it may be more effective to keep only a subset of all CPVs in memory and consider site sorting. 2.1.2 New matrix exponential and CPV computation In  Schabauer  et al.  (2012) , we transformed the problem of computing the matrix exponential of non-symmetric  Qt  into a symmetric problem as follows: we define the symmetric matrix   and compute its eigendecomposition  . By introducing  , the matrix exponential of  Qt  becomes  . An additional modification transforms the final asymmetric matrix–vector multiplication   into a symmetric matrix–vector product:
 (1) 
 (2) 
 Note that   is by construction a symmetric matrix, whereas   is generally asymmetric. The advantage of this modification is that the symmetry reduces the number of necessary matrix memory accesses by approx. 50% ( Golub and Van Loan, 2013 , p.18). This technique has been implemented in FastCodeML. 2.1.3 LRT optimization When optimizing parameter values for H 0  and H 1  one after the other, one can save on parameter optimization steps. Each step in the parameter optimization procedure improves the associated lnL of the tree until convergence has been reached. In this discussion, the optimizer may modify all parameter values at each single step. One can either (i) optimize H 0  first with high accuracy and iteratively improve H 1  afterwards: once   becomes larger than  , the parameter optimization for H 1  can be stopped because the LRT is already significant. This potentially saves optimization steps for H 1 . Or we can (ii) optimize H 1  first, then proceed analogously: the parameters of H 0  are optimized until   becomes smaller than  . In general, a significant LRT (i.e. detecting positive selection) is a relatively rare event ( Kosiol  et al. , 2008 ;  Studer  et al. , 2008 ). Strategy (i) saves optimization steps if positive selection occurs; strategy (ii) saves optimization steps if not. Consequently, without prior knowledge of the frequency of occurrence of positive selection in the MSA at hand, strategy (ii) (implemented in FastCodeML) will yield larger savings. If the LRT is significant, a BEB is applied to identify the sites under positive selection. Otherwise, FastCodeML does not execute the BEB, in contrast to CodeML. In the general case, this optimization is applicable if different models are compared, where each of them is optimized iteratively. 2.2 Parallelization While the parallelization of ML-based nucleotide- protein- and codon models has already been addressed ( Stamatakis, 2011 ) (e.g. RAxML, IQPNNI, HyPhy), it has mostly been in the context of tree topology optimization, and not for the likelihood itself. The main challenge in parallelizing ML-based phylogeny computations comes from the tree structure that leads to an irregular domain decomposition ( Tomko, 1995 ). An efficient parallelization of the BSM is even more challenging due to its site classes and dependencies in between. Our implementation optimizes simultaneously all the parameters. The maximizer acts as an impenetrable boundary for parallelization, and we distinguish parallelization ‘above’ (coarse-grain) and ‘within’ (fine-grain) this boundary (cf.  supplementary material ,  Fig. 1 ). 2.2.1 Coarse-grain parallelization: Gene-wise  parallelization . Because distinct genes typically have different evolutionary histories with distinct branch lengths and evolutionary parameters, phylogenies for genes are commonly estimated independently for each gene. Consequently, single genes cannot be concatenated into multi-gene alignments to attain high scalability by means of a fine-grain parallelization of the likelihood function [see, e.g. ( Stamatakis and Ott, 2009 )]. Here we test for selection independently (gene-wise), these analyses can be carried out in an embarrassingly parallel way [see, e.g. ( Foster, 1995 , p.21)]. Foreground branch  parallelization .  A further BSM parallelization option is the simultaneous analysis of distinct foreground branches. This is possible because we want to test for positive selection on each branch of a given phylogeny. Thus, the   tests for positive selection, where  n  is the number of taxa, can be conducted in parallel by duplicating the tree data structure and CPVs. Under this parallelization strategy, a dedicated master process broadcasts all model parameters, tree topologies and branch lengths to all worker nodes. The workers then conduct the tests independently of each other on different foreground branches of the same tree. Afterwards, the worker nodes return the estimated parameter values and the lnL scores to the master process. We implemented this approach using MPI ( Message Passing Interface Forum, 1994 ). The foreground-branch based parallelization can be combined with a site-wise fine-grain parallelization of the per-tree likelihood computations ( Section 2.2.2 ) into a  hybrid  parallelization scheme. Hypotheses  parallelization .  Note that for each foreground branch, hypotheses  H 0  and  H 1  can be computed independently and simultaneously, thus increasing the degree of parallelism. However, the simultaneous computation of  H 0  and  H 1  prevents us from using the aforementioned LRT optimization ( Section 2.1.3 ). Although the LRT and the subsequent BEB must be computed after  H 0  and  H 1 , they can be parallelized between different foreground branch computations. This parallelization strategy can be applied whenever two evolutionary models are compared. It is implemented in FastCodeML via the same master-worker scheme. 2.2.2 Fine-grain parallelization: Site-wise  parallelization . A common way to parallelize likelihood computations on shared memory architectures is by parallelizing over the sites of the MSA. This site-wise parallelization can be implemented using OpenMP or POSIX Threads. MPI-based implementations exist but focus on large MSAs that are outside the scope of this article. However, while our subtree patterns re-use scheme ( Section 2.1.1 ) reduces the number of computations along the branches, it poses a load balance challenge: (i) a particular CPV for a site can only be computed after the site whose results it reuses (i.e. data dependency) has been computed and (ii) a site that reuses a previously computed CPV exhibits a smaller workload which leads to load imbalance. The load balancing strategy we use in FastCodeML subdivides the alignment sites into groups such that each group exclusively reuses subtrees from the previous groups ( Fig. 3 ). Each group is assigned a rank value starting from zero. CPVs from groups with lower rank values can potentially be reused. The first group does not reuse any subtree. All subtrees of a group can be parallelized, because they are independent of each other. The groups are then computed sequentially in order of rank. To balance the load for each group, subtrees can be moved to higher ranked groups. To increase parallelism, the trees of each group are replicated for each site class that should be computed until no lower rank group depends on it. The parallelization inside each group has been implemented using OpenMP.
 Fig. 3. Load balancing strategy: the sites of the tree are grouped so that each group depends only on groups at its left (continuous lines). A tree can be moved to a group to its right (dashed line) only if it has no dependencies from other trees in intermediate groups This site-wise parallelization strategy including load balancing can likewise be applied to nucleotide- or protein-based MSAs. The parallel performance may vary due to different computational load per site. 2.3 Implementation FastCodeML has been implemented from scratch (except for the BEB that was largely taken from the CodeML codebase) in ISO C++ 2003 utilizing BLAS and LAPACK for linear algebra operations, and Spirit ( http://www.boost.org/doc/libs/release/libs/spirit/ ) for tree parsing. We use the parameter optimization codebase of CodeML. 3 EVALUATION We measure median runtimes of 10 individual runs for each evaluation (three on the large scale analysis in  Section 3.5 ). Speedup values are determined by  , where  T 1  is the runtime (elapsed time, wall-clock time) of the reference execution and  T 2  the runtime of the execution to be evaluated on the same dataset; for a  relative  speedup  T 1  and  T 2  denominate runtimes of the same executable, while for the  absolute  speedup  T 1  is strictly the original CodeML. Initial branch lengths were read from file, while model parameters are initialized randomly. Memory consumption of CodeML, SlimCodeML and FastCodeML for these datasets is not a limiting factor and therefore not performance critical. Although a single executable can be used for all subsequent evaluations, we built sequential, OpenMP parallelized, MPI parallelized and hybrid executables separately. A summary of the platforms used can be found in the  supplementary material . 3.1 Datasets Table 1  contains the six datasets we used for evaluation. With respect to the Selectome database, these empirical datasets are representative for the cases: (D1) small number of species/medium sequence length; (D2) small number of species/large sequence length; (D3) medium number of species/small sequence length; (D4) large number of species/short sequence length; (D5) a simulated dataset with positive selection based on dataset D1 (using PAML’s evolver choosing ‘evolverNSbranchsites’ for the BSM with  ). Finally, we analyse in D6 a very large rbcL dataset ( Grass Phylogeny Working Group II, 2012 ) which cannot be processed in a feasible time by CodeML.
 Table 1. Test datasets of our analyses; remaining branches is the percentage of non-redundant branches for the given data over all sites of the alignment; dataset D5 is generated based on ENSGT00390000016702.Primates.1 with  Abbr. Full name No. of species No. of branches Remaining branches [%] Length (codons) D1 ENSGT00390000016702.Primates.1 7 12 37.74 299 D2 ENSGT00530000063518.Primates.1 95 188 75.49 39 D3 ENSGT00550000073950.Euteleostomi.7 25 48 56.31 67 D4 ENSGT00580000081590.Primates.1 6 10 20.92 5004 D5 Generated by evolver (PAML) 7 12 38.04 282 D6 Grass_rbcL 506 1242 19.54 414 3.2 Accuracy In  Table 2  we analyse the accuracy of FastCodeml with respect to lnLs and LRT scores. We use SlimCodeML as a proxy for good accuracy, as it gives very similar results as CodeML ( Schabauer  et al. , 2012 ), which is the established gold standard. We note that the accuracy of computed lnLs is much higher than typically required to discriminate between significant and insignificant LRTs.
 Table 2. Accuracy of SlimCodeML and FastCodeml on Macpro;   is the absolute difference of lnLs comparing either SlimCodeML or FastCodeML with CodeML on  H 0  ( H 1 ), respectively Dataset LRT pos. selection SlimCode versus CodeML D1 no (✓) D2 no (✓) D3 no (✓) D4 no (✓) D5 10.4 site 239 (✓) FastCodeML versus CodeML D1 no (✓) D2 no (✓) D3 no (✓) D4 no (✓) D5 10.4 site 239 (✓) Note : ‘✓’ indicates agreement of the computed result with CodeML. 3.3 Sequential runtimes Sequential speedups of FastCodeML (single-threaded) versus CodeML and SlimCodeML for five datasets ( H 0  and  H 1 , respectively) on platform Macpro (cf.  supplementary material ) are depicted in  Figure 4 ; here, FastCodeML includes the following improvements: faster matrix exponentiation ( Section 2.1.2 ) and subtrees reuse ( Section 2.1.1 ). LRT optimization ( Section 2.1.3 ) is not considered, as either  H 0  or  H 1  is computed per run. We observe speedups of FastCodeML versus CodeML ranging from 2.6 to 5.8. The sequential FastCodeML is significantly faster than both CodeML and SlimCodeML on all five datasets.
 Fig. 4. Sequential speedups of FastCodeML in comparison with CodeML and SlimCodeML on Macpro for  H 0  and  H 1 , respectively 3.4 Parallel runtimes 3.4.1 Site-wise parallelization Figure 5  shows the scaling of FastCodeML on a site-wise (OpenMP based) parallelization strategy for dataset D2 on 1–12 CPU cores (one thread per core); we observe relative speedups comparing FastCodeML in   versus 1 threads, reaching 11.1 for 12 cores without subtrees reuse, and speedups up to 7.6 for 12 cores with subtrees reuse. These relative speedups correspond to absolute speedups versus CodeML of up to 23.4 without subtrees reuse, and speedups up to 19.9 with subtrees reuse. While scaling of subtrees reuse is slightly worse than without subtrees reuse, absolute runtimes on this particular platform and dataset suggest to enable subtrees reuse on 1–11 cores but not on 12. The worse scaling of subtrees reuse is presumably caused by load imbalance. Due to differences in the sequential performance of subtrees reuse, we also expect the performance of parallel subtrees reuse to vary with different datasets. In general, the effectiveness of parallel subtrees reuse is a trade-off between the number of redundant branches versus the data dependencies introduced.
 Fig. 5. Parallel site-wise relative ( top ) and absolute ( bottom ) speedups of FastCodeML on Castor on dataset D2 for  H 1 3.4.2 Foreground branch-based parallelization Figure 6  depicts the relative scaling of FastCodeML on a foreground-branch based parallelization strategy. The evaluation has been done for dataset D3 on 1–7 worker nodes (single thread per node). Due to the master–worker scheme used, performance gains are observed for two or more worker nodes. The analysis is done for all possible 22 foreground branches, where the runtime for CodeML is measured only on a single foreground branch but multiplied by 22; running CodeML on all foreground branches is expected to consume more than a day. We observe relative speedups of up to 5.9 on 7 worker nodes, which corresponds to absolute speedups from 3.3 to 19.4. In general, the relative speedup for foreground branch-based parallelizations benefits from a high ratio of foreground branches to available nodes, as the workload can more easily be divided into balanced parts.
 Fig. 6. Parallel foreground branch (MPI based) relative speedups of FastCodeML for dataset D3 on Castor for  H 1 ; only a  single  CPU core per node was used 3.4.3 Hybrid parallelization Figure 7  depicts absolute scaling of FastCodeML on a hybrid (foreground branch and site-wise) parallelization strategy implemented using OpenMP and MPI on 1–7 worker nodes, where all 12 CPU cores are used. Corresponding runtimes, relative and absolute speedup values are summarized in  Table 3 . We observe relative speedups up to 6.3 on 7 worker nodes, which corresponds to absolute speedups up to 170.9.
 Fig. 7. Parallel hybrid (OpenMP and MPI based) scaling of FastCodeML for dataset D3 on Castor for  H 1 
 Table 3. Overall parallel performance of FastCodeml versus CodeML on Castor for dataset D3 on all possible foreground branches for  H 1 ; CodeML runtime for absolute speedups is extrapolated from computing a single foreground branch Worker nodes (cores) FastCodeML runtime [s] Rel. speedup Abs. speedup 1 (12) 429 1 27.6 2 (24) 218 2 54.2 3 (36) 151 2.9 78.4 4 (48) 114 3.8 103.7 5 (60) 93 4.7 126.5 6 (72) 81 5.4 145.3 7 (84) 69 6.3 170.9 3.5 Large scale analysis A large scale analysis has been conducted to prove the use of FastCodeML beyond the capabilities of CodeML. In initial tests, we verified that dataset D6 achieves its best runtime performance on platform Castor (cf.  supplementary material ) by using all 12 available cores per node and by reusing subtrees ( Section 2.1.1 ). We analysed D6 for  H 0  and  H 1  running FastCodeML (multi-threading) on 12 CPU cores and determined average runtimes of three test runs. The average runtime of FastCodeML on dataset D6 is 21.9 h for  H 0  and 31.9 h for  H 1 . Due to time restrictions, we evaluated only a single iteration of CodeML for D6 which took 2.2 h on  H 0  (367 iteration steps) and 2.3 h on  H 1  (426 iteration steps) on the same platform. As we apply the same parameter optimization codes, we use the average number of optimization steps of FastCodeML on dataset D6 for the following speedup metric: we extrapolate that CodeML would have finished executing in approximately   h (i.e. ca. 33.6 days) for  H 0  and   h (i.e. ca. 40.8 days) for  H 1 . The estimated speedups comparing the single threaded CodeML with FastCodeML running in 12 threads is thus 36.9 for  H 0  and 30.7 for  H 1 . In this example, the LRT optimization saves 268 optimization steps for  H 1  (63%). 4 CONCLUSIONS We introduced here three sequential code optimizations: an improved matrix exponential, subtrees reuse and LRT optimization. We observed significant speedups versus both CodeML and our previous version SlimCodeML, and the first two optimizations can be used in various likelihood computations in phylogenetics. Moreover, we present a parallelization strategy that uses a fine-grain and a coarse-grain approach. Overall, our improvements allow for testing selection on phylogenetic trees which exceed the possibilities of the original CodeML software; this is crucial to tackle the genomic data avalanche. The discussed improvements are motivated by the branch-site model but can, due to the likelihood framework, be extended to nucleotide- and amino acid-based MSAs as well as Bayesian approaches. We briefly identified such opportunities where applicable, but an extensive discussion is subject to future work. The optimization of the likelihood surface for phylogenetics problems is complex and we have started experimenting with the alternative parameter optimizers available in NLopt ( http://ab-initio.mit.edu/wiki/index.php/NLopt ). It may be interesting to compare different implementations of the Broyden–Fletcher–Goldfard–Shanno (BFGS) optimization method, but a deeper investigation of the global and derivative-free optimizers is needed to better understand the potential solutions to find the maximum likelihood estimator for complex evolutionary models. In a future version the dependencies between nodes could be modelled as a directed acyclic graph and the parallelism be based on a dataflow model ( YarKhan  et al. , 2011 ) to study and potentially further improve parallel performance. Moreover, the site classes could be included into the dependency graph. This way a more fine-grained parallelism could be achieved. Increasing the parallel performance becomes crucial with the trend of more parallelism in future computer platforms ( Dongarra, 2012 ). 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Identification of OBO nonalignments and its implications for OBO enrichment</Title>
    <Doi>10.1093/bioinformatics/btn194</Doi>
    <Authors>Bada Michael, Hunter Lawrence</Authors>
    <Abstract>Motivation: Existing projects that focus on the semiautomatic addition of links between existing terms in the Open Biomedical Ontologies can take advantage of reasoners that can make new inferences between terms that are based on the added formal definitions and that reflect nonalignments between the linked terms. However, these projects require that these definitions be necessary and sufficient, a strong requirement that often does not hold. If such definitions cannot be added, the reasoners cannot point to the nonalignments through the suggestion of new inferences.</Abstract>
    <Body>1 INTRODUCTION Several efforts in recent years have focused on the semiautomatic addition of links between existing terms in the Open Biomedical Ontologies (OBOs) through the creation of formal definitions of these terms using more atomic terms, a process to which we refer as  ontology enrichment . Of note, the Gene Ontology Next Generation (GONG) project first used the description-logic-based language DAML+OIL to formally define 250 Gene Ontology (GO) metabolism terms using MeSH terms (Wroe  et al. ,  2003 ), and later OWL to formally define a much larger number of GO metabolism, binding and transport terms again using MeSH terms (Aranguren,  2004 ); this project has since evolved into the more general Biological Ontology Next Generation (BONG), which currently exists as a plugin to the Protege ontology editor. The Obol effort uses a series of Prolog production rules that can be used to decompose a given matching GO term into an Aristotelean genus (category) and one or more differentiae (necessary and sufficient conditions that differentiate the term from other terms of the same genus); the Gene Ontology Consortium is currently using Obol to generate Aristotelean definitions of OBO terms that refer to other OBO terms (Mungall,  2004 ). In our frame-based Protege ontology-enrichment effort, we have created over 9600 assertions linking terms in the GO (The Gene Ontology Consortium,  2000 ), Chemical Entities of Biological Interest (ChEBI) ontology (Degtyarenko,  2003 ), and the Cell Type Ontology (CL) (Bard  et al. ,  2005 ); these base assertions have been integrated into this set of ontologies such that each assertion is consistent with all assertions made at more general levels (Bada and Hunter,  2007 ). Both GONG and Obol have been able to take advantage of associated reasoners; for the former, an OWL reasoner can be used, while for the latter, the Aristotelean definitions can be imported into OBO-Edit ( www.oboedit.org ), the primary tool in which OBOs are developed, and its associated reasoner invoked. A great advantage of using such a reasoner is its ability to make new inferences derived from the added formal term definitions. For example, in the second published GONG study, using the newly added formal defintions for the GO molecular function (MF) terms  neurotransmitter binding  and  glutamate binding  (which use the MeSH terms  Neurotransmitters  and  Glutamates , respectively), the OWL reasoner inferred that  neurotransmitter binding  subsumes  glutamate binding , a link absent at that point in GO. However, both GONG/BONG and Obol/OBO-Edit require that these definitions use necessary and sufficient conditions in order for these inferences to be made. This is a strong requirement that does not hold bidirectionally in many, if not most cases: it is necessary and sufficient that catecholamine transport is a transport that results in the directed movement of a catecholamine. However, the semantics of OWL or OBO say that, for an existential restriction expressed for a subject class A linking it to an object class B via property p, each instance of A must have at least one value from B for p. Since we cannot say that every catecholamine takes part in a catecholamine-transport process, it is not even possible to make this a necessary assertion. Consequently, using terms from these two terminologies that have been linked, these new subsumptive inferences can only be made between subject terms for which necessary and sufficient definitions can be created (e.g. substance-transport terms) and not with the object terms (e.g. the substances that are being transported) used in these definitions. The inferences that are made by these reasoners point to what we call  nonalignments— subsets of terms that are linked (other than via  is_a ), but that are not aligned in that the terms of one side of the links are linked by subsumption while the terms of the other side are not. (The nonalignments we identify all consist of subject terms that are subsumptively linked and object terms that are not subsumptively linked.) For example, as can be seen in  Figure 1 , we have linked the ChEBI term  chlorohydrocarbons  to the GO term  chlorinated hydrocarbon metabolism  and also the ChEBI term  1,3-dichloro-2-propanol  to the GO term  1,3-dichloro-2-propanol metabolism . These pairs of terms are not aligned in that  1,3-dichloro-2-propanol  is subsumed by  chlorohydrocarbons  in ChEBI, but  1,3-dichloro-2-propanol metabolism  is not subsumed by  chlorinated hydrocarbon metabolism  in GO. We expect the two sides to be aligned in that if 1,3-dichloro-2-propanol is indeed a kind of chlorohydrocarbon (as represented in ChEBI), then it should be metabolized in a kind of chlorinated-hydrocarbon metabolism—but 1,3-dichloro-2-propanol metabolism is not a kind of chlorinated-hydrocarbon metabolism (as represented in GO). In the nonalignments we identify, if the more specific subject entity (e.g. 1,3-dichloro-2-propanol) is indeed a kind of the more general subject entity (e.g. chlorohydrocarbons), then the assertion made for the more specific subject entity (e.g. that 1,3-dichloro-2-propanol can be metabolized in a 1,3-dichloro-2-propanol-metabolism process) should be subsumed by the assertion made for the more general subject entity (e.g. that a chlorohydrocarbon can be metabolized in a chlorinated-hydrocarbon-metabolism process).
 Fig. 1. The relationships between a pair of terms from ChEBI and another pair of terms from the GO BP ontology, the analysis of which an ontology nonalignment has been identified. Specifically,  1,3-dichloro-2-propanol  is subsumed by  chlorohydrocarbons  in the former, but  1,3-dichloro-2-propanol metabolism  is not subsumed by  chlorinated hydrocarbon metabolism  in the latter. This nonalignment was identified by analyzing the respective object classes of  is metabolized in  at the levels of  1,3-dichloro-2-propanol  and of  chlorohydrocarbons . In this example, with necessary and sufficient definitions of  chlorinated hydrocarbon metabolism  and  1,3-dichloro-2-propanol metabolism  in terms of  chlorohydrocarbons  and  1,3-dichloro-2-propanol , respectively, these reasoners would point to this nonalignment through the suggestion of an  is_a  link from  1,3-dichloro-2-propanol metabolism  to  chlorinated hydrocarbon metabolism . However, if instead  1,3-dichloro-2-propanol  was not subsumed by  chlorohydrocarbons  and  1,3-dichloro-2-propanol metabolism  was subsumed by  chlorinated hydrocarbon metabolism , these reasoners would not be able to suggest an  is_a  link from  1,3-dichloro-2-propanol  to  chlorohydrocarbons , because the required necessary and sufficient definitions of  1,3-dichloro-2-propanol  and c hlorohydrocarbons  in terms of 1, 3-dichloro-2-propanol metabolism  and c hlorinated hydrocarbon metabolism , respectively, could not be created using these terms in an ontologically valid way. This is not a fault of OWL or of Aristotelean formalism; these representational systems have strict semantics, to which ontologists should adhere when making assertions. It is just that reasoners relying solely on necessary and sufficient definitions will likely miss many of these nonalignments because ontologically valid definitions cannot be created, and it is desirable that as many of these nonalignments as possible be rectified. We have implemented our ontology-enrichment project in Protege-Frames (mainly because this is part of a larger frame-based effort). There is no associated reasoner to Protege-Frames, so we implemented a simple reasoning system to ensure the global consistency of the added assertions in our set of integrated ontologies. It is this same reasoning system we use here to discover nonalignments in the constituent ontologies through structural analysis of the assertions we added in our previous work (Bada and Hunter,  2007 ). Reasoning over these assertions, we were able to discover nearly 1700 instances of nonredundant nonalignments, 39.8% of which likely could not be identified via suggested inferences by OWL or OBO-Edit reasoners due to the fact that the required necessary and sufficient definitions could not be created in an ontologically valid way using these terms of the linked ontologies. We propose that those nonalignments for which such inferences cannot be made by these reasoners also be examined to increase consistency among the linked ontologies. 2 METHODS The method by which we ensure the global consistency of the set of assertions to the ontologies is through an analysis of the object classes of the properties of the classes. Specifically, this analysis relies on the fact that the object expression (here, an object class or union of object classes) of a property at a given class level must be subsumed by the object expression of the property at higher (i.e. more general) class levels. Furthermore, the object expression of a given property must be subsumed by the object expression at higher property levels. Put more simply, object expressions should monotonically narrow as one descends to more specific classes and slots. In order for each assertion to be consistent with each assertion made at more general levels, any object class of a property at a given class level that was not subsumed by an object class at a higher class and/or property level such that these conditions were satisfied was appropriately propagated up the class and/or slot hierarchies. The full details of this procedure can be read in the initial publication of our OBO-enrichment work (Bada and Hunter,  2007 ). Our methodology for discovering ontology nonalignments follows from this global consistency enforcement. For each base assertion (represented as a triple of a subject class, property and object class), each of the class's direct superclasses is checked to see if it is within the domain of the property. If so, it is checked if at least one of the object classes of the property of the superclass subsumes the object class of the property of the base assertion. If there is no such subsuming class, this is a nonalignment between the subject and object classes of the two assertions. If there is such a subsuming class at the level of this direct superclass, the same examination is performed for each of its direct superclasses. This continues recursively until either all direct superclasses are outside of the domain of the given property or a root of the ontology is reached. This can be made clearer with a simple but real example. Consider the base assertion  1,3-dichloro-2-propanol is metabolized in 1,3-dichloro-2-propanol metabolism , which states that 1,3-dichloro-2-propanol can be metabolized in a 1,3-dichloro-2-propanol-metabolism process. The sole direct superclass of  1,3-dichloro-2-propanol-chlorohydrocarbons  is obtained. It is checked that  chlorohydrocarbons  is within the domain of the slot i s metabolized in , which is the case. The set of allowed classes of  is metabolized in  at the level of  chlorohydrocarbons  is then obtained, which is the single class  chlorinated hydrocarbon metabolism  (which indicates that a chlorohydrocarbon can be metabolized in a chlorinated-hydrocarbon-metabolism process). The set of allowed classes at the superclass level (the one-member set  chlorinated hydrocarbon metabolism ) should subsume the set of allowed classes at the base-assertion level (the one-member set  1,3-dichloro-2-propanol metabolism ). However, it does not; this is thus a nonalignment.  Figure 1  illustrates this example. For each discovered nonalignment, we extracted four entities into which the nonalignment can be distilled: the subject class of the base assertion, the superclass of this subject class at the level of which the nonalignment was found, the object class of the base assertion (i.e. the allowed class of the assertion), and the set of object classes at the level of the superclass (i.e. the set of allowed classes for the slot at the level of the superclass). There is only one object class for each base assertion, while there can be more than one object class at the level of the superclass, since monotonicity as one travels down the class hierarchy is preserved as long as an object class of a property of a class is subsumed by at least one object class of the property of the superclass.  Figure 2  illustrates another real example where the set of allowed classes at the level of the supeclass has more than one member. In this example, the set of object classes for  results in binding of  at the level of  protein binding  was assigned the set [ proteins, protein polypeptide chains, protein complex ]. Such a multiply membered set of object classes is represented as a union of classes, so this assertion indicates that a protein-binding process can result in the binding of either a protein, a protein polypeptide chain, or a protein complex. (This was done because the definition of protein binding is ‘interacting selectively with a protein or protein complex’.) However, relatively few terms so far have been assigned multiple allowed classes as in this example, so this is currently an exceptional case.
 Fig. 2. The relationships between terms from the GO BP ontology and ChEBI and the GO CC ontology, the analysis of which an ontology nonalignment has been identified. Specifically,  histone binding  is subsumed by  protein binding  in the former, but  histones  is not subsumed by  proteins, protein polypeptide chains  or  protein complex  in the latter. This nonalignment was identified by analyzing the respective object classes of  results in binding of  at the levels of  histone binding  and of  protein binding . Each stored nonalignment represented by the four summarizing entities was written out to a text file in the following format:
 subject class of base assertion -&gt; superclass of subject class object class of base assertion !-&gt; object-class set at level of superclass 
 This neatly summarizes the nonalignment by stating that the subject class of the base assertion is subsumed by the superclass, but the object class of the base assertion is not subsumed by any of the object classes at the level of the superclass. Thus, the nonalignment illustrated in  Figure 1  is represented as:
 1,3-dichloro-2-propanol -&gt; chlorohydrocarbons 1,3-dichloro-2-propanol metabolism !-&gt; chlorinated hydrocarbon metabolism 
 Such a representation makes clear the essence of the nonalignment—that  1,3-dichloro-2-propanol  is subsumed by  chlorohydrocarbons  (in ChEBI), but  1,3-dichloro-2-propanol metabolism  is not subsumed by  chlorinated hydrocarbon metabolism  (in the GO biological process (BP) ontology). Due to the extensive multiple inheritance of the component ontologies, it is possible to discover redundant nonalignments or even the same nonalignment more than once. Only nonredundant nonalignments were stored and exported, as examining redundant nonalignments to assess whether there are true semantic discrepancies entails additional, unnecessary effort and biases statistics. Two nonalignments are redundant if the resolution of the one also results in the resolution of the other. Consider the following two nonalignments:
 benzoate -&gt; anions benzoate transport !-&gt; anion transport benzoate -&gt; ions benzoate transport !-&gt; ion transport 
 These two nonalignments are redundant with respect to one another. If the first nonalignment was resolved by adding an  is_a  link from  benzoate transport  to  anion transport , the second nonalignment would also be resolved since this link addition would result in the implication that  benzoate transport  is a type of  ion transport ; thus, the second nonalignment would also be resolved. In cases of redundancy, we have kept the more specific nonalignment; thus, for the example above, only the first nonalignment was stored. The relevant relationships between the terms of these two nonalignments are illustrated in  Figure 3 .
 Fig. 3. The relationships between terms from ChEBI and the GO BP ontology, the analysis of which two redundant ontology nonalignments were identified. Specifically,  benzoate  is subsumed by  anions  in the former, but  benzoate transport  is not subsumed by  anion transport  in the latter. Also,  benzoate  is subsumed by  ions  in the former, but  benzoate transport  is not subsumed by  ion transport  in the latter. The March 6, 2008 versions of GO, ChEBI and CL were used for this study. These base ontologies were previously enriched with 10 270 additional assertions linking the component terms using 50 specific relationships detailed in the initial publication of our OBO-enrichment work. It is important to note that although this study relies upon the links we created in our previously published ontology-enrichment work, our methodology for nonalignment identification is not limited by the specific relationships we chose to use. (The quality of the nonalignments, however, is dependent on the quality of the links that the methodology analyzes.) In fact, we have recently generated nonalignments based on links created by members of the OBO Consortium and have begun a discussion of ways of managing these nonalignments. 3 RESULTS Using this methodology resulted in a total of 1938 nonredundant nonalignments within the set of GO, ChEBI and CL; this set of nonalignments can be examined at  http://compbio.uchsc.edu/Hunter_lab/Bada/nonalignments_2008_03_06.html . To better characterize their distribution, we clustered the nonalignments according to the ontologies that were the sources of the subject and object terms of the nonalignments. For example, the nonalignment illustrated in  Figure 1  is a ChEBI-to-BP nonalignment, since the subject terms ( 1,3-dichloro-2-propanol  and  chlorohydrocarbons ) are from ChEBI and the object terms ( 1,3-dichloro-2-propanol metabolism  and  chlorinated hydrocarbon metabolism ) are from the GO BP ontology. There is a slight complication in that the two sets of object terms of a nonalignment may be from different ontologies, but this is rare. In such a case, the object term of the base assertion is used for the classification of the nonalignment. Table 1  lists the number of assertions and nonredundant nonalignments for each directed pairwise combination of ontologies for which there is at least one corresponding assertion. For example, there are 2710 total added assertions from a GO BP term to another GO BP term, and 94 nonredundant nonalignments were identified from these assertions. The numbers of nonalignments are largely symmetric. The biggest discrepancy is that between the 598 nonalignments identified from the BP-to-ChEBI assertions and the 1022 nonalignments identified from the ChEBI-to-BP assertions.
 Table 1. Numbers of assertions and nonredundant alignments for each directed combination of ontologies for which there is at least one added assertion Ontology to ontology Assertions Nonalignments GO BP to GO BP 2710 94 GO BP to GO CC 156 17 GO BP to ChEBI 3022 598 GO BP to CL 117 5 GO BP to GO MF 65 3 GO CC to GO BP 156 19 GO CC to GO CC 154 10 GO CC to GO MF 32 3 ChEBI to GO BP 3022 1022 ChEBI to GO MF 242 79 CL to GO BP 117 10 GO MF to GO BP 65 0 GO MF to GO CC 32 9 GO MF to ChEBI 242 69 Table 2  lists the numbers of assertions and nonredundant nonalignments and the ratio of nonalignments to assertions for each undirected pairwise combination of ontologies for which there is at least one corresponding assertion. The lowest ratios of nonalignments to assertions are those between BP terms and MF terms (0.02), between BP terms and BP terms (0.034), between BP terms and CL terms (0.064) and between cellular component (CC) terms and CC terms (0.065). This suggests that terms within these pairs of ontologies are relatively well-aligned. The highest ratios of nonalignments to assertions are those between ChEBI terms and MF terms (0.306), between BP terms and ChEBI terms (0.2680) and between CC terms and MF terms (0.19). This suggests that these pairs of ontologies are relatively not aligned well, which agrees with our empirical observations in our ontology-enrichment work that ChEBI is relatively not aligned well with GO.
 Table 2. Numbers of assertions and nonredundant alignments and the ratio of nonalignments to assertions for each undirected pairwise combination of ontologies for which there is at least one added assertion Ontology - ontology Assertions Nonalignments Ratio GO BP - GO BP 2798 94 0.034 GO BP - GO CC 312 36 0.12 GO BP - ChEBI 6044 1620 0.2680 GO BP - CL 234 15 0.064 GO BP - GO MF 130 3 0.02 GO CC - GO CC 154 10 0.065 GO CC - GO MF 64 12 0.19 ChEBI - GO MF 484 148 0.306 Another way to characterize the nonalignments is whether the subject terms of the nonalignments are the more complex terms or the more atomic terms. For example, in the example illustrated in  Figure 1 , the subject terms ( 1,3-dichloro-2-propanol  and  chlorohydrocarbons ) are more atomic than the object terms in that the latter are built up from the former. Conversely, in the example illustrated in  Figure 2 , the subject terms ( protein binding  and  histone binding ) are more complex than the object terms. As will be explained more fully in the next section, this characterization has important implications in that the new inferences made by the GONG/BONG and Obol projects correspond to the first type of nonalignment, in which the subject classes are more atomic, since ontologically valid necessary and sufficient definitions, which are required for these projects, can more easily be constructed in these cases. The second type of nonalignment includes all of the BP-to-CC, BP-to-ChEBI, BP-to-CL, BP-to-MF, MF-to-CC and MF-to-ChEBI nonalignments, while the BP-to-BP and CC-to-CC sets of nonalignments have mixtures of the two types of nonalignments. We have found that 772 (39.8%) of the 1938 nonredundant nonalignments are of the second type, thus showing that our methodology can identify a large number of nonalignments that may be missed by the reasoning methods of the other projects. 4 DISCUSSION 4.1 Evaluation and management of nonalignments In this study, we have used the term nonalignment to refer to two analogous sets of entities such that one entity is subsumed by the other in the first pair while one entity is not subsumed by the other in the second pair. Upon examination of a given nonalignment, if it is determined that the pairs of entities should be aligned, we term this a  discrepancy . Not all nonalignments are discrepancies;  Figure 4  illustrates such an example. Here,  laminin-1 binding  is subsumed by  extracellular matrix binding  in the GO MF ontology, but  laminin-1 complex  is not subsumed by  extracellular matrix  in the GO CC ontology. Even though it is a nonalignment, we believe that this is not a discrepancy in that these pairs of terms should not be aligned; that is, laminin-1 binding is a type of extracellular-matrix binding, but the laminin-1 complex is not a type of extracellular matrix (but rather a component of the extracellular matrix). Nevertheless, we assert that a large majority of the nonalignments we have identified are indeed discrepancies.
 Fig. 4. The relationships between a pair of terms from the GO MF ontology and a pair of terms from the GO cellular-component ontology, the analysis of which an ontology nonalignment has been identified. We assert this is an example of nonalignment that is not a discrepancy in that the subsumption relationship between the subject terms and the lack of a subsumption relationship between the object terms appear to be valid. If a given nonalignment is assessed to be a discrepancy, there are two ways to resolve it. The first is to add an  is_a  link from the object term of the base assertion to the object term at the superclass level (or, in the case of multiple object terms at the superclass level, to at least one of the object terms). For example, we assert the nonalignment illustrated in  Figure 1  is a discrepancy: according to this model, a chlorohydrocarbon can only be metabolized in a chlorinated-hydrocarbon-metabolism process, but a molecule of 1,3-dichloro-2-propanol, which is a kind of chlorohydrocarbon (according to ChEBI), can only be metabolized in a 1,3-dichloro-2-propanol-metabolism process, which is not a kind of chlorinated-hydrocarbon-metabolism process (according to GO BP). One way to resolve this discrepancy is the addition of an  is_a  link from  1,3-dichloro-2-propanol metabolism  to  chlorinated hydrocarbon metabolism . With this addition, a molecule of 1,3-dichloro-2-propanol can be metabolized in a 1,3-dichloro-2-propanol-metabolism process, which is now a more specific kind of chlorinated-hydrocarbon-metabolism process. The second way to resolve a discrepancy is the removal of the  is_a  link from the subject term of the base assertion to the subject term at the superclass level. In  Figure 1 , this corresponds to the removal of the is_a link from  1,3-dichloro-2-propanol  to  chlorohydrocarbons . With the removal of this link, 1,3-dichloro-2-propanol is no longer a more specific kind of chlorohydrocarbon, which aligns with the fact that a 1,3-dichoro-2-propanol-metabolism process is not a kind of a chlorinated-hydrocarbon-metabolism process. In the case of a nonalignment that is not a discrepancy, there is still a logical inconsistency, and action should be taken to rectify the inconsistency. A general, automatic solution to such an inconsistency is the propagation of the object class of the base assertion up to the superclass level; this is the type of upward propagation we previously extensively employed in our ontology-enrichment work so as to ensure the global consistency of the ontologies when adding enriching assertions. For example, in  Figure 4 , we assert that neither of the two steps described in the previous paragraphs should be performed; however, there is still a logical inconsistency in that an extracellular-matrix-binding process results in the binding of an extracellular matrix, but a laminin-1-binding process, which is a kind of extracellular-matrix-binding process (according to GO MF), results in the binding of a laminin-1 complex, which is not an extracellular matrix (according to GO CC). (According to GO CC,  laminin-1 complex  is transitively  part_of extracellular matrix .) The rectification we describe here consists of adding  laminin-1 complex  as an object class of  results in binding of  at the level of  extracellular matrix binding ; this is illustrated in  Figure 5 . The semantics of this new model are that an extracellular-matrix-binding process results in the binding of an extracellular matrix or a laminin-1 complex, while a laminin-1-binding process further restricts this to a laminin-1 complex.
 Fig. 5. The relationships between terms from the GO MF ontology and cellular-component ontologies in which the nonalignment identified in  Figure 4  has been rectified by the propagation of  laminin-1 complex . Specifically,  laminin-1 complex  has been added as an object class of  results in binding of  at the level of  extracellular matrix binding . A more elegant solution in this example is to instead add the GO CC term  extracellular matrix part  as an allowed class of  results in binding of  at the level of  extracellular matrix binding ; the semantics of this are that an extracellular-matrix-binding-process results in the binding of an extracellular matrix or an extracellular-matrix part, which seems to be a valid definition for  extracellular matrix binding . The original nonalignment would be resolved in that  laminin-1 complex  at the level of  laminin-1 binding  would be subsumed by  extracellular matrix part  at the level of  extracellular matrix binding . Though this is semantically closer to the definition of  extracellular matrix binding , it is also more manual and thus more labor-intensive (which is not to say that it should not be done). Our methodology could be used to either automatically upwardly propagate the specific classes so as to make the ontologies consistent, as described in the previous paragraph, or it could be used to automatically make suggestions to the ontology curators, who would decide to add either the specific terms or more general terms (such as  extracellular matrix part ). Of total of 1938, 100 nonredundant nonalignments were randomly selected for an evaluation. Out of these 100, 96 were assessed to be discrepancies; that is, we assert that they should be similarly aligned through the addition or removal of an  is_a  link, corresponding to the first two types of resolution. The remaining four nonalignments are analogous to the example seen in  Figure 4 , in which the subject and object terms should not be aligned; rather, the third type of resolution should be undertaken, in which an object term should be added to the higher-level assertion such that the lower-level assertion is subsumed, as seen in  Figure 5 . 4.2 Comparison to other projects Both the GONG/BONG and Obol projects have been focusing on creating formal defintions of OBO terms using more atomic OBO terms in necessary and sufficient conditions. These definitions can then be reasoned over (by an OWL reasoner for the former and by the Obol reasoner or the OBO-Edit reasoner for the latter), which can make new inferences using the definitions. However, the reasoner can only make new inferences using the linked terms if ontologically valid necessary and sufficient definitions can be constructed. The type of inferences that can be made largely corresponds to the absent subsumptions in the type of nonalignments in which the subject terms are more atomic than the object terms.  Figure 1  is such an example. Necessary and sufficient definitions could be produced for  1,3-dichloro-2-propanol metabolism  (as a subclass of  metabolism  with a  results in metabolism of 1,3-dichloro-2-propanol  condition) and for  chlorinated hydrocarbon metabolism  (as a subclass of  metabolism  with a  results in metabolism of chlorohydrocarbons  condition). If the associated reasoner reasons over ChEBI and GO (including these added definitions), given that  1,3-dichloro-2-propanol  is subsumed by  chlorohydrocarbons  as in  Figure 1 , it can infer an  is_a  link from  1,3-dichloro-2-propanol metabolism  to  chlorinated hydrocarbon metabolism . This is the same link that is the absent subsumption between the object terms (i.e. that  1,3-dichloro-2-propanol metabolism  is not subsumed by  chlorinated hydrocarbon metabolism ) of the nonalignment described for this example. Thus, these projects could predict analogous inferences for all of our nonalignments in which the subject terms are more atomic than the object terms, so long as ontologically valid necessary and sufficient definitions could be constructed, as was done in this example. Our methodology does not automatically suggest that all object pairs in each identified nonalignment be linked via  is_a , as this may not be the correct action to take; it allows the curator to resolve the nonalignment with any of the four methods described in the previous section. However, these projects likely could not predict new inferences for many if not all of the nonalignments in which the object terms are more atomic than the subject terms presented here, because the required necessary and sufficient definitions likely could not be made in an ontologically valid manner.  Figure 6  illustrates such an example. The nonalignment identified here is that  aldonate transport  is subsumed by  hexose transport  in GO BP, but  aldonates  is not subsumed by  hexoses  in ChEBI. Given necessary and sufficient definitions of  hexose transport  in terms of  hexoses  and  aldonate transport  in terms of  aldonates  and the fact that  aldonate transport  is subsumed by  hexose transport , a reasoner from one of these projects cannot infer that  hexoses  subsumes  aldonates . In order for the reasoner to infer an  is_a  link from  aldonates  to  hexoses  (which is one way to resolve this nonalignment) from these terms and their definitions, necessary and sufficient definitions for  aldonates  (perhaps as a subclass of  molecular entities  and an  is transported in aldonate transport  condition) and  hexoses  (perhaps as a subclass of  molecular entities  and an  is transported in hexose transport  condition) would have to be created. However, this is too strong a condition, as, for example, an aldonate is not necessarily transported elsewhere; it may be used where it was synthesized. Without these necessary and sufficient definitions, this inference cannot be made.
 Fig. 6. The relationships between a pair of terms from the GO BP ontology and a pair of terms from ChEBI from which a nonalignment was identified. This is an example of a nonalignment that is not currently examined in other ontology-enrichment methodologies, which require necessary and sufficient conditions to make new inferences. It can be argued that a reasoner in one of these other projects can infer an  is_a  link between chemicals by creating ontologically valid necessary and sufficient definitions in terms of, for example, parts or functions of these chemicals. However, this presupposes that not only such a more basic ontology but the required specific object terms exist. Such an approach laboriously requires the creation of an entirely new set of assertions, and there may be recursion in that the more basic object terms may not exist in a hierarchical relationship, thus once again preventing the inference of the  is_a  link between the more composite subject terms. Our approach only requires one set of assertions and their automatically generated inverse assertions and relies on a different kind of reasoning than the deduction used by reasoners in the aforementioned projects. However, we assert that a functionally equivalent methodology could be implemented, e.g. using an OWL API, without the use of explicitly represented inverse assertions. We have found that 39.8% of the total nonredundant nonalignments identified in this study are those in which the subject terms of the nonalignments are built up from the object terms; these correspond to the instances in which it is difficult to produce the required ontologically valid necessary and sufficient conditions, in which case new inferences by the aforementioned reasoners cannot be made using the linked terms of the ontologies. Our methodology essentially uses subsumptive analysis of term attributes toward quality assurance of ontologies, a technique which has been used by others in the field. The BERNWARD system reconstructed sets of medical concepts into hierarchies based on five subsumptive principles, but it is different in that it takes into account partonomy in its subsumption without resolution of the type we perform as in  Figures 4  and  5  (Bernauer,  1994 ). In an analysis of UMLS, Cimino ( 1998 ) found that the semantic type of 0.5% of concepts was neither the same as nor more specific than the semantic type of their respective parents. In an analysis of the links between diseases and their respective anatomical locations in SNOMED CT, Burgun  et al.  ( 2005 ) looked for differences between sets of disorders associated with all descendants of given anatomical entities and the sets of descendant disorders of the disorders associated with the given anatomical entities. Bodenreider  et al.  ( 2007 ) found that SNOMED CT contained 7226 parent-child pairs in which a role or value present in the parent was not present in the child and 21 799 pairs in which a value of a role present in the parent was not identical or more specific in the child. In addition to being the first subsumptive study of links among OBO terms, ours suggests both fully automatic and semiautomatic solutions to correct the inconsistencies that result upon linking the terms and highlights those that are not currently found by existing reasoning methods in other biomedical ontology-enrichment projects. We are not calling for the abolition of the use of the OWL, Obol or OBO-Edit reasoners. Rather, we assert that functionality that identifies the type of nonalignments for which inferences cannot be made (due to absence of required necessary and sufficient conditions) can and should be built into ontology-enrichment tools such as BONG. A methodology analogous to ours appears possible through the use of an OWL API through a subsumptive analysis of directly asserted and inherited property-value pairs. Consider  Figure 7 , in which the nonalignment of  Figure 6  has been resolved through the addition of an  is_a  link from  aldonates  to  hexoses . The links from the subject terms to the object terms can be represented as necessary and sufficient existential (i.e.  someValuesFrom ) conditions. Comparing the value of  results in transport of  at the level of  aldonate transport  ( aldonates ) to the value of  results in transport of  at the level of  hexose transport  ( hexoses ), it can be determined that the former is subsumed by the latter; thus, there is no inconsistency. Conversely, considering  Figure 6 , using the same procedure,  aldonates  is not subsumed by  hexoses , which could result in the suggestion of a nonalignment. The same methodology could be used to suggest nonalignments where necessary and sufficient definitions can be made, but this appears unnecessary, since existing reasoners can suggest new inferences for such cases. Moreover, this would require the use of statements for which ontologically valid necessary and sufficient conditions likely could not be made. Thus, the subsumptive inferences made by currently used reasoners and the nonalignments discovered by our methodology are complementary if the OBO curators continue to solely examine those nonalignments indicated by the inferences made by the reasoners using necessary and sufficient definitions.
 Fig. 7. The relationships between a pair of terms from the GO BP ontology and a pair of terms from ChEBI that result from the resolution of the nonalignment of  Figure 6  via the addition of an  is_a  link from  aldonates  to  hexoses . 5 SUMMARY We have described a methodology by which we have identified over 1900 instances of nonredundant nonalignments between terms from GO, ChEBI and CL. Analysis of the ratios of nonalignments to assertions from which the nonalignments were identified suggests that BP–MF, BP–BP, BP–CL and CC–CC terms are relatively well-aligned, while ChEBI–MF, BP–ChEBI and CCMF terms are relatively not aligned well. We propose that three ways to resolve an identified nonalignment are the addition of an  is_a  link between the object terms, the removal of an  is_a  link between the subject terms and the upward propagation of the object term to the superclass level. Many of the 39.8% of these nonalignments in which the object terms are more atomic than the subject terms likely are not currently examined in other ontology-enrichment projects due to the fact that the necessary and sufficient conditions required for the inferences likely could not be added, as they are semantically too strong. We assert that a methodology analogous to ours could be implemented using an OWL API in ontology-enrichment tools in order to identify such nonalignments that are currently not examined. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>A gene–phenotype relationship extraction pipeline from the biomedical literature using a representation learning approach</Title>
    <Doi>10.1093/bioinformatics/bty263</Doi>
    <Authors>Xing Wenhui, Qi Junsheng, Yuan Xiaohui, Li Lin, Zhang Xiaoyu, Fu Yuhua, Xiong Shengwu, Hu Lun, Peng Jing</Authors>
    <Abstract/>
    <Body>1 Introduction The biomedical literature is vast ( Cohen and Hersh, 2005 ), and there is an urgent need to process publications automatically and mine embedded knowledge in the literature to create research hypotheses. Recently, biomedical relationship extraction has gained attention for many downstream text-mining applications, such as event extraction, database creation, knowledge discovery, question answering and decision-making. Natural language processing (NLP) systems have been used for mining special relationships from texts as protein–protein interactions ( Papanikolaou  et al. , 2015 ;  Yang  et al. , 2011 ;  Zhu  et al. , 2015 ), genes and diseases ( Coulet  et al. , 2010 ;  Kim  et al. , 2017 ), drug–drug interactions ( Segura Bedmar  et al. , 2011 ,  2013 ), as well as among genes, drugs and mutations ( Cheng  et al. , 2008 ;  Rindflesch  et al. , 1999 ). Such relationship extraction contributes to the development of pharmacogenomics, clinical trial screening and adverse drug reaction identification ( Luo  et al. , 2017 ). The central challenge of modern genetic analysis is to establish genotype–phenotype correlations ( Cobb  et al. , 2013 ;  Fu  et al. , 2014 ), which are often found in the biomedical literature, but the volume warrants an automatic and reliable system to extract these information from the text. Although relationships have been identified among numerous biological entities, the system for extracting gene–phenotype relationships from the literature is very limited. Regarding species types, the current research focuses more on the relationships between human genes and phenotypes ( Collier  et al. , 2015 ;  Yang  et al. , 2015 ). To our knowledge, there is few such studies for plants. Regarding entity types, research on identifying specific phenotypes such as diseases and gene relationships has received great attention ( Kim  et al. , 2017 ;  Özgür  et al. , 2008 ;  Singhal  et al. , 2016 ). However, text-mining systems that can recognize various phenotype and gene relationships are more difficult and are less robust. The system generally involves annotating raw text with named entities and extracting relationships between these entities. ( Luo  et al. , 2017 ) Named entity recognition (NER) is the foundation of relationship extraction and the effect of entity recognition greatly affects relationship extraction results. ( Chun  et al. , 2006 ) With gene–phenotype relationship extraction, gene and phenotype should be identified. Because lexical features are relatively regular, there are many methods to identify genes in the text. ( Campos  et al. , 2012 ;  Wei  et al. , 2015 ) However, although research on NER has been improved ( Gaizauskas  et al. , 2003 ;  Horn  et al. , 2004 ;  Segura-Bedmar  et al. , 2008 ), phenotype identification is still challenging and this negatively influences relationship extraction. First, a phenotype is usually composed of multiple words, such as ‘ calcium sensitivity ’ or ‘ genic male sterility-photoperiod sensitive ’. Thus, name boundaries are complex. Second, phenotypic descriptions are often study- or author-specific due to a lack of standard expressions, complicating this search. For example, in the two sentences ‘…resulting in root growth inhibition, smaller rosettes, and  leaf curling' . (PMID: 26734017) and ‘…leading to early flowering and  curly leaves phenotypes’ . (PMID: 25693187), the same leaf morphology has two different descriptions, i.e. ‘ leaf curling ’, ‘ curly leaves ’. In addition, while there are specialized lexicons in many areas, no lexicon can be directly used to identify overall phenotypic descriptions in text, especially for plants. For example, the Unified Medical Language System (UMLS) MetaThesaurus ( Humphreys  et al. , 1998 ) is a vocabulary database that includes numerous semantic types, except for  Phenotype  type. In the plant domain, the controlled vocabulary plant trait ontology (PTO) ( http://bioportal.bioontology.org/ontologies/PTO  ) is too general, so it may not include all species traits. The Arabidopsis Information Resource (TAIR) ( Lamesch  et al. , 2012 ) is curated by manually summarizing published literature so it is limited and difficult to organize for future use. The AraPheno ( Seren  et al. , 2017 ) database is an organization of the Genome-Wide Association Study (GWAS) phenotypic results in only six published studies, so the data are few. These manual curation processes are time-consuming and cannot keep up with rapidly increasing literature. Here, we propose a novel gene–phenotype relationship extraction pipeline using model plant  Arabidopsis thaliana . First we improved the word-embedding-to-sentence-embedding cascaded approach ( Xing  et al. , 2017 ) as representation learning to recognize various broad phenotypic descriptions in large-scale biomolecular literature. Then, genes from the same phenotype-containing sentence were found, using the dictionary-based method. Next, a relationship extraction system Open Language Learning for Information Extraction (OLLIE) was applied to extract gene–phenotype relationships. The proposed pipeline improves relationship extractions by identifying more phenotypic descriptions in the text. We identified many types of phenotypic descriptions based on their boundary delimitation: phenotypic phrases and phenotypic long/short sentences. To locate sentences that include the phenotype, we use word embedding to learn distributed representations for words and phrases. Then, we can extract phenotypic phrases missed by ontology, thus extracting more sentences containing phenotypes. Then we cascade the sentence-embedding method for specific phenotype-containing sentences. Due to numerous candidate phenotypic sentences, expert verification is time-consuming. According to the similarity mechanism, we find that sentences with high similarity to the phenotype-containing sentences have similar sentence structures. This prompted us to design a Phenotypic Sentence Template Extraction arChitecture (PSTEC) algorithm that automatically extracts phenotype sentence templates. With these templates, we can extract complex non-phrase forms of long/short phenotypic sentences. Ultimately, we evaluated the proposed pipeline from two aspects. (i) We designed three baselines to compare with our proposed relationship extraction pipeline. From the results, we identified more phenotypes (expanding the original ontology almost 3-fold), which significantly improved recall value (improving 24.05% compared to the traditional ontology-based method). Meanwhile, identifying phenotypic descriptions from multiple perspectives also increased the precision of whole recognition. Using the OLLIE system based on machine learning method, we effectively improved F1-Measure compared with traditional relation extraction approach. Thus, our pipeline had a F1-Measure of 66.83%, the greatest of all baselines. (ii) We applied the pipeline to 481 full articles from the TAIR gene–phenotype relationship dataset, and the coverage was 70.94%. Moreover, we added 373 relationships to expand this dataset. Our pipeline automatically identified new relationships with a growing body of literature showing strong scalability. The proposed pipeline is versatile and can be used not only for extraction of relationships in  Arabidopsis  but also for other plant species such as soybean and cotton. 2 Our gene–phenotype relationship pipeline 2.1 The overview of our pipeline The pipeline starts with scanning abstracts in PubMed using the keyword  ‘A.thaliana’  and the Entrez Programming Utilities (E-utilities) web service ( https://www.ncbi.nlm.nih.gov/books/NBK25501 ). We clean irrelevant author information and acquire 63 459 abstracts that mention  A.thaliana . Next, we improve the proposed cascaded representation learning approach ( Xing  et al. , 2017 ) to recognize various broad phenotypes in the literature. Our representation learning approach, combined with the syntactic and semantic analysis of texts, identifies phenotypes in multiple directions from phenotypic phrases to complex short/long phenotypic sentences. Using ontology terms as input, our approach greatly expands the recognition of ontology term synonyms in the literature and establishes a bridge from ontology to literature description, so that study- or author-specific terms can be identified. Then we use the results of phenotypic identification to extract gene–phenotype relationships. We use dictionary- and rule-based methods to identify  Arabidopsis  genes in the literature. Then, we combine the workflow of the Open Information Extraction (IE) system with our entity recognition to extract and establish an  Arabidopsis  gene–phenotype binary relationship. The pipeline was implemented and run on a 24 2.4 GHz Xeon core server running on Ubuntu Linux 16.04.  Figure 1  shows the overview of the pipeline.
 Fig. 1. The overview of our gene–phenotype relationships extraction pipeline 2.2 Cascaded approach for phenotype extraction Before entity recognition, we used domain-resource ontology to establish the original phenotypic dataset. We extracted phenotypic descriptions from phenotypic phrases and sentences based on different boundaries. We used the parse tree combined with the word embedding method to extract phenotypic phrases, the majority of which were described by noun phrases. Because some synonyms in ontology are not described as phenotype in the text, the previous approach did not consider it leading to some errors. Therefore, we added abbreviation recognition and revision algorithm into the improved cascaded approach. Because some special phenotypes are non-phrase forms or long/short sentence descriptions, we used phenotypic sentences from word embedding results as positive samples to cascade the sentence embedding method for finding phenotype sentences. We transformed the unsupervised sentence-embedding model into a weakly supervised model. Due to the lack of training of positive and negative samples, we use the Negative Class Label Enhanced (NCLE) algorithm ( Xing  et al. , 2017 ) to label negative samples and train the sentence-embedding model in combination with the positive samples of the word-embedding results. We analyzed results of sentence embedding, finding that phenotypic sentences gathered by the similarity mechanism had similar structures. However, the previous approach estimated these results through expert verification, which is time-consuming. Therefore, we extracted sentence templates that described the phenotype by improving the algorithm of the statistical combination to expand phenotype recognition. 2.2.1 Constructing the phenotype dataset First, we use two ontologies to create the original phenotype dataset  P , i.e. PTO and Arabidopsis Hormone Database 2.0 ( http://ahd.cbi.pku.edu.cn/cgi-bin/phenotypeBrowse.pl  .) ( Jiang  et al. , 2011 ). PTO is an important controlled vocabulary that describes phenotypic traits in plants. Each trait is a distinguishable, characteristic, quality or phenotypic feature of a developing or mature plant or a plant part. Arabidopsis Hormone Database 2.0 provides a systematic and comprehensive view of genes participating in plant hormonal regulation of the model organism  A. thaliana . Its phenotypic ontology was developed to describe precisely myriad hormone-regulated morphological processes with standardized vocabularies in  Arabidopsis . When processing PTO, we extract ‘name’ and ‘synonym’ from every term in the ontology. Approximately 84% of these names are associated with synonyms; on average, each name has 1.07 synonyms. For example, the phenotype ‘ alkali soil sensitivity ’ has two synonyms: ‘ AlkS ’ and ‘ alkali sensitivity ’. Not all of terms in these ontologies appear in the literature. We found 805 terms in abstracts after removing duplicate entries. We combined these into a complete phenotype dataset  P . 2.2.2 Word embedding We followed the word embedding method published in ( Xing  et al. , 2017 ). First, we used the collected PubMed texts to train the word-embedding model, which gave each word or phrase a distributed representation in low and dense dimensional vector space. By finding phrases with high similarity to phenotypic entities in  P , the original ontology of the phenotype is expanded as  P update . Therefore, we can obtain more sentences containing phenotypic information. Because some phenotypic synonyms contained in  P  are abbreviated forms, they may not represent as phenotype in the text and are incorrectly identified. For example, the abbreviation ‘ AC ’ in the ontology corresponds to the full name of ‘ leaf sheath auricle color ’. However, in the sentence ‘Many of these proteins have complex domain architectures with AC or GC centers …’ (PMID: 26721677), ‘ AC ’ is not a phenotype. The previous method did not consider abbreviation recognition such as this, so we required post-processing of word-embedding results. After obtaining a high similarity phenotype phrase, we recognized and revised the abbreviation. We used ( Xu  et al. , 2009 ) algorithms for identifying abbreviations in the biological literature, matching pairs of all abbreviations and full names in the processed texts. When we used an updated phenotype dataset  P update  to reidentify the phenotype in the literature, if there was an abbreviated form, it was first matched with a full name. Only the full name of the abbreviation also in  P update , remained as a phenotype, otherwise it was deleted. The abbreviation recognition and revision can increase pipeline precision value and identify phenotypes more accurately. 2.2.3 Sentence embedding Using the word-embedding results, we classified and tagged PubMed texts as input for the sentence-embedding ( Le and Mikolov, 2014 ) method. The trained model can find sentences containing phenotypic information, acquiring new phenotypic sentences. To improve diversity of phenotype recognition, we transformed the unsupervised sentence-embedding model into a weakly supervised model. We used the results of word-embedding as positive samples,  S pos , and combined the NCLE algorithm for negative samples,  S neg , for the training of the Sen2Vec model. Sentence embedding can aggregate similar phenotypic expressions. We found that large-scale gathered sentences have a similar sentence context structure. For example, the more similar sentences with ‘Solute import across the pollen plasma membrane, which occurs via proteinaceous transporters,  is required to  support pollen development  and also for  subsequent germination and pollen tube growth ’  always have the same structure  ‘be required {prep_*} + [phenotype]’ , such as:
 ‘During pollination, constant communication between male pollen and the female stigma  is required for  pollen adhesion, germination, and tube growth ’. ‘Two  A.thaliana  genes, QRT1 and QRT2,  are required for  pollen separation during normal development ’ . Due to many similar sentences, it is time-consuming to identify all phenotypic sentences and analyze their phenotype with expert evaluation. Therefore, we used sentence structure to automate extraction of complicated long/short phenotypic sentences of non-phrase types. These structures may contain complex phenotypic descriptions, likely with punctuation, prepositions, and conjunctions. We designed an automated algorithm to find frequently occurring sentence templates and with this, we extracted relatively complex descriptions of phenotypic long/short sentences from many sentence-embedding results. At present, there are few studies about automatic generation of sentence templates in NLP. We borrowed the idea of modular algorithms from Sentence Pattern Extraction arChitecturte (SPEC) systems in ( Michal  et al. , 2011 ) and proposed our own solution for combinatorial explosion problem. With the SPEC algorithm, a ‘sentence template’ is considered as  n -element ordered combination of sentence elements. It generates all possible combinations of patterns from a sentence and selects the frequency occurrence combination as a sentence pattern. However, we focused on the phenotype-containing structure and created the algorithm Phenotypic Sentence Template Extraction arChitecture (PSTEC) which consists of three components:
 Preprocessing Generation of all ordered combinations from sentence elements Insertion of a wildcard 
 Preprocessing : We tokenized all positive sentences  S pos  of sentence embedding. Because we must extract phenotype-containing sentence structures, we treated phenotypic phrases as a whole and replaced phenotypic descriptions appearing in the sentence with ‘PHE’. 
 Generation ordered combinations : In every  n -element sentence, there is  k -number of ordered combination groups (1 ≤  k  ≤   max). After processing all sentences in corpora, we choose a combination of frequencies greater than a threshold  fre  as a  k -length template. Because the phenotype-containing template is not too long, so we set max as the length of the element threshold. We set two restrictions to prevent the combination explosions:
 Combination of the  k -element must include the specific word ‘PHE’ Any ‘PHE’ contained ( k -1)-element subset of  k -element combination must be in the ( k -1)-element template. After iteration processing, we obtained all ordered, not duplicated, high frequency combinations for all values of  k  from the range of {1, …, max} as  k -element sentence templates. 
 Insertion of a wildcard : During combination, we combined the original word order. To improve the applicability of templates, we specified whether the elements appeared next to each other or were separated. Therefore, we placed a wildcard between all non-subsequent elements using one heuristic rule. If an absolute difference of word order assigned to the two subsequent elements of a combination &gt;1, we added a wildcard between them. An example of PSTEC algorithm appears in  Figure 2 .
 Fig. 2. The procedure for sentence template extraction using high frequency three-element combinations to generate four-element template When we obtained the high-frequency  max -element sentence templates, we applied these templates to the results of a large number of sentence embeddings. Extracting the description of the more complex phenotypes in sentences that are highly similar to the positive samples improved phenotype recognition. 2.3 Gene–phenotype relationship extraction For gene–phenotype relationship extraction, the gene is required and gene lexical features are relatively regular in texts, gene IDs or gene names may be used to represent them. Therefore, we used a dictionary- and rule-based method to identify genes. After entity recognition was complete, our pipeline extracted the relationship with the open information extraction (IE) system. Results of the relationship extraction are expressed as triplets (arg1; r; arg2). The r (relationship phrase) represents arg1 and arg2 entity relationships. 2.3.1 Gene extraction First, we searched all related genes in the UniProt database ( http://www.uniprot.org  ) using ‘ A.thaliana ’ as a key word and obtained 129 648 records. Each record contained the fields ‘Organism’, ‘Gene locus’, ‘Gene name’. Although we use  Arabidopsis  as a keyword, the results included other species, such as ‘Oryza sativa subsp. japonica (Rice)’. After processing, we obtained 89 287 Arabidopsis  gene ID and gene name pairs and these were used as a dictionary to identify genes. Due to the large number of gene names and not a gene locus in the literature, part of the gene name is not in the dictionary. Therefore, we use gene lexical rules and semantic description rules in the text to improve gene recognition. Gene name spelling had some character-level rules as follows:
 All capital letters. A combination of uppercase and lowercase letters. A combination of numbers, uppercase and lowercase letters. Those containing hyphens. Therefore, we used two rule types, mixed character-levels and contextual-levels, to identify the gene. When an input sentence contained these expressions:  Expression of, Accumulation of, Expression levels/patterns of, Targets of, mRNA abundance of, Transcript profiles/levels of , and the ‘NNP’ (Proper noun, singular) tagged parts in the part-of-speech (POS) tagged sentence complies with our character-level rules, we extracted this special expression as a gene. For example, with the POS tagged sentence: “…HTR4K27Q (‘ NNP ’) overexpression (‘ NN ’) lines (‘ NNS ’) exhibited (‘ VBD ’) deregulated (‘ JJ ’)  expression  (‘ NN ’)  of  (‘ IN ’)  H3K27me3-enriched (‘NNP’)  genes  (‘ NNS ’).” (PMID: 27926813) contains the specific contextual-level description ‘ Expression of  ’, and the ‘NNP’ tagged words satisfy the third and fourth character-level rules. Thus, we can identify gene ‘ H3K27me3-enriched ’. Then, we used all sentences that contained the phenotype as input, and the output is two entities that cooccur in sentences. These sentences were used as input to subsequent relationship extraction. 2.3.2 Relationship extraction To the best of our knowledge, there is a limited document annotation corpus of gene–phenotype relationships in  Arabidopsis  species. Currently we are only concerned with gene–phenotype relationships in single sentences. Most relationship recognition systems are not generic and portable so we used the open information extraction (OpenIE) system for this specific relationship identification. OpenIE can extract assertions from massive corpora without a specified vocabulary ( Fader  et al. , 2011 ) from open-domain corpora, such as the Internet and Wikipedia, but in recent years, OpenIE has used biological literature for systematic testing. We used an existing OpenIE system, OLLIE ( Schmitz  et al. , 2012 ) as a relationship phrase recognition tool. OLLIE improved several shortcomings of the state-of-the-art system, extracting only relationships mediated by verbs and ignoring context, extracting tuples not asserted as factual. OLLIE is popular for information extraction and used in many fields, such as Question-Answer ( Berant  et al. , 2013 ), knowledge graphs ( Nickel  et al. , 2016 ), and named entities’ network ( Tariq  et al. , 2017 ). OLLIE uses high-precision results of the previous generation OpenIE system i.e. REVERB ( Fader  et al. , 2011 ). With many syntactic analyses of sentences that contain relationships, learning relationship patterns can be extended to find relationships of new input sentences. We input the co-occurring sentences into the OLLIE system and extracted relationship sentences and their corresponding relationships. OLLIE automatically gives NP pairs of sentences as arguments in the relationship. However, these NP pairs contain too much noise, and the partially extracted arguments are not genes or phenotypes. Therefore, we limited our screening to eligible relationship groups. For the first (agr1) and third (agr2) parts of one triple, we need map them to the previous phenotype and gene entity list. When one or some genes and phenotypes are in each of the two arguments, we consider the relationship as a gene–phenotype relationship and stored such a relationship. 3 Results and discussion 3.1 Phenotype extraction results 3.1.1 Word-embedding results We used Word2Vec ( https://code.google.com/p/word2vec  ) to train a skip-gram model with a 4 D size, i.e. 300, 500, 700 and 900. Due to a lack of standards for this topic, we needed expert evaluation and annotation. Therefore, the results of word embedding first were semi-automatically classified and then manually evaluated by one expert and confirmed by another. Ultimately, the word-embedding method can extend original phenotype datasets  P , increasing 1303 new phenotype data by up to 161.86%. We used the extended dataset  P update  to match the phenotypic descriptions in the abstracts. Mapping sentences numbered 88 243. After abbreviations were identified and revised, 87 613 sentences containing phenotypes were obtained. Some examples of phenotypes recognized by the word-embedding method appear in  Table 1 . ‘Ontology term’ as the original input, using the similarity mechanism to get ‘Phenotype’ and the corresponding ‘Similarity Score’. ‘Class’ represents the corresponding categories in the PTO 10 basic categories (10 basic categories are: TO: 0000277 biochemical trait; TO: 0000283 biological process trait; TO: 0000183 other miscellaneous trait; TO: 0000357 plant growth and development trait; TO: 0000017 plant morphology trait; TO: 0000597 quality trait; TO: 0000133 stature or vigor trait; TO: 0000392 sterility or fertility trait; TO: 0000164 stress trait; TO: 0000371 yield trait).
 Table 1. Examples of word-embedding results Ontology term Phenotype Similarity Score Class  Cell elongation Cell expansion 0.671 TO: 0000357 Cell enlargement 0.531 Organ expansion 0.528 Cell proliferation 0.526  Chlorophyll content Lower ion leakage 0.625 TO: 0000277 Photosystem II activity 0.557 Photosynthetic quantum yield 0.550 Higher relative water content 0.531  Chloroplast structure Photosynthetic phenotype 0.498 TO: 0000017 Thylakoid structure 0.495 Leaf chloroplast ultrastructure 0.484 Pale green leaves 0.479  Leaf curling Dark green leaves 0.613 TO: 0000357 Altered leaf shape 0.581 Curly leaves 0.576 Serrated leaves 0.558  Drought sensitivity Reduced water loss 0.550 TO: 0000164 Enhanced drought resistance 0.544 Drought stress tolerance 0.539 Reduced drought tolerance 0.535 Note:  According to the original ‘Ontology term’, we use similarity mechanisms to extract ‘Phenotype’ and its corresponding ‘Similarity Score’. ‘Class’ represents the corresponding categories in the PTO 10 basic categories. As shown in  Table 1 , the word-embedding method can find a phenotypic description according to the syntax and context of the text. For example, for the same ontology term ‘ leaf curling ’ (TO: 0002681), the method can extract similar words by considering syntax (‘ leaf curling ’—‘ curly leaves ’) and context semantics (‘ leaf curling ’—‘ altered leaf shape ’). Some new phenotypes are not synonyms of their corresponding original ontology terms. For example, the new phenotype ‘ serrated leaves ’ is not synonymous with ‘ leaf curling ’. This may because the contextual environment that describes the new phenotype and the original term is similar, but the semantics of expression are not the same. 3.1.2 Sentence-embedding results We used Doc2Vec ( http://radimrehurek.com/gensim/models/doc2vec.html  ) to train the PV-DBOW model, and the trained corpora are positive/negative labeled abstracts. Then, we used the results of word embedding  S pos  as inputs and acquired candidate sentences with similarities greater than  Sim  after calculating for cosine distance with  S pub . A reasonable  Sim  value greatly influenced the results. After testing, if  Sim  was too high (&gt;0.4), high similarity sentences were too few and an average of 1.2 high-similarity sentences was obtained for each original sentence. If  Sim  was too low (¡0.2), we get a lot of dissimilar sentences. Therefore, we set  Sim  as 0.3, and an average of 4.5 high-similarity sentences was obtained for each original sentence. The sentence-embedding method can find many candidate phenotypic sentences, which contain many non-phrase, complex long/short phenotypic sentences. For example, the phenotypic structure ‘ response to …stress ’ in the sentence ‘GmaPHO1 genes had altered expression in  response to salt, osmotic, and inorganic phosphate stresses ’ . Such phenotypic descriptions are special and numerous and can improve relationship identification. Therefore, we designed a PSTEC algorithm to automatically generate phenotypic sentence templates for extracting them. We tested and selected template length  max  and template frequency  fre  of the PSTEC algorithm. When  max  is too long (&gt;6), the template will contain a lot of noise, such as too many prepositions and stop words. When  max  is too short (&lt;4), the template cannot contain complete template structure information. Thus, we set  max  as 5. The size of  fre  directly affects the efficiency and uptime of the algorithm. After testing, we set  fre  as 100 and only kept templates that appeared more often than 100 in the corpus. Ultimately, we obtained 250 sentence templates. There are many types of duplicate templates and high frequency but not intention-containing templates, such as  ‘Show/Suggest + prep_*’ . Therefore, we merged and selected these results.  Table 2  shows 5 high frequency sentence templates that can recognize combination type phenotypes (‘ Tolerance to salt/drought/methyl viologen stress in Arabidopsis’ ), with environmental or time factors (‘ Hypocotyl growth in response to unilateral blue-light illumination ’) and are rich in diversity of phenotypes. Meanwhile, we noticed that phenotypes recognized by different templates may differ. For example, the ‘Respond’ template can identify more ‘stress trait’ types.
 Table 2. Examples of sentence templates Sentence template Example of phenotype Number of phenotype Inhibition of + (PHE) Root growth the root-swelling phenotype; Germination and elongation of  Arabidopsis  seedling 127 Involve(d) in + (PHE) Host cell death in the hypersensitive disease-resistance response;  A. thaliana  seedling root to a rapid change in salinity 532 (Play a/an adj./n.) Role in + (PHE) Coordinate the directional growth of plant tissue; Tolerance to salt/drought/methyl viologen stress in  Arabidopsis 243 Regulator/regulation of + (PHE) Secondary wall synthesis in fiber of  A.thaliana  stem; Stomatal clustering and density early in  Arabidopsis  leaf development 197 (In) Response to + (PHE) Both high- and low-temperature stress; Signal emanate from cell undergo pathogen-induced hypersensitive cell death 215 Note : PHE represents phenotype, parentheses indicate optional parts. We can extend 1314 phenotypic descriptions using the sentence template. After merged results of word-embedding, we expanded 2409 phenotypic expressions and increased them 2.99-fold compared to the original phenotype dataset  P . 3.2 Gene–phenotype relationship results We evaluated results of gene–phenotype extraction from two perspectives.
 According to different phenotype recognition and relation extraction methods, we compared with baselines. We used the entire pipeline in the TAIR database, which manually extracted gene–phenotype relationships from 555 full papers. 3.2.1 Performance comparison with baselines Using the phenotype recognition cascaded approach can improve the identification of phenotypes in the literature and improve relationship identification. To illustrate the importance of phenotypic recognition in relationship extraction and to verify the accuracy of our approach, we establish two baselines for performance comparison.
 B1: Using the traditional ontology-based method ( Müller  et al. , 2004 ) to recognize phenotype and extracting the gene and relationship using method described in this article. B2: Using the ontology-based with word embedding method ( Mikolov  et al. , 2013 ) to recognize phenotype and extracting the gene and relationship using method described in this article. we also compare with another baseline that use traditional relation extraction methods. B3: Using method described in this article to extract phenotype and gene, the relation extraction method is based on syntatic rules ( Coulet  et al. , 2010 ) which uses the collapsed dependencies graph representation. We randomly selected 100 abstracts to identify the relationships by expert verification and to calculate Precision, Recall, F1-Measure. Results are shown in  Table 3 .
 Table 3. Performance of baselines compared with our pipeline Type Phenotype extraction Relation extraction Precision (%) Recall (%) F1-Measure (%) B1 Ontology-based ( Müller  et al. , 2004 ) OLLIE 52.98 33.76 41.24 B2 Ontology-based + word embedding ( Mikolov  et al. , 2013 ) OLLIE 73.91 50.21 59.80 B3 Representation learning approach Syntatic rules  ( Coulet  et al. , 2010 ) 55.75 26.58 36.00 Our pipeline Representation learning approach OLLIE 7 9.19 5 7.81 6 6.83 Among the different methods on phenotype recognition, the effect of recognizing the gene–phenotype relationship using only ontology-based efforts is the poorest. Because of loss of many phenotypes, recall value in relationship recognition is low. For example, the phenotype ‘ NaCl stress-sensitive phenotype ’ is not in ontology, so the relationship (MCK1; complemented;  NaCl stress-sensitive phenotype ) cannot be found. However, we can identify this phenotype using the proposed approach and obtain relationships with the best recall. This is because we recognized the phenotypic phrase and the more complex phenotypic long/short sentences based on the sentence template. As the integrity of the phenotype increased, the precision is improved. For this sentence, ‘…a structurally related Arabidopsis MADS-box gene involved in the  negative control of Arabidopsis flowering time , …’ (PMID: 15539492), due to the template: ‘ (gene) involve + {prep.} + PHE ’, we can identify the whole phenotypic description ‘ negative control of Arabidopsis flowering time ’, and get the relationship (MADS-box gene; involved in;  negative control of Arabidopsis flowering time ). However, the first two baselines only extracted part of the whole expression ‘ flowering time ’ and missed the complete relationships. Thus, our approach can extend relationship extraction by improving phenotype recognition. Compared with the B3 baselines, which only change the relation extraction method, our pipleline also has the best performance. Because the syntactic rule method misses many results and only getting 26.58% recall value, its F1-Measure is about 36.00%. We have considered to use generic tools such as GNormPlus ( Wei  et al. , 2015 ), GenNorm ( Wei and Kao, 2011 ) and so on for gene identification but found that these tools identify the gene of all species that appear in the text. Therefore, noise information is mixed in the targeted identification of  Arabidopsis  gene information, which requires expert screening. So, we finally chose a more targeted rule- and dictionary-based approach and obtained 88.76% precision value in the above test dataset. This is slightly higher than the results given in the article ( Wei  et al. , 2015 ) by GNormPlus (precision 87.1%) and GenNorm (precision 78.9%). Although the proposed pipeline can improve the effectiveness of final relationship identification compared with baselines, there are misidentifications and omissions due to the following reasons:
 Error of relationship recognition. The OLLIE system is limited as it can only identify the relationship in a single sentence, and the length of the sentence cannot be too long. Sentences &gt;20 words have increased errors for relationship analysis ( Schmitz  et al. , 2012 ). For example, the sentence ‘Hence, the narrow organ shape, reduced plant height, and reduced whorl 4 organ primordia are consistent with a general reduction of cell number, and, perhaps, reflect a role of SEU in promoting cell proliferation’ can be assessed by OLLIE to get this relationship (whorl 4 organ primordia; perhaps reflect; a role SEU in promoting cell proliferation). The wrong relationship association results in inaccurate identification of it. Inaccurate phenotypic boundary. Although we can identify phenotype from phrases and long/short sentences, more complex phenotypes cause errors or incomplete identification. For example ‘The AGAMOUS gene of Arabidopsis is necessary for the  proper development of stamens and carpels and the prevention of indeterminate growth of the floral meristem ’ . We did not recognize this sentence structure, resulting in incomplete recognition of relationships. Problem of gene recognition. Although we use a relatively complete  Arabidopsis  gene database as a dictionary for gene ID and gene name identification, and get high precition value of 88.76%, the database may still missing some gene name as well as the corresponding relationship for it. These errors reduce precision and recall because each case results in an incorrect or incomplete relationship extraction. 3.2.2 Comparison with TAIR The TAIR database ( Lamesch  et al. , 2012 ) is one of the most informative databases for storing  Arabidopsis  information, which contains a gene–phenotype relationships dataset. This information was manually extracted from 555 full texts. To verify pipeline effectiveness, we calculated coverage of relationship for these papers. Because some documents cannot be downloaded, we retrieved only 481 full papers. Preprocessing the TAIR dataset by deleting irrelevant fields, i.e. ‘Phenotype not described’ and ‘No visible phenotype’ was done and we retrieved 1397 sets of gene–phenotype relationships. We noticed that there are duplicate types of relationships in the dataset. For example, the gene ‘MSSP1’ is related with:
 Under normal growth temperature conditions, the double mutant leaves’ content in glucose and fructose is slightly reduced (30%) in a similar fashion to that observed with the tmt1 single mutants. Under normal temperature conditions, a substantial reduction in glucose and fructose contents in leaves is observed compared to wild type, and even the single tmt1 and double tmt1/tmt2 mutants. As they are the same type, we treat them as the identical relationships. We applied our pipeline to this dataset, extracted data were compared with processed TAIR datasets by four experts and offered coverage of 70.94%. Moreover, our pipeline can identify 373 new relationships, which the TAIR dataset does not include. The results are shown in  Supplementary Material . We had limited coverage for a few reasons:
 Many relationships in TAIR come from cross-sentence or even cross-paragraph relationships. Such relationships are unrecognizable to our pipeline that only extracts from a single sentence, so there is the main reason of limited coverage. However, due to redundancy of much information, our pipeline use repetitive relationships extracted from many studies to compensate extraction of the relationship representations in small samples. Such work cannot be done manually. Many phenotypes in TAIR have not been described in the original literature after subsequent manual processing and summary and this will influence coverage. There is only a gene locus name in the TAIR dataset, but most documents only describe the gene name. Some gene loci in the gene database do not have corresponding names. Thus, our pipeline cannot recognize these genes or any corresponding relationships. After analysis, we found that articles in the TAIR dataset are relatively old (most prior to 2000). Due to limitations to manual reading, this dataset failed to update gene–phenotype relationships as the literature grew, so scalability was poor. However, with our pipeline we can quickly find relationships for updated literature, greatly improving efficiency for summarizing data. 4 Conclusion and future works Much plant gene–phenotypic information exists in the biomedical literature, and it continues to grow. Thus, we propose a pipeline to extract relationships between genes and phenotypes using  A.thaliana  as an experimental object. Our pipeline can expand the expression of original phenotype ontology terms in the literature using an improved cascaded representation learning approach of phenotype recognition. This can enhance relationship extraction. Our pipeline obtained an F1-score (66.83%) that outperformed other baselines. Applying the pipeline to the TAIR dataset, we can complement 373 new relationships. Future studies may include considering environmental influences and phenotypic conditions for constructing gene–phenotype event extraction instead of binary relationships. If the division of phenotype and relationship boundaries is more detailed, performance will be improved. Funding This work was supported by National Key Research and Development Program [grant no. 2016YFD0101900], National Natural Science Foundation of China [grant no. 31701144]. 
 Conflict of Interest : none declared. Supplementary Material Supplementary Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>An optimization framework for network annotation</Title>
    <Doi>10.1093/bioinformatics/bty236</Doi>
    <Authors>Patkar Sushant, Sharan Roded</Authors>
    <Abstract/>
    <Body>1 Introduction With increased mapping of physical interactions in living cells ( Huttlin  et al. , 2017 ), we now have a blueprint of the inner workings of the cell. However, the functional interpretation of this map to simulate the behavior of the cell under different genetic and environmental cues is still beyond reach. One fundamental piece of information that is often missing is the annotation of the network interactions with direction of signal flow and functional activation/repression (sign) effects. The interpretation of the latter effects depends on the type of the physical interaction being considered. For protein–DNA interactions (PDIs), a +/– sign describes a regulatory effect; for protein–protein interactions (PPIs), it represents a functional activation/repression effect. Currently, direction and sign information is available to only a few well-studied pathways (see  Fig. 1  for an example), although a large fraction (40–70%) of the PPIs are expected to admit such an annotation ( Silberberg  et al. , 2014 ). The inference of such annotation information is a pre-condition to any logical model of a system under study [see, e.g. ( Morris  et al. , 2010 )].
 Fig. 1. The yeast signaling pathways from KEGG in one network depicting the organization of different types of physical interactions with their respective experimentally derived signs (activation/repression) and directions A pioneering work by  Yeang  et al.  (2004)  for network annotation suggested a simple yet effective logical model for signaling whereby physical interactions are directed and signed, and a signal can flow along a directed path with its effect being the aggregate effect of its member interactions, i.e. the product of their signs. To tackle the annotation challenge, Yeang  et al.  suggested a machine learning framework, but their method was limited to physical networks of small scale where it is possible to enumerate all paths. Subsequent work in this area adopted the signaling model suggested by Yeang  et al.  but employed (to the most part) combinatorial methods to learn the hidden annotations. In the most common scenario, one is given a partially annotated physical interaction network and a list of pairs of genes obtained from knockout experiments in which a knockout gene (cause, or source) affected the expression of some other gene (effect, or target). The goal is to annotate the remaining interactions in the physical network with directions and signs such that a maximum number of knockout pairs can be explained by the model. The problem of inferring interaction directions so that a maximum number of pairs admit a directed path from the cause to the effect was shown to be non-deterministic polynomial time (NP)-hard and a sub-logarithmic approximation algorithm was given for it ( Blokh  et al. , 2013 ). Using SAT solvers and integer linear programming (ILP) techniques, optimal algorithms were given for various variants of the problem ( Gitter  et al. , 2011 ;  Silverbush  et al. , 2011 ;  Silverbush and Sharan, 2014 ), potentially restricting the length of the path connecting each cause-effect pair. In addition, a machine learning based inference method was suggested by  Stelzl  et al.  (2005) . In contrast, the problem of inferring interaction signs received far less attention.  Ourfali  et al.  (2007)  considered explanatory paths of very short length (3) and provided ILP formulations to maximize the expected number of pairs that can be explained in a probabilistic network.  Peleg  et al.  (2010)  showed that the sign assignment problem is NP-hard and developed network-free algorithms for predicting genome-wide effects of gene knockouts. A related approach using regression was adopted by  Cosgrove  et al.  (2008)  to distinguish direct and indirect targets of cell perturbation.  Houri and Sharan (2012)  were the first to tackle the problem of inferring physical interaction signs on a network while accounting for paths of any length. Specifically, they searched for an assignment that maximized the number of pairs that admit a path of the required sign. They provided network reduction techniques and an ILP formulation to solve this problem to optimality on current physical interaction networks. However, their algorithm could only account for a small fraction of physical interactions in the network (low coverage), as most were contracted in their network reduction step. In this paper, we present novel network based ILP formulations for the purpose of predicting interaction signs in a physical network. The models we propose bypass the issue of network reduction and thus significantly improve the scale of predictions that can be made. In particular, we consider signaling models where a pair is explained by (i) a shortest path connecting the pair having a desired sign (ASP), (ii) a directed shortest path connecting its nodes having a pre-defined sign (AdirSP) and (iii) all shortest paths connecting its nodes having a desired sign (AllSP). We then evaluate the performance of each model in predicting physical interaction signs in yeast over two different gene expression datasets. We show that these models lead to  ≈ 15-fold higher coverage and higher accuracy than the state-of-the-art method of  Houri and Sharan (2012) . Additionally, we propose a machine learning approach for predicting interaction signs that combines features from each of these models and show that it improves over any individual model in predicting signs of previously annotated interactions. 2 Materials and methods 2.1 An optimization framework for sign prediction In this section we describe novel algorithms for inferring signs of physical interactions. We start by formally defining the problem and sketching the previous approach of ( Houri and Sharan, 2012 ). Then, we study three variants of the original problem (each describing a hypothetical signaling model) and develop novel ILP formulations to solve them to optimality on current networks. We assume we are given a (potentially partially signed) physical interaction network along with a collection of cause-effect gene pairs, such as commonly obtained from knockout experiments. The maximum sign assignment (MSA) problem is to assign signs to the unsigned edges of the network in a way that best explains the given pairs. We say that a cause-effect pair ( s ,  t ) with sign  δ st  (+ encoding down-regulation of  t  in response to the knockout of  s , − encoding up-regulation of  t  in response to the knockout of  s ) is  explained  or  satisfied  by a sign assignment, if there exists a path in the network from  s  to  t  whose aggregate sign (the product of the signs along its edges) is  δ st . Formally, MSA is defined as follows: 
 Input.  A partially signed network G(V, E) and a set of k cause-effect pairs ( s 1 , t 1 ) , … , ( s k , t k ) with signs δ s 1 t 1 , … , δ s k , t k ∈ { + , − } Goal.  A sign assignment to the unsigned edges of the network such that a maximum number of input pairs are satisfied by the assignment. 
 This problem focuses on the hypothetical A-path signaling model of  Yeang  et al.  (2004) .  Houri and Sharan (2012)  showed that due to the nature of the model, any unsigned edge that lies on a cycle in the network cannot be uniquely signed. They generalized this notion to any 2-connected component (or block) by determining if these components are  strongly signed . They then proposed an approach to reduce the input network to an acyclic one by contracting all edges in these strongly signed components without affecting the maximum number of pairs that could be satisfied. In the reduced network, every pair is connected by a unique path, facilitating the formulation of an ILP to assign signs to the unsigned edges of this path such that the number of satisfied pairs is maximized. A key drawback of this approach is that reducing the network to an acyclic one severely restricts the number of edges participating in the ILP (coverage) and, hence, restricts the number of interactions that can be uniquely signed. In subsequent paragraphs, we discuss three variants of MSA, each describing a different plausible signaling model, where edges lying on cycles may have unique signs and, hence, may no longer be contracted. The first variant we consider, A-shortest-path (ASP), considers a signaling model where the length of a satisfying path is always assumed to be the shortest possible. The shortest path assumption is motivated from the observation that signaling pathways tend to be of short length ( Silverbush and Sharan, 2014 ). For each edge  ( u , v ) ∈ E , let  x uv  = 1 denote whether its sign is − (0 if +). Similarly, we re-write the signs  δ s t ∈ { + , − }  as  δ s t ∈ { 0 , 1 } . Due to the nature of knockout experiments, there are usually much fewer sources compared to targets. Hence, for each source  s , we construct a subnetwork  G s ( V s , E s )  such that each edge in this subnetwork lies along a shortest path from  s  to one of its targets  t . This is done by applying a breadth-first-search starting from each source and target ( Silverbush and Sharan, 2014 ). Furthermore, we denote by  N s ( v )  the set of neighbors of  v  in  G s  and by  d sv  the length of the shortest path from  s  to  v . Additionally, for each pair ( s ,  v ) in  G s , we define auxiliary variables  c sv ,  r sv  where  c sv  = 0 implies that under the selected sign assignment there exists a shortest path from  s  to  v  with aggregate sign  r sv , i.e. the node pair ( s ,  v ) is  satisfied  under the selected assignment. [Note, ( s ,  s ) is trivially assumed to be satisfied]. We also define  E + , E −  which represent subsets of edges in the ILP with known prior positive and negative signs, respectively. Then the following ILP formulation can be used to solve this variant of MSA:
 max ⁡ ∑ s t y s t s . t . 1 + ∑ u ∈ { N s ( v ) | d s v = d s u + 1 } ( c s u − 1 ) ≤ c s v ∀ s , v ∈ V s ∖ s r s v = XOR ( r s u , x u v | c s v = 0 ) ∀ s , ( u , v ) ∈ { E s : d s v = d s u + 1 } c s t + y s t ≤ 1   ∀ ( s , t ) r s s = 0 , c s s = 0 , r s t = δ s t ∀ ( s , t ) x u v = 0 ∀ ( u , v ) ∈ E + x u v = 1 ∀ ( u , v ) ∈ E − y s t , x u v , r s v , c s v ∈ { 0 , 1 } ∀ s , t , u , v The XOR relation between  r sv ,  r su  and  x uv  is conditioned on the value of  c sv . That is,  r s v = r s u ⊕ x u v  only if  c sv  = 0. It is linearized as follows:
 r s v − c s v ≤ 2 − x u v − r s u r s v − c s v ≤ x u v + r s u r s v + c s v ≥ x u v − r s u r s v + c s v ≥ r s u − x u v Let  l  denote a layer of  G s  such that all nodes belonging to this layer have  d s v = l . Given a feasible solution to the ILP, if  y st  = 1 we can show that there exists a shortest path from  s  to  t  with aggregate sign  δ st . Indeed, if  y st  = 1 then  c st  = 0 by the third constraint. This implies that  ∑ u ∈ N s ( t ) | d s t = d s u + 1 ( c s u − 1 ) &lt; 0 . Thus, if  t  is in layer  l  of  G s , there must exist a neighbor  u  of  t  in layer  l  −1 such that  c su  = 0. Furthermore, if  c st  = 0,  x ut  is bound by the XOR constraint to have a sign whose product with  r su  is  δ st . Similarly, if  c su  = 0, there must be a neighbor  w  in layer  l  − 2 where  c sw  = 0 and  r s w ⊕ x w u ⊕ x u t = δ s t . By carefully investigating the constraints applicable to the subsequent layers of  G s  (i.e.  l − 3 , … , 0 ) we find that there must exist a shortest path from  s  to  t  such that the product of signs along its edges is  δ st . The final two constraints incorporate prior knowledge of signs in the ILP. The second variant we study, ‘A-directed-shortest-path’ (AdirSP), additionally assumes each shortest path explaining a pair to be directed from the cause to the effect. It is worth noting that one cannot adapt existing ILP solutions to the orientation and sign assignment problems, as both rely on reducing the input graph into an acyclic one. This reduction does not work when simultaneously optimizing both. Instead, we simply adapt the ASP formulation above to simultaneously find sign and direction assignments to the network. Specifically, we consider a pair ( s ,  t ) to be satisfied by a sign and direction assignment over the network if a directed shortest path from  s  to  t  in this assignment has aggregate sign  δ st . We call this variant of MSA the ‘A-directed-shortest-path’ (AdirSP). Let  o uv  = 1 denote whether an edge ( u ,  v ) is directed from  u  to  v  (0 if from  v  to  u ) and let the flow variables  f u v s  indicate the existence of a flow from  u  to  v . The flow variables allow computing pair reachability in a directed network. The new ILP is:
 max ⁡ ∑ s t y s t s . t . o u v + o v u = 1   ∀ ( u , v ) ∈ E f u v s ≤ ∑ w ∈ N s ( u ) ∖ v f w u s ∀ s , ( u , v ) ∈ { E s : d s v = d s u + 1 , d s u ≥ 1 } f u v s ≤ o u v ∀ s , ( u , v ) ∈ E s a u v s = ( 1 − f u v s )   OR   c s u ∀ s , ( u , v ) ∈ { E s : d s v = d s u + 1 } 1 + ∑ u ∈ N s ( v ) | d s v = d s u + 1 ( a u v s − 1 ) ≤ c s v ∀ s , v ∈ V s ∖ s r s v = XOR ( r s u , x u v | c s v = 0 , f u v s = 1 ) ∀ s , ( u , v ) ∈ { E s : d s v = d s u + 1 } c s t + y s t ≤ 1   ∀ ( s , t ) r s s = 0 , c s s = 0 , r s t = δ s t ∀ ( s , t ) x u v = 0 ∀ ( u , v ) ∈ E + x u v = 1   ∀ ( u , v ) ∈ E − y s t , x u v , o u v , a u v s , r s v , c s v , f u v s ∈ { 0 , 1 } ∀ s , t , u , v 
The first constraint ensures that each edge has a unique orientation. In some feasible solution, if  f u v s = 1 , then the second and third constraint ensure that a directed path exists from  s  to  v  containing edge ( u ,  v ). Note that the XOR relation that helps determine the sign of an edge now additionally depends on the existence of a flow in that edge. The constraint is linearized as follows:
 r s v − c s v − 1 + f u v s ≤ 2 − x u v − r s u r s v − c s v − 1 + f u v s ≤ x u v + r s u r s v + c s v + 1 − f u v s ≥ x u v − r s u r s v + c s v + 1 − f u v s ≥ r s u − x u v 
Another change from the previous formulation is the definition of auxiliary variables  a u v s  for each edge participating the ILP. Their value depends on the flow in edge ( u ,  v ) originating from  s  and on  c su . The OR relation between these variables is linearized as follows.
 a u v s ≤ ( 1 − f u v s ) + c s u a u v s ≥ 1 − f u v s a u v s ≥ c s u 
Given a feasible solution in which  y st  = 1, we show that there exists a shortest path oriented from  s  to  t  such that its aggregate sign is  δ st . Let  t  be in layer  l  of the shortest path graph  G s . If  y st  = 1, then by the seventh constraint  c st  = 0. It follows that  ∑ u ∈ N s ( t ) | d s t = d s u + 1 ( a u t s − 1 ) &lt; 0  (by constraint 5), which implies that there exists a neighbor  u  in layer  l  − 1 where  a u t s = 0 . This implies  f u t s = 1 ,  c su  = 0 (constraint 4) and  δ st  must be the product of the signs given by  x ut  and  r su  (constraint 6). Additionally,  c su  = 0 implies there exists a neighbor  w  in layer  l  − 2 where  a w u s = 0  (constraint 5). This implies  f w u s = 1 ,  c sw  = 0 and  r s w ⊕ x w u ⊕ x u t = δ s t . In this manner after carefully investigating the constraints through subsequent layers of  G s  (i.e.  l − 3 , l − 4 , … , 0 ) we can find a directed shortest path from  s  to  t  such that the product of signs along its edges is  δ st . The last two constraints account for signs that are already known. The underlying assumption in both signaling models above is that a single path is sufficient to force a pre-defined effect. However, due to the inherent stochasticity in signaling, this might not always be the case ( Ladbury and Arold, 2012 ). Moreover, on careful examination of signed interactions of  Figure 1 , we find that for any node pair connected by more than one completely signed path, the product of signs on each path is the same. Hence, we strengthen the pair satisfaction assumption in the ASP model to require that a pair ( s ,  t ) is satisfied if all shortest paths connecting  s  to  t  admit the same aggregate sign  δ st . We call this variant ‘All-shortest-paths’ (AllSP) and solve for it using the following formulation:
 max ⁡ ∑ s t y s t s . t . c s u ≤ c s v ∀ s , ( u , v ) ∈ { E s : d s v = d s u + 1 } r s v = XOR ( r s u , x u v | c s v = 0 ) ∀ s , ( u , v ) ∈ { E s : d s v = d s u + 1 } c s t + y s t ≤ 1 ∀ ( s , t ) r s s = 0 , c s s = 0 , r s t = δ s t ∀ ( s , t ) x u v = 0 ∀ ( u , v ) ∈ E + x u v = 1 ∀ ( u , v ) ∈ E − y s t , x u v , r s v , c s v ∈ { 0 , 1 } ∀ s , t , u , v 
As above, let  t  belong to layer  l  of  G s . Given a feasible solution to this new formulation, if  y s t = 1 , c s t   must   be   0  (from third constraint). Hence, for every neighbor  u  of  t  that lies in layer  l  – 1 of  G s ,  c su  = 0 (from first constraint). This in turn constrains the sign assignment of the respective edges (i.e.  r s u ⊕ x u t = δ s t , for all neighbors  u  in layer  l  − 1). By carefully investigating the constraints through subsequent layers of  G s  (i.e.  l − 2 , l − 3 , … , 0 ), it becomes apparent that for any node  v  in  G s , all shortest paths from  s  to  v  must admit the same aggregate sign ( r sv ). Hence, all shortest paths from  s  to  t  must have an aggregate sign  δ st . Notably, the models discussed above permit mathematically efficient formulations. Specifically, if  p  is the number of sources ( p ≪ k ), then each formulation contains  O ( k + p | V | + | E | )  variables and  O ( k + p ( | V | + | E | ) )  constraints. 2.2 Quantifying the activation/repression potential of a physical interaction Each of the above models may admit multiple sign assignments with optimal or near optimal scores. Hence, it is necessary to quantify the robustness of a sign assignment to an edge. To this end, we solve each ILP repeatedly  n  times; each time adding a small Gaussian noise of mean 0 and variance 0.01 to the objective function as shown below. This stochastic approach, motivated by  Hazan and Jaakkola (2012) , effectively results in a random sampling of different likely solutions that exist nearby in the optimum solution space, thereby allowing us to assess the robustness of the sign on each edge. The procedure is as follows:
 1:  procedure  G et S cores ( ILP ,  n ) 2:  scores u v ← 0 ,   ∀ ( u , v ) ∈ E  that are in  ILP 3: for  i  = 1:  n 4:    set   objective :   ∑ s t ( 1 + ɛ s t ) y s t , where  ɛ st ∼ N (0, 0.01) 5:    x * ← solve ( I L P ) 6:    scores u v = scores u v + x u v * / n ,   ∀ ( u , v ) ∈ E  that are in  ILP 7:  return  scores An edge score close to 1 implies that the sign is negative with high confidence, a score close to 0 implies a positive sign with high confidence and a score close to 0.5 implies that the sign on that edge cannot be uniquely determined (possibly implicating the absence of an activation/repression effect). For efficiency, we use  n  = 10 throughout. Our results remain qualitatively the same for larger values of  n . 3 Results 3.1 Input data We focused our analysis on budding yeast ( Saccharomyces cerevisiae ). We obtained 4095 PDIs spanning 2079 proteins (conserved across at least two other yeast species) from  MacIsaac  et al.  (2006) . We additionally downloaded 2930 high-quality experimentally verified PPIs from  Yu  et al.  (2008 ), 1361 kinase–substrate/phosphatase–substrate interactions (KPIs) among 802 proteins from  Breitkreutz  et al.  (2010) , and 189 physical interactions from signaling pathways of yeast in Kyoto Encyclopedia of Genes and Genomes (KEGG). We merged these sets into a  unified  yeast network of 8268 unique physical interactions among 3695 proteins. We extracted all 110 487 knockout pairs spanning 6228 proteins from  Reimand  et al.  (2010)  and additionally 699 771 pairs spanning 6110 proteins from  Kemmeren  et al.  (2014) . A pair was assigned a positive sign if the target gene was repressed in response to knockout of the source, and a negative sign if the target gene was activated/up-regulated. We restricted ourselves to knockout pairs such that the absolute log fold change in expression of the target gene is &gt;2 and FDR &lt; 0.001. This leaves us with 1756 significant knockout pairs from  Reimand  et al.  (2010) , referred to here as the  Reimand set , and 3524 significant knockout pairs from  Kemmeren  et al.  (2014) , referred to here as the  Kemmeren set . The above choice of thresholds was made while taking into consideration the inherent computational complexity of the problem. 3.2 Validation data For a systematic validation of our sign prediction models we collected sign information as follows. Only 147 of 192 physical interactions in yeast had an experimentally confirmed sign from KEGG (See  Fig. 1 ). In addition, following  Houri and Sharan (2012) , we extracted gene ontology (GO) molecular function annotations related to transcriptional activators (GO: 0045893) and transcriptional repressors (GO: 0045892). PDIs originating from transcriptional activators were given a positive sign whereas PDIs originating from transcriptional repressors were given a negative sign. Finally, we also extracted information on protein kinases (GO: 0004672) and protein phosphatases (GO: 0004721). We reasoned that since there are roughly three times as many confirmed functionally activating phophorylation sites compared to repressive ones (PhosphoNET database,  www.phosphonet.ca ), and that 71% of phosphorylation interactions of yeast in KEGG are annotated as activating and 81% of de-phosphorylation interactions of yeast are annotated as repressing, kinase–substrate interactions tend to be activating while phosphatase–substrate interactions tend to be repressing. Thus, physical interactions linking a GO annotated kinase and a substrate were given a positive sign whereas interactions linking a GO annotated phosphatase to a substrate were given a negative sign. Any interaction in the unified network that had conflicting signs was left unsigned (unless it had sign information from KEGG, in which case this latter information was used). In summary, the validation set consists of three groups of signed interactions in the network: (i) 2014 (1131+, 883−) signed PDIs, (ii) 1044 (872+, 172−) signed kinase/phosphatase–substrate interactions and (iii) 147 (96+, 51−) signed KEGG interactions. 3.3 Performance evaluation of individual models We evaluated each of the four models presented above (A-path/ASP/AdirSP/AllSP) in a 5-fold cross-validation setting on the unified yeast network, focusing on the interactions covered by each model, i.e. participating in the corresponding ILP. To this end, we randomly divided all signed and covered interactions into five equal parts. Using each model, we predicted the activation/repression potential of the interactions in each part (see Section 2.2) while constraining the signs of interactions in the remaining parts. Then we measured the performance of the activation/repression scores of a given model across the five parts for different subsets of signed interactions covered by the model. For each subset, we denote its set of covered positive and negative interactions by  E +  and  E − , respectively. As a benchmark, we discuss the performance of the previous A-path model. Recall that in this model we should contract all interactions that lie in a strongly signed block of size  ≥ 3  (see Section 2.1). Since all blocks were strongly signed, this resulted in an acyclic network with 77% of the interactions contracted. When working with knockout pairs from the Reimand set, we observe that only 1% of all the network interactions participate in the ILP constraints due to network reduction, and 25 of them belong to the validation set. Due to low coverage over the validation set, we instead evaluated this framework using knockout pairs from the Kemmeren set. Overall, 4% of network interactions are covered in this instance and 73 interactions from the validation set were part of the ILP formulation, yielding an AUC of 0.66. Since there were only 73 interactions to validate our predictions, we could not evaluate the performance on individual subsets. Next, we evaluated the ASP, AdirSP and AllSP models over the unified network.  Tables 1  and  2  summarize the performance over the validation PDIs, KPIs and the KEGG interactions. We find that our new formulations lead to sign assignments on 35% of network interactions when working with the Reimand set and 59% of network interactions when working with the Kemmeren set;  ≈ 15-fold coverage increase compared to previous work (A-path).
 Table 1. Performance evaluation using the Reimand set (coverage of 35%) Interaction | E + | , | E − | AUC AUC AUC (ASP) (AdirSP) (AllSP) PDI 435, 458 0.75 0.63 0.84 KPI 205, 20 0.83 0.56 0.72 KEGG 40, 27 0.56 0.52 0.65 Table 2. Performance evaluation using the Kemmeren set (coverage of 59%) Interaction | E + | , | E − | AUC AUC AUC (ASP) (AdirSP) (AllSP) PDI 744, 653 0.63 0.59 0.83 KPI 522, 98 0.61 0.51 0.77 KEGG 46, 32 0.58 0.54 0.71 In order to directly compare the performance of the A-path model to our suggested alternative models, we evaluated them on the restricted validation set of 73 interactions covered by the A-path model. On this set ( | E + | = 49 , | E − | = 24 ) the performance of AdirSP was lower to A-path (AUC of 0.64), while ASP and AllSP had better performance (AUCs of 0.73 and 0.68, respectively). 3.4 Performance evaluation of the combined model Previous work as well as our models above vary in the assumptions they make on the way a knockout effect is explained, going all the way from requiring a single path of any length to requiring all paths of shortest length. Note that we adopt these models partly because they are grounded in our very own observations of cellular signaling pathways (see Section 2.1) and because they permit an efficient mathematical formulation. These descriptions are not perfect. In turn, the solution of each model allows different degrees of freedom on the signs of underlying interactions. To make the best inference possible for each physical interaction given the complex nature of cellular signaling, we integrate the predictions of each model in an ensemble. That is, using the sign scores from solutions to ASP, AdirSP and AllSP as features, we train a hybrid model, specifically a random forest classifier, that makes an overall prediction of the sign of an interaction (A-path was excluded due to low coverage). The ensemble model is evaluated via nested cross-validation. In detail, the validation set is divided into the same five parts as above. Four of the parts are used for training the individual models to score the fifth part. Next, we perform a 5-fold cross-validation on the fifth part to train and test the classifier. Finally, using the cross-validated predictions across all parts, we report the mean classifier performance (AUC) against the signs of different validation subsets.  Tables 3  and  4  summarize the performance of the random forest classifier on the different knockout sets and validation subsets.
 Table 3. Performance evaluation of the random forest classifier using the Reimand set Interaction | E + | , | E − | AUC (classifier) PDI 435, 458 0.86 KPI 205, 20 0.85 KEGG 40, 27 0.77 Table 4. Performance evaluation of the random forest classifier using the Kemmeren set Interaction | E + | , | E − | AUC (classifier) PDI 744, 653 0.80 KPI 522, 98 0.67 KEGG 46, 32 0.81 The performances of the classifier and the individual models are depicted in  Figures 2  and  3 . Importantly, we observe that the classifier outperforms all individual models on the set of curated interactions from KEGG. It also outperforms the different models with respect to PDIs and KPIs on the Reimand set. The lower performance of the classifier on the KPI set (compared with the AllSP model) when working with the Kemmeren set is likely an artifact resulting from the skewed distribution of class labels. Such a skew may influence ensemble classifier performance on unseen data. ( Galar  et al. , 2012 ).
 Fig. 2. Performance evaluation of all models using the Reimand set Fig. 3. Performance evaluation of all models using the Kemmeren set 4 Conclusions In summary, we developed novel ILP formulations for predicting signs of physical interactions under different signaling models. We discussed the underlying assumptions guiding the predictions of each model and its advantages in terms of coverage relative to prior work by  Houri and Sharan (2012) . We then measured the cross-validation accuracy of our models in predicting signs across two knockout datasets to find that our models lead to improvement in accuracy and coverage over the previous state-of-the art method by  Houri and Sharan (2012) . Finally, we derive a hybrid signaling model based classifier that uses the sign assignment confidence scores of each model studied for predicting interaction signs. This was partly motivated by the fact that the three models presented in the paper, although mathematically efficient to represent, may be insufficient to capture the complex nature of cell signaling. Furthermore, this warrants the exploration of other plausible models that could be potentially integrated into the classifier to improve its predictions. For instance, one could additionally formulate an ILP that considers All-Directed Shortest paths (All-dirSP) as a plausible signaling mechanism to explain some of the knockout effects. Notably, the AdirSP model is the first to combine both direction and sign prediction within the same optimization framework and may be of independent interest to readers to expand on for the general problem of annotating a network with directions and signs. A potential limitation of our approach is the computational cost involved in solving the models (especially if the network is sparsely annotated with signs). In this work, running the AllSP model on the Kemmeren set took the most time (up to 3 h to obtain a single solution). In all other cases it took on average 3 min to obtain a single solution (note that these values may differ based on the solver and the computer used). A theoretical comparison of efficiency can be done by analytically reasoning about the size of the search space for each model. The ASP and AdirSP models require  O ( d ) XOR constraints to be satisfied to explain a single knockout pair ( s ,  t ) (where  d  denotes the shortest path distance from  s  to  t ). In contrast, the AllSP model requires a much larger number of  O ( | E | )  XOR constraints to be satisfied to explain a single knockout effect (where  | E |  denotes the number of edges in the network). Each constraint introduces at least one new Boolean variable and hence the search space for AllSP is much larger than that for ASP or AdirSP. This might pose a major limitation when it comes to predicting physical interaction signs over much larger networks, e.g. the human network, which still remains mostly unsigned. To enhance the scalability of our methods, one can pre-process the network and either contract interactions involved in protein complexes or pre-annotate them with positive signs (which are logically consistent with their role as a means to propagate signal forward without influencing the overall effect of the path taken). This vastly cuts down the size of the solution space; by half per interaction. The different models we proposed represent a trade-off between model coverage (of networks edges) and complexity (of its solution). Another limitation of our results is that they are based on two approximate sources of sign information, namely KPI signs derived from whether the source protein is a kinase or a phosphatase and PDI signs derived from whether the involved transcription factor is an activator or a repressor. We expect our models to yield more accurate results as better quality sign information becomes available. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Recognition of beta-structural motifs using hidden Markov models trained with simulated evolution</Title>
    <Doi>10.1093/bioinformatics/btq199</Doi>
    <Authors>Kumar Anoop, Cowen Lenore</Authors>
    <Abstract>Motivation: One of the most successful methods to date for recognizing protein sequences that are evolutionarily related, has been profile hidden Markov models. However, these models do not capture pairwise statistical preferences of residues that are hydrogen bonded in β-sheets. We thus explore methods for incorporating pairwise dependencies into these models.</Abstract>
    <Body>1 INTRODUCTION Profile hidden Markov models (HMMs) have been one of the most successful methods to date for recognizing both close and distant homologs of given protein sequences. Popular HMM methods such as HMMER (Eddy  et al. ,  1998a ,  b ) and SAM (Hughey and Krogh,  1996 ) have been behind the design of databases such as Pfam (Finn  et al. ,  2006 ), PROSITE (Hulo  et al. ,  2006 ) and SUPERFAMIILY (Wilson  et al. ,  2007 ). However, a limitation of these HMMs is, since there is only finite state information about the sequence that can be held in any particular position, HMMs cannot capture dependencies that are far, and variable distance apart, in sequence. On the other hand, in β-structural motifs, as was noticed by Lifson, Sander and others (Hubbard and Park,  1995 ; Lifson and Sander,  1980 ; Olmea  et al. ,  1999 ; Steward and Thornton,  2002 ; Zhu and Braun, 1995), amino acid residues that are hydrogen bonded in β-sheets exhibit strong pairwise statistical dependencies. These residues, however, can be far away and a variable distance apart in sequence, making them impossible to capture in an HMM. Early work of Bradley  et al.  (Bradley  et al. ,  2001 ; Cowen  et al. ,  2002 ) show that these pairwise correlations help to recognize protein sequences that fold into the right-handed parallel β-helix fold. More recent work has used a conditional random field or Markov random field framework, both of which generalize HMMs beyond linear dependencies, to identify the right-handed parallel β-helix fold (Liu  et al. ,  2009 ), the leucine rich repeat fold (Liu  et al. ,  2009 ) and the β-propeller folds (Menke  et al. ,  2010 ). While these conditional random field and Markov random field models are extremely powerful in theory, in practice, substantial computational barriers remain for template construction, training and computing the minimum energy threading of an unknown sequence onto a template. Thus, a general structure software tool designed for β-structural folds, in the same manner as HMMER and SAM packages recognize all protein structural folds, remains a challenging unsolved problem. In this article, we find an unusual and different way to incorporate pairwise dependencies into profile HMM. In particular, we generalize our recent work (Kumar and Cowen,  2009 ) on augmenting HMM training data to include these very pairwise dependencies as a part of a larger training set (see below). While this method of incorporating pairwise dependencies is undoubtedly less powerful than MRF methods, it has the advantage of being simple to implement, computationally fast and allows the modular application of existing HMM software packages. We show that our augmented HMMs perform better than ordinary HMMs on the task of recognizing β-structural SCOP (Lo Conto  et al. , 2002) protein superfamilies. In particular, we consider the problem of how well an HMM trained on only one family β-structural SCOP superfamily can learn to recognize members of other SCOP families in that SCOP superfamily, as compared to decoys. We show a median AUC improvement of nearly 5% for our approach compared to ordinary HMMs on this task. 2 APPROACH Our approach is based on the simulated evolution paradigm introduced in Kumar and Cowen ( 2009 ). The possibility that motif recognition methods could be improved with the addition of artificial training sequences had been previously suggested in the protein design community (Koehl and Levitt,  1999 ), though the methods of Koehl and Levitt ( 1999 ); Larson  et al.  ( 2003 ) and Am Busch  et al.  ( 2009 ) to generate these sequences are much more computationally intensive than the simple sequence-based mutation model of Kumar and Cowen. In particular, Kumar and Cowen created new training sequences by artificially adding point mutations to the original sequences in the training set, using the BLOSUM62 matrix (Eddy,  2004 ). The HMM training was then used on this larger, augmented training set unchanged. In this article, we compare ordinary HMMER Profile HMMs, HMMER Profile HMMs augmented with a point mutation model (similar to Kumar and Cowen,  2009 ), and HMMs augmented with training sequences based on pairwise dependencies of β-sheet hydrogen bonding (see  Fig. 1 ). Thus we have generalized the single frequency approach of Kumar and Cowen ( 2009 ), to pairwise probabilities. More specifically, to create our new training sequence based on β-strand constrained evolution, the following pipeline is followed:
 The input to HMM training is a set of PDB files for sequences that lie in the same SCOP family. The sequences are aligned by way of multiple structure alignment program. Positions corresponding to paired residues that hydrogen bond in adjacent β-strands are found using SmurfParse package. For each sequence that lies in the original training set, additional sequences are added to the training set using random mutations according to a probability distribution based on the paired positions within β-strands, as described below. The multiple sequence alignment, including sequences in the original training set as well as the new sequences generated by simulated evolution, is passed to the ordinary HMM training module. 
This pipeline is illustrated in  Figure 1 B, along with HMM-C, an approach that combines both point mutations and pairwise mutations in the training set.
 Fig. 1. Training HMMs by ( A ) a pointwise mutation model, ( B ) a pairwise mutation model and ( C ) combining (A and B). We use these augmented HMMs to solve the following task: trained only on the sequences from single SCOP family can our HMMs distinguish between the following two classes: (i) sequences from other SCOP families in the same SCOP superfamily as the training set and (ii) decoy sequences that lie outside the fold class of the family of the training set. 3 METHOD 3.1 Datasets We employed an approach similar to that of Wistrand and Sonnhammer ( 2004 ) to pick SCOP families and superfamilies from among those that belong to the ‘mainly beta proteins’ class in SCOP and train HMMs. First, we chose sequences from SCOP that are &lt;95% identical based on the ASTRAL database version 1.73 (Chandonia  et al. ,  2004 ). The dataset was then filtered to include only the SCOP families that belonged to ‘mainly beta proteins’ class and had at least 10 sequences. Another constraint imposed in order to have test sets was to make sure that other SCOP families in the superfamily hierarchy had at least one sequence but not more than 50 sequences. Our test set consisted of all the sequences from the rest of the families in the superfamily and an equal number of decoy sequences chosen at random from different SCOP folds. The dataset is available at:  http://bcb.cs.tufts.edu/pairwise/ . 3.2 Multiple sequence alignment This is the process of aligning the homologous residues in protein sequences into columns and thus generating a multiple sequence alignment (MSA). 3.2.1 Aligning sequences with MUSCLE For the single frequency augmented training model, we used the popular program MUSCLE Version 4 (Edgar,  2004 ) to generate the MSA that was provided to the HMM training methods. It is one of the fastest programs available and produces global sequence alignments for the set of sequences from a family. We developed a script to transform the MUSCLE alignment output to .ssi (STOCKHOLM) format since other MUSCLE output formats are not supported by HMMER 3.0a2. 3.2.2 Aligning sequences with Matt For the pairwise augmented training model, and the hybrid model, we used multiple alignment with translations and twists (Matt) (Menke  et al. ,  2008 ) to align the sequences based on the structure. By allowing local flexibility and allowing small translations and rotations, Matt demonstrates an ability to better align the ends of α-helices and β-strands. We used Matt in default configuration for aligning the sequences in a family. Alignment based on structure is essential to determine the location of β-strands in the sequences and thus augment the dataset based on conserved residue pairs in β-strands. 3.3 Mutation models 3.3.1 Simple mutation model We used the BLOSUM62 matrix as our simple model of evolutionary mutations (Eddy,  2004 ). Mutations in a sequence are added by randomly picking a position in the sequence and the replacing the amino acid in that position with a new amino acid based on the BLOSUM62 probability until a desired threshold of  s % mutations is reached. For each training sequence,  N  new mutated sequences with  s % mutations are created and added to the training set. Therefore a family with 100 sequences will have 100 +  N  × 100 (100 original +  N  × 100 mutated) sequences in the training set. In this study, we create training sets with 5, 10, 15, 20 and 25% mutations per the length of sequence and tested several values of  N  ranging from 10–1000. We picked a value of  N  at the 20% mutation rate for which the results were stable (see  Section 3.5 ). 3.3.2 β-Strand mutation model In this step we augment the MSA with a set of sequences that are produced by mutating the original sequences in such a way that the frequency of pairs of amino acids hydrogen bonded in β-sheets resembles the frequency observed in known protein fold space. We use the pairwise conditional probability frequency tables from the recent paper of Menke  et al.  ( 2010 ). There are two tables, representing the in–out residue positions, respectively, for β-sheets that have one side buried and one side exposed to solvent. The tables were learned from solved protein structures in the PDB. β−Strands in the aligned set of structures are found by the program SmurfPreparse which is part of the Smurf Package (Menke,  2009 ; Menke  et al. ,  2010 ). The program not only outputs the positions of the consensus β-strands in the alignment, it also declares a position buried or exposed based on which of the two tables is the best fit to the amino acids that appear in that position in the training data. For each sequence in the training set,  M  mutated sequences with  p % mutations are created and added to the training set. Here ‘ p ’ is set not to be proportional to the total length of the entire sequence, but instead to the total length of the β-strand positions in the alignment. New sequences are created as follows. Residue positions contained in β-strands are selected uniformly at random. If position ‘ i ’ is selected, its pair residue ‘ j ’ is found (note that  j  may appear before or after ‘ i ’ in sequence) and  i  is mutated according to the appropriate pairwise table, conditioned on it being hydrogen bonded to the residue of type in position ‘ j ’. This process is repeated  p  times and the resulting sequence is added to the augmented training set. At the end of this process, for example, a family with 100 original sequences in the training set will have 100 +  M  × 100 (100 original +  M  × 100 mutated) sequences in the augmented training set. In this study, we set values of  p  that would result in training sets with 10–100% mutations ( note : we allow sites to mutate more than once, for example some of the positions even a sequence with a 100% mutation rate may not end up mutated) and tested multiple values of  M  ranging from 10–1000. We picked a value of  M  at which the results were stable at the 20% mutation rate (see  Section 3.5 ). 3.4 Building the HMM In our approach, the primary steps in building the HMM remain the same except the training set is augmented with mutated sequences based on the two evolutionary models. The process is shown in  Figure 1 . Two packages are widely adopted to work width profile HMMs: SAM (Hughey and Krogh,  1996 ) and HMMER (Eddy,  1998a ,  b ). SAM has been demonstrated to be more sensitive overall, while HMMER's model scoring is more accurate (Wistrand and Sonnhammer,  2004 ). In this study we use HMMER versions 3.0a2 to evaluate the models of protein families as it is freely available and can be easily downloaded from the website. We construct HMMs from the MSAs using the  hmmbuild  program which is part of the HMMER package. In this approach, the model of the HMM is made up of a linear set of match ( M ) states, one per consensus column in the MSA. Each  M  state emits a single residue, with a probability score that is determined by the frequency that residues have been observed in the corresponding column of the MSA. Each match state therefore carries a vector of 20 probabilities, for scoring the 20 amino acids. The HMMs also model the gapped alignments by including insertion (I) and deletion (D) states in between the match states. The match, insertion and deletion states are connected by the transition probabilities. In our experiment, HMMER is used as a black box except the constraints on choosing match states are made tighter. Using default settings, HMMER creates a match state whenever a column in the MSA has &lt;50% gaps. We found empirically in Kumar and Cowen ( 2009 ) that the default cutoff was not optimal for our datasets because homology was too remote, and creating a column whenever there are &lt;20% gaps yielded the best HMMs on our datasets. Thus we duplicate this threshold in the current study. By default, HMMER uses a maximum a posteriori (MAP) architecture algorithm to find the model architecture with the highest posterior probability for the alignment data. The algorithm is guaranteed to find a model and constructs the model by assuming that the MSA is correct and then marks columns that correspond to match states. An HMM is created for every MSA, thus there is a one to one correspondence between an MSA and an HMM, generating a library of HMMs. Therefore, for any sequence from the MSA, the HMM can be used to determine if it belongs to the MSA. In addition, the HMM can be used to check if a new sequence is similar to the sequences in the MSA and if it is then one can place the new protein in the same family. We used the default ‘glocal’ setting to construct the models which are global with respect to model and find multiple hit local with respect to sequence. In order to reduce the skewness in the distribution of sequences used to construct an HMM, HMMER supports several options to weight the sequences in training data. The default option GSC assigns lower weights to sequences that are over-represented (Gerstein  et al. ,  1994 ). In addition, HMMER supports external and internal sequence weighting strategies based on information theoretic principles. Based on our study of different sequence weighting options for HMMs with and without the point mutation augmented training for the task of learning SCOP superfamilies (Kumar and Cowen,  2009 ) we used SAM sequence entropy (Karplus  et al. ,  1998 ) throughout the present study. 3.5 HMM scoring Once an HMM is build from an MSA, a new sequence can be scored by the HMM. The score ( S ) is the log of the probability of observing the sequence from a HMM divided by the probability of observing the same sequence from the ‘null hypothesis’ model or HMM.
 P (seq|HMM) is the probability of the target sequence according to a HMM and  P (seq|null) is the probability of the target sequence given a ‘null hypothesis’ model of the statistics of random sequence. In HMMER, this null model is a simple one-state HMM that says that random sequences are independently and identically distributed sequences with a specific residue composition. In addition, HMMER also generates an  E -value which is the expected number of false positives with a score as high as the hit sequence. While the log odd scores ( S ) provides information on the quality of a hit, the  E -value gives a measure relative to other sequences. Therefore a lower  E -value implies that the sequence matches more closely to the HMM. After constructing an HMM, a cutoff for the score ( S ), or  E -value, is set. A new sequence that lies within the cutoff is said to belong to the family that is associated with the HMM. Thus by varying the cutoff, the true positive and false positive rates of the classifier can be tuned. We run experiments over a range of cutoffs to generate receiver operating characteristics (ROC) plots that graph the tradeoffs of the true and false positives, as the cutoffs are tuned. We also compute the area under the ROC curve (AUC) to summarize the classifier statistic in a single number (Sonego  et al. ,  2008 ). We also use average errors at minimum error point (MEP) statistics to assess the performance of HMMs. An MEP is the score threshold at which the classifier makes fewest errors of both kinds, i.e. false positives and false negatives (Karchin  et al ,  2002 ). The percentage of both types of errors provides a comparison of both sensitivity and specificity. 3.6 HMM stability Because our method for augmenting the training data is randomized there is a legitimate concern that any reported result might vary each time the algorithm is run. While results will in fact vary, in fact the variation decreases as  N  and  M  grow larger. We refer to the variation between different runs of the algorithm as the  stability  of the procedure and we empirically experimented with different values of  N  and  M  in order to ensure sufficiently consistent results. We augmented the training set with 10, 50, 100, 200, 500 and 1000 mutated sequences for each original sequence in the training set, for both pointwise and pairwise mutation models. We generated the augmented training set 40 times at 20% mutation rate for each protein family in our training set with a different random seed and constructed the HMMs as described above. For each HMM, we computed the MEP for each iteration.  Figure 2  shows the variation in the SD of the MEP for the single mutation model, and  Figure 3  shows the variation in the SD of the MEP for the pairwise mutation model. Based on these results we set  N  and  M  to each be 150 in this artilcle.
 Fig. 2. Variation in SD of MEP for HMM training augmented with 10–100 sequences based on the point mutation model. 
 Fig. 3. Variation in SD of MEP for HMM training augmented with 10–1000 sequences based on pairwise β-sheet mutation model. 4 RESULTS As described in  Section 3.1 , our dataset consisted of the 41 SCOP families from the ‘mainly beta’ section of SCOP hierarchy, each of which had at least 10 structures, after filtering at the 95% sequence identity level and for which between 1 and 50 sequences in their associated SCOP superfamily but outside the SCOP family existed. In each of the 41 cases, the training set was derived from the training sequences from the SCOP family, and the test set consisted of the sequences outside the SCOP family from the same SCOP superfamily (the positive examples) as well as an equal number of decoy sequences chosen randomly from outside the associated SCOP fold (the negative examples). For each SCOP family in the training set, we trained an ordinary HMM model and plotted the ROC curve and calculated the AUC for this task. Over all 41 families, the median AUC was 69%. We then augmented the training set with the point mutation model ( Fig. 4 ), our new pairwise β-sheet model ( Fig. 5 ) and using training sequences generated from both models simultaneously ( Fig. 6 ).  Figure 4  displays how the median AUC varies with the pointwise mutation rate. Similar to Kumar and Cowen,  2009 , the median AUC improves by training set augmented with simulated evolution all the way up to just above a 15% mutation rate, which gives a median AUC improvement of 3.72%. When we look at the same statistics for the pairwise mutation model in  Figure 5 , the results are less linear with a peak 3.94% improvement at the 10% mutation rate and maximum median AUC improvement of 4.79%. Combining both types of augmented data it is the first peak of pairwise mutations combined with pointwise mutation rate of 15% that give the maximum median AUC for our experiment, an AUC improvement of 4.95%. However, the variance in different runs of this randomized procedure might mean that the best setting is sometimes here and sometimes closer to the second highest peak in  Figure 6  (around 50% pairwise mutations).
 Fig. 4. Median percent AUC improvement with mutation rate for HMMs trained with pointwise mutations. The maximum median improvement is 3.72% at 15% mutation rate. 
 Fig. 5. Median percent AUC improvement with mutation rate for HMMs trained SAM with pairwise mutations. The maximum median improvement is 4.79% at 50% mutation rate. 
 Fig. 6. Median percent AUC improvement with mutation rate for HMMs trained with and dataset augmented with combined pointwise and pairwise mutations. The maximum median improvement is 4.95% at pairwise mutation rate of 10% and pointwise mutation rate of 15%. Finally in  Figures 7  (pointwise) and  8  (pairwise), we break down the increase and decrease in AUC as a function of mutation rate family by family. Most families show some positive increase in AUC in all augmented training models, but for around a fifth of the families performance degrades for the pointwise mutation model. The non-linearity in the median AUC as a function of mutation rate in the pairwise mutation model is partially explained by examining the proportion of families where performance improves versus degrades in  Figure 8 . In particular, performance degrades for &lt;25% of the families at the 10% pairwise mutation rate, but this jumps up to 25% or more thereafter. Meanwhile the families where AUC improves with pairwise mutations shows a peak improvement level between 40 and 50% mutation rate.
 Fig. 7. Distribution of families with improved performance for pointwise mutation model. 
 Fig. 8. Distribution of families with improved performance for pairwise mutation model. It would be nice if there was a biological characterization of what families will have improved versus degraded AUC with pairwise mutated augmented training data. However, in this study, the biological variation is almost certainly swamped by the variation we see due to the different extent varying families within the same superfamily are represented among solved structures in the PDB and hence the size and diversity of our test sets, as well as the difficulty of the different random decoy structures that were chosen when we constructed our datasets. Although the present study cannot therefore address exactly how to tune mutation rate parameters on a per family level, it is clear from our results that out pairwise mutation model is successful in improving the detection of remote homologs of β-structural motifs. While we cannot make any strong conclusions, we did find, as a general rule, that the pairwise mutations helped the most when there was the smallest diversity in the training sequences at a family level, that is, when there were the fewest number of known families for a given superfamily. 5 DISCUSSION We have shown how pairwise dependencies in β-sheets can be incorporated into an augmented HMM training set using simulated evolution, resulting in improved recognition of β-structural motifs. Our datasets, augmented training sets, and our HMMs are all available online at  http://bcb.cs.tufts.edu/pairwise/ . In the present work, it was assumed that the structural information was available for sequences in the training set; thus structural information was used to construct the multiple sequence alignment, to locate β-strands, and to determine how the β-strands were hydrogen bonded into β-sheets. However, ordinary HMMs and our earlier, simpler, point mutation model of simulated evolution require only sequence information, not structure. Extending our work to the case where no solved protein strcuture is known is an interesting open question. Secondary-structure prediction programs (Rost,  2001 ) could be used to find β-strands, but determining how they are paired and hydrogen bonded is a much more difficult issue. Computationally predicting how β-strands are paired in the absence of structural information is a well-studied problem since 1995 (Cheng and Baldi,  2006 ; Hubbard and Park,  1995 ; Jeong  et al. ,  2007 ; Steward and Thornton,  2002 ; Zhu and Braun,  1999 ). Recent work that has tried to computationally model transmembrane β-barels (Waldispuhl  et al. ,  2008 ) and β-amyloids (Bryan  et al. ,  2009 ) without a structural template may also be relevant. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>An evolutionary model motivated by physicochemical properties of amino acids reveals variation among proteins</Title>
    <Doi>10.1093/bioinformatics/bty261</Doi>
    <Authors>Braun Edward L</Authors>
    <Abstract/>
    <Body>1 Introduction Many studies have examined the role of models in phylogenetic estimation using maximum likelihood (ML). Most studies have focused on tree topology (e.g.  Hoff  et al. , 2016 ), but the estimates of model parameters also have the potential to provide biological insights. For example, early studies revealed a bias toward transition (rather than transversion) substitutions ( Yang, 1994 ) and later studies examined neighboring-nucleotide effects ( Hwang and Green, 2004 ) and strand asymmetries ( Polak and Arndt, 2008 ). The ratio of non-synonymous to synonymous substitutions in coding regions (called  K A / K S  or  d N / d S ) is the most common use of a model parameter for inference in molecular evolution; the  K A / K S  ratio can be estimated using models of codon evolution ( Yang, 1998 ;  Yang and Nielsen, 2002 ). Codon models are used in many studies, sometimes at the whole-genome scale (e.g.  Weber  et al. , 2014 ;  Zhang  et al ., 2014 ). In contrast to models that use nucleotide multiple sequence alignments (MSAs), either coding or non-coding, there is a relative paucity of methods to conduct similar analyses of protein MSAs. In principle, the ratio of ‘radical’ to ‘conservative’ amino acid substitutions ( K R / K C ) could be used in a manner similar to the  K A / K S  ratio ( Hanada  et al. , 2007 ;  Hanada  et al. , 2009 ;  Smith, 2003 ;  Zhang, 2000 ), although the  K R / K C  ratio is harder to interpret than the  K A / K S  ratio. There are two challenges associated with using ML methods to understand patterns of protein evolution. First, there are many ways to define radical versus conservative amino acid substitutions ( Hanada  et al. , 2007 ), unlike non-synonymous versus synonymous substitutions, which can be defined unambiguously. Second, estimates of the  K R / K C  ratio will ultimately reflect the estimates of parameters in the instantaneous rate matrix (IRM), or  Q  matrix, which describes amino acid evolution for specific proteins. However, amino acid models have an IRM with a much larger number of free parameters than models of nucleotide sequence evolution. If we assume time reversibility the IRM, which is used to calculate the likelihood, can be is the product of a symmetric rate matrix ( R ) reflecting the ‘exchangeability’ for specific pairs of character states (i.e. nucleotides, codons or amino acids) and a diagonal matrix  (Π ) with the equilibrium frequencies of each state ( Swofford  et al. , 1996 ). The general time reversible model of nucleotide evolution (GTR 4 ) has eight free parameters (five for  R  and three for  Π ), so the variance of the parameter estimates will be acceptable if they are estimated using typical nucleotide MSAs. The analogous amino acid model (GTR 20 ) has 208 free parameters (189 for  R  and 19 for  Π ). Individual proteins are often fairly short (e.g. 280–600 amino acids;  Tiessen  et al. , 2012 ) so typical protein MSAs are unlikely to provide enough information to generate accurate estimates of that many free parameters. This raises the question of how a codon model can be implemented in a practical manner. After all, GTR 61  is the analogous codon model (assuming the universal code); that model has a very large number of free parameters (1829 for  R  and 60 for  Π ). However, the dimension of codon models can be reduced using an IRM where all elements that require multiple simultaneous substitutions are set to zero and the remaining elements are assigned values based on a single transition–transversion ratio and  K A / K S  ratio (the κ and ω parameters, respectively, in the study by  Yang, 1998 ). This dimension reduction actually reveals valuable biological information because estimates of  K A / K S  are easier to interpret than the collection of values in the IRM. An analogous approach for models of amino acid evolution would be useful. Most phylogenetic studies that use proteins eschew estimation of the  R  matrix parameters using a fixed  R  matrix generated using a training set of protein MSAs. This approach was pioneered by  Kishino  et al.  (1990) , who used  Dayhoff  et al . (1978)  PAM matrix as the  R  matrix, and it solves the problem of parameter estimation as long as the training set is large enough. Subsequent studies have used other  R  matrices ( Table 1 ). Although these ‘empirical models’ with fixed  R  matrices may be useful for phylogenetics they cannot provide insights into the process of protein evolution. For example,  Keane  et al.  (2006)  reported that rtREV is the best-fitting model for 33% of archaeal, 21% of proteobacterial and 4% of vertebrate proteins MSAs. However, rtREV was trained using retroviral  pol  proteins ( Dimmic  et al. , 2002 ) so it is unclear why diverse archaeal proteins would fit rtREV better than the more general models trained on a diverse set of proteins. This observation raises a fundamental question: does identifying the best-fitting model provide any useful information about specific proteins? That question can only be answered in the negative if we focus on empirical models.
 Table 1. Empirical models of protein sequence evolution Model Training data References General models:  JTT — Jones  et al.  (1992)  LG — Le and Gascuel (2008)  PAM (Dayhoff) — Dayhoff  et al.  (1978)  PMB — Veerassamy  et al.  (2003)  VT — Müller and Vingron (2000)  WAG — Whelan and Goldman (2001) Specialized models:  HIVb HIV (eight proteins) Nickle  et al.  (2007)  rtREV retroelement  pol Dimmic  et al.  (2002) Note:  ‘—’ indicates that many protein MSAs were used for training. Many different methods were used to estimate  R  matrix parameters. Only a selected subset of specialized models is shown; many specialized models were trained using viral data (e.g. FLU) or organelle-encoded proteins (e.g. mtREV24 and cpREV). A lower dimensional model of protein evolution with parameters that have a clear biological interpretation would allow us to examine the ways that patterns of evolution differ among proteins. Simply using the  K R / K C  ratio described above is unlikely to solve this problem, since there are many ways to divide amino acid substitutions into radical versus conservative subsets ( Fig. 1 ). This reflects the fact that there are likely to be many axes in ‘selection space’ (e.g. one for selection against radical changes in amino acid side chain size, a second related to radical changes in side chain polarity and so forth). The complexity of amino acid properties ( Fig. 1 ) suggests that it would be better to eschew simply classifying amino acids interchanges as radical or conservative and devise a parameter that can capture different degrees of ‘radicalness’ (e.g. the selection against a large-to-tiny interchange is likely to be stronger than selection against a large-to-small substitution). Finally, information about the relative rates at which different non-synonymous mutations enter populations is also likely to be important ( Yampolsky and Stoltzfus, 2005 ). I propose a six-parameter model, with two parameters related to mutational input and four parameters that capture the physicochemical properties of amino acids (to address the impact of selection against radical substitutions). Thus, the model only has one more  R  matrix parameter than the GTR 4  model (although it does have 19 equilibrium frequency parameters). It is likely to be possible to estimate these parameters from typical protein MSAs. The biological interpretability of these parameters should allow us to ask about general patterns across all proteins and to assess the degree to which different proteins exhibit distinct patterns of evolution. The proposed model is used to examine several datasets to explore those general patterns and the variation among proteins in their patterns of sequence evolution.
 Fig. 1. 
 Dividing amino acid interchanges into radical and conservative is difficult.  Amino acids can be divided into many different groups; radical changes are those between groups whereas conservative changes are within groups.  Dayhoff  et al.  (1978)  groups reflect patterns in their PAM matrix and their physicochemical properties.  Hanada  et al.  (2007)  groups maximized the correlation between  K R / K C  and  K A / K S  for mammalian proteins. Many studies (e.g.  Weber  et al. , 2014 ) calculate  K R / K C  using a simple polar-nonpolar and/or large-small categorization. However, changes in many amino acid properties (i.e. any interchanges that cross lines in the diagram) can be radical, at least in some contexts. In fact, certain amino acids (C and P, shaded) have unique properties and any substitution involving them might be radical. Thus, radical versus conservative changes should be viewed as a matter of degrees rather than absolutes 2 Materials and methods This section focuses on generating the  R  matrix; readers are referred to various reviews ( Felsenstein, 2004 ;  Swofford  et al. , 1996 ;  Warnow, 2018 ;  Yang, 2006 ) for general information about likelihood calculations in phylogenetics. The models proposed here populate an  R  matrix using the general approach shown as follows:
 (1) r i j = K i j e x p - φ 1 Δ i j 1 e x p - φ 2 Δ i j 2 e x p - φ 3 Δ i j 3 … 
where  r ij  are  R  matrix elements,  φ  are weighting parameters and  K ij  is a constant. The weighting parameters are estimated by ML (see below). Δ ij  are the absolute value of the difference between amino acids  i  and  j  in some property (e.g. polarity) divided by the maximum absolute value for all possible differences between pairs of amino acids. Thus, Δ ij  are fixed numbers between zero and one for any specific property and pair of amino acids (see  Supplementary File S1 ).  Equation (1)  has the property that setting any  φ  value to zero yields a sub-model in which the amino acid property related to that  φ  parameter has no impact on the model. The amino acid properties examined here were side chain volume ( V ), polarity ( P ), composition ( C ) and aromaticity ( A ). The first three are from the work by  Grantham (1974)  and the fourth is from work by  Xia and Li (1998) . The general approach shown in  Equation (1)  can be rewritten in a more specific manner as 
 (2)   r i j = e x p - V Δ i j V e x p - P Δ i j P e x p - C Δ i j C e x p - A Δ i j A 
where the letters are the  φ  parameters for the properties studied hare and all  K ij  are set to one. There are 16 models based on  Equation (2) , ranging from the simplest model, where  V  =  P = C  =  A  =   0, to the most complex where all parameters are free to vary. The simplest model is actually an F81-like ( Felsenstein, 1981 ) model for proteins. 
 Equation (2)  models [hereafter, eq2 models] only capture the impact of selection (hereafter,  V ,  P ,  C  and  A  are called selective parameters). Mutational input was modeled by incorporating the structure of the genetic code, using ‘gencode’ ( G ) and transversion ( T ) parameters. The full model is shown as
 (3) r i j = e x p - V Δ i j V e x p - P Δ i j P … N i j - G e x p - T Δ i j T N ij  is the minimum number of nucleotide substitutions necessary for an interchange of amino acids  i  and  j  and  Δ i j T  is one if at least one of those substitutions is a transversion and zero otherwise. Gencode is dealt with in a different way than the other parameters, but it has the same behavior as the other parameters (i.e.  G  =   0 means the number of substitutions necessary for the interchange does not have an impact on the model). There are 64 potential eq3 sub-models. However, the 16 sub-models where  T  is free to vary but  G  =   0 will penalize a single transversion more than simultaneous changes to multiple nucleotide so they were not considered. Thus, 48 eq3 models (16 of which are eq2 models) were examined. The models are named based on the free parameters. Although the eq2 and eq3 models can reveal the amino acid properties that contribute the most to the patterns of evolution for a specific protein they cannot be used to examine the ways that available empirical models fail to capture those processes. However, it is possible to modify  Equation (2)  to include information from an empirical model:
 (4)   r i j = K i j E M P e x p - V Δ i j V e x p - P Δ i j P … N i j - G e x p - T Δ i j T 
where  K i j E M P is the relevant  R  matrix element from an empirical model (e.g. those listed in  Table 1 ). The eq4 models can be used to establish which properties an empirical model fails to capture for specific protein. These parameters were optimized using a perl program that calls IQ-TREE v. 1.5.5 ( Nguyen  et al. , 2015 ) to perform the likelihood calculations. Briefly, IQ-TREE was called and used to optimize the amino acid frequency parameters and Γ-distribution shape parameter (α); this study only considered +F + Γ models. Then α and the amino acid frequencies were fixed and the eq3 or eq4 model parameters ( V ,  P ,  C ,  A ,  G  and  T ) were optimized. A simple one-dimensional optimization was performed for each parameter in succession. The optimization began by determining whether adding or subtracting a fixed value (δ) to the focal parameter improves the likelihood. If  φ +δ or  φ −δ had a higher likelihood than the starting  φ  value, then δ was added (or subtracted) until the likelihood was maximized. After optimizing all free parameters δ was reduced and another round of optimization was conducted. After δ value reached a minimum (0.00001), the α and amino acid frequencies were re-optimized using the  R  matrix generated using the estimated parameter values. This procedure was repeated until the likelihood failed to change any further. The best-fitting model was identified using the corrected Akaike information criterion (AIC c ;  Hurvich and Tsai, 1989 ), using the number of aligned sites in the protein MSA as the sample size. Empirical models were identified in IQ-TREE using the settings ‘-m TESTONLY -mfreq FO -mrate G -merit AICc’; this finds the best fitting model from a set of 18 candidate models (all models in  Table 1  and eight additional specialized models). The eq4 models used  K i j E M P from the best-fitting empirical model identified using IQ-TREE. 3 Results and discussion This study had the following four major goals: (i) to establish which parameters are necessary to fit eq3 models to protein MSAs; (ii) to determine whether the eq3 parameter estimates differ among proteins; (iii) to compare the fit of eq3 models to empirical models; and (iv) to examine whether the fit of empirical models can be improved using the eq4 models. To accomplish these goals, we examined proteins from yeasts ( Rokas and Carroll, 2005 ), vertebrates ( Chen  et al. , 2015 ) and birds ( Jarvis  et al. , 2014 ). The specific proteins were chosen arbitrarily and only the MSAs judged free of homology errors by  Springer and Gatesy (2018 ) were chosen from birds. Individual gene trees can differ from the species tree ( Maddison, 1997 ) and the true species tree is unknown (it is especially uncertain for birds;  Reddy,  et al. , 2017 ), so we optimized the model parameters on the ML tree generated using the best-fitting empirical model. To complement the analyses of individual genes I used eight concatenated datasets from  Wolf  et al.  (2004) . Each  Wolf  et al.  (2004)  dataset was limited to proteins with a specific function, so the potential of the eq3 and eq4 models to highlight differences among classes of proteins could be assessed. All datasets and trees are available in  Supplementary File S2 . 3.1 The most important parameters vary among proteins All single-parameter eq3 models resulted in substantial likelihood increases relative to the F81-like model. Polarity ( P ) was the selective parameter that increased per site Δ ln L the most; the median Δ ln L/site for eq3 models with  P  as the only free parameter increased by 0.8359 for vertebrates, 0.5845 for yeasts and 0.2788 for birds. The estimate of the  P  parameter was also larger on average than the other selective parameters ( Table 2 ). The least important selective parameters based on those criteria were composition ( C ) for the vertebrates and yeasts and aromaticity ( A ) for birds. Gencode ( G ), the primary mutational input parameter, was very important; the median Δ ln L/site increased by 1.054 for vertebrates, 0.5102 for yeasts and 0.3352 for birds. Estimates of  G  were especially high in birds ( Table 2 ). Indeed, adding the  G  parameter resulted in a larger likelihood increase than any other parameter for birds and vertebrates and the second largest (after  P ) for the yeasts.
 Table 2. Parameter estimates for single parameter eq3 models Dataset Sites V P C A G G + T Yeasts:  Flc2p 494 4.86 4.87 3.33 2.73 3.25 3.05/0.65  Ptc1p 217 4.43 5.72 2.50 2.85 3.61 3.53/0.27  Rfc2p 296 4.69 4.79 3.22 3.71 3.07 2.90/0.54  Ung1p 185 3.71 4.39 1.99 3.22 2.08 1.89/0.48  Tkl1p 629 4.69 4.33 2.64 3.08 2.50 2.42/0.26 Mean 4.48 4.82 2.74 3.12 2.90 2.76/0.44 Birds:  APC (54) 2862 3.51 4.88 2.57 3.76 7.14 6.56/1.03  GFPT1 (15) 700 2.60 4.21 3.43 2.07 3.88 3.70/0.42  HMBS (76) 353 3.54 4.16 3.12 1.53 5.52 5.25/1.09  IFGN1 (78) 845 3.34 4.75 3.12 1.53 7.07 7.21/1.06  PCNX (79) 2359 3.31 4.31 2.52 3.01 5.33 4.89/0.96 Mean 3.26 4.46 3.05 2.55 5.79 5.52/0.91 Vertebrates:  AQR 1020 4.07 4.43 2.94 3.64 4.72 4.59/0.73  COX10 490 3.13 4.06 2.57 4.15 4.70 4.54/0.69  EDC4 761 4.22 5.43 2.80 4.02 4.52 4.43/0.58  GPATCH1 515 3.52 4.74 3.01 3.92 4.12 3.97/0.67  VPS54 347 3.94 4.91 2.40 4.37 4.52 4.24/0.77 Mean 3.78 4.71 2.74 4.02 4.52 4.36/0.69 Parameter estimates are rounded to the nearest 0.01. Estimates of the  T  parameter were only obtained in combination with  G ; those parameter estimates are listed in the order  G / T . Bird gene numbers are from the study by  Jarvis  et al . (2015) . Complete output of the parameter optimization program is available in  Supplementary File S3 . Single-parameter eq3 models provide information analogous the commonly used  K R / K C  and  K A / K S  ratios. Unlike the  K A / K S  ratio (ω), there is no obvious expected value of the  K R / K C  ratio. Assuming synonymous sites evolve at the neutral rate (which may not be true;  Chamary  et al. , 2006 ;  Lawrie  et al. , 2013 )  K A / K S  = 1 provides evidence of neutral evolution. In contrast,  K R / K C  only allows the exploration of differences among proteins (or lineages). The single parameter eq3 models provide similar information while eschewing a simplistic radical versus conservative classification of interchanges. The number of free parameters in the best-fitting eq3 models for each of the 15 test datasets ranged from two to six ( Table 3  and  Supplementary File S3 ); the more parameter-rich (i.e. five- or six-parameter) models had the best fit for most datasets. However, the same patterns revealed in single-parameter eq3 analyses were also evident in the best-fitting models.  C  had the least impact on the likelihood in the single parameter analyses and  C  was not included the best-fitting model for 10 of the 15 proteins. In fact,  C  was not included in the best-fitting model for any yeast protein.  P  and  G  had the largest impact on the likelihood in single-parameter analyses and both of those parameters were included in the best-fitting models for the test datasets. However, the parameter estimates obtained using more parameter-rich models tended to be much lower than those obtained using the single-parameter models. This appeared to reflect interactions among the parameters.  C  presented an interesting case since it was negative for the vertebrate VPS54 dataset. This can happen when certain amino acid interchanges that might be viewed as radical based on composition alone were actually overly penalized by the other parameters (i.e. their instantaneous rate is too low).
 Table 3. Parameter estimates and Δ ln L/site for the best-fitting eq3 models Dataset V P C A G T Δ ln L Best  EMP Yeasts:  Flc2p 2.13 3.24 — 1.97 1.59 0.63 0.9621 LG (−0.2062)  Ptc1p 1.91 4.08 — 1.47 2.48 — 0.8091 LG (−0.1132)  Rfc2p 2.27 3.63 — 2.55 1.50 0.56 0.8790 LG (−0.1692)  Ung1p 1.85 3.51 — 2.66 0.72 0.35 0.7615 rtREV (−0.1217)  Tkl1p 2.65 2.91 — 1.71 1.08 0.24 0.6869 LG (−0.2438) Mean 2.16 3.48 0.00 2.07 1.47 0.35 Birds:  APC — 2.66 0.55 2.72 5.95 0.89 0.7518 HIVb (−0.0002)  GFPT1 — 2.91 — — 3.36 — 0.0862 JTT (−0.0183)  HMBS 1.71 2.00 — — 4.71 0.93 0.7319 JTT (−0.0006)  IFGN1 0.55 2.40 0.55 1.53 6.46 0.83 2.9576 HIVb (−0.0667)  PCNX 0.91 2.08 0.66 1.78 4.14 0.86 0.2512 HIVb (−0.0039) Mean 0.63 2.41 0.35 1.21 4.92 0.70 Vertebrates:  AQR 1.33 2.56 — 2.54 3.67 0.57 0.8500 JTT (−0.1015)  COX10 — 2.48 — 3.04 3.73 0.53 1.9200 JTT (−0.1689)  EDC4 0.85 3.32 — 3.13 3.36 0.57 1.8728 JTT (−0.1061)  GPATCH1 0.76 2.55 0.75 2.45 3.12 0.48 2.2052 JTT (−0.2815)  VPS54 0.88 3.15 −0.89 3.13 3.46 0.72 1.2291 JTT (−0.0804) Mean 0.76 2.81 −0.03 2.86 3.47 0.57 Note : ‘—’ indicates parameters that were not in the best-fitting eq3 model (based on the AIC c ). Any parameters absent from the best-fitting model were assumed to be zero when the mean was calculated. Parameter estimates are rounded to the nearest 0.01. Δ ln L is the likelihood difference per site (Δ ln L/site) relative to the F81-like model. Δ ln L/site is rounded to the nearest 0.0001. The best-fitting empirical model (‘Best  EMP ’) is followed by the Δ ln L/site relative to the best-fitting eq3 model. Complete output of the parameter optimization program is available in  Supplementary File S3 . The best-fitting empirical models differed among proteins in the test datasets. LG had the best fit for most of the yeast proteins (the exception had the best fit to rtREV), whereas JTT was the best-fitting model for all of the vertebrate proteins. The avian proteins were split between HIVb (three proteins) and JTT (two proteins). The likelihood of the best-fitting empirical model was higher than the best-fitting eq3 model in all cases, although the best eq3 model had a likelihood that of the best empirical model for two avian proteins. In fact, adding the  V  parameter to analyses of APC actually resulted in a slightly higher likelihood than that of the best-fitting empirical model ( ln L = −23308.4891 for the full VPCAGT model and  ln L = −23308.9690 for the HIVb model; Δ ln L = 0.4799). However, the estimate of  V  was quite low ( V  = 0.23) when the VPCAGT model was used to analyze APC; that is why the  V  parameter was not included in the best-fitting eq3 model for that protein. The  V  parameter was also absent from the best-fitting eq3 model for one other avian protein and one vertebrate protein. Regardless, eq3 parameter estimates for proteins with the same best-fitting empirical model were often very different. This suggests that eq3 models can reveal patterns of evolution for different proteins; simply identifying the best-fitting empirical model cannot reveal that information. 3.2 The fit of empirical models can be improved Although the six-parameter models result in substantial likelihood improvements relative to a simple F81-like model, they did not fit the data for any protein MSA and the best-fitting empirical model (with the exception of APC). The raises two questions. First, can the fit of empirical models be improved? Second, which aspects of the evolutionary process do empirical models fail to capture? Using eq4 to adjust the best-fitting empirical model resulted in improvements (based on the AIC c ) in all but one case (GFPT1;  Table 4 ). Parameter estimates for the eq4 models were much lower than those obtained using eq3 models (compare  Tables 3  and  4 ); this was expected since the ‘starting point’ for the models (i.e. the empirical model) was presumably much better than the F81-like model. However, the parameters that played a role in best-fitting eq4 model differed among proteins, emphasizing the fact that the ‘one size fits all’ nature of empirical models is inappropriate.
 Table 4. Parameter estimates for eq4 models using the best-fitting empirical model Dataset V P C A G T Best  EMP Yeasts:  Flc2p — — — — 0.60 — LG (0.0098)  Ptc1p — 1.03 — — 1.00 −0.44 LG (0.0550)  Rfc2p 1.34 — — — — — LG (0.0164)  Ung1p — 1.06 −1.06 1.13 — — rtREV (0.0464)  Tkl1p 0.92 — — — — — LG (0.0086) Mean 0.45 0.42 −0.21 0.23 0.32 −0.09 Birds:  APC −0.26 0.97 −0.63 1.38 1.55 0.25 HIVb (0.0251)  GFPT1 — — — — — — JTT (—)  HMBS — — — — 2.57 0.56 JTT (0.0834)  IFGN1 0.35 0.40 — −0.28 2.76 — HIVb (0.0381)  PCNX — 0.45 — 0.80 — — HIVb (0.0027) Mean 0.02 0.36 −0.13 0.38 1.18 0.16 Vertebrates:  AQR 0.56 — — 0.85 1.47 — JTT (0.0452)  COX10 −0.56 — — 1.11 1.56 — JTT (0.0961)  EDC4 — 1.01 — 1.71 1.11 0.20 JTT (0.1488)  GPATCH1 — — 0.63 0.52 0.92 — JTT (0.0605)  VPS54 — 0.86 −1.22 1.54 1.16 0.30 JTT (0.0858) Mean 0.00 0.38 −0.12 1.15 1.25 0.10 LG (−0.2062) Note:  ‘—’ indicates parameters that were not in the best-fitting (based on the AIC c ) eq4 model. In all cases, the best-fitting empirical model was used as the ‘base model’ that provided the  K ij  values in eq3. Any parameters not present in the best-fitting model were assumed to be zero for calculating the mean. Parameter estimates are rounded to the nearest 0.01. The best-fitting empirical model is followed by the Δ ln L per site relative to that model (‘—’ indicates the empirical model was not improved using eq4). Complete output of the parameter optimization program is available in  Supplementary File S3 . 3.3 Parameter estimates for concatenated datasets of functionally related proteins Empirical models obtained ultimately correspond to fixed  R  matrix values estimated using large training sets. From a conceptual standpoint, the simplest way to estimate those parameters would be optimize the GTR 20  model given a diverse set of functionally unrelated proteins. In practice, many empirical models used less computationally demanding approximate methods for parameter estimation. However, the general point is that  R  matrix values for empirical models should be close to the average GTR 20  model parameters for many different proteins. Thus, one might expect parameter estimates for concatenated datasets to converge on some average value that is as close as possible to the empirical model parameters. However, this might not be true for concatenated datasets that comprise functionally related proteins. Most concatenated datasets used in phylogenomics (e.g.  Chen  et al. , 2015 ;  Jarvis  et al. , 2014 ;  Rokas and Carroll, 2005 ) comprise diverse and functionally-unrelated proteins. An early phylogenomic study ( Wolf  et al. , 2004 ) represents an exception to this; that study analyzed eight separate six-taxon concatenated datasets, each of which comprises functionally related proteins. The six focal taxa for  Wolf  et al.  (2004)  are three animals (a vertebrate, an insect and a nematode), two fungi (fission yeast and budding yeast) and a plant. There are two plausible trees for those taxa: (i) Ecdysozoa (an insect + nematode clade) and (ii) Coelomata (an insect + vertebrate clade); these trees are available in  Supplementary File S2 .  Wolf  et al . (2004)  supported Coelomata but later phylogenomic studies with larger taxon samples have strongly supported Ecdysozoa (e.g.  Dunn  et al. , 2008 ;  Hejnol  et al. , 2009 ). Thus,  Wolf  et al.  (2004)  data provide an opportunity to ask two questions. First, do parameter estimates for functionally related sets of proteins differ, like those for individual proteins? Second, do analyses using the proposed models support the Ecdysozoa tree? To do this the likelihood given the best-fitting models (empirical, eq3, and eq4) was calculated using both plausible topologies (Ecdysozoa and Coelomata). The eq3 parameter estimates for the concatenated datasets did show variation, albeit less than for individual proteins (compare  Tables 3  and  5 ). The  C  and  A  parameter estimates were especially variable. All  Wolf  et al.  (2004)  datasets had the same best-fitting eq3 model (VPCAGT) and empirical model (LG). LG always had a better fit than the VPCAGT model. However, it was always possible to improve model fit relative to the LG model using eq4 ( Table 5 ). The  V  and  G  parameters were included in every eq4 model, although the estimates of  G  were always negative, suggesting LG overcorrects for the impact of the genetic code on these data.
 Table 5. Parameter estimates for concatenated datasets Dataset Sites V P C A G T Eq3 models:  Chaperonins 3970 2.68 3.87 −0.20 1.86 0.74 0.19  Clathrin 2138 2.11 3.62 −0.25 3.06 0.68 0.39  DNA polymerase 1782 2.19 2.99 0.53 2.15 1.03 0.16  DNA replication 2284 2.36 3.10 0.47 2.08 0.98 0.15  Proteasome 2474 2.43 3.18 0.21 2.44 0.76 0.18  Ribosomal proteins 11 586 2.23 2.92 0.29 2.28 0.62 0.10  RNA polymerase 3274 2.26 2.97 0.29 1.96 0.86 0.27  Translation factors 2045 2.13 3.21 0.32 2.36 0.80 0.21 Mean 2.30 3.23 0.21 2.27 0.81 0.21 Eq4 models:  Chaperonins 3970 1.10 0.88 −0.55 — −0.44 −0.18  Clathrin 2138 0.49 0.74 −0.65 1.11 −0.54 —  DNA polymerase 1782 0.68 — — 0.24 −0.24 −0.21  DNA replication 2284 0.89 — — — −0.21 −0.21  Proteasome 2474 0.84 0.27 — 0.51 −0.47 −0.19  Ribosomal proteins 11 586 0.73 −0.23 — 0.45 −0.34 −0.55  RNA polymerase 3274 0.54 — — 0.25 −0.31 −0.11  Translation factors 2045 0.44 — — 0.42 −0.33 −0.20 Mean 0.71 0.21 −0.15 0.37 −0.38 −0.18 All parameter estimates reflect the Ecdysozoa tree. ‘—’ indicates parameters that were not in the best-fitting eq4 model (based on the AIC c ). Any parameters not present in the best-fitting model were assumed to be zero for calculating the mean. Parameter estimates are rounded to the nearest 0.01. The empirical model used for the eq4 models was always LG. ‘DNA replication’ refers to DNA replication licensing factors, i.e. the MCM family. Complete output of the parameter optimization program is available in  Supplementary File S4 . Parameter estimates in  Table 5  were calculated using the Ecdysozoa topology but analyses using the Coelomata topology resulted in similar values ( Supplementary File S4 ).  Wolf  et al.  (2004)  found that different datasets supported different trees, with three (proteasome subunits, ribosomal proteins and RNA polymerase) supporting Ecdysozoa and the other five datasets supporting Coelomata. Analyses using LG, eq3 and eq4 also revealed conflict; four datasets (chaperonins and the same datasets as  Wolf  et al. , 2004 ) supported Ecdysozoa and the other four supported Coelomata. Although that result was equivocal, the Ecdysozoa tree had the highest overall likelihood in all analyses, consistent with the results of studies with more taxa (e.g.  Dunn  et al. , 2008 ;  Hejnol  et al. , 2009 ). The overall likelihood is the sum of the likelihoods of for eight of the concatenated MSAs given a specific tree; this is the likelihood given a model where each of the eight MSAs has distinct parameters and distinct branch lengths. Surprisingly, the likelihood difference (Δ ln L) favoring Ecdysozoa was actually larger for eq3 than for eq4 (Δ ln L = 44.9233 for eq3; Δ ln L = 33.5569 for eq4), despite the better fit (based on AIC c ) of the eq4 models. However, the relationship between model and topology was complex; the overall Δ ln L favoring Ecdysozoa was smallest for LG (Δ ln L = 29.3364) and largest for the F81-like model (Δ ln L = 60.1499). Moreover, one additional dataset (clathrin) supported Ecdysozoa when the F81-like model was used (see Supporting File 4 for details). Despite these complexities the fact that the eq4 models resulted in a modest increase in the likelihood difference relative to LG should be viewed as encouraging. Broader surveys will be necessary to explore the potential of these models for estimating phylogenetic tree topologies. 3.4 General patterns and variation among proteins A general framework for models of protein evolution that can be used to explore general patterns of protein evolution and variation among proteins was proposed (eq1). Specific versions of that general model that focused amino acid physicochemical properties (eq2, eq3 and eq4) emphasized the important roles of side chain volume ( V ), polarity ( P ), aromaticity ( A ) and the structure of the genetic code ( G ) in determining relative rates of amino acid interchanges. The role of polarity, volume, and the genetic code in determining rates of amino acid interchanges has long been appreciated, but a role for aromaticity independent of volume might be viewed as surprising since aromaticity and volume are correlated (Pearson’s  r  = 0.716). Composition ( C ) had less impact on protein evolution. That could reflect way composition is calculated; composition has a modest correlation with the other parameters (the maximum is with aromaticity;  r  = -0.474) if all amino acids are considered, but it is strongly correlated with polarity ( r  = 0.809) if cysteine is excluded (the composition-polarity correlation is  r  = 0.37 when cysteine is included). Thus,  C  could reflect two distinct aspects of protein evolution (polarity and the special nature of cysteine). This may explain why negative estimates of the  C  parameter emerged in some analyses using eq3 ( Tables 3  and  5 ). It also suggests that it may be desirable to abandon the  C  parameter in favor of other properties. Regardless of the details of the amino acid properties used for analyses, it is clear that the patterns of evolution vary among proteins in ways that cannot be examined using empirical models. The models proposed here can reveal that variation and highlight the best amino acid properties to examine in future studies. The goal of this study was to develop an amino acid model that could reveal the ways that patterns of molecular evolution vary among proteins. The eq3 models include six parameters, four of which reflect selection against radical amino acid substitutions. This could make the eq3 models testable in a way that empirical models (e.g.,  Table 1 ) are not. For example, if analyses of a specific protein using eq3 results in a high estimate of  V  that protein is likely to be more sensitive to volume changing substitutions than another protein associated with a lower estimate of  V . Thus, eq3 could be tested by mutagenesis experiments (e.g. using methods similar to the work by  Georgelis  et al. , 2007 ). The fact that  K R / K C  and effective population size appear to be negatively correlated ( Hughes and Friedman, 2009 ;  Weber  et al. , 2014 ) could permit another test of eq3. The correlation probably reflects the higher efficiency of selection in organisms with large population sizes ( Akashi  et al. , 2012 ). Since the eq3 model parameters are analogous to  K R / K C  they should exhibit the same correlation. However, eq3 also of highlights the properties of amino acids that contribute to the radical versus conservative nature of substitutions in different proteins. Overall, eq3 provides a novel tool to explore the differences among proteins. Eq4 models provide different information than the eq3 models. Specifically, eq4 reveals the ways that empirical models fail to capture specific patterns of amino acid substitution. The results shown in  Tables 4  and  5  reflect the use of eq4 with the best-fitting empirical model; they do not show the degree to which other empirical models might be improved. Testing the full set of empirical models in  Table 1  revealed three cases (APC, IFGN1 and PCNX) where eq4 with a suboptimal ‘base model’ performed better than eq4 with the best-fitting empirical model ( Supplementary File S5 ). In all three cases, combining eq4 and the JTT model resulted in a better likelihood than eq4 with the HIVb model; the estimate of the  G  parameter was much larger for the JTT + VPCAGT model than for the HIVb + VPCAGT model for all three proteins ( Supplementary File S5 ). The parameter estimates that can be obtained using the eq4 models ( Supplementary Files S5 and S6 ) provide an interesting way to examine the ways each empirical model fails to capture the patterns of amino acid substitution for individual proteins. This study did not address variation among sites in patterns of sequence evolution or the impact of these models on the estimation of tree topology. Many studies have revealed variation among sites within proteins in their evolutionary rate ( Echave  et al. , 2016 ); substantial variation in the pattern of evolution is also likely to exist. These models do not address that variation (except to the extent that the Γ distribution captures variation in rates for all models examined here). However, it would be straightforward to extend eq3 or eq4 to a mixture model where one or more of the parameters are drawn from a prior distribution (e.g. a Γ distribution or a uniform distribution); the mean and variance of that distribution could be estimated by ML. Likewise, the potential for the proposed models to improve tree topology estimation is unclear, although the fact that eq4 improves the fit of empirical models makes it reasonable to speculate that it could be useful. However, other analytical approaches should also be considered in studies focused on tree estimation (e.g. site heterogeneous CAT models;  Lartillot and Philippe, 2004 ;  Le,  et al. , 2008 ). However, information analogous to the  K R / K C  ratio cannot be obtained from analyses using standard empirical models (or the CAT models). Ultimately, the value of the proposed models is their potential to reveal differences among proteins in their patterns of evolution and to identify the characteristics of amino acids that contribute to protein evolution. Supplementary Material Supplementary Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Multifactor dimensionality reduction for graphics processing units enables genome-wide testing of epistasis in sporadic ALS</Title>
    <Doi>10.1093/bioinformatics/btq009</Doi>
    <Authors>Greene Casey S., Sinnott-Armstrong Nicholas A., Himmelstein Daniel S., Park Paul J., Moore Jason H., Harris Brent T.</Authors>
    <Abstract>Motivation: Epistasis, the presence of gene–gene interactions, has been hypothesized to be at the root of many common human diseases, but current genome-wide association studies largely ignore its role. Multifactor dimensionality reduction (MDR) is a powerful model-free method for detecting epistatic relationships between genes, but computational costs have made its application to genome-wide data difficult. Graphics processing units (GPUs), the hardware responsible for rendering computer games, are powerful parallel processors. Using GPUs to run MDR on a genome-wide dataset allows for statistically rigorous testing of epistasis.</Abstract>
    <Body>1 INTRODUCTION Genome-wide association studies hold promise for the discovery of the genetic factors that underlie common human diseases (Hirschhorn and Daly,  2005 ; Wang  et al. ,  2005 ). Unfortunately this promise has largely not been realized (Shriner  et al. ,  2007 ; Williams  et al. ,  2007 ). It is thought that this failure could be due to epistasis, the role of gene–gene interactions, which has commonly been ignored in these studies. Powerful and model-free methods such as multifactor dimensionality reduction (MDR) have been developed (Ritchie  et al. ,  2001 ), but an exhaustive examination of even pair-wise interactions in a 550 000 SNP dataset would require the analysis of 1.5 × 10 11  combinations. While an analysis of this scale is approachable with modern cluster computing, an analysis that includes permutation testing to assess the statistical significance of results remains infeasible with CPU-based approaches. Rendering photo-realistic video games in real time is also computationally difficult. For video game graphics, specific hardware (the graphics processing unit or GPU) has been developed. The GPU is a massively parallel computing platform that can be adapted to some scientific tasks. We have previously shown that MDR is one of these tasks (Sinnott-Armstrong  et al. ,  2009 ). Here we provide software which makes practical the analysis of epistasis in genome-wide data through the use of GPUs and demonstrate its application to a genome-wide analysis of epistasis of sporadic amyotrophic lateral sclerosis (ALS). 2 METHODS MDRGPU, a software tool capable of analyzing genome-wide data, is a Python implementation of MDR, which uses the PyCUDA library to run MDR on GPUs. MDRGPU 1.0 supports balanced accuracy, large datasets, execution across an arbitrary number of GPUs, permutation testing and the analysis of high-order interactions. It runs on GPUs which support CUDA (i.e. the NVIDIA GeForce 8800 series and higher). Parallel execution of one realization across multiple GPUs is supported with the pp library for Python. MDRGPU provides a command-line interface for scripted analysis. The GPU architecture has various memory spaces available. MDRGPU uses the constant cache, global memory, shared memory and registers. Shared memory is used to store the intermediate case and control counts for each attribute combination and to store the number of true and false positives and negatives. The global memory is accessed directly to fetch attributes. The constant cache is used in MDRGPU to store the case–control status. Dataset sizes of greater than 65 536 attributes require splitting which is handled seamlessly by MDRGPU. This splitting does not cause linear slowdown; there is simply more overhead of launching, so datasets with large numbers of instances see less of a performance reduction than datasets with few instances. The largest number of addressable attributes is 4 billion requiring 4 GB RAM per instance. In order for the case–control status to be held in constant memory, there can be at most 16 384 instances. Our proof of concept analysis was performed on three GPU workstations (detailed in  Supplementary Material S1 ). These systems contain three GeForce 295 cards, each of which contains two GPUs. For the first stage of this analysis, we used an ALS dataset from Schymick  et al.  ( 2007 ) as our detection dataset. This dataset was obtained from QUEUE at Coriell, but has since been moved to dbGaP. It contains 276 individuals with sporadic ALS and 271 control individuals. These individuals are genotyped at 555 352 SNPs using the Illumina Infinium II HumanHap550 SNP chip. We processed this dataset by removing SNPs with a minor allele frequency &lt;0.2 or those in which &gt;10% of values were missing for either cases or controls. We further used Haploview 's tagSNP algorithm (Barrett  et al. ,  2005 ) to select representative SNPs from groups of correlated SNPs ( r  &gt; 0.8). After this, 210 382 SNPs remained and were used in the analysis. For the replication stage, we used a dataset of Irish individuals containing of 221 sporadic ALS patients and 211 controls described in Cronin  et al.  ( 2008 ). We used MDRGPU to perform a two-way analysis across the entire detection dataset. We selected the SNP combination with the best balanced accuracy measure. We then permuted the dataset 1000 times while repeating this analysis. We measured the accuracy of the best pair in each permuted dataset. We then used the 50th best accuracy obtained from these permuted datasets as our significance cutoff. This permutation test yields an experiment-wise α of 0.05. A pair of SNPs with a significant association in the detection phase was tested in the replication dataset. In this phase, the two detected SNPs were selected from the dataset and MDR was used to evaluate only this pair. A permutation test was performed here using MDR on only these two SNPs, and an α of 0.05 was used to assess significance. 3 RESULTS Our three GPU systems completed an analysis of pairwise interactions in a single permutation approximately every 6 min. The time to analyze the dataset itself for pairwise interactions is the same as the time required for one permutation. One thousand permutations were used to assess statistical significance which required ∼100 h. The time to analyze the same dataset on a cluster with 200 AMD Opteron 2384 (2.7 GHz) CPU cores was just over 1 h without permutation testing and thus a CPU-based permutation test was considered infeasible as the estimated time required on 200 CPU cores was &gt;40 days. In the proof-of-concept analysis, the highest accuracy combination in our dataset was SNPs rs4363506 and rs6014848 with a balanced accuracy of 0.6551. In our permutation test, this accuracy was statistically significant ( P  &lt; 0.048). In the replication dataset this pair had a balanced accuracy of 0.5821. Permutation testing the replication dataset showed that this result was also statistically significant ( P  &lt; 0.021). Therefore, not only have we discovered a statistically significant pair of SNPs using an experiment-wise α of 0.05, but we have replicated the significant relationship in an independent dataset. Here is evidence of how the permutation testing allowed by MDRGPU enables the discovery of combinations of SNPs that are significantly associated with a disease. 4 DISCUSSION While SNP rs4363506 has been reported as associated with disease in Schymick  et al.  ( 2007 ), it did not have a statistically significant effect in Cronin  et al.  ( 2008 ) when considered alone (χ 2 ,  P  = 0.18) and would have failed to replicate without considering pairwise effects. SNP rs6014848 has not previously been described as associated with sporadic ALS, although it shows main effects (uncorrected χ 2 ,  P  &lt; 0.05) in both datasets. Greene  et al.  ( 2009 ) have shown that SNPs can fail to replicate a significant association when the joint effect of those SNPs is ignored. This is particularly likely when the populations from which patients are ascertained differs. Schymick  et al.  ( 2007 ) collected individuals from the USA, while Cronin  et al.  ( 2008 ) collected individuals from Ireland. By considering the joint effect of SNPs, MDRGPU discovers a novel association which replicates in an independent dataset. GPUs provide a platform for epistasis analysis in genome-wide data where computational requirements far exceed what CPUs can cost-effectively provide. MDRGPU is a software package for this emerging computing platform that enables human geneticists to tackle analyses previously found to be intractable. 
          Supplementary Material 
          
             
                [Supplementary Data] 
             
             
             
          
       </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Large scale microbiome profiling in the cloud</Title>
    <Doi>10.1093/bioinformatics/btz356</Doi>
    <Authors>Valdes Camilo, Stebliankin Vitalii, Narasimhan Giri</Authors>
    <Abstract/>
    <Body>1 Introduction and background Microbes are ubiquitous and a microbiome is a collection of microbes that inhabit a particular environmental niche such as the human body, earth soil and the water in oceans and lakes. Metagenomics is the study of the combined genetic material found in microbiome samples, and it serves as an instrument for studying microbial biodiversities and their relationships to humans. Profiling a microbiome is a critical task that tells us what microorganisms are present, and in what proportions; this is particularly important as many human diseases are linked to changes in human microbiome composition ( Haiser  et al. , 2013 ;  Koeth  et al. , 2013 ;  Wu and Lewis, 2013 ;  Zhang  et al. , 2015 ), and large research projects have started to investigate the relationships between the two ( The Integrative HMP iHMP Research Network Consortium, 2014 ). A powerful tool for profiling microbiomes is high-throughput DNA sequencing ( Metzker, 2010 ), and whole metagenome sequencing experiments generate data that give us a lens through which we can study and profile microbiomes at a higher resolution than 16S amplicon-based sequencing analyses ( Ranjan  et al. , 2016 ). Advances in sequencing technologies have steadily reduced the cost of sequencing and have led to an ever increasing number of extremely large and complex metagenomic  datasets (Ansorge, 2009 ;  Caporaso  et al. , 2012 ). The resulting computational challenge is the production of even larger intermediate results, and need for large indexes of the reference genome collections ( Vernikos  et al. , 2015 ), making it impossible to process on commodity workstations or laptops. Powerful multi-user servers and clusters are an option, but the cost of higher processor speeds, greater storage volumes and huge memory sizes are out of reach for small laboratories. To deal with the barrage of sequencing data, distributed cloud computing platforms and frameworks such as Amazon Web Services ( Amazon.com Inc., Amazon Web Services, 2018 ), Apache Hadoop ( Apache Hadoop, 2018 ) and Apache Spark ( Apache Spark, 2018 ) have been used by researchers by taking advantage of parallel computation and economies of scale: large sequencing workloads are distributed in a cloud cluster that is comprised of many cheap, off-the-shelf compute nodes. These cloud-based solutions have been successfully used for human genomics ( Langmead  et al. , 2009a ), transcriptomics ( Roberts  et al. , 2013 ) and more recently for metagenomics applications ( Huang  et al. , 2018 ;  Zhou  et al. , 2017 ). Standard genomics and transcriptomics analyses for sequencing datasets usually begin by aligning sequencing reads to a reference genome ( Trapnell and Salzberg, 2009 ;  Wang  et al. , 2009 ), and producing abundance counts ( Trapnell  et al. , 2010 ); but in metagenomic analyses, the alignment step is performed against a collection of reference genomes that can be extremely large, slowing down the entire operation. The MapReduce model ( Dean and Ghemawat, 2008 ) along with the Spark framework have been popular in speeding up these crucial steps in the analysis of single-organism sequencing datasets, as researchers have framed the read-alignment and quantification tasks in terms of  map  and  reduce  operations: Langmead  et al.  used it to align human sequencing reads using the Bowtie read-mapping utility ( Langmead  et al. , 2009b ) and searching for single nucleotide polymorphisms (SNPs); while  Roberts  et al.  (2013 ) used it to speed up the quantification of human gene transcripts by the expectation-maximization (EM) algorithm. 2 Approach 2.1 Spark and MapReduce The MapReduce model was originally developed by Google ( Dean and Ghemawat, 2008 ), and most notably popularized by the  Apache Hadoop (2018 ) open-source project from the Apache foundation ( The Apache Software Foundation, 2018 ). The  Apache Spark (2018 ) project further expanded the Hadoop project, and introduced new optimizations for calculation speeds, and programming paradigms ( Zaharia  et al. , 2012 ). The MapReduce model abstracts away much of the boiler-plate programming details of developing distributable applications, and frees scientists and developers to focus their work on other critical, domain-specific, areas. The model is composed of two distinct steps: the  map()  step, and the  reduce()  step. Hadoop and Spark offer basic functions that can be used as the building blocks of a distributed computing model: the  map()  function takes as input a pair of parameters that make up a tuple consisting of a key and a value; while the  reduce()  function merges the output of the  map()  function by coalescing tuples with the same key. The MapReduce model, and the Spark framework in particular, have been employed in many DNA sequencing workflows for a number of years now ( Cattaneo  et al. , 2016 ;  Guo  et al. , 2018 ). The Crossbow project ( Langmead  et al. , 2009a ) from 2009 used Spark’s MapReduce implementation to identify Single Nucleotide Polymorphisms (SNPs) in human samples; eXpress-D ( Roberts  et al. , 2013 ) also used Spark to implement the expectation maximization (EM) algorithm for ambiguous DNA-fragment assignment. Spark has also been used in metagenomic analyses ( Guo  et al. , 2018 ) for mapping sequencing reads against small reference databases and for clustering metagenomes ( Rasheed and Rangwala, 2013 ). A natural approach to use the Spark framework for the analysis of mWGS datasets is to partition the input of reads into smaller subsets of reads to be processed by worker nodes in a Spark cluster. This strategy works well when the dataset of reads is large. The limitation of this strategy is that it does not scale to large collections of reference genomes because a data structure (index) of the reference collection of genomes must either be duplicated in each of the worker nodes, or multiple passes of the input can be used. Indexes built from large reference collections using a k-mer based strategy are often too large to be accommodated on a single commodity machine on the cloud ( Nasko  et al. , 2018 ). Fast k-mer based profiling strategies have been used for profiling of mWGS datasets ( Schaeffer  et al. , 2015 ;  Wood and Salzberg, 2014 ). But they trade-off speed for enormous indexes. More recently, alternative index-building strategies have been developed to allow the use of large collections of references with k-mer based tools, albeit only at species-level resolutions ( Zhou  et al. , 2018 ), but were not designed for use with a cloud-based infrastructure. Zhou  et al.  developed MetaSpark ( Zhou  et al. , 2017 ) to align metagenomic reads to reference genomes. The tool employs Spark’s Resilient Distributed Dataset (RDD) ( Zaharia  et al. , 2012 )—the main programming abstraction for working with large datasets—to cache reference genome and read information across worker nodes in the cluster. By using Spark’s RDD, MetaSpark is able to align more reads than previous tools. MetaSpark was developed with two reference datasets of bacterial genomes: a 0.6 GB reference, and the larger 1.3 GB from RefSeq’s bacterial repository. These reference sets are small compared to the 170 GB reference set of Ensembl, and because of MetaSpark’s use of an RDD to hold its index, it is unlikely that MetaSpark can scale to use them: the contents of an RDD are limited to available memory, and large reference sets would require correspondingly large memory allocations. It is worth pointing out the RDD memory limitations of MetaSpark in aligning reads: it took 201 min (3.35 h) to align 1 million reads to the small 0.6 GB reference using 10 nodes ( Zhou  et al. , 2017 ). SparkHit ( Huang  et al. , 2018 ) was developed by Huang  et al.  as a toolbox for scalable genomic analysis and also included the necessary optimizations for the preprocessing. SparkHit includes a metagenomic mapping utility called ‘SparkHit-recruiter’ that performs much faster than MetaSpark with similar sets of reference genomes. SparkHit performs well with large dataset of reads and small reference genome sets—the authors profiled 2.3 TB of whole genome sequencing reads against only 21 genomes in a little over an hour and a half. The limitation of SparkHit is that it builds its reference index using a k-mer strategy that does not scale to large collections of reference genomes ( Nasko  et al. , 2018 ), assuming that the reference database will change with each study that is analyzed. This assumption, and the method of index building, makes SparkHit unsuitable for profiling large metagenomic datasets against large collections of reference genomes. 2.2 Streaming techniques In order to process the large quantities of both input metagenomic datasets, and the large collections of reference genomes to profile against, new analysis paradigms are required that take advantage of highly parallelizable cloud infrastructure, as well as real-time data streams for consuming large input datasets. LiveKraken ( Tausch  et al. , 2018 ) was developed as a real-time classification tool that improves overall analysis times, and is based on the popular Kraken ( Wood and Salzberg, 2014 ) method for profiling metagenomic samples in Kraken-based workflows. LiveKraken uses the same approach as the HiLive ( Lindner  et al. , 2017 ) real-time mapper for Illumina reads, but extends it to metagenomic datasets. LiveKraken can ingest reads directly from the sequencing instrument in illumina’s binary basecall format (BCL) before the instrument’s run finishes, allowing real-time profiling of metagenomic datasets. Reads are consumed as they are produced at the instrument, and the metagenomic profile produced by LiveKraken is continuously updated. LiveKraken points the way to future classification systems that use streams of data as input, but its limitation is that it uses a k-mer based reference index—in its publication, LiveKraken was tested with an archived version of RefSeq (circa 2015) that only contained 2787 bacterial genomes. Since then, RefSeq has grown to over 50k genomes in the latest release (version 92), and creating a K-mer based index of it would require substantial computational resources. More recently, a Spark streaming-based aligner has been developed that uses streams of data to map reads single reference genomes. The tool, StreamAligner (Rathee and Kashyap, 2018), is implemented with Spark and the Spark-streaming API, and uses novel MapReduce-based techniques to align reads to the reference genome of a single organism. Unlike other methods, it creates its own reference genome index using suffix arrays in a distributed manner that reduces index-build times, and can then be stored in memory during an analysis run. By using the Spark streaming API, StreamAligner can continuously align reads to a single reference genome without the need of storing the input reads in local storage, and although StreamAligner has high performance when using a single genome, there is no evidence if it can scale to metagenomic workflows where tens of thousands of genomes are used, and the footprint of the reference genomes are much larger than could be fit in memory. 3 Materials and methods A natural approach to using MapReduce for large metagenomic analyses tasks is as follows. The  map  step divides the task of mapping the reads against a genomic index and the  reduce  step collects all the hits to each genome and constructs the microbial profile of the metagenomic sample. This approach works well when the same copy of the full genomic index can be farmed out to each node in the cluster. The approach fails when the index is too large to be provided to each cluster node or the collection of reads is too large for each cluster node. Streaming the reads allows for arbitrarily large collections of reads to be processed by each cluster node. Building an index of a ‘shard’ of the reference genome database and providing each cluster node with a smaller index allows for much larger reference databases to be used for mapping the reads ( Fig. 1 ).
 Fig. 1. Overview of the  Flint  System. Reference genomes are partitioned so that a large reference set is be distributed across a Spark cluster, and the number of partitions matches the number of worker nodes. Samples are streamed into the cluster to avoid storage overheads as shards of 250k reads. Reads are aligned to the distributed reference genomes using a double MapReduce pipeline that continually updates metagenomic profiles as samples are streamed into the cluster. Read alignments are never stored, and are processed by each worker node as soon as they are produced Our computational framework is primarily implemented using the  MapReduce  model ( Dean and Ghemawat, 2008 ), and deployed in a cluster launched using the  Elastic Map Reduce  (EMR) service offered by AWS (Amazon Web Services) ( Amazon.com Inc., Amazon Web Services, 2018 ). The cluster consists of multiple ‘commodity’ worker machines (a computational ‘worker’  node ), each with 15 GB of RAM, 8 vCPUs (each being a hyperthread of a single Intel Xeon core) and 100 GB of disk storage. Each of the worker computational nodes will work in parallel to align the input sequencing DNA reads to a ‘shard’ of the reference database ( Fig. 2 ); after the alignment step is completed, each worker node acts as a regular Spark executor node. By leveraging the work of multiple machines working at the same time,  Flint  is able to align a large number of reads to a large database of reference genomes in a much more efficient manner than that achieved by using a single powerful machine.
 Fig. 2. MapReduce workflow. Metagenomic samples can be streamed in from a distributed filesystem into the cluster were they are stored in an RDD. The first Map step generates alignments through Bowtie2 and feeds its resulting pairs to the first Reduce step, which aggregates the genomes that a single reads aligns to. The second Map step generates read contributions that are used in the second Reduce step to aggregate all the read contributions for a single genome. An output abundance matrix is generated which contains the abundances for each genome 3.1 Cluster provisioning A Spark ( Apache Spark, 2018 ) cluster was created using the AWS Console with the following software configuration: EMR-5.7.0, Hadoop 2.8.4, Ganglia 3.7.2, Hive 2.3.3, Hue 4.2.0, Spark 2.3.1 and Pig 0.17.0 in the US East (N. Virginia) region. The cluster is composed of homogeneous machines for both the driver node and worker nodes, and each machine is an Amazon machine instance of type c4.2xlarge. These instances contain 8 vCPUs, 15 GB of RAM, 100 GB of EBS storage and each cost on average $0.123 USD to run per hour on the ‘us-east’ availability zone on the Spot ( EC2 Spot Market, 2018 ) market as of this writing in January 2019. Newer instances (c5.2xlarge) are also available for use, but their availability is infrequent in large numbers, in addition to having a higher cost per hour to run. Resilient Distributed Datasets (RDD) ( Zaharia  et al. , 2012 ) are robust programming abstractions that can be used to persist data across a cluster of machines. We ingest reads from datastreams in batches of 500 000 reads that are processed by our mapreduce pipeline. Reads are consumed either directly from their location in an Amazon S3 bucket, or from a datastream source such as a Kafka or Kinesis source. An RDD of the input read stream is created in the master node that is then broadcasted out into all the worker nodes in the cluster. The input RDD of reads is partitioned into sets of reads that are each independently aligned to a reference genome partition in each of the worker nodes. 3.2 A ‘double’ MapReduce An obvious way to perform MapReduce for metagenomic analysis is to have the Map function produce tuples of the form  〈 g , 1 〉 , for every read  r  that is aligned to genomes  g , while the Reduce function aggregates all tuples of the form  〈 g , 1 〉  to obtain the abundance of genome  g  in the sample being analyzed, effectively generating output tuples of the form  〈 g , A ( g ) 〉 , where  A ( g )  is the reported abundance of genome  g  in the sample being analyzed. Unfortunately, a read may align to multiple genomes. Instead of counting a hit for every genome that the read aligns to, or counting it for only one of the genomes that the read aligns to, we follow the algorithm of  Valdes  et al.  (2015 ), which assigns fractional counts for the genomes that a read aligns to. In order to implement this, we employ a novel double MapReduce steps, thus making it a multi-stage operation. In the modified MapReduce, the Map function generates alignments in SAM format ( Li and 1000 Genome Project Data Processing Subgroup, 2009 ) by dispatching a subprocess of the Bowtie2 aligner and produces tuples of the form  〈 r , ( g , 1 ) 〉 , for every read  r  that is aligned to genomes  g . All tuples for the same read are aggregated by the first Reduce step to generate tuples of the form  〈 r , ( g , 1 / C ( r ) ) 〉 . The second Map step generates contributions of reads for a given genome, and the second Reduce step aggregates all tuples of the form  〈 g , c 〉  to obtain the abundance of genome  g  in the sample being analyzed, effectively generating output tuples of the form  〈 g , A ( g ) 〉 , where  A ( g )  is the reported abundance of genome  g  in the sample being analyzed obtained by aggregating all the fractional contributions of reads that map to that genome. Note that all intermediate tuples are stored in RDDs, one for each step. 3.3 Reference genome preparation Before we can use the bacterial genomes in the cluster, they need to be prepared. The process entails creating a Bowtie2 index for each shard of the reference database, and specific details on this procedure can be found in Section 2.1 of the supplementary manuscript. Briefly, the reference genomes are divided into smaller partitions that are each independently indexed by Bowtie2. The index preparation step can take considerable computational resources and time with a single machine. A parallel version of the indexing system can greatly improve performance and will be completed in the next release of  Flint . Once the partitions have been indexed they are then copied to an Amazon S3 (2018) bucket that serves as a staging location for the reference shards. The staging S3 bucket holds the index so that worker nodes can copy it during their provisioning step and the analysis can start; the S3 bucket is also public, and researchers can download copies of the prepared indices for their use. It should be noted that Ensembl’s bacterial genome collections have grown only modestly in the last couple of releases to minimize redundancy, and reference indices for new Ensembl releases can be built relatively quickly with utility scripts provided by  Flint . The cost of building a partitioned reference index is only accrued the first time it is built for a cluster of a particular size, and as part of the release of the  Flint  project, we are making available partitioned indices of Ensembl (v.41) of sizes 48, 64, 128, 256 and 512 which should be useful for researchers employing clusters of those sizes. These indices, along with the scripts necessary to build future versions, can be found at the GitHub repository. We currently use minimal annotations that keep track of basic attributes for each bacterial strain; these include taxonomic identifiers, assembly lengths, etc. Future releases of the software will include a more robust annotations package that will contain data on gram staining, pathogenicity and other properties. 
 Flint  uses a streaming model to quickly map a large number of reads to a large collection of reference bacterial genomes by using a distributed index. The Bowtie2 DNA aligner is used internally in Spark worker nodes to align reads to the local partition of the reference index, by using a MapReduce that continuously streams reads into worker nodes. Output alignments are parsed and tabulated by worker nodes, and then sent back to master node as alignment tasks finish.  Flint  can be deployed on any Spark cluster, as long as the necessary software dependencies are in place; the partitioned reference index for Ensembl’s 43k genomes is made available at the  Flint  website, and scripts are provided as part of the provisioning step that copy the partitions into worker nodes. 4 Results and discussion 4.1 Comparison to existing tools 
 Flint  was evaluated by comparing abundance profiles generated with  Flint  to those provided by HMP and those generated by Kraken ( Wood and Salzberg, 2014 ). Note that Kraken is a  k -mer based algorithm to align reads to genomic sequences and is known to be one of the most accurate ones ( McIntyre  et al. , 2017 ). We selected an anterior nares sample (SRS019067) with 528k reads from the Human Microbiome Project (HMP) and analyzed it with Kraken (2.0.7-beta) and  Flint  and compared the results to those provided by HMP in their community abundance profiles. HMP reported 36.7% aligned reads using a bacterial database of 1751 genomes, while Kraken was able to classify 36% of the reads using their RefSeq bacterial database of 14 506 genomes; in contrast,  Flint  was able to align 81% of the reads using Ensembl’s 43k bacterial genomes. The increase number of aligned reads is due to the larger number of genomes in Ensembl—Kraken uses RefSeq’s so-called ‘complete’ bacterial genomes, while Ensembl contains many draft genomes that increases the probability for mapping a read.  Flint  also aligns reads with Bowtie2 directly to the bacterial strain genomes, and does not apply lowest common ancestor (LCA) assignment to reads as Kraken does, which should mitigate any database diversity influences (genus, species and strain ratios) as noted by Nasko  et al.  ( Nasko  et al. , 2018 ). As shown in  Supplementary Figure S4 , both  Flint  and Kraken identify roughly the same set of genera, but at the species level,  Flint  identifies significantly more species. MetaSpark ( Zhou  et al. , 2017 ) and SparkHit ( Huang  et al. , 2018 ) are spark-based methods with a cluster infrastructure similar to  Flint  but their lack of support for large genome references makes direct comparison impossible. MetaSpark has a 201 min runtime for 1 million reads with 10 nodes, profiled against a 0.6 GB reference of bacterial genomes from NCBI. In comparison,  Flint  takes 67 s to profile 1 million paired-end reads against Ensembl’s 43 552 genomes (170 GB) with 64 nodes. 4.2 Reference genome collections To test the speed of our read alignment step, we downloaded a reference collection of bacterial genomes from the  Ensembl Bacteria (2018 ) repository (version 41). A total of 43 552 bacterial genomes (strain level) were downloaded in FASTA format, accounting for 4.6 million individual FASTA assembly references. The collection included reference sequences for fully assembled chromosomes and plasmids, as well as containing sequences for draft-quality supercontigs, the latter accounting for most of the reference files in the database. The Ensembl bacterial genomes (v.41) were downloaded from the public FTP site at  ftp.ensemblgenomes.org . Ensembl stores the FASTA files in ‘collection’ directories, and we recursively downloaded the ‘dna’ directory in each of the bacterial sub-folders. In total, 4 672 683 FASTA files were downloaded, with a data footprint on disk of just over 170 GB, accounting for 43 552 bacterial strains. Creating the Bowtie2 index for the bacterial genomes is a one-time operation as the index can be reused across cluster deployments. With a 64 worker-node cluster, we created 64 reference shards, each having a size of 2.6 GB on average. The total sequential indexing time for the 64 shards was 1d 20 h 4 m 33 s on a single machine, but we also used an LSF cluster (IBM Spectrum LSF., 2019) that indexed the 64 shards in parallel, and brought down the total indexing time to just over 3 h. Existing metagenomic profiling tools such as MetaSpark and SparkHit use an archived version of RefSeq as their reference genomes database—MetaSpark’s RefSeq bacterial references was for 1.3 GB of size. Given the fact that the Ensembl database used by  Flint  is roughly ten times larger, we looked into how a metagenomic profile could be different by looking at how many genomes are identified by using a large or small reference collection. To do this we randomly selected 1 M reads from an HMP anterior nares sample (SRS015996) and aligned its reads using Bowtie2 to two genome reference indices: the large collection created from the 43k Ensembl bacterial genomes, and the small collection created from 5591 bacterial representative and reference genomes from NCBI’s Genomes (RepNG). We investigated how many clades are identified by both references, and  Figure 3  displays the results.  Figure 3  shows a phylogenetic tree [created with the Interactive Tree Of Life (iTOL) visualization tool ( Letunic and Bork, 2016 )] showing the differences in the phylogenetic diversity of the taxa identified in the anterior nares sample. Genomes are called as ‘present’ by selecting only those genomes that have an average coverage greater than 80% along their genomic sequence. Nodes at the inner level of the figure represent the phylum taxonomic level, while nodes in the outer rings are at the species level. Green branches represents the clades identified by both references, blue branches represent clades identified by Ensembl, and red branches are clades identified by the RepNG reference set. Note that the number of clades identified by Ensembl at the higher Class and Genus taxonomic levels outnumber those identified when only using the RepNG subset.
 Fig. 3. Phylogenetic tree of taxa identified by Flint using 43k Ensembl bacterial genomes (blue), and 5k NCBI’s Genomes references (red) with an input of 1 M randomly selected reads from the HMP anterior nares sample (SRS015996). Genomes are identified if the average coverage in their genomic sequence is 80% or more 4.3 Experimental setup As mentioned earlier, the computational framework is primarily implemented using the  MapReduce  model ( Dean and Ghemawat, 2008 ), and deployed in a cluster launched in Amazon Web Services ( Amazon.com Inc., Amazon Web Services, 2018 )  Elastic Map Reduce  (EMR) service. The cluster consists of multiple worker machines (i.e. a computational ‘worker’  node ), each with 15 GB of RAM, 8 vCPUs (each being a hyperthread of a single Intel Xeon core) and 100 GB of disk storage. Each of the worker computational nodes will work in parallel to align the input sequencing DNA reads to a shard of the reference database; after the alignment step is completed, each worker node acts as a regular Spark executor node. By leveraging the work of multiple machines working at the same time, we are able to align millions of reads to the over 43k reference genomes in a much more efficient manner than either using only a single machine with considerable computational resources, or using other parallel computation approaches. Benchmarking tests were performed in Spark clusters of size 48, 64 and 128 worker nodes, all deployed in Amazon’s EMR service for very low costs. 4.4 Measuring accuracy using simulated datasets To get a measure of the accuracy of  Flint’ s read-alignment pipeline, and to test the robustness of the streaming infrastructure, we simulated synthetic Illumina reads using the InSilicoSeq ( Gourlé  et al. , 2018 ) metagenomic simulator. We created three replicate dataset groups to test the accuracy of the overall pipeline, and to verify that the streaming system would not introduce any duplicate artifacts, or that the reduce steps in the Spark cluster would not exclude any of the output alignments. Each replicate group consists of 12 datasets ranging from a dataset with 1 read to a dataset with 1 million reads, created with a log-normal abundance profile, and using the default error model for the HiSeq sequencing instrument available in InSilicoSeq. Specific details on the simulation protocol, cluster configuration and detailed results for each replicate set are available in the  Supplementary Materials . 
 Table 1  outlines the results for the synthetic HiSeq datasets. Dataset evaluations were performed on a 64 worker-node cluster in AWS, with each worker node containing 8 vCPUs and 15 GB of memory.  Flint  achieves good performance with the HiSeq dataset achieving 99% sensitivity across all three HiSeq replicates. Alignment times on the 64 node Spark cluster using the database of over 43k Ensembl bacterial genomes show that 1 million reads are aligned in just over 1 min with no loss of sensitivity. The ‘Alignments’ column contains the number of alignments that are produced as output for each dataset—these output alignments are not stored by the system, but rather they are processed as soon as they are generated by the worker nodes in the cluster. Table 1. HiSeq synthetic datasets Reads Alignments Time Alignment rate (%) % Sensitivity 1 1 2 s 344 ms 100 100 10 23 2 s 400 ms 100 100 100 172 2 s 376 ms 100 100 1000 1356 2 s 455 ms 100 100 5000 8592 2 s 517 ms 90 98 10 000 23 791 3 s 193 ms 94 99 50 000 74 543 5 s 138 ms 96 100 100 000 103 835 8 s 320 ms 93 99 250 000 187 349 15 s 788 ms 95 100 500 000 275 917 29 s 18 ms 93 97 750 000 513 954 45 s 91 ms 95 99 1M 617 933 1 m 14 s 713 ms 96 99 
 Note : Average alignment times and alignment rates for three synthetic datasets aligned against Ensembl’s 43k bacterial genomes. Sensitivity is the proportion of paired-end reads that were mapped correctly to the genome from which they were generated. Evaluations were performed on a 64 worker-node Spark cluster. 4.5 Human metagenomic samples After verifying the performance of the  Flint  system on simulated datasets, we tested the capabilities of the system on real metagenomic samples from the Human Microbiome Project (HMP) ( Human Microbiome Project Consortium, 2012 ), which was generated using an Illumina-based sequencing system. We therefore expected a comparable performance with the HMP data as with the synthetic dataset. 4.6 Cluster benchmarks Before testing the system with full human metagenomic samples, we ran a benchmark of randomly sampled paired-end reads from a HMP anterior nares sample (SRS015996) to confirm our previous observations on the synthetic datasets. Each of these read datasets was then processed through the  Flint  system running on a 64 worker-node cluster in AWS.  Table 2  presents the runtimes for each of the datasets, and  Flint  can process 1 million reads in about 67 s. Table 2. Initial cluster benchmarks Paired-end reads Alignments Time (ms) Memory (GB) 1 0 2 s 320 ms 4 10 36 2 s 422 ms 4 100 902 2 s 336 ms 4 1000 9252 2 s 316 ms 4.3 5000 53 918 2 s 455 ms 4.5 10 000 106 160 2 s 700 ms 4.9 50 000 538 594 5 s 437 ms 5.2 100 000 1 006 122 8 s 318 ms 5.8 250 000 2 349 518 17 s 164 ms 6.4 500 000 5 327 040 33 s 950 ms 7.6 750 000 8 439 356 50 s 880 ms 9.5 1M 10 710 420 1 m 7 s 609 ms 10.3 
 Note : Average alignment times in a 64 worker-node cluster for a set of randomly selected reads from a HMP anterior nares sample. The number of alignments column contains the output alignments that are generated by each set of reads; these alignments are processed as soon as they are produced and are not stored, therefore minimizing the local storage requirements necessary for profiling metagenomic samples. 4.7 Full human samples We analyzed 173 million paired-end reads from three HMP samples sequenced from anterior nares (SRS019067, 528k reads), stool (SRS065504, 116 M reads), and supragingival plaque (SRS017511, 56 M reads). These paired-end reads represent samples with varying levels of metagenomic diversity. For the purposes of analysis and the comparison of our execution pipeline, we created diversity classes defined by the number of unique genera present in each sample. To obtain our diversity classes, we analyzed 753 HMP samples for their abundance profiles and surveyed the number of unique genera as reported in the community abundance profiles provided by HMP (see  Supplementary Materials  for details); we then selected representative samples that contained 133 unique genera (high diversity class), 60 unique genera (medium diversity class) and 8 unique genera (low diversity class). The reasoning for using these samples was to test the performance of the  Flint  system in samples with varying degrees of metagenomic diversity. We speculated that low diversity samples would contain reads from a relatively small number of organisms, and therefore the alignment system would not spend too much time finding their genomes of origin. In contrast, the high diversity samples would contain reads from a large number of organisms, and the alignment system would spend more time and resources locating their origins. 
 Table 3  contains the results from running the three samples through the  Flint  system. The sample with the biggest number of paired-end reads, sample SRS065504 with 116 million paired-end reads, was profiled against Ensembl’s 43k genomes in about 105 min. The sample with the second largest number of paired-end reads, i.e. sample SRS017511 with 56 million paired-end reads, was profiled against the 43k genomes in about 94 min; while the sample with the lowest number of paired-end reads was profiled in 53 s. Note that the sample with 116 million paired-end reads was processed in about 10 min more than the sample with 56 million paired-end reads—this sample with 56 million reads is the sample that contains the highest number of unique genera (highest metagenomic diversity, 133 versus 60 in the larger sample). Since more alignments were found, the reads required more time to be processed. Table 3. HMP sample analysis Diversity class Unique genera Sample ID Paired-end reads Alignment execution time Streamed shards Avg. alignments per stream shard Low 8 SRS019067 528 988 0 h 0 m 53 s 2 1 763 227 Medium 60 SRS065504 116 734 970 1 h 45 m 30 s 234 1 471 036 High 133 SRS017511 56 085 526 1 h 34 m 51 s 113 1 535 626 
 Note : Diversity classes were established based on the number of unique genera in 753 HMP samples. Three samples were selected from each diversity class and analyzed in a 64 worker-node cluster. Alignment execution time measures the total time to align all the sample reads against Ensembl’s 43k bacterial genomes. The streamed shards are the number of 250k read sets that are streamed into the cluster, and the average alignment per shard is the average number of alignments produced by each shard. 4.8 Streaming performance The samples in  Table 3  were streamed into the cluster through Spark’s streaming engine. The entire sample is never ingested all at once, but rather, we stream in shards of each sample so that we do not overrun the cluster with so much data that it would cause a cluster failure. To find the ideal number of reads that we could use a size of a stream shard, we looked at the results in  Table 2  and  Figure 4 .  Figure 4B  displays a logarithmic curve of the alignment times for all 12 sizes of the paired-end read datasets, and while we can align 1 million reads in about 67 s, doing so creates so many alignments that each of the Spark executor processes running in each worker node could run out of memory. We looked for the ‘knee-in-the-curve’ in  Figure 4B , marked by the vertical magenta line, and identified a size of 250k paired-end reads as a good trade-off between shard size and cluster performance. When we analyzed the three HMP samples in  Table 3  we set the streaming shard size to 250k reads, and 2 shards were created for the anterior nares sample (low diversity, 500 k reads), 234 shards were created for the stool sample (medium diversity, 116 M reads) and 113 shards for the supragingival plaque sample (high diversity, 56 M reads).
 Fig. 4. Initial Benchmarks. ( A ) The running time for 12 paired-end read datasets in a 64 worker-node cluster. These 12 datasets were used to estimate the optimal number of reads that a 64 worker-node cluster could handle without any memory pressure, or network issues. Note that while 1 million paired-end reads can be mapped in 67 s against 43k bacterial strains, it is not ideal as the cluster’s memory is overwhelm with alignments. ( B ) The logarithmic running time of the 12 datasets, and the 250k paired-end read dataset was chosen as a good trade-off between speed and resource availability 4.9 Cloud costs All experiments were conducted in Amazon’s Elastic MapReduce service (EMR) ( Amazon EMR, 2018 ) and used the ‘c4.2xlarge’ machine instance type. These machines contain 8 vCPUs, 15 GB of RAM and 100 GB of EBS storage; at the time of the experimental runs, each machine cost $0.123 USD in the Amazon’s Spot market ( EC2 Spot Market, 2018 ). All results reported here were obtained on a cluster of 65 total machines (64 worker-nodes, 1 master node) with a cost of $0.123 USD per node, for an overall cluster cost of $8.00 per hour. 5 Conclusion In this work we have shown how large metagenomic samples comprising millions of paired-end reads can be profiled against a large collection of reference bacterial genomes in a fast and economical way. Our implementation relies on the freely available Spark framework to distribute the alignment of millions of sequencing reads against Ensembl’s collection of 43k bacterial genomes. The reference genomes are partitioned in order to distribute the genome sequences across worker machines, and this allows us to use large collections of reference sequences. By using the well-known Bowtie2 aligner under the hood in the worker-nodes, we are able to maintain fast alignment rates, without loss of accuracy. To date, profiling metagenomic samples against thousands of reference genomes has not been possible for research groups with access to modest computing resources. This is due to the size of the reference genomes and the financial costs of the computing resources necessary to employ them. By using distributed frameworks such as Spark, along with affordable cloud computing services such as Amazon’s EMR, we are able to distribute a large collection of reference genomes (totaling 170 GB of reference sequence, and 4.6 million assembly FASTA files) and use a MapReduce strategy to profile millions of metagenomic sequencing reads against them in a matter of hours, and at minimal financial costs, thus bringing sophisticated metagenomic analyses within reach of small research groups with modest resources. 
 Flint  is open source software written in Python and available under the MIT License (MIT). The source code can be obtained at the following GitHub repository:  https://github.com/camilo-v/flint . The repository includes instructions and documentation on provisioning an EMR cluster, deploying the necessary partitioned reference genome indices into worker nodes, and launching an analysis job.  Supplementary Materials , simulation datasets and partitioned reference indices can be found in the  Flint  project website at  http://biorg.cs.fiu.edu/ . Supplementary Material btz356_Supplementary_Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>An R package to analyse LC/MS metabolomic data: MAIT (Metabolite Automatic Identification Toolkit)</Title>
    <Doi>10.1093/bioinformatics/btu136</Doi>
    <Authors>Fernández-Albert Francesc, Llorach Rafael, Andrés-Lacueva Cristina, Perera Alexandre</Authors>
    <Abstract>Summary: Current tools for liquid chromatography and mass spectrometry for metabolomic data cover a limited number of processing steps, whereas online tools are hard to use in a programmable fashion. This article introduces the Metabolite Automatic Identification Toolkit (MAIT) package, which makes it possible for users to perform metabolomic end-to-end liquid chromatography and mass spectrometry data analysis. MAIT is focused on improving the peak annotation stage and provides essential tools to validate statistical analysis results. MAIT generates output files with the statistical results, peak annotation and metabolite identification.</Abstract>
    <Body>1 INTRODUCTION Liquid chromatography and mass spectrometry (LC/MS) is an analytical technique used widely in metabolomics to detect molecules in biological samples ( Theodoridis  et al. , 2012 ). A wide array of software tools is available for LC/MS profiling data analysis, including commercial, programmatic and online tools. A commercial example is Analyst®, whereas some open-source packages permit programmatic processing, such as the R package XCMS ( Smith  et al. , 2006 ) to detect peaks or CAMERA ( Kuhl  et al. , 2012 ) and AStream ( Alonso  et al. , 2011 ) for peak annotations. There have been efforts on just peak annotation using JAVA ( Brown  et al. , 2011 ). MZmine and mzMatch are modularized tools coded in JAVA that are focused on LC/MS data preprocessing and visualization ( Katajamaa  et al. , 2006 ;  Pluskal  et al. , 2010 ;  Scheltema  et al. , 2011 ). Online tools permit sample processing through a web Graphical User Interface, such as XCMSOnline ( http://xcmsonline.scripps.edu ) or MetaboAnalyst ( Xia  et al. , 2009 ). Refer to  Supplementary Table S1  for a comparative between the capabilities for some of the main available tools. In this context, we introduce a new R package called Metabolite Automatic Identification Toolkit (MAIT) for automatic LC/MS analysis. The goal of the MAIT package is to provide an array of tools that makes programmable metabolomic end-to-end statistical analysis possible (see Section 3 of the  Supplementary Material  for details about the MAIT modularity). MAIT includes functions to improve peak annotation through the process called biotransformations and to assess the predictive power of statistically significant metabolites that quantify class separability. 2 METHODS MAIT includes the stages peak detection, peak annotation, statistical analysis and table and plots creation ( Fig. 1 ). The peak detection stage detects the peaks in the LC/MS sample files. The peak annotation stage improves the identification of the metabolites in the metabolomic samples by increasing the chemical and biological information in the dataset. A statistical analysis reveals the significant sample features and measures their predictive power. MAIT uses the R package XCMS to detect and align peaks. For the peak annotation step, MAIT uses three steps:
 First, MAIT uses the CAMERA package to perform the first annotation step ( Kuhl  et al. , 2012 ). In this stage, MAIT uses a peak correlation distance and a retention time window to find which peaks came from the same source metabolite based. The peaks within each peak group are annotated following a reference adduct/fragment table and a mass allowance window. Biotransformations could be related to specific in-source mass losses. Therefore, in the second annotation step, they are detected using a mass allowance window inside the peak groups ( Breitling  et al. , 2006 ). For this search, MAIT already includes a biotransformations table (here Human biotransformations). User-defined biotransformation tables can be set as input, following the procedure defined in  Supplementary Text  (Section 6.6). Finally, a predefined metabolite database is mined for significant masses. This identifies metabolites with the help of the Human Metabolome Database ( Wishart  et al. , 2009 ), 2009/07 version. 
 Fig. 1. Correspondence between MAIT functions (centre column), generated output files (left column) and their functionality (right column) The objective of analysing the metabolomic profiling data is to obtain the statistically significant features (SSF) that contain the highest amount of class-related information. To gather these features, MAIT can apply statistical tests such as ANOVA or Student’s t-test to every feature, selecting the significant set of features given a threshold  P -value. A validation test is included to quantify SSF class separability by a repeated random subsampling cross-validation using three methods: partial least squares and discriminant analysis, support vector machines and K-nearest neighbours ( Hastie  et al. , 2009 ). MAIT computes overall and class-related classification ratios to evaluate the SSF class-related information. 3 RESULTS The example data files are a subset of the data used in the reference ( Saghatelian  et al. , 2004 ), which are distributed freely through the faahKO package ( Smith, 2012 ). MAIT was used to read and analyse these samples using the functions depicted in  Figure 1  (see the tutorial in the  Supplementary Information ). The significant features for each class are found using statistical tests and analysed through the different plots that MAIT produces. Using the following function call, 2640 peaks were detected:
 R&gt; MAIT &lt;- sampleProcessing(dataDir =  “ Dataxcms ” , project 
 =  “ MAIT_Demo ” , snThres = 2, rtStep = 0.03 ) 
 At this point, the first annotation stage is launched:
 R&gt; MAIT &lt;- peakAnnotation(MAIT.object = MAIT) 
 Next, we gather the significant features from the peaks detected. After the Welch’s tests, 106 of these features were found to be significant through the spectralSigFeatures function. Statistical plots such as heat maps, boxplots and principal component analysis score plots can be generated ( Supplementary Figs S3  and  S4 ). Significant features are annotated after checking for certain neutral losses (biotransformations).
 R&gt; MAIT &lt;- spectralSigFeatures(MAIT,  P  = 0.05) R&gt; MAIT &lt;- Biotransformations(MAIT, peakPrecision = 0.005) 
 By using only the SSF, a validation stage is launched, obtaining a classification ratio of 100% with three training samples for all classifiers. These results suggest that the significant variables separate both classes completely.
 R&gt; MAIT &lt;- Validation(MAIT, Iterations = 20, trainSamples 
 = 3 ) 
 Finally, the database is mined to identify the significant features.
 R&gt; MAIT &lt;- identifyMetabolites(MAIT, peakTolerance = 0.005) 
 4 CONCLUSIONS MAIT provides a set of tools and functions to perform an automatic end-to-end analysis of LC/MS metabolomic data, putting special emphasis on peak annotation and metabolite identification. In addition, MAIT validation functions make it possible to estimate predictive power for significant variables. Funding :  Spanish national  (grants  AGL2009-13906-C02-01/ALI  and  AGL2010-10084-E ), the CONSOLIDER INGENIO 2010 Programme,  FUN-C-FOOD  ( CSD2007-063 ) from the  MICINN  and Merck Serono 2010 Research Grants (Fundación Salud 2000).  Spanish Ministerio de Ciencia y Tecnología  through  TEC2010-20886-C02-02  and  TEC2010-20886-C02-01  (in part) A.P. is part of the  2009SGR-1395  consolidated research group of the  Generalitat de Catalunya, Spain . CIBER-BBN is an initiative of the Spanish ISCIII. R.L. thanks the MICINN and the European Social Funds for their financial contribution to the R. L. Ramón y Cajal contract (Ramon y Cajal Programme, MICINN-RYC). F.F.-A. thanks EVALXARTA-UB and Agència de Gestió d’Ajuts Universitaris I de Recerca, AGAUR (Generalitat de Catalunya), for their financial support. Conflict of Interest:  none declared. 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>ISA software suite: supporting standards-compliant experimental annotation and enabling curation at the community level</Title>
    <Doi>10.1093/bioinformatics/btq415</Doi>
    <Authors>Rocca-Serra Philippe, Brandizi Marco, Maguire Eamonn, Sklyar Nataliya, Taylor Chris, Begley Kimberly, Field Dawn, Harris Stephen, Hide Winston, Hofmann Oliver, Neumann Steffen, Sterk Peter, Tong Weida, Sansone Susanna-Assunta</Authors>
    <Abstract>Summary: The first open source software suite for experimentalists and curators that (i) assists in the annotation and local management of experimental metadata from high-throughput studies employing one or a combination of omics and other technologies; (ii) empowers users to uptake community-defined checklists and ontologies; and (iii) facilitates submission to international public repositories.</Abstract>
    <Body>1 HIGH-THROUGHPUT OMICS STUDIES The development of high-throughput genomic and post-genomic (hereafter, ‘omics’) technologies entails changes in the handling, processing and sharing of data (Schofield  et al. ,  2009 ). Omics datasets are often complex and rich in context. Studies may run material through several kinds of assay, using both omics and other technologies; for example, studying the effect of a compound on rat liver through transcriptome, proteome and metabolome profiling (using high-throughput sequencing and two kinds of mass spectrometry, respectively) alongside conventional analyses (e.g. histopathology). Such data must be accompanied by enough contextual information (i.e. metadata; sample characteristics, technology and measurement types; instrument parameters and sample-to-data relationships) to make datasets comprehensible and reusable if they are to underpin future investigations. Many funders and journals require that researchers share data, and encourage the enrichment and standardization of experimental metadata (Field  et al. ,  2009 ). Consequently, more and richer studies are flowing into public databases. However, two bottlenecks can significantly hamper this process, necessitating urgent solutions. First, international public repositories for ‘omics data such as GEO (Barrett  et al. ,  2009 ), ArrayExpress (Parkinson  et al. ,  2009 ), PRIDE (Vizcaíno  et al. ,  2010 ), ENA, SRA and DRA (Shumway  et al. ,  2010 ), have their own submission formats, data models and terminologies, created for specific types of assay. This complicates the submission process for researchers producing multi-assay studies (and greatly increases the risk that these datasets become irrevocably fragmented). Secondly, the shortage of curators to check and annotate submissions to public repositories—a situation unlikely to change soon—necessitates better annotation at source (by experimentalists or community-based efforts; Howe  et al. ,  2008 ). Free software, with automated content validation, is required to facilitate the collection, management and curation of a variety of study inhouse, and to format those data for submission to public repositories. Such software should support community-defined reporting standards, such as the minimum information checklists listed by the MIBBI Portal (Taylor  et al. ,  2007 ), and ontologies, (Côté  et al. ,  2006 ; Smith  et al. ,  2007 ; Noy  et al. ,  2009 ). The Investigation/Study/Assay (ISA) infrastructure described here is the first general-purpose format and freely available desktop software suite designed to regularize local management of experimental metadata by enabling curation at source, supporting community-defined reporting standards and preparing studies for submission to public repositories. 2 THE ISA FORMAT AND SOFTWARE SUITE The software suite comprises five platform-independent Java-based software components for local use, including a relational database ( Fig. 1 ), built around the ISA-Tab format. The components work both as stand-alone applications and as a unified system to assist in the local management and storage of experimental metadata, and to facilitate data submission to international public repositories. All components run as ‘desktop’ applications; in addition, the database component features a web-based query interface.
 Fig. 1. The role of each ISA software component, showing their interrelations, target users and the flow of information through the system. 2.1 ISA-Tab: an extensible, cross-domain format ‘ Investigation ’, ‘ Study ’ and ‘ Assay ’ are the three key entities around which the general-purpose ISA-Tab format for structuring and communicating metadata is built (Sansone  et al. ,  2008 ).  Investigation  contains all the information needed to understand the overall goals and means used in an experiment;  Study  is the central unit, containing information on the subject under study, its characteristics and any treatments applied. Each  Study  has associated  Assay(s) , producing qualitative or quantitative data, defined by the type of measurement (i.e. gene expression) and the technology employed (i.e. high-throughput sequencing). The hierarchical structure of ISA-Tab enables the representation of studies employing one or a combination of omics and other technologies, overcoming the fragmentation of the existing submission formats built for specific types of assay. To ensure conversion, ISA-Tab has been designed with reference to these existing ‘omics formats (Jones  et al. ,  2007 ), complementing and extending their work where necessary; for example, it shares both syntax and the use of easily-manipulable tab-delimited text files with ArrayExpress’ MAGE-Tab (Rayner  et al. ,  2006 ). Additionally, where omics-based technologies are used in clinical or non-clinical studies, ISA-Tab complements existing biomedical formats such as the Study Data Tabulation Model ( http://www.cdisc.org/sdtm ), endorsed by the US Food and Drug Administration. ISA-Tab also complements the XML formats used by the PRIDE, ENA, SRA and DRA repositories, and consequently offers a way to render their experimental metadata documents in a more user-friendly format. Note though that ISA-Tab is simply a format; the decision on how to regulate its use (i.e. enforcing the filling of required fields, or the use of ontologies) is left to local administrators' use of ISA software components, or the growing number of other systems and groups implementing the format (e.g. Krestyaninova  et al. ,  2009 ; SysMO-DB  http://www.sysmo-db.org/community ; XperimentR,  http://www.imperial.ac.uk/bioinfsupport/resources/data_management/ ; more given on the ISA web site). 2.2 ISAcreator: a user-friendly editor This desktop application enables users (i.e. experimentalists) to compile experimental metadata sets, and to import and edit existing ISA-Tab formatted files. It breaks down overall descriptions into relatively simple parts, uses graphical abstraction to enable visualization of the information described and facilitates time-efficient description of experimental steps by remembering prior behaviour (through user profiles). ISAcreator's aesthetically pleasing interface makes extensive use of Java Swing and external open source libraries (e.g. Prefuse,  http://prefuse.org/ ). The editor uses a style of form- and spreadsheet-based data entry that is likely to be familiar to researchers, augmenting basic functionality such as ‘auto-fill’ and ‘undo’ with advanced features, listed below. 2.2.1 Ontology support A dedicated ‘widget’ allows ontology terms to be searched for and inserted in real time  via  the BioPortal (Noy  et al. ,  2009 ) and the Ontology Lookup Service (Côté  et al. ,  2006 ). Terms from those sources are imported along with core metadata (identifiers, definitions and ontology version); term selection is facilitated by a search history displaying prior choices (through user profiles). 2.2.2 Design wizard An alternative way for users to enter information that leverages common patterns to reduce repetitive tasks by guiding users through a series of questions that elicit information about the design of the  Study  and associated  Assay(s) . 2.2.3 Spreadsheet import As a second alternative, this widget enables the mapping and import of information from existing spreadsheets; also the reformatting and reannotation of  legacy  data. 2.2.4 Data file chooser This widget appends data files located either local to the operator, or identified by FTP on a remote system, to an experimental metadata sets. Upon completion of a valid investigation report, ISAcreator outputs a compressed ‘ISArchive’ containing the ISA-Tab-formatted metadata and either the actual data files, or a reference to them, if necessary (e.g. because of their large size), consisting of their address and file name. 2.3 ISAconfigurator: standards-compliant templates This desktop application allows ‘power users’ (i.e. community curators) to customize the fields displayed by ISAcreator, and for example, to meet the requirements of one or more MIBBI minimum information checklists by declaring certain fields mandatory, or by specifying allowed values (e.g. drawn from a set of ontology terms, or formatted in a specific manner). Configuration files from ISAconfigurator are read by ISAcreator, which then generates interface components as required. 2.4 ISAvalidator: adherence to templates This desktop application also reads configuration files and checks both that completed ISA-Tab files meet specified requirements and that associated data files have been linked. Whether ISA-Tab files are created with ISAcreator or another way (e.g. with spreadsheet software), ISAvalidator checks that the document is syntactically correct and internally consistent, and reports on errors (i.e. missing or incorrect values). 2.5 BioInvestigation Index: local storage An ISArchive provides a simple way to store and share information in a structured manner, but those tasks are better performed by uploading such a file to an instance of our ‘BioInvestigation Index’ (BII), or another system that implements ISA-Tab import. The BII includes a management tool and relational database (tested with Oracle, MySQL and PostgreSQL). The former enables validation and loading of an ISArchive and provides simple permissions functionality to link users (or groups of users) to studies. The latter manages the storage of experimental metadata, which can be collectively searched and browsed  via  a query interface or web services; the destination for associated data files, and their protocol for transfer, is custom defined by the local administrator on installation. As an example, a publicly accessible instance of the BII, maintained by the European Bioinformatics Institute ( http://www.ebi.ac.uk/bioinvindex ), has proven useful as a curation and storage system for multi-assay studies, and as a mechanism for submitting data files to ArrayExpress, PRIDE, ENA and SRA. Installation of the BII system requires some knowledge of database management. However, it is portable enough to be easily installed in individual labs, to maximize the efficiency with which high-throughput studies can be managed and shared among users that have been granted access to them. 2.6 ISAconverter: submission to public repositories ISAconverter recodes the relevant parts of ISArchives as MAGE-Tab, PRIDE XML or SRA-XML (used by ArrayExpress, PRIDE and ENA, SRA and DRA, respectively), enabling combined submission to public omics repositories. It is readily extensible to support export of other formats, e.g. SOFT required by GEO (Barrett  et al. ,  2009 ). Mappings for format elements are available in the ISA-Tab specification and documentation on the ISA web site. 3 COLLABORATIONS AND CASE STUDIES Developed for the European multi-site ‘CarcinoGENOMICS’ project (Vinken  et al. ,  2008 ), the ISA software suite version one was released in early 2009. The core ISA developers are engaged with an ever-growing number of collaborators: case studies from early implementers already provide evidence of the diverse life science scenarios in which the suite's various components have been successfully tested and are being used with large datasets (details on the ISA web site). The main limitations recorded to date are simply the person hours required to specify the standards and ontologies to be used and to actually curate studies. Demonstrable acceptance and community engagement has also brought a new funding stream for this project, allowing us to continue the collaborative development of this exemplar system that supports data sharing policies, promotes the uptake of community-defined reporting standards and ontologies and enables curation at source (Field  et al. ,  2009 ). The ISA components, in particular the BII, have been designed to provide core functionalities. Inevitably, each collaborator has additional in-house requirements that are too specific to be included as core functionality. This may be due to the nature of their studies or their need for one or more ISA software components to be interoperable with existing systems. To support further collaborative development, the core ISA developers are setting up an environment for distributed development, and are augmenting the ISA code base with Application Programming Interfaces (APIs). Ongoing collaborative activities include: a module to enable the analysis of ISA-Tab formatted metadata and any associated data, using R; integration with other data management and analysis systems (e.g. Fang  et al. ,  2009 ; MetWare,  http://metware.org ); and giving assistance to the growing number of projects exploring the tools and underlying format (e.g. Sage  http://sagecongress.org/WP/workstreams/Standards ; Kawaji  et al. ,  2009 ). Other collaborative activities include an enhanced user authentication system, support for additional formats such as RDF, OWL and SOFT, converters to/from lab equipment-related file formats (e.g. sampling robots and mass spectrometers) and improved packaging and distribution mechanisms to offer a single download bundle to facilitate installation. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Utopia documents: linking scholarly literature with research data</Title>
    <Doi>10.1093/bioinformatics/btq383</Doi>
    <Authors>Attwood T. K., Kell D. B., McDermott P., Marsh J., Pettifer S. R., Thorne D.</Authors>
    <Abstract>Motivation: In recent years, the gulf between the mass of accumulating-research data and the massive literature describing and analyzing those data has widened. The need for intelligent tools to bridge this gap, to rescue the knowledge being systematically isolated in literature and data silos, is now widely acknowledged.</Abstract>
    <Body>1 INTRODUCTION The typhoon of technological advances witnessed during the last decade has left in its wake a flood of life-science data, and an increasingly impenetrable mass of biomedical literature describing and analysing those data. Importantly, the modern frenzy to gather more and more information has left us without adequate tools either to mine the rapidly increasing data- and literature-collections efficiently, or to extract useful knowledge from them. To be usable, information needs to be stored and organized in ways that allow us to access, analyze and annotate it, and ultimately to relate it to other information. Unfortunately, however, much of the data accumulating in databases and documents has not been stored and organized in rigorous, principled ways. Consequently, finding what we want and, crucially, pinpointing and understanding what we already know, have become increasingly difficult and costly tasks (Attwood  et al. ,  2009 ). A group of scientists for whom these problems have become especially troublesome are biocurators, who must routinely inspect thousands of articles and hundreds of related entries in different databases in order to be able to attach sufficient information to a new database entry to make it meaningful. With something like 25 000 peer-reviewed journals publishing around 2.5 million articles per year, it is simply not possible for curators to keep abreast of developments, to find all the relevant papers they need, to locate the most relevant facts within them, and simultaneously to keep pace with the inexorable data deluge from ongoing high-throughput biology projects (i.e. from whole genome sequencing). For example, to put this in context, Bairoch estimates that it has taken 23 years to manually annotate about half of Swiss-Prot's 516 081 entries (Bairoch,  2009 ; Boeckmann  et al. ,  2003 ), a painfully small number relative to the size of its parent resource, UniProtKB (The UniProt Consortium,  2009 ), which currently contains ∼11 million entries. Hardly surprising, then, that he should opine, ‘It is quite depressive to think that we are spending millions in grants for people to perform experiments, produce new knowledge, hide this knowledge in a often badly written text and then spend some more millions trying to second guess what the authors really did and found’ (Bairoch,  2009 ). The work of curators, and indeed of all researchers, would be far easier if articles could provide seamless access to their underlying research data. It has been argued that the distinction between an online paper and a database is already diminishing (Bourne,  2005 ); however, as is evident from the success stories of recent initiatives to access and extract the knowledge embedded in the scholarly literature, there is still work to be done. Some of these initiatives are outlined below. The Royal Society of Chemistry (RSC) took pioneering steps towards enriching their published content with data from external resources, creating ‘computer-readable chemistry’ with their Prospect software (Editorial,  2007 ). They now offer some of their journal articles in an enhanced HTML form, annotated using Prospect: features that may be marked up include compound names, bio- and chemical-ontology terms, etc. Marked-up terms provide definitions from the various ontologies used by the system, together with InChI (IUPAC International Chemical Identifier) codes, lists of other RSC articles that reference these terms, synonym lists, links to structural formulae, patent information and so on. Articles enriched in this way make navigation to additional information trivial, and significantly increase the appeal to readers. In a related project, the  ChemSpider Journal of Chemistry  exploits the ChemMantis System to mark up its articles ( http://www.chemmantis.com ). With the ChemSpider database at its heart, ChemMantis identifies and extracts chemical names, converting them to chemical structures using name-to-structure conversion algorithms and dictionary look-ups; it also marks up chemical families, groups and reaction types, and provides links to Wikipedia definitions where appropriate. In an initiative more closely related to the life sciences,  FEBS Letters  ran a pilot study (Ceol  et al. ,  2008 ) with the curators of the MINT interaction database (Chatr-aryamontri  et al. ,  2007 ), focusing on integration of published protein–protein interaction and post-translational modification data with information stored in MINT and UniProtKB. Key to the experiment was the Structured Digital Abstract (SDA), a device for capturing an article's key facts in an XML-coded summary, essentially to make them accessible to text-mining tools (Seringhaus and Gerstein,  2007 ); these data were collected from authors via a spreadsheet, and structured as shown in  Figure 1 —while clearly machine-readable, this format has the notable disadvantage of being rather human unfriendly.
 Fig. 1. Structured summary for an article in  FEBS Letters  (Lee  et al. ,  2008 ). Three interactions are shown, with their links to MINT and UniProtKB. A different approach was taken with BioLit (Fink  et al. ,  2008 ), an open-source system that integrates a subset of papers from PubMed Central with structural data from the Protein Data Bank (PDB) (Kouranov  et al. ,  2006 ) and terms from biomedical ontologies. The system works by mining the full text for terms of interest, indexing those terms and delivering them as machine-readable XML-based article files; these are rendered human-readable via a web-based viewer, which displays the original text with colored highlights denoting additional context-specific functionality (e.g. to view a 3D structure image, to retrieve the protein sequence or the PDB entry, to define the ontology term). A more adventurous approach was taken by Shotton  et al.  ( 2009 ), who targeted an article in  PLoS Neglected Tropical Diseases  for semantic enhancement. The enrichments they included were live Digital Object Identifiers and hyperlinks; mark-up of textual terms (disease, habitat, organism, etc.), with links to external data resources; interactive figures; a re-orderable reference list; a document summary, with a study summary, tag cloud and citation analysis; mouse-over boxes for displaying the key supporting statements from a cited reference; and tag trees for bringing together semantically related terms. In addition, they provided downloadable spreadsheets containing data from the tables and figures, enriched with provenance information and examples of ‘mashups’ with data from other articles and Google Maps. To stimulate further advances in the way scientific information is communicated and used, Elsevier offered its Grand Challenge of Knowledge Enhancement in the Life Sciences in 2008. The contest aimed to develop tools for semantic annotation of journals and text-based databases, and hence to improve access to, and dissemination of, the knowledge contained within them. The winning software, Reflect, focused on the dual need of life scientists to jump from gene or protein names to their molecular sequences and to understand more about particular genes, proteins or small molecules encountered in the literature (Pafilis  et al. ,  2009 ). Drawing on a large, consolidated dictionary that links names and synonyms to source databases, Reflect tags such entities when they occur in web pages; when clicked on, the tagged items invoke pop-ups displaying brief summaries of entities such as domain and/or small molecule structures, interaction partners and so on, and allow navigation to core biological databases like UniProtKB. All of these initiatives differ slightly in their specific aims, but nevertheless reflect the same aspiration—to get more out of digital documents by facilitating access to underlying research data. As such, it is interesting to see that a number of common themes have emerged: most are HTML- or XML-based, providing hyperlinks to external web sites and term definitions from relevant ontologies via color-coded textual highlights; most seem to ignore PDF as a foundation for semantic enrichment (despite a significant proportion of publisher content being offered in this format). The results of these projects are encouraging, each offering valuable insights into what further advances need to be made: clearly, we need to be able to link more than just a single database to a single article, or a single database to several articles, or several databases to a single issue of a single journal. Although necessary proofs of principle, these are just first steps towards more ambitious possibilities, and novel tools are still needed to help realize the goal of fully integrated literature and research data. In this article, we describe a new software tool, Utopia Documents, which builds on Utopia, a suite of semantically integrated protein sequence/structure visualization and analysis tools (Pettifer  et al. ,  2004 ,  2009 ). We describe the unique functionality of Utopia Documents, and its use in semantic mark-up of the  Biochemical Journal  ( BJ ). We also outline the development of a number of new plugins, by means of which we have imported additional functionality into the system via web services. 2 SYSTEM AND METHODS Utopia Documents was developed in response to the realization that, in spite of the benefits of ‘enhanced HTML’ articles online, most papers are still read, and stored by researchers in personal archives, as PDF files. Several factors likely contribute to this reluctance to move entirely to reading articles online: PDFs can be ‘owned’ and stored locally, without concerns about web sites disappearing, papers being withdrawn or modified, or journal subscriptions expiring; as self-contained objects, PDFs are easy to read offline and share with peers (even if the legality of the latter may sometimes be dubious); and, centuries of typographic craft have led to convergence on journal formats that (on paper and in PDF) are familiar, broadly similar, aesthetically pleasing and easy to read. In its current form, Utopia Documents is a desktop application for reading and exploring papers, and behaves like a familiar PDF reader (Adobe Acrobat, KPDF, OS X Preview, etc.); but its real potential becomes apparent when configured with appropriate domain-specific ontologies and plugins. With these in place, the software transforms PDF versions of articles from static facsimiles of their printed counterparts into dynamic gateways to additional knowledge, linking both explicit and implicit information embedded in the articles to online resources, as well as providing seamless access to auxiliary data and interactive visualization and analysis tools. The innovation in the software is in implementing these enhancements without compromising the integrity of the PDF file itself. Suitably configured, Utopia Documents is able to inspect the content and structure of an article, and, using a combination of automated and manual mechanisms, augment this content in a variety of ways: 2.1 Adding definitions Published articles are typically restricted to a defined page count, and are usually written for a specific audience. Explanations of terms that might be useful to newcomers to a particular field are therefore frequently omitted. Utopia Documents allows editors and authors to annotate terms with definitions from online resources (Wikipedia, UniProtKB, PDB, etc.), and permits readers to easily find definitions for themselves. 2.2 Interactive content and auxiliary data Figures and tables in printed form are typically static snapshots of richer data (which are nowadays often available elsewhere online). For example, a table may represent the salient fragment of a much larger experimental dataset, or an image of a protein structure might highlight one specific feature of that molecule. Utopia Documents is able to transform such static tables and figures,  in situ , into dynamic, interactive objects, providing richer access to the underlying data. 2.3 Linking references to source articles Most articles published today are made available in electronic form, and substantial efforts are also made by publishers to make their back-catalogues electronically accessible. Navigating the multitude of online repositories and bibliographic tools, however, is complex. Utopia Documents simplifies the process of finding related articles by automatically linking references to their digital online versions. 3 IMPLEMENTATION The software architecture comprises three main components, as shown in  Figure 2 : ‘the core’, providing generic mechanisms for displaying and manipulating articles, both programmatically and interactively; ‘the plugins’, which analyze, annotate and visualize document features, either automatically or under the guidance of a user; and ‘the ontology’, which is used to semantically integrate the other components.
 Fig. 2. The architecture of Utopia Documents, showing the relationship between the GUI (top), plugins (middle) and ontology (bottom). 3.1 The core Optimized for interactivity, the multi-threaded core of Utopia Documents is written in C++ using Trolltech/Nokia's Qt toolkit. The core serves two purposes: (i) it performs the relatively mundane tasks necessary to generate and manage the interactive Graphical User Interface (GUI) and to co-ordinate the behavior of the plugins, which are loaded on demand at run-time; (ii) it carries out the low-level analysis of PDF documents, including reading their file format and converting them into both a visual representation to be displayed on-screen and a hierarchical semantic model for later higher-level analysis and annotation by the plugins. The analysis performed by the core is generic in nature, and is restricted at this stage to identifying typographical and layout-based features common to scholarly papers from any discipline. Once this raw structure has been generated, using various heuristics, the system then identifies higher-level typographical constructs (titles, sections, headings, figures, tables, references, etc.). Annotations identifying these features are assembled, and added to the raw hierarchy to form a semantic model that is then shared with, and further annotated by, the plugins. From these ‘structural semantics’, a ‘fingerprint’ is created that uniquely identifies the article being read, and allows the annotations to be associated with it. 3.2 The plugins Two broad classes of plugin are defined. ‘Annotators’ inspect a document's content and semantic structure, then either apply local algorithms or communicate with external services in order to create annotations containing additional content (e.g. definitions of terms, user comments, links to other resources). Annotator plugins may be configured to execute automatically when a document is loaded, typically performing document-wide tasks, such as identifying terms of biological or chemical interest; alternatively, they may be invoked manually via the GUI—in these cases, the plugins have access to the GUI's state, and can generate context-specific annotations (e.g. associating a highlighted region of text with a specific comment made by a user, or finding the definition of a highlighted concept in an online database.) ‘Visualizers’ provide various mechanisms for displaying and interacting with annotations: e.g. an annotation containing static images, links and ‘rich text’ may be displayed using a browser-like visualizer, whereas one containing the structure of a molecule from the PDB might be displayed as an interactive 3D object. Both types of plugin may be written in C++ or Python, and are executed in their own asynchronous environment, marshalled by the core. 3.3 The ontology Rather than create ‘hard-wired’ relationships between the system's components, a simple ontology (in its current form, a hierarchical taxonomy) connects the plugins to the core and to one another. This form of semantic integration allows the components to cooperate flexibly in the analysis of document content and structure, and allows plugins to be developed independently of one another, with sensible relationships and behavior being inferred at run-time rather than being pre-determined: e.g. an annotator plugin may mark content as containing ‘protein structure’; a visualizer plugin, encountering this annotation at a later stage, can then decide whether to display this as a 2D static image, an interactive 3D model or as a 1D amino acid sequence. 3.4 Access to remote resources Via its plugins, Utopia Documents has access to a wealth of bioinformatics data. Each plugin can use whatever client libraries are appropriate to access web-service endpoints (both SOAP- and REST-style), as well as other remotely accessible resources, such as relational databases and RDF stores. Of particular note here are two substantial ‘linked data’ initiatives that have proven to be of enormous value to our work. The first of these, the Bio2RDF project (Belleau  et al. ,  2008 ), combines the content of many of the major life-science databases as a federated linked-data network accessible via SPARQL and REST interfaces. This both offers a single mechanism via which Utopia Documents can search multiple primary databases, and enforces a consistent naming scheme between sources, allowing results to be interrelated. The second (and more general), DBPedia, is a machine-readable RDF-based conversion of the popular human-readable Wikipedia (Auer  et al. ,  2007 ). Although containing much information that is irrelevant to the life sciences, Wikipedia (and thus DBPedia) has evolved to represent a significant and mostly authoritative corpus of scientific knowledge—a study performed by the journal  Nature  concluded that its entries were as accurate (or indeed, as error prone) as those published in  Encylopaedia Britannica  (Giles,  2005 ,  2006 ). The combined application of ontologies and RDF in DBPedia allows queries performed by Utopia Documents to traverse only the portions of the DBPedia network that are semantically related to the life sciences. Thus, in the context of a paper on enzymatic substrate cleavage, a search initiated via Utopia Documents for the term ‘cleavage’ returns far more appropriate definitions than would the same search in a more generic context. Utopia Documents is freely available via the project web site for Mac OS X (10.4 and later), Microsoft Windows XP and Vista and Ubuntu Linux. We welcome any feedback on the software. 4 RESULTS AND DISCUSSION Utopia Documents was developed in response to the need to achieve tighter coupling between published articles and their underlying data, ultimately to facilitate knowledge discovery. The tool was designed with two classes of user in mind; the reader, as consumer of published material; and the journal editor, as curator. To this end, the software was piloted with Portland Press Limited (PPL) with the goal of rendering the content of  BJ  electronic publications and supplemental data richer and more accessible. To achieve this, an ‘editor's version’ of Utopia Documents, with customized plugins, was integrated with PPL's editorial and document-management workflows, allowing  BJ  editors to mark up article content prior to publication. In terms of functionality, the editor's version of the software behaves much the same as the reader's, with the additional feature that relationships between concepts in a document and online definitions/records can be made permanent in order to be shared with readers ( Fig. 3 g). The role of the editors was therefore to explore each pre-publication PDF, annotating terms and figures with definitions and interactive content and then validating them with a ‘stamp of approval’ (i.e. the  BJ  icon).
 Fig. 3. Utopia Documents' user interface showing: ( a ) a selected term in the article; ( b ) manual term lookup; ( c ) resulting definitions of that term retrieved from Wikipedia (via DBPedia) and the PDB; ( d ) metadata relating to the whole document (shown when no specific term definition is selected); ( e ) live links to articles in the article's bibliography; ( f ) an icon indicating the ‘authority’ for a particular annotation (here, the  BJ ) and ( g ) the panel used by  BJ  editorial staff to associate terms with annotations (note that this is only available in the ‘editor's version’ of Utopia Documents). With the customized software in-house, article annotation was fairly swift, individual papers taking 10–30 min, depending on their suitability for mark-up. The launch issue of the Semantic  BJ  (December 2009;  http://www.biochemj.org/bj/424/3/ ) was primarily handled by two editors; since then, the whole editorial team has been involved in the successful mark-up of eight further issues. Entities relating to protein sequences and structures have been, of necessity, the main targets for mark-up, because this was the functionality built into the original Utopia toolkit. The kinds of additional mark-up provided by the software include links from the text to external web sites, term definitions from ontologies and controlled vocabularies, embedded data and materials (images, videos, etc.) and links to interactive tools for sequence alignment and 3D molecular visualization. To allow readers to benefit from these semantic enhancements, a reader's version of the software was made freely available ( http://getutopia.com ). The tool installs easily on the desktop as an alternative PDF viewer. Once opened, it displays a window consisting of three regions ( Fig. 3 ): the main reading pane displays the article itself and supports the pagination, searching, zooming and scrolling features typical of PDF readers. Below this, thumbnail images give an overview of the document and allow rapid navigation through it. The sidebar on the right displays the contents of annotations, providing term definitions and access to auxiliary data as the article is explored. When no specific terms are selected, the sidebar defaults to displaying document-wide metadata [including the title, authors, keywords, abbreviations, etc. (3d)], in addition to the cited references (3e)—these are linked, where available, via open-access publishing agreements or institutional or individual subscriptions, to the online versions of the original articles. Where the PDF version is not available to the reader, clicking on the reference currently launches a Google Scholar search instead. To avoid cluttering the text with ‘highlighter pen’-type marks, the presence of annotations, or availability of auxiliary data, is indicated by discreet colored glyphs in the margin. Similar marks are added to the corner of the corresponding thumbnail in the pager, to indicate that additional information exists somewhere on that page. Mousing-over a glyph highlights the nearby terms, or document regions, that contain annotations; selecting these areas causes the associated data to be displayed—this may involve populating the sidebar with definitions, or may activate an embedded interactive visualization. Highlighting any word or phrase in the paper (3a) initiates a context-sensitive search of the online resources to which Utopia Documents is connected, all results again appearing in the sidebar. At the bottom of the sidebar (3b), a ‘lookup’ feature allows searches for terms not explicitly mentioned in the paper. 4.1 Annotations An annotated term or region in a document may be associated with definitions and/or database records from a variety of sources. Selecting a term invokes the display of all possible definitions, allowing the reader (or editor) to select for themselves the most appropriate version. The provenance of these definitions is indicated in their headers, as illustrated in  Figure 3 : the icon on the left (3c) represents the item's origin [e.g., UniprotKB, Wikipedia, KEGG (Kanehisa  et al. ,  2010 )], while the presence of an icon on the right-hand side of the header (3f) indicates the person, group or organization who made, and endorsed, the association between a term and this specific definition (here, publisher-validated annotations carry the  BJ  logo). 4.2 Interactive content The current version of Utopia Documents supports three forms of embedded interactive content; as with term definitions, these are indicated by red glyphs in the margins. Selecting these causes a ‘media player’-like panel to appear, which the reader can use to control the behavior of the interactive content. Activating the triangular ‘play’ button replaces the static content,  in situ , with its interactive version; the neighboring ‘pop-up’ button opens a new window leaving the static page unchanged. Each type of interactive content has its own functionality: 3D molecules ( Fig. 4 ), for example, can be rotated, zoomed and rendered in a variety of styles (e.g. space-fill, backbone or cartoon); sequences and their associated features can be inspected individually, or edited as multiple alignments; and tables of data can be manipulated or converted automatically into scatter-plots or histograms.  Figure 4  illustrates the simple transformation from static images of tables and figures into semantically annotated, interactive objects.
 Fig. 4. Image sequences showing the transformation of a 2D image (left-hand panel) and of a static table of figures (right-hand panel) into interactive objects: i.e. a manipulable 3D model (coordinates extracted from the PDB) and a set of ‘live’ figures and a customizable semantic graph. Utopia Documents provides new ways of reading, of interacting with and ultimately of assimilating the knowledge embodied within research articles. The approach taken here departs from many initiatives in scholarly publishing in that the focus for enrichment is the hitherto-largely-neglected static PDF file, rather than HTML- or XML-based files. The subject of ‘static PDF’ versus ‘dynamic online’ articles has been hotly contested in the literature, the general consensus being that PDF is semantically limited by comparison with other online formats and is thus antithetical to the spirit of web publishing (Lynch,  2007 ; Renear and Palmer,  2009 ; Shotton  et al. ,  2009 ; Wilbanks,  2007 ). We argue that PDFs are merely a mechanism for rendering words and figures, and are thus no more or less ‘semantic’ than the HTML used to generate web pages. Utopia Documents is hence an attempt to provide a semantic bridge that connects the benefits of both the static and the dynamic online incarnations of published texts. Inevitably, those who prefer to read articles online in a web browser will view the need to download a new, desktop-based PDF reader as a weakness. Our view is, rather, that Utopia Documents complements browser-based tools, providing a novel mechanism for unleashing knowledge that is otherwise locked in personal, publisher and/or institutional PDF-file archives. In contrast with approaches for creating dynamic (as opposed to ‘semantic’) life-science PDF articles (Kumar  et al. ,  2008 ; Ruthensteiner and Hess,  2008 ) that use Adobe Acrobat's support for ‘Universal 3D Data’ (U3D), Utopia Documents does not insert its augmented content into the PDF file itself, but instead blends additional visual material into the display process at the final stages of rendering. This mechanism presents a number of benefits over the generic U3D approach: (i) the underlying PDF file remains small and compact, and does not become bloated by the large polygonal meshes necessary for rendering 3D molecules; (ii) rather than the ‘one size fits all’ U3D approach, Utopia Documents is able to select appropriate rendering and interaction algorithms for different types of artifact; (iii) Utopia Documents is able to maintain a semantic relationship between the underlying scholarly article and the object being rendered; and importantly; (iv) the original PDF, as an ‘object of record’, remains unadulterated and its integrity can be verified by examining it with a conventional PDF viewer. The philosophy embodied in Utopia Documents is to hide as much of the underlying complexity as possible, to avoid requiring users (whether editors, authors or readers) to change their existing document-reading behaviors, and to present no significant extra hurdles to publication. Like the initiatives in semantic publishing outlined earlier, the Semantic  BJ , powered by Utopia Documents, is a pilot, the success of which will depend on various factors, including whether the barriers to adoption are sufficiently low, and whether the approach is considered to add sufficient value. Although it is too early to assess the impact of the pilot on readers of the Semantic  BJ , the take-up of the software by the  BJ 's full editorial team, and it use to mark up every issue since the launch, is a testament to the software's ease-of-use. Of course, as the project with PPL develops, we will gather relevant usage and usability data in order to provide a more meaningful evaluation. Many of the projects discussed in this article have exploited fairly traditional text-mining methods, in conjunction with controlled vocabularies and ontologies, to facilitate the launch of relevant external web pages from marked-up entities in documents. As such, they come with all the limitations in precision of current text-mining tools; this brings a significant overhead to readers in terms of having to identify errors. Of course, the difficulty for non-experts in any given field is to be able to recognize when particular annotations really are errors, and failure to identify them as such leads to the danger of error propagation. In light of these issues, we took a slightly different approach to entity mark-up in this first incarnation of Utopia Documents, taking advantage of linked-data initiatives to facilitate mark-up and add value to published texts. However, because the functionality of the system is easily customizable via its flexible plugin architecture, any text-mining tool or database that is accessible via web services can be trivially added to the suite. As a demonstration of the potential of this architecture, in collaboration with their developers, three prototype plugins that link Utopia to other systems have been implemented: Reflect: as mentioned earlier, the Reflect system is primarily used as a means of augmenting HTML content online, either by accessing a web page via the project's portal, or by installing a browser plugin ( http://reflect.ws/ ). Its entity-recognition engine, however, may also be accessed programmatically via a web service, which, given a section of text, identifies objects of biological interest and returns links to the summary pop-ups. Integration of Reflect's functionality with Utopia Documents is therefore a comparatively straightforward task: as a user reads a PDF document, its textual content is extracted and sent to the Reflect web service; the resulting entities are then highlighted in the PDF article, and linked to the appropriate pop-up, which is displayed when a highlighted term is selected. A particular advantage of this integration is that it provides the reader with a light-weight mechanism for verifying or cross-checking results returned from multiple sources (e.g. Reflect, Bio2RDF, DBpedia/Wikipedia). GPCRDB: this is a specialist database describing sequences, ligand-binding constants and mutations relating to G protein-coupled receptors ( http://www.gpcr.org/ ). Its recently developed web-service interface provides programmatic access to much of its content, enabling Utopia Documents to identify and highlight receptors and their associated mutants when encountered in PDFs. Thus, the presence of a GPCR in an article triggers the creation of a link to a description of that receptor in the database, which is displayed in the sidebar. The article is then scanned for mutants, which in turn are linked to the relevant mutant records in GPCRDB. Having identified an appropriate receptor, the software then automatically translates between the sequence co-ordinates, allowing ‘equivalent’ residues to be readily mapped between them. ACKnowledge Enhancer and the Concept Wiki: the Concept Wiki is a repository of community-editable concepts, currently relating to people and proteins, stored as RDF triples and fronted by a wiki-like interface ( http://www.conceptwiki.org ). Its associated ACKnowledge Enhancer is an analysis tool that links HTML content to relevant objects in the Concept Wiki and other online sources, exposing these to the user as selectable HTML highlights that, when activated, generate dynamic pop-ups. As with the Reflect plugin, integration with these systems via their web services provides a straightforward way of migrating functionality previously only available for HTML content to scientific PDF articles. Videos showing these plugins in use are available at  http://getutopia.com . Utopia Documents is at an early stage of development and there is more work to be done. In the future, as well as opening its APIs to other developers, we plan to extend its scope to systems and chemical biology, and to the medical and health sciences, as many of the requisite chemical, systems biology, biomedical, disease and anatomy ontologies are already in place and accessible via the OBO Foundry (Smith  et al. ,  2007 ). Furthermore, the growing impetus of ‘institutional repositories’ as vehicles for collecting and sharing scholarly publications and data, and an increase in the acceptance of open access publishing, together present many interesting possibilities that we are keen to explore. Another planned extension is to allow readers to append annotations and notes/comments to articles. There are various scenarios to consider here: (i) a reader might wish to make a ‘note to self’ in the margin, for future reference; (ii) a reviewer might wish to make several marginal notes, possibly to be shared with other reviewers and journal editorial staff; (iii) a reader might wish to append notes to be shared with all subsequent readers of the article (e.g., because the paper describes an exciting breakthrough or because it contains an error)—these scenarios involve different security issues, and hence we will need to investigate how to establish appropriate ‘webs of trust’. Ultimately, allowing users to append their own annotations (in addition to those endorsed by publishers) should help to involve authors in the manuscript mark-up process. Utopia Documents brings us a step closer to integrated scholarly literature and research data. The software is poised to make contributions in a number of areas: for publishers, it offers a mechanism for adding value to oft-neglected PDF archives; for scientists whose routine work involves having to attach meaning to raw data from high-throughput biology experiments (database curators, bench biologists, researchers in pharmaceutical companies, etc.), it provides seamless links between facts published in articles, information deposited in databases and the requisite interactive tools to analyze and verify them; for readers in general, it provides both an enhanced reading experience and exciting new opportunities for knowledge discovery and ‘community peer review’. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>The EBI RDF platform: linked open data for the life sciences</Title>
    <Doi>10.1093/bioinformatics/btt765</Doi>
    <Authors>Jupp Simon, Malone James, Bolleman Jerven, Brandizi Marco, Davies Mark, Garcia Leyla, Gaulton Anna, Gehant Sebastien, Laibe Camille, Redaschi Nicole, Wimalaratne Sarala M., Martin Maria, Le Novère Nicolas, Parkinson Helen, Birney Ewan, Jenkinson Andrew M.</Authors>
    <Abstract>Motivation: Resource description framework (RDF) is an emerging technology for describing, publishing and linking life science data. As a major provider of bioinformatics data and services, the European Bioinformatics Institute (EBI) is committed to making data readily accessible to the community in ways that meet existing demand. The EBI RDF platform has been developed to meet an increasing demand to coordinate RDF activities across the institute and provides a new entry point to querying and exploring integrated resources available at the EBI.</Abstract>
    <Body>1 INTRODUCTION The European Bioinformatics Institute (EBI) is the largest bioinformatics resource provider in Europe. Our databases are accessible via dedicated interfaces, web services, data download and (in a few cases) direct database access. Modern research in the life sciences necessitates an understanding of data at many different levels: multi-omics, from cells to biological systems, across many different species and studying many different experimental conditions. The biology underpinning these research questions is intrinsically connected, yet data are often collected and stored in technology or domain-specific repositories. Efforts in the Semantic Web community are already beginning to invest in technology that enables data to be readily integrated ( Belleau  et al. , 2008 ;  Katayama  et al. , 2010 ;  Marshall  et al. , 2008 ). One method used among the Semantic Web community is using the W3C’s resource description framework (RDF) model to represent data. RDF provides a common mechanism for describing data and querying data using SPARQL. To better serve complex research questions across resources, and to meet an increased demand on the EBI to produce RDF, we have developed an RDF platform. The aim of such a platform is to offer users the ability to ask questions using multiple connected resources that share common identifiers and have a common format (RDF) and query interface (SPARQL). This platform complements other existing data access modes such as our Web site and RESTful web services, but additionally contains explicit links between the different data resources. This enables a single query to be asked across multiple distributed datasets and across a range of biological domains. This approach has been applied for the following EBI resources: Gene Expression Atlas ( Kapushesky  et al. , 2012 ), ChEMBL ( Gaulton  et al. , 2011 ), BioModels ( Li  et al. , 2010 ), Reactome ( Matthews  et al. , 2008 ), BioSamples ( Gostev  et al. , 2012 ) and also includes a collaboration with the UniProt Consortium to deliver UniProt RDF ( Redaschi and UniProt Consortium, 2009 ). 2 METHODS The RDF platform presents a coordinated effort to bring together RDF resources from multiple services and databases at the EBI. The development of the platform began by collecting requirements from both a scientific and a technical perspective. The scientific requirements were gathered as a series of use cases and competency questions collected from research scientists and users of EBI services. In particular, we were looking for questions that required data to be integrated from multiple resources and that are not trivial to answer with our existing infrastructure due to the disparate nature of the data. These questions were used to identify points of integration between resources. The scientific use cases informed the technical requirements on what infrastructure, in terms of both software and hardware, would be needed to deliver a stable and scalable platform. Given RDF technology is still maturing, there are open questions on how to deliver such a platform on this scale; our existing infrastructure is delivered after evaluation of various technologies that will be the subject of another paper. Data from UniProt, ChEMBL, Reactome and BioModels represents curated knowledge from protein sequence and function, bio-active molecules and their targets, to biochemical pathways and computational models of molecular interactions. The Gene Expression Atlas database provides differential gene expression data from a variety of samples that are highly annotated and curated using the Experimental Factor Ontology (EFO) ( Malone  et al. , 2010 ). Generating linked RDF for these resources provides a new entry point for exploring the data, such as putting gene expression in the context of protein function, pathways and drug targets. An outline of how resources are connected is shown in  Figure 1 .
 Fig. 1. Connections between services (boxes) and ontologies (circles). The graph illustrates how the data are linked within the RDF platform, enabling queries to span all data. Asterisk: ENSEMBL to UniProt (gray line) mappings are included via expression atlas The graph-based nature of the RDF data model provides a natural fit for explicitly publishing how data are connected. In RDF, resources are identified using uniform resource identifiers (URIs), which provide a web-based global identification system. Guidelines for minting new URIs for EBI resources were established using the new rdf.ebi.ac.uk domain (details can be found at  http://www.ebi.ac.uk/rdf/documentation/uris-ebi-data ). Canonical URIs are used when existing databases, such as UniProt, already provide stable URIs. In cases where no canonical URIs are provided by external resources, the Identifiers.org registry of scientific identifiers ( Juty  et al. , 2012 ) was used to provide a referencing URI. As part of the URI strategy, every effort has been made to ensure all EBI RDF datasets only use URIs that can be dereferenced using  http,  supporting content negotiation for human-orientated HTML views, alongside machine processable versions in various RDF syntaxes. Using common URI schemes assists data integration with RDF. In addition, ontologies provide a mechanism to semantically describe the data, and the OWL ontology language can be serialized in RDF. The EBI makes extensive use of ontologies to annotate data, however, the richness of these annotations is rarely available in native RDF for exploitation by external applications. The EBI RDF platform adopts a range of common vocabularies and ontologies to annotate data. The ontologies used span common biomedical terminologies such as the Gene Ontology, Chemical Entities of Biological Interest, UBERON, Cell Type Ontology, Biological Pathways Exchange, EFO and more. Additionally, we adopted metadata standards for describing datasets and provenance such as Dublin Core, Data Catalog Vocabulary and Vocabulary of Interlinked Datasets. 3 RESULTS Complete dumps of the RDF data are available via FTP downloads. These are published in line with existing production and release cycles, ensuring the most up-to-date data are readily available. We are also using triple store technology to index the RDF files and make them available for querying and exploration via SPARQL endpoints and our linked data browser. The underlying infrastructure at the EBI is built on open source triple store technology provided by OpenLink, ( http://www.openlinksw.com/ ), whereas the UniProt data are served by the SIB’s Vital-IT HPC platform using technology from OntoText ( http://www.ontotext.com/ ). We developed LODEStar ( http://www.ebi.ac.uk/fgpt/sw/lodestar/ ) as a generic SPARQL endpoint and linked data browser to provide a consistent interface and some enhanced functionality for querying and browsing EBI-based datasets. In addition to providing access to the underlying data, an equally important component of the platform is the Web site at  http://www.ebi.ac.uk/rdf  that provides an entry point to discover all RDF resources being served by the EBI. This site includes documentation on how to find the datasets and provides examples of how to query the data using the SPARQL endpoints ( http://www.ebi.ac.uk/rdf/example-sparql-queries ). We also provide examples showing developers how they can use the SPARQL API programmatically from common programming environments like Perl, Java and R.  4 CONCLUSION The EBI RDF platform allows explicit links to be made between datasets using shared semantics from standard ontologies and vocabularies, facilitating a greater degree of data integration. SPARQL provides a standard query language for querying RDF data. Data that have been annotated using ontologies, such as EFO and the Gene Ontology, enable data integration with other community datasets and provides the semantics to perform rich queries. Publishing these datasets as RDF along with their ontologies provides both the syntactic and semantic integration of data long promised by semantic web technologies. As the trend toward publishing life science data in RDF increases, we anticipate a rise in the number of applications consuming such data. This is evident in efforts such as the Open PHACTS platform ( http://www.openphacts.org ) and the AtlasRDF-R package ( https://github.com/jamesmalone/AtlasRDF-R ). Our aim is that the EBI RDF platform enables such applications to be built by releasing production quality services with semantically described RDF to enable pertinent biomedical use cases to be addressed. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Fast and accurate long-read alignment with Burrows–Wheeler transform</Title>
    <Doi>10.1093/bioinformatics/btp698</Doi>
    <Authors>Li Heng, Durbin Richard</Authors>
    <Abstract>Motivation: Many programs for aligning short sequencing reads to a reference genome have been developed in the last 2 years. Most of them are very efficient for short reads but inefficient or not applicable for reads &gt;200 bp because the algorithms are heavily and specifically tuned for short queries with low sequencing error rate. However, some sequencing platforms already produce longer reads and others are expected to become available soon. For longer reads, hashing-based software such as BLAT and SSAHA2 remain the only choices. Nonetheless, these methods are substantially slower than short-read aligners in terms of aligned bases per unit time.</Abstract>
    <Body>1 INTRODUCTION Following the development of sensitive local alignment software, such as FASTA (Pearson and Lipman,  1988 ) and BLAST (Altschul  et al. ,  1997 ) around 1990, a new generation of faster methods to find DNA sequence matches was developed since 2000, including MegaBLAST (Morgulis  et al. ,  2008 ; Zhang  et al. ,  2000 ), SSAHA2 (Ning  et al. ,  2001 ), BLAT (Kent,  2002 ) and PatternHunter (Ma  et al. ,  2002 ), greatly speeding up matching capillary sequencing reads against a large reference genome. When new sequencing technologies arrived that generated millions of short (&lt;100 bp) reads, a variety of new algorithms were developed which were 10–1000 times faster, including SOAP (Li,R.  et al. ,  2008 ), MAQ (Li,H.  et al. ,  2008 ), Bowtie (Langmead  et al. ,  2009 ) and BWA (Li and Durbin,  2009 ). However, Roche/454 sequencing technology has already produced reads &gt;400 bp in production, Illumina gradually increases read length &gt;100 bp, and Pacific Bioscience generates 1000 bp reads in early testing (Eid  et al. ,  2009 ). Reads coming from the new sequencing technologies are not short any more, which effectively rules out many of the new aligners exclusively designed for reads no longer than 100 bp. Efficiently aligning long reads against a long reference sequence like the human genome poses a new challenge to the development of alignment tools. Long-read alignment has different objectives from short-read alignment. First, in short-read alignment, we would usually like to align the full-length read to reduce the reference bias caused by the mismatches toward the ends of the read. Given this requirement, we can design spaced seed templates (Ma  et al. ,  2002 ) spanning the entire read (Jiang and Wong,  2008 ; Lin  et al. ,  2008 ; Smith  et al. ,  2008 ), or quickly filter out poor matches, for example, by applying q-gram filtration (Rumble  et al. ,  2009 ; Weese  et al. ,  2009 ) or by bounding the search process (Li and Durbin,  2009 ), and thus accelerate the alignment. In long-read alignment, however, we would prefer to find local matches because a long read is more fragile to structural variations and misassemblies in the reference but is less affected by the mismatches close to the ends of a read. Secondly, many short-read aligners are only efficient when doing ungapped alignment or allowing limited gaps, e.g. a maximum of one gap. They cannot find more gaps or the performance quickly degrades when they are tuned for this task. Long-read aligners, however, must be permissive about alignment gaps because indels occur more frequently in long reads and may be the dominant source of sequencing errors for some technologies such as 454 and Pacific Bioscience. When considering algorithms to speed-up long-read alignment, hash table indexing as is used in most current software is not the only choice. Meek  et al.  ( 2003 ) found a Smith–Waterman-like dynamic programming that can be applied between a query sequence and the suffix tree of the reference, effectively aligning the query against each subsequence sampled from the suffix tree via a top-down traversal. As on a suffix tree identical sequences are collapsed on a single path, time is saved by avoiding repeated alignment of identical subsequences. Lam  et al.  ( 2008 ) furthered this idea by implicitly representing the suffix tree with an FM-index (Ferragina and Manzini,  2000 ), which is based on the Burrows–Wheeler Transform (BWT; Burrows and Wheeler,  1994 ), to achieve a small memory footprint. Their new algorithm, BWT-SW, is able to deliver identical results to the standard Smith–Waterman alignment, but thousands of times faster when aligning against the human genome sequence. While BWT-SW is still slower than BLAST on long query sequences, it finds all matches without heuristics. One can imagine that introducing heuristics would further accelerate BWT-SW. Our BWA-SW algorithm follows this route. To some extent, BWA-SW, as well as BWT-SW, also follows the seed-and-extend paradigm. But different from BLAT and SSAHA2, BWA-SW finds seeds by dynamic programming between two FM-indices and allows mismatches and gaps in the seeds. It extends a seed when the seed has few occurrences in the reference sequence. Speed is gained by reducing unnecessary extension for highly repetitive sequences. In this article, we will describe this new alignment algorithm, BWA-SW, for long-read alignments and evaluate its practical performance along with BLAT and SSAHA2 on both simulated and real data. We will also give a brief introduction to suffix array and FM-index, but readers are referred to Li and Durbin ( 2009 ) for more details. 2 METHODS 2.1 Overview of the BWA-SW algorithm BWA-SW builds FM-indices for both the reference and query sequence. It implicitly represents the reference sequence in a prefix trie and represents the query sequence in a prefix directed acyclic word graph (prefix DAWG; Blumer  et al. ,  1985 ), which is transformed from the prefix trie of the query sequence ( Section 2.3 ). A dynamic programming can be applied between the trie and the DAWG, by traversing the reference prefix trie and the query DAWG, respectively. This dynamic programming would find all local matches if no heuristics were applied, but would be no faster than BWT-SW. In BWA-SW, we apply two heuristic rules to greatly accelerate this process. First, traversal on the query DAWG is carried in the outer loop, and therefore without finishing the dynamic programming, we know all the nodes in the reference prefix trie that match the query node with a positive score. Based on the observation that the true alignment tends to have a high alignment score, we can prune low-scoring matches at each node to restrict the dynamic programming around good matches only. The scale of dynamic programming can thus be dramatically reduced. It is possible for the true alignment to be pruned in this process, but in practice, this can be controlled by the use of heuristics and happens rarely, given long or high-quality query sequences. Secondly, BWA-SW only reports alignments largely non-overlapping on the query sequence instead of giving all the significant local alignments. It heuristically identifies and discards seeds contained in a longer alignment and thus saves computing time on unsuccessful seed extensions. 2.2 Notations and definitions 2.2.1 Suffix array and BWT Let Σ={ A ,  C ,  G ,  T } be the alphabet of nucleotides and $ be a symbol that is lexicographically smaller than all the symbols in Σ. Given a nucleotide sequence  X = a 1 … a n −1  with  a n −1 =$, let  X [ i ]= a i  be the  i -th symbol,  X [ i ,  j ]= a i … a j  a subsequence of  X  and  X i = X [ i ,  n −1] a suffix of  X . The suffix array  S  of  X  is a permutation of integers 0,…,  n −1 such that  S ( i )= j  if and only if  X j  is the  i -th lexicographically smallest suffix. The BWT of  X  is a permutation of  X , where  B [ i ]=$ if  S ( i )=0 and  B [ i ]= X [ S ( i )−1] otherwise. 2.2.2 Suffix array interval Given a sequence  W , the  suffix array interval  or  SA interval    of  W  is defined as
 
In particular, if  W  is an empty string,  R ( W )=1 and  . The set of the positions of all the occurrences of  W  is  . Let  C ( a )=#{0≤ j ≤ n −2: X [ j ]&lt; a } and  O ( a , i )=#{0≤ j ≤ i : B [ j ]&lt; a }, where #{·} calculates the cardinality (or size) of a set. Ferragina and Manzini ( 2000 ) proved that
 
and that   if and only if  aW  is a substring of  X . 2.2.3 FM-index The suffix array  S , array  C  and  O  suffice for the exact search of a pattern in  X . FM-index (Ferragina and Manzini,  2000 ) is a compressed representation of the three arrays, consisting of the compressed BWT string  B , auxiliary arrays for calculating  O , and part of the suffix array  S . BWA-SW, however, uses a simplified FM-index where we do not compress  B  and store part of the occurrence array  O  without auxiliary data structures. The simplified version is more efficient for DNA sequences with a very small alphabet. Details on the construction are presented in our previous paper (Li and Durbin,  2009 ). 2.2.4 Alignment An  alignment  is a tuple ( W 1 ,  W 2 ,  A ) where  W 1  and  W 2  are two strings and  A  is a series of copying, substitution, insertion and deletion operations which transform  W 2  into  W 1 . Insertions and deletions are  gaps . Gaps and substitutions are  differences . The  edit distance  of the alignment equals the total number of differences in  A . A score can be calculated for an alignment given a scoring system. We say  W 1  matches  W 2  if  W 1  and  W 2  can be aligned with a positive score, and in this case, we also say ( W 1 ,  W 2 ) is a match. A match ( W 1 ,  W 2 ) is said to be  contained  in ( W ′ 1 ,  W ′ 2 ) on the first sequence if  W 1  is a substring of  W ′ 1 . Similarly, we can define the ‘contained’ relationship between alignments (a stronger condition) and between an alignment and a match. 2.3 Prefix trie and prefix DAWG The  prefix trie  of string  X  is a tree with each edge labeled with a symbol such that the concatenation of symbols on the path from a leaf to the root gives a unique prefix of  X . The concatenation of edge symbols from a node to the root is always a substring of  X , called the string represented by the node. The  SA interval  of a node is defined as the SA interval of the string represented by the node. Different nodes may have an identical interval, but recalling the definition of SA interval, we know that the strings represented by these nodes must be the prefixes of the same string and have different lengths. The  prefix DAWG , of  X  is transformed from the prefix trie by collapsing nodes having an identical interval. Thus in the prefix DAWG, nodes and SA intervals have an one-to-one relationship, and a node may represent multiple substrings of  X , falling in a sequence where each is a prefix of the next as is discussed in the previous paragraph.  Figure 1  gives an example.
 Fig. 1. Prefix trie and prefix DAWG of string ‘ GOOGOL ’. ( A ) Prefix trie. Symbol ‘∧’ marks the start of a string. The two numbers in a node gives the SA interval of the node. ( B ) Prefix DAWG constructed by collapsing nodes with the identical SA interval. For example, in the prefix trie, three nodes has SA interval [4, 4]. Their parents have interval [1, 2], [1, 2] and [1, 1], respectively. In the prefix DAWG, the [4, 4] node thus has parents [1, 2] and [1, 1]. Node [4, 4] represents three strings ‘ OG ’, ‘ OGO ’ and ‘ OGOL ’ with the first two strings being the prefix of ‘ OGOL ’. (A) is modified from  Figure 1  in Li and Durbin ( 2009 ). 2.4 Aligning prefix trie against prefix DAWG We construct a prefix DAWG 𝒢( W ) for the query sequence  W  and a prefix trie 𝒯( X ) for the reference  X . The dynamic programming for calculating the best score between  W  and  X  is as follows. Let  G uv = I uv = D uv =0 when  u  is the root of 𝒢( W ) and  v  the root of 𝒯( X ). At a node  u  in 𝒢( W ), for each of its parent node  u ′, calculate
 
where  v ′ is the parent of  v  in 𝒯( X ), function  S ( u ′,  u ;  v ′,  v ) gives the score between the symbol on the edge ( u ′,  u ) and the one on ( v ′,  v ), and  q  and  r  are gap open and gap extension penalties, respectively.  G uv ,  I uv  and  D uv  are calculated with:
 
 
where pre( u ) is the set of parent nodes of  u .  G uv  equals the best score between the (possibly multiple) substrings represented by  u  and the (one) substring represented by  v . We say a node  v matches u  if  G uv &gt;0. The dynamic programming is performed by traversing both 𝒢( W ) and 𝒯( X ) in the reverse post-order (i.e. all parent nodes are visited before children) in a nested way. Noting that once  u  does not match  v ,  u  does not match any nodes descending from  v , we only need to visit the nodes close to the root of 𝒯( X ) without traversing the entire trie, which greatly reduces the number of iterations in comparison to the standard Smith–Waterman algorithm that always goes through the entire reference sequence. 2.5 Acceleration by the standard Smith–Waterman In comparison to the standard Smith–Waterman alignment whose time complexity is  O (| X |·| W |), BWA-SW has better time complexity since it is no slower than BWT-SW whose time complexity  O (| X | 0.628 | W |) (Lam  et al. ,  2008 ). This conclusion comes because for short sub-alignments we are considering multiple possible matches with a single  uv  comparison. However, the constant associated with each iteration is much larger due to the complex steps related to the traversal of prefix trie and prefix DAWG, which makes BWA-SW inefficient when we use BWA-SW to extend a unique alignment. A more efficient strategy would be to use BWA-SW to find partial matches and apply the Smith–Waterman algorithm to extend. In dynamic programming, we know the number of partial matches being considered at any pair because this can be calculated from the size of the SA interval. When  G uv  is good enough and the SA interval size of  v  is below a certain threshold (3 by default), we save the ( u , v ) pair, called a  seed interval pair , and do not go deeper from the  v  node in 𝒯( X ). By looking up the suffix array of  X  and  W , we can derive  seed matches , or simply  seeds , from seed interval pairs. These seeds are then extended by the Smith–Waterman algorithm later. If the entire query is a highly repetitive sequence, it will be aligned purely with the algorithm described in the last section without the Smith–Waterman extension. Because we are stopping the dynamic programming early to generate seeds, the global best alignment may contain multiple seeds and in practice this will tend to be the case for long alignments. Typically for 1 kb alignments there will be 10–20 seeds. Below we will take advantage of this observation to heuristically speed up the search. BWT-SW deploys a similar strategy in performing the dynamic programming between a sequence and a prefix trie to find seed matches followed by Smith–Waterman extension. The main difference from our algorithm is that BWT-SW initiates the Smith–Waterman alignment once the score is high enough, regardless of the SA interval size. Sometimes a repetitive sequence may match to thousands of places in the human genome and extending partial matches each time may be slow. 2.6 Heuristic accelerations 2.6.1 Z-best strategy The algorithm described so far is exact in that it is able to deliver the same results as the Smith–Waterman algorithm. Although it is much faster than the standard algorithm given a long reference sequence, it is not fast enough for aligning large-scale sequencing data. Closer investigation reveals that even for a unique 500 bp query sequence, a few million nodes in 𝒯( X ) may match the query with a positive alignment score. The majority of these nodes are random matches or matches in short low-complexity regions. Visiting all of them is wasteful. To accelerate alignment, we traverse 𝒢( W ) in the outer loop and 𝒯( X ) in the inner loop, and at each node  u  in 𝒢( W ) we only keep the top  Z  best scoring nodes in 𝒯( X ) that match  u , rather than keep all the matching nodes. This heuristic strategy is called  Z-best . Of course, when we apply the  Z -best strategy, we could miss a seed contained in the true alignment when a false match has a higher score. But if the query is nearly identical to the reference, this happens less often. In addition, if the true alignment is long and contains many seeds, the chance of all seeds being false is very small. On both simulated and real data ( Section 3 ), we find even  Z =1 works well with high-quality 200 bp reads (&lt;5% sequencing error rate). Increasing  Z  to 10 or higher marginally improves the accuracy but greatly reduces the alignment speed. To reduce alignment errors, we also align the reverse query sequence to the reverse reference sequence, namely reverse–reverse alignment, in addition to the forward–forward alignment. Ideally, the forward–forward and the reverse–reverse alignments should yield identical outcomes, but if a seed in the true alignment has a low-scoring suffix (or prefix), the forward–forward (or reverse–reverse) alignment is likely to miss it, while combining the two rounds of alignment reduces the chance. Moreover, if the best alignment from the forward–forward alignment contains many seed matches, the chance of it being false is also small. In implementation, we do not apply the reverse–reverse alignment if the best alignment contains, by default, 5 or more seeds. 2.6.2 Filtering seeds before the Smith–Waterman extension Like BLAST, both BLAT and SSAHA2 report all significant alignments or typically tens of top-scoring alignments, but this is not the most desired output in read mapping. We are typically more interested in the best alignment or best few alignments, covering each region of the query sequence. For example, suppose a 1000 bp query sequence consists of a 900 bp segment from one chromosome and a 100 bp segment from another chromosome; 400 bp out of the 900 bp segment is a highly repetitive sequence. For BLAST, to know this is a chimeric read we would need to ask it to report all the alignments of the 400 bp repeat, which is costly and wasteful because in general we are not interested in alignments of short repetitive sequences contained in a longer unique sequence. On this example, a useful output would be to report one alignment each for the 900 bp and the 100 bp segment, and to indicate if the two segments have good suboptimal alignments that may render the best alignment unreliable. Such output simplifies downstream analyses and saves time on reconstructing the detailed alignments of the repetitive sequence. In BWA-SW, we say two alignments are  distinct  if the length of the overlapping region on the query is less than half of the length of the shorter query segment. We aim to find a set of distinct alignments which maximizes the sum of scores of each alignment in the set. This problem can be solved by dynamic programming, but as in our case a read is usually aligned entirely, a greedy approximation would work well. In the practical implementation, we sort the local alignments based on their alignment scores, scan the sorted list from the best one and keep an alignment if it is distinct from all the kept alignments with larger scores; if alignment  a 2  is rejected because it is not distinctive from  a 1 , we regard  a 2  to be a suboptimal alignment to  a 1  and use this information to approximate the mapping quality ( Section 2.7 ). Because we only retain alignments largely non-overlapping on the query sequence, we might as well discard seeds that do not contribute to the final alignments. Detecting such seeds can be done with another heuristic before the Smith–Waterman extension and time spent on unnecessary extension can thus be saved. To identify these seeds, we chain seeds that are contained in a band (default band width 50 bp). If on the query sequence a short chain is fully contained in a long chain and the number of seeds in the short chain is below one-tenth of the number of seeds in the long chain, we discard all the seeds in the short chain, based on the observation that the short chain can rarely lead to a better alignment than the long chain in this case. Unlike the  Z -best strategy, this heuristic does not have a noticeable effect on alignment accuracy. On 1000 10 kb simulated data, it halves the running time with no reduction in accuracy. 2.7 Approximating mapping quality Li,H.  et al.  ( 2008 ) introduced the concept of mapping quality to estimate the probability of a query sequence being placed at a wrong position. If an alignment algorithm guarantees to find all local alignments, mapping quality is determined by these local alignments only. However, as BWA-SW deploys heuristic rules, the chance of producing a wrong alignment is also related to the heuristics. To estimate the mapping quality of a BWA-SW alignment, we fit an empirical formula: 250· c 1 · c 2 ·( S 1 − S 2 )/ S 1 , where  S 1  is the score of the best alignment,  S 2  the score of the second best alignment,  c 1  equals 1 if the alignment covers more than four seeds or 0.5 otherwise, and  c 2  equals to 1 if the best alignment is found by both forward–forward and reverse–reverse alignments or 0.2 otherwise. 3 RESULTS 3.1 Implementation The BWA-SW algorithm is implemented as a component of the BWA program (Li and Durbin,  2009 ), which is distributed under the GNU general public license (GPL). The implementation takes a BWA index and a query FASTA or FASTQ file as input and outputs the alignment in the SAM format (Li  et al. ,  2009 ). The query file typically contain many sequences (reads). We process each query sequence in turn, using multiple threads if applicable. Memory usage is dominated by the FM-index, about 3.7 GB for the human genome. Memory required for each query is roughly proportional to the sequence length. On typical sequencing reads, the total memory is &lt;4 GB; on one query sequence with 1 million base pairs (Mbp), the peak memory is 6.4 GB in total. In the implementation, we try to automatically adjust parameters based on the read lengths and sequencing error rates to make the default settings work well for inputs of different characteristics. This behavior is convenient to users who are not familiar with the algorithm and helps performance given the reads of mixed lengths and error rates. 3.2 Evaluation on simulated data On simulated data, we know the correct chromosomal coordinates from the alignment and the evaluation is straightforward. 3.2.1 Overall performance Table 1  shows the CPU time, fraction of confidently aligned reads and alignment error rates for BLAT (v34), BWA-SW (version 0.5.3) and SSAHA2 (version 2.4) given different read lengths and error rates. Unless necessary, we tried to use the default command-line options of each aligner. Fine tuning the options based on the characteristics of the input data may yield better performance.
 Table 1. Evaluation on simulated data Program Metrics 100 bp 200 bp 500 bp 1000 bp 10 000 bp 2% 5% 10% 2% 5% 10% 2% 5% 10% 2% 5% 10% 2% 5% 10% BLAT CPU sec 685 577 559 819 538 486 1078 699 512 1315 862 599 2628 1742 710 Q20% 68.7 25.5 3.0 92.0 52.9 7.8 97.1 86.3 21.4 97.7 96.4 39.0 98.4 99.0 94.0 errAln% 0.99 2.48 5.47 0.55 1.72 4.55 0.17 1.12 4.41 0.01 0.52 3.98 0.00 0.00 1.28 BWA-SW CPU sec 165 125 84 222 168 118 249 172 152 234 168 150 158 134 120 Q20% 85.1 62.2 19.8 93.8 88.7 49.7 96.1 95.5 85.1 96.9 96.5 95.0 98.4 98.5 98.1 errAln% 0.01 0.05 0.17 0.00 0.02 0.13 0.00 0.00 0.04 0.00 0.00 0.00 0.00 0.00 0.00 SSAHA2 CPU sec 4872 7962 9345 1932 2236 5252 3311 8213 6863 1554 1583 3113 – – – Q20% 85.5 83.8 78.2 93.4 93.1 91.9 96.6 96.5 96.1 97.7 97.6 97.4 – – – errAln% 0.00 0.01 0.19 0.01 0.00 0.01 0.00 0.01 0.04 0.00 0.00 0.00 – – – Approximately 10 000 000 bp data of different read lengths and error rates are simulated from the human genome. Twenty percent of errors are indel errors with the indel length drawn from a geometric distribution (density: 0.7·0.3 l −1 ). These simulated reads are aligned back to the human genome with BLAT (option -fastMap), BWA-SW and SSAHA2 (option −454 for 100 and 200 bp reads), respectively. The aligned coordinates are then compared with the simulated coordinates to find alignment errors. In each cell in this table, the three numbers are the CPU seconds on a single-core of an Intel E5420 2.5 GHz CPU, percent alignments with mapping quality greater than or equal to 20 (Q20), and percent wrong alignments out of Q20 alignments. SSAHA2 and BWA-SW report mapping quality; BLAT mapping quality is estimated as 250 times the difference of the best and second best alignment scores divided by the best alignment score (essentially the same calculation as the one for BWA-SW). 
 From  Table 1 , we can see that BWA-SW is clearly the fastest, several times faster than BLAT and SSAHA2 on all inputs, and its speed is not sensitive to the read length or error rates. The accuracy of BWA-SW is comparable with SSAHA2 when the query is long or has low error rate. Given short and error-prone reads, SSAHA2 is more accurate, although it has to spend more time on aligning such reads. SSAHA2 is not tested on the 10 kb reads because it is not designed for this task initially and thus does not perform well. BLAT with the -fastMap option is faster than SSAHA2, but less accurate. Under the default option, BLAT is several to tens of times slower than SSAHA2. The accuracy is higher in comparison to the -fastMap mode, but still lower than that of BWA-SW in general (data not shown). On memory, both BWA-SW and BLAT uses ∼4 GB memory. SSAHA2 uses 2.4 GB for ≥500 bp reads with the default option, and 5.3 GB for shorter reads with the −454 option which increases the number of seed sequences stored in the hash table and increases the memory as a result. In addition, BWA-SW supports multi-threading and thus may take less memory per CPU core if it is run on a multi-core computer. SSAHA2 and BLAT do not support multi-threading at present. 3.2.2 Chimera detection We first study the behavior of each aligner given a chimeric read. To do so, we fabricated two chimeric reads with both consisting of one 1000 bp piece from one chromosomal position and one 300 bp piece from another position. The main difference between the two reads is that the 1000 bp piece in the second read has a ∼750 bp repetitive sequence, while the first read is highly unique. When we align the two chimeric reads to the human genome, BWA-SW reports four alignments, one for each piece, as is desired. The latest SSAHA2 fails to find the alignment of the 300 bp pieces in both reads, although it is able to find the alignments if we align the 300 bp piece as an individual read. An older version (1.0.9) is able to align the 300 bp piece in the first read by default, but for the second read, we need to switch to a more thorough but much slower configuration that reports all the hits to the 750 bp repeat. BLAT with -fastMap does not find the alignment of the 300 bp piece for the second read. On the two examples, only BWA-SW has sufficient power to detect chimera. Furthermore, BWA-SW rarely produces high-quality false chimeric alignments. For example, given the 10 000 1 kb reads with 10% errors but without chimera in simulation, BWA-SW predicts 18 chimeric reads. The mapping quality of the wrongly aligned pieces on these reads is only 2.4 (maximum 11), implying that BWA-SW is aware that these chimera are unreliable. As is expected, BWA-SW produces fewer false chimeric reads given lower base errors. 3.3 Evaluation on real data Evaluation on real data is complicated by the lack of a ground truth. However, it is still possible to evaluate the relative accuracy by comparing the results from two aligners using the principle that the true alignment tends to have a considerably higher alignment score, because most errors arise from failing to find a seed. Suppose we align a read using two aligners  A  and  B  and get different results. If both  A  and  B  give low mapping qualities, the alignment is ambiguous and it does not matter if either alignment is wrong. If  A  gives high mapping quality and the  A  alignment score is worse than  B ,  A  alignment is probably wrong; even if  A  alignment score is just a little better than  B ,  A  alignment is not reliable and the high mapping quality given by  A  is still questionable. In practice, defining ‘a little better’ alignment score requires to set a arbitrary threshold on the score difference and therefore this evaluation method is approximate. Table 2  gives a summary of 454 reads which are mapped by only one aligner or mapped to different places, and are assigned a mapping quality greater or equal to 20 by either BWA-SW or SSAHA2. We can see that BWA-SW tends to miss short alignments with high error rates (946 of them), which agrees with the evaluation on simulated data. SSAHA2 misses alignments for a different reason. On 1188 reads, SSAHA2 produces obviously wrong alignments. It is aware that these alignments are wrong by assigning low mapping quality, but the true alignments are missed anyway.
 Table 2. Summary of alignments inconsistent between the BWA-SW and SSAHA2 on real data Condition Count BWA-SW SSAHA2 avgLen avgDiff avgMapQ avgLen avgDiff avgMapQ BWA-SW≥20; SSAHA2 unmapped 0 – – – – – – BWA-SW≥20 plausible; SSAHA2&lt;20 1188 398.2 1.3% 178.4 198.3 13.1% 3.9 BWA-SW≥20 questionable 40 183.0 7.8% 41.2 280.3 9.4% 2.4 SSAHA2≥20; BWA-SW unmapped 946 – – – 75.4 6.3% 51.2 SSAHA2≥20 plausible; BWA-SW&lt;20 47 129.0 9.3% 2.5 200.5 8.8% 34.4 SSAHA2≥20 questionable 185 400.2 1.7% 13.4 399.2 2.9% 216.4 A total of 137 670 454 reads uniformly selected from  SRR003161  were mapped against the human genome with BWA-SW and SSAHA2, respectively. A read is said to be aligned inconsistently if the leftmost coordinates of the BWA-SW and SSAHA2 alignment differs by over 355 bp, the average read length. A score, which equals to the number of matches minus three multiplied by the number of differences (mismatches and gaps) in the aligned region, is calculated for each alignment. A BWA-SW alignment is said to be  plausible  if the score derived from the BWA-SW alignment minus the one derived from the SSAHA2 alignment of the same read is greater than or equal to 20 (i.e. the BWA-SW alignment is sufficiently better); otherwise the BWA-SW alignment is said to be  questionable . Plausible and questionable SSAHA2 alignments are defined in a similar manner. In the table, ‘BWA-SW≥20’ denotes the BWA-SW alignments with mapping quality higher than 20. In all, BWA-SW misses 993 (=946 + 47) alignments which SSAHA2 aligns well, while SSAHA2 misses 1188; 40 BWA-SW Q20 alignments and 185 SSAHA2 Q20 alignments are possibly wrong. 
 For both aligners, most wrong alignments are caused by overlooking alignments with a similar score to the best reported alignment. For example, SSAHA2 aligns read  SRR003161.1261578  to X chromosome with mapping quality 244 and BWA-SW aligns it to chromosome 2 with identical alignment length and edit distance. The existence of two best scoring alignments means the read cannot be uniquely placed and a mapping quality as high as 244 is inaccurate. SSAHA2 gives this high mapping quality probably because it overlooks the match on chromosome 2. And in this specific example, BWA-SW properly gives a mapping quality zero, although it may overlook alternative matches in other examples. On simulated 100 and 200 bp reads, SSAHA2 with the −454 option delivers better alignments than BWA-SW. On this real dataset, BWA-SW is more accurate possibly because the average read length is relatively long (355 bp). To confirm this speculation, we compared the two aligners on 99 958 reads from run  SRR002644  with average read length 206 bp. This time BWA-SW misses 1092 SSAHA2 Q20 alignments and produces 39 questionable alignments; SSAHA2 misses 325 and produces 10 questionable ones. SSAHA2 is more accurate on this shorter dataset, although it is nine times slower than BWA-SW and uses 40% more memory. 4 DISCUSSION BWA-SW is an efficient algorithm for aligning a query sequence of a few hundred base pairs or more against a long reference genome. Its sensitivity and specificity tend to be higher given a long query or a query with low error rate, and on such query sequences, the accuracy of BWA-SW is comparable with the most accurate aligner so far. Furthermore, BWA-SW is able to detect chimera, potentially caused by structural variations or reference misassemblies, which may pose a challenge to BLAT and SSAHA2. BWA-SW, BLAT and SSAHA2 all follow the seed-and-extend paradigm. The major difference comes from the seeding strategy. BLAT and SSAHA2 identify short exact matches as seeds, typically of length 11 or 12 bp. For  k -mer seeding between two sequences of length  L  and  l , respectively, the expected number of seeds is  L · l /4 k , or of the order of 10 5  for alignment against the human genome. Extending these seeds each with the Smith–Waterman algorithm is expensive. To reduce unnecessary seed extension, both BLAT and SSAHA2 use non-overlapping seeds by default and require multiple seed matches, which should work well for random sequences, but still involves many seed extensions in highly repetitive regions. BWA-SW resolves this issue by using a few long gapped seeds in unique regions. On real biological data, it saves many unnecessary seed extensions and leads to a better overall performance. However, to reduce time when identifying long seeds, BWA-SW only maintains a very small fraction of the dynamic programming matrix, which may miss all seeds for true matches. This heuristic is the major source of alignment errors especially for short queries when there are only few valid unique seeds between the sequences to be aligned. On long alignments, fortunately, the chance of missing all seeds is small. We have shown BWA-SW works equally well as SSAHA2. BWA-SW differs from BWT-SW in several aspects. First of all, BWT-SW guarantees to find all local matches, whereas BWA-SW is a heuristic algorithm which may miss true hits but is much faster. Secondly, BWA-SW aligns two FM-indices while BWT-SW aligns one sequence and a FM-index. Building a prefix DAWG for the query sequences potentially helps to avoid repeatedly aligning identical substrings in the query, and thus improves the theoretical time complexity. Thirdly, BWA-SW traverses the reference prefix trie in the inner loop while BWT-SW loops through the query sequence in the inner loop. Without heuristics, the BWA-SW approach would hurt performance because we have to trade speed for memory in traversing the reference prefix trie, and it would be more efficient to traverse it in the outer loop. Nonetheless, applying the  Z -best strategy requires to know the top-scoring reference nodes matching a query substring without finishing the dynamic programming and thus only works when the reference is traversed in the inner loop. Fourthly, BWA-SW only reports alignments largely non-overlapping on the query sequence, while BWT-SW, like BLAST, reports all statistically significant alignments. BWA-SW retains key information of alignments and generates much smaller and more convenient output. For BWT-SW, end users usually need to post-process the results to filter out many alignments of little interest to them. In all, BWA-SW is tuned toward practical usefulness given large-scale real data. The high speed of BWA-SW largely comes from two strategies: the use of FM-indices and the suppression of short repetitive matches contained in a better match. While the first strategy is not applicable to hash table-based algorithms such as SSAHA2 and BLAT, the second strategy could be implemented in such programs and may substantially accelerate them by saving much time on the construction of repetitive alignments. And although the use of BWT reduces unnecessary alignments in repeats, each BWT operation comes with a large constant in comparison with a hash table look up. It is still possible that hash table-based algorithms could be faster than BWA-SW if they incorporated some of these features. 
          Supplementary Material 
          
             
                [Supplementary Data] 
             
             
             
          
       </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Presenting and sharing clinical data using the eTRIKS Standards Master Tree for tranSMART</Title>
    <Doi>10.1093/bioinformatics/bty809</Doi>
    <Authors>Barbosa-Silva Adriano, Bratfalean Dorina, Gu Wei, Satagopam Venkata, Houston Paul, Becnel Lauren B, Eifes Serge, Richard Fabien, Tielmann Andreas, Herzinger Sascha, Rege Kavita, Balling Rudi, Peeters Paul, Schneider Reinhard, Valencia Alfonso</Authors>
    <Abstract/>
    <Body>1 Introduction The European Translational Information and Knowledge Management Services (eTRIKS,  https://www.etriks.org/, 2017 ) is tasked with providing tools and services to support data management and analysis for &gt;60 diverse biomedical research projects which have been funded by the Innovative Medicines Initiative (IMI). As Europe’s largest public-private partnership, IMI funds projects ranging from molecular and systems biology to clinical trials and full translational research projects. The community translational research system under use is tranSMART ( Athey  et al. , 2013 ;  Dunn  et al. , 2017 ), first developed by the pharma industry, and then gifted to a global translational research development community. The tranSMART system has undergone extensive development extended by its own community and the eTRIKS project, which has focused on an implementation that serves IMI projects users based in the European Union (EU). The flexibility and capability of tranSMART is well presented in a recent paper showing the availability of workflows within a sandbox environment ( Satagopam  et al. , 2016 ). tranSMART serves as the central knowledge management system for eTRIKS, while other tools and complimentary services applicable to the data value chain, such as data harmonization, sharing, analysis, visualization and preservation, have been developed. To expedite medical breakthroughs the sharing of clinical research data is vital owing to legislative incentives and increased public pressure, many clinical trial registries are expanding their remit to share not only basic summary trial registration data but also results. Wider data sharing is one way of tackling reporting bias by increasing visibility of successful studies as well as failed ones. Additionally, data standards play a pivotal role in tackling the omnipresent problem of reproducibility. Begley  et al.  reproduced 53 experiments from landmark publications to find 47 out of 53 could not be replicated; a very worrying trend for preclinical studies that are used as the scientific basis for target identification for new drug development ( Begley and Ellis, 2012 ). The Data FAIRport initiative in 2014 prescribed a set of guiding principles known as FAIR: Findable, Accessible, Interoperable, Reusable which should be applied where data is deemed scientifically valuable ( Wilkinson  et al. , 2016 ). Those principles have gained official recognition from G20, NIH and the Directorate General for Research and Innovation of the European Commission. The consistent application of common semantics and data structures, as outlined within data standards, is a key factor to ensure interoperability and reusability of data. The eTRIKS Data Standards Work Package created a Standards Starter Pack ( https://doi.org/10.5281/zenodo.50398/ , 2016), which outlines the FAIR principles and recommendations for the main clinical and genomic standards as well as supporting vocabularies and minimum information guidelines that should be applied in the entire translational research landscape. eTRIKS has also produced the IMI Data catalogue which centralizes metadata of ongoing and past IMI projects. It is part of the service that eTRIKS provides in its key knowledge management performance with a focus on the findability of project level study description metadata. Furthermore, this well received initiative facilitates broader sharing and accessibility of data ( http://datacatalog.elixir-luxembourg.org/ckan/, 2017 ). For clinical research data, The Clinical Data Interchange Standards Consortium (CDISC,  https://www.cdisc.org/, 2018 ) data standards have been implemented in over 90 countries, and are now mandated by Food and Drug Administration of the United States ( FDA, 2014 ) and Pharmaceuticals and Medical Devices Agenda (PMDA) in Japan ( https://www.pmda.go.jp/files/000206449.pdf, 2018 ) in order to increase the uptake of data standards, which, when applied, contribute to higher data quality. The lack of implementing standards will render datasets from different cohorts inadequate when integrating with complementary research data for meta-analyses ( Elefsinioti  et al. , 2016 ). A recent paper by the American College of Medical Genetics and Genomics ( Acmg, 2017 ) discussed the importance of using the information from one patient cohort to benefit other patients. The ACMG’s framework for data sharing will work best if standards are implemented within the framework, as within tranSMART, and datasets are gathered by utilizing those standards from the beginning of the research, as is also recommended by CDISC. 2 Implementation The eTRIKS Standard Master Tree is based on the standards for clinical data representation developed by CDISC, mainly the Study Data Tabulation Model (SDTM) standard. The proposal of eTRIKS was to create a hierarchical navigation tree in which the raw data, collected at the multiple cohorts, should be promptly mapped to the elements of this tree so that data are loaded automatically with the correct topology into tranSMART i2b2 (Informatics for Integrating Biology and the Bedside) framework. The requirement for this is that all the data collected from a patient will be organized and formatted using the SDTM model. SDTM modeling increases the ability to compare information among systems and/or organizations, whilst also decreasing the time to initiate a new research study. The use of these data standards improves the data quality, their interoperability and their management, which allows easier, faster and more reliable data aggregation. The eTRIKS Standard Master Tree presents the clinical data within tranSMART i2b2. eTRIKS has radically updated the original tranSMART engine that sorts and presents the clinical data within the system. Users can choose to map their clinical data content to a favorite terminology prior to the SDTM modelling using global standards such as OMICS, NCI ( https://www.cancer.gov/digital-standards, 2017 ) or LOINC ( https://loinc.org/, 2017 ), as long as the SDTM variable names as maintained. Further, the clinical data is mapped to a ‘clinical mapping file’, which requires a good working knowledge of the CDISC foundational standards, in order to represent the SDTM structure of the clinical data correctly in the hierarchy of the tranSMART i2b2 repository ( Abend  et al. , 2009 ). In practice if one thinks about the outcome of a ‘glucose test’, this test may be named ‘sugar test’ or ‘glucose test’ in different differing cohorts, which may be well understood by experts but not a machine as the same concept. The use of standard name ‘Glucose Tolerance Test’ (NCBI’s, MeSH Unique ID: D005951) would avoid any confusion or wrong interpretation and enable data query across cohorts. Further to this, considering that the metabolite ‘glucose’ could be measured in different samples (e.g. blood, urine), the test results could be reported in different units (mg/dl or mmol/l) and/or the test could be performed at different periods of the time (screening, visit 1, visit 2, etc.), error prone aspects during the data analysis. If the problem is proposed, ‘How to standardize the manner by which this information should be organized and formatted for effective and precise cohorts comparisons?’ One answer should be: ‘Use a Standard Master Ontology Tree’ or in this case, the eTRIKS Standard Master Tree. The application of this tool coupled with a good application of controlled vocabularies will increase greatly the Reusability and Interoperability Principles mentioned above. In the ‘Glucose Tolerance Test’ example, upon mapping to the tranSMART Standard Master Tree, the outcome of this test would already be represented as displayed in  Figure 1A  below. The test result is reported in this example by means of 14 variables (columns A-N) for the subject CDISC01.100008 (column C). Note that column G collects one variable called LBTEST (Lab Test Examination Name), which is filled with the standard value ‘Glucose’ and another variable LBSPEC (Specimen Type) is used to distinguish ‘BLOOD’ from ‘URINE’ samples. In terms of readout values, the variable LBORRES (Result or Finding in Original Units) records the original values as collected reported units in LBORRESU (Original Units) the unit itself (e.g. mg/dl). The example shows results converted to numeric type and this reported value to a standard unit, which is achieved by using the pair variables LBSTRESN (Numeric Result/Finding in Standard Units) and LBSTRESU (Standard Units), for values and units (e.g. mmol/l), respectively. The SDTM Implementation Guide provides a comprehensive description including four sessions: 1—Overview of topics for specific general observation class associated with specific domains, 2—Specification for table of variables, 3—Rules for correct implementation of standards and 4—Examples.
 Fig. 1. Content representation of the eTRIKS Master Tree Package. ( A ) SDTM data file for one patient (USUBJID) for the LB domain. ( B ) SMOT_Lite definition session for the LB domain. ( C ) Hierarchical (i2b2) tree created for the LB domain and displayed in the tranSMART web app To avoid the all too common pitfalls of redundant data eTRIKS developed the Standard Master Tree, using the comprehensive SDTM domain structure, to support and give structure and context to the data so it can be easily identified. The Standard Master Tree follows a basic and easy-to-understand logic, which was built upon the premise of tranSMART rules for data loading. This means that multiple data collected for one patient for the same domain (e.g. Laboratory Test Results—LB) should be distinguished based on  Data Labels . This way, for the LB domain, results of ‘Glucose’ and ‘Creatinine’ tests for example, could be loaded in the same run. Moreover, multiple results for the same test should also be distinguished based of the  Visit Names . Respecting these two basic rules, any results from any sort of laboratory tests and even results for any other domains, can be easily represented in tranSMART via the Standard Master Tree. 3 Features The eTRIKS Standard Master Tree model consists of a package of three main components: (i) a CDISC clinical dataset reported as define.xml metadata and converted in . txt  tabulate files composing of 16 SDTM domains represented as review data as collected for 4 fictitious subjects ( Fig. 1A ); (ii) the tranSMART standard master ontology tree as TM SMOT-SDTM_lite.txt definition file, where all the information concerned about the correct positioning of the SDTM variables can be found ( Fig. 1B ); and the (iii) Mapper script where users can map their data files to the TM SMOT-SDTM_Lite definition, avoiding manual work. The script reads a target directory containing the input SDTM files and maps all the collected variables against the SMOT-SDMT_Lite.txt master tree file mentioned above. This is achieved with a single command line: ‘php mapper.php SMOT_Lite.txt CDISC01_ClinicalData’ (further details are explained on the package’s README file).  Figure 1A  depicts an example for the Glucose test of one such subject.  Figure 1B  depicts part of the TM SMOT-SDTM_Lite file where definitions for the domain LB is displayed (Note the seven columns required for the annotation of each of the SDTM variables used in this domain). This information can be found easily on the SDTM implementation guide as should be adopted by the data curators. Finally,  Figure 1C  displays the graphical hierarchy tree, known as tranSMART i2b2 tree, where the loaded data can be further queried and used to create comparison subsets on the tranSMART i2b2 web app, these can be visualized in a sandbox implementation available at  http://public.etriks.org/transmart/datasetExplorer  under the eTRIKS—Master Tree branch. The strategy for clinical research data standards representation proposed above offers a readily available method to integrate multiple translational research datasets while meeting the Interoperability and Reusability aspects of the FAIR principles. Once the data is within the eTRIKS Standard Master Tree, it can then take advantage of the tranSMART environment, where it will receive a unique study and server specific identifier and the metadata can be given greater and essential specificity. With an effective tranSMART search tool where multiple datasets and/or studies can be pooled and queried, coupled with an entry within the eTRIKS data catalogue the data has undergone FAIR-ification to a satisfactory degree. Now the data is Findable and also Accessible, and it can begin its hopefully long life adding scientific value to any number of future studies or aggregated data comparisons. 4 Conclusion The tranSMART Standard Master Tree presented here adds to other efforts to make other software data interoperable with tranSMART. Projects such as ‘ODM to i2b2’ converts data stored in XML/ODM based systems such as OpenClinica and REDCap into i2b2 format ( https://github.com/CTMM-TraIT/trait_odm_to_i2b2 , 2018); and ‘REDCap2SDTM’ converts electronic data capture system data to SDTM ( Yamamoto  et al. , 2017 ). Taken together, this software could benefit from the Master Tree concept in order to standardize the manner that SDTM studies should appear within a tranSMART navigation tree to users. If the tools and processes above are adopted in the scope of the NIH funded projects, it will contribute greatly to creating an overseas bridge for data sharing initiatives with the EU/EFPIA-funded (IMI) translational medicine research projects, of which over 60 are being supported by the eTRIKS project. While not all of the eTRIKS supported projects have implemented the tranSMART Standards Master Tree they have all received the appropriate guidance and advice from eTRIKS experts or as laid out in the eTRIKS standards starter pack. Tremendous curation efforts were necessary to guarantee that IMI data was collected in good quality, once that, frequently, the big challenge for translational research projects lies on the quality of the data itself, not only its metadata. The adoption of the technologies and standards developed and presented in this paper will support a significant step towards a position where IMI data can be shared, and the findings reproduced to benefit the health care research community, allowing a standardized representation of SDTM data across multiple tranSMART servers. The eTRIKS Standard Master Tree package can be downloaded at  https://doi.org/10.5281/zenodo.1009098 . </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Drug–target interaction prediction through domain-tuned network-based inference</Title>
    <Doi>10.1093/bioinformatics/btt307</Doi>
    <Authors>Alaimo Salvatore, Pulvirenti Alfredo, Giugno Rosalba, Ferro Alfredo</Authors>
    <Abstract>Motivation: The identification of drug–target interaction (DTI) represents a costly and time-consuming step in drug discovery and design. Computational methods capable of predicting reliable DTI play an important role in the field. Recently, recommendation methods relying on network-based inference (NBI) have been proposed. However, such approaches implement naive topology-based inference and do not take into account important features within the drug–target domain.</Abstract>
    <Body>1 INTRODUCTION Detecting and verifying new connections among drugs and targets is a costly process. From a historical point of view, the pharmaceutical chemist’s approach has been commonly focused on the development of compounds acting against particular families of ‘druggable’ proteins ( Yildirim  et al. , 2007 ). Drugs act by binding to specific proteins, hence changing their biochemical and/or biophysical activities, with many consequences on various functions. Furthermore, because proteins operate as part of highly interconnected cellular networks (i.e. the interactome networks), the ‘one gene, one drug, one disease’ paradigm has been challenged in many cases ( Hopkins, 2008 ). For this reason, the concept of polypharmacology has been raised for those drugs acting on multiple targets rather than a single one ( Hopkins, 2008 ). These polypharmacological features of drugs bring a wealth of knowledge and enable us to understand drug side effects or find their new uses, namely, drug repositioning ( Ashburn and Thor, 2004 ;  Boguski  et al. , 2009 ). Nevertheless, many interactions are still unknown, and given the significant amount of resources needed for  in situ  experimentation, it is necessary to develop algorithmic methodologies allowing the prediction of new and significant relationships among elements interacting at the process level. In the literature, several computational tools have been proposed to afford the problem of DTI prediction and drug repositioning. Traditional methods rely either on ligand-based or receptor-based approaches. Among ligand-based methods, we can cite quantitative structure-activity relationships, and a similarity search-based approach ( Gonzalez-Daz  et al. , 2011 ;  Keiser  et al. , 2007 ). On the other hand, receptor-based methods, such as reverse docking, have also been applied in drug–target (DT) binding affinity prediction, DTI prediction and drug repositioning ( Ashburn and Thor, 2004 ;  Li  et al. , 2006 ;  Xie  et al. , 2011 ). However, the latter have the shortcoming that cannot be used for targets whose 3D structures are unknown. Recently, much attention has been devoted to network-based and phenotype-based approaches. Most of these methods rely on the successful idea of using bipartite graphs. In  Yildirim  et al.  (2007) , a bipartite graph linking US Food and Drug Administration-approved drugs to proteins by DT binary associations is exploited.  Campillos  et al.  (2008)  identified new DTIs using side effect similarity. Iorio  et al.  (2010)  make use of transcriptional responses, predicted and validated new drug modes of action and drug repositioning. Recently,  Dudley  et al.  (2011)  and  Sirota  et al.  (2011)  have presented drug repositioning methods exploiting public gene expression data. Furthermore,  Yamanishi  et al.  (2008)  developed a bipartite graph learning method to predict DTI by integrating chemical and genomic data. Cheng  et al.  (2012)  present a technique based on network-based inference (NBI) implementing a naive version of the algorithm proposed by  Zhou  et al.  (2007) . All these results clearly show the good performance of this approach. On the other hand, knowledge about drug and protein domain is not properly exploited. van Laarhoven  et al.  (2011 ) use a machine learning method starting from a DTI network to predict new ones with high accuracy. The calculation of the new interactions is done through the regularized least squares algorithm. The regularized least squares algorithm is trained using a kernel (GIP—Gaussian interaction profile) that summarizes the information in the network. The authors developed variants of the original kernel by taking into account chemical and genomic information. This improved the accuracy, in particular for small datasets. Chen  et al.  (2012)  introduced their Network-based Random Walk with Restart on the Heterogeneous network (NRWRH) algorithm predicting new interactions between drugs and targets by means of a model based on a random walk with a restart in a ‘heterogeneous’ network. The model is constructed by extending the network of DTI interactions with drug–drug and protein–protein similarity networks. This methodology shows excellent performance in predicting new interactions. However, its disadvantage is due to its random nature, mainly caused by the initial probabilities selection. Mei  et al.  (2013)  proposed the Bipartite Local Model-Interaction-profile Inferring (BLM-NII) algorithm. Interactions between drugs and targets are deduced by training a classifier (i.e. support vector machine or regularized least square). This is achieved by exploiting interaction information, drug and target similarities. This classifier is appropriately extended to include knowledge on new drug/target candidates. This is used to predict the new target probability of a specific drug. The algorithm is highly reliable in predicting interactions between new drug/target candidates. On the other hand, its capability of training several distinct classifiers to obtain the final model is not strong enough. In this present article, we propose a novel method called domain tuned-hybrid (DT-Hybrid). It extends the NBI algorithm proposed in  Zhou  et al.  (2007)  and applied in  Cheng  et al.  (2012)  by adding application domain knowledge. Similarity among drugs and targets is plugged into the model. Despite its simplicity, the technique provides a complete and functional framework for  in silico  prediction of drug and target relationships. To demonstrate the reliability of the method, we conducted a wide experimental analysis using four benchmark datasets drawn from DrugBank. We compared our method with the one proposed by  Chen  et al. , 2012 . The experiments clearly show that DT-Hybrid overcomes the problems shown by the naive NBI algorithm, and it is capable of producing higher quality predictions. 2 METHODS 2.1 Algorithm The method we propose is based on the recommendation technique presented by  Zhou  et al.  (2007)  and extended by  Zhou  et al.  (2010) . Let   be a set of small molecules (i.e. biological compounds, molecules), and   a set of targets (i.e. genes, proteins); the X-T network of interactions can be described as a bipartite graph   where  . A link between  x i  and  t j  is drawn in the graph when the structure  x i  is associated with the target  t j . The network can be represented by an adjacency matrix  , where   if  x i  is connected to  t j ; otherwise,  . Zhou  et al.  (2010)  proposed a recommendation method based on the bipartite network projection technique implementing the concept of resources transfer within the network. Given the bipartite graph defined above, a two-phase resource transfer is associated with one of its projections: at the beginning, the resource is transferred from nodes belonging to  T  to those in  X , and subsequently the resource is transferred back to the  T  nodes. This process allows us to define a technique for the calculation of the weight matrix ( ) in the projection as follows:
 (1) 
where Γ determines how the distribution of resources takes place in the second phase, and   is the degree of the  x  node in the bipartite network. By varying the Γ function, we obtain the following algorithms ( Table 1 ):
 NBI, introduced by  Zhou  et al.  (2007)  and used by  Cheng  et al.  (2012)  for the prediction of the interactions between drugs and proteins; HeatS, introduced by  Zhou  et al.  (2010) ; Hybrid N+H, introduced by  Zhou  et al.  (2010) , in which the functions defined in NBI and HeatS are combined in connection with a parameter called λ; DT-Hybrid, introduced here, is an enhanced version of the Hybrid algorithm in which previous domain-dependent biological knowledge is plugged into the model through a similarity matrix. 
 Table 1. List of algorithms with the associated Γ functions Algorithm Γ Function (1) NBI ( Zhou  et al. , 2007 ) (2) HeatS ( Zhou  et al. , 2010 ) (3) Hybrid N+H ( Zhou  et al. , 2010 ) (4) DT-Hybrid Given the weight matrix  W  and the adjacency matrix  A  of the bipartite network, it is possible to compute the recommendation matrix   by the product:
 (2) 
 For each  x i  in  X , its recommendation list is given by the set  , where  r ji  is the ‘score’ of recommending  t j  to  x i . This list is then sorted in a descending order with respect to the score because the higher elements are expected to have a better interaction with the corresponding structure. Notice that the method described above does not make use of any previous biological knowledge of the application domain. Here we propose the DT-Hybrid algorithm, which extends the recommendation model by introducing: (i) similarity between small molecules (i.e. molecular compounds), and (ii) sequence similarity between targets. Let   be the target similarity matrix [i.e. either BLAST bits scores ( Altschul  et al. , 1990 ) or Smith-Waterman local alignment scores ( Smith and Waterman, 1981 )]. This information can be taken into account by using  equation (1)  with   defined as in row 4 of  Table 1 . Including structural similarity requires more effort. Therefore, it is necessary to manipulate such information to obtain a variant of the  S  matrix, and simplify the computation of the  equation (1) . Let   be the structure similarity matrix [i.e. SIMCOMP similarity score ( Hattori  et al. , 2003 ) in the case of compounds]. It is possible to obtain a matrix   (where each element   describes similarity between  t i  and  t j  based on the common interactions in the network weighted by compound similarity) by putting:
 (3) 
This matrix can be linearly combined with the target similarity matrix  S ,
 (4) 
where α is a tuning parameter. This additional biological knowledge yields faster computation and higher numerical precision. The matrix defined by  equation (4)  in connection with  equations (1)  and  (2)  allows the prediction of recommendation lists. 2.2 Datasets and benchmarks We evaluated our method using four datasets ( Cheng  et al. , 2012 ) containing experimentally verified interactions between drugs and genes. We analyzed the performances of NBI [ equation (1)  using Γ(i,j) in  Table 1 , row 1], Hybrid [ equation (1)  using Γ(i,j) in  Table 1 , row 3] and DT-Hybrid [ equation (1)  using Γ(i,j) in  Table 1 , row 4]. The datasets were built by grouping all possible interactions between genes and drugs (DTI) based on their main gene types: enzymes, ion channels, G-protein coupled receptors (GPCRs) and nuclear receptors ( Table 2 ). The following similarity measures have been used: (i) SIMCOMP 2D chemical similarity of drugs ( Hattori  et al. , 2003 ), and (ii) Smith-Waterman sequence similarity of genes ( Smith and Waterman, 1981 ).
 Table 2. Description of the dataset: number of biological structures, targets and interactions together with a measure of sparsity Dataset Structures Targets Interactions Sparsity Enzymes 445 664 2926 0.0099 Ion channels 210 204 1476 0.0344 GPCRs 223 95 635 0.0299 Nuclear receptors 54 26 90 0.0641 Complete DrugBank 4398 3784 12 446 0.0007 Note:  The sparsity is obtained as the ratio between the number of known interactions and the number of all possible interactions. Similarities have been normalized according to  Yamanishi  et al.  (2008) :
 (5) 
 Results are evaluated by combining the methods presented by  Zhou  et al.  (2010)  and  Cheng  et al.  (2012) . More precisely, we applied a 10-fold cross-validation and repeated the experiments 30 times. Notice that, the random partition used in the cross-validation could cause isolation of nodes in the network on which the test is performed. Because all the tested algorithms are capable of predicting new interactions only for drugs and targets for which we already have some information, we computed the partition so that for each node, at least one link to the other nodes remains in the test set. According to  Zhou  et al.  (2010) , the following four metrics were considered: precision and recall enhancement, recovery, personalization and surprisal. Precision and Recall Enhancement ,  
 and 
 . Quality is measured in terms of the top  L  elements in the recommendation list of each biological structure. Let  D i  be the number of deleted interactions recovered for drug  i , and let   be its position in the top  L  places of  i ’s recommendation list. The average precision and recall for the prediction process can be computed as follows:
 (6) 
 (7) 
where   is the number of structures with at least one deleted link. A better perspective can be obtained by considering these values within random models   and  . If the structure  i  has a total of  D i  deleted interactions, then   [given that  ]. Consequently, averaging for all structures we obtain  , where  D  is the number of links in the test set. On the other hand, the average number of links deleted in the first  L  positions is given by  . Again by averaging for all structures,  . Given these random models, it is possible to compute the precision and recall enhancement as follows:
 (8) 
 (9) 
 Finally, as opposed to the recommendation on social systems, the three other metrics—recovery, personalization and surprisal—are not so significant in drug–target systems. For this reason, we report the details of such metrics (their definitions together with the experimental results), just for completeness, in the  Supplementary Materials . 3 RESULTS In this article, we propose a method called DT-Hybrid, which extends NBI ( Cheng  et al. , 2012 ;  Zhou  et al. , 2007 ) and the Hybrid ( Zhou  et al. , 2010 ) algorithms by integrating previous domain-dependent knowledge. Experiments show that this extension improves both algorithms in terms of prediction of new biologically significant interactions. In the supporting materials, we report a comprehensive analysis of DT-Hybrid and Hybrid, together with their behavior varying the α (only for DT-Hybrid) and λ parameters.  Table 3  illustrates the result of comparing NBI, Hybrid and DT-Hybrid in terms of precision and recall enhancement. DT-Hybrid clearly outperforms both NBI and Hybrid in recovering deleted links. It is important to point out that hybrid algorithms are able to significantly improve recall ( e R ) measuring the prediction ability of recovering existing interactions in a complex network.  Figure 1  illustrates the receiver operating characteristic (ROC) curves calculated over the complete DrugBank dataset. Simulations were executed 30 times, and the results were averaged to obtain a performance evaluation. Experiments show that all three techniques have a high true-positive rate against a low false-positive rate. However, hybrid algorithms provided better performance than NBI. In particular,  Table 3  clearly shows an increase of the average areas under the ROC curves (AUC) in the complete dataset (a detailed analysis can be found in the supporting materials section). This indicates that hybrid algorithms improve the ability of discriminating known links from predicted ones. The increase of the AUC values for the DT-Hybrid algorithm demonstrates that adding biological information to prediction is a key choice to achieve significant results.  Table 4  demonstrates that exploiting biological information leads, in most cases, to a significant increase of the adjusted precision and recall.  Figure 2  illustrates the ROC curves calculated on the enzymes, ion channels, GPCRs, and nuclear receptor datasets using the top-30 predictions. Finally, it can be asserted that adding similarity makes prediction more reliable than an algorithm, such as NBI, which applies only network topology to score computation. Indeed, using only known interactions of a new structure without any target information makes it impossible to predict new targets for this drug. This weakness is a problem for all methods based on recommendation techniques. The introduction of new biological structures is equivalent to the addition of isolated nodes in the network, whose weight, based on the  equation (1) , is always zero. Such a weight, ultimately, leads to the impossibility of obtaining a prediction for this new molecule.
 Fig. 1. Comparison between DT-Hybrid, Hybrid, and NBI by means of receiver operating characteristic (ROC) curves, computed for the top- L  places of the recommendation lists, which were built on the complete DrugBank dataset 
 Fig. 2. Comparison between DT-Hybrid, Hybrid and NBI by means of receiver operating characteristic (ROC) curves, computed for the top-30 places of the recommendation lists, which were built on the four datasets (enzymes, ion channels, GPCRs and nuclear receptors) 
 Table 3. Comparison between DT-Hybrid, Hybrid and NBI Algorithm NBI 538.7 55.0 0.9619 ± 0.0005 Hybrid 861.3 85.7 0.9976 ± 0.0003 DT-Hybrid 1141.8 113.6 Note:  For each algorithm the complete DrugBank dataset was used to compute the precision and recall metrics, and the average area under ROC curve (AUC). The parameters used to obtain the following results are  , and  . Values are obtained using the top-20 predictions. Bold values represents best results. 
 Table 4. Comparison of DT-Hybrid, Hybrid, and NBI through the precision and recall enhancement metric, and the average area under ROC curve (AUC) calculated for each of the four datasets listed in  Table 2 Precision enhancement [ ] Recall enhancement [ ] Area Under Curve for the top-20 predictions [ ] Data set NBI Hybrid DT-Hybrid NBI Hybrid DT-Hybrid NBI Hybrid DT-Hybrid Enzymes 103.3 104.6 228.3 19.9 20.9 32.9 Ion channels 22.8 25.4 37.0 9.1 9.7 10.1 GPCRs 27.9 33.7 50.4 7.5 8.8 5.0 Nuclear receptors 28.9 31.5 70.2 0.3 1.3 1.3 Note:  The results were obtained using the optimal values for λ and α parameters as shown in the supporting materials. We set for both Hybrid and DT-Hybrid  . Concerning the α parameter, we have the following setting: enzymes  ; ion channels  ; GPCRs  ; nuclear receptors  . Bold values represents best results. Another important feature of the DT-Hybrid algorithm that we would like to highlight is its ability of increasing performance by keeping computational complexity acceptable. The asymptotic complexity of the NBI algorithm is  , whereas that of DT-Hybrid is  . However, parallelization and optimization techniques can be easily applied to speed computation. We investigated the dependence of DT-Hybrid prediction quality with respect to the α and λ parameters (see the supporting materials for the details). Results show that we cannot discern a law that regulates the behavior of the metrics based on the values of these parameters. They depend heavily on the specific characteristics of each dataset, and therefore require  a priori  analysis to select the best ones. In the reported results, we made such analysis before to run our experiments to establish the parameters yielding the best results in terms of precision and recall enhancement. Finally, notice that our analysis has shown an increase in the precision, recall and AUC, neglecting other metrics, such as recovery, personalization and surprisal. This was done because the latter measure only the capability of analyzing the structure of an interaction network without evaluating the biological significance of predictions. 4 CONCLUSION DT-Hybrid is a technique proposed for the prediction of new interactions between small molecules. Thanks to the domain-dependent additional knowledge, it clearly outperforms the NBI algorithm for DTI prediction. DT-Hybrid integrates biological knowledge and the bipartite interaction network into a unified framework. This yields high quality and consistent interaction prediction, allowing a speedup of the experimental verification activity. Finally, thanks to the hybrid approach, the algorithm overcomes numerical instability that we experienced in the NBI algorithm in presence of particular datasets (i.e. highly sparse). Funding : The publication costs for this article were funded by  PO grant  -  FESR 2007-2013 
 Linea di intervento 
 4.1.1.2 ,  CUP 
 G23F11000840004 . Conflict of Interest : None declared. 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Prediction of cancer driver genes through network-based moment propagation of mutation scores</Title>
    <Doi>10.1093/bioinformatics/btaa452</Doi>
    <Authors>Gumpinger Anja C, Lage Kasper, Horn Heiko, Borgwardt Karsten</Authors>
    <Abstract/>
    <Body>1 Introduction Cancer is a disease of unchecked cellular growth, caused by genetic alterations such as mutations, copy number variations or gene fusions in so called  cancer driver genes . Those alterations can modify both, the activity and cellular function of the gene, and can be classified into activating (proto-oncogenes), or loss of function (tumor suppressor genes and DNA repair genes). Identification of such cancer driver genes is one of the main goals of oncogenic research, as it facilitates mechanistic, diagnostic and therapeutic insights. Cancer genes can be identified through statistical tests that evaluate the mutational burden of the gene (e.g.  Kandoth  et al. , 2013 ;  Leiserson  et al. , 2015 ;  Mularoni  et al. , 2016 ). However, those analyses are complicated by the extensive mutational heterogeneity: Many genes are mutated in a small number of samples, and only few genes show significant mutation across many samples ( Vogelstein  et al. , 2013 ). This phenomenon convolutes the differentiation between genes that only carry passenger mutations, and rarely mutated cancer genes. A potential explanation of this diversity in candidate genes is that genes interact in various pathways ( Hanahan and Weinberg, 2011 ) and protein complexes, and the cancerous potential of a cell is a consequence of the disruption of the pathway, but not necessarily the mutation of  one  specific gene within the pathway. Recent research adopted this interaction-based view on cancer biology: the combination of biological networks and summary statistics that measure each gene’s association to cancer helped the identification of novel cancer driver genes (e.g.  Horn  et al. , 2018 ;  Leiserson  et al. , 2015 ;  Reyna  et al. , 2018 ). In those networks, nodes correspond to genes and edges represent relationships between the adjacent genes. There exists a vast number of biological networks, that are derived from different sources and that cover different scales. Prominent examples are co-expression networks ( Willsey  et al. , 2013 ), co-dependency networks (e.g. AchillesNet;  Li  et al. , 2018 ), co-evolution networks ( Niu  et al. , 2017 ), metabolic pathways ( Kanehisa  et al. , 2017 ) or protein–protein interaction (PPI) networks ( Lage  et al. , 2007 ;  Li  et al. , 2017 ;  Szklarczyk  et al. , 2019 ). Especially, PPI networks constitute an interesting representation of gene interactions, as they commonly combine information from different data sources, tissues, and molecular processes at different scales. However, those PPI networks are far from complete, and our knowledge of them is biased toward well-studied genes ( Horn  et al. , 2018 ). This phenomenon is referred to as  knowledge contamination : well-studied (cancer) genes have a tendency to have more connections in the networks. The potentially large impact on the interpretation of network analyses has to be considered and accounted for, as it might confound results. Methods that use networks as a representation of molecular relationships commonly start with superimposing scores on the nodes. These scores measure the marginal association of the gene to the disease of interest. A prominent choice to represent each gene’s association to cancer is the MutSig  P -value ( Lawrence  et al. , 2014 ): it is a meta- P -value describing whether there is a statistically significant difference in (i) the mutational burden, (ii) the clustering of mutations and (iii) the functional impact of mutations in a gene between healthy and cancer tissues. A plethora of methods has been developed to analyze such gene scores in combination with network information to identify altered subnetworks of genes within the original network. They can be broadly categorized into clustering methods, that aim to find modules of associated genes that cluster together in a network (e.g.  Jia  et al. , 2011 ;  Rossin  et al. , 2011 ) and methods that use network diffusion or network propagation (reviewed in  Cowen  et al. , 2017 ;  Reyna  et al. , 2018 ) to detect altered subnetworks. Both types of methods underlie the common paradigm that genes influencing the same phenotype interact within a network. Especially network propagation methods have shown success in identifying novel cancer driver genes ( Hristov  et al. , 2020 ;  Leiserson  et al. , 2015 ;  Reyna  et al. , 2018 ;  Ruffalo  et al. , 2015 ;  Vandin  et al. , 2011 ,  2012 ). However, network propagation methods exploit by construction the flow of information between genes along paths, and the longer the paths are, the more information gets diluted. This complicates the detection of cancer genes that do not lie on short paths between other cancer genes. Another approach that has proven successful and does not leverage this assumption is  NetSig . It identifies cancer genes based solely on the local neighborhood of genes in a network ( Horn  et al. , 2018 ). At its core lies the computation of an empirical  P -value for each gene that describes the aggregation of genes with low MutSig  P -values in the direct neighborhood. Due to knowledge contamination, the size of a gene’s local neighborhood is affecting the NetSig statistic. To circumvent this, NetSig implements various permutation schemes that take the node degree into account, thereby correcting for this bias. Although the aforementioned methods showed great success in many biomedical applications ( Cowen  et al. , 2017 ), including the discovery of novel cancer genes, they approach the task of gene identification from an unsupervised perspective. However, there exists knowledge on well-established cancer genes (e.g  Sondka  et al. , 2018 ), an important layer of additional information that has, to the best of our knowledge, only been leveraged in few methods for the prediction of cancer driver genes, namely Bayesian modeling ( Sanchez-Garcia  et al. , 2014 ) and unsupervised network propagation ( Hristov  et al. , 2020 ). In most cases, well-established cancer genes are only used to validate the importance and correctness of findings from new methods as a post-processing step. It seems to be an interesting approach to reformulate the task of identifying novel cancer genes as a supervised problem, and learning by exploiting  what we already know . Herein, we propose a novel approach to classify cancer genes in a supervised manner, leveraging the cancer gene annotations from the Cancer Gene Census (CGC) in the COSMIC database ( Sondka  et al. , 2018 ). We achieve this by formulating the problem of finding novel cancer driver genes as a node-classification problem in an interaction network. The heart of our contribution is a novel embedding of nodes in the network based on the distributions of node-features in  k -hop neighborhoods, coupled with a network propagation. We combine the InWeb PPI network ( Lage  et al. , 2007 ;  Li  et al. , 2017 ) with MutSig  P -values ( Lawrence  et al. , 2014 ), and the CGC genes, resulting in an imbalanced dataset due to the low number of known cancer genes compared with the gene corpus. To address this, we develop a cross-validation scheme that enables the supervised prediction of cancer driver genes with a set of classifiers. We compare our approach against both, supervised and unsupervised baselines, and show an improvement with respect to all classification metrics. Last, we evaluate the resulting set of high confidence novel cancer driver candidate genes and find strong links between the predictions and cancer. The list includes known tumor suppressors such as GATA4 ( Agnihotri  et al. , 2011 ), genes known to be affected by recurrent rearrangements FOS ( Fittall  et al. , 2018 ) as well as genes known to be involved in tumor relevant pathways ID2 ( Kijewska  et al. , 2019 ), MYLK ( Avizienyte  et al. , 2005 ;  Cui  et al. , 2010 ;  Zhou  et al. , 2008 ), RALA ( Seibold  et al. , 2019 ). 2 Materials and methods Before we present our novel node embedding procedure, we start by introducing the notation used in this Section, and formally state the problem at hand. 2.1 Notation and problem statement Consider a PPI network that describes interactions between genes. We can represent this interaction network as a graph  G , where the  n  nodes correspond to the genes, and the  m  edges correspond to interactions between genes. We denote the vertex-set as  V , and the edge set as  E . We denote an edge between two nodes  u , v ∈ V  as  e ( u ,  v ), and assume a weighting function  ω : V × V → [ 0 , 1 ]  that assigns each pair of nodes in the network a value between 0 and 1, such that  ω ( u , v ) &gt; 0 ⇔ e ( u , v ) ∈ E . In the case of a weighted network, the function  ω  might correspond to confidence scores of edges, in the case on an unweighted network,  ω  is a binary indicator. Additionally, we assume the existence of a  d -dimensional feature representation for every vertex  v ∈ V , denoted by  x v ∈ ℝ d  or in matrix notation by  X ∈ ℝ n × d . We write the graph as  G = ( V , E , X , ω ) . We define the  k -hop neighborhood of a vertex  v ∈ V  as the set of all genes that can be reached from  v  along at least  k  edges. This can be expressed recursively as
 (1) N v k = { u   |   e ( w , u ) ∈ E ,   ∀ w ∈ N v k − 1 ,         u ∉ N v l   ∀   l ∈ { 0 , .. , k − 1 } } , where  N v 0 = v  (see  Fig. 1a  for visualization of one- and two-hop neighborhoods).
 Fig. 1. Illustration of moment propagation embeddings for node feature  i . (a) Computation of moment embedding  η i k ( v )  for vertex  v  and  k  =   2. Blue nodes indicate the one-hop, orange nodes the two-hop neighborhood. Moments of the distributions  P v : i 1  and  P v : i 2  that describe the values of feature  i  in the one- and two-hop neighborhood of vertex  v  are computed and aggregated. ( b ) Computation of propagation embedding: The node representation is updated by aggregating over all nodes in its one-hop neighborhood. ( c ) Two paths  q 1  and  q 2  connect root vertex  v  with its two-hop neighbor  u     Problem statement We assume a partially labeled, one-class setting in which we have a positive label  l 1  for a subset  V l  of the nodes, but the majority of the nodes are unlabeled, denoted by the set  V u . Our goal is to develop a node embedding  γ G ( · )  based on the feature representation  X  of all vertices  v ∈ V  and the network  G  that, in combination with a binary classifier  C , enables the decision of whether any unlabeled node  u ∈ V u  belongs to the class  l 1 . That is  C ( γ G ( x u ) ) = P ( y u = l 1 ) , where  y v  denotes the label of node  v . More specifically, our goal is to develop a node embedding that serves as input for the supervised, binary classification task of identifying cancer driver genes. It is based on the integration of a PPI network with scores that measure the marginal association of each gene to cancer, when those scores are superimposed on the nodes in the network. 2.2 Generation of node embeddings for the prediction of cancer driver genes The node embeddings proposed in this article are based on two different concepts. The first one is the representation of each node as a distribution across its neighbors’ feature vectors. It is motivated by the success of methods such as NetSig ( Horn  et al. , 2018 ) that focus on the local neighborhood of nodes in the network. However, we extend this idea in three directions: (i) we do not restrict ourselves to one-hop neighborhoods, (ii) we condense the distributions into their  moments , resulting in a concise and computationally efficient representation and (iii) we integrate edge weights into the approach. This moment representation addresses the knowledge-bias in the network, as the description of a distribution via its moments is independent of the number of draws from that distribution. The second concept is a Weisfeiler–Lehman like ( Weisfeiler and Lehmann, 1968 ;  Shervashidze  et al. , 2011 ) aggregation of local features, that is an iterative combination of features from a node’s local neighborhood. It is similar to the network propagation approaches described in  Cowen  et al.  (2017) , and is widely used in methods such as hierarchical HotNet ( Reyna  et al. , 2018 ) and its earlier versions ( Leiserson  et al. , 2015 ;  Vandin  et al. , 2012 ), but also in graph convolutional networks (e.g.  Gilmer  et al. , 2017 ;  Kipf and Welling, 2016 ), where different regimes for aggregation over local neighborhoods are being actively researched. 2.2.1 Embedding genes using moments of local neighborhood distributions Each node  v ∈ V  in the network is represented by its  d -dimensional feature vector  x v ∈ ℝ d , and we denote the  i th feature by  x v : i . The moment embeddings described in this section are computed for each of the  d  features identically and independently, and eventually stacked together. We assume the existence of a probability distribution  P v : i k  that generates the  i th feature for all nodes in the  k -hop neighborhood of node  v . That is, the  i th feature values of  k -hop neighbors of  v  constitute draws from this distribution,  ∀ u ∈ N v k : x u : i ∼ P v : i k . We create an embedding for every vertex  v  that is based on a concise description of the distributions  P v : i k  for  i = 1 , .. , d , and hyperparameter  k ∈ ℕ + . For this, we start by defining a function  ν ¯ ( X )  that maps a scalar random variable  X ∼ P  to its first four moments, that is:
 (2) ν ¯ ( X ) = [ E X [ X ] ,   E X [ X 2 ] ,   E X [ X 3 ] ,   E X [ X 4 ] ) ] . In practice, the expectations in  Equation (2)  can be replaced with the sample mean  μ ( · ) , variance  σ ( · ) , skewness  ξ ( · )  and kurtosis  κ ( · )  and applied to a realization of the random variable  X , denoted by  x = [ x 1 , … , x q ] . We write this function as
 (3) ν ( x ) = [ μ ( x ) ,   σ ( x ) ,   ξ ( x ) ,   κ ( x ) ] , and call it a  moment embedding function . For a vertex  v , we denote with  X v : i k  the values of the  i th feature of vertices in the  k -hop neighborhood of  v , i.e.  X v : i k = { x u : i   |   u ∈ N v k } . Those values constitute a draw from the distribution  P v : i k . We describe the node embedding of vertex  v  with respect to feature  i  by applying the function  ν ( · )  up to its  k -hop neighborhoods, that is
 (4) η i k ( v ) = [ x v : i   , ν ( X v : i 1 ) , …   ,   ν ( X v : i k ) ] , The value  k  is a hyperparameter of the embedding that indicates the maximum neighborhood to be included (see  Fig. 1a  for an example of the node embeddings). The moment embedding  η i k  is a function that creates a representation of every vertex  v ∈ V  by describing its  k -hop neighborhoods with respect to a scalar feature indexed by  i , such that  η i k ( v ) ∈ ℝ ( 1 + 4 k ) . This function can be applied to each of the  d  node features separately, and the resulting representations are stacked to give
 (5) η k ( v ) = [ η 1 k ( v )   , …   , η d k ( v ) ] T . This results in the moment embedding function  η k : V → ℝ d × ( 1 + 4 k ) . 2.2.2 Embeddings using network propagation The second type of node embeddings is based on a Weisfeiler–Lehman like aggregation of nodes in the neighborhood with continuous node features. In this procedure, the representation of every node is simultaneously updated based on the representations of the node’s direct neighborhood (see  Fig. 1b  for an example). That is, given an initial feature representation  x v  of vertex  v , it is represented as
 (6) x v t = 1 | N v 1 | ∑ v ′ ∈ N v 1 x v ′ t − 1 at the  t th Weisfeiler–Lehman iteration. This aggregation corresponds to the element-wise mean across a node’s one-hop neighborhood, and can be used to generate node embeddings by stacking the representations for  t  iterations as follows:
 (7) ρ t ( v ) = [ x v 0 , x v 1 , … , x v t ] , and  ρ t ( v ) : ℝ d → ℝ ( 1 + t ) × d . The number  t  of iterations of this propagation scheme is treated as a hyperparameter that can be tuned during learning. 2.2.3 Combining moment and propagation embeddings to represent genes in a network Here, we propose a combination of the two concepts introduced above, and call the resulting node embedding a  moment propagation embedding , short  MoPro  embedding. It corresponds to a composition of the moment embeddings  η k  and the propagation embedding  ρ t  above, and can be written as:
 (8) γ t , k ( v ) = ( ρ t ° η k ) ( v ) . This function first creates the moment embedding from the feature vector  x v  of a vertex  v , and continues to propagate this representation of the local neighborhoods through the network. As the combination of both functions, it maps the original feature representations of the vertices to a higher dimensional space as follows:  γ t , k : V → ℝ ( 1 + t ) ( 1 + 4 k ) × d . 2.2.4 Extension to networks with weighted edges If a non-binary weighting function  ω  exists, i.e. the edges in the network are weighted and weights can for instance represent confidence scores, we can incorporate this layer of information into our approach: for every edge in the network, the value of the weighting function is non-zero, that is  ∀ e ( v , u ) ∈ E :   ω ( v , u ) ∈ ( 0 , 1 ] , with 1 indicating the highest confidence. These weights can be used to distribute importance of neighbors in a local neighborhood by rescaling the node-features in the moment embedding. This rescaling is done for each feature  i  separately, such that the values of features  i  in the  k -hop neighborhood of node  v  become
 (9) X v : i k , weight = { f ( x u : i , ω ( v , u ) )   |   x u ∈ N v k , } with a problem-specific weighting-function  f ( · , · ) . For  k -hop neighbors  u  of  v  with  k  &gt;   1 the weight  ω ( u , v )  is zero by definition. Hence, we compute it in the following three-step process, and denote it as  ω k ( u , v ) : (i) First, we enumerate all paths of length  k  between  v  and  u , denoted by the set  Q k , and individual paths in  Q k  are denoted as  q j ( v , u )  (see  Fig. 1c ). (ii) Second, we compute the weight of each path in  Q k  as the product of its  k  edge weights and (iii) third we compute the weight  ω k ( v , u )  as a function on the set of path weights,  g : [ 0 , 1 ] | Q k | → ℝ . We treat this function  g ( · )  as a hyperparameter, and use either  g ( · ) = max ( · )  or  g ( · ) = mean ( · ) . 3 Results 3.1 Dataset description In order to find novel cancer driver genes, we combine data from a The Cancer Genome Atlas (TCGA) pan-cancer study of 9 423 tumor exomes (comprising all 33 of TCGA projects;  Bailey  et al. , 2018 ) with the well-established InBio Map PPI network ( Lage  et al. , 2007 ;  Li  et al. , 2017 ). The network constitutes our view of interactions between genes on a protein level. The network has an average degree of 61.02 (±128.33), and the sizes of the  k -hop neighborhoods are illustrated in  Figure 2a . We represent each node in the network with its −log 10  transformed MutSig  P -value ( Lawrence  et al. , 2014 ). Those  P -values measure whether a gene shows significantly different mutational patterns in tumor versus normal tissues. In total, we have access to  P -values for 18 154 genes. As a pre-processing step, we remove all nodes from the network that cannot be represented with a MutSig  P -value, as well as all isolated nodes. This results in a total of 11 449 genes that are present in the InBio Map network, are connected to at least one other node and have been tested with the MutSig tool. Those constitute our candidates for network-based prediction of cancer driver genes.
 Fig. 2. Dataset description: ( a ) the distribution of neighborhood sizes, for neighborhoods defined as in  Equation (1) , for  k = 1 , 2 , 3 . ( b ) The distribution of the node degree, shown for 635 cancer genes and 10 816 unlabeled genes. ( c ) The distribution of the MutSig  P -values, in cancer genes and unlabeled genes. ( d ) The correlation between the degree and the MutSig  P -values for cancer genes and for unlabeled genes     Class labels In general, supervised machine learning requires access to labeled data to train a classifier. To obtain labels for the genes, we use the CGC data from the COSMIC database ( Sondka  et al. , 2018 ). We downloaded a list of 723 genes that have been causally implicated in cancer, and use this set as our ground truth. Genes in the CGC are categorized into Tiers 1 and 2, where genes in Tier 1 show a documented activity relevant to cancer, and genes in Tier 2 show strong indications to play a role in cancer. For our analysis we treat both tiers equally. We overlap the set of 723 genes with our network, giving a total of 635 cancer genes. This leads to a dataset, in which ‘positive’ samples make up &lt;6.0% of our dataset. We refer to the remaining genes as  unlabeled genes , and we are interested in finding new cancer genes among them. Using the CGC genes, we observe a knowledge-bias in the InBio Map PPI network (see  Fig. 2b ), that is cancer genes tend to have higher degrees in the network. We furthermore observe an increased correlation between degree and MutSig  P -values for cancer genes (Pearson correlation: 0.17) compared with unlabeled genes (Pearson correlation: 0.10), as can be seen in  Figure 2c and d . Although this indicates that MutSig can identify the highly mutated cancer genes, there exist many well-established cancer genes whose mutation rates lie within the background distribution (i.e. their MutSig  P -values are undistinguishable between cancer genes and unlabeled genes). This poses three challenges that have to be addressed: (i) we do not have a high-quality negative class, i.e. in general any gene not classified as a cancer gene might potentially be a cancer driver, (ii) the dataset is imbalanced, a fact that requires attention during supervised classification and (iii) the dataset is affected by knowledge contamination. We address challenges (i) and (ii) with an elaborate and unbiased cross-validation procedure to train and test a classifier, as well as to predict cancer driver genes from the unlabeled genes. The third challenge is addressed by using the moments in the MoPro embeddings. Although we observe that moments such as skewness and kurtosis exhibit positive correlations with the node-degree, this is the case for both, cancer genes and unlabeled genes (see  Supplementary Fig. S1 ). 3.2 Experimental setup 3.2.1 Cross-validation for one-class, imbalanced learning To address the above mentioned challenges imposed by the class imbalance and the lack of a negative class, we developed a cross-validation procedure that is based on the repeated undersampling of the majority class. The cross-validation procedure is illustrated in  Figure 3 , and a pseudocode can be found in the  Supplementary Algorithm S1 . The dataset can be represented as a matrix  D ∈ ℝ 11   449 × d , where  d  is the number of node features ( Fig. 3a ). The cross-validation procedure consists of three main steps: Step 1: Data splits We split the dataset  D  into two disjoint datasets,  D l  and  D u  ( Fig. 3b ), where  D l  consists of all genes in the positive class, and a random subsample of the unlabeled genes. We undersample the majority class such that 10% of samples in  D l  are cancer genes. For the sake of training a classifier, we assign the unlabeled samples in  D l  to the negative class, and assume this to be the ground truth for the current split. This dataset will be used in the second step to train and evaluate a classifier. The genes in  D u  remain unlabeled, and we use the classifier trained on  D l  to predict their cancer status. Step 2: Training and evaluation of the classifier Next, the dataset  D l  is split into a cross-validation (80% of data) and a hold-out test set (20% of data;  Fig. 3c ). On the cross-validation set, we do a 5-fold stratified cross-validation to find the best hyperparameters of the classifier  C , resulting in  C ′ . We retrain the classifier on the complete cross-validation set, and evaluate the predictive performance of  C ′  on the hold-out test set. Importantly, the cross-validation and hold-out test sets are disjoint. This implies that samples in the hold-out test set have never been seen during training, nor were they used to choose the best hyperparameters of the classifier. This set is solely used to evaluate the ability of the classifier to generalize to unseen samples. The strict separation of the cross-validation and the hold-out test set is necessary to avoid an inflation of the evaluation metrics. Furthermore, each classifier was run and evaluated on the test data only once. Step 3: Prediction. Last, we apply the classifier  C ′  from the previous step to predict the cancer status of genes in the unlabeled dataset  D u  ( Fig. 3d ).
 Fig. 3. ( a ) The dataset consists of 11 449 genes, each one represented by a set of features. 635 of those genes are classified as cancer genes ( Sondka  et al. , 2018 ), highlighted in yellow. The remaining 10 814 genes are unlabeled (green). Cross-validation scheme on  one  data split, resulting in one best classifier  C ′ . ( b ) The unlabeled genes are sub-sampled at random and combined with the cancer genes, giving rise to the labeled dataset  D l . The unlabeled genes in  D l  are assigned a negative class label. The remaining unlabeled genes make up the set  D u . Those are the genes for which the cancer status will be predicted in the current split. ( c ) The  D l  dataset is split, and 80% of the data are used to find the best hyperparameters of the classifier via 5-fold CV, resulting in the classifier  C ′ . The remaining 20% are used as test set for evaluation of the classifier  C ′ . ( d ) The classifier  C ′  that has been trained on the cross-validation set in  D l  is used to predict the cancer status of genes in  D u This cross-validation procedure learns to distinguish cancer genes from a random split of the unlabeled genes. However, this is a potentially incorrect assumption, since unlabeled genes in the set  D l  might be yet-to-discover cancer genes. For this reason, we repeat the complete cross-validation procedure for  r  different random splits of the dataset into a labeled subset  D l  and a unlabeled subset  D u , resulting in a  set of classifiers   E = { C ′ 1 , .. , C ′ r } . Each  D l  is divided into an 80% cross-validation dataset and a 20% test dataset. The cross-validation dataset is used for hyperparameter optimization of the classifiers and the test dataset is used for evaluation in terms of area under the precision recall curve (AUPRC), precision, recall and F1-score.  C ′ i  is the best classifier on the  i th random split of the data, determined on the cross-validation dataset. For each  C ′ i ∈ E , where  i = 1 , .. , r , we compute the performance metrics (AUPRC, precision, recall, F1-score) on the test set of the  i th split, and report the mean and standard deviation of the metrics across all  r  classifiers in the set  E  as the final result. In order to obtain comparable results for different classification algorithms, we ensure that each algorithm is trained and evaluated on the same  r  splits of the data. We determine the total number of splits  r  based on the minimum number of predictions we want to obtain for every gene without a cancer label in the dataset. We set this value to five, resulting in  r  =   11 data splits, and evaluate the effect of varying  r  in terms of the average AUPRC on the test sets in an experiment (see Section 3.3.3 and  Fig. 4b ). Fig. 4. Evaluation of a set of logistic regression classifiers (hyperparameters as in  Table 2 ). ( a ) AUPRC when varying the number of random splits  r , and therefore the number of classifiers in the set  E . ( b ) Evaluation metrics as functions of the training set size Since this splitting of the data is random, the underlying data distribution of the negative class varies from split to split, and the classifier optimized on each split learns the data modalities of the negative class in the current split. Every gene is predicted with each classifier in our set of classifiers (excluding the ones for which it was in the  D l  set used for training), and might be classified as a cancer gene by some of the classifiers, but not by others. This can be interpreted as the fact that a gene might be more similar to a cancer gene in some aspects, but more similar to a non-cancer gene in others. Eventually, a gene is classified as a cancer gene according to the majority vote across all classifiers (for which it was not in the training data). In the case of ties, we resort to the conservative prediction of ‘no cancer gene’. Importantly, as there exist no known labels for those genes, we analyze them qualitatively. 3.2.2 Classification We represent each node in the network by the MoPro embeddings computed from the log-transformed MutSig  P -values, as described in Section 2.2.3. We apply four different state-of-the-art classification algorithms to predict the binary class labels in the cross-validation procedure described above, using python’s sklearn module: logistic regression, random forests, support vector machines (SVMs) and gradient boosting. For every classifier, we optimize across a grid of standard hyperparameters, as well as the following data-specific hyperparameters: (i) whether or not to include a scaling step in the classification pipeline (SCALE), (ii) whether to use edge weights to generate node embeddings (WEIGHT), (iii) how to represent weights between two nodes in a  k -hop neighborhoods with  k  &gt;   1 (PATH; see Section 2.2.4), as well as (iv) the number of propagation steps  t  and (v) the number of  k -hops to generate moment embeddings from, where  k ∈ { 1 , 2 } ,  t ∈ [ 1 , … , 6 ] . We restrict the value of  k  to a maximum of 2, as we observe that three-hop neighborhoods in the InBio Map network already span major parts of the network (see  Fig. 2a ). In order to weight the contribution of node features in local neighborhoods during the generation of moment embeddings (function  f ( · , · )  in  Equation 9 ), we use a simple multiplication between the node features and the edge weights, resulting in a lowering of the contribution of the −log 10  transformed  P -values for edges that exhibit confidences below 1.0. In order to evaluate the predictive performance of each classifier, we use the AUPRC, as well as precision, recall and the F1-score. As described in the previous Section, to evaluate the performance of a set of classifiers  E  resulting from the cross-validation procedure, those metrics correspond to the average across the classifiers in  E . We furthermore report the number of predicted genes that are novel, i.e. those that are not contained in the COSMIC CGC gene set. We would like to note that, although the area under the ROC curve (AUROC) is a common metric to evaluate binary classifiers, its interpretation is difficult in our setting. Due to the high class imbalance and our primary interest in detecting members of the minority class (i.e. the cancer genes), measuring precision and recall on the minority class is a better suited metric for our task. The AUROC values can be found in the  Supplementary Tables S1 and S2 . 3.3 Classification of cancer genes 3.3.1 Baseline methods We compare our approach against univariate baselines, that is we determine the cancer driver status of a gene based on (i) its degree, (ii) its MutSig  P -value and (iii) its NetSig  P -value. Those results are listed in  Table 1 . For all node features, we compare a ranking of the genes by the respective feature ( ranking ) and a prediction with a set of logistic regressors ( LogReg ), generated with the cross-validation procedure described in Section 3.2.1. When ranking the genes based on features, we report precision and recall at the threshold that gave the best F1-score. For the MutSig and NetSig  P -values, we also evaluate predictive performance after Benjamini–Hochberg (BH) correction at a false discovery rate of 10%. Note that since there is only one classification result for the ranking and Benjamini–Hochberg procedure, there are no standard deviations reported in the table. Furthermore, in both cases, the number of novel genes does not correspond to a majority vote, but is based on a single prediction, using the prediction threshold that resulted in the highest F1-score. We also apply hierarchical HotNet ( Reyna  et al. , 2018 ), a state-of-the-art network propagation method for the detection of altered subnetworks in cancer, to the MutSig  P -values (after −log 10  transformation). We chose the score permutation scheme to obtain a measure of significance ( P  =   0.01) and report all genes in subnetworks of sizes &gt; 1 as positives. For all methods we observe that using the set of classifiers improves predictive performance with respect to the AUPRC. Furthermore, we observe that the degree of a gene in the network reaches AUROC values of up to 70% (see  Supplementary Table S1a ), hinting toward the problem of knowledge bias in biological networks. That is, the degree operates as a confounder in those networks.  Table 1. Results of cancer gene classification for the baselines Feature Method AUPRC Precision Recall F1 No. of novel Degree Ranking 0.096 0.105 0.436 0.169 2368 Degree LogReg 0.199 (0.007) 0.243 (0.012) 0.236 (0.000) 0.239 (0.006) 905 MutSig Ranking 0.248 0.474 0.202 0.283 142 MutSig LogReg 
 0.312 (0.007) 
 0.552 (0.060) 0.236 (0.000) 0.330 (0.011) 243 MutSig BH 0.248 0.490 0.191 0.274 126 MutSig Hier. HotNet — 0.137 0.111 0.123 444 NetSig Ranking 0.158 0.219 0.235 0.226 532 NetSig LogReg 0.275 (0.012) 0.278 (0.019) 0.228 (0.000) 0.250 (0.008) 704 NetSig BH 0.158 0.263 0.169 0.205 300 
 Note : The first column indicates the feature that was used to represent each gene during classification, the second column indicates the method that was used for classification. In case of LogReg, we used the cross-validation procedure described in Section 3.2.1 and fixed the recall at 23.5%. AUPRC is the area under the precision recall curve, the method with the highest AUPRC is printed in bold. The last column indicates the number of de novo cancer genes, i.e. those genes that are not contained in the set of cancer genes. 3.3.2 Cancer gene classification with MoPro embeddings We generate MoPro embeddings from the −log 10  transformed MutSig  P -values, and use these embeddings as input to the classifiers. The results are listed in  Table 2 . We evaluate the four classifiers on a grid of hyperparameters, and list the best values of the ones specific to our proposed approach (see Section 3.2.2) in the table. We observe a similar performance of all classifiers with respect to AUPRC, with a minor exception for the random forest classifier. The classification using MoPro embeddings combined with the cross-validation procedure to handle imbalanced classes clearly outperforms the baselines. The baseline with the best AUPRC is the logistic regression classification using MutSig  P -values (AUPRC = 31.2%). With the MoPro embeddings, AUPRC values of up to 43.7% are achieved with the gradient boosting classifier (closely followed by logistic regression and SVMs). A similar trend can be observed for AUROC scores (see  Supplementary Table S1 ).  Table 2. Results of cancer gene classification for the moment propagation embeddings Method Scale Weight Path 
 t 
 
 k 
 AUPRC Precision Recall F1 No. of novel LogReg True — -- 3 2 0.434 (0.014) 0.572 (0.046) 0.236 (0.000) 0.334 (0.009) 202 SVM False — -- 6 2 0.431 (0.012) 0.584 (0.058) 0.236 (0.000) 0.336 (0.010) 198 RandFor True standard Mean 3 1 0.396 (0.021) 0.560 (0.057) 0.234 (0.004) 0.330 (0.011) 193 GradBoost True standard Max 3 2 
 0.437 (0.020) 
 0.636 (0.088) 0.236 (0.000) 0.343 (0.012) 150 
 Note : Classification results for different classifiers using the proposed moment propagation embeddings and the described cross-validation procedure. The Columns 2–6 indicate the hyperparameters that gave the best classification performance for each set of classifiers.  t  and  k  are the hyperparameters of the moment propagation embeddings, namely the number of propagation steps and the neighborhood degree up to which moments are computed. AUPRC is the area under the precision recall curve, the method with the highest AUPRC is printed in bold. The last column indicates the number of de novo cancer genes, i.e. those genes that are not contained in the set of cancer genes. For all analyses, we fixed the recall at 23.5%, that is the recall achieved by ranking the NetSig  P -values. We observe that with MoPro embeddings, we obtain an up to three-fold improvement of precision at that same recall value compared with the NetSig approach (ranked NetSig  P -values: 21.9%, gradient boosting 63.6%). When contrasting the precision of MoPro embeddings with the one of the best baseline, that is logistic regression using the MutSig  P -value, we observe an improvement of ∼8%. We optimize the data-specific hyperparameters, and find that for all classifiers, using at least three propagation steps enables best classification. All methods but random forests performed best when deriving moments from the  k  =   2-hop neighborhoods. Although random forests and gradient boosting achieve better classification performance when using weighted neighborhood distributions, this was not the case in logistic regression and SVMs. There seems to be no clear winner between the generation of weights in  k -hop neighborhoods when using the mean or the maximum aggregation (Section 2.2.4). 3.3.3 Dependence of results on cross-validation parameters The results in  Table 2  are produced with  r  =   11 splits of the data into  D l  and  D u  (Section 3.2.1). We evaluate the performance of classification with MoPro embeddings for values of  r  in the range [5, 500] while keeping all data-specific hyperparameters (as described in Section 3.2.2) fixed (using logistic regression). We observe that the classification performance is not affected by changes in the parameter  r  (see  Fig. 4a ). Note that we lose ∼2% in AUPRC due to fixing the data hyperparameters. We furthermore evaluate how the size of the training set affects the classification performance. In our proposed cross-validation scheme, the training set size is fixed due to the 5-fold cross-validation, and contains 4064 genes (the cross-validation set contains 5080 genes, such that 4064 genes are used in a 5-fold cross-validation to train the classifier). We conducted an experiment where this number is reduced, ranging between 10 and 4000 samples. We observe a steep increase in performance with an increasing training set size up to 1000 training samples (see  Fig. 4b ), and a saturation when using more than 1000 samples. This indicates, that at least 1000 samples are required to represent the data distribution during classification. 3.3.4 Ablation study We conduct an ablation study to understand how the individual parts in the moment propagation embedding contribute to the improved performance. The results can be found in  Table 3 . We evaluate two different types of experiments: (i) we only use the node feature (i.e. the −log 10  transformed MutSig  P -value), and propagate it through the network with the propagation embedding  ρ t ( · )  described in Section 2.2.2. This representation of the node features is used to train a set of logistic regression classifiers. The results of this analysis are listed in the row with label ‘propagation only’. (ii) We represent each node with the moment embedding  η k ( · ) , k = 2  described in Section 2.2.1, without propagating the resulting representation through the network. The results of this approach are listed in the row with label ‘moments only’. We observe that removing the moment embedding results in a severe drop in performance of ∼8%, while keeping moments but not propagating them leads to a less severe reduction (∼2.8%). This observation indicates that the main improvement of performance compared with the baselines is due to the description of a node by means of the distribution of node features in its neighborhood, motivating the development of methods that improve the representation of local neighborhoods.  Table 3. Results of the ablation study for the set of logistic regression classifiers Setting Method AUPRC Baseline LogReg 0.434 (0.014) Propagation only LogReg 0.348 (0.010) Moments only LogReg 0.406 (0.011) 
 Note : In  propagation only , the node feature is propagated, but no moment embedding is computed. In  moments only , moments are computed, but no propagation embedding is computed. The first row repeats the baseline results (moment propagation embeddings) for comparability reasons. 3.4 Evaluation of predicted cancer driver genes To generate a candidate gene list, we created a consensus set of all genes as follows: for each of the four classification algorithms (logistic regression, SVM, random forest, gradient boosting), we took the intersection of genes that were classified as ‘novel’ (see  Table 2 ). This means that a gene in the consensus set has (i) not been classified as a cancer driver gene in the CGC dataset, and (ii) all four sets of classifiers identified the gene as a cancer driver (as described in Section 3.2.1). This led to 50 candidate genes, 31 of which were significant in the MutSig data ( P = 8.04 * 10 − 42 , hypergeometric test), 10 in the NetSig data ( P = 1.07 * 10 − 6 , hypergeometric test) and 12 with hierarchical HotNet ( P = 2.04 * 10 − 7 , hypergeometric test), where  P -values measure whether the set of 50 consensus genes is significantly enriched with MutSig, NetSig and hierarchical HotNet hits, respectively. By removing all genes from the consensus set that were detected with at least one other method, 14 novel genes remained. For those, we performed a literature review to estimate the evidence for links to cancer. In brief, four genes have a direct link to tumorigenesis in human. The transcription factor GATA4 is a known tumor suppressor in Glioblastoma Multiforme ( Agnihotri  et al. , 2011 ). . In breast cancer patients, ID2 is upregulated in brain metastasis and high expression is linked to an increased risk of developing relapse ( Kijewska  et al. , 2019 ). Last, FOS exhibits recurrent rearrangements Osteoblastoma ( Fittall  et al. , 2018 ). Five genes can be strongly linked to tumor relevant behavior and pathways. ACVR1B (also known as ALK4) is linked to tumorigenesis through its interaction with activin-A ( Kalli  et al. , 2019 ;  Rautela  et al. , 2019 ). CASP10 inhibition leads to reduced apoptosis, while loss-of-function of RAP1A causes a reversion to a non-malignant phenotype in a model of invasive carcinoma ( Stammer  et al. , 2017 ). MYLK is involved in proliferation and the migration of cancers of the breast, prostate and colon ( Avizienyte  et al. , 2005 ;  Cui  et al. , 2010 ;  Zhou  et al. , 2008 ). CSNK1A1, a member of the CK1 kinase family, is a regulator of the autophagic pathway in RAS-driven cancers, and knock-out experiments lead to cell death in Multiple myeloma ( Carrino  et al. , 2019 ;  Cheong  et al. , 2015 ). For the remaining six genes, five (CASP1, CASP14, RBL1, HNF4A and RALA) had weaker links (e.g. expression linked or pathway membership), but no clear experimental evidence ( Gouravani  et al. , 2020 ;  Krajewska, 2005 ;  Schade  et al. , 2019 ;  Seibold  et al. , 2019 ;  Wang  et al. , 2020 ). Only for one gene (DLGAP2) we could not find any evidence for a link to cancer. 4 Discussion and conclusions In this article, we proposed a novel approach for the identification of cancer driver genes by integrating MutSig summary statistics ( Lawrence  et al. , 2014 ) with PPI networks ( Lage  et al. , 2007 ;  Li  et al. , 2017 ). In stark contrast to state-of-the-art approaches that set out to solve this problem with unsupervised processes, we developed an innovative node embedding procedure (MoPro embeddings) to enable supervised classification of cancer driver genes. Reformulating the problem of cancer-gene prediction in a supervised fashion enables learning from  what we already know : we include knowledge on the data distributions of well-established cancer driver genes and learn from these distributions to improve the prediction task. We do so by combining two concepts: (i) the representation of a node based on the distribution of node features in its  k -hop neighborhood, followed by (ii) a network propagation. The neighborhood distributions in (i) are described concisely by their first four moments, which constitutes a computationally efficient summary, and addresses the knowledge contamination that often confounds analyses of biological networks. We show that our approach outperforms baselines with respect to AUPRC by a margin of more than 10%, and that results are stable with respect to the hyperparameters of the method. Interestingly, we find that the main improvement in predictive performance is presumably caused by the representation of the distributions of node features in a gene’s neighborhood, rather than the network propagation. This finding paves the way for further research: while the proposed representation of the distributions by means of their moments is straightforward and computationally efficient, another option is to exploit principles of optimal transport ( Villani, 2008 ) to compare two nodes based on the distributions of features in their neighborhoods.  Togninalli  et al.  (2019)  developed a kernel based on Wasserstein distances between distributions for graph classification, and this idea can be readily extended to node classification. Another possible route for future research is to build models that combine both, node embeddings and classification, and to train them end-to-end, as is done with graph convolutional networks (e.g.  Duvenaud  et al. , 2015 ;  Hamilton  et al. , 2017 ;  Kipf and Welling, 2016 ). The set of high confidence consensus genes discovered with our proposed approach contained both, genes that were previously identified as cancer drivers with methods such as MutSig, NetSig and hierarchical HotNet, as well as novel genes that were not detected with established methods. Those genes constitute promising targets for future biological evaluation, and their detection showcases the potential of combining network-derived features with supervised machine learning techniques for the prediction of cancer driver genes. Supplementary Material btaa452_Supplementary_Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Identification and removal of ribosomal RNA sequences from metatranscriptomes</Title>
    <Doi>10.1093/bioinformatics/btr669</Doi>
    <Authors>Schmieder Robert, Lim Yan Wei, Edwards Robert</Authors>
    <Abstract>Summary: Here, we present riboPicker, a robust framework for the rapid, automated identification and removal of ribosomal RNA sequences from metatranscriptomic datasets. The results can be exported for subsequent analysis, and the databases used for the web-based version are updated on a regular basis. riboPicker categorizes rRNA-like sequences and provides graphical visualizations and tabular outputs of ribosomal coverage, alignment results and taxonomic classifications.</Abstract>
    <Body>1 INTRODUCTION Metatranscriptomic approaches are drastically improving our understanding of metabolism and gene expression in microbial communities. By investigating all functional mRNA transcripts isolated from an environmental sample, metatranscriptomic analyses provide insights into the metabolic pathways important for a community at the time of sampling. Although metatranscriptomes are used to investigate metabolic activities, the majority of RNA recovered in metatranscriptomic studies is ribosomal RNA (rRNA), often exceeding 90% of the total reads ( Stewart  et al. , 2010 ). Even after various treatments prior to sequencing, the observed rRNA content decreases only slightly ( He  et al. , 2010 ) and metatranscriptomes still contain significant amounts of rRNA. Although rRNA-like sequences are occasionally removed from metatranscriptomes, the removal is performed only with a subset of the publicly available rRNA sequences. Failure to remove all rRNA sequences can lead to misclassifications and erroneous conclusions during the downstream analysis. It is estimated that misannotations of rRNA as proteins may cause up to 90% false positive matches of rRNA-like sequences in metatranscriptomic studies ( Tripp  et al. , 2011 ). The potential for false positives arrises from a failure to completely remove all rRNA prior to translating the putative rRNA and querying a protein database. The rRNA operons in Bacteria and Archaea are not known to contain expressed protein coding regions that at the same time code for rRNA and therefore, annotations of proteins in rRNA coding regions should be presumed to be misannotations ( Aziz  et al. , 2008 ). Metagenomic sequence data generated to asses the metabolic potential of a community will also be affected by false positive matches of rRNA sequences when querying a protein database. Therefore, transcript analysis should only proceed after it has been verified that all rRNA-like sequences have been found and removed from the dataset to allow accurate identification of the transcribed functional content. The high-throughput nature of community sequencing efforts necessitates better tools for the automated preprocessing of sequence datasets. Here, we describe an application able to provide graphical guidance and to perform identification, classification and removal of rRNA-like sequences on metatranscriptomic data. The application incorporates a modified version of the BWA-SW program ( http://bioinformatics.oxfordjournals.org/citmgr?gca=bioinfo;26/5/589 ), and is publicly available through a user-friendly web interface and as stand-alone version. The web interface allows online analysis using rRNA sequences from public databases and provides data export for subsequent analysis. 2 METHODS 2.1 Implementation and computational platform The riboPicker application was implemented as stand-alone and web-based version in Perl. The web application is currently running on a web server with Ubuntu Linux using an Apache HTTP server to support the web services. The alignments are computed on a connected computing cluster with 10 working nodes (each with 8 CPUs and 16 GB RAM) running the Oracle Grid Engine version 6.2. All graphics are generated using the Cairo graphics library ( http://cairographics.org/ ). 2.2 Identification of rRNA-like sequences The identification of rRNA-like sequences is based on sequence alignments using a modified version of the BWA-SW program. The modifications do not change the default behavior of the algorithm and include parameter forced changes in the alignment of ambiguous bases and the generation of an alternative output. The documentation provides a detailed list of changes and is available on the program website. riboPicker uses query sequence coverage, alignment identity and minimum alignment length thresholds to determine if an input sequence is an rRNA-like sequence or not. This approach is based on the idea that looking for similar regions consists of grouping sequences that share some minimum sequence similarity over a specified minimum length. Threshold percentage values are rounded toward the lower integer and should not be set to 100% if errors are expected in the input sequences. The results for multiple databases are automatically joined before generating any outputs. Using simulated datasets, we evaluated the classification of rRNA-like sequences and showed that riboPicker performed with high accuracy comparable to the latest version of meta_rna ( Huang  et al. , 2009 ) and BLASTn ( Supplementary Material ). A comparison on real metatranscriptomic data showed that riboPicker processes data more than twice as fast as Hidden Markov Model (HMM)-based programs and &gt;100 times faster than BLASTn ( Supplementary Material ). 2.3 Reference databases The web-based version offers preprocessed databases for 5S/5.8S,16S/18S and 23S/28S rRNA sequences from a variety of resources, currently including SILVA ( Pruesse  et al. , 2007 ), RDP ( Cole  et al. , 2009 ), Greengenes ( DeSantis  et al. , 2006 ), Rfam ( Gardner  et al. , 2011 ), NCBI ( Sayers  et al. , 2011 ) and HMP DACC ( The NIH HMP Working Group  et al. , 2009 ). To reduce the number of possibly misannotated entries, sequences were filtered by length to remove very short and long sequences and by genomic location to remove overlapping rRNA misannotations. The remaining sequences were then converted into DNA sequences (if required) and filtered for read duplicates to reduce redundancy in the sequence data. Detailed information for each reference database is provided on the website. Taxonomic information was either retrieved with the sequence data from the resources or was added based on the NCBI Taxonomy. The databases are automatically updated on a regular basis and can be requested from the authors for offline analysis. A non-redundant database is made available for the stand-alone version on the program website. 3 WEB-INTERFACE 3.1 Inputs The web interface allows the submission of compressed FASTA or FASTQ files to reduce the time of data upload. Uploaded data can be shared or accessed at a later point using unique data identifiers. It should be noted at this point that the input datasets should only contain quality-controlled, preprocessed sequences to ensure accurate results ( Schmieder and Edwards, 2011 ). In addition to the sequence data, the rRNA reference databases have to be selected from the list of available databases. Unlike the stand-alone version, the web-based program allows the user to define threshold parameters based on the results after the data are processed. This does not require an  a priori  knowledge of the best parameters for a given dataset and the parameter choice can be guided by the graphical visualizations. 3.2 Outputs Users can download the results in FASTA or FASTQ (if provided as input) format or its compressed version. Results will be stored for the time selected by the user (either 1 day or 1 week), if not otherwise requested, on the web server using a unique identifier displayed during data processing and on the result page. This identifier additionally allows users to share the result with other researchers. The current implementation offers several graphical and tabular outputs in addition to the processed sequence data. The Coverage versus Identity plot shows the number of matching reads for different coverage and identity threshold values. The coverage plots show where the metatranscriptomic sequences aligned to the rRNA reference sequences and provide an easy way to check for possible bias in the alignment or the rRNA-removal prior to sequencing. The coverage data for each database sequence is available for download. The taxonomic classifications of rRNA-like sequences are presented as bar charts for each selected database. The summary report includes information about the input data, selected databases and thresholds, and rRNA-like sequence classifications by database, domain and phyla. 4 BRIEF SURVEY OF ALTERNATIVE PROGRAMS There are different applications that can identify rRNA-like sequences in metatranscriptomic datasets. The command line program meta_rna ( Huang  et al. , 2009 ) is written in Python and identifies rRNA sequences based on HMMs using the HMMER package ( Eddy, 2009 ). Another program based on HMMER is rRNASelector ( Lee  et al. , 2011 ), which is written in Java and can only be used through its graphical interface. The web-based MG-RAST ( Meyer  et al. , 2008 ) uses the BLASTn program, identifying rRNA-like sequences based on sequence similarity. The HMM-based programs currently allow identification of bacterial and archaeal rRNAs. The sequence similarity-based programs make it easy to assign sequences to taxonomic groups. 5 CONCLUSION riboPicker allows scientists to efficiently remove rRNA-like sequences from their metatranscriptomic datasets prior to downstream analysis. The web interface is simple and user-friendly, and the stand-alone version allows offline analysis and integration into existing data processing pipelines. The tool provides a computational resource able to handle the amount of data that next-generation sequencers are capable of generating and can place the process more within reach of the average research lab. 
          Supplementary Material 
          
             
                Supplementary Data 
             
             
             
          
       </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Characterization of the N-ATPase, a distinct, laterally transferred Na+-translocating form of the bacterial F-type membrane ATPase</Title>
    <Doi>10.1093/bioinformatics/btq234</Doi>
    <Authors>Dibrova Daria V., Galperin Michael Y., Mulkidjanian Armen Y.</Authors>
    <Abstract>An analysis of the distribution of the Na+-translocating ATPases/ATP synthases among microbial genomes identified an atypical form of the F1Fo-type ATPase that is present in the archaea Methanosarcina barkeri and M.acetivorans, in a number of phylogenetically diverse marine and halotolerant bacteria and in pathogens Burkholderia spp. In complete genomes, representatives of this form (referred to here as N-ATPase) are always present as second copies, in addition to the typical proton-translocating ATP synthases. The N-ATPase is encoded by a highly conserved atpDCQRBEFAG operon and its subunits cluster separately from the equivalent subunits of the typical F-type ATPases. N-ATPase c subunits carry a full set of sodium-binding residues, indicating that most of these enzymes are Na+-translocating ATPases that likely confer on their hosts the ability to extrude Na+ ions. Other distinctive properties of the N-ATPase operons include the absence of the delta subunit from its cytoplasmic sector and the presence of two additional membrane subunits, AtpQ (formerly gene 1) and AtpR (formerly gene X). We argue that N-ATPases are an early-diverging branch of membrane ATPases that, similarly to the eukaryotic V-type ATPases, do not synthesize ATP.</Abstract>
    <Body>1 INTRODUCTION F 1 F o -type (F-type) and V/A-type ATPases are membrane-anchored rotary enzymes that couple translocation of H +  or Na +  ions across the membrane to the synthesis or hydrolysis of ATP. Although most of their subunits are evolutionarily related, the two classes of ATPases have clear differences in structure and phylogenetic distribution: F-type ATPases are found in bacteria, mitochondria and chloroplasts, whereas V/A-type ATPases are found in eukaryotic cell membranes (in particular, vacuoles), as well as in all archaea and in some bacteria (Hilario and Gogarten,  1998 ; Forgac,  2007 ; von Ballmoos  et al. ,  2008 ; Mulkidjanian  et al. ,  2009 ). V/A-type ATPases are often subdivided into two classes based (i) on the ability of the prokaryotic (A-type) enzyme to function in the direction of ATP synthesis and (ii) presence of additional subunits in the eukaryotic (V-type) enzyme that normally functions only as an ion-translocating ATPase, using the energy of ATP hydrolysis to acidify cellular compartments (Hilario and Gogarten,  1998 ; Forgac,  2007 ). Structural characterization of the Na + -binding sites in the  c -subunits of F- and V-type ATPases (Meier  et al. ,  2005 ; Murata  et al. ,  2005 ) allowed unequivocal assignment of the cation specificity for the membrane ATPases encoded in numerous sequenced genomes: only those  c -subunits that contain full sets of Na +  ligands appeared to transport Na + , the loss of at least one of those ligands correlated with the loss of Na +  specificity (Mulkidjanian  et al. ,  2008a ; Meier  et al. ,  2009 ). In the view of several reports on the sodium dependence of energy-coupled reactions in cyanobacteria (Willey  et al. ,  1987 ; Skulachev,  1988 ; Pogoryelov  et al. ,  2003 ), we have searched cyanobacterial genomes for the Na + -translocating ATPases. We found several apparently Na + -dependent cyanobacterial ATPases, but always as second copies, in addition to the H + -translocating ATP synthases. Here, we report the common properties of these ATPases, which are encoded in apparently highly mobile operons and have a set of specific traits that qualify them as a separate subfamily of the F-type ATPases. Since these ATPases, besides forming a  novel  subfamily, are always encoded  next to  the typical rotary ATPases and are predominantly  Na + - dependent , we refer to them hereafter as N-ATPases. 2 METHODS Phylogenetic distribution of the N-ATPase operons was deduced from protein BLAST (Altschul  et al. ,  1997 ) searches against the NCBI's RefSeq database (Pruitt  et al. ,  2009 ) (last searched February 1, 2010) and verified by examining gene neighborhoods of the retrieved ORFs and by checking for the presence of the N-ATPase-specific subunit AtpR. Phylogenetic trees were constructed using the neighbor-joining algorithm with MEGA (Kumar  et al. ,  2008 ). Multiple alignments were constructed from BLAST outputs with manual editing. Transmembrane segments were predicted using TMHMM (Krogh  et al. ,  2001 ). Sequence logos were drawn with WebLogo (Crooks  et al. ,  2004 ). 3 RESULTS Search of the NCBI's RefSeq database (Pruitt  et al. ,  2009 ) for cyanobacterial  c  (proteolipid) subunits that would have a full set of Na + -binding ligands (Mulkidjanian  et al. ,  2008a ; Meier  et al. ,  2009 ) returned five hits, all coming from marine cyanobacteria ( Supplementary Fig. S1 a). In each case, the operon encoding the Na + -binding  c  subunit was present in the genome along with another ATPase operon, encoding an H + -translocating F-type ATPase. As depicted below for  Synechococcus  sp. PCC 7002, an alignment of the Na + - and H + -binding  c -subunits (top and bottom, respectively) from the same cyanobacteria revealed a Glu substitution of the Gln residue (shown in blue) that serves as a Na +  ligand (uncharged residues of the transmembrane segments are highlighted in yellow, Na +  and H +  ligands are labeled with asterisks) Among the 4084 c -subunit sequences currently listed in the Pfam family ATP-synt_C (PF00137; Finn  et al. ,  2010 ), only 227 (5.5%) contain a Glu residue in that position and only ∼1% combine it with a typical ESTxxY Na + -binding motif ( Supplementary Fig. S1 ). The Na + -dependent ATPase operons in all cyanobacteria had a similar gene order, with a single gene insertion in  Acaryochloris marina  and a two-gene insertion in the two strains of  Cyanothece  sp. ( Supplementary Fig. S2 ). 3.1 Always the second: conservation and distinctive properties of the N-ATPase A search of the complete genome database identified homologous N-ATPase operons in some representatives of the bacterial phyla  Aquificae ,  Chlorobi  and  Planctomycetes , in certain members of α-, β-, γ- and δ-subdivisions of  Proteobacteria , and in two archaea,  Methanosarcina barkeri  and  M.acetivorans  (see  Supplementary Fig. S2 ). All these operons had the same  atpDCI - urf2 - atpBEFAG  gene order, encoding, respectively, β-, ε-, 1-, Urf2-, a-, c-, b−, α- and γ-subunits of this particular F-type ATPase, first described in  M.barkeri  by Sumi  et al.  ( 1997 ). The only exception outside of cyanobacteria was  Persephonella marina , a member of the  Aquificae , which has an N-ATPase operon with the  urf2 - atpBEFAGDCI  gene order and also harbors operons for F- and A-type ATPases ( Supplementary Fig. S2 ). Identification of the N-ATPase operons in diverse microorganisms was simplified by the fact that these operons are always present in the genomes as second copies alongside operons that encode typical H + -transporting F-type ATPases (in  M.barkeri  and  M.acetivorans , A-type ATPases). In contrast, most N-ATPase  c -subunits contain full sets of Na +  ligands, indicating that these enzymes are specific for Na +  ions. Just like the cyanobacterial Na + -binding  c -subunits,  c -subunits of most N-ATPases had Glu residues in the middle of both transmembrane helices ( Supplementary Fig. S1 ). The same subunits of the N-ATPases from diverse bacteria are closely related and form distinct branches on the phylogenetic trees, well separated from the equivalent subunits of the F- and V-type ATPases ( Fig. 1  and  Supplementary Fig. S3 , see also  Supplementary Fig. 5  in Swingley  et al. ,  2008 ). Phylogenetic trees built for individual N-ATPase subunits showed similar topologies, so that the whole N-ATPase operons appeared to co-evolve ( Supplementary Fig. S4 ).
 Fig. 1. A neighbor-joining tree comparing α-subunits of N-ATPases with α-subunits of F-type ATPases and B-subunits of A- and V-type ATPases. See  Supplementary Fig. S3  for the full tree. Another distinct trait of the N-ATPase operons was the presence of an extra gene,  urf2  (Sumi  et al. ,  1997 ). Its product is currently annotated as ‘F1/F0 ATPase,  Methanosarcina  type, subunit 2’ (F1F0_chp_2, TIGR03165, DH Haft, December 4, 2006) in the JCVI's Comprehensive Microbial Resource (Davidsen  et al. ,  2010 ) and as ‘ATPase_F1/F0-cplx_su2_Meth-typ’ (IPR017581) in the InterPro database (Hunter  et al. ,  2009 ), whereas UniProt uniformly annotates these proteins as ‘Putative uncharacterized protein’. The  urf2  (hereafter  atpR ) gene was found in every N-ATPase operon and could be used as a tell-tale sign of these operons. Its product is a hydrophobic protein with three predicted transmembrane segments, two of which carry conserved Arg residues ( Supplementary Fig. S5 ). Presence of charged residues in the hydrophobic core of the membrane is rare and usually indicative of a functional role. The N-ATPase operons also include so-called gene 1 (hereafter  atpQ ) that encodes another distinct membrane protein. Although this gene was originally marked as  atpI  (Sumi  et al. ,  1997 ) and is occasionally still labeled this way (Saum  et al. ,  2009 ), its product shows no statistically significant sequence similarity to the genuine AtpI (UncI) proteins, previously described in the genomes of  Escherichia coli  and other model organisms. In accordance with their distinct sequences, AtpI and AtpQ proteins are assigned to two different Pfam families, PF03899 and PF09527, respectively, and to two different COGs, COG3312 and COG5536 in the CDD database (Marchler-Bauer  et al. ,  2009 ). In contrast to  atpR , the  atpQ  gene is not limited to the N-ATPase operons. While the N-ATPase operon lacks the  atpH  gene that encodes the δ-subunit of the F-type ATPase, its  atpF  gene ( atpF N ) is unusually long (Sumi  et al. ,  1997 ). We were able to align C-terminal regions of the AtpF N  and AtpH gene products ( Supplementary Fig. S6 ). This showed that N-ATPase contains at least part of the δ-subunit, in agreement with the earlier analyses (Saum  et al. ,  2009 ). 3.2 Lateral transfer of N-ATPase genes Presence of the N-ATPase operon in the genomes of  M.barkeri  and  M.acetivorans , but not in the closely related  M.mazei , suggested that this operon had been acquired via lateral gene transfer. This suggestion is consistent with the gene neighborhoods of the  atpDCQRBEFG  operons in  M.barkeri  and  M.acetivorans  ( Fig. 2 ) and the absence of these operons in any other archaeal genomes sequenced so far. Gene neighborhoods of the N-ATPase operons in various bacteria are also consistent with the insertion of this operon ( Supplementary Fig. S7 ). The widespread presence of the N-ATPase genes among diverse bacteria deprecates the historical designation of these enzymes as ‘archaebacterial F 1 F o -ATPases’. The strict conservation of the gene order and co-linearity of the phylogenetic trees for distinct N-ATPase subunits suggests that the whole operon is being transferred as a single unit. However, the GC content of the N-ATPase operons shows a good correlation with the average GC content of the host genomes ( Supplementary Fig. S8 ), indicating either a relatively ancient gene transfer or a rapid adaptation of the N-ATPase genes to their host environment. An additional indication of the lateral mobility of the N-ATPase operon is its presence on plasmids, pREB4 in  A.marina  and pAQ7 in  Synechococcus  sp. PCC 7002. These data might be related to the earlier functional evidence of the presence of two ion-translocating ATPases in  M.mazei  Gö1. While the A-type ATP synthase was apparently H + -dependent, the second, F-type ATPase appeared to be Na + -translocating (Becher and Müller,  1994 ; Pisa  et al. ,  2007 ). Although this second ATPase has not been found in the sequenced genome of  M.mazei  ( Fig. 2 ), the respective genes could be plasmid-borne, as is the case of N-ATPases in at least two cyanobacteria.
 Fig. 2. Conserved gene neighborhoods in  M.acetivorans ,  M.mazei  and  M.barkeri , indicating the points of insertion of the N-ATPase operons in the former (top line, showing the subunit names) and the latter (bottom line, showing the gene names). Orthologous genes are indicated with the same colors. The arrows do not reflect the lengths of the genes. 4 DISCUSSION Following the description of an ‘archaebacterial F 1 F o -ATPase’ in  M.barkeri  (Sumi  et al. ,  1997 ), the presence of a ‘ Methanosarcina -like’ F-ATPase operon was repeatedly noted in bacterial genomes (Glöckner  et al. ,  2003 ; McInerney  et al. ,  2007 ; Swingley  et al. ,  2008 ), although the exact function(s) of these enzymes and their cation specificity remained obscure. McInerney  et al.  ( 2007 ) noted two F-type ATPases encoded in the genome of  Syntrophus aciditrophicus  and suggested that both of them were Na + -translocating (incidentally, the F-type ATPase of this organism is definitely H + -specific, whereas the cation specificity of its N-ATPase is unknown; it might be specific for Na + , see below). In their analysis of the  A.marina  genome, Swingley  et al.  ( 2008 ) noted the presence of a plasmid-encoded ‘set of ATP synthase genes that were arranged into a unique operon … conserved with full synteny in a remarkable array of organisms, including cyanobacteria, archaea, planctomycetes, chlorobi and proteobacteria’. The authors noted that the α-subunits encoded in these operons clustered together on a phylogenetic tree and suggested that these enzymes formed a separate new family of ATP synthases. However, they slightly overstated their case by claiming that its ‘individual proteins do not clearly fit into any of the described families’ (Swingley  et al. ,  2008 ). They also disputed the idea that these enzymes were Na + -translocating. In addition, the key observation by Daniel Haft that these enzymes always ‘represent a second F 1 /F o  ATPase system’ has only been published online in the JCVI's Comprehensive Microbial Resource (Davidsen  et al. ,  2010 ). As a result, there still exists a significant confusion as to the phylogenetic distribution, organization and the functional role(s) of these enzymes. The alignment in  Supplementary Fig. S1  shows that, despite the doubts of Swingley  et al.  ( 2008 ), the  c  subunit of the  A.marina  N-ATPase has a full set of Na + -binding ligands, including the recently recognized additional Thr residue of the ESTxxY motif (Mulkidjanian  et al. ,  2008a ; Meier  et al. ,  2009 ). While this residue is missing in  c -subunits of several N-ATPases, including the one from  S.aciditrophicus , a Glu residue is present instead of the Na + -coordinating Gln residue in the first transmembrane helix of the  c  subunit of nearly all N-ATPases ( Supplementary Fig. S1 ). As has been noted previously (Meier  et al. ,  2009 ; Saum  et al. ,  2009 ), this Glu residue could potentially provide two ligands for the Na +  ion and thereby complete the Na +  coordination shell. If so, all these N-ATPases would end up being capable of binding Na +  ions. A hallmark of the N-ATPase operons is the presence of the  atpR  gene. Because of the low dielectric permittivity of the membrane, the strategic positioning of two Arg residues of AtpR in the hydrophobic core of the membrane ( ) implies the presence of negatively charged residues in their vicinity. Given the absence in the N-ATPase operons of the  atpI  gene, whose product was recently shown to interact with the  c -ring (Suzuki  et al. ,  2007 ), we suggest that the product of the AtpR gene serves essentially the same function, regulating N-ATPase assembly and/or its activity. Just like AtpI assists  c -ring assembly by directly interacting with the  c -subunits (Suzuki  et al. ,  2007 ), AtpR could do that through the interaction of its two Arg residues with N-ATPase-specific  c -subunits, most of which carry two Glu residues in the middle of their transmembrane helices ( Supplementary Fig. S1 ). The observation that N-ATPases are always found alongside typical F- or A-type ATPases suggests that the N-ATPases cannot functionally replace those enzymes in their role as ATP synthases. Indeed, in  M.acetivorans , deletion of the N-ATPase operon had no visible effect on cell growth or ATP synthesis, whereas a mutant lacking the A-ATP synthase genes could not be obtained (Saum  et al. ,  2009 ). We conclude that, similarly to the eukaryotic V-ATPases, the N-ATPases do not catalyze ATP synthesis, which leaves ATP-driven ion pumping as the most plausible function for these enzymes. By analogy with V-ATPases, the N-ATPase  c -oligomer ring can be expected to consist of a smaller number of  c -subunits than the  c -oligomer ring of F-type ATP synthases. Acquisition of an operon capable of extrusion of Na +  ions would be beneficial to the marine bacteria and other organisms living in high-salt environments. Accordingly, many N-ATPase-encoding bacteria are either marine organisms or grow in the presence of salt. Since Na + -translocating ATPases can also translocate protons (von Ballmoos  et al. ,  2008 ), the N-ATPases could in principle function as outward proton pumps in low-sodium and/or acidic environments. A typical N-ATPase operon (with untranslated AtpR) has been found in an industrial strain of  Pseudomonas veronii  growing on 2-butanone (Onaca  et al. ,  2007 ). The presence of N-ATPase genes in such pathogens as  Burkholderia mallei  appears to be inherited from their free-living relatives and might be related to their survival in blood. Using a larger set of sequences than the one used by Swingley  et al.  ( 2008 ), we have confirmed that N-ATPases branch separately from other F-type ATPases ( Fig. 1 ,  Supplementary Fig. S3 ). This separate branching suggests a possible early divergence of the N-ATPases, which is compatible with the following, supposedly ancestral traits of these enzymes:
 Both AtpF N  subunit of the N-ATPase and the E subunit of the peripheral stalk of the V/A-type ATPases correspond to a fusion of b- and δ-subunits of the typical F-ATPases (Pallen  et al. ,  2006 ). The presence of the second membrane-embedded Glu residue is consistent with the evolution of the  c -subunit from a duplication of an amphiphilic helix that contained a Glu residue in the middle (Davis,  2002 ). Similar two-Glu  c -subunits are found in the A-type ATPases of methanogens and F-type ATPases of  Thermotogae . Outward pumping of Na +  ions, the predicted function of N-ATPases, appears to be an ancient trait and has been previously suggested as a function of the common ancestor of the F- and V-type ATPases (Mulkidjanian  et al. ,  2007 ;  2008a ,  b ;  2009 ). 
 All these features, which N-ATPases share either with V/A-ATPases—to the exclusion of most F-ATPases—or with the putative common ancestor of all rotary ATPases, suggest that N-ATPases represent a distinct early-diverging family of rotary ATPases. Thus, the Na-translocating common ancestor of all F-type ATPases apparently gave rise to two different families of ATPases: (i) the reversible ATPases/ATP synthases (‘genuine’ F-ATPases) and (ii) ATP-driven ion pumps (N-ATPases). In conclusion, the N-ATPases (until now usually referred to as ‘archaebacterial F 1 F o -ATPases’) are encoded in an apparently highly mobile operon that, most likely, confers on its hosts the ability of ATP-driven outward pumping of Na +  ions, which complements the H +  specificity of the native chromosome-encoded F-ATPase (or A-ATPase). We predict that, similarly to the eukaryotic V-ATPases, the N-ATPases do not catalyze ATP synthesis, which is why an N-ATPase is never found alone in a genome, only as the second enzyme in cells that already encode an F-type or a A-type ATP synthase. Accordingly, caution should be exercised when referring to their components as subunits of the ATP synthase. We believe that experimental verification of the predicted functions of the N-ATPases would be useful for the proper annotation of these interesting enzymes and could also help in understanding the evolution of energy conservation mechanisms. 
          Supplementary Material 
          
             
                [Supplementary Data] 
             
             
             
          
       </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>MicroRNA prediction with a novel ranking algorithm based on random walks</Title>
    <Doi>10.1093/bioinformatics/btn175</Doi>
    <Authors>Xu Yunpen, Zhou Xuefeng, Zhang Weixiong</Authors>
    <Abstract>MicroRNA (miRNAs) play essential roles in post-transcriptional gene regulation in animals and plants. Several existing computational approaches have been developed to complement experimental methods in discovery of miRNAs that express restrictively in specific environmental conditions or cell types. These computational methods require a sufficient number of characterized miRNAs as training samples, and rely on genome annotation to reduce the number of predicted putative miRNAs. However, most sequenced genomes have not been well annotated and many of them have a very few experimentally characterized miRNAs. As a result, the existing methods are not effective or even feasible for identifying miRNAs in these genomes.</Abstract>
    <Body>1 INTRODUCTION MicroRNAs (miRNAs) are endogenous single-stranded non-coding RNAs of ∼22 nt in length. They are derived from long precursors that fold into hairpin structures (Bartel,  2004 ; Jones-Rhoades  et al. ,  2006 ). miRNAs have been shown to play fundamentally important roles in animal and plant development (Bartel,  2004 ; Jones-Rhoades  et al. ,  2006 , in stress response in plants (Bari  et al. ,  2006 ; Chiou  et al. ,  2006 ; Jones-Rhoades and Bartel,  2004 ; Lu  et al. ,  2005 ; Sunkar and Zhu,  2004 ; Zhou  et al. ,  2007 ), and in genetic diseases including various types of cancer (Blenkiron  et al. ,  2007 ; Chen  et al. ,  2007 ; He  et al. ,  2007 ; Hobert,  2007 ; Ma  et al. ,  2007 ; Ozen  et al. ,  2008 ; Pedersen  et al. ,  2007 ; Tili  et al. ,  2007 ). In animals, most miRNAs bind to 3′ untranslated regions of their target mRNAs and repress the translation of the targets (Bartel,  2004 ). In contrast, in plants, most mature miRNAs directly base-pair with complementary sites in the coding regions of target mRNAs, resulting in cleavage or degradation of the targets (Bartel,  2004 ; Jones-Rhoades  et al. ,  2006 ). Identification of novel miRNA genes is an eminent and challenging problem towards the understanding of post-transcriptional gene regulation. Two major strategies for identifying novel miRNAs are experimental cloning and  in silico  prediction (Bartel,  2004 ; Berezikov  et al. ,  2006 ; Jones-Rhoades  et al. ,  2006 ). In a cloning-based approach, distinct ∼22 nt RNA transcripts were first isolated and then intensively cloned and sequenced. However, these methods are highly biased towards abundantly and/or ubiquitously expressed miRNAs; only abundant miRNA genes can be easily detected (Bartel,  2004 ; Berezikov  et al. ,  2006 ; Jones-Rhoades  et al. ,  2006 ). Note that not all miRNAs are well expressed in many tissues, cell types and developmental stages that have been tested (Bartel,  2004 ). Some miRNAs are expressed constitutively at low abundance, and some miRNAs have preferential or specific temporal and spatial expression patterns. Computational approaches have been developed to overcome some of the technical difficulties of experimental methods. First,  in silico  methods have been shown to be efficient for finding miRNAs that are expressed at constitutively low levels or in specific tissues (Bartel,  2004 ). Further, breakdown products of mRNA transcripts, other endogenous ncRNAs (e.g. tRNAs, rRNAs, nat-siRNAs) as well as exogenous siRNAs comprise a large portion of the non-coding small RNA population isolated from the cytoplasmic total RNA extracts (Bartel  2004 ; Berezikov  et al.   2006 ; Jones-Rhoades  et al.   2006 ; Lagos-Quintana  et al. ,  2001 ). To avoid erroneously designation of other non-coding small RNAs and even broken mRNA fragments as novel miRNAs, the secondary structures of flanking genomic sequences of cloned small RNAs are further assessed computationally. Only those small RNA sequences reside on arms of hairpin-structured precursors are likely to be miRNAs. However, due to their short length, cloned small RNAs may match to many genome regions that can potentially fold into hairpin structures which may not be unique to miRNAs. Thus, genome-wide screening of novel miRNA precursors is technically involved. Based on particular strategies adopted, the existing computational methods for miRNA prediction can be grouped into two categories. Methods in the first category were developed to find homologous miRNAs in  closely related  species. In these methods, known miRNA precursors were first folded into typical hairpin structures, local features in the hairpins were extracted, and extreme values of these featured were obtained from all known miRNAs. A filter was then constructed to screen novel hairpinned sequences. Those hairpinned sequences that passed the filter were further analyzed in related species to assess their evolutionary conservation (Berezikov  et al. ,  2005 ; Bonnet  et al. ,  2004 ; Grad  et al. ,  2003 ; Lai  et al. ,  2003 ). As they were designed for, these conservation-based methods built upon this framework were successful in detecting evolutionarilly conserved miRNAs. In order to further identify distantly homologous miRNAs, a probabilistic co-learning method using a paired hidden Markov model (HMM) was recently developed as a more general miRNA prediction method (Nam  et al. ,  2005 ). All these methods relied mainly on evolutionary conservation to eliminate a large number of false positive predictions. However, a substantial number of lineage- or species-specific miRNA genes do exist (Fahlgren  et al. ,  2007 ; Kasschau  et al. ,  2007 ; Lindow and Krogh,  2005 ; Molnar  et al. ,  2007 ; Rajagopalan  et al. ,  2006 ; Zhang  et al. ,  2006 ), which escape the detection of a conservation-based approach. In the second category, supervised classification methods, e.g. Support Vector Machines (SVMs), were adopted to train models based on positive sets of genuine miRNA precursors and negative sets of hairpins obtained from exon regions of protein coding genes (Hertel and Stadler,  2006 ; Ng and Mishra,  2007 ; Xue  et al. ,  2005 ). Thanks to the sequence features extracted from the training examples, such classification models were expected to preform well in predicting novel miRNAs from unseen sequences. However, many sequenced genomes are poorly annotated. Hence, it is difficult to obtain a good set of negative training samples for these methods. Moreover, miRNAs are located in intergenic regions and introns. A major task for miRNA prediction is to distinguish miRNA hairpins from other hairpin structures originated from intergenic or intronic sequences. In summary, classification-based methods are not effective on not well-annotated genomes. The methods in both categories discussed above all require a non-trivial set of positive samples of known miRNAs. Unfortunately, except well-studied model species such as human and  Caenorhabditis elegans , most sequenced genomes have a small number of miRNAs reported. For instance, only 38 miRNAs have been identified in the genome of  A.gambiae . This is particularly true for viral genomes for which not many miRNAs have been studied or reported (Stern-Ginossar  et al. ,  2007 ). Furthermore, the small number of known viral miRNAs rarely share any sequence homology (Stern-Ginossar  et al. ,  2007 ). Yet, viral miRNAs have been shown to play important roles in the pathology of viral infection by targeting host immune systems (Stern-Ginossar  et al. ,  2007 ). In short, the importance of miRNAs in post-transcriptional regulation, the lack of a sufficient number of known miRNAs and poorly annotated genomes collectively call for novel effective computational approaches to miRNA prediction. In this work, we propose and develop a novel ranking algorithm based on random works to computationally identify novel miRNAs from genomes with a few known miRNAs and with poor annotation. Our algorithm uses very few positive samples, requires no negative sample and does not rely on genome annotation. When applied to human data, our approach was able to identify known miRNAs with a relatively high precision and recall. As an application, we applied our algorithm to predict novel miRNAs in  A.gambiae , which is the most important vector of malaria in Africa and one of the most efficient malaria vectors in the world. Our analysis also showed that some of putative miRNAs encode mature miRNAs conserved in other species. 2 PROBLEM FORMULATION In our study, we cast the problem of miRNA prediction as a problem of information retrieval, in which novel miRNAs are to be retrieved from a pool of candidates by the known miRNAs as query samples. Specifically, we model this information retrieval process as belief propagation on a weighted graph, and develop a new ranking algorithm based on random walks. For a given species, the known miRNAs, putative candidate miRNAs and their relationship can be modeled by a weighted  graph   G =〈 V ,  E 〉. Each vertex  v ∈ V  in  G  represents a known miRNA precursor or a putative candidate, an edge  e ∈ E ⊆ V  ×  V  captures the relation between two vertices linked by the edge, and the  weight   w  of edge  e  quantifies the relation. In general, edge weights are determined by pairwise distances. For example, two closely related samples may share an edge with a large weight. The  degree  of vertex  v i  is  d i =∑ j   w ij , i.e. the total weight of all edges that are connected to  v i . Note that the graph is fully connected, so that there always exists a path between each pair of nodes in the graph. We refer the known miRNA precursors as  query samples  and putative candidates as  unknown samples . Consider query samples  X Q ={ x q 1 ,  x q 2 , …,  x qn } and unknown samples  X U ={ x u 1 ,  x u 2 , …,  x um }, where  x qi ,  x uj ,∈ℝ d . Our goal is to rank  X U  with respect to  X Q . To achieve this goal, we associate each sample  x i  with a relevancy value  f i , where  f i =1 for all query samples and  f i ∈[0, 1] for all unknown samples. A larger  f i  value means a higher relevancy of  x i  with respect to the queries. We then sort the relevancy values of all unknown samples and select the top ranked samples as retrieved samples, which constitute our predicted miRNA precursors. Therefore, the key to the ranking algorithm is to precisely compute the relevancy values of all unknown samples. In this study, we adopted the random walks method for this ranking problem, to be discussed next. 3 METHOD Query by samples is a paradigm for information retrieval in the information retrieval and machine learning fields. Zhou  et al.  proposed a manifold ranking method, which ranks the data with respect to the intrinsic manifold structure collectively revealed by the given data (Zhou  et al. ,  2004 ). Different from their work, our method uses Markov random walks to model a belief propagation process and therefore has a direct physical interpretation. 3.1 Ranking based on random walks Random walks is a classical stochastic process on a weighted finite state graph, which exploits the structure of the given data probabilistically. In the random walks formulation, each sample is treated as a graph vertex, which corresponds to a state on a Markov chain. The one-step transition probability  p ij  from vertex  v i  to vertex  v j  can be defined as
 (1) 
or written in the matrix form
 (2) 
 where  W =( w ij ) is the weight matrix,  D  is a diagonal matrix whose  i -th diagonal element is  d i . To facilitate our discussion, we reorder the vertices in a vector so that the query samples come first, followed by the unknown samples. We then partition the matrix as
 (3) 
 where  P QQ  is an  n  ×  n  transition matrix among the query states,  P QU  is an  n  ×  m  transition matrix from the query states to the unknown states,  P UQ  is a  m  ×  n  transition matrix from the unknown states to the query states and  P UU  is a  m  ×  m  transition matrix for the unknown states. Correspondingly, we partition the weight matrix  W  and the degree matrix  D  as
 (4) 
where  O  is a matrix with all 0. In our model, when a random walker transits from  v i  to  v j , it will transmit the current relevancy information of  v i  to  v j . This forms a dynamic process of relevancy information propagation: at each iterative step, a vertex transmits its relevancy information to its neighbors, and simultaneously receives the relevancy information from its neighbors. At the end of the iteration, each vertex updates its relevancy value according to the received information. Note that the query vertices only transmit their relevancy information and will never update their relevancy values. Furthermore, since query samples are more important than the unknown ones, the former are assigned higher weights for their relevancy information in transmission. This suggests the following relevancy updating rule,
 (5) 
where  k  is the iteration index, and α∈(0, 1) the weight of the relevancy from the unknown samples. In our method,  f i  is also called the ranking score of sample  i , which we use to rank the unknown samples. The matrix form of () is
 (6) 
By ( 2 ), () can be turned into
 (7) The convergence of the iterations is guaranteed by the following theorem. T HEOREM  1. The sequence     generated by the updating rule of  ()  converges when k approaches infinity . P ROOF . For convenience, let  , 𝔻=[0,1] m . Let  ,  . Then ( 7 ) can be written as
 (8) 
To finish our proof, we introduce a mapping  T  : 𝔻→𝔻 and the measure  d  on it as follows:
 (9) 
 (10) 
where  x i  is the  i -th element of vector  X . It is easy to show that (𝔻,  d ) is a complete metric space. According to the  Contractive Mapping Theorem of Banach  (Istratescu,  1981 ), it is suffice to prove that  T ( f ) is a contractive mapping, which holds if  R  satisfies
 (11) 
To prove ( 11 ), recall that 0 &lt; α &lt; 1, and
 (12) 
where  1 n =[1, 1, …, 1] T  is an  n -dimensional vector with all elements 1. It follows that
 (13) 
Therefore,  T ( f ) is a contractive mapping, and ( 7 ) converges to a unique fixed point.▪ Therefore, the limit of   and   can be substituted by  f U , resulting in
 (14) 
which is equivalent to
 (15) 
where  I  is an  m  ×  m  identity matrix. Therefore,
 (16) 
 3.2 The ranking algorithm The key to the above discussion and derivation is that we can compute the relevancy values of all unknown states according to ( 16 ) without actually performing the procedure of iterative random walks. Therefore, we propose the following ranking algorithm, which runs as follows:
 Step 1:  Construct graph  G =〈 V ,  E 〉. For each pair of vertices  x i  and  x j , we introduce an edge between them if they are close to each other. Step 2:  Measure graph weights  W . Here a heat kernel is adopted, i.e. if  x i  and  x j  are connected by an edge, the weight of the edge is defined as
 
where  d (·,·) is a distance measure defined on the graph, σ is the heat kernel parameter. Usually,  d ( i , j ) is the Euclidian distance between samples  i  and  j . Step 3:  Compute relevancy values of the samples by solving the matrix problem in equation ( 16 ). Step 4:  Rank the samples. Sort the relevancy values of all samples, and select some top ranked samples as the final result. For the problem of predicting new miRNAs, each sample is represented by a vector of 36 features ( Section 3.3 ), and distance between two samples is the Euclidean distance of the feature vectors. In order to reduce the data noise and computational expense, we make the graph sparse by removing the ‘weak’ edges with low weights. Since putative miRNAs are to be retrieved by known miRNAs according to the ranking scores of the candidates, we named our miRNA prediction method as  miRank . 3.3 Extraction of global and local sequence-structure features The most salient characteristics of miRNA precursors are their hairpinned secondary structures. In our study, we used RNAfold (Hofacker,  2003 ) to predict RNA secondary structures. We postulated that the entire hairpin structure of a miRNA precursor could be characterized solely by 36 global and local intrinsic attributes that capture sequence, structural and topological properties of the miRNA precursor. Four important global features at the structural and topological levels are the normalized minimum free energy of folding (MFE), the normalized base-pairing propensities (number of nucleotides that are paired) of both arms and the normalized loop length. Here the normalization factor is the length of the precursor sequence. Local features characterize various sequence properties in RNA sequences, which we discuss using an example. As shown in  Figure 1 , in the hairpin structure from RNAfold (Hofacker,  2003 ), each nucleotide in the sequence is either paired (indicated by a bracket, ‘(’ or ‘)’) or unpaired (denoted by a dot, ‘.’). The left bracket ‘(’ indicates that a nucleotide is located in the 5′ arm and the paired right bracket ‘)’ means the correspondingly paired nucleotide is in the 3′ arm. No evidence has shown that mature miRNAs have a preference of the 5′ or 3′ arms of their hairpinned precursors. Thus, it is not necessary to distinguish ‘(’ and ‘)’ in the local feature extraction; we use ‘(’ for both situations in the subsequent discussion. For any three adjacent nucleotides, there are eight (2 3 ) possible structure configurations: ‘(((’, ‘((.’, ‘(..’, ‘(.(’, ‘.((’, ‘.(.’, ‘..(’ and ‘·s’. We further differentiate each of these eight configurations by the nucleotide—A,C,G,U—in the middle position. Thus, there are 32 (4 × 8) possible sequence-structure configurations for each triplet in the precursors of miRNAs. Similar features have been used in some of the existing methods, e.g. that in Xue  et al.  ( 2005 ).
 Fig. 1. Extraction of local structure-sequence features. ( a ): the sequence and secondary structure of  hsa-mir-20a . ( b ): the configurations of 32 local sequence-structure features. Frequencies of the 32 features are obtained for each miRNA precursor or putative candidate, and further normalized by the length of the corresponding sequence. 
 4 EXPERIMENTAL EVALUATIONS AND DISCUSSIONS 4.1 Evaluation of the ranking algorithm on toy examples In order to show the belief propagation of our new ranking algorithm, along the manifold of the data, we generated a set of toy data, selected one data point as a query sample, and queried the rest samples to rank them. Figure 2  shows the results on two toy examples, where data are two-dimensional points distributed on X–Y plates. In each of these examples, we chose one point as the query, which is marked by a red plus. The rest points were treated as unlabeled data. In  Figure 1 , the relevancy values are color coded, i.e. a red indicates a high-relevancy value and a blue indicates a low-relevancy value. It can be seen that the method successfully propagates the query belief to the unknown samples so that the samples that are closer to the query have higher relevancy values.
 Fig. 2 Two toy examples of information propagation using the Markov random walks algorithm. Each dot represents a two-dimensional point distributed on an X-Y plate. In each figure, the dot marked by a red plus is the query. (See online version for the color figure). 
 4.2 Datasets for miRNA prediction All reported miRNAs precursor sequences of  H.sapiens  and  A.gambiae  (533 and 38, respectively) were downloaded from the miRBase (http://microrna.sanger.ac.uk/sequences/) (Griffiths-Jones  et al. ,  2006 ) as of September 1, 2007. Genome sequences of  H.sapiens  and  A.gambiae  were retrieved from UCSC Genome Browser (http://genome.ucsc.edu/). For the  H.sapiens  genome, we randomly extracted non-overlapping fragments of 90 nt from the genome so that no information of genome annotation was used. We first discarded all fragments overlapping with known miRNA precursors. For a fragment not overlapping with any known  H.sapiens  miRNA precursor, we further predicted its secondary structure using RNAfold (Hofacker,  2003 ). Nevertheless, not every such fragment would be kept for further analysis. The criteria for retaining hairpinned fragments are as follows: minimum 18 base pairings on the stem of the hairpin structure, maximum -12 kcal/mol free energy of the secondary structure and no multiple loops. The threshold 18 is the lowest number of base pairings, and the threshold -12 is the highest free energy among all genuine animal miRNA precursors. Finally, 1000 of such fragments were arbitrarily chosen and pooled together with some of the known human miRNA precursors—which were all known human miRNA precursors except the ones used as query samples in our experiments—to form the pool of candidates to be ranked by the miRank algorithm. The reason we added those known human miRNA precursors to this pool of samples is to evaluate the prediction performance of the miRank algorithm. Every chromosome of  A.gambiae  was fragmented, from 5′-end to 3′-end, using a sliding window of 90 nt and a shift increment of 45 nt. These fragments were folded using RNAfold (Hofacker,  2003 ), and hairpinned fragments were selected by the same criteria described above. The chosen hairpin sequences formed the initial candidate pool. In the fragmentation, some putative candidates might be cut into two pieces, and have lost their hairpin structures, hence were excluded from the candidate pool. To avoid this, we further fragmented, with the same sliding window and increment, the sequences between each pair of hairpinned fragments next to each other. The secondary structures of the new set of fragments were predicted and selected by the same tool and criteria. This process was iterated until no hairpinned fragment could be found. We obtained 22 297 hairpinned sequences, which included all 38 known miRNAs precursors for  A.gambiae . 4.3 Evaluation on human miRNAs We first evaluated the performance of our miRank algorithm on  H.sapiens  data based on the known miRNA precursors embedded in the pool of samples to be ranked ( Section 4.2 ). The prediction quality was assessed by the recall and the precision, which are, respectively, defined as:
 (17) 
 (18) 
where  TP ,  FP  and  FN  are numbers of true positive predictions, false positive predictions and false negative predictions, respectively. The number of query samples is the most critical parameter for algorithm miRank. We tested on  H. sapiens  data with 1, 5, 10, 15, 20 and 50 known miRNA precursors randomly chosen as query samples. To reiterate, in each of these experiments, the rest known miRNA precursors were combined with the 1000 hairpin sequences extracted from the genome to form the pool of candidates to be ranked ( Section 4.2 ). In each experiment, we chose  n  topmost ranked candidates, and determined the precision and recall of the result by comparing the chosen candidates with the known human miRNAs that were hidden in the candidate pool. By varying the number  n , we obtained the receiver operating characteristic (ROC) curves (Spackman,  1989 ) for the performance of miRank using different number of query samples, as shown in  Figure 3 .
 Fig. 3 Precision-recall curves obtained by setting different number of queries. All experiments were performed on human data. The curves from right to left show the results of experiments with query number equal to 1, 5, 10, 15, 20, 50 (See online version for the color figure). 
 In the extreme case of querying with only one sample, we obtained a precision greater than 70% in the 20 topmost retrieved samples. When we queried with more than 20 query samples, we obtained a high precision of over 95% in the 160 top ranked samples. These results suggest that we can expect miRank to be effective in predicting miRNAs on a species with a small number of reported miRNAs and a poorly annotated genome. Note that the precisions in these experiments could be under-estimated because genuine but unvalidated miRNA precursors may actually exist in the 1000 human hairpinned sequences that were analyzed in the experiment. Candidates with higher ranks are most likely to be true miRNAs; they are excellent candidates for further experimental validation. To obtain as many putative miRNAs with high confidence as possible, we need to set a ranking score cutoff.  Figure 4  shows the prediction precisions with respect to ranking score cutoffs in each of these six experiments. Evidently, each curve in  Figure 4  has an inflection point. The precise inflection points in  Figure 4  can be obtained by second-order differential. Notably, in the experiments with more query samples, the ranking scores of unknown sample tend to be higher, and prediction precisions by setting the scores at the inflection points as cutoffs are also improved. For each experiment, the score at the inflection point can be a reasonable cutoff that gives the largest number of predictions with a low false positive prediction rate. The second row in  Table 1  shows the recalls and precisions of these six experiments with scores at corresponding inflection points as cutoffs. In these experiments, the samples with ranking scores greater than the cutoffs at the inflection points of corresponding curves in  Figure 4  are about 25% of the sizes of the candidate pool.
 Fig. 4 Precision-cutoff score curves of six experiments on human data with different number of queries. 
 In order to further evaluate miRank, we compared it with some supervised classification algorithms (Hertel and Stadler,  2006 ; Ng and Mishra,  2007 ; Xue  et al. ,  2005 ), which are the best existing miRNA prediction algorithms, on the human data. Since these existing methods are Support Vector Machines (SVMs)-based classification models trained with more than a hundred known miRNA precursors, it is difficult to directly compare miRank with these methods. To overcome the difficulty, we followed the strategy in (Xue  et al. ,  2005 ), and trained six SVM-based models with 1, 5, 10, 15, 20 and 50 known miRNA precursors as positive training examples. We included twice as many negative samples as positive samples in the training sets as that gave the best results. All training examples and candidates used by the existing methods were represented by the same features as used by miRank. The results in  Table 1  show that miRank is more accurate than the SVM-based methods. With fewer known miRNA precursors, miRank outperforms them all. Essentially, miRank possesses the advantage of Semi-supervised Learning (Zhu,  2005 ), which incorporates the data distribution information of the unlabeled data in the training process. Therefore, miRank has a much better performance than SVMs when the number of training samples is small.
 Table 1. Test results on human data Name No. pos samples a 1 5 10 15 20 50 miRank Recall b 0.358 0.691 0.69 0.71 0.707 0.682 Precision c 0.64 0.75 0.805 0.845 0.86 0.939 SVM Recall d 0.68 0.621 0.709 0.715 0.719 0.720 Precision e 0.318 0.631 0.698 0.703 0.735 0.840 miRank Recall f 0.68 0.621 0.709 0.715 0.719 0.720 Precision g 0.429 0.765 0.795 0.836 0.853 0.888 a  Number of positive samples.  b , c  Recall and precision of miRank when the scores at the inflection points of the corresponding curves in  Figure 4  were set as cutoffs.  d , e  Recall and precision of the SVM-based methods.  f ,  g  Precisions of miRank were obtained by retrieving enough predictions to reach the same recalls as the SVM-based models. 4.4 Novel miRNA genes in  A.gambiae Anopheles gambiae  is one of the most important vectors of malaria in Africa, and one of the most efficient malaria vectors in the world. Identification of miRNAs is important for the study of the development of  A. gambiae , hence may broaden our perspectives on the control of the prevalence of malaria (Moffett  et al. ,  2007 ). To date, only 38 miRNAs of  A.gambiae  have been reported and curated in miRBase (Griffiths-Jones  et al. ,  2006 ), and its genome is not well annotated. As discussed in  Section 1 , the existing methods are not effective in predicting novel miRNAs in  A.gambiae . We took the 38 miRNAs curated in miRBase (Griffiths-Jones  et al. ,  2006 ) as query samples, and applied miRank to 22 259  A.gambiae  candidate sequences that have hairpinned secondary structures ( Section 4.2 ). Candidates with higher ranks are most likely to be true miRNAs, as indicated by  Figure 3  on the human data, and are thus suitable for further experimental validation. We took the top 200 candidates as our prediction. A further analysis showed that among these 200 candidates, 76 contain matured miRNAs that are conserved in at least one other animal species. Conservation across related species has been widely exploited for predicting miRNAs in animals and plants. Three major observations have been made on known conserved miRNAs (Reinhart  et al. ,  2002 ). First, the mature miRNA sequences are conserved, whereas the rest of the precursor sequences can be diverged. Second, the propensity of precursor sequences to form hairpinned secondary structures is conserved, although the actual structures may vary. Third, the conserved mature miRNA homologs are mostly located on the same arm of hairpinned secondary structures. In our research, we used hairpin structures as the most significant features to extract 22 259 candidates. We further analyzed the conservation of mature miRNAs of our top candidates.  Figure 5  shows the distribution of the conserved mature miRNAs in the top 500 candidates. As shown, the highly ranked candidates are more likely to be evolutionarilly conserved. Note that all homologs of these conserved candidates are located on the same arms of the hairpins in corresponding species.  Figure 6  shows two examples of novel putative miRNAs that we predicted. According to the IDs of their homologs in other species, we named these two miRNAs as aga-mir-135 and aga-mir-49.
 Fig. 5 Percentage of conserved miRNAs in different sets of putative candidates. X-axis shows the sets of putative candidates with different ranks. For instance, ‘200’ means that candidates in this set rank from 151 to 200. 
 Fig. 6. Two novel predicted miRNAs in  A.gambiae  which are conserved in more than one other animal species. According to the IDs of their homologs in other species, we named them  aga-mir-135 , and  aga-mir-49 , respectively. The precursor sequences of homologs in different species are shown and the mature miRNAs are highlighted and underscored.  mmu-mir-135b ,  rno-mir-135b ,  mdo-mir135b  and  hsa-mir-135b  are homologs of mir-135 in mouse, rat,  Monodelphis domestica  and human, respectively.  cel-mir-49  and  cbr-mir-49  are homologs of mir-49 in  C.elegans  and  C.briggsae . 
 5 CONCLUSIONS AND FINAL REMARKS In this study, we cast the problem of miRNA prediction as a problem of information retrieval where novel miRNAs were retrieved by the known miRNAs (as query samples) from a genome-scale pool of candidate sequences that can form hairpinned structures. We modeled the novel miRNA retrieval process by a process of belief propagation on a weighted graph, which was constructed from known miRNAs and candidate RNA sequences to be analyzed. We then developed a novel ranking algorithm based on random walks to propagate information of known miRNAs to candidates. We named our final miRNA prediction method based on ranking  miRank . The miRank method has the following remarkable properties. First, it does not require information of genome annotation. This is particularly important because many sequenced genomes have not been well annotated, and their closely related species are yet to be sequenced. Thus, a large number of false positive candidates with hairpinned secondary structures cannot be filtered out with genome annotation or by phylogenetic conservation. miRank can be applied to such newly sequenced genomes with little annotation. Second, it does not rely on cross-species conservation so that it can identify species-specific miRNAs. Third, miRank is able to accommodate a small number of known miRNAs while enjoys a high-prediction accuracy. Hence, miRank is a useful tool for many species including most viruses that have a very few reported miRNAs. Identification of novel miRNAs in various viral genomes will be our future research topic. To demonstrate these favorable features of the miRank algorithm, we tested it on human data where more than 530 miRNAs have been reported which we used to validate our result. miRank achieved a prediction accuracy of more than 95% using a small number of known miRNAs as labeled samples. We also applied miRank to  A.gambiae  to predict 200 novel miRNA precursors. Our result showed that 78 of the novel miRNA precursors encode mature miRNAs that are conserved in at least one other animal species. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>A geometric approach for classification and comparison of structural variants</Title>
    <Doi>10.1093/bioinformatics/btp208</Doi>
    <Authors>Sindi Suzanne, Helman Elena, Bashir Ali, Raphael Benjamin J.</Authors>
    <Abstract>Motivation: Structural variants, including duplications, insertions, deletions and inversions of large blocks of DNA sequence, are an important contributor to human genome variation. Measuring structural variants in a genome sequence is typically more challenging than measuring single nucleotide changes. Current approaches for structural variant identification, including paired-end DNA sequencing/mapping and array comparative genomic hybridization (aCGH), do not identify the boundaries of variants precisely. Consequently, most reported human structural variants are poorly defined and not readily compared across different studies and measurement techniques.</Abstract>
    <Body>1 INTRODUCTION Characterizing the DNA sequence differences that distinguish individuals is a major challenge in human genetics. Until recently, these differences were thought to be mostly single nucleotide changes. There is increasing appreciation for the prevalence of structural variation, including duplications, deletions and inversions of large blocks of DNA sequence, in the human genome (Sharp  et al. ,  2006 ). Structural variants have recently been linked to diseases such as autism (Marshall  et al. ,  2008 ), and cataloging these variants is an important step in determining the genetic basis of disease. Structural variants typically span thousands of nucleotides and are more difficult to define than single nucleotide polymorphisms (SNPs). Two techniques have been used to identify structural variants in the human genome:  array comparative genomic hybridization (aCGH)  and  end sequence profiling (ESP) , also called  paired-end mapping . Both of these techniques were first developed for the analysis of somatic structural rearrangements in cancer genomes (Pinkel and Albertson,  2005 ; Pinkel  et al. ,  1998 ; Volik  et al. ,  2003 ) and later applied to discover structural variation in normal genomes (Iafrate  et al. ,  2004 ; Kidd  et al. ,  2008 ; Korbel  et al. ,  2007 ; Sebat  et al. ,  2004 ; Tuzun  et al. ,  2005 ). In aCGH, differentially fluorescently labeled DNA from test and reference genomes are hybridized to an array of genomic probes derived from the reference genome. Measurements of test:reference fluorescence ratio at each probe identifies locations of the test genome that are present in higher or lower copy in the reference genome. This technique detects copy number variants but is blind to copy neutral variants such as inversions. In paired-end mapping approaches, DNA fragments, or clones, from a test genome are sequenced from both ends, and these sequences are mapped to a reference genome sequence. Pairs of end sequences, called  end sequence pairs  or  mate pairs , with discordant mappings identify inversions, translocations, transpositions, duplications and deletions that distinguish the test genome from the reference genome. Next-generation DNA sequencing technologies such as those from Illumina, Applied Biosystems and 454 Life Sciences now make it possible to apply this approach to a large number of individuals. We distinguish paired-end mapping from whole-genome assembly, which would provide the ultimate dataset for studies of structural variation (Levy  et al. ,  2007 ; Wheeler  et al. ,  2008 ), but remains cost prohibitive for a large number of human genomes. Both aCGH and paired-end mapping do not precisely identify the boundaries, or  breakpoints , of the measured variant. In aCGH, a breakpoint is localized only to the distance between the genomic probes straddling the copy number change, while in paired-end mapping the localization depends on the number and size of fragments that span the variant ( Fig. 1 A). There are no standard methods for identifying the boundaries of structural variants in paired-end mapping studies and different heuristics have been used (Kidd  et al. ,  2008 ; Korbel  et al. ,  2007 ). Remarkably, despite ambiguity resulting from both measurement and analysis, published studies of structural variants report the breakpoints to single nucleotide resolution without revealing the measurement uncertainty, or ‘error bars’, in the localization of the variant. Consequently, the existing databases of human structural variants (Iafrate  et al. ,  2004 ) also do not contain information on the uncertainty of the breakpoints.
 Fig. 1. Derivation of regions of uncertainty ( breakpoint regions ) from ESP and aCGH data. ( A ) (Top panel) In ESP, or paired-end mapping, both ends of a fragment of a test genome are sequenced and aligned to the reference genome. Here, alignment of ends of fragments  C  and  D  yields ES pairs ( x C ,  y C ) and ( x D ,  y D ) on the reference genome that suggest an inversion. (Bottom panel) The intersection of breakpoint regions defined by Equation ( 1 ) indicates the possible locations of inversion breakpoints  a  and  b  that are consistent with the ES pairs. ( B ) (Top panel) In an aCGH experiment, the reference genome is segmented into regions of equal copy number according to measurements at genomic probes (boxes). A deletion with breakpoints  a  and  b  is identified as a change in copy number between probes  p i  and  p i +1  and between probes  p j  and  p j +1 . (Bottom panel) The intervals [ p i ,  p i +1 ] and [ p j ,  p j +1 ] define a rectangular breakpoint region. This region is intersected with the breakpoint region defined by an ES pair ( x C ,  y C ) to refine the locations  a  and  b  of the deletion. Each new study of structural variation compares the newly discovered variants to those previously reported. In addition to the ambiguities described above, there is also the problem of deciding when two variants (perhaps measured via different approaches) are the same. The usual approach is to define two variants to be the same if they are ‘near’ each other, where near is defined using an arbitrary and study-dependent threshold. With such an approach, there is no assurance that the two variants are indeed the same, or are merely closely located on the genome. The situation is further exacerbated by reports that different human structural variants may overlap or have multiple states (Perry  et al. ,  2008 ; Scherer  et al. ,  2007 ), and that recurrent (but not identical) variants may exist at the same locus. Standard methods for defining structural variants and publicly available tools for comparing structural variants across different studies are urgently needed. Two recent works introduced more refined approaches for analysis of structural variants and are promising steps in this direction. Lee  et al.  ( 2008 ) describe a probabilistic method for resolving ambiguities in mapping end-sequenced fragments using the distribution of fragment lengths in a single sample. Bashir  et al.  ( 2008 ) estimate the probability that paired-end sequenced clones from cancer genomes contain fusion genes and explicitly incorporate the uncertainty in measurement of rearrangement breakpoint into their calculation. Neither of these approaches address the comparison of variants across multiple samples, and are further limited in their handling of measurement uncertainty and consideration of all classes of structural variants, respectively. Here, we introduce a general geometric framework for classification and comparison of structural variants. Our approach provides a principled way to cluster multiple measurements of a variant in a single sample and to compare variants across samples. We explicitly model the underlying measurement uncertainty of both paired-end mapping (from both older and next-generation sequencing technologies) and aCGH. We represent the uncertainty in the measurement of a structural variant, which we refer to as the  breakpoint region , as a polygon in the plane. We formulate the problems of comparing variants as computing all intersections and maximal intersections of breakpoint regions. These formulations allow the user to examine conserved variants at varying levels of granularity, instead of only producing a single best cluster of overlapping variants. We derive an efficient plane sweep algorithm from computational geometry to compute these intersections. We demonstrate our Geometric Analysis of Structural Variants (GASV) program with three applications. First, we apply our method to recent paired-end sequencing studies of nine human individuals. We show that GASV identifies rearrangement breakpoints with high precision. In dozens of cases, we localize rearrangement breakpoints to &lt;2.5 kb by combining the measurements from ≈40 kb clones across multiple individuals. In the most extreme example, eight end-sequenced clones from four different individuals localize the inversion breakpoints to within 286 bp. Such precise localization was not reported in the original published analysis of these nine individuals. Moreover, we show that the published locations of many variants are different from the breakpoints supported by the data. Second, we perform a comparative analysis of variants from the nine normal individuals with variants identified in paired-end sequencing of several cancer samples. We find that a significant fraction (5–53%) of rearrangements identified in the cancer genomes are consistent with inversion and deletion variants found in the normal genomes. Finally, we show how GASV integrates both aCGH and paired-end sequencing measurements of variants in three cancer genomes. Our geometric method for multi-sample and multi-platform identification and comparison of structural variants should prove useful for studies of human structural variation such as the 1000 Genomes Project and for cancer genome sequencing studies such as The Cancer Genome Atlas. 2 METHODS Consider a  reference genome  represented as a single interval  G  (i.e. we concatenate multiple chromosomes) and a closely related  test genome . We define a structural variant to be a difference between a  test genome  and  reference genome  that is due to a rearrangement resulting from DNA breakage followed by a aberrant repair or insertion of a new DNA. Structural variants include inversions, translocations, transpositions, and insertions/deletions. Each of these variants is thus associated with a set of breakpoints where DNA breaks and/or repair occurs. For example, an inversion is a result of the reference genome being cut at two genomic coordinates,  a  and  b , and the DNA segment between  a  and  b  flipped in the test genome so that the nucleotide at position  a  − 1 is adjacent to the nucleotide at position  b  and  a  is adjacent to  b  + 1 ( Fig. 1 A). Similarly, a deletion is defined by coordinates  a  and  b  in the reference such that  a  − 1 is joined to  b  + 1 in the test genome ( Fig. 1 B). Note that this is a simplification of the underlying biology, as there are sometimes small insertions or deletions at breakpoints, but these small changes have limited effect on the analysis of larger structural variants. 2.1 Breakpoint regions and variant uncertainty Neither paired-end mapping nor aCGH measure the breakpoints of a structural variant exactly. Rather, each technique localizes breakpoints to a region of the reference genome, which we refer to as the  breakpoint region . We describe the derivation of this region for each of these experimental techniques. 2.1.1 Paired-end mapping In the paired-end mapping, or ESP, fragments of genomic DNA from a test genome are sequenced from both ends, and the resulting pair of end sequences are aligned to the reference genome. We assume that each fragment 1   C  has ends that map uniquely to the reference genome. Thus, each fragment  C  corresponds to a pair of locations in the reference genome where the end sequences map. An end sequence may align to either DNA strand, and so each mapped end has a sign (+ or −) indicating the mapped strand. We call such a signed pair ( x C ,  y C ) an end sequence pair ( ES pair ), where by convention | x C |&lt;| y C |. Typically, the length of the fragment,  L C , is known to lie within a range [ L min ,  L max ]. Fragment sizes range from ≈150 kb for BAC clones to a few hundred base pairs for next-generation sequencing methods. We say that a ES pair is a  valid pair  (Raphael  et al. ,  2003 ) if the ends have opposite, convergent orientations and the distance between the mapped ends is within the range of fragment lengths: i.e. (+ x C , − y C ) is valid if  L min ≤| y |−| x |≤ L max . Otherwise, if the ends have abnormal distance or orientation, we say that the pair is an  invalid pair . Invalid pairs indicate putative genome rearrangements or possibly mapping/assembly errors. For concreteness, consider the case of a test genome that differs from the reference genome by a single inversion with breakpoints  a  and  b  ( Fig. 1 A) that fuse at a single coordinate ζ in the test genome. A fragment  C  from the test genome with length between  L min  and  L max  and containing ζ is end-sequenced. The resulting ES pair ( x C ,  y C ) will be an invalid pair indicating that  C  is not a contiguous piece of the reference genome ( Fig. 1 A). The invalid pair ( x C ,  y C ) does not uniquely identify the breakpoint ( a ,  b ). However, if we assume that: (i) only a  single  breakpoint is contained in the fragment  C ; and (ii)  a  &gt;  x C  and  b  &gt;  y C  (without loss of generality); then the length  L C  of  C  is equal to ( a  −  x C )+( b − y C ). Thus, a breakpoint ( a ,  b ) that is consistent with ( x C ,  y C ) must satisfy
 (1) 
We define the  breakpoint region B ( C ) of an invalid fragment  C  to be the breakpoints ( a ,  b ) satisfying the above equation. The constraint ( 1 ) has a straightforward geometric interpretation: if we plot an invalid pair ( x C ,  y C ) as a point in the 2D space  G  ×  G  then the breakpoint region defines a trapezoid ( Fig. 1 A). We emphasize that  a  and  b  cannot be chosen independently; doing so corresponds to defining the breakpoint region to be a rectangle, and allows breakpoints that give insert sizes outside the allowed range [ L min ,  L max ]. If another fragment  D  contains the same fusion point ζ, then the corresponding breakpoint ( a ,  b ) lies within the intersection  B ( C )∩ B ( D ) of the trapezoids  B ( C ) and  B ( D ) ( Fig. 1 A). Conversely, we will assume that if the trapezoids defined by several invalid pairs intersect, then they share a common breakpoint. As the number of fragments that are end-sequenced increases, more fragments will contain the same fusion point and the area of the intersection of breakpoint regions will decrease. Thus, the uncertainty in the location of the breakpoint ( a ,  b ) decreases. We define a  cluster  to be a set of fragments whose breakpoint regions have non-empty intersection. The description above generalizes to other types of structural variants including translocations, insertions, deletions and transpositions. For example, invalid pairs with sign( x C )=sign( y C )=− also indicate inversions (corresponding to the other fusion point), while invalid pairs with sign( x C )=+ and sign( y C )=− indicate insertions or deletions. Fragments with ends mapped to different chromosomes indicate translocations. As above, we assume that the fragment  C  contains only a  single  breakpoint. The breakpoints ( a , b ) that are consistent with the invalid pair ( x C ,  y C ) satisfy the inequalities
 (2) 
This equation generalizes ( 1 ) and is summarized by the rule: ‘end sequences point toward the breakpoint’. 2.1.2 Array comparative genomic hybridization In aCGH, a  breakpoint region  is defined as the genomic interval between the two adjacent probes  p i  and  p i +1  that define the endpoints of segments with unequal copy number ( Fig. 1 B). A pair of such breakpoint regions (e.g. those resulting from a deletion) give two intervals  U =[ p i ,  p i +1 ] and  V =[ p j , p j +1 ] that define a rectangle  U  ×  V  in 2D space  G  ×  G . This rectangle determines the locations of breakpoints ( a ,  b )∈ U  ×  V  consistent with the segmentation. Note that in addition to fragments that span deletions ( Fig. 1 B), the boundaries of aCGH segments often indicate the locations of other types of rearrangements including translocations (Aerni  et al. ,  2009 ; Campbell  et al. ,  2008 ). 2.2 Efficient computation of overlapping breakpoint regions Given a set  B 1 ,…,  B n  of breakpoint regions, our goal is to identify subsets of intersecting breakpoint regions. Such a subset suggests these breakpoint regions are multiple measurements of the same structural variant. In addition, we want to identify all such regions of intersection, and to label these by the breakpoint regions that are part of the intersection. We formalize these problems as follows:
 A ll  I ntersections of  B reakpoint  R egions .  Given a set  ℬ={ B 1 ,…,  B n }  of breakpoint regions ,  identify and label all non-empty intersections of subsets of  ℬ. 
 Since each breakpoint region  B i  is a convex polygon (trapezoid or rectangle), the solution to the above problem relies on computing intersections of convex polygons, a well-known problem in computational geometry (Preparata and Shamos,  1985 ). A naive brute-force approach that checks all 2 n  subsets of ℬ for intersection is very inefficient. Moreover, a single breakpoint region can have distinct intersections with different subsets of other breakpoint regions ( Fig. 2 ). Thus, it is not sufficient to consider only pairwise intersections or iteratively merge breakpoint regions to existing intersections. Below, we describe an efficient plane sweep algorithm that solves the ‘All Intersections Problem’.
 Fig. 2. Breakpoint regions determined by fragments from Kidd  et al.  ( 2008 ) whose orientations suggest an inversion variant(s). Breakpoint region 2 has distinct intersections with regions 1 and 3, and thus iterative merging of breakpoint regions will not identify all intersections. While the ‘All Intersections Problem’ provides the most comprehensive description of the overlaps between breakpoint regions, the output can be quite large since the number of regions of intersections grows rapidly as  n  increases. However, many of these regions of intersection are not interesting because they are dominated by intersections of a larger number of breakpoint regions. For example, if three breakpoint regions  B i ,  B j  and  B k  have a non-empty intersection, then reporting this intersection is perhaps more desirable than reporting the (geometrically larger) intersections  B i ∩ B j  and  B i ∩ B k , particularly as the number of such intersecting regions becomes large. Thus, it is desired to identify regions of intersection of a maximal number of  B i . We formalize this problem by defining a partial order on intersections of subsets of ℬ. For  S ⊆{1,…,  n }, let  B ∩ S =∩ s ∈ S B s  denote the intersection of the breakpoint regions indexed by  S . Let ℐ n  be the set of subsets of {1,…,  n } whose corresponding breakpoint regions have non-empty intersection. Formally,
 (3) 
ℐ n  has the natural partial order of subset inclusion ⊆, where for two elements  I  and  J  of ℐ n ,  I  ≺  J  provided  I ⊆ J . We denote this partially ordered set (poset) as (ℐ n , ⊆). We formalize the problem as follows. M aximal  I ntersections of  B reakpoint  R egions .  Given a set  ℬ={ B 1 ,…,  B n }  of breakpoint regions ,  identify all maximal elements of  (ℐ n , ⊆). Below, we describe how to solve the ‘Maximal Intersections Problem’ by extending the plane sweep algorithm for the ‘All Intersections Problem’. 2.3 Plane sweep algorithm The plane sweep algorithm was introduced by Shamos and Hoey ( 1976 ) for the problem of determining whether  n  line segments in the plane have any intersections. Clearly this question can be answered in  O ( n 2 ) time by checking all pairs of segments for intersection. A plane sweep algorithm performs the same task in  O ( n  log  n ) time by first sorting the segments by the  x -coordinate of their left endpoint, and then moving the line  x = c , called the  sweep line  through the plane from left to right. The efficiency of the plane sweep algorithm is derived from two observations. First, not all coordinates  c  need to be considered. A data structure called the event-point schedule ℰ records the necessary values of  c , and is updated dynamically as the sweep line moves from left to right. Second, for a given position  c  of the sweep line, the segments intersecting the sweep line can be ordered by the  y -coordinate of the intersection. These ordered segments are stored in a data structure called sweep-line status ℒ. Only adjacent segments in ℒ need to be examined for intersections. By employing appropriate data structures for ℰ and ℒ one obtains an efficient algorithm for segment intersection. Further details of this algorithm can be found in Preparata and Shamos ( 1985 ). The basic framework of the plane sweep algorithm has been extended to numerous related problems in computational geometry such as the counting of the  k  intersections of  n  segments in provably optimal  O ( n  log  n + k ) time (Chazelle and Edelsbrunner,  1992 ), and reporting the regions of intersection of polygons in the plane (Nievergelt and Preparata,  1982 ). Here, we modify the algorithm of Nievergelt and Preparata ( 1982 ) to solve the ‘All Intersections’ and ‘Maximal Intersection’ problems described above. Our extension exploits the particular geometry of the trapezoids and rectangles that define breakpoint regions in order to: (i) efficiently compute their intersection; (ii) label the intersecting regions by the breakpoint regions that are inside; and (iii) iteratively determine the maximal elements of (ℐ n , ⊆). 2.3.1 Overview of the algorithm We provide an overview of the plane sweep algorithm for the case of breakpoint regions defined by inversion variants; i.e. fragments with parallel orientations (+,+) or (−, −). These breakpoint regions are trapezoids with the two parallel sides having slope −1 ( Fig. 1 A). Thus, we define the sweep line to be a line  y =− x + c  of slope −1. The sweep line will encounter all inversion breakpoint regions as  c  increases from  c min  to  c max . The algorithm is identical for insertion/deletion variants except the sweep line is chosen to have slope +1,  y = x + c , to match the parallel sides of the trapezoids in this case ( Fig. 1 B). As the sweep line advances through the plane, one of three possible events ( Fig. 3 ) can occur: (i)  addition  of a breakpoint region; (ii)  intersection  between two line segments defining the boundaries of breakpoint regions; (iii)  removal  of a breakpoint region ( Fig. 3 ). Since the sweep line is parallel to the two sides of each trapezoid, it is only necessary to consider intersections between the horizontal/vertical sides of the trapezoid. For each breakpoint region  B  in ℬ, we define  B top  and  B bottom  as the horizontal/vertical sides of the trapezoid. We designate the side with the largest  y  value as ‘top’.
 Fig. 3. Examples of the three events of the plane sweep: ( A ) addition, ( B ) intersection and ( C ) removal. In each case black dots label the points recorded in the cyclic lists  a  and  b  (indicated as dashed paths) that form ℛ. In addition, we show in {}'s the labels assigned to the intersecting breakpoint regions. 2.3.2 Data structures As in the plane sweep algorithm for line segment intersection, we maintain two data structures ℰ and ℒ. We define the event point schedule ℰ as the list of positions for the sweep line. The event point schedule ℰ is initialized with the starts and ends of  B top  and  B bottom ; these correspond to the addition/removal events. ℰ is updated with new intersection points as they are discovered. For a given event point (position of the sweep line), the sweep line status structure ℒ stores an ordered list of the segments intersecting the sweep line and two terminal segments  y =±∞. ℒ is analogous to the same structure in the plane sweep algorithm for line segment intersection. In this case all line segments intersecting the sweep line are either horizontal or vertical edges of breakpoint regions. We first check for intersection between line segments that are adjacent in ℒ ( Fig. 3 B). If a non-empty intersection is computed, the intersection point is added to the event schedule ℰ. The regions of intersection are recorded using a third data structure ℛ introduced by Nievergelt and Preparata ( 1982 ). ℛ is attached to the sweep−line status ℒ and records the vertices of the regions of intersection encountered thus far on the sweep. For each segment  s  in ℒ, ℛ maintains two cyclic lists,  a ( s ) and  b ( s ), that contain the points on the boundaries of the regions above and below  s , respectively. Equivalently, if  s  and  t  are adjacent line segments in ℒ, let [ s ,  t ] denotes the region to the left of the sweep line and between  s  and  t . Then ℛ contains the vertices defining the boundary of [ s ,  t ]. We augment the ℛ structure of Nievergelt and Preparata ( 1982 ) with a label for each region of intersection. This label is the set of breakpoint regions that contain the region of intersection. For example, the region consisting of the non-empty intersection of breakpoint regions  B 1  and  B 2  is labeled {1, 2}. Finally, we maintain a interval tree (Preparata and Shamos,  1985 ) ℋ from which we derive the maximal elements of the poset (ℐ n , ⊆) encountered thus far on the sweep line. The algorithm (Algorithm 1) consists of iterating through the event point schedule and updating the regions of intersection found at each step according to whether the event point is an addition, removal, or intersection. This update is briefly described in the next section. 2.3.3 Computing regions of intersection The procedure  ProcessEvent  in Algorithm 1 updates the data structures, ℰ, ℒ, ℛ and ℋ according to the type of event. For a removal event,  ProcessEvent  ends the regions [ s ,  t ] for each pair of adjacent segments in ℒ by joining  b ( s ) and  a ( t ) with the points  p ( s ) and  p ( t ) where  s  and  t  intersect the sweep line ( Fig. 3 C). For an intersection event,  ProcessEvent  also swaps the order of  s  and  t  in ℒ ( Fig. 3 B). Further details of these operations are described in (Nievergelt and Preparata,  1982 ) and in the Supplementary Text (available at  http://www.cs.brown.edu/people/braphael/supplements/structvar ). Finally, each identified region of intersection is labeled by the constituent breakpoint regions. Region labels are represented as sets of breakpoint region names and are updated using the following procedure. If  s  and  t  are consecutive line segments along a sweep line, let  I ([ s ,  t ]) denote the label set of the region [ s ,  t ]. When processing an addition event of breakpoint region  i , new regions are introduced with labels  I ([ s ,  t ])∪ i . When a processing a removal event of breakpoint region  i , new regions are introduced with label  I ([ s ,  t ])\ i . Region labels also change during intersection events. When regions are completed, their labels and list of boundary vertices are inserted into the interval tree ℋ. Finally, all regions, or alternatively only maximal regions, are output as they are identified. 2.4 Extensions We briefly describe two natural extensions of our method. First, we include aCGH data. Second, we compute the probability that a paired-end sequenced fragment matches an existing structural variant. 2.4.1 Incorporating aCGH data As described above, the uncertainty in the breakpoints of a copy number change measured by aCGH is represented as a rectangle ( Fig. 3 B). The plane sweep algorithm is readily extended to include intersections with the rectangular breakpoint regions. 2.4.2 Incorporating fragment length distribution In most paired-end sequencing approaches, various procedures are used to select fragments of a specified size  L , with the resulting fragments having lengths distributed around this selected size. Thus far, we considered each fragment length between  L min  and  L max  to be equally likely. We can instead derive the empirical distribution  f ( L ) of the values | y | − | x | over all valid pairs ( x ,  y ) and use this distribution to better ascertain whether fragments provide evidence for a specific rearrangement. The breakpoint ( a ,  b ) defines a length  l C ( a ,  b )=(sign( x C ) a  −  x C )+(sign( y C ) b  −  y C ) for fragment  C . Given a polygon  P  defining the breakpoint region of a structural variant, we compute the probability that the invalid pair ( x C ,  y C ) is consistent with this variant as  . Note that this length is constant for all points ( a ,  b ) on the same line of slope −1 or 1, according to the orientation of the invalid pair (Bashir  et al. ,  2008 ). 3 RESULTS We implemented our geometric approach in a program called GASV. We applied GASV to: (i) analyze recent paired-end sequencing data of nine human individuals; (ii) perform a comparative analysis of genetic structural variants and those identified in paired-end sequencing of several cancer samples; and (iii) integrate data across measurement techniques by comparing variants identified by both aCGH and paired-end sequencing in cancer samples. 3.1 Paired-end sequencing of human structural variants We used GASV to analyze fosmid paired-end sequencing data from eight individuals from the HapMap populations (Kidd  et al. ,  2008 ) and another individual from an earlier study (Tuzun  et al. ,  2005 ). The Kidd  et al.  ( 2008 ) study reported a total of 224 inversion, 724 insertion and 747 deletion variants, which were validated by fingerprint analysis, clone sequencing or FISH. These studies are presently the most comprehensive, high-resolution survey of structural variants in the human genome. The mean insert sizes for the fosmid clones ranged from 36 kb to 41 kb with SD from 1.4 kb to 3.9 kb. In our analysis, we used  L min  = 20 kb and  L max  = 60 kb to provide a generous buffer for intersecting breakpoint regions. 3.1.1 Analysis of reported inversion variants We first analyzed the 180 validated inversions reported on the 22 autosomes in Kidd  et al.  ( 2008 ). We obtained the list of the boundaries of each inversion, the names of the clones that support each variant and the mapped coordinates of the end sequences. We used our geometric approach to compute the intersections of the breakpoint regions for each set of supporting clones, and we compared the reported boundaries of the inversions with the intersections we obtained. Surprisingly, 41/180 of the validated inversions had an empty intersection of breakpoint regions. That is, there were no candidate inversion breakpoints common to all of the reported supporting clones suggesting that the mapped clones are inconsistent with only a single inversion at the locus.  Figure 4 A shows an example of one such set, where multiple, distinct non-overlapping intersections are visible. One hypothesis is that the three distinct regions of intersection might represent slightly different breakpoints in different individuals. However, one individual contains clones from all three regions, suggesting that this genomic locus harbors a more complex rearrangement.
 Fig. 4. Geometric analysis of inversion polymorphisms from Kidd  et al.  ( 2008 ) reveals disparities between the reported boundary of variants (black dots) and the intersections of breakpoint regions. ( A ) An inversion on chr1 with 79 reported supporting clones from all nine individuals has no point in common to all breakpoint regions. The number  x  next to each of the three regions indicates a clone from individual labeled ABCx in Kidd  et al.  ( 2008 ) is present in the cluster; a ‘G’ indicates the G95 individual from Tuzun  et al.  ( 2005 ). The bottom right region contains clones from all nine individuals, while individual ABC13 has clones from all three regions suggesting multiple distinct structural variants or mapping difficulties at this locus. ( B ) An inversion from chr3 with 22 supporting clones from all eight HapMap individuals. We examined one fully sequenced clone (dashed trapezoid) from individual ABC7 and found two possible inversion breakpoints (black squares). Both of these lie in the intersection of all breakpoint regions but are ∼37 kb from the reported boundary. In the remaining 139 cases the reported boundaries were not in the region of intersection.  Figure 4 B shows one example where the reported coordinates for an inversion are clearly outside the region of intersection. In this case, Kidd  et al.  ( 2008 ) sequenced one of the clones in this cluster. We aligned this sequence to the reference genome and obtained two possible inversion breakpoints, both of which lie in the region of intersection computed by GASV. These two breakpoints could not be further resolved due to repetitive sequence near the inversion breakpoints. Analysis of additional sequenced clones from Kidd  et al.  ( 2008 ) showed a number of additional inversion breakpoints that occur within segmental duplications. Thus even with complete sequence data available, resolving the breakpoint with greater precision is challenging. The method used to derive the reported boundaries of the variants in Kidd  et al.  ( 2008 ) is mysterious. It is possible that the reported boundaries were intended to represent a ‘consensus’ of a single structural variant locus. For complicated loci with multiple, overlapping rearrangements ( Fig. 4 A), consensus coordinates might provide a reasonable summary of the data. However, we find that in many cases, the data allow us to refine the breakpoint region, and that significant information is lost when only a single pair of coordinates is reported for an inversion. 3.1.2 Analysis of intersecting breakpoint regions Using the complete set of mapped locations provided by Kidd  et al.  ( 2008 ), we computed the intersections of breakpoint regions for all nine individuals using GASV. In total, 30 853 clones on the 22 autosomes were consistent with an inversion. There were 1361 groups of intersecting breakpoint regions. Of these, 1200 had non-empty intersections, indicating that these clusters have a putative breakpoint in common for all of the clones. The remaining 161 groups had no breakpoints common to all the clones. Thus, over 10% the intersecting breakpoint regions suggest either complicated loci that are not easily explained as single inversion variants, or mapping/alignment artifacts. For the 1200 clusters with non-empty intersection, we computed the area of the intersection and defined the  localization  of a breakpoint as the square root of this area. Thus, if the region of intersection was a square, the localization would give the genomic range allowed for each breakpoint. There are 19 clusters with breakpoint localization &lt;2500 bp. In the best example, eight clones from four individuals localize the breakpoints to within 286 bp on each end (Supplementary Text). This is a remarkably small region of uncertainty, considering that a single fosmid clone localizes a breakpoint to ≈40 kb. We also found that breakpoint localization is not directly correlated with the number of clones in the breakpoint region. In 21/1200 cases the breakpoint region was supported by more than 50 clones. In only two of these cases was the breakpoint localization &lt;5000 bp. A possible reason for this discrepancy is the presence of repeats/duplications near the inversion breakpoints. These would lead to a relatively small genomic region where end sequences can be mapped uniquely. 3.1.3 Overlap of inversion and deletion variants We used GASV to compare the locations of inversions and deletions in the data of Kidd  et al.  ( 2008 ). We identified 5054 instances of intersection between inversion and deletion breakpoint regions.  Figure 5  shows a sample cluster containing 33 clones indicating an inversion and 4 clones indicating a deletion at the same locus. There are several examples where inversion heterozygotes lead to deletions in progeny (Stankiewicz and Lupski,  2002 ), a possible explanation for these overlapping variants. Alternatively, this overlap might suggest that these regions are unstable and subject to repeated rearrangement (Stankiewicz and Lupski,  2002 ).
 Fig. 5. Intersection of 33 inversion breakpoint regions (blue) and 4 deletion breakpoint regions (red), indicates common genomic location of two structural variants. 3.2 Cross-study comparison of structural variants Our geometric approach allows for the comparison of structural variants identified in different individuals with different measurement techniques. We tested this feature by comparing the genetic structural variants identified by Kidd  et al.  ( 2008 ) with variants identified in ESP studies of cancer genomes (Raphael  et al. ,  2008 ; Volik  et al. ,  2006 ). The later studies aimed to identify somatic, and possibly cancer-related, rearrangements in three breast cancer cell lines and five primary tumors from various tissues. The cancer ESP studies used large insert clones (BACs) with average sizes of 150 kb. We first identified clusters of invalid pairs in each cancer dataset that were suggestive of either inversions or deletions, and then computed the intersection of these clusters with the inversion and deletion clusters computed from the nine normal individuals. Approximately 5–53% of invalid clusters from the cancer clusters are consistent with inversion or deletion variants identified in normal individuals ( Table 1 ). The larger percentages are found in the primary tumor samples; this is consistent with the lower sequence coverage in the primary tumor samples, and the fact that tumor samples frequently contain significant admixture of normal cells resulting from difficulty of separating normal from tumor cells.
 Table 1. A comparison of the inversion and deletion variants identified in nine normal individuals (Kidd  et al. ,  2008 ; Tuzun  et al. ,  2005 ) and several cancer genomes (Raphael  et al. ,  2008 ; Volik  et al. ,  2006 ) Cancer No. of concordant No. of concordant sample inversions (%) deletions (%) MCF7 8 (5) 40 (28) BT474 12 (19) 8 (11) SKBR3 8 (13) 7 (11) Breast 11 (19) 21 (27) Breast 12 (32) 19 (38) Prostate 3 (9) 12 (27) Ovary 8 (53) 12 (29) Brain 2 (11) 10 (26) 
 We then clustered all the cancer data together with the nine normal individuals, and identified overlapping breakpoint regions containing at least two invalid pairs from different cancer samples. Of the 22 such clusters, 10 are consistent with inversion variants identified in at least one normal individual (9/10 cases were observed in at least four normal individuals), demonstrating that a large fraction of structural variants found in more than one cancer dataset are inherited genetic variants and not somatic rearrangements. 3.3 Comparing variants identified by paired-end sequencing and aCGH We used GASV to compare the breakpoints identified by ESP and aCGH for three cancer cell lines, MCF7, BT474 and SKBR3, using data from Volik  et al.  ( 2006 ), Raphael  et al.  ( 2008 ), and Aerni  et al.  ( 2009 ). We formed rectangles corresponding to pairs of copy number changes identified by segmentation ( Fig. 1 B) of aCGH data using CBS (Olshen  et al. ,  2004 ). We found that 35/152, 20/380 and 35/149 of the clusters defined from paired-end sequenced data intersected aCGH breakpoint regions in BT474, MCF7 and SKBR3, respectively.  Figure 6  shows an example of a cluster containing 19 breakpoint regions identified by ESP in the BT474 cell line, intersecting a breakpoint region determined by aCGH.
 Fig. 6. Intersection between six breakpoint regions from ESP data (blue trapezoids) and two breakpoint regions determined by aCGH (red rectangle) on chr17 in the BT474 breast cancer cell line. In this case, the spacing between aCGH probes provides a more precise localization of the breakpoint region that the paired-end sequencing data. 4 DISCUSSION We introduced GASV, a geometric approach for classification and comparison of structural variants. To our knowledge, this is the first comprehensive method for structural variant analysis across multiple samples that supports both paired-end sequencing data with arbitrary fragment sizes and aCGH with varying array resolutions. We illustrated the generality of our approach through several applications, including the clustering of variants from a paired-end sequencing study of nine individuals, the comparison of variants in normal and cancer genomes derived through different sequencing approaches, and the comparison of variants identified by aCGH and paired-end sequencing of the same cancer samples. In many cases we are able to localize the breakpoints of single variants, but in other cases the end-sequence pairs suggest more complicated variants. The precise localization of the boundaries of structural variants provided by the GASV is helpful for distinguishing simple variants shared across multiple individuals from more complex variants resulting from repeated rearrangements at the same locus. These results also demonstrate the importance of identifying and reporting the uncertainty in structural variant boundaries. The current convention of publishing approximate coordinates that were derived from study-specific heuristics can lead to unnecessary errors and misannotations of complicated variants. We expect that GASV will be useful for analyzing data from the 1000 Genomes Project and for cancer genome sequencing efforts that are part of The Cancer Genome Atlas. In the latter application, GASV will help distinguish genetic from somatic rearrangements. There are several directions for future work. First, it would be useful to perform a more comprehensive comparison of the variants that are identified by different measurement techniques. aCGH has limited power to detect variants whose breakpoints lie in repeat-rich regions of the genome due to the inability to identify probes in these regions. Paired-end sequencing approaches can be similarly limited, particularly if small fragment sizes are used, since the end sequences will not align uniquely to repeat-rich regions of the genome. Current studies of structural variants with next-generation sequencing technologies have used small fragment sizes from 200 bp (Campbell  et al. ,  2008 ) to 3 kb (Korbel  et al. ,  2007 ). An unresolved question is the optimal fragment size to use for studies of human structural variation. We have shown that clustering of breakpoint regions from relatively large clones (40 kb) with GASV can yield very precise localization of variant breakpoints (a few hundred base pairs). Kidd  et al.  ( 2008 ) reported that most of the clones that they sequenced had highly repetitive sequence at the breakpoints, complicating the precise breakpoint identification and assembly of the clone sequence. Thus, even in cases where complete sequence is available GASV can be used to record uncertainty in breakpoint location. An additional area of future work is to incorporate breakpoint uncertainty into databases of known structural variants. Our geometric approach could then be used to query this database and thus provide a more robust procedure for comparing newly discovered and existing variants. In addition, knowledge of existing structural variants can be used to guide mapping of end sequences that do not map uniquely to the reference genome. This is a common problem in human genome resequencing, where up to 60% percent of ES fragments are not used because of their ambiguous mappings (Korbel  et al. ,  2007 ). Lee  et al.  ( 2008 ) recently described a probabilistic model for resolving ambiguities that arise when mapping ES pairs in a single sample. Developing a model for multi-sample comparison that incorporates variant ambiguity across samples is a promising future endeavor. Finally, we focused exclusively on structural variation in the human genome, but such variation is also found in the mouse genome (Egan  et al. ,  2007 ) and other model organisms (Dopman and Hartl,  2007 ). Thus, there will continue to be an increasing demand for better analysis tools for structural variation. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Predicting small ligand binding sites in proteins using backbone structure</Title>
    <Doi>10.1093/bioinformatics/btn543</Doi>
    <Authors>Bordner Andrew J.</Authors>
    <Abstract>Motivation: Specific non-covalent binding of metal ions and ligands, such as nucleotides and cofactors, is essential for the function of many proteins. Computational methods are useful for predicting the location of such binding sites when experimental information is lacking. Methods that use structural information, when available, are particularly promising since they can potentially identify non-contiguous binding motifs that cannot be found using only the amino acid sequence. Furthermore, a prediction method that can utilize low-resolution models is advantageous because high-resolution structures are available for only a relatively small fraction of proteins.</Abstract>
    <Body>1 INTRODUCTION Many proteins rely on non-covalently bound metal ions or small molecules for their enzymatic function or regulation of their activity. The presence and location of these binding sites can therefore give useful clues for deducing the biochemical function of an uncharacterized protein. As the pace of protein sequence and structure determination quickens (Burley  et al. ,  2008 ), the assignment of protein function is becoming an increasingly important task. Computational methods can assist in this search by generating experimentally verifiable predictions of ligand binding sites in proteins. A number of successful methods have been developed for predicting ligand binding sites by finding characteristic sequence patterns (Andreini  et al. ,  2004 ; Passerini  et al. ,  2006 ; Shu  et al. ,  2008 ; Sigrist  et al. ,  2002 ). Such methods are particularly useful when there is no protein structure with detectable sequence similarity. Although ligands bind to residues that are localized in 3D space, the binding residues are generally not contiguous in the amino acid sequence. Because of this fact, methods that also utilize structure information, when it is available, are expected to perform better than those that use sequence information alone. Several distinctive properties of metal ion binding sites can be used for their prediction. Metal ion binding sites were found to have an outer shell of hydrophobic atomic groups that contains the inner shell of hydrophilic groups that coordinate the ion (Yamashita  et al. ,  1990 ). However, as pointed out in that study, this may simply be due to the fact that the coordinating atoms are covalently bound to hydrophobic carbon-containing groups in the protein rather than favorable enthalpic or entropic contributions to binding. It was also observed that metal binding sites often correspond to statistically significant clusters of negatively charged residues (Zhu and Karlin,  1996 ). Furthermore, divalent metal cations have characteristic preferences for coordinating groups (backbone carbonyl, specific side-chain groups or water molecules) and coordination number (Dudev and Lim,  2003 ; Harding,  2004 ). Finally, binding site residues are usually conserved in orthologous proteins, presumably because of their crucial role in the protein's function. One early prediction method used the total electrostatic valence of oxygen ligands, calculated from the inter-atomic distances, in order to predict calcium ion binding sites (Nayal and Di Cera,  1994 ). Another study used the FEATURE method (Bagley and Altman,  1995 ), which calculates properties in concentric shells around a potential binding site, with a Bayesian score to predict calcium ion binding sites (Wei and Altman,  1998 ). A later study used a similar method to predict zinc ion binding sites and demonstrated its applicability to unbound structures (Ebert and Altman,  2008 ). Also properties of clustered triplets of residue types that commonly coordinate zinc ions were successfully used to predict transition metal ion binding sites in apo protein structures (Babor  et al. ,  2008 ). Binding sites for several different metal ions were also identified using the Fold-X force field applied to the atomic structure of the protein (Schymkowitz  et al. ,  2005 ). Finally, the method of Sodhi  et al.  ( 2004 ) used neural networks trained on PSI-Blast position-specific scoring matrices (PSSMs), secondary structure states, solvent accessible surface area (SASA) and the inter-residue distance matrix for neighboring residues to predict different metal ion binding sites. The prediction of small molecule binding sites is facilitated by their tendency to bind in pockets on the surface of the protein, due to the requirement of forming sufficient energetically favorable contacts. A variety of algorithms that locate such surface pockets have been developed (An  et al. ,  2005 ; Harris  et al. ,  2008 ; Laurie and Jackson,  2005 ). Many structure-based binding site prediction methods use predicted pockets. One study combined pocket prediction with evolutionary conservation in order to predict binding sites for an arbitrary small molecule (Huang and Schroeder,  2006 ). Another study (Glaser  et al. ,  2006 ), used an optimized pocket finding algorithm along with conservation to locate pockets that bind ligands. The study of Burgoyne and Jackson ( 2006 ) compared different properties for ligand binding prediction and found that the total electrostatic potential, desolvation energy and conservation were the best. One challenge with such methods for predicting general ligand binding sites, irrespective of the identity of the ligand, is that it is difficult to compile a reliable negative dataset of pockets that do not bind any small molecule. Two papers, Guo  et al.  ( 2005 ) and Nebel  et al.  ( 2007 ), described methods that use 3D motifs to predict binding sites for adenosine triphosphate (ATP) and adenine-based ligands, respectively. Finally, Brylinski and Skolnick ( 2008 ) employed threading of the query sequence to identify similar structures in the PDB with bound ligands. We have developed the SitePredict ligand binding site prediction method that can be applied, with minor modification, to both metal ion and small molecule binding sites. The primary difference is that only surface pockets are considered for small molecule binding sites whereas clusters of residues throughout the protein are considered as potential metal ion binding sites. A machine learning method, Random Forests, is used to make a prediction for a specific ligand based on a combination of diverse properties including evolutionary conservation, median SASA, counts of nearby residue pairs and statistically significant clustering of residue types in the site. This method was motivated by the success of a similar approach for predicting protein–protein interfaces (Bordner and Abagyan,  2005 ). Unlike many previous approaches, SitePredict does not rely on the exact positions of side chains contacting the ligand. This is critical since actual predictions are performed on unbound structures and the protein structure undergoes conformational changes upon binding. In fact, except for the median SASA, which is relatively insensitive, there is no dependence on side-chain conformations at all. This means that it can be applied to unrefined homology models or low-resolution structures. The method's performance was evaluated using cross-validation on comprehensive non-redundant sets of both holo and apo protein structures. Also the relative contribution of the different site properties to the prediction accuracy revealed specific determinants of binding for different ligands. Binding site predictions were made for all protein structures in the PDB and are available for searching or download at  http://sitepredict.org/ . Also, as a demonstration of the method's utility in functional annotation, novel binding sites in proteins of unknown function whose structures were solved by structural genomics projects were examined and several examples which corroborate independent functional assignments are discussed. Finally, SitePredict was applied to predict ligand binding sites in homology models of human proteins. 1.1 Non-redundant sets of protein structures and ligand binding sites The data used to train the machine learning classifier and assess its prediction performance was derived from a non-redundant set of X-ray structures created for each ligand. Each set was generated by first clustering all protein chains in X-ray structures from the wwPDB database (Berman  et al. ,  2003 ) at 25% sequence identity using the CD-HIT program (Li and Godzik,  2006 ). Next, each cluster was examined and a ligand-bound protein, if present, was selected and otherwise a protein without the bound ligand was selected. If multiple chains were found in a cluster then the highest resolution chain without breaks was chosen. This procedure resulted in a non-redundant set of proteins containing the largest number of ligand-bound structures. This is important for creating a good benchmark set with an adequate number of diverse ligand binding sites, since most proteins in the structure database do not have the particular ligand bound. Although the methodology may be easily extended to include the prediction of ligands that interact with multiple protein molecules, for simplicity we only considered structures in which the ligand interacts with a single protein molecule. 1.2 Overview of the SitePredict method Small molecules usually bind in pockets on the protein surface where they can form sufficient energetically favorable interactions with the protein. Furthermore these pockets usually remain even in the apo structure (An  et al. ,  2005 ). Metal ions are also coordinated by multiple chemical groups (Harding,  2004 ), however they do not bind to pre-existing pockets in the surface because of their small size. Because of this difference in binding proclivities, two different prediction procedures are employed for each class of ligand. For small molecules, only the binding pockets are considered as potential binding sites and a prediction is made for each pocket as to whether or not it binds a particular ligand, based on the properties of the pocket. In contrast, for metal ions, the predictions are made for approximately spherical regions of the protein, each comprised of a cluster of a fixed number of neighboring residues. The prediction aims to identify the residue clusters that significantly overlap with a binding site for the metal ion of interest. 1.3 Residue cluster properties used for metal ion binding site prediction The overlapping residue clusters were defined by starting with each of the residues in the query protein as a central residue and adding the nine nearest residues to form 10-residue clusters. Increasing the size of the clusters did not significantly improve prediction performance (data not shown). The fractional overlap of a cluster with the binding site, which varies between 0 and 1, was defined by the ratio:
 
in which the numerator is the number of residues in common between the cluster and the binding site and the denominator is the least of either the number of cluster residues or the number of binding site residues. Clusters with an overlap fraction &gt;0.5 were considered as positive instances, i.e. ligand binding site regions, whereas the remainder were considered as negative instances, i.e. non-binding site regions. The following properties were calculated for each residue cluster: nearby residue pair counts, number of nearby backbone O atom pairs, residue propensity log  P -values, evolutionary conservation log  P -value and median-relative SASA. The residue pair counts were calculated by counting the total number of nearby cluster residue pairs of each residue type. Because there are 20 different standard residue types, the number of unordered pairs of residue types, and hence the number of residue pair counts, is 210. Nearby residue pairs were required to have C β  atom, or C α  atom for glycine, separation &lt;10 Å. The number of nearby backbone O atom pairs was calculated by first creating a graph in which nodes represent O atoms and are connected by an edge if the corresponding atoms are separated by &lt;5 Å and then counting the number of nodes in the maximal clique. This procedure efficiently locates backbone O atom clusters. The residue propensity  P -values were calculated for each of the 20 residue types by the statistical significance of observing as many residues of that type in the cluster versus what would be expected from a random arrangement of the protein's residues. The random probability is described by the hypergeometric cumulative distribution function as
 
in which  N i cluster  is the number of residues of type  i  in the cluster and  N i total  ( N j ≠ i total ) is the number of residues of type  i  (not of type  i ) in the protein. The log  P -value is expected to be more robust than raw residue-type counts, since it accounts for the different distributions of residue types occurring in distinct proteins. In other words, the log  P -value is high only if the fraction of residues of a particular type in a cluster is considerably higher than the fraction of the same residue type among all residues in the protein. 1.4 Pocket properties used for small molecule binding site prediction As mentioned above, only surface pockets on the protein surface were considered as potential binding sites for small molecules. Also pockets that are too small to contain the ligands, with volumes &lt;200 Å 3 , were excluded. Surface pockets were calculated using the PocketFinder algorithm (An  et al. ,  2005 ) as implemented in the ICM program version 3.5 (Molsoft LLC). Properties were calculated for the set of residues lining each pocket, which are within 4 Å of the calculated pocket surface. Properties that describe the size and shape of the surface pocket as well as some of the same properties used for the metal ion site prediction were used for small molecule site prediction. The following properties of each pocket were used for the prediction of small molecule binding sites: nearby residue pair counts, residue propensity log  P -values, evolutionary conservation log  P -value, pocket volume and pocket principal components. The properties not involving the pocket shape are the same as those defined for metal ion sites and described in  Section 1.3 , except that they were calculated using the set of pocket residues. The three principal components are the axis lengths of an ellipsoid that approximates the pocket boundary. They conveniently summarize the generally complicated shape of the pocket in a few numbers. 1.5 Random Forest classifiers Binding site predictions are made using a supervised learning method called Random Forests (Breiman,  2001 ). The Random Forest method has many advantages compared with other machine learning methods including: high accuracy, speed, resistance to overfitting, the ability to use heterogeneous training data without rescaling, estimation of the generalization error during training, and the ability to estimate the contribution of each variable to the overall prediction accuracy. A Random Forest is a collection of classification trees that are randomized by training on a bootstrap sample of the training data and also using only a subset of M (&lt; N ) of the variables. A prediction is made based on the fraction of trees selecting each class. In this application the two classes are binding site and non-binding site. A cutoff is chosen and if the fraction of trees predicting the site to be a binding site is higher than the cutoff then the overall prediction is binding site, otherwise the site is predicted to be a non-binding site. Because of bootstrap sampling, approximately one-third of the data samples are not used for training each tree. The importance of each variable to the prediction performance can be assessed by calculating the decrease in prediction accuracy for this so-called out-of-bag data upon permuting values for the variable. 1.6 Evaluation of prediction performance The machine learning method requires a sufficient number of independent examples of known binding sites for a ligand in order to evaluate the prediction performance and avoid potential overfitting. Only ligands with least 40 independent binding sites in the non-redundant set of protein–ligand structures, described in  Section 1.1 , were considered. The following metal ions fulfilled this criterion: Ca, Cu, Fe, Mg, Mn and Zn. Fe ions annotated in both oxidation states Fe(III) and Fe(II), with PDB heterocompound codes FE and FE2, respectively, were grouped together. Because many small molecules are converted into another molecule by the enzymatic action of the protein in the binding pocket, both the reactant and product were considered together as a group. For example, ATP is often hydrolyzed to adenosine diphosphate (ADP) so that these two molecules are in one group. Also, structures of protein–ligand complexes are often solved with non-hydrolyzable analogs bound so that these structures are also included within the same group. Datasets were compiled for the following groups of small molecules:
 adenosine monophosphate (AMP), ATP, ADP and analogs (ACP, ANP, ATS, SAP and TAT), flavin-adenine dinucleotide (FAD), heme (HEM and HEB), nicotinamide adenine dinucleotide (NAD) and derivatives (NAP, NDP and NAI). 
The PDB heterocompound ID, which coincides with the usual abbreviations for some of the compounds, are shown in parentheses.  Table 1  shows the total number of proteins and sites for each ligand.
 Table 1. Number of ligand binding proteins and sites in the non-redundant datasets for each ligand Ligand Number of proteins in the training set Number of sites in the training set Ca 273(355) 434(536) Cu 33(45) 51(67) Fe 84(95) 105(123) Mg 433(479) 549(575) Mn 148(172) 232(239) Zn 401(541) 517(687) AMP 48(71) 48(83) ATP, ADP+analogs 172(269) 173(299) FAD 52(85) 52(105) Heme 53(123) 57(208) NAD, NADP, NADPH 107(170) 107(199) The values in parentheses are the numbers of proteins or sites before removing sites
that contact multiple proteins. Random Forest input datasets, which contain all of the properties described above, were then generated for each metal ion or group of small molecules. Missing conservation values, due to an insufficient number of related protein sequences (&lt;20), were imputed as the median of all conservation values in the input dataset. This procedure results in the assignment of a neutral value, which does not bias the prediction, for evolutionary conservation in examples with missing data. Because there are many more negative examples (non-binding sites) in the data than positive examples (binding site residue cluster or pocket) it was necessary to randomly select only a subset of the negative data to obtain a balanced dataset. This is important since training on highly unbalanced data gives a predictor that is accurate only for the majority class. Datasets with twice as many negative as positive examples were used because they gave slightly better performance than evenly balanced (1:1 ratio) datasets (data not shown). All positive examples were included in the datasets. To evaluate the performance of the Random Forest prediction, 10-fold cross-validation was used. The cross-validation sets were constructed so that the corresponding training and test sets contain data for proteins from different Pfam families (Finn  et al. ,  2006 ). This insures the independence of the cross-validation sets, which is important for an accurate assessment of the actual prediction performance. The overall cross-validation prediction performance was summarized by the total area under the receiver operating characteristic (ROC) curve, which plots sensitivity versus (1 - specificity). The ROC curves were generated by varying the Random Forest score cutoff. The area under the curve (AUC) can vary from 0.0 to 1.0. A random prediction is expected to yield an AUC of 0.5 and the closer the AUC value is to 1.0 the more accurate the prediction is. Furthermore, the AUC is proportional to the Wilcoxon rank-sum statistic and so can be associated with a  P -value for discriminating the two classes. 1.7 Class likelihood ratio for prediction confidence The Random Forest score, defined as the fraction of trees voting for the positive class, varies from 0.0 to 1.0. Potential sites with high scores, near 1.0, are expected to be more confidently predicted as binding sites than those with lower scores. Likewise, potential sites with low scores, near 0.0, are expected to be more confidently predicted as non-binding sites than those with higher scores. This implies that the Random Forest score contains more useful information than simply whether it is above or below the binary classification cutoff. The confidence of each prediction was estimated as a class likelihood ratio calculated from class-dependent score distributions using a method similar to that described for support vector machine classification in Bordner and Abagyan ( 2005 ). First, the distributions of cross-validation prediction scores for negative and positive examples were estimated using kernel density estimation as implemented in R (R Development Core Team,  2008 ). The likelihood ratio  R (score) was then calculated as the ratio of the distributions, i.e.
 
A high value for  R  (much greater than the fraction of all residues expected to bind the ligand) indicates that the site is confidently predicted as a binding site for the ligand, a low value indicates that the site is confidently predicted not to be a binding site for the ligand, and an intermediate value indicates an ambiguous prediction. The likelihood ratio is useful for prioritizing predictions for experimental validation. Residue-level scores and likelihood ratios for metal ion site predictions were calculated as the median values for all residue clusters containing a particular residue. 2 RESULTS 2.1 Prediction accuracy The accuracy of the SitePredict method in predicting binding sites for different ligands, as assessed by the 10-fold cross-validation AUC values, is shown in  Table 2 . Results using both ligand-bound and ligand-unbound structures are given.
 Table 2. Area under the ROC curve for 10-fold cross-validation prediction of ligand binding sites using ligand-bound and ligand-unbound protein structures AUC Ligand Bound structure Only top 20 variables (bound structure) Without conservation (bound structure) Unbound structure Ca 0.861 0.850 0.856 0.813 Cu 0.952 0.784 0.952 Fe 0.960 0.948 0.953 Mg 0.823 0.809 0.794 0.763 Mn 0.897 0.884 0.879 0.895 Zn 0.964 0.958 0.958 0.913 AMP 0.799 0.842 0.797 ATP, ADP+analogs 0.884 0.901 0.852 0.836 FAD 0.941 0.928 0.941 Heme 0.971 0.955 0.971 NAD, NADP, 0.959 0.959 0.959 0.876 NADPH AUC values are also given for predictions using only the top 20 most important variables and without evolutionary conservation. The transferability and generality of SitePredict is demonstrated by the high AUC values for the cross-validation prediction because the prediction is made for proteins in different Pfam families than those used for training the Random Forest classifier. In other words, high cross-validation AUC values imply that the method is expected to perform well on proteins dissimilar to those used for training, such as those without any known binding sites. Table 1  also shows the cross-validation AUC values for binding site predictions without using evolutionary conservation. There is only a relatively small decrease in prediction performance for all ligands. This is advantageous since the evolutionary conservation could not be calculated for about 20% of the proteins due to a lack of similar protein sequences. This shows that the accuracy does not appreciably degrade for such proteins. Other methods that rely on a multiple sequence alignment through, e.g. PSSMs, cannot be applied to the significant fraction of proteins with few or no known orthologs. The prediction results for all PDB structures are available at the SitePredict website ( http://sitepredict.org/ ). Results for individual PDB entries can be retrieved and visualized in 3D using the Jmol viewer (Jmol,  2008 ) or the complete prediction tables can be downloaded for analysis. The training datasets of binding sites for each ligand, including those for bound/unbound pairs, are provided as  Supplementary Material . 2.2 Prediction results for unbound structures The decrease in the AUC for unbound as compared with bound structures was relatively small (≤0.083) showing that SitePredict is insensitive to rearrangements in the protein structure induced by ligand binding. Only surface pockets are considered as potential binding sites for small molecules so that the lack of a pocket in the unbound structure that sufficiently overlaps the binding sites (O&gt;0.5) results in a false negative prediction. All unbound structures for NAD retained pockets in the binding sites, however, pockets for two out of the 56 unbound structures (chain A of PDB entries 1BYI and 1I7N) for ATP had too little overlap with the binding site (O=0.47 and 0.28, respectively) so that the method missed these binding sites. As expected, this can be attributed to large conformational changes in loops near the binding site leading to relatively high RMSDs between binding site residues in bound and unbound structures of 3.0 Å and 2.3 Å, respectively. Interestingly, there is a surface pocket that sufficiently overlaps the ATP binding site (O=0.61) for chain B of PDB entry 1I7N, which is the same protein as chain A (C domain of rat synapsin II), and this gives a correctly predicted ATP binding site with a high likelihood ratio of 3.9. The structural differences between the chains A and B are mostly in the binding site loop, probably due to its flexibility in the apo protein. Overall, these results support the findings of An  et al.  ( 2005 ) that ligand binding pockets are almost always retained in unbound structures and furthermore that the prediction accuracy is not significantly degraded by differences in these pockets due to structural changes upon ligand binding. 2.3 Contribution of binding site properties to prediction performance The top 20 properties that contribute the most to the prediction accuracy for ligand sites were calculated using the procedure described in  Section 1.5  (see  Supplementary Material  for results). As can be seen in  Table 2 , the AUC changes little for all ligands except Cu if only these top 20 properties are used for prediction. The most important residue propensities for each metal ion include the most common coordinating residues for that particular ion according to the analysis of Harding ( 2004 ). Evolutionary conservation and SASA are also among the important variable for predicting metal ion binding sites since they appear in the top 20 properties for all ions examined. Metal ion binding sites are usually conserved and generally, but not always, on the protein surface. In contrast, evolutionary conservation only appears among the most important properties for two small molecules, ATP and NAD. Also, residue propensities appear less among the top properties for the small molecules. This may be due to several factors: (i) metal ions are positively charged and bound by clusters of negatively charged residues, (ii) small molecule binding sites are larger so that the spatial arrangement of residues within the site is more important than for smaller ion binding sites and (iii) there are more residues in a typical binding pocket than in the 10-residue clusters used for metal ion prediction so that any clustering of residue types is less statistically significant. Residues that are present in previously identified sequence motifs are also contained in the important residue pairs. For instance, A, G, K and S are in both the important residue pair variables for ATP binding sites and also in the Walker A motif. Also glycine appears in most of the important residue pairs for NAD binding sites and a glycine-rich turn was found to be a determinant of such sites (Baker  et al. ,  1992 ). 2.4 Discrimination between different ligands The ability of a Random Forest classifier trained on one ligand to reject binding sites of different ligands was assessed by comparing Random Forest scores from 10-fold cross-validation for one ligand (ligand 1) with scores from training on all cross-validation data for ligand 1 and then predicting for data from a different ligand (ligand 2). The average scores, AUC values and associated Wilcoxon rank-sum  P -values for all pairs of metal ions and small molecules are given  Tables 1  and  2  in the  Supplementary Materials .  Figure 1  shows an example of the successful discrimination between distinct Ca and Cu ion binding sites in the same protein.
 Fig. 1. An example illustrating the successful discrimination between two different metal ions, which in this case bind to the same protein (fungal lysyl oxidase, PDB entry 1N9E). Calcium and copper binding site predictions are shown in the top and bottom figures, respectively. Predicted binding residues, with  R &gt;20, are colored red, predicted non-binding residues, with  R &lt;10, are colored blue, and residues with intermediate ambiguous values are colored yellow. The bound copper ion is orange and the calcium ions are green. The insets show a detailed view of the binding sites. It is apparent from the tables that the discrimination performance is non-symmetric, i.e. a Random Forest trained on ligand 1 may have poor selectivity for ligand 2 but one trained on ligand 2 may be good at rejecting ligand 1 binding sites in favor of the correct ligand 2 sites. There are only two sets of ligands for which the method cannot discriminate in both reciprocal predictions: Ca and Mg ions and AMP and ATP. It is known that one of the most prevalent Ca binding motifs, the EF-hand motif, can also bind Mg in some cases (Lewit-Bentley and Rety,  2000 ) and that both ions have similar coordination propensities (Kaufman-Katz  et al. ,  1996 ). This is reflected by the fact that 15 out of the 20 top variables are shared by both Ca and Mg ion binding sites. More generally, predictors trained on Mg or Mn had lower specificity that those trained on other ions. The difficulty in differentiating between the binding of some metal ions, such as Mg and Mn, is also probably due in large part to the fact that many sites are known to actually bind different ions  in vitro  and may do so  in vivo  (Bock  et al. ,  1999 ). The difficulty in distinguishing binding sites for the ATP group ligands, which also include ADP, and AMP is due to the similar properties of the binding sites. Half of the 20 top variables contributing to the prediction accuracy are shared between these two ligands. One possible explanation for this similarity in the important binding site properties is the chemical similarity of the ligands; ADP and AMP differ only by a phosphate group. In addition, all small molecules considered, with the exception of heme, contain adenine moieties, which make their discrimination relatively difficult. In fact, none of the cases of poor ligand discrimination involve the chemically dissimilar heme. 2.5 Binding site predictions for modeled human proteins Although current comparative modeling methods can usually accurately reproduce the core backbone geometry for proteins with available homologous template structures, they have difficulty in predicting the correct conformations of side chains and loops (Ginalski,  2006 ). Because SitePredict only requires backbone structural information it can locate ligand binding sites in homology models, even if they contain errors in side chain conformations. Furthermore, because the prediction procedure is fast it can be applied to homology models on a genome-wide scale for functional annotation. Typically, between 30% and 60% of proteins in sequenced genomes have a related protein of known structure and these percentages are increasing as new structures become available (Xiang,  2006 ). The application of SitePredict to homology models was demonstrated by predicting ligand binding sites in a set of models for 688 human proteins downloaded from MODBASE (Pieper  et al. ,  2006 ), a database of protein structures generated by an automated modeling pipeline using the MODELLER program (Sali and Blundell,  1993 ). Only models that are expected to be accurate [score&gt;0.7 (Melo  et al. ,  2002 ) and PSI-BLAST  E -value&lt;1×10 −4 ] but that have low sequence identity to the template protein sequence (&lt;25%), and thus potentially yield novel binding site predictions not inferable from sequence homology alone, were considered. Prediction results for all ligands are available on the SitePredict website. Although the binding sites, or even which ligands bind, are unknown for most of the human proteins, high-resolution structures with bound ligands are available for comparison in a few cases. Structures with bound ATP group ligands (PDB entries 2GK6 and 2GT0, respectively) were available for human type 1 RNA helicase (Upf1) and nicotinamide riboside kinase 1 (NRK1). In both cases, the highest scoring predicted binding pockets in the model structures, with  R =7.5 and 11.9, respectively, overlapped the actual binding sites. Also the structure of one human protein in the set with heme bound, prostacyclin synthase (PGIS, PDB entry 3B6H) is available. Again the highest scoring binding pocket ( R =50) overlapped with the heme binding site. Even though the sequence similarity was low, the template structures for NRK1 and PGIS (2BBW and 1TQN) contained the corresponding ligands. No ligands are bound to the Upf1 template structure (1PJR). 2.6 Identifying new ligand binding sites in structures of uncharacterized proteins In recent years, the number of new X-ray structures of proteins with no significant sequence similarity to those already solved has been rapidly increasing, largely due to structural genomics projects (Chandonia and Brenner,  2006 ). Even with available high-resolution structures, the biological function of many of these proteins remains unknown. Knowledge of what ligands bind to a protein and where they bind can help in inferring the function. Binding site predictions were examined for a set of proteins with available X-ray structures but lacking functional annotation, downloaded from the PSI Structural Genomics Knowledgebase ( http://kb.psi-structuralgenomics.org/KB/ ). Although the binding site predictions will require experimental verification there were some proteins for which independent evidence suggests that they are correct. One example is a predicted NAD binding site in  Haemophilus influenzae  shikimate dehydrogenase-like protein HI0607 (PDB entry 1NPY). Phylogenetic analysis showed that this protein is in a distinct group from the two previously known functional classes (Singh  et al. ,  2005 ). The protein was also shown to catalyze the NADP + -dependent oxidation of shikimate. In addition, mutagenesis of two conserved residues, D103 and K67, inactivated the enzyme, thus implying that they are important catalytic groups. Both of these residues are in the predicted NAD-binding pocket. Finally, the protein has 30% sequence identity to an AroE shikimate dehydrogenase (PDB entry 1NVT) that has the same fold and NADP bound in the predicted pocket region. Another example is a predicted NAD-binding pocket in a mouse protein annotated as a putative NADPH-dependent oxidoreductase (PDB entry 1VJ1). A structural alignment revealed similarity to a quinone oxidoreductase (PDB entry 1QOR) even though the sequence identity is only 18% (Levin  et al. ,  2004 ). Furthermore, the quinone oxidoreductase structure has an NADPH molecule bound in the pocket corresponding to the predicted NAD binding pocket in the 1VJ1 structure. 3 CONCLUSIONS SitePredict was shown to perform well in predicting specific metal ion and small molecule binding sites in protein structures, with AUC≥0.80 for bound structures. Its performance on unbound structures was only slightly lower, demonstrating that the method is insensitive to most ligand-induced conformational changes in the benchmark set, which include side-chain reorganization and small to moderate backbone changes. SitePredict was also applied to predicting binding sites in uncharacterized proteins in PDB structures as well as automatically generated homology models of human proteins. Almost all of these predictions await experimental verification and potentially offer valuable clues to each protein's function. There are several possible areas of future investigation. One is to extend the binding site predictions to additional small molecules for which there are not enough ligand–protein complexes in the PDB for adequate training and validation. This could be accomplished, for example, by either collecting independent experimental binding data or training the method on clusters of similar binding sites, which presumably bind the same or chemically similar ligands. Finally, it would be useful to train additional classifiers that are optimized for discrimination between difficult to distinguish ligands. Funding : Mayo Clinic and a Biopilot project from the DOE Office of Advanced Scientific Computing Research; ERKP558 ‘An integrated knowledge base for the Shewanella Federation’ from the DOE Office of Biological and Environmental Research. Conflict of Interest : none declared. 
 Supplementary Material 
 
 
 [Supplementary Data] 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Close lower and upper bounds for the minimum reticulate network of multiple phylogenetic trees</Title>
    <Doi>10.1093/bioinformatics/btq198</Doi>
    <Authors>Wu Yufeng</Authors>
    <Abstract>Motivation: Reticulate network is a model for displaying and quantifying the effects of complex reticulate processes on the evolutionary history of species undergoing reticulate evolution. A central computational problem on reticulate networks is: given a set of phylogenetic trees (each for some region of the genomes), reconstruct the most parsimonious reticulate network (called the minimum reticulate network) that combines the topological information contained in the given trees. This problem is well-known to be NP-hard. Thus, existing approaches for this problem either work with only two input trees or make simplifying topological assumptions.</Abstract>
    <Body>1 INTRODUCTION Reticulate evolution, a form of evolution with hybridization and genetic exchanges between two species, are common in many organisms: bacteria, plants, fish, amphibians and many others. For better understanding of reticulate evolution, several reticulate evolutionary models have been proposed and actively studied to address various reticulate processes, such as hybrid speciation, lateral gene transfer and recombination. Since most of these models are in the forms of networks, we call them reticulate networks 1 . We refer the readers to (Huson,  2007 ; Huson and Bryant,  2006 ; Nakhleh,  2009 ; Semple,  2007 ) for surveys of different reticulate network models. The  key  computational problem related to these models is the  inference  of reticulate networks. Depending on the types of biological processes involved, data for network inference may be in different forms, such as phylogenetic trees for some short genomic regions (called genes in this article) or aligned DNA sequences. In this article, we focus on inferring reticulate networks from a set of correlated phylogenetic trees. Here is the biological motivation for our problem. Suppose multiple phylogenetic trees (called  gene  trees in this article) are reconstructed, each from some gene for these species. Due to reticulate evolution, different genomic regions (say genes) may be inherited from different ancestral genomes and their evolutionary histories may not be the same (but are still related). Thus, these trees are  correlated  but  not  identical.  No  single phylogenetic tree can faithfully model the evolution of the species, and a more complex network model (i.e. reticulate network as studied in e.g. Baroni  et al. ,  2004 ; Huson,  2007 ; Huson  et al. ,  2005 ; Nakhleh  et al. ,  2004 ; Semple,  2007 ) is needed. Imagine we are given a set of ‘true’ gene trees and a ‘true’ reticulate network that models the evolutionary history of these genes. The network can be considered as a compact representation of these gene trees in the sense that one should be able to ‘trace’ a gene tree within the network. We say such a gene tree is  displayed  in the network. This motivates a natural problem, which is called ‘the holy grail of reticulate evolution’ in (Nakhleh,  2009 ): given a set of gene trees, reconstruct a reticulate network that displays  every  given gene tree. Such an inferred network reveals important correlation of evolutionary history of multiple genes. Since there exists many such networks, a common formulation is to find the one with the  fewest  reticulation events. Such a network is called the  minimum  reticulate network. The central computational problem on reticulate networks, the  minimum reticulate network problem , is: given a set of gene trees, reconstruct the minimum reticulate network that displays these gene trees. This formulation may be reasonable when reticulation is believed to be rare. In general, this problem is computationally challenging: even the case with only  two  gene trees is known to be NP-complete (Bordewich and Semple,  2004 ,  2007 ; Hein  et al. ,  1996 ). There are several existing approaches for reconstructing the  exact  minimum reticulate networks when there are only two gene trees (Bordewich  et al. ,  2007 ; Linz and Semple,  2009 ; Wu,  2009 ; Wu and Wang,  2010 ). Clearly restricting to just two gene trees is a  big  limitation: more gene trees will be more informative to phylogenetic inference, and DNA sequences of many genes are available. Alternatively, there are also a number of approaches making simplifications to the reticulate network model, e.g. by imposing additional topological constraints on reticulate networks (Gusfield,  2005 ; Huson and Klopper,  2007 ; Huson  et al. ,  2009 ; Nakhleh  et al. ,  2005 ) or working with small-scale tree topological features (Huson  et al. ,  2005 ; Huson and Klopper,  2007 , van Iersel  et al. ,  2008 ). Such simplification often leads to significantly faster approaches. However, it is sometimes unclear how biologically meaningful these added topological constraints are. Even in the case where additional simplifications are reasonable, one may still want to compare with the unconstrained minimum reticulate networks. Contributions: In this article, we present new approaches for the minimum reticulate network problem with  three or more  gene trees for unconstrained, general, reticulate networks (e.g. without needing to assume that the network has some restricted form, such as being a galled-tree or galled network). Thus our work is more general than some previous approaches (Huson and Klopper,  2007 ; Huson  et al. ,  2005 ; Huson  et al. ,  2009 ). In particular, we develop a  lower  bound (the  RH  bound) and an  upper  bound (the  SIT  bound) for the minimum reticulate network problem with multiple gene trees. We show the correctness of the bounds. We give a closed-form formula for the  RH  bound for the case of three gene trees. We also show how to compute these bounds efficiently in practice using integer linear programming (ILP). Practical results on simulated and real biological data show that the bounds can be computed for wide range of data. Moreover, the lower and upper bounds are often  close , especially when the number of trees is small or reticulation level is relatively low. In fact, for many simulated datasets of this type, the lower and upper bounds often match, which means our methods can reconstruct the  exact  minimum reticulate networks for these datasets. We also show the  RH  bound clearly outperforms a simple bound. 2 DEFINITIONS AND BACKGROUND Throughout this article, we assume trees are rooted. A phylogenetic tree is rooted and leaf-labeled by a set of species (called taxa). A leaf of a phylogenetic tree corresponds to an extant species. An internal vertex corresponds to a speciation event. In-degrees of all vertices (also called nodes), except the root, in a tree are one, while out-degrees are zero for leaves and at least two for internal nodes. A binary phylogenetic tree requires out-degrees of internal nodes to be two. A non-binary phylogenetic tree contains nodes with out-degree of three or more. Many existing phylogenetic methods assume binary phylogenetic trees, although sometimes only non-binary trees can be reconstructed in practice. Our definition of reticulate networks is similar to that in (Huson,  2007 ; Huson  et al. ,  2005 ; Semple,  2007 ) (Hallett and Lagergren,  2001 ; Nakhleh,  2009 ; Nakhleh  et al. ,  2004 ). A reticulate network (sometimes simply network) is a directed acyclic graph with vertex set  V  and edge set  E , where some nodes in  V  are labeled by taxa.  V  can be partitioned into  V T  (called tree nodes) and  V R  (called reticulation nodes).  E  can be partitioned into  E T  (called tree edges) and  E R  (called reticulation edges). Moreover,
 No nodes with total (in and out) degree of two is allowed. Except the root, each node must have at least one incoming edge. V R  contains nodes whose in−degrees are two or more.  V T  contains nodes whose in-degrees are one. E R  contains edges that go into some reticulation nodes.  E T  contains edges that go into some tree nodes. A node is labeled by some taxon iff its out-degree is zero. This helps to ensure labeled nodes correspond to extant species and remove some redundancy in the network. 
 In addition, we have one more restriction:
 R 1  For a reticulate network 𝒩, when only  one  of the incoming edges of each reticulation node is kept and the rest are deleted, we always derive a tree  T ′. 
 We first consider the derived tree  T ′ (that is embedded in 𝒩) as in restriction  R 1 . When we recursively remove non-labeled leaves and contract edges to remove degree-two nodes of  T ′ (called cleanup), we obtain a phylogenetic tree  T  (for the same set of species as in 𝒩). Now suppose we are given a phylogenetic tree  T . We say  T  is  displayed  in 𝒩 when we can obtain an induced tree  T ′ from 𝒩 by properly choosing a single edge to keep at each reticulation node so that  T ′ is topologically  equivalent  to  T  after cleanup. We denote the induced  T ′ (if exists) as  T 𝒩 . See  Figure 1  for an illustration.
 Fig. 1. An illustration of a reticulate network with three reticulation events for three trees. Each tree is displayed in the network: the tree can be obtained by keeping one incoming edge at each reticulation node. Note restriction  R 1  implies the network is  acyclic . Biologically, reticulate networks often forbid cycles. This is because many reticulation events need to be properly time-ordered. Thus, we focus on  acyclic  reticulate networks in this paper. That is, when we refer to a reticulate network, we mean an acyclic reticulate network (unless otherwise stated). There are subtle issues related to networks with nodes whose out-degrees are more than two (called non-binary nodes). See the  Supplementary Materials  for more discussion. Note that we do not require that in-degrees of reticulation nodes are precisely two as what was imposed in (Huson  et al. ,  2005 ). We also assume the root of each input tree  T i  is attached to an outgroup species  o . The root of a reticulate network for these trees is also attached to  o . We define the reticulation number of a reticulation node as its in-degree minus one. For a reticulate network 𝒩, we define the reticulation number (denoted as  R 𝒩 ) as the summation of the reticulation number of each reticulation node in the network. Sometimes  R 𝒩  is also called the number of reticulation events in 𝒩. For the reticulate network in  Figure 1 , the reticulation node 2 has three entering edges, and the other reticulation node 1 has two entering edges. Thus,  R 𝒩 =(3−1)+(2−1)=3. Our definition of reticulation number is similar to that of the hybridization number in (Bordewich  et al. ,  2007 ; Semple,  2007 ). Suppose we are given a set of  K  gene trees  T 1 ,  T 2 , …,  T K  (for the same set of species). The minimum reticulate network 𝒩 min  for  T 1 ,  T 2 ,…,  T K  is a reticulate network 𝒩 that displays  each T i  and  R 𝒩  is  minimized  among all possible 𝒩. We call  R 𝒩 min  the reticulation number of  T 1 ,…,  T K , which is denoted as  R ( T 1 ,  T 2 ,…,  T K ). For the special case of  K =2, we call  D T i , T j = R ( T i ,  T j ) the reticulation  distance  between two trees  T i  and  T j . Now we formulate the central problem in this article. The general minimum reticulate network (GMRN) problem: Given a set of phylogenetic trees  T ={ T 1 ,…,  T K }, reconstruct the minimum reticulate network 𝒩 min  for  T 1 ,  T 2 ,…,  T K . This formulation is based on parsimony, and may be justified when reticulation is relatively rare in the evolutionary history. One should note that the GMRN problem can be further specified by the type of input trees. There are two types of input phylogenetic trees: binary or non-binary. For a non-binary tree  T , we say  T  is displayed in 𝒩 if some refinement of  T  (i.e. splitting the non-binary nodes in  T  in some way to make  T  binary) is displayed in 𝒩. When input trees are binary, network reconstruction may be easier. For simplicity, in this following, the input trees are assumed to be  binary , unless otherwise stated. We remark that some of our results are applicable to non-binary input trees: the  RH  bound in  Section 3  clearly works for non-binary trees too, and the high-level approach of the  SIT  bound may also be applicable to non-binary trees. Previous work on the GMRN problem: There is an exact method for the  K =2 case of the minimum reticulate network problem (Bordewich  et al. ,  2007 ), although this special case is known to be NP-complete (Bordewich and Semple,  2007 ). It is useful to note that when we allow cycles in the network, the minimum reticulate network problem is equivalent to the rooted subtree prune and regraft (rSPR) distance problem. The rSPR distance problem, another NP-complete problem (Bordewich and Semple,  2004 ; Hein  et al. ,  1996 ), is well known to be closely related to reticulate evolution. Previously, we showed that the rSPR distance can often be practically computed for many moderately sized trees (Wu,  2009 ). We give more background to the rSPR distance problem in the  Supplementary Material . It was shown in (Baroni  et al. ,  2005 ) that the reticulation number (called hybridization number in (Baroni  et al. ,  2005 )) for trees  T 1  and  T 2  is closely related to the rSPR distance between  T 1  and  T 2 , although the two values are not always equal. The main difference between the rSPR distance and the reticulation number is that the latter forbids  cycles  and thus can be more realistic biologically. Recently, we have extended our previous approach in (Wu,  2009 ) to allow computing the pairwise  reticulation  distance between two rooted binary trees (Wu and Wang,  2010 ). Although the worst case running time of the practical methods in (Bordewich  et al. ,  2007 ; Wu,  2009 ; Wu and Wang,  2010 ) are exponential, these methods may work reasonably well in practice. As shown in (Bordewich  et al. ,  2007 ; Wu,  2009 ; Wu and Wang,  2010 ), exact reticulation number (with or without cycles) can be computed for two quite different trees with 20 or more leaves. Thus, although intractable theoretically, the two-tree minimum reticulate network problem can be solved in practice if the size of two trees is moderate or the two trees are not very different topologically. It becomes more computationally challenging when there are three or more gene trees. There is currently  no  known practical methods for either computing the reticulation number  R ( T 1 ,…,  T K ) or reconstructing 𝒩 min  for trees  T 1 ,…,  T K  when  K  ≥ 3. Often approximation is made. A common approach is to impose structural constraints to limit the complexity of the network (Gusfield,  2005 ; Huson and Klopper,  2007 ; Huson  et al. ,  2009 ; Nakhleh  et al. ,  2005 ). Although these approaches are theoretically interesting and have been shown to work for some biological data, it is still very desirable to explore the reconstruction of reticulated networks displaying multiple complete gene trees  without  additional structural constraints. 3 A LOWER BOUND We now focus on developing a lower bound on  R ( T 1 ,…,  T K ). The lower bound helps to better quantify the  range  of  R ( T 1 ,…,  T K ). Recall that several exact methods (Bordewich  et al. ,  2007 ; Wu and Wang,  2010 ) exist for computing the  pairwise  reticulation distance  D T i , T j  for two trees  T i ,  T j , which are practical for many pairs of trees of moderate sizes. Now suppose that we compute  D T i , T j  for each pair of trees  T i  and  T j , using the methods (Bordewich  et al. ,  2007 ; Wu and Wang,  2010 ). We store these pairwise distances in a matrix  D , where  D [ i ,  j ]= D T i , T j 2 . Admittingly, computing  D [ i ,  j ] for all  T i  and  T j  can be slow when  K  and/or the size of trees are large (unless  T i  and  T j  are very similar). One should note that the GMRN problem is much more complex, and thus, calculation of  D [ i ,  j ] is justifiable computationally. This leads to the following question: can we use the pairwise reticulation distances  D  to estimate  R ( T 1 ,…,  T K ) when  K  ≥ 3? Clearly, the  largest  value  D [ i 0 ,  j 0 ] in  D  is necessarily a  lower  bound of  R ( T 1 ,…,  T K ) when  K  ≥ 3: a reticulate network displaying all trees certainly also displays trees  T i 0  and  T j 0  and thus is a reticulate network for  T i 0  and  T j 0 . We now show a  stronger  lower bound (called  RH  bound) based on  D  values. Here is the high-level idea. The pairwise distance  D T i , T j  specifies how similar trees  T i  and  T j  are: the larger  D T i , T j  is, the more different  T i  and  T j  are. Recall that if tree  T i  is displayed in a network 𝒩, we should be able to derive  T i  by keeping only one incoming edge at each reticulation node and performing cleanup. The choice (called display choice for  T i ) of keeping which incoming edge at each reticulation node for a tree  T i  may not be unique. However, clearly if one makes the same display choices for  T i  and  T j  when displaying  T i  and  T j  in 𝒩, then  T i  and  T j  will be identical. More generally, the more similar the display choices for trees  T i  and  T j , the closer  T i  and  T j  will be. Thus, to allow an 𝒩 for trees with pairwise distances  D [ i ,  j ]= D T i , T j , we need to make display choices for the trees different enough: if the display choices for  T i  and  T j  are too similar, it will lead to contradiction when  D [ i ,  j ] suggests  T i  and  T j  are more different. In the following, a rigorous analysis based on this idea allows us to decide whether an 𝒩 with a specific number (say  r ) of reticulation events is feasible. To fix ideas, we first consider the situation where each reticulation node in 𝒩 has in-degree of two. We will remove this assumption in a moment. Suppose that 𝒩 has  r  reticulation nodes (each with two incoming edges). For each reticulation node, we arbitrarily call one incoming edge the left edge and the right edge for the other. We encode the left edge as 0 and the right edge as 1, and call these two edges 0-edge and 1-edge. Recall that to display a tree, we need to keep exactly one of these two edges. Since a tree  T i  is displayed in 𝒩, we create a binary vector  v i [1 …  r ] to represent which incoming edge  T i  is kept at each reticulation node  V j  in 𝒩. Here,  v i [ j ] is 0 if  T i  keeps the 0-edge at reticulation node  V j , and 1 if  T i  keeps the 1-edge at  V j . We call  v i  the  display vector  for  T i . For example, in  Figure 1 , consider the reticulation node labeled as 1, and we assign the left/right edges as shown.  T 1  and  T 3  keep the left edge at this node, while  T 2  keeps the right edge. Thus,  v 1  and  v 3  have value 0, and  v 2  has value 1 at the node. For a given  T i  and a network 𝒩,  v i  can always be constructed (at least conceptually) based on how  T i  is displayed in 𝒩. Note that if there are multiple choices to display  T i , we simply pick an arbitrary one and this does not affect our solution. We define  D h [ v i ,  v j ] as the Hamming distance between two display vectors  v i  and  v j . Here,  v i  and  v j  (and thus  D h [ v i ,  v j ]) depend on 𝒩. To simplify notations, we do not explicitly include 𝒩 in their definitions. Lemma 3.1 is crucial to our lower bound. L emma  3.1. For any two trees T i   and T j   displayed in a reticulate network  𝒩,  D h [ v i ,  v j ] ≥  D [ i ,  j ]. P roof . For contradiction, assume  D h [ v i ,  v j ]&lt; D [ i ,  j ]. Thus  T i  and  T j  make different choices at less than  D [ i ,  j ] reticulation nodes of 𝒩. Imagine we remove from 𝒩 those incoming edges at reticulation nodes that are  not  kept by both  T i  and  T j . This produces a network with less than  D [ i ,  j ] reticulation nodes. This is because all reticulation nodes where  v i  and  v j  match (and thus  T i  and  T j  keep the  same  incoming edges) have only one incoming edge and are no longer reticulation nodes in the reduced network. This contradicts the fact that  D [ i ,  j ] is the reticulation distance between  T i  and  T j . ▪ Lemma 3.1 implies that if a network 𝒩 with  r  reticulation events exists, then we should be able to find binary vectors  v i  (of length  r ) for each tree  T i , and  D h [ v i ,  v j ] ≥  D [ i ,  j ] for any two such vectors  v i  and  v j . On the other hand, if such vectors do  not  exist, we know that at least  r +1 reticulation events are needed (and the value  r +1 is a lower bound on  R ( T 1 ,…,  T K )). We can illustrate this formulation more intuitively using a binary  hypercube . On a hypercube with  r  binary bits per node, we want to know whether we can pick  K  points  v 1  …  v K  that are far apart enough such that the Hamming distance between  v i  and  v j  is at least  D [ i ,  j ] for each  i  and  j . One should note this is not always feasible due to the limited size of the hypercube. Formally, The binary hypercube point placement problem: Can we choose  K  nodes  v 1 ,…,  v K  from a  r -dimensional binary hypercube so that  D h [ v i ,  v j ] ≥  D [ i ,  j ] for each pair of  v i  and  v j ? A lower bound on  R ( T 1 ,…,  T K ) based on the Hypercube Point Placement problem is to find (possibly in a binary search style) the  smallest  integer  r  such that the hypercube point placement problem  has  a solution. Such  r  is necessarily a lower bound on  R ( T 1 ,…,  T K ). We call this lower bound reticulation on hypercube bound (or  RH  bound). We do not know a polynomial-time algorithm for the binary hypercube point placement problem with more than three trees. When  K =3, however, the  RH  bound has a simple analytical form (see  Section 3.2 ). To develop a practical method for the general case, we use integer linear programming (ILP) to solve this problem. We create a binary variable  V i , k  to represent the coordinates for point  v i . That is, the coordinates of  v i  on the hypercube are specified by  V i ,1  …  V i , r . Without loss of generality, we set  V 1, k =0 for all 1≤ k ≤ r . We create a binary variable  M i , j , k  for each  v i ,  v j  and position k (1≤ k ≤ r ) to indicate whether two vectors  v i  and  v j  match at position  k .  M i , j , k =1 if  V i , k = V j , k , and 0 otherwise. Now, we have the following formulation.
 Optimization goal: none (since this is a feasibility problem) Subject to 
 
 M i , j , k + V i , k + V j , k  ≥ 1, for each  v i ,  v j , where  i &lt; j  and 1≤ k ≤ r . M i , j , k  −  V i , k  −  V j , k  ≥ −1, for each  v i ,  v j , where  i &lt; j  and 1≤ k ≤ r . ∑ k =1 r   M i , j , k  ≤ r − D [ i ,  j ], for each  v i ,  v j , where  i &lt; j . 
 
 For each 1≤ i ≤ K  and 1≤ k ≤ r , there is a binary variable  V i ,  k . For each 1≤ i &lt; j ≤ K , and 1≤ k ≤ r , there is a binary variable  M i , j , k . 
 Constraint 1 says if both  V i , k  and  V j , k  are 0,  M i , j , k  is 1 (i.e. matched). Similarly, constraint 2 says if both  V i , k  and  V j , k  are 1,  M i , j , k  is 1 (i.e. matched). Constraint 3 imposes the pairwise Hamming distance requirement. Our experience shows that the ILP is practical to solve for all datasets we simulated (see  Section 5 ). 3.1 Networks with in-degree of three or more We now resolve the remaining issue where some reticulation nodes have in-degree of three or more. In this section, we call a reticulation node ‘refined’ if its in-degree is two, and ‘unrefined’ if its in-degree is at least three. Here, we can no longer represent a reticulation node as binary value, as done previously. So we extend our definitions of display vectors  v i  to allow  v i  to be non-binary. That is, if there are  d  incoming edges at a reticulation node, we allow  v i  to be from 0 to  d −1, where the value indicates which one of the  d  branches  T i  is kept at this node. The incoming edges are numbered starting from zero on the left and to the right with increment of one. We still let  D h [ v i ,  v j ] be the Hamming distance between vectors  v i  and  v j . In this general case, Lemma 3.1 still holds for non-binary vectors  v i  and  v j . To see this, we prune any incoming edge at reticulation nodes if it is not chosen by  T i  and  T j . Then each remaining reticulation node has only  two  incoming edges (since we only have two trees). Thus, there are  D h [ v i ,  v j ] reticulation events in this reduced network, and the rest of proof for Lemma 3.1 follows. We now show that it is not necessary to consider unrefined reticulation nodes in the sense that if a network 𝒩 with unrefined reticulation nodes satisfies pairwise distances  D , then there exists another network 𝒩′ that has only refined reticulation nodes and gives the binary vectors  v i  satisfying the pairwise distance constraints of  D . That is, if we can not find a network with only refined reticulation nodes, we also can not find a network with unrefined reticulation nodes and the same reticulation number. To see this property, we consider a network 𝒩 with one reticulation node  q  with  d  ≥ 3 incoming edges. Then we transform 𝒩 to 𝒩′ by replacing  q  with  q 1 ,…,  q d −1 , where each  q i  is a reticulation node with in-degree of two. Note that we do not have to ensure 𝒩 and 𝒩′ are equivalent: we only need to show 𝒩′ gives a solution to the Binary Hypercube Point Placement problem. Clearly, 𝒩 and 𝒩′ have the same reticulation number (although vectors for 𝒩′ are longer). Now, suppose tree  T i  keeps edge  j  at  q  (where 0≤ j ≤ d −1), we then keep edge 1 at  q j  in 𝒩′ if  j  ≥ 1 (and 0 if  j =0), and keep edge 0 for all other  q j ′  (where  j ′≠ j ). In other words, we create a mapping of the display vectors  v i  from 𝒩 to 𝒩′ for each  T i . Note that such mapping ensures that if two trees keep the same edge at  q , they will keep the same edges at  q 1 ,…,  q d −1  in 𝒩′; otherwise, they will keep at least one different incoming edge at  q 1 ,…,  q d −1  in 𝒩′. In either case, if the pairwise distance constraints are satisfied in 𝒩, they are also satisfied in 𝒩′. So, if we can not find display vectors 𝒩′ for networks with refined reticulation nodes only, we also can not find display vectors for networks allowing unrefined reticulation nodes. In other words, the  RH  bound holds for networks with unrefined nodes. Remark.  The  RH  lower bound is still applicable when the input trees are non-binary, as long as the pairwise reticulation distances are obtained for the non-binary trees. These are easy to verify and we omit the details due to the lack of space. Remark.  A commonly used concept in reticulate networks is the so-called maximum agreement forest (MAF). A brief description on MAF is given in the  Supplementary Material . Also see e.g. in (Semple,  2007 ) for more details. It is easy to see that the size of a MAF of multiple trees is a lower bound on  R ( T 1 ,…,  T K ). However, experience show that the  RH  bound is often higher than the MAF bound (see  Section 5 ). 3.2 Special case of three trees The special case of  K =3 allows us to study the  RH  bound in an analytical way. We let  d 1 ,  d 2  and  d 3  be the pairwise reticulation distances of the three trees, where  d 1  ≥  d 2  ≥  d 3 . Proposition 3.2 shows the  RH  bound for three trees in an analytical form. P roposition  3.2. The RH lower bound for three trees T 1 ,  T 2   and T 3   is equal to     if d 2 + d 3 &gt; d 1 ,  and equal to d 1   if d 2 + d 3 ≤ d 1 . P roof . We first consider the case  d 2 + d 3 &gt; d 1 . Clearly, the  RH  bound is at least  d 1 , which is the minimum size of the hypercube. Now we investigate whether there exists a reticulate network with  d 1 + e  reticulation nodes for these three trees. Without loss of generality, let  T 1  be the input tree where  d 1 = D T 1 , T 2  and  d 2 = D T 1 , T 3 , and the display vector  v 1  (for  T 1 ) is fixed to be all-0. Then, the display vector  v 2  for  T 2  must have at least  d 1  positions with value 1 (and thus  v 2  has no more than  e  positions with value 0). Similarly, the display vector  v 3  must have at least  d 2  positions with value 1 (and thus  v 3  has no more than  d 1 + e − d 2  positions with value 0). Note that  D h [ v 2 ,  v 3 ] ≥  d 3 . We claim that  D h [ v 2 ,  v 3 ]≤ d 1 +2 e − d 2 . This is because the Hamming distance between  v 2  and  v 3  counts the positions where  v 2  has value 0 and  v 3  has value 1 (or vice versa). Since the number of 0s in  v 2  is no more than  e , there are at most  e  positions where  v 2  has 0 and  v 3  has 1. Similarly, there are at most  d 1 + e − d 2  positions where  v 2  has 1 and  v 3  has 0. Thus,  D h [ v 2 ,  v 3 ]≤ e + d 1 + e − d 2 = d 1 +2 e  −  d 2 . Also note that we can always construct  v 2  and  v 3  so that  D h [ v 2 ,  v 3 ]= d 1 +2 e − d 2 . See  Figure 2  for an illustration.
 Fig. 2. Vectors  v 1 ,  v 2  and  v 3  (listed from top to bottom) that maximize the Hamming distance between  v 2  and  v 3 .  v 1  is all-0, while the suffix of  v 2  and prefix of  v 3  are zeros. Therefore, if  d 1 +2 e − d 2  &lt;  d 3 , we can  not  find three vectors  v 1 ,  v 2  and  v 3  satisfying the pairwise distances  D  and thus  R ( T 1 ,  T 2 ,  T 3 ) ≥  d 1 + e +1 in this case. The largest such  e  is equal to   (which is non-negative since  d 2 + d 3  &gt;  d 1 ). The  RH  bound is then   =  . The case when  d 2 + d 3 ≤ d 1  is simple. We create three vectors of  d 1  bits:  v 1  is an all-0 vector,  v 2  is an all-1 vector and  v 3  contains  d 2  1s. It is easy to verify these three vectors satisfy all three pairwise distance constraints. ▪ In practice, it is very likely  d 2 + d 3  &gt;  d 1 . In this case,  , where  d 1  is a trivial lower bound. 4 AN UPPER BOUND We now present an upper bound on  R ( T 1 ,…,  T K ). The combination of the  RH  lower bound and the upper bound quantifies the  range  of  R ( T 1 ,…,  T K ). In the best scenario, if the upper bound  matches  the  RH  bound, these bounds would actually determine the  exact  value of  R ( T 1 ,…,  T K ) (and also reconstruct 𝒩 min ). On the high level, the upper bound performs  s tepwise  i nsertion of  t rees into a reticulate network (and thus is called the  SIT  bound). The  SIT  bound is very accurate and also computable in practice for many datasets. The basic idea of the  SIT  bound is to reconstruct a reticulate network 𝒩 in a step-by-step way: ‘insert’ the given gene trees one by one into 𝒩 in some fixed order. When we say a tree  T  is inserted into 𝒩, we mean adding reticulation edges into 𝒩 such that  T  is displayed in the updated network 𝒩′. Note that addition of new reticulation edges increases  R 𝒩 . Thus, every time we insert a new tree, we seek to add as  few  new reticulation edges as possible by  reusing  existing reticulation edges. At the same time, we also ensure no cycles exists in 𝒩′. Often, it is unclear which order of inserting trees gives the best result. For now, we assume that  K  is relatively small so that we can enumerate all possible orders of insertion to find the best result. See  Section 4.2  for ways to handle larger  K . Thus, we can assume the order of tree insertion is fixed to  T 1 ,  T 2 ,…,  T K . The general procedure of the  SIT  bound (for a fixed order) is as follows.
 Initialize 𝒩 to be  T 1 . for  i =2 to  K Insert  T i  into 𝒩 by adding the smallest number of new reticulation edges. 
 Note that we only add reticulation edges in 𝒩 and do not delete any existing edges. Thus, any tree already displayed in 𝒩 is still displayed in the updated 𝒩′ by choosing the original reticulation edges when the tree is first inserted for their display vectors in 𝒩′. This ensures that each of the input trees is displayed in the final 𝒩. Obviously, step 3 is most critical, which we will discuss next. 4.1 Inserting tree  T  into 𝒩 We consider the ‘min-cost tree insertion problem’, where we want to update 𝒩 by adding the  fewest  reticulation edges to 𝒩 so that a given tree  T  is displayed in the updated 𝒩′, and 𝒩′ remains acyclic. Note that the min-cost tree insertion problem is NP-complete because it contains the two-tree minimum reticulate network problem (an NP-complete problem) as a sub-problem. That is, constructing the minimum reticulate network for trees  T 1  and  T 2  can be solved by inserting  T 2  into  T 1  with the minimum cost. In the following, we develop a practical method to solve the min-cost tree insertion problem. Each node of the reconstructed network here has one or two incoming edges (except the root), and one or two outgoing edges (except the leaves). After inserting  T  (and some new reticulation edges are added),  T  is displayed in the updated network 𝒩′. Suppose we  remove  all the new reticulation edges in 𝒩′. The edge removals break tree  T (𝒩′) (the tree created by keeping edges in 𝒩′ according to a display vector of  T ) into a  forest F ( T (𝒩′)). Thus, the number of newly added reticulation edges is  exactly  the number of trees in  F ( T (𝒩′)) minus one. To minimize the number of needed new reticulation events, we need to minimize the number of trees in  F ( T (𝒩′)). A useful observation is that the problem of finding  F ( T (𝒩′)) with the fewest subtrees is closely related to the maximum agreement forest problem (see the  Supplementary Material ) as follows. Imagine that we choose a tree  T ′ that is displayed in 𝒩 so that the display vector of  T ′ agrees with that of  T  for 𝒩′ at each reticulation node of 𝒩. Recall that 𝒩′ may contain a number of new reticulation nodes that are not in 𝒩. Also note  T ′ is not necessarily one of the input trees  T i . We claim that  F ( T (𝒩′)) is an agreement forest for  T  and  T ′. To see this, we note that the display choices made by  T ′ are identical to  T  except those at the new reticulation nodes (where  T ′ follows the original edge and  T ( N ′) follows the new edge). So the subtrees in  F ( T (𝒩′)) must also be subtrees of  T ′. So, we have: L emma  4.1. The forest induced by removing newly added reticulation edges of T (𝒩′)  is an agreement forest between T and some tree T ′  that is displayed in the original  𝒩. Lemma 4.1 implies that to find the best tree insertion, we can find some tree  T ′ displayed in 𝒩 s.t. the number of trees in the maximum agreement forest between  T  and  T ′ is minimized.  Figure 3  shows an example of tree insertion. The dashed lines in the tree (left) divide the tree into a forest, which also appears in the existing network (middle, thick lines). Inserting the tree into the network is to add new reticulation edges (right, thick lines) into the networks so that the subtrees in the forests are properly connected to match the given tree.
 Fig. 3. Inserting a tree (left) to a network (middle). After adding new reticulation edges (thick lines), the resulting network (right) displays the tree. When the number of reticulation nodes in 𝒩 is small, we may simply enumerate all trees  T ′ displayed in 𝒩 and then find which  T ′ gives the smallest agreement forest with  T . This quickly becomes infeasible as the number of reticulation nodes in 𝒩 grows: when there are  r  reticulation nodes in 𝒩, there may exist 2 r  trees  T ′ displayed in 𝒩. To develop a practical method, we develop an integer linear programming (ILP) formulation to solve the tree insertion problem in an optimal way (without explicit enumeration). The output of the ILP formulation includes the display choices of  T ′ as well as the associated agreement forest formed by cutting edges in  T . See the  Supplementary Material  for detailed description of the formulation. Updating 𝒩: after tree  T ′ and the associated agreement forest are found, we update 𝒩 as follows. We add new reticulation edges in 𝒩 to connect subtrees of  T ′ in the found agreement forest to make  T  displayed in the updated network 𝒩′. First, we determine the order of subtree connection with an approach similar to the algorithm building two-tree hybridization networks in (Semple,  2007 ). The subtree with the special outgroup taxon  o  acts as the base. Then we repeatedly pick the subtree not intersecting any already connected subtree as the next to connect. Now, for each tree connection:
 Find the root  r  of the next subtree (in 𝒩) to attach. Find the node  v  in the existing network as the attaching point to accept this subtree. Create a new reticulation node in 𝒩 to connect the subtree. 
 This operation depends on the types of  r  and  v . Two cases are shown in  Figure 4 . The other cases are similar. In all the cases, only a single new reticulation edge is created to connect a subtree.
 Fig. 4. Attaching a subtree in 𝒩. Left: insert a reticulation edge between two tree nodes  r  and  v . Right: insert a reticulation edge between two reticulation nodes  r  and  v . The dashed lines are the newly added edges. Cycles: a remaining issue is that cycles can be introduced when connecting subtrees in 𝒩. There are two sources of cycles. First, the found agreement forest may induce cycles (see (Baroni  et al. ,  2005 )). Enhancing the ILP formulation to avoid cycles may significantly complicate the formulation and slow the ILP solving. A practical observation is that cycles in an agreement forest are often caused by two pairs of leaves  a ,  b  and  c ,  d  so that the a/b pair is ancestral to c/d pair in  T  and the c/d pair is ancestral to a/b pair in  T ′. Here, we say a pair of leaves a and b is ancestral to a pair of leaves c and d if the the MRCA of a and b is ancestral to the MRCA of c and d. MRCA stands for the most recent common ancestor, and node a is ancestral to node b in tree  T  if a is on the path from b to the root of  T . To forbid this type of simple cycles, we enhance the ILP formulation: for such pairs a/b and c/d, we require either a and b are not in the same subtree, or c and d are not in the same subtree of the resulting forest. Although this does not guarantee to remove all cycles, we found that cycles in the agreement forest are rare after this change. This observation is also useful for the method of computing pairwise reticulation distances in (Wu and Wang,  2010 ). Second, cycles can appear in other parts of the network when subtrees in the agreement forest are connected. In practice, however, we find this happens relatively rare. When this type of cycles does occur, we simply start over and try another order of tree insertion. This works well in practice: in  Section 5 , we build acyclic networks successfully for all (thousands of) simulated datasets. 4.2 Handling larger datasets When the size and the number of trees grow, the running time increases. To handle larger datasets, we make several simplifications. (i) Instead of enumerating all possible orders of tree insertion, we start with an arbitrary tree. At each step, we pick a tree with the smallest reticulation distance to one of the already inserted trees. (ii) Solving the min-cost tree insertion problem optimally becomes more difficult when data grows. So instead of considering all possible  T ′ displayed in 𝒩 when inserting  T , we randomly choose a fixed number (say 10) of trees  T ′ displayed in 𝒩 (in addition to all the inserted gene trees) and find the best way of inserting  T  based on one of the chosen  T ′. This heuristic is called the  coarse  mode (and the original approach is called the  full  mode). Our experience shows that the coarse mode works reasonably well in practice (see  Section 5 ). 5 EXPERIMENTAL RESULTS We have implemented a software tool called PIRN (which stands for Parsimonious Inference of Reticulate Network) to compute the  RH  and  SIT  bounds. Program  PIRN  is available for download from:  http://www.engr.uconn.edu/˜ywu/ . The tool is written in C++ and uses either CPLEX (a commercial ILP solver) or GNU GLPK ILP solver (mainly a demo of the functionalities for users without a CPLEX license). In computing the  SIT  bound,  PIRN  can run full mode (slower but can give better results) or coarse mode (faster but less accurate). We test our methods for both simulated and biological data on a 3192 MHz Intel Xeon workstation. 5.1 Simulation data We generate simulation data using a two-stage approach: first simulate reticulate networks, and then generate a fixed number of trees displayed in the networks according to randomly generated display vectors. We simulate reticulate networks using a scheme similar to the coalescent simulation implemented in program ms (Hudson,  2002 ). For a given number of taxa (denoted as  n ), we start with  n  isolated lineages and simulate reticulation  backwards  in time. At each step, there are two possible events: (i) lineage merging, which occurs at rate 1; (ii) lineage splitting, which occurs at rate  r . We choose the next event according to relative probabilities of all feasible events. Lineage merging generates speciation events, while lineage splitting generates reticulation events. To speedup the simulation, lineage splitting is disabled when the number of current lineages is no more than three. The parameter  r  dictates the level of reticulation in the simulated network: larger  r  will lead to more reticulation events in simulation. Full mode of the  SIT  bound: to test the performance of the bounds, we generate data with varying number of trees  K , number of taxa  n  and level of reticulation  r . For each settings of these three parameters, we simulate 100 datasets. We report the percentage of datasets where  optimal  solution is found (i.e. lower bound matches upper bound) in  Figure 5 a. To show how close the lower and upper bounds are, we report the average gap (the difference between the upper and the lower bounds, divided by the lower bound) in  Figure 5 b. We also report the average lower bound in  Figure 5 c, which somewhat reflects how complex the simulated networks are. We also give the average running time for each setting in  Figure 5 d. For five larger datasets, there are a small number of test cases that are too slow to run the full mode, and are excluded. The percentage of unfinished computation is usually one or two out of 100 datasets, except the cases with  n =30/ r =3.0/ K =5 (18% unfinished) and  n =30/ r =5.0/ K =4 (6% unfinished). This suggests the current practical range of the full mode of the  SIT  bound.
 Fig. 5. Performance of the  RH  bound and  SIT  bound (full mode), for 10, 20, 30, 40 and 50 taxa, number of trees K from 3 to 5, and reticulation level  r  at 1.0, 3,0 and 5.0. ( a ). The percentage of exact reticulation number found among 100 simulated datasets. ( b ) Average gaps (in percentage) between the  SIT  bound and  RH  bound (normalized by the  RH  bound), and the average  RH  bound is shown in ( c ). ( d ) The average running time (in seconds). The reticulation level r for left, middle and right figures is 1.0, 3.0 and 5.0, respectively. Horizontal axis is the number of taxa, and each curve in a figure is for a value of  K . As shown in  Figure 5 a,  PIRN  performs very well when the number of trees  K =3 or reticulation level r is small:  optimal  solution can be found for at least 80% of simulated datasets when  r =1.0 and  K =5. Even with higher reticulation level ( r =3.0) and larger number of taxa (say 50), still about 60% of datasets can be solved exactly when  K =3. As expected, as the number of taxa, reticulation level and the number of trees grow, fewer datasets can be solved to exact, and correspondingly,  Figure 5 b shows gaps between the  RH  and  SIT  bounds increase.  Figure 5 c shows the complexity of networks increases too. Nevertheless, the gaps are still relatively small in these cases. For the more difficult settings simulated (i.e. 30 taxa, high reticulation level and five trees as input), the gap is about 25%. Running time depends on the complexity of the networks.  Figure 5 d shows that  PIRN  is practical for data of medium size. Coarse mode of the  SIT  bound: we also test the coarse mode of our methods for larger data, as described in  Section 4.2 . The results are shown in  Figure 6 . The figure on the left shows the effects (on accuracy and running time) of increasing the number of taxa  n . The figure on the right shows the effects of having more trees (i.e. increasing  K  from three to nine) for 10 taxa and reticulation level 5.0. There is clear trade-off between the accuracy of solutions and efficiency. The coarse mode under-performs in terms of the quality of solutions, but is more scalable, especially when  K  increases. When the number of taxa increases, the coarse mode is likely to run faster than the full mode, but the difference is less significant.
 Fig. 6. Comparing the coarse (C) mode and the full (F) mode of the  SIT  bound. Left: fix  r =3.0 and  K =4, while varying  n . Right: fix  r =5.0 and  n =10 and vary  K . Both percentages of optimal solutions found and running time (in minutes for the left figure and seconds for the right one) are shown. GLPK: the CPLEX solver is used in the simulation. The GLPK version in general is less robust and can handle smaller data than the CPLEX version. Our experience shows that the GLPK solver can often solve for five trees with 30 taxa when reticulation level is low and fewer number of taxa when reticulation level is higher. The  RH  bound: we now compare the performance of the  RH  bound with the MAF bound. We note that the MAF bound is not easy to compute: finding the MAF of only two trees is known to be NP-hard. When data is small, the MAF bound can be computed (e.g. using ILP similar to that in (Wu,  2009 )). In  Table 1 , we compare the  RH  bound and the MAF bound for 100 datasets. Each data has 10 or 20 taxa and contain three to seven correlated trees. The trees are selected from the local trees for recombining sequences generated by a coalescent simulator, program ms (Hudson,  2002 ).
 Table 1. Compare the  RH  and MAF bounds for K trees K =3 K =4 K =5 K =6 K =7 n =10 RH  &gt; MAF 46 56 61 58 72 RH  = MAF 54 44 38 41 27 RH  &lt; MAF 0 0 1 1 1 100 ( RH  −  MAF / MAF ) 16.5 18.4 17.2 18.0 20.9 n =20 RH  &gt; MAF 65 66 68 70 - RH  = MAF 34 34 30 30 - RH  &lt; MAF 1 0 2 0 - 100 ( RH  −  MAF / MAF ) 12.6 11.9 12.1 12.8 - RH  &gt; MAF: number of datasets where the  RH  bound is larger than the MAF bound among 100 datasets. The average gaps (in percentage) between the  RH  and MAF bounds are also shown. 
 Table 1  shows that the  RH  bound outperforms the MAF bound in a majority of the simulated datasets and only very rarely the  RH  bound is lower than the MAF bound. In general, as the number of trees increases, the  RH  bound tends to outperform the MAF bound in both accuracy and running time. For example, for a dataset with 20 sequences and six input trees, CPLEX runs for over 11 h without reporting a solution (it found a solution of 11, but did not validate its optimality). In contrast, it only takes less than 1 minute to compute the  RH  bound of value 12 (higher than the MAF result). 5.2 Biological data To evaluate how well our bounds work for real biological data, we test our methods on a Poaceae dataset. The dataset was originally from the Grass Phylogeny Working Group (Grass Phylogeny Working Group,  2001 ). The dataset contains sequences for six loci: internal transcribed spacer of ribosomal DNA (ITS); NADH dehydrogenase, subunit F (ndhF); phytochrome B (phyB); ribulose 1,5-biphosphate carboxylase/oxygenase, large subunit (rbcL); RNA polymerase II, subunit β′′ (rpoC2); and granule bound starch synthase I (waxy). The Poaceae dataset was previously analyzed and rooted binary trees were inferred for these loci (Schmidt,  2003 ). Pairwise comparison were performed in (Bordewich  et al. ,  2007 ; Wu and Wang,  2010 ). Here, we provide in  Table 2  our results on estimating the reticulation number using multiple (three to five) trees. Only  shared  taxa of a set of trees are kept. Thus, the pairwise distances reported here are different from those in (Bordewich  et al. ,  2007 ; Wu,  2009 ). As shown in  Table 2 ,  PIRN  finds  optimal  solutions for both datasets with three trees, and computes lower and upper bounds that are  close  for the dataset with five trees.  Figure 7  shows the reconstructed reticulate network for these five trees. See  Supplementary Material  for a graphical display of the five grass trees. The network contains 13 reticulation events, and the lower bound is 11. Although the network may not be optimal, the gap between the lower and upper bounds is relatively small.
 Fig. 7. A reticulate network found by program  PIRN  for five trees of a grass dataset. Red (shaded) balls represent reticulation nodes. 
 Table 2. Results for the grass data Trees n D RH SIT Time rpoC2, waxy, ITS 10 1, 6, 6 7 7 1 s ndhF, phyB, rbcL 21 4, 5, 6 8 8 1 s ndhF, phyB, rbcL,rpoC2, ITS 14 11 13 26 min 38 s Both  RH  and  SIT  bounds are shown, as well as the running time .  n : the number of taxa.  D : pairwise distances. Remark : the following lists several aspects on the performance of  PIRN . (i) Simulation shows that  PIRN  is often able to find the exact reticulation number when  K  or  r  is small, even when the number of taxa increases to medium size (say 50). Moreover,  PIRN  can compute the  RH  and  SIT  bounds for a wide range of data, despite the fact that we do not currently have polynomial-time algorithms for computing the bounds. We achieve this with the help of integer linear programming. (ii) Computing the  RH  bound is often much faster and more scalable than the  SIT  bound. Experience shows that the ILP formulation for computing the  RH  bound is often very fast to solve and computing the pairwise reticulation distances usually takes less time than finding a good upper bound for all trees. The  RH  bound computation will also benefit from future improvements in computing the pairwise reticulate distances. The simulation results in this section are based on an earlier version of the method in (Wu and Wang,  2010 ) and speedup may be possible with the latest methods. (iii) The number of trees  K  and the similarity of tree topologies have impact on  PIRN 's optimality and running time. Using more powerful ILP solver (e.g. CPLEX) and/or more powerful machines may also help for more difficult cases. (iv) Finally, our general approaches can be applied to larger data by using efficient computable lower bounds of pairwise rSPR distances in computing the  RH  bound, and faster but less accurate heuristics to insert trees into a network. 
          Supplementary Material 
          
             
                [Supplementary Data] 
             
             
             
          
       </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Robust quantitative scratch assay</Title>
    <Doi>10.1093/bioinformatics/btv746</Doi>
    <Authors>Vargas Andrea, Angeli Marc, Pastrello Chiara, McQuaid Rosanne, Li Han, Jurisicova Andrea, Jurisica Igor</Authors>
    <Abstract>The wound healing assay (or scratch assay) is a technique frequently used to quantify the dependence of cell motility—a central process in tissue repair and evolution of disease—subject to various treatments conditions. However processing the resulting data is a laborious task due its high throughput and variability across images. This Robust Quantitative Scratch Assay algorithm introduced statistical outputs where migration rates are estimated, cellular behaviour is distinguished and outliers are identified among groups of unique experimental conditions. Furthermore, the RQSA decreased measurement errors and increased accuracy in the wound boundary at comparable processing times compared to previously developed method (TScratch).</Abstract>
    <Body>1 Introduction Since cell migration is significant in repair processes and disease advancement such as metastasis and cancer angiogenesis, it can be used as a parameter to describe the state of health of various cell lines ( Gebäck  et al. , 2009 ;  Zordan  et al. , 2011 ). Furthermore, quantification of cell migration can be used to measure the effect of drug treatments on the same cell line. The Wound Healing Assay (WHA) is used to quantify motility by monitoring the evolution of a wounded monolayer of cells. The experimental set-up consists of growing a confluent monolayer of cells under specific conditions (i.e. treated or not treated with a drug), then creating a wound or scratch on each layer/well, and then imaging all wounds multiple times over a period of time. Since the rate how wounds close may depend on conditions used, measuring wound areas from individual images are used to estimate migration rates, and thus quantify condition-specific cell motility. ( Jonkman  et al.  2014 ) provide extensive insight on experimental design considerations and workflow of the WHA measurements. In order to obtain reliable results, it is necessary to measure wound areas across multiple replicates of the same cell condition, and record multiple frames per replicate to determine whether changes are statistically significant. To make this task feasible, an automated system is required to process the high volumes of data and make measurements independent of image artefacts such as uneven illuminations, smudges or scratches on the well plate lid, in addition to some variation of initial wound width and shape. Sample images of wound areas are shown in  Supplementary Material . While previous methods by Gebäck  et al.  and Zordan  et al.  support automated WHA image analysis, they lack a final statistical output estimating the migration rates per condition, and lack an image set for validation, therefore depending on the acquisition parameters, these methods may or may not work. Therefore, we have developed the Robust Quantitative Scratch Assay (RQSA) algorithm using MATLAB 7.11.0 to address the challenges imposed by the large datasets, variability in image illuminations and implemented a statistical output for an improved quantification of cell motility using the wound healing assay. Furthermore, we provide image datasets for reproducibility and have developed a set of guidelines for future WHA image acquisitions. 2 Methods The algorithm requires MATLAB 7.11.0 (or GNU Octave) and uses the Statistics and Image Processing Toolboxes. In order to design and validate the RQSA algorithm a training and testing image datasets were acquired and are described in detail in Section 1.1 of the  Supplementary Material . The image and data processing comprises measuring individual open wound areas for each well using a sequence of morphological and spatial filters (Gonzales, 2003;  http://www.mathworks.com ). Then the resulting  open wound area vs. frame  curve is fitted to a polynomial curve using the Least Squares method in order to calculate migration rates, coefficients of determination ( R 2 ), and to label the temporal behavior for each well. The estimated migration rates for all wells in a single condition are grouped, then outliers are flagged and discarded before calculating the average migration rate for each experimental condition. Last, the average migration rates are compared across all experimental conditions. The statistical analysis is output as a text file; Section 1.3 in  Supplementary Material  shows sample open wound area curves, sample output text file for a single condition and describes the process for outlier detection. 3 Results The robustness and improved performance of the RQSA algorithm was evaluated over a ‘Robustness set’—a set of images ( N  = 268) with various features, such as high/low image illumination, resolution, cell confluence, and stray cells in the wound area.  Open Wound Areas  and  Wound Estimate  were measured with the RQSA algorithm and TScratch were compared to manual measurements,  Figure 1  shows the error in  Open Wound Area  for this subset. Section 2.1 in  Supplementary Material  describes these metrics of comparison with more detail and discusses the robustness of the algorithm. A paired t-test was performed on the errors in  Open wound area  and errors in  Wound estimate  using both methods, where resulting p-values ( P  &lt; 0.05) determined that there is a statistical difference between both methods. Overall RQSA showed better representation of the wound boundaries resulting in a more accurate estimate of migration rates over all conditions.
 Fig. 1. Measurement errors resulting from TScratch (dashed line) and RQSA (solid line) 
 Both methods were tested using a validation set, and results were compared by determining  R 2  from the fitted curves in both experimental conditions. The non-treated condition with RQSA shows an  R 2  value centered at 1.0, suggesting that the  open wound areas vs. frame  from the RQSA algorithm yield a better description of the temporal behavior of the wound, and shows less variability across time points. Meanwhile, the lower  R 2  values of the TScratch polynomial fits suggest that measurements fluctuate highly from one frame to the other. The RQSA was run using an additional independent set with a different cell line and treatments, to demonstrate its robustness. Results of both datasets with statistical analysis including average migrations rates, uncertainties, identified possible well outliers, and are further discussed in Section 2.2 of  Supplementary Material . Future directions for the RQSA include the incorporation of cell segmentation routines to provide a morphological and proliferation assessment on individual cells, and implementation of parallel computing techniques to decrease processing times and increase efficiency. Conflict of Interest : none declared. 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>ISOLATE: a computational strategy for identifying the primary origin of cancers using high-throughput sequencing</Title>
    <Doi>10.1093/bioinformatics/btp378</Doi>
    <Authors>Quon Gerald, Morris Quaid</Authors>
    <Abstract>Motivation: One of the most deadly cancer diagnoses is the carcinoma of unknown primary origin. Without the knowledge of the site of origin, treatment regimens are limited in their specificity and result in high mortality rates. Though supervised classification methods have been developed to predict the site of origin based on gene expression data, they require large numbers of previously classified tumors for training, in part because they do not account for sample heterogeneity, which limits their application to well-studied cancers.</Abstract>
    <Body>1 INTRODUCTION While most cancerous tumors present at their cancer site of origin (CSO), ∼4% of all new tumors do not (American Cancer Society,  2001 ). Without knowledge of this site, treatment regimens are highly limited in their specificity and result in high mortality rates (Blaszyk  et al. ,  2003 ; Shaw  et al. ,  2007 ). In an effort to identify CSO, patients routinely undergo extensive clinical examination, radiology and histoimmunological testing (Hainsworth and Greco,  1993 ). However, these drastic interventions fail to identify the site of origin more than half of the time (Blaszyk  et al. ,  2003 ). Gene expression profiling provides a precise, high-resolution molecular fingerprint of a tumor that also offers insight into the underlying transcriptional activity that gave rise to its aberrant behavior (Liotta and Petricoin,  2000 ). To date, a number of supervised classification methods have been used to categorize tumors according to their site of origin based on gene expression profiles, including support vector machines (Ramaswamy  et al. ,  2001 ; Su  et al. ,  2001 ; Tothill  et al. ,  2005 ), decision trees (Dennis  et al. ,  2005 ; Shedden  et al. ,  2003 ),  K -nearest neighbors (Bridgewater  et al. ,  2008 ; Giordano  et al. ,  2001 ; Horlings  et al. ,  2008 ), neural networks (Bloom  et al. ,  2004 ) and others (Buckhaults  et al. ,  2003 ; Dennis  et al. ,  2002 ; Varadhachary  et al. ,  2008 ). These studies all share a similar three-step strategy: transcriptional profiles of many tumors with known sites of origin are used to identify individual marker genes whose expression levels discriminate cancers of different origin; then the expression levels of these marker genes in each tumor are used to train a classifier that is subsequently used to classify new tumors not previously labeled with a site of origin. These microarray-based models have shown great diagnostic potential for identifying the site of origin of patients with carcinomas of unknown primary origin: accuracies of &gt;80% were commonly reported for some types of carcinomas. However, because these methods are supervised classification methods, they require large amounts of transcriptionally profiled tumors with identified origin upon which to train. While this data may be available for mature tumors from common sites of origin, there are many less well-characterized cancers or poorly differentiated tumors that often have very little or no data available upon which to train. Reported prediction accuracy on these underrepresented tumors is little better than random performance (Ramaswamy  et al. ,  2001 ; Shedden  et al. ,  2003 ; Su  et al. ,  2001 ). Classifier performance also depends critically on the CSO-specific marker genes identified in the preprocessing step, making downstream analysis highly sensitive to the marker set (Tothill  et al. ,  2005 ). Unsupervised methods that neither rely on previously collected training data nor prescreen for marker genes are therefore of high value. However, to the best of our knowledge, such methods are not currently available. It is often of interest to not only identify the site of origin, but also to identify the genes differentially expressed in the cancer cells with respect to the site of origin. Ideally, the tumor expression profile can be directly compared with that of the CSO to identify those differentially expressed genes. However, tumors are not homogeneous masses of cancer cells, but are mixtures of cell populations with varying levels of heterogeneity, not only between tumors of the same cancer but also even across the samples from the same tumor (Dennis  et al. ,  2005 ). Contaminating cell populations can include surrounding healthy tissues (such as the site of origin or the local site of a metastatic tumor) and supporting stroma (Masters and Lakhani,  2000 ). Sample heterogeneity contributes significantly to the large diversity of expression profiles observed even from similar tumor samples, and in many cases contaminating non-cancer cells can dominate the expression profile (Golub  et al. ,  1999 ; Liotta and Petricoin,  2000 ; Reya  et al. ,  2001 ), as illustrated in  Figure 1 . While methods exist for de-convolving heterogeneous expression profiles into their individual component profiles and inferring the so-called mixing proportions (also known as coefficients), based on techniques like Independent Components Analysis (ICA) (Hyvarinen,  2001 ; Lahdesmaki  et al. ,  2005 ; Venet  et al. ,  2001 ), they have been developed independently of the models for identifying CSO and therefore are currently applied as a preprocessing step.
 Fig. 1. Multiple samples taken from even the same tumor can be composed of different mixing proportions of component sources, giving rise to significantly different tumor expression profiles compared with the expression profile of the homogeneous cancer cell population. Methods for de-convolving sample heterogeneity and removing the contributions of non-cancerous cell populations to the measured expression profile aim to re-construct the expression profile of the homogeneous cancer cell population. 
 The advent and rapidly decreasing cost of high-throughput sequencing (HTS) methods for expression profiling promises much higher reproducibility and a wider dynamic range of detectable gene expression than microarrays (Marioni  et al. ,  2008 ; Mortazavi  et al. ,  2008 ). HTS methods are quickly becoming feasible for highly accurate characterization of the transcriptome profile of tumors. However, the digital counting of sequence tags in HTS methods leads to a different observation of noise process compared with the analog measurement of probe intensity in microarrays. This change requires an update to the statistical models used to analyze these data. In this article, we present ISOLATE, a model for the Identification of Sites of Origin by LATEnt variables. ISOLATE is the first method that simultaneously identifies sites of origin in an unsupervised fashion and addresses sample heterogeneity using HTS cancer expression profiling. ISOLATE is designed to achieve three goals: identification of the site of origin from a set of profiled candidate sites, de-convolution of heterogeneous expression profiles into their individual components and identification of differentially expressed genes. We demonstrate on both synthetic and clinical datasets that ISOLATE achieves higher accuracy on all of these goals than a similar ICA-based unsupervised strategy that mirrors existing tools. The high accuracy levels achieved by ISOLATE demonstrate the feasibility of unsupervised methods for complementing traditional immunohistological and supervised classification models for identifying the site of origin of and characterizing tumors from carcinomas of unknown primary origin. 2 APPROACH OVERVIEW ISOLATE is designed to achieve three goals, illustrated in  Figure 2 : identification of the CSO, identification of the differentially expressed genes in the homogeneous cancer cell population and characterization of the cellular composition of each heterogeneous tumor sample (by estimation of the mixing proportions of their component cell populations). We compared ISOLATE with an ICA-based strategy that can be applied using existing tools to address the three challenges. In this study, we use Latent Dirichlet Allocation (LDA) (Blei  et al. ,  2003 ) to implement the ICA strategy. LDA is an equivalent model to ICA (Shashanka  et al. ,  2008 ) but with an observation noise model more appropriate for digital HTS data.
 Fig. 2. Outline of the ISOLATE and LDA methods for de-convolving cancer gene expression data and identifying the site of origin. The input to each method consists of the expression profile of a heterogeneous tumor sample(s), as well as the Source Panel representing previously profiled cell populations that may act either as contaminants or as candidate sites of origin. Each method performs three tasks: identification of the site of origin, identification of those genes differentially expressed in the cancer cells and characterization of the cellular composition of each heterogeneous sample by estimating the mixing proportions of each component cell population. The ICA-based strategy operates serially by first de-convolving the heterogeneous sample without constraining the cancer profile to be derived from the Source Panel, then predicts the site of origin and differentially expressed genes. This is in stark contrast to ISOLATE, which solves all three problems cooperatively. 
 Both ISOLATE and LDA model the expression profile of a heterogeneous tumor sample as a weighted mixture of expression profiles of ‘source’ cell populations (representing candidate sites of origin or contaminants), all of which have been previously characterized except for the homogeneous cancer cell population. The set of source cell populations are herein called the ‘Source Panel’. Candidate sites of origin and potential contaminating cells can be treated similarly in the context of LDA and ISOLATE and are hence both referred to as sources. ISOLATE differs from LDA in that it explicitly models the similarities in expression profile between the cancer cell population and the site of origin by representing the homogeneous cancer expression profile as a sparsely perturbed version of the profile of its site of origin. Tumor cells display functional, developmental and morphological similarities to their site of origin (Lobo  et al. ,  2007 ; Sell and Pierce,  1994 ). This similarity is also reflected at the gene expression level, both between the primary tumor and the site of origin (Khan  et al. ,  2001 ; Ross  et al. ,  2000 ), and between the primary and metastatic tumor (D'Arrigo  et al. ,  2005 ; Weigelt  et al. ,  2003 ). By explicitly modeling the cancer cell expression profile as a perturbation of the site of origin profile, our model is a more precise representation of cancer that naturally leads to the identification of differentially expressed genes as those whose expression was perturbed to produce the tumor expression profile. ISOLATE then uses the estimate of the homogeneous cancer profile in conjunction with the Source Panel to decompose each tumor sample. A key feature of ISOLATE is that it recognizes the interdependence of the solutions of all three goals and iteratively solves all of them simultaneously. In contrast, the ICA strategy first iteratively decomposes the tumor samples while estimating the profile of the homogeneous cancer cell population. Then it compares the estimated homogeneous cancer profile to the Source Panel and identifies the parent site as the most similar profile, and finally identifies differentially expressed genes by comparing the estimated cancer profile to that of the identified site of origin. This makes the ICA-based strategy for identifying the site of origin sensitive to imperfect de-convolution of the tumor expression profiles and often leads to misidentification of the site of origin as the surrounding tissue. The following sections describe how we generated the synthetic datasets and collected and processed the clinical datasets that we used to test our model. We also describe statistical inference with ISOLATE and LDA. 3 METHODS 3.1 Synthetic data collection We measured the performance of ISOLATE on a comprehensive set of synthetic data for which the correct answer is known. Our strategy for generating data is shown in  Figure 3  and summarized below, with more details provided in following sections. Each experiment is defined by five parameters: the number of genes whose expression is perturbed in the cancer cells, their multiplicative perturbation factor, the number of heterogeneous tumor samples profiled, the number of sources in the Source Panel and the level of biological variability of the expression profiles of the same sources between different cancer patients. First, using human kidney and liver data from Marioni  et al.  ( 2008 ), we generate expression profiles for each source, both for the (a) training profiles that make up the Source Panel, and (b) for a template healthy patient. From the template healthy patient, (c) we randomly select one component source as the site of origin, from which we perturb the expression profile to construct a cancer cell expression profile. The original template of healthy source expression profiles together with the cancer cell expression profile make up a template cancer patient, from which we (d) generate one or more unique cancer patients by adding variability to the template cancer patient independently for each cancer patient. (e) One heterogeneous tumor sample is generated from each individual using a unique set of mixing proportions to combine the source profiles of the cancer patient. Finally, we use the Source Panel and the heterogeneous tumor samples as input into the LDA and ISOLATE models, to (f) identify the CSO, (g) de-convolve the heterogeneity of each tumor sample, and identify differentially expressed genes.
 Fig. 3. Overall experimental strategy for generating the heterogeneous tumor samples from three sources (i.e. candidate sites of origin) to input into the LDA and ISOLATE models. The sources color-matched between the Source Panel and the template healthy patient differ only by technical variability in their expression profiles. Yellow represents cancer cells, while orange represents the site of origin. 
 3.1.1 Dataset Human liver and kidney transcriptome profiling data from a single human male was obtained from Marioni  et al.  ( 2008 ) who sequenced each tissue seven times, split across two runs of an Illumina Genome Analyzer and at two concentrations, 1.5 pM and 3 pM. All reads were mapped to the genome using the Illumina ELAND algorithm, and only uniquely mapped reads were retained. A gene copy number is computed by counting the number of reads mapped to each known transcript, then computing the median number of copies for each gene over all of its respective transcripts. We discarded all genes for which there was not even one copy in all of the runs of both tissues, leaving 13 061 genes. Gene abundances (also called the expression profile) were computed from gene copy numbers by dividing each copy number by the sum of all gene copy numbers. 3.1.2 Generating a new source expression profile We first applied a differential expression test (Lu  et al. ,  2005 ) to identify the top 40% of all genes that were most likely to be constitutively expressed across all of the kidney and liver datasets and deemed these to be candidate house-keeping genes (Zhu  et al. ,  2008 ). We then randomly selected two runs from either the kidney or liver datasets, and permuted the expression levels of their non-house-keeping genes randomly in the same order. One run is used in the Source Panel ( Fig. 3 a) as previously profiled abundances in LDA and ISOLATE, while the other is used in the template healthy individual ( Fig. 3 b). 3.1.3 Generating a cancer cell expression profile To generate a cancer cell expression profile given the expression profile of the site of origin, the number of genes to perturb and their perturbation factor, we first randomly selected the set of genes to become differentially expressed, then randomly perturbed the abundances of each gene in that set either up or down (with equal probability), then renormalize the abundances to sum to 1 to make gene abundances correspond to parameters of a multinomial distribution. This cancer expression profile and the healthy tissue profiles in the template healthy individual combine to make the sources in the template cancer patient ( Fig. 3 c). 3.1.4 Generating a cancer patient from the template cancer patient We use the template cancer patient to obtain a cancer patient profile by adding biological variability to each source expression profile. We represent biological variability by resampling the expression levels of a fraction of genes from the entire set of expression levels observed in that source's original expression profile. The expression profile is subsequently rescaled to sum to 1. 3.1.5 Generating a heterogeneous tumor sample from a cancer patient We first determine what proportion of each source will compose the tumor sample, then we generate the sequence reads that are observed in the sample. For the tumor sample  d , the mixing proportions of the sources  θ d  are drawn from a Dirichlet distribution with parameters  α  = {α s c , α s 1 , α s 2 ,…, α s S }, where  s c  indicates the cancer source. In our experiments, for all non-cancer sources  i  ≠  c , α s i  = 1, and α s c  = 3 by default. Larger values of α s c  will result in tumor samples containing larger proportions of cancer cells. Once the mixing proportions  θ d  are generated, for each transcript read to generate, we randomly select a source using the mixing proportions  θ d , then randomly select a transcript from which to generate a read using the multinomial distribution specified by the expression profile of the chosen source. Each tumor sample was generated with 1 675 078 reads, the average number of reads collected per experiment in Marioni  et al.  ( 2008 ), though the results were not sensitive to the total number of reads generated per tumor sample (data not shown). 3.2 Clinical data processing Both the ISOLATE and LDA strategies require a fully profiled Source Panel and heterogeneous tumor samples, but owing to the current lack of such data available, we took advantage of the vast quantities of microarray data available and chose to digitize such datasets to make them compatible with our model. We downloaded a total of 93 tumor expression profiles from Su  et al.  ( 2001 ), consisting of 10 kidney, 6 liver, 24 lung, 23 ovary, 6 pancreatic and 24 prostate-originating tumors collected using Affymetrix U95a GeneChip arrays. Following the procedure of Su  et al.  ( 2001 ), for each tumor sample, raw intensity values were thresholded at 20. Mappings from the probe identifier to Ensembl gene identifiers were downloaded from the Affymetrix web site, and multiple probes matching the same Ensembl gene identifier were averaged together to produce a single measurement for each gene. The resulting array intensities were rounded to the nearest integer and treated as transcript counts from a HTS experiment. As a Source Panel, we downloaded a separate set of microarray data collected using Affymetrix Human Genome U133A arrays (Su  et al. ,  2004 ), giving us a healthy profile version of those same six tissues. Intensities for replicate array measurements were averaged together for each respective tissue, and using the provided annotation files, each probe was mapped to its respective Ensembl gene identifier, and multiple probes matching the same Ensembl gene identifier were averaged together. The total set of common genes profiled in the Source Panel and the tumor profiles were 8667 genes. For these 8667 genes, their raw averaged intensities in each source of the Source Panel were divided by the total intensity measured to produce a proper multinomial distribution over the profiled genes. 3.3 Inference with the LDA model The input to the LDA model (Blei  et al. ,  2003 ), for both the synthetic and clinical datasets, consists of the expression profiles over all the genes in each source. Each profile is represented by a vector from the set {β s } s =1 S  that contains one vector for each of the  S  sources from the Source Panel. Also input to the LDA model are  D  sets of reads { t d , n } d =1 D  originating from transcriptome profiling experiments of  D  heterogeneous tumor samples that each generate  N d  reads. LDA estimates the expression profile  β s c  of all genes in the cancer source  s c  and performs de-convolution by inferring hidden variables { z d , n } (one for each read  t d , n ) that indicate from which of the  S  + 1 sources ( S  from the Source Panel, and 1 from the cancer source) the transcript most likely originates. In doing so, LDA estimates the fraction of each cancer sample  d  (the mixing proportions), θ d , s , coming from each of the  S  + 1 sources. The full model is specified below:
 (1) 
 (2) 
 (3) 
The model was trained using the same variational Expectation Maximization (EM) framework used in Blei  et al.  ( 2003 ) with 100 iterations, and rerun  S  times with random parameter initializations. The initialization that resulted in the highest log likelihood of the data is chosen. The model parameters estimated include  α ,  θ d  for all tumor samples  d , and cancer abundances  β s c . Using the output of LDA, we predict the site of origin by choosing the source (from the Source Panel) whose expression profile has the least Kullback–Liebler divergence from the estimated cancer expression profile  β s c . To rank genes in order of differential expression, we applied a two-class differential expression test (Lu  et al. ,  2005 ) to compare the expression profile of the predicted site of origin against the set of reads the cancer cells are responsible for in each tumor sample ( z d , n  =  s c ), and sorted the genes based on the resulting  P -value. Lastly, the mixing proportions (heterogeneity) of each sample  d  are estimated directly from the learned parameters of the model,  θ d . 3.4 Inference with the ISOLATE model ISOLATE maintains the same probabilistic framework as LDA [Equations ( 1 –  3 )], but introduces the following key constraints on the learned parameters  β s c , where β :, g  is a column vector of abundances of gene  g  across all  S  non-cancer sources:
 (4) 
 (5) 
 (6) 
 ω  is a ( S  × 1)-dimensional parameter, where ω s  = 1 denotes that source  s  is the site of origin. ρ g  is the estimated perturbation (multiplicative) factor that describes how much the cancer cells perturb the expression of gene  g  relative to the site of origin described by  ω . Since we expect many genes to maintain similar expression levels to that of the CSO, we put a Gamma prior on ρ g  [Equation ( 5 )], with mean  E [ρ g ] = 1 to emphasize that we expect many genes to not have perturbed expression. ISOLATE uses the same variational EM framework as LDA (Blei  et al. ,  2003 ) with 100 iterations, and rerun using  S  different initializations to test different candidate CSO. There is exactly one initialization per source  s  where the value ω s  is set to 1, and the remaining entries set to 0. The initialization that resulted in the highest log likelihood of the data is chosen. To rank genes in order of differential expression, we sorted the genes based on the distance of ρ g  from the value 1. That is, the farther ρ g  is from 1, the more perturbed its abundance is from that of the site of origin. Finally, mixing proportions of each sample  d  is estimated directly from the learned parameters of the model,  θ d . 3.5 Performance metrics The error rate in identifying the primary site of origin is the fraction of experiments in which the CSO was incorrectly identified. For the synthetic datasets, the reported error is averaged over the 20 datasets generated for each specific setting of the parameters. The model heterogeneity error is computed by averaging, over all tumor samples, the mean absolute error of the mixing proportions  θ d  of the cancer cells and the true site of origin. We only measure the error with respect to these two sources because we found that almost all of the error in the mixing proportion estimates is from these two sources. Finally, we assess the error of the identification of differentially expressed genes for the synthetic datasets by using the ranks of the genes (in order of differential expression as defined by each model) and our knowledge of which genes are truly differentially expressed to compute an area under the receiver operator curve (ROC), where larger values correspond to higher accuracy. Error is computed as [1 − (Area under ROC)]. 4 RESULTS 4.1 Synthetic datasets We have evaluated the relative performance of ISOLATE and LDA as a function of realistic parameter settings to demonstrate their robustness to different conditions. We also compared naiveLu, a simple method for identifying differentially expressed genes by simply applying a differential expression test directly without accounting for sample heterogeneity. We do these comparisons because of the dearth of clinical data and the difficulties associated with defining gold standards therein. We are also able to query a larger variety of experimental conditions. In the absence of analytical estimates of performance, which are likely impossible due to the complexity of our models, these comparisons provide the best support for our claims of improved performance over LDA. We varied the following parameters: the number of differentially expressed genes, the perturbation factor by which their expression levels are differentially expressed in cancer, the number of heterogeneous tumor samples, the number of sources in the Source Panel, and the (biological) variability between our profiled Source Panel and the corresponding profiles used to generate the tumor samples (see  Section 3 ). This variability represents the expected differences between the normal source profiles in our Source Panel, which will likely come from different individuals, and the corresponding source profiles for the patient from which the tumor sample is drawn. Biological variability, which could represent either biological variation or technical noise, is a key parameter because it limits our ability to detect differentially expressed genes, as seen below. In the following, we vary only a single parameter from the default; the default parameters we use are 100 perturbed genes, 3 tumor samples, 10 sources, a perturbation factor of two, perturbation scale prior κ = 10 [see Equation ( 5 )], and a biological variability of 0.16 (16%), which empirically leads to ∼14% of genes differentially expressed, as measured by Lu  et al.  ( 2005 ). This level of differential expression between simulated individuals is similar to reported variation between unrelated individuals (Sharma  et al. ,  2005 ). 4.1.1 Identification of differentially expressed genes One of the principal objectives of identifying CSO is to identify genes that are differentially expressed in the cancer cell population with respect to healthy cells of the site of origin. We tested each models' ability to identify the differentially expressed genes, defined as those genes whose expressions were perturbed to differentiate the cancer source from the site of origin source, and measured the performance by the area under the ROC curve (see  Section 3 ).  Figure 4 a demonstrates that ISOLATE consistently achieves higher accuracy at identifying differentially expressed genes than LDA across all three parameters at almost all settings. Surprisingly, the performance of both LDA and ISOLATE seem to stay constant despite increasing the number of tumor samples available.  Figure 5 a illustrates that beyond a variability level of 15%, increasing the amount of data does not improve ISOLATE performance. Because 15% is near the average variability between unrelated individuals (Sharma et al,  2005 ), this result suggests that ISOLATE's identification of differentially expressed genes can be improved by analyzing multiple tumor samples from the same individual but not necessarily by analyzing multiple samples from different individuals. Both ISOLATE and LDA improve performance as the perturbation factor increases—a direct result of its increasing differentiation from the site of origin source and hence easier de-convolution of sample heterogeneity—though ISOLATE improves at a much faster rate. The performance of naiveLu, which does not address heterogeneity, illustrates that de-convolution clearly improves the identification of differentially expressed genes. The ISOLATE performance gain over LDA is not just simply due to a difference in the specific method that ISOLATE uses to compute differential expression: we computed differential expression using the same method as for LDA (ISOLATE-Lu) and see that its performance is still better than LDA in many cases.
 Fig. 4. Performance of ISOLATE and LDA on synthetic datasets. Each row represents a different parameter tuned: the number of heterogeneous tumor samples, the number of sources (non-cancer) in the Source Panel and the perturbation factor of the differentially expressed genes (manipulating the number of perturbed genes within the range of 50–500 genes did not result in changes in performance to either model and are not shown). Each column represents a different performance metric applied to each dataset. ( a ) Differential expression error, ( b ) origin error and ( c ) heterogeneity error are as defined in  Section 3 . Two additional models are plotted in (a): ISOLATE-Lu is the performance achieved when applying the same method as ICA for identifying differentially expressed genes (Lu  et al. ,  2005 ) to the output of the ISOLATE model, and naiveLu is the performance achieved when directly comparing the heterogeneous tumor expression profiles to the site of origin to identify differentially expressed genes, without accounting for sample heterogeneity. 
 Fig. 5. Performance of ISOLATE and LDA on synthetic datasets under different biological variability conditions. Each column represents a different performance metric, and each row a different model (top, ISOLATE; bottom, LDA). Here, we co-vary the biological variability added to each tumor sample independently, and the number of tumor samples made available to each model. The performance metrics of ( a ) differential expression error, ( b ) origin error and ( c ) heterogeneity error are as defined in  Section 3 . 
 4.1.2 Identification of CSO Figure 4 b compares LDA and ISOLATE based on how often they are able to correctly identify the site of origin. ISOLATE consistently outperforms LDA across all datasets. Most importantly, while ISOLATE is robust against the number of sources in the Source Panel, the performance of LDA diminishes rapidly after six sources. This makes LDA and other ICA-like techniques impractical for considering many potential candidates for the CSO, an important feature given the potentially large set of candidate CSO to query. The difference between ISOLATE and LDA also illustrates the improvement in CSO identification, sometimes as staggeringly as 70%, achieved by solving for both cell population mixture coefficients and CSO simultaneously within the same framework. From  Figure 5 b, we see that ISOLATE achieves high accuracy at identifying CSO under even high variability conditions, while LDA accuracy varies quite widely even under low variability conditions. ISOLATE is therefore able to capture the underlying signal of the site of origin even despite large amounts of noise in the expression profiles. Most importantly, even when looking at very small sets of tumor samples, ISOLATE performs as well as it does with many more samples, an important feature given the cost of profiling tumors in a diagnostic setting. 4.1.3 Correction of sample heterogeneity Figure 4 c illustrates that when considering Source Panels containing fewer than 10 profiles, ISOLATE achieves better de-convolution of heterogeneity than LDA. However, their performance is nearly identical regardless of the perturbation factor applied to the differentially expressed genes, suggesting that the amount of tumor sample data is far more important for de-convolution than the difference in expression profiles of the cancer cells and the site of origin. As expected, as the number of tumor samples increases, both LDA and ISOLATE increase in performance.  Figure 5 c illustrates that ISOLATE achieves better accuracy for the same number of data points and level of variability, although it takes more samples for a given level of biological variability than LDA to achieve its maximum performance. Most of the performance loss in LDA appears to be due to confusion of the contributing expression signatures from the cancer cells and the site of origin source ( Fig. 6  illustrates an example). This is a problem that ISOLATE is able to mitigate because of the constraints it places on the learned cancer expression profile.
 Fig. 6. An example of heterogeneity error for a single tumor sample. Here, we illustrate a representative tumor sample whose mixing proportions of component sources were estimated by ISOLATE (top row) and LDA (bottom row), compared with the actual values (middle row). Each of the four columns on the left represent a source from the Source Panel (Sources 1–3, as well as the site of origin source), while the right-most fifth column represents the cancer cells. The area of each square is proportional to the fraction of the sample composed of that particular source. Whereas ISOLATE estimated mixing proportions fairly close to the truth, the LDA estimate of the CSO and cancer sources were quite erroneous. 
 4.2 Clinical dataset We used ISOLATE and LDA to predict the site of origin of 93 tumor samples from Su  et al.  ( 2001 ). Heterogeneity and differential expression error could not be measured on these datasets as the true values are not known. Each tumor, regardless of its site of origin, is predicted independently of all other tumors, to reproduce clinical diagnostic conditions. As a benchmark besides LDA, we constructed a predictor that chooses the source from the Source Panel whose Kullback-Leibler (KL)-divergence is least with respect to the tumor sample's expression profile (KL). To set the perturbation scaling prior κ of Equation ( 5 ), we tried several values of κ (10 0 , 10 1 , 10 2 , 10 3 , 10 4 , 10 5 ), and performed 2-fold cross-validation by choosing the κ that maximized performance of half the data, in order to predict the other half of the dataset, and vice versa. The optimal value of κ was 10 5  for both halves of multiple splits of the data, and so was used to generate the results shown in  Figure 7 . Over the entire dataset of 93 tumor samples, ISOLATE achieves the highest performance of 65.59% accuracy, compared with 52.69% of both LDA and the KL measure. On a class-by-class basis, ISOLATE ties or performs better than LDA and ISOLATE in the larger classes, only performing worse when predicting tumors originating from the pancreas, the smallest class. Note that though previously reported performance of supervised classification methods is higher on some of these cancer types, ISOLATE achieves the observed performance considering each tumor separately without reference to any of the other tumor samples and without any training, mirroring clinical settings for the CSO identification of tumors underrepresented among previously profiled samples.
 Fig. 7. The performance of ISOLATE, LDA and another Kullback-Leibler (KL) divergence-based measure on the clinical dataset of 93 tumor samples. Each sample is predicted independently of all other samples in the dataset. The number of samples in each class is shown beside the class name, and classes are in decreasing order of size, from left to right, with the overall performance shown in the leftmost column. The black line shows random performance. 
 5 DISCUSSION We have developed ISOLATE to provide a molecular diagnostic tool to aid in identifying the site of origin of tumors of poorly characterized cancers, situations in which classical supervised models perform poorly. ISOLATE simultaneously de-convolves tumor expression profiles, and identifies the CSO and genes differentially expressed in the cancer cells, three tasks that were previously solved independently. Our experiments detail the performance of ISOLATE under a wide range of realistic experimental conditions for synthetic and digitized clinical microarray data, showing that solving all three tasks simultaneously leads to greater predictive performance than solving them individually. ISOLATE, unlike previous methods for classifying cancers of unknown primary origin, is an unsupervised classification algorithm, which provides it with several inherent advantages. It does not require a large training set of tumors of known primary origin, and in our clinical validation we only use data from a single tumor, a particularly important feature given the difficulty and cost of procuring many high-quality tumor samples in a diagnostic setting. Because it is an unsupervised algorithm, ISOLATE's performance is also less sensitive to the number of candidate sites of origin, as suggested by our synthetic data validation, in contrast to supervised learning methods that have difficulty with more than 10 classes (Su  et al. ,  2001 ). By its construction, ISOLATE is also less prone to overfitting than supervised learning algorithms, and as such, does not require a prescreening stage to identify marker genes upon which cancers could be discriminated. Despite our use of microarray data for our clinical validation, we recommend that ISOLATE be used exclusively with HTS expression profiles, which we believe support more accurate tumor diagnosis. HTS methods promise a substantial reduction in sample-to-sample variability, which our synthetic data-based validation shows limits the accuracy. Also, because HTS measurements are not probe-based, they are both less platform-specific, allowing easier integration of data from multiple labs, and less sensitive to polymorphisms in transcript sequence that are common in highly polymorphic cancer genomes. For these reasons, we have tailored ISOLATE's statistical model for HTS gene expression data. The successful application of ISOLATE or other expression-based models will depend on the availability of expression profiles for a wide range of human tissues, in order to consider them as potential sites of origin. With the costs of high-throughput expression profiling dropping quickly and the number of studies using these technologies to profile tumors increases (Jones  et al. ,  2008 ; Parsons  et al. ,  2008 ), soon it will be practical to collect a compendium of expression data from many of the individual tissues of humans along with multiple tumor samples, as is currently available for microarrays (see, e.g. Su  et al.  ( 2004 ). Molecular-based diagnostic tools for identifying cancer sites of origin represent an important class of tools that can potentially facilitate faster, more accurate diagnoses leading to the successful identification of primary sites. ISOLATE will be an invaluable tool for exploring new, uncharacterized cancers of unknown primary origin for which little expression data are available, or clinically ambiguous samples for which more traditional models cannot classify with high accuracy. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Tablet—next generation sequence assembly visualization</Title>
    <Doi>10.1093/bioinformatics/btp666</Doi>
    <Authors>Milne Iain, Bayer Micha, Cardle Linda, Shaw Paul, Stephen Gordon, Wright Frank, Marshall David</Authors>
    <Abstract>Summary: Tablet is a lightweight, high-performance graphical viewer for next-generation sequence assemblies and alignments. Supporting a range of input assembly formats, Tablet provides high-quality visualizations showing data in packed or stacked views, allowing instant access and navigation to any region of interest, and whole contig overviews and data summaries. Tablet is both multi-core aware and memory efficient, allowing it to handle assemblies containing millions of reads, even on a 32-bit desktop machine.</Abstract>
    <Body>1 INTRODUCTION The advent of next-generation sequencing (NGS) technologies such as Roche 454 (Margulies  et al. ,  2005 ) and Illumina Solexa ( http://www.illumina.com/sequencing ) has brought about a need for fast, efficient and user-friendly tools for analyzing the outcome of sequencing runs. This includes visualization software for viewing the resultant assemblies or alignments, for example, Consed (Gordon  et al. ,  1998 ), Hawkeye (Schatz  et al. ,  2007 ), EagleView (Huang and Marth,  2008 ), MapView (Bao  et al. ,  2009 ), SAMtools' tview (Li  et al. ,  2009 ) and Maqview ( http://maq.sourceforge.net/maqview.shtml ). All assembly visualization packages face the following challenges when dealing with NGS data: processing a very large number of reads, providing high-quality rendering and navigation of assembled reads, and supporting a widening range of assembly formats. Additionally, as analysis and interpretation of the data moves from large-genome centers to smaller laboratories, there is an increasing need for biologist-friendly software that has an intuitive user interface, is available for a range of common desktop platforms and has no complicated installation dependencies. With these features in mind, we have developed Tablet, a lightweight, high-performance and memory efficient assembly viewer. Tablet is aimed at users of all abilities and combines simple installation on a desktop machine with ease of use and a visually rich interface. The application supports both single and multi-core processor architectures and will scale its performance according to the number of processor cores available. 2 FEATURES Tablet can import data from ACE, AFG, MAQ and SOAP assembly formats (with preliminary support for SAM), and can handle both 454 and Solexa data. Its visualizations are split into several areas; the main display provides a view of a single contig at a time, with reads aligned against their consensus sequence. Reads are colored according to nucleotide type and subtle use of gradients and color choice allow visual structure to be maintained even when fully zoomed out. Tablet will lay out the data in either packed (showing as many reads per line as possible without overlap) or stacked (showing one read per line) formats, and allows the user to switch instantly between them. A sortable list, containing all available contigs shows contig lengths as well as numbers of reads and annotation features, and can be dynamically filtered by any of its fields. Continuous zooming of the entire contig in real time is supported by means of a slider, and there is also an option for varying the contrast between variant and non-variant nucleotides which adjusts the brightness used to display read bases that differ from the consensus, thus aiding identification of potential single nucleotide polymorphisms (SNPs) or sequencing errors. An overview window located above the consensus can display either a scaled-to-fit summary of all the reads in a contig, or a coverage graph which shows the read coverage along the entire length of the contig independent of the current zoom level. Navigation within a contig is catered for in several ways. First, the current view point is controlled by manipulating the scroll bars to move in either direction around the display or by dragging with the mouse directly on the canvas itself. We also provide a page-at-a-time navigation option that will move the view left or right by the number of bases that are currently visible. High-speed navigation to any area of the view is also available by clicking and dragging on the overview window, which always displays a bounding rectangle representing the portion of the overall data currently visible within the main display. Protein translations are optionally provided for all six reading frames of the consensus sequence. Annotation features such as SNPs and indels can either be imported with the assembly file itself or separately in GFF3 format, and are then listed on a separate tab attached to the contig list. Information on a given read is provided as a graphical overlay as soon as the mouse is moved over it. Tablet will display its name, start and end positions (optionally with unpadded consensus values), the sequence length, whether it is complemented or not, and also provide a scaled-to-fit graphical representation of the bases within the read. All of the raw data can be copied to the clipboard at any time. 3 IMPLEMENTATION AND PERFORMANCE Tablet is written in Java and is compatible with any Java-enabled system with a runtime level of ≥1.6. We provide installable versions that include everything required to run the application, including a suitable Java runtime. The installers are available for Windows, Mac OS X, Linux and Solaris, in both 32- and 64-bit versions. Once installed and running, Tablet will also monitor our server for new versions and will prompt, download and update quickly and easily whenever a new release is available, along with redirecting the user to a web page describing the new features that have been added. A prime requisite during development of Tablet has been computing efficiency and speed. The two main approaches to handling assembly data in viewers are either memory-based, where all the data are loaded into memory, or disk-cached, where the data reside on disk with only the currently visible segment of the dataset held in memory. Memory-based applications are faster for viewing and navigation (after an initial delay while loading the data) and can provide whole dataset overviews and statistical summaries, but the size of dataset they can handle is limited by the amount of available memory. In contrast, cache-based applications can display views from much larger datasets using a minimum of memory, but access to the data can be orders of magnitude slower (which then affects navigation and rendering), and the feature sets available are often limited. With Tablet, we have chosen a hybrid solution that provides us with advantages from both approaches. We hold a ‘skeleton’ layout of the reads in memory, with data on each read limited to just an internal ID, its position against the consensus or reference sequence and its length. The nucleotide data itself (efficiently compressed so it can be read as quickly as possible), along with other supplementary information—such as the read's name and its orientation—is held in an indexed disk-cache and is only accessed (via the read's ID) when required. Tablet also allocates memory on a per-contig basis, including information for features such as how to pack the data for display, coverage calculations, padded-to-unpadded mappings, etc. These data are calculated and stored before each contig is rendered and discarded again after display. This approach allows us to provide maximum functionality—instant access to any portion of the data; extremely fast and high-quality rendering; entire dataset overviews—yet memory usage is kept relatively low. Comparing data indexing/loading times and memory consumption across a range of tools for an assembly file containing ∼2.9 million Illumina Solexa reads of length 51, we found that the cache-based viewers (Maqview, MapView, tview) were fairly constant in memory usage (between 35 MB and 70 MB while viewing), with indexing times varying from 10 s to 50 s, although memory consumption during indexing did peak as high as 350 MB with MapView. For the memory-based viewers, we compared Hawkeye (5500 MB; 107 s), Consed (2600 MB; 73 s) and EagleView (2450 MB; 98 s). Tablet, being a hybrid, loads the data in 25 s, and uses just 175 MB of memory.
 Fig. 1. Tablet showing Illumina Solexa reads against an  Arabidopsis thaliana  cDNA reference sequence (additional screenshots can be seen online at  http://bioinf.scri.ac.uk/tablet/screenshots.shtml ). 4 FUTURE WORK Work is in progress to support paired-end sequence data, and to enhance Tablet's visualization of annotation data. We also plan to further reduce Tablet's memory requirements by cutting down on the amount of reference/consensus information held at any time. Experiments have shown that further reductions should be possible without compromising data access times, graphical rendering speed or visualization quality. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>GimmeMotifs: a de novo motif prediction pipeline for ChIP-sequencing experiments</Title>
    <Doi>10.1093/bioinformatics/btq636</Doi>
    <Authors>van Heeringen Simon J., Veenstra Gert Jan C.</Authors>
    <Abstract>Summary: Accurate prediction of transcription factor binding motifs that are enriched in a collection of sequences remains a computational challenge. Here we report on GimmeMotifs, a pipeline that incorporates an ensemble of computational tools to predict motifs de novo from ChIP-sequencing (ChIP-seq) data. Similar redundant motifs are compared using the weighted information content (WIC) similarity score and clustered using an iterative procedure. A comprehensive output report is generated with several different evaluation metrics to compare and evaluate the results. Benchmarks show that the method performs well on human and mouse ChIP-seq datasets. GimmeMotifs consists of a suite of command-line scripts that can be easily implemented in a ChIP-seq analysis pipeline.</Abstract>
    <Body>1 INTRODUCTION The spectacular development of sequencing technology has enabled rapid, cost-efficient profiling of DNA binding proteins. Chromatin immunoprecipitation followed by high-throughput deep sequencing (ChIP-seq) delivers high-resolution binding profiles of transcription factors (TFs) ( Park, 2009 ). The elucidation of the binding characteristics of these TFs is one of the obvious follow-up questions. However, the  de novo  identification of DNA sequence motifs remains a challenging computational task. Although many methods have been developed with varying degrees of success, no single method consistently performs well on real biological eukaryotic data ( Tompa  et al. , 2005 ). The combination of different algorithmic approaches, each with its own strengths and weaknesses, has been shown to improve prediction accuracy and sensitivity over single methods ( Hu  et al. , 2005 ). Here, we report on GimmeMotifs, a motif prediction pipeline using a ensemble of existing computational tools (Supplementary Fig. S1). This pipeline has been specifically developed to predict TF motifs from ChIP-seq data. It uses the wealth of sequences (binding peaks) usually resulting from ChIP-seq experiments to both predict motifs  de novo , as well as validate these motifs in an independent fraction of the dataset. GimmeMotifs incorporates the weighted information content (WIC) similarity metric in an iterative clustering procedure to cluster similar motifs and reduce the redundancy which is the result of combining the output of different tools (see Supplementary Material). It produces an extensive graphical report with several evaluation metrics to enable interpretion of the results ( Fig. 1 ).
 Fig. 1. An example of the GimmeMotifs output for p63 ( Kouwenhoven  et al. , 2010 ). Shown are the sequence logo of the predicted motif ( Schneider and Stephens, 1990 ), the best matching motif in the JASPAR database ( Sandelin  et al. , 2004 ), the ROC curve, the positional preference plot and several statistics to evaluate the motif performance. See the Supplementary Material for a complete example. 2 METHODS 2.1 Overview The input for GimmeMotifs is a file in BED format containing genomic coordinates, e.g. peaks from a ChIP-seq experiment or a FASTA file. This dataset is split: a prediction set contains randomly selected sequences from the input dataset (20% of the sequences by default) and is used for motif prediction with several different computational tools. Predicted motifs are filtered for significance using all remaining sequences (the validation set), clustered using the WIC score as described below, and a list of non-redundant motifs is generated. 2.2 Motif similarity and clustering The WIC similarity score is based on the information content (IC) and is defined for position  i  in motif  X  compared with position  j  of motif  Y  as:
 (1) 
where  c  is 2.5, and DIC( X i ,  Y j ) is the differential IC defined in Equation ( 3 ). The IC of a specific motif position is defined as:
 (2) 
where IC( X i ) is the IC of position  i  of motif  X ,  f x i , n  is the frequency of nucleotide  n  at position  i  and  f bg  is the background frequency (0.25). The differential IC (DIC) of position  i  in motif  X  and position  j  in motif  Y  is defined as:
 (3) The WIC score of all individual positions in the alignment is summed to determine the total WIC score of two aligned motifs. To calculate the maximum WIC score of two motifs, all possible scores of all alignments are calculated, and the maximum scoring alignment is kept. Similar motifs are clustered using an iterative pair-wise clustering procedure (Supplementary Material). 2.3 Evaluation The motifs can be evaluated using several different statistics: the absolute enrichment, the hypergeometric  P -value, a receiver operator characteristic (ROC) graph, the ROC area under the curve (AUC) and the mean normalized conditional probability (MNCP) ( Clarke and Granek, 2003 ). In addition to these evaluation metrics, GimmeMotifs generates a histogram of the motif position relative to the peak summit, the positional preference plot. Especially in case of high-resolution ChIP-seq data, this gives valuable information on the motif location. 2.4 Implementation The GimmeMotifs package is implemented in Python, while the similarity metrics are written as a C extension module for performance reasons. It is freely available under the MIT license. Sequence logos are generated using WebLogo ( Schneider and Stephens, 1990 ). 3 BENCHMARK RESULTS We performed a benchmark study of GimmeMotifs on 18 TF ChIP-seq datasets. The ROC AUC and MNCP of the best performing motif were calculated and compared with the best motif of two other ensemble methods: SCOPE ( Carlson  et al. , 2007 ) and W-ChipMotifs ( Jin  et al. , 2009 ) (Supplementary Tables S1 and S2) . The results show that GimmeMotifs consistently produces accurate results (median ROC AUC 0.830). The method also significantly improves on the results of SCOPE (ROC AUC 0.613). The recently developed W-ChIPmotifs shows comparable results to GimmeMotifs (ROC AUC 0.824), although this tool does not cluster similar redundant motifs. In addition, the focus of GimmeMotifs is different. While the web interface of W-ChipMotifs is very useful for casual use, the command-line tools of GimmeMotifs can be integrated in more sophisticated analysis pipelines. 4 CONCLUSION We present GimmeMotifs, a  de novo  motif prediction pipeline ideally suited to predict transcription factor binding motifs from ChIP-seq datasets. GimmeMotifs clusters the results of several different tools and produces a comprehensive report to evaluate the predicted motifs. We show that GimmeMotifs performs well on biologically relevant datasets of different TFs and compares favorably to other methods. 
          Supplementary Material 
          
             
                Supplementary Data 
             
             
             
             
             
             
             
          
       </Body>
  </Article>
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Variable locus length in the human genome leads to ascertainment bias in functional inference for non-coding elements</Title>
    <Doi>10.1093/bioinformatics/btp043</Doi>
    <Authors>Taher Leila, Ovcharenko Ivan</Authors>
    <Abstract>
          Motivation: Several functional gene annotation databases have been developed in the recent years, and are widely used to infer the biological function of gene sets, by scrutinizing the attributes that appear over- and underrepresented. However, this strategy is not directly applicable to the study of non-coding DNA, as the non-coding sequence span varies greatly among different gene loci in the human genome and longer loci have a higher likelihood of being selected purely by chance. Therefore, conclusions involving the function of non-coding elements that are drawn based on the annotation of neighboring genes are often biased. We assessed the systematic bias in several particular Gene Ontology (GO) categories using the standard hypergeometric test, by randomly sampling non-coding elements from the human genome and inferring their function based on the functional annotation of the closest genes. While no category is expected to occur significantly over- or underrepresented for a random selection of elements, categories such as ‘cell adhesion’, ‘nervous system development’ and ‘transcription factor activities’ appeared to be systematically overrepresented, while others such as ‘olfactory receptor activity’—underrepresented.
        </Abstract>
    <Body>
       1 INTRODUCTION 
       Almost 20 vertebrate genomes have been fully sequenced up to date. Gene annotation of the human, mouse and several other genomes reaches high confidence levels (Pruitt  et al. ,  2007 ), and functional classification databases provide valuable information to understand the biological processes associated with different groups of genes in these genomes. Gene Ontology (GO) (Ashburner  et al. ,  2000 ), KEGG (Kanehisa  et al. ,  2006 ,  2008 ), OMIM (Boyadjiev and Jabs,  2000 ; Hamosh  et al. ,  2002 ,  2005 ) and OBO Cell Ontology (Smith  et al. ,  2007 ), are just some of the most widely used functional annotation databases, which have enabled intriguing discoveries during the last decade (Hvidsten  et al. ,  2001 ; King  et al. ,  2003 ). 
       The classical approach to functional inference identifies annotation terms that are significantly over- or underrepresented within a given class of genes; over- or underrepresentations are identified by comparing the count of occurrences for each annotation term to the expected value, which usually arises from considering the number of genes assigned to each category in the complete genome. Several tools have been developed to perform the classification analysis that are mainly based on the hypergeometric test [among them BiNGO (Maere  et al. ,  2005 ), GO::TermFinder (Boyle EI,  2004 ) and GOToolBox (Martin,  2004 )] or Fisher's exact test, which relies on properties of the hypergeometric distribution [like GOstat (Beissbarth and Speed,  2004 ) and FatiGO (Al-Shahrour  et al. ,  2004 ,  2007 )]. 
       However, the vast majority of the genome consists of non-protein-coding (non-coding) sequences. Functional non-coding sequences may be associated with protein-coding sequences by either directly or indirectly regulating the expression of protein-coding genes, or playing structural roles in chromosome architecture or encoding RNA genes. In any case, annotation databases for non-coding elements are still in their infancy. In particular, there are at least two databases that store and openly share functional annotation of candidate gene regulatory elements in vertebrates, tested  in vivo  in mice and zebrafish—Vista Enhancer Database (VED) (Pennacchio  et al. ,  2006 ) and CONDOR (Woolfe  et al. ,  2007 ). However, ∼1000 elements profiled in these databases represent only a small fraction of gene regulatory elements in a vertebrate genome, which are expected to exceed the number of exons (∼200 000) (Waterston  et al. ,  2002 ). In practice, this precludes the application of these databases to the functional annotation of non-coding elements, which could be represented by a set of non-coding SNPs (Schwarz  et al. ,  2008 ), a set of transcription factor binding sites from ChIP-chip experiments (Hu  et al. ,  2007 ), or a set of candidate regulatory elements scattered across a vertebrate genome (Ovcharenko  et al. ,  2005 ; Woolfe  et al. ,  2005 ), for example. A sensible solution to this problem proposes to infer the function of a given non-coding element from that of the gene it belongs to or the closest neighboring gene; this strategy is especially well justified for promoter or UTR elements. However, the interpretation of the results is not always straightforward—promoter elements only represent a small component of the complex gene regulatory machinery, also constituted by distant intergenic and intronic elements (Machon  et al. ,  2002 ; Nobrega  et al. ,  2003 ), which do not necessarily regulate the gene they are inserted in or close to (Lettice  et al. ,  2003 ; Santagati  et al. ,  2003 ). 
       Uncertain association of putative regulatory elements and genes aside, gene annotation databases can be useful to characterize non-coding elements. While the association with the gene is often straightforward for promoters and UTR elements, intronic elements are commonly associated with the gene containing them and intergenic elements are usually associated with the nearest gene. After that, it is reasonable to infer the function of non-coding elements by examining the function of the corresponding set of genes. In a classical approach, the number of genes assigned to a given functional category in this set is compared with the number of genes assigned to that category in the entire genome, and deviations are evaluated according to a statistical test. The problem with this logic is the implicit assumption that the probability of sampling a particular annotation term is equal to the fraction of genes associated with it in the genome, which does not depend on the total number of non-coding elements a particular gene is associated with. Basically, a gene with many non-coding elements and a gene with zero non-coding elements are assumed to have equal probability of discovery through the analysis of their non-coding DNA space, which is obviously wrong and leads to a GO ascertainment bias. As a result, non-coding elements will be predicted in some loci of the genome more often than in others purely by chance, and any random subset of non-coding DNA may appear significantly enriched or depleted for some annotation terms, i.e. the above-mentioned strategy for an indirect functional analysis is biased due to the variable locus length. To correct for the GO ascertainment bias, within the context of functional inference on non-coding elements, the probability of a given annotation term should be set proportional to the fraction of the length of non-coding DNA assigned to it, which is strongly correlated with the length of the locus that contains it, and is highly variable across different loci ( Supplementary Fig. 1 ).  
           Fig. 1. 
           
             Distribution of GO categories with respect to the locus length. Left and right tables list the GO categories particularly associated with short and long loci, respectively. 
           
           
         
       
       The aim of this work is to evaluate the effect of the variability in locus length on the functional analysis of non-coding DNA. We consider the total population of non-coding DNA elements in the human genome and the annotation terms attributed to their neighbor genes, and assess whether a set of non-coding DNA elements randomly sampled from the genome will appear artificially enriched and/or depleted for any annotation term. In our study, we report systematic false positive associations for a particular set of GO categories; the choice of the GO database is just exemplary. Finally, we propose a statistical method and a set of correction coefficients to perform an unbiased functional analysis for a set of non-coding elements in a genome. 
     
       2 METHODS 
       
         2.1 GO assignment 
         We performed the functional classification of non-coding elements based on the GO gene annotation database. Each non-coding DNA element can be associated with a set of GO categories that corresponds to either the gene containing that element or the closest flanking gene, in case of intergenic or promoter elements. Therefore, a locus will consist of a gene together with half of its adjacent intergenic regions; a delimitation closer to the real transcription units would be desirable, but impracticable, while the boundaries proposed here eliminate any ambiguity in the gene assignment of the non-coding elements. 
         Also, each gene usually has several associated GO categories. Furthermore, the structure of the GO database is hierarchical, so that each GO category is connected to other categories, which may be associated with other genes. The version of the GO database that we employed contained 6592 terms, each assigned to an average of 17 genes. Three quarters of the GO categories are ascribed to at most five genes, while the average gene count for the remaining quarter is 64. Only 18 GO categories are attributed to 1000 or more genes; from these, five describe some molecular ‘binding’, and seven refer to cellular components. We downloaded the RefSeq gene annotation of the human genome (NCBI Build 36.1; hg18) from the UCSC Genome Browser (Karolchik  et al. ,  2003 ), and identified 17 475 discrete gene loci, with an average locus length of 152 057 bp (the shortest locus was 612 bp, while the longest locus was 4 767 747 bp). The average locus length for a GO category was 159 918 bp, ranging from 1979 bp to 3 204 335 bp; the average locus length of 25% GO categories was longer than 194 230 bp. 
       
       
         2.2 Sampling 
         For the purpose of this study, we define a non-coding DNA element as a non-repetitive non-coding DNA sequence stretch within a gene locus, much shorter than the complete locus length. This allowed direct sampling of genes from the genome with a probability being a function of the non-coding non-repetitive length of the gene locus. 
         The population of non-coding DNA elements in the genome is finite, and its probability distribution is discrete. The probability of a given non-coding DNA element is given by its length divided by the total non-coding DNA in the genome ( L 
           nc 
           HG =1 359 884 776 nucleotides). We took 1000 random samples for each sample size ( n  ranging from 100 to 200 000), using the algorithm described in  Supplementary Figure 5 . We computed the frequency of the GO categories corresponding to each different gene associated with the non-coding elements in each of the 1000 samples.
         
       
       
         2.3 GO enrichment/depletion 
         The usual statistical test for functional enrichment compares the count of GO category associations for a given set of genes to the expected number, which is derived from the count of GO category associations in the complete genome. For each GO category, the test evaluates the probability of observing a number of genes associated with a particular GO category, by comparing it with the total number of genes in the genome that are assigned to that category. This analysis assumes that all genes are equally likely, and the probability of attributing a given function or GO category to a gene only depends on the total number of genes carrying that GO category. 
         Under such hypotheses, the probability of associating a certain non-coding DNA element to a given GO category can be regarded as  
             (1) 
             
           
         
         where  N 
           GO  is the number of genes/loci associated with a given GO category in the set of  N 
           HG  genes analyzed. Enrichment in a certain GO category can be quantified by computing the probability that the number of non-coding DNA elements in a sample of size  n  that are associated with a GO category,  N 
           GO , is larger than or equal to the observed value  m  assuming the frequency  P 
           GO  in all genes. This follows a hypergeometric distribution which approximates the binomial distribution when  n / N 
           HG  is small  n / N 
           HG &lt;20.
         
         For each of the 1000 random samples, we identified the set of different genes associated with non-coding elements, and subsequently counted the frequency of the GO categories associated with these genes. The frequencies of the GO categories were compared with the genomic frequencies. This procedure allows distinguishing enrichments or depletions of specific GO categories in the sample. The probability of obtaining  k  non-coding DNA elements for a given GO category among a sample of size  n  by chance, knowing that the reference dataset contains  N 
           GO  such annotated genes/loci out of  N 
           HG =17 475 genes/loci, can be calculated using the hypergeometric distribution. The significance of the enrichment in each of the GO categories was evaluated by summing over the upper tail of the hypergeometric distribution (α=0.05) and applying Bonferroni's multiple test correction. For the later, we multiplied the nominal  P -values calculated as described above by the number of tests performed, i.e. the total number of GO categories.
         
       
     
       3 RESULTS 
       
         3.1 Variable locus length of GO categories 
         In the present study, we utilized RefSeq gene annotation to define the genomic location of genes and their corresponding loci. First, we utilized all available transcripts to identify 17 475 non-overlapping genes. Next, we split the genome into a set of loci by dividing intergenic intervals in half. We also tested an alternative locus definition, in which the locus boundaries were determined by proximity to the transcription start site, and did not observe an impact on our conclusions (see next section for details). 
         In assessing the variation in the average locus length of different GO categories belonging to the three hierarchies ‘biological process’, ‘molecular process’ and ‘cellular component’, we observed a wide distribution of average locus lengths centered at around 100 kb ( Fig. 1 ). A substantial fraction of GO categories was found to be assigned to genes with either unusually short or unusually long loci. Interestingly, we noted a particular bias towards specific GO categories, and therefore, biological functions, in short and long loci. Concretely, several GO categories related to metabolic processes, as well as some involved in specific responses, are particularly overrepresented in short loci (&lt;80 kb), while GO categories corresponding to development, morphogenesis, regulation and signaling are significantly overrepresented in loci longer than 120 kb ( Table 1 ). Although the substrate of this study is the GO database, other functional annotation databases will most probably present a conceptually similar bias in locus length, as this bias has a biological origin, namely the heterogeneity in the locus length.  
             Table 1. 
             
               GO categories significantly associated with genes in shorter loci and in longer loci 
             
             
               
                 
                   Process/function 
                   Locus length (kb) 
                   
                     P -value
                   
                 
               
               
                 
                   Genes in shorter loci 
                   
                   
                 
                 
                   Response 
                   
                   
                 
                 
                       To unfolded protein 
                   28.7 
                   2.4e-5 
                 
                 
                       To bacterium, defense 
                   58.5 
                   1.3e-5 
                 
                 
                       To biotic stimulus 
                   58.8 
                   1.8e-12 
                 
                 
                   Oxidative phosphorylation 
                   32.6 
                   6.1e-9 
                 
                 
                   Oxidoreductase activity 
                   38.3 
                   1.2e-5 
                 
                 
                   Electron transport 
                   
                   
                 
                 
                       Mitochondrial 
                   34.3 
                   1.1e-5 
                 
                 
                       ATP synthesis coupled 
                   36.1 
                   1.4e-6 
                 
                 
                   Ribosome 
                   
                   
                 
                 
                       Structural constituent 
                   36.8 
                   1.7e-8 
                 
                 
                       Biogenesis and assembly 
                   44.9 
                   1.8e-7 
                 
                 
                   Keratinization 
                   38.2 
                   1.2e-6 
                 
                 
                   Epidermal cell differentiation 
                   43.3 
                   1.2e-5 
                 
                 
                   rRNA 
                   
                   
                 
                 
                       Processing 
                   50.2 
                   9.1e-7 
                 
                 
                       Metabolic process 
                   51.3 
                   8.4e-7 
                 
                 
                   Genes with longer loci 
                   
                   
                 
                 
                   Morphogenesis 
                   
                   
                 
                 
                       Embryonic limb 
                   525.2 
                   6.8e-7 
                 
                 
                       Neurite 
                   185.2 
                   1.4e-7 
                 
                 
                   Development 
                   
                   
                 
                 
                       Limb 
                   483.1 
                   6.0e-8 
                 
                 
                       Lung 
                   283.2 
                   7.3e-6 
                 
                 
                       Respiratory tube 
                   277.0 
                   4.4e-6 
                 
                 
                       Brain 
                   228.2 
                   1.3e-7 
                 
                 
                       Central nervous system 
                   228.1 
                   1.3e-11 
                 
                 
                       Tube 
                   202.0 
                   4.4e-9 
                 
                 
                   Regulation of 
                   
                   
                 
                 
                       Developmental process, positive 
                   325.3 
                   2.1e-5 
                 
                 
                       Cell differentiation, negative 
                   316.4 
                   2.5e-5 
                 
                 
                       Transcription, positive 
                   183.4 
                   1.8e-6 
                 
                 
                   Axon guidance 
                   320.5 
                   2.5e-5 
                 
                 
                   Signaling 
                   
                   
                 
                 
                       Cyclic-nucleotide-mediated 
                   214.7 
                   2.7e-6 
                 
                 
                       G-protein 
                   214.7 
                   1.3e-6 
                 
               
             
           
         
       
       
         3.2 Ascertainment bias impact 
         The effect of the ascertainment bias caused by the locus length non-uniformity in GO categories will vary depending on the number of genes each GO category is assigned to and the number of non-coding elements used in a study. A GO category associated with very few genes is less likely to result in an incorrect prediction than a GO category associated with many genes, simply because a GO category with few genes is less likely to be detected at all. A small set of non-coding elements is also less likely to produce false positive associations, as it is less likely to produce any associations at all. 
         To explore the need of accounting for such ascertainment bias, we randomly selected sets of non-coding elements in the human genome, associated them with their closest genes, and performed a classical GO analysis on the indirectly selected sets of genes. (It should be noted that although this study concentrates on the GO database, the conclusions can be generalized to any other system of functional classification.) We also excluded repetitive elements from the analysis, as functional non-coding elements are expected to be mainly non-repetitive. We will refer to the process of sampling  n  non-coding DNA elements from the human genome as an experiment. We performed 1000 independent experiments for each sample size  n , which ranged from 100 to 200 000, and evaluated enrichment and depletion for different GO categories. We adjusted the significance level by applying the strict Bonferroni's multiple-testing correction (Bonferroni,  1935 ). Unexpectedly often, aleatory sets of non-coding DNA elements were found to be significantly associated with multiple GO categories ( Fig. 2  shows the number of GO categories that appeared to be significant in at least 5% of the experiments for different sample sizes). The number of significantly overrepresented GO categories reached the maximum of 22 for sample size 20 000, and decreased to five as the sample size increased to 200 000. The number of underrepresented categories rapidly plateaued at 10 GO categories in the range of sample sizes plotted. By sampling non-coding elements we indirectly select genes, but the occurrence of each gene is considered only once. For that reason, 20 000 non-coding samples result in ∼43% of the total number of genes in the human genome. When the sample size is large enough so that every gene is effectively represented, the sample coincides with the population. In this case, the number of occurrences for each category meets its expected value. In other words, the number of occurrences of a given GO category converges to the expected value as more genes become represented in the sample, and this accounts for the variation in the number of artificially over- or underrepresented GO categories with the sample size.  
             Fig. 2. 
             
               The average number of GO categories that show up as significantly over- or underrepresented in experiments with random sets of non-coding elements for different sample sizes. 
             
             
           
         
         To test whether this effect is a simple consequence of our locus definition, in which intergenic space is split in half, we repeated this experiment using an alternative locus definition, in which a non-coding element is associated with the gene that has the most proximal transcription start site to the element. We found that the alternative locus definition has no impact on the observed effect ( Supplementary Fig. 2 ). 
         In summary, we found that up to 31 GO categories were significantly over- or underrepresented, depending on the sample size. Specifically, within the usual sample size ranges, over 10 GO categories were overrepresented with a striking confidence level. Considering that each experiment consisted of randomly sampled non-coding DNA elements, and that the experiment was independently repeated a large number of times, this result is not expected. However, the outcome can be easily explained by considering that the non-coding DNA elements do not all have the same probability of being assigned to a gene, but instead have a probability that depends on the locus length. 
       
       
         3.3 Systematically biased GO category assignments 
         The fact that random sets of non-coding elements appear to be significantly enriched in certain GO categories is alarming. Nevertheless, an even more worrisome question is whether any of such associations between random sets of non-coding elements and GO categories occurs systematically, as this would suggest that some particular GO categories are likely to be reported as significant on a regular basis. For that purpose, for a given sample size  n , we analyzed GO categories that were reported as significantly over- or underrepresented in at least 25% of the experiments ( Fig. 3 ). We observed a systematic significant association for a total of 13 GO categories (nine overrepresentations and four underrepresentations). It is interesting to note that the majority of the overrepresented GO categories relate to basic cellular processes (cell adhesion, binding, transcription factors and development), while underrepresented GO categories correspond to lineage-specific and adaptive features (response and receptor categories). Not surprisingly, these constitute a subset of the GO categories for which we observe a large deviation from the uniform distribution in relation to the locus length ( Table 2  summarizes the ratio between the average locus length for the loci associated with each particular GO category  
             
            represented in  Figure 3  and the average locus length in the human genome  
             
           .  
             Fig. 3. 
             
               Significantly over- and/or underrepresented GO categories (showing only categories which are significant in at least 25% of the experiments). The  x -axis represents different sample sizes, only within a range in which the number of GO categories over- and/or underrepresented shows high variation. 
             
             
           
           
             Table 2. 
             
               Significantly over/underrepresented GO categories (showing only categories which are significant in at least 25% of the experiments) 
             
             
               
                 
                   GO id 
                   Description 
                   
                     
                       
                     
                   
                 
               
               
                 
                   Overrepresentation 
                   
                   
                 
                 
                   GO:0007156 
                   Homophilic cell adhesion 
                   4.7 
                 
                 
                   GO:0007155 
                   Cell adhesion 
                   2.3 
                 
                 
                   GO:0007399 
                   Nervous system development 
                   1.9 
                 
                 
                   GO:0005509 
                   Calcium ion binding 
                   1.7 
                 
                 
                   GO:0007242 
                   Intracellular signaling cascade 
                   1.4 
                 
                 
                   GO:0043565 
                   Sequence-specific DNA binding 
                   1.4 
                 
                 
                   GO:0007275 
                   Multicellular organismal development 
                   1.3 
                 
                 
                   GO:0006468 
                   Protein amino acid phosphorylation 
                   1.3 
                 
                 
                   GO:0003700 
                   Transcription factor activity 
                   1.2 
                 
                 
                   Underrepresentation 
                   
                   
                 
                 
                   GO:0007186 
                   G-protein coupled receptor protein signaling pathway 
                   0.7 
                 
                 
                   GO:0050896 
                   Response to stimulus 
                   0.6 
                 
                 
                   GO:0007608 
                   Sensory perception of smell 
                   0.4 
                 
                 
                   GO:0004984 
                   Olfactory receptor activity 
                   0.3 
                 
               
             
             
               
                 Overrepresented GO categories appear to have ratios &gt;1, while underrepresented GO categories consist of shorter loci, on average. 
               
             
           
         
         For example, the category ‘homophilic cell adhesion’ appeared to be consistently overrepresented in random sets of 500 and more non-coding elements. More precisely, sets of 500 non-coding elements were significantly associated with this category in 25% of independent experiments, while sets of 2500 non-coding elements were significantly associated with this category in more than 85% of experiments. Interestingly, 55 of the 94 genes associated with homophilic cell adhesion are cadherins. Cadherins ( Supplementary Table 2 ) are a superfamily of adhesion molecules with function in cell recognition, tissue morphogenesis and tumor suppression (Angst  et al. ,  2001 ). Cadherin genes are often flanked either on one or on both sides by a so-called gene desert [an extremely long intergenic region (Ovcharenko  et al. ,  2005 )], and this genome architecture is well conserved in mammals and birds (Angst  et al. ,  2001 ; Wu and Maniatis,  2000 ; Wu  et al. ,  2001 ). The characteristic long locus length of these cadherins contributes to the association bias of the homophilic cell adhesion category, which appears as one the top candidates for the systematic false positive annotation of non-coding elements. 
         In summary, these results indicate that the effect of the locus length heterogeneity and the unevenness of the GO category distribution with regards to it are not negligible and should be appropriately accounted for in functional inference of non-coding elements. The consequence of observing artificially overrepresented categories is conceptually different from that of detecting underrepresented categories. Given a non-coding element, in the former case the results might suggest a function that it does not actually fulfill (false positive), while in the latter case, evidence for a certain function might be simply omitted (false negative). 
       
       
         3.4 Locus length correction 
         We have shown that the distortion in the distribution of the GO categories in relation to the locus length may lead to erroneous conclusions in the context of the functional annotation of non-coding elements. However, such bias can be excluded by simply introducing probability correction coefficients that depend on the average locus length of each GO category. To account for the heterogeneous locus length in the human genome, we suggest considering the length of the non-coding DNA associated with each GO category, as described below. 
         If we randomly sample non-coding elements from the human genome, the probability of observing a certain GO category is  
             (2) 
             
            where  L 
           nc 
           GO  is the total length of the non-coding DNA in the loci a given GO category, and  L 
           nc 
           HG  is the total length of the non-coding DNA in the human genome ( Supplementary Fig. 3 ).
         
         The probability of observing a certain GO category assuming that all genes in the human genome occur randomly with the same frequency is  
             (3) 
             
            where  N 
           GO  is the number of genes/loci associated with a given GO category and  N 
           HG  genes is the number of genes/loci in the human genome.
         
         Then, we define a correction coefficient CC GO  for each GO category ( Supplementary Table 1 ), such that  
             (4) 
             
            and  
             (5) 
             
            The selection of  n  GO categories at random from the entire genome can be modeled as a binomial distribution where the success of an event is defined as selecting a certain GO category with a probability that depends on the length of the non-coding DNA in the loci that GO category is associated with.
         
         If we observe  m  instances of a GO category, we can calculate its  P -value under a random model, as 1 minus the cumulative binomial probability of selecting that particular GO category with a frequency  m −1, which is calculated as  
             (6) 
             
            In order to correct for multiple testing, we must multiply that probability by the number B of hypothesis we test for (Bonferroni's multiple-comparison correction). The expected frequency of a GO category is  
             
           
         
         We propose to use the ratio of observed to expected frequencies as a rough indicator of enrichment; a ratio above one indicates that the GO category is enriched in the sample with respect to its average expectation, while a ratio below one indicates a depleted GO category. However, it must be noted that this ratio will overestimate GO categories with few expected occurrences. 
       
       
         3.5 Validation 
         In addition to the aforementioned experiments, we discarded artifacts caused by the sampling method, correlation between GO categories or the threshold chosen for establishing the significance by repeating 1000 sampling experiments from the finite population of non-coding DNA elements at random and testing each GO category for enrichment/depletion using a binomial distribution. As expected, when we computed the  P -values using a binomial distribution with parameters  n  (sample size) and  
             
           , where  
             
            is the probability of observing the total length of non-coding DNA indirectly assigned to a particular GO, we could not detect any particular GO category significant in 5% or more of the experiments. However, when we repeated the calculations using a binomial distribution with parameters  n  (sample size) and  P 
           GO , where  P 
           GO  is the probability of observing all genes in the genome that are assigned to a particular GO, we obtained a list of over- and underrepresented categories very similar to that produced with the hypergeometric distribution.
         
         Finally, we would like to mention that the inclusion of repetitive elements in the analysis does not alter the results, as their locus span is strongly correlated with the locus length (data not shown). Also, to confirm that the observed effect is not associated with either repeat-rich or repeat-poor regions we analyzed the relation between the number of non-coding non-repetitive elements in a locus and repeat density. We found that loci with the excessive number of non-coding non-repetitive elements that contribute to an enrichment of artificial GO associations have average repeat density and are not biased towards either repeat-rich or repeat-poor regions ( Supplementary Fig. 4 ). 
       
     
       4 DISCUSSION 
       GO databases provide a variety of tools for the functional analysis of genes. Due to the current lack of exhaustive databases describing functional non-coding DNA elements, it has become a usual practice to indirectly infer the biological role of selected non-coding elements from the functional analysis of their flanking genes. As we have shown, the high heterogeneity locus length in the human genome and the uneven distribution of the GO categories in relation to the locus length can bias functional inference. Therefore, the  P -values for the GO categories that clearly deviate from the assumptions made by the hypergeometric test should be computed considering that the probability of a given GO category does not only depend on the number of genes assigned to it, but also on the length of their loci. Otherwise, categories that are particularly associated with very long or very short loci might appear artificially over- or underrepresented, respectively. As an approximate solution to the problem caused by the variability in the locus length, we propose the use of correction coefficients, which take into consideration the genome span of non-coding DNA corresponding to different GO categories. The coefficients in  Supplementary Table 1  can be easily recomputed for other genomes and other annotation databases according to the procedure described in  Section 2 . 
       An increasing number of studies report that conserved non-coding sequences tend to cluster in the vicinity of genes implicated in development and transcriptional regulation (termed trans-dev genes) (see for example, Bejerano  et al. ,  2004 ; Dermitzakis  et al. ,  2005 ; McEwen  et al. ,  2006 ; Ovcharenko,  2008 ; Sandelin  et al. ,  2004 ; Woolfe  et al. ,  2005 ). We observe the association of similar GO categories with random sets of non-coding DNA, suggesting that the heterogeneity of the locus length might have had an adverse effect on previous reports. In a reanalysis of studies describing ultraconserved elements (Bejerano  et al. ,  2004 ) and non-coding elements conserved between human and fish (Ovcharenko  et al. ,  2004 ; Woolfe  et al. ,  2005 ), we found that the originally reported association with transcriptional regulation and transcription factors can be strongly confirmed even after the application of the correction for the GO ascertainment bias, while the  P -values for associations related to the nervous system and multicellular organismal development fall below the level of statistical significance ( Tables 3  and  4 ). However, it is important to note that our results do not necessarily object the validity of previously published conclusions—if the extreme length of some loci is the result of evolutionary selection and not simply of the locus length variability, the proposed non-coding length correction might artificially reduce the significance of biologically important associations. Obviously, without the availability of extensive annotation databases for non-coding elements, it might be quite difficult to establish a bulletproof approach for using gene annotation databases for an indirect annotation of non-coding elements, but it is also unwise to ignore the potential impact of the locus length on the inference of the function for non-coding elements. Therefore, until we have a large-scale sampling of non-coding functional elements in the human genome that we can use to infer function of other non-coding elements, a practical solution might consist of utilizing the classical GO analysis approach, applying the proposed correction, and analyzing differences and commonalities in the results.  
           Table 3. 
           
             Overrepresented GO categories computed using the usual hypergeometric test (panel A) and accounting for variable locus length (panel B) on the datasets described by Ovcharenko  et al.  ( 2004 ) and Woolfe  et al.  ( 2005 ) 
           
           
             
               
                 
                   
                 
               
             
           
           
             
               Categories removed by the GO ascertainment correction are highlighted, as well as additional categories found after applying the correction. 
             
           
         
         
           Table 4. 
           
             Overrepresented GO categories computed using the usual hypergeometric test (panel A) and accounting for variable locus length (panel B) on the datasets described by Bejerano  et al.  ( 2004 ) 
           
           
             
               
                 
                   
                 
               
             
           
           
             
               Categories removed by the GO ascertainment correction are highlighted, as well as additional categories found after applying the correction. 
             
           
         
       
     
       Supplementary Material 
       
         
           [Supplementary Data] 
         
         
         
       
     </Body>
  </Article>
</Articles>
