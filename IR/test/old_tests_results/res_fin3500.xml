<Articles>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>SmashCell: a software framework for the analysis of single-cell amplified genome sequences</Title>
    <Doi>10.1093/bioinformatics/btq564</Doi>
    <Authors>Harrington Eoghan D., Arumugam Manimozhiyan, Raes Jeroen, Bork Peer, Relman David A.</Authors>
    <Abstract>Summary: Recent advances in single-cell manipulation technology, whole genome amplification and high-throughput sequencing have now made it possible to sequence the genome of an individual cell. The bioinformatic analysis of these genomes, however, is far more complicated than the analysis of those generated using traditional, culture-based methods. In order to simplify this analysis, we have developed SmashCell (Simple Metagenomics Analysis SHell-for sequences from single Cells). It is designed to automate the main steps in microbial genome analysis—assembly, gene prediction, functional annotation—in a way that allows parameter and algorithm exploration at each step in the process. It also manages the data created by these analyses and provides visualization methods for rapid analysis of the results.</Abstract>
    <Body>1 INTRODUCTION The rapid evolution of DNA sequencing platforms has had a dramatic, beneficial impact on the study of microbial ecology and population genetics. So far, these benefits have mostly come from shotgun community metagenomics that provides a high-level overview of the taxonomic and functional composition of microbial communities [see  Arumugam  et al.  (2010 ) for details]. However, this approach is limited in its ability to yield complete genome sequences as well as the fine-scale genetic variation that defines population substructures within these communities. One possible solution uses a combination of single-cell manipulation technologies, multiple-displacement amplification (MDA) and high-throughput sequencing to generate single-cell amplified genomes (SAGs). This approach has already been used to characterize the genomes of uncultivated microbes ( Marcy  et al. , 2007 ;  Woyke  et al. , 2009 ) and as the throughput of the associated technologies increase it should become possible to obtain high-resolution profiles of populations or communities. However, it is more difficult to produce a high-quality assembly and functional annotation from a SAG than from the output of traditional methods due to limitations inherent in sample preparation and sequencing (detailed below). To overcome this requires an iterative, exploratory approach that transforms the traditional linear process of genome assembly, gene prediction and functional annotation into a tree-like structure, each branch defined by a different choice of algorithm or parameters, one of which will be chosen as the final version ( Fig. 1 A). This approach is not easily achieved using existing tools, which take an assembled genome as their input and do not allow parameterization of subsequent steps (for a comparison with existing tools see  Supplementary Table ). In order to automate the process and deal with the resulting combinatorial increase in data we have created SmashCell. While developed for use on SAGs many of its analyses are equally applicable to traditional microbial genome sequences and low-complexity metagenomes.
 Fig. 1. ( A ) The data model used in SmashCell is designed to reduce redundancy and facilitate the comparison of results using different parameters and/or algorithms [MC: metagenome collection, MG: metagenome (equivalent to a SAG), AS: assembly, GP: gene prediction, FUNC: functional annotation]. ( B ) K-mer frequency statistics supplement sequence similarity information to identify potential contaminants. This shows a self-organizing map (SOM) trained on the tetramer frequencies of an assembly. The left panel shows a series of pie charts highlighting the taxonomic identity (determined by best hit in GenBank, those with no hits are uncoloured) of the contigs assigned to each neuron. The right panel shows the U-matrix of the SOM. ( C ) The abundance of single-copy COGs can be used to assess genome completeness, the presence of contamination and the quality of the assembly. ( D ) SmashCell uses different graphs to aid in parameter and algorithm selection. Here the results from two different gene prediction algorithms are presented, along with GC-content, quality scores and read depth. 2 FEATURES SmashCell automates the steps common to most genome analyses—assembly, gene prediction and functional annotation—and addresses some of the challenges posed by SAGs. For instance, it is difficult to isolate a single cell for sequencing without including some environmental DNA, in effect creating a metagenome. As a result, SmashCell includes both sequence similarity and k-mer based tools to identify potential contaminants, the latter being especially useful when the target genome and/or contaminants are not closely related to existing genome sequences (see  Fig. 1 B for details). Another challenge with SAGs is the orders of magnitude variance in MDA product abundance along the genome, which creates several obstacles to obtaining high-quality annotation. First, it hampers genome assembly, as most algorithms are designed for lower and more evenly distributed read depth. Secondly, it vastly increases the amount of sequencing required to obtain a complete genome sequence. To address the first challenge, SmashCell includes scripts to downsample overrepresented regions of the SAG and to address the second, SmashCell uses the STRING database ( Jensen  et al. , 2009 ) to obtain counts of single-copy orthologous groups ( Fig. 1 C), which can then be used to estimate genome completeness. In addition to these SAG-specific features, SmashCell contains genome visualization and other tools ( Fig. 1 D) that are generally applicable to genomic and metagenomic data. SmashCell uses the same basic data model as SmashCommunity [designed for shotgun community sequencing;  Arumugam  et al.  (2010 )]. As a result, several of the analyses available in SmashCell can be run on data generated by SmashCommunity and vice versa. Documentation for these and many more features are available on the SmashCell website. 3 DESIGN AND IMPLEMENTATION SmashCell is a framework written in Python that provides a variety of analysis tools that can be used either from the command line or from within other Python scripts. The main function of SmashCell is to automate the common steps in genome analysis in a way that facilitates parameter and algorithm exploration. Using the data model shown in  Figure 1 A, SmashCell manages the files and data associated with each of these steps, reducing redundancy and providing a layer of abstraction that simplifies access to these data. SmashCell also uses generic databases to provide a common format for assembly and gene prediction information, allowing it to work with a variety of third-party assemblers and gene prediction algorithms. In order to facilitate the exploration of genomic data, SmashCell automatically generates many different types of graphs (e.g. see  Fig. 1 B–D) and provides wrappers for exploratory statistical techniques. 
          Supplementary Material 
          
             
                Supplementary Data 
             
             
             
          
       </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Dynamic deterministic effects propagation networks: learning signalling pathways from longitudinal protein array data</Title>
    <Doi>10.1093/bioinformatics/btq385</Doi>
    <Authors>Bender Christian, Henjes Frauke, Fröhlich Holger, Wiemann Stefan, Korf Ulrike, Beißbarth Tim</Authors>
    <Abstract>Motivation: Network modelling in systems biology has become an important tool to study molecular interactions in cancer research, because understanding the interplay of proteins is necessary for developing novel drugs and therapies. De novo reconstruction of signalling pathways from data allows to unravel interactions between proteins and make qualitative statements on possible aberrations of the cellular regulatory program. We present a new method for reconstructing signalling networks from time course experiments after external perturbation and show an application of the method to data measuring abundance of phosphorylated proteins in a human breast cancer cell line, generated on reverse phase protein arrays.</Abstract>
    <Body>1 INTRODUCTION Studying the molecular biology of cells and tissues has developed from the investigation of few genes or proteins in one experiment to the analysis of the interplay of many components as a system. Various array techniques have been devised for analysing cellular behaviour on DNA, RNA and protein level that make it possible to generate thousands of measurements in a single experiment. These data can be plugged into  de novo  network reconstruction methods in order to infer regulatory interactions between the measured components. For this purpose, several approaches have been developed in the past. Bayesian Networks (BN; Heckerman,  1996 ) have been frequently used to reconstruct gene regulatory networks from RNA expression experiments (Friedman  et al. ,  2000 ; Segal  et al. ,  2005 ) as well as causal protein–protein relationships for intensity data from protein quantification (Sachs  et al. ,  2005 ). The latter is an example where directed perturbations of several measured proteins were performed in order to resolve the structure of the underlying interactions. External interventions can be introduced by multiple means, for example, changing environmental conditions, applying drugs or using gene silencing methods such as RNA interference. Another example for BN usage under perturbation conditions is given in Pe'er  et al.  ( 2001 ). Besides BNs, there are several related approaches to infer networks from perturbation data. Markowetz  et al.  ( 2005 ) derived networks after knocking out specific genes by analysing expression patterns in the discretized gene expression measurements. Fröhlich  et al.  ( 2008 ) extended this approach to perform inference on non-discretized expression levels. Tegner  et al.  ( 2003 ) suggested iterative perturbation of the system in order to reveal the underlying network structure. They modelled perturbations as a linear combination of inputs and inferred weights for the pairwise node to node influences. Nelander  et al.  ( 2008 ) improved this idea by using nonlinear perturbation effects and modelled the interaction behaviour of a number of components after several single and combinatorial perturbations. Time resolved measurements provide insight into the dynamical behaviour of the system and do not restrict modelling to a ‘snapshot’ of the system's state. A suitable approach for network inference from time resolved data are dynamic Bayesian networks (DBN), a family of reconstruction methods including Boolean network models, state-space models or regression models (Akutsu  et al. ,  1999 ; Imoto  et al. ,  2002 ; Lébre,  2009 ; Murphy and Mian,  1999 ; Rau  et al. ,  2010 ). While these methods model the dynamics of the system over time, they do not model perturbation effects directly. However, Geier  et al.  ( 2007 ) studied reverse engineering methods on simulated data for time courses and external perturbations and came to the conclusion that additional perturbation of the system is beneficial. So methods that explicitly include perturbations in the modelling approach for time course analysis are still needed. In addition, most of the current network reconstruction methods are tailored to the analysis of gene regulatory networks based on gene expression data from microarray experiments. Rather few studies deal with the signalling flow between proteins based on the analysis of protein activation and abundance coupled with intervention effects. Fröhlich  et al.  ( 2009 ) developed a network inference method for protein networks after knockdown of the measured components that allows time series measurements, too. But their method treats each time point as independent measurement and does not model the time-dependent behaviour of the system explicitly. However, using only few perturbations and gathering information on the signal flow through longer time series would be desirable, too. In this study, we set up a framework for reconstructing signalling networks from time course measurements after external perturbation (both inhibitory and stimulating).  Figure 1  shows an outline of the proposed workflow. Networks are represented as directed cyclic graphs with distinct edge types for activating and inhibiting interactions. We model signalling dynamics by a Boolean signal propagation mechanism defining state transitions for a given network structure. An optimal state transition series is found in a hidden Markov model (HMM; Durbin  et al. ,  1998 ) and the fit of the data to such a transition matrix is assessed by our proposed likelihood score. Network structure search is performed in a genetic algorithm (GA) optimizing the overall likelihood of a population of candidate networks. Our method shows good performance for reconstructing signalling networks from artificial data and outperforms two current DBN approaches.
 Fig. 1. Overview of the approach: given a network hypothesis ( A ), we generate a set of reachable system states by applying a fixed signal propagation scheme ( B ) which in effect reduces the number of possible system states. An optimal path through these reachable system states over time is identified by an HMM ( C ). Using the series of system states from the HMM, model parameters for two Gaussian distributions for each protein (one for active, one for passive) are estimated ( D ) and a total likelihood of our measurements given the network and model parameters is calculated ( E ). We use this likelihood score in a GA in order to optimize the overall score for an evolving population of candidate networks ( F ) and generate a final network from this population, after we found convergence in the GA. As an application of the algorithm, we used protein phosphorylation measurements for 16 ERBB signalling-related phosphoproteins. The data were generated on reverse phase protein arrays (RPPA; Loebke  et al. ,  2007 ) in the human breast cancer cell line HCC1954, which overexpresses the ERBB2 receptor that is associated with reduced disease-free and overall survival in breast cancer patients (Slamon  et al. ,  1987 ,  1989 ). Note that dynamic deterministic effects propagation networks (DDEPN) was developed for protein phosphorylation data, but in principle is also applicable to other types of high-throughput data, e.g. for RNA microarrays. We stimulated the cells with two ligands [epidermal growth factor (EGF) and heregulin (HRG)], both as separate and combined stimulation experiments. All three experiments were combined to infer a signalling network which was compared with current literature knowledge. DDEPN was able to identify several well-known signalling chains from the MAPK and AKT signalling pathways, some of which originally were found in ERBB2 overexpressing cells. This shows the ability of our method to identify meaningful interactions from experimental proteomics data. 2 SYSTEM AND METHODS 2.1 Modelling the dynamics of the system Let  V  = { v i :  i  ∈ 1,…,  N } be the set of nodes representing proteins and Φ =  V  ×  V  → {0, 1, 2} an adjacency matrix defining a network, where 0 means no edge, 1 activation and 2 inhibition between two nodes. The signal flow through a given network of proteins is represented as a matrix Γ = {γ ik  ∈ {0, 1} :  i  ∈ 1,…,  N ,  k  ∈ 1,…,  M }, which contains a series of possible system states  γ k  = {γ i  :  i  ∈ 1,…,  N , γ i  ∈ {0, 1}}. These are vectors of activation states for each node at a time step  k . Define 0 &lt;  M  ≤ 2 N  as number of reachable system states, determined as soon as a state is repeated during the signal propagation. Each perturbation is seen as an external influence which is included as a node into the network and whose state is constantly active (i.e. 1). Starting at the stimuli nodes, the status of all children is subsequently determined. A child is active if at least one parent connected by an activation edge is active and all parents connected via inhibition edges are inactive in the preceding step. For example, in the matrix Γ shown in  Figure 1 , the state γ B 2  of protein B at Step 2 is determined by γ B 2  = γ S 1  ∧ ¬γ A 1  = 1 ∧ 1 = 1 (where ‘¬’ is the logical negation which is used whenever a parent is connected via an inhibitory edge). A formal description of the signal propagation follows: given a set of nodes  V  and a network Φ. Define  S  ⊆  V  as the set of input stimuli and consider the network Φ as fixed for the propagation. We derive the state matrix Γ that comprises all  M  reachable state vectors  γ k  for the given network. The propagation is stopped at a step  M , if ∃  k  ≤  M , such that  γ k  =  γ M , i.e. if one of the preceding states is found a second time. All stimuli nodes  s  ∈  S  are active in all steps, i.e. γ sk  = 1 ∀ k , and all other nodes are initialized to be 0 in the first step, i.e. γ v i 1  = 0 ∀ v i  ∈  V  ∖  S . Let  pa ( v i ) be the set of all parents of a node  v i  and ϕ wv i  an edge from a node  w  to  v i . For any status  k  and protein  v i , define
 
as the sets of states of parental nodes of  v i  in step  k  − 1, connected by activating edges ( E k −1 + ) and connected by inhibiting edges ( E k −1 − ). An entry γ v i k  in Γ is then determined by:
 (1) This procedure reduces the maximal number of columns in the system state matrix Γ from 2 N  to  M  ≤ 2 N . However, the states in Γ do not necessarily correspond to the actual measured time points in the data. In general, it is expected that a different number of reachable states than time points is found. For example, in the hypothetical case that the system remains in a constant state, only one state would be present Γ. Thus, we have to find a series of system states that is consistent with the measured experimental data and represents expected dynamics under our given network hypothesis, which is described in the next section. 2.2 HMM for searching the optimal sequence of system states Let  t  ∈ 1,…,  T  denote the index for the time point and  r  ∈ 1,…,  R  denote the index for the replicated measurements. Our measured data are recorded in a data matrix  X  = { x itr  :  i  ∈ 1,…,  N ,  t  ∈ 1,…,  T ,  r  ∈ 1,…,  R }. The true sequence of reachable system states is represented in an unknown matrix Γ *  = {γ * itr  :  i  ∈ 1,…,  N ,  t  ∈ 1,…,  T ,  r  ∈ 1,…,  R }. Each entry in Γ *  represents the state of a node  i  at time point  t  and corresponds to each measurement  x itr , where replicate measurements indexed by  r  are assumed to have the same state. We omit the index  r  for notational simplicity for the rest of this section, but the reader should be aware that optimization in the HMM is done by multiplying all replicate emission probabilities for determining the entries in the Viterbi matrix [as shown in Equation ( 4 )]. Intuitively, Γ *  provides a classification of measurements into measurements coming from an active state and those from an inactive state. We infer an estimate   for Γ *  by using an HMM  H  = ( W , Γ,  A ,  e ). Here,  W  represents the range of possible values for observations, i.e. all positive real-valued intensities generated by the array scanning software (in our case [0, 2 16  − 1]). Γ is the set of possible states, as derived in  Section 2.1 .  A  is a matrix of transition probabilities for the system states. We refer to  e  as the emission probability   [Equation ( 4 )], where   is the matrix of estimated model parameters [Equation ( 3 )].  e  corresponds to the likelihood of observing data point  x t  given its state  . Note that  x t  is a column in the measurement matrix  X , i.e. a vector of intensity values. We use the Viterbi training algorithm (Durbin  et al. ,  1998 ) to find an optimal sequence of system states and optimize the transition matrix  A  as well as the parameter matrix  . We initialize   by sampling random states from Γ, while preserving the order of the states, and the transition matrix  A  to uniform probabilities for all state transitions. We estimate model parameters   depending on   [Equations ( 2 ) and ( 3 )]. Now   is updated using the HMM and the procedure iterated until convergence, as described in Durbin  et al.  ( 1998 ). This yields the final state matrix estimate   used for the likelihood calculation, described in the next section. 2.3 Likelihood model For calculation of emission probabilities in  Section 2.2  as well as computation of the total network likelihood in the structure search ( Section 2.4 ), we set up a likelihood score that describes the probability of observing measurements under our model, represented by the network hypothesis. Given a state matrix estimate  , each measurement  x itr  for protein  i , time point  t  and replicate  r  comes from an ‘active’ normal distribution 𝒩(μ i 1 , σ i 1 ), if its state  , and from a ‘passive’ normal distribution 𝒩(μ i 0 , σ i 0 ), if  :
 (2) The parameters of each distribution for one protein are obtained as unbiased empirical mean and SD of all measurements for this protein in the given class. This yields the parameter matrix:
 (3) Now we can write the likelihood for a data point  x t  as:
 (4) The total likelihood for a network hypothesis Φ can be written as:
 (5) 2.4 Network structure search The previous two sections dealt with the assessment of a single network hypothesis. However, the aim of our method is to optimize the network structure with respect to the network likelihood, so a suitable network structure search strategy has to be chosen. We use a GA as sampling-based technique for network structure search that optimizes a whole population of candidate networks. Studies of Wahde and Hertz ( 2000 ) and Spieth  et al.  ( 2006 ) show the usefulness of evolutionary strategies for network reconstruction. We evolve a population of networks in parallel by selection and mutation of the individuals. Selection should choose the fittest individuals and mutations should be beneficial for the overall fitness of all networks. Further, we allow ‘communication’ between the networks in form of crossovers. To avoid overfitting by inclusion of too many edges in the networks, we use the Bayesian information criterion (BIC; Schwarz,  1978 ) as fitness score, which penalizes higher numbers of edges and is calculated from the likelihood [Equation ( 5 )]:
 
where  K  is the number of edges in Φ and  n  is the number of data points in  X . This will result in sparse network structures. 2.4.1 GA specification A population  P  = {Φ j  :  j  ∈ 1,…,  p } of  p  networks, a crossover (selection) rate  q  (1 −  q ) and mutation rate  m  with  q ,  m  ∈ [0; 1] are given. During selection we choose a fraction ⌊(1 −  q ) p ⌋ individuals with probability proportional to their fitness. We require that BICs of selected networks are smaller than the median of the BICs of all individuals in the population, mimicing a simple greedy search, but leaving the possibility for selecting suboptimal moves. The selected individuals are added to the next generation population  P ′. For crossing over we choose   random pairs from  P , again proportional to each individuals' fitness. To perform crossing over of two networks, each network adjacency matrix is represented as a vector (simply attaching all columns to each other) and two point crossover is performed for these vectors. The modified individuals are added to  P ′ if their BICs are smaller than the median BIC for all individuals in  P ′. In case that after crossover the size of the modified population  P ′ is smaller than  p , we add as many random individuals from  P  to  P ′, such that the population size stays constant. Finally, we perform mutation of ⌊ mp ⌋ networks chosen from the new population  P ′. For each selected network a random edge is drawn and its type is changed randomly to one of the remaining types. As an example, given an edge ϕ vw  = 2, it can be either changed to ϕ vw ′ = 1 or ϕ vw ′ = 0. Mutations are allowed if the fitness of the individual improves by introducing the mutation. These three steps are repeated until a prespecified number of iterations (usually 1000) have been run or the median of all BICs in the population does not change for 10 times in a row. At the end of the GA, the population of candidate networks is combined into a final network by including each edge that occurs in more than a prespecified fraction of all networks in the population (usually 50% if not stated explicitly). 2.5 Data generation and preprocessing for HCC1954 RPPA data The human breast cancer cell line HCC1954 was cultivated as recommended by ATCC and cells were split three times per week. For stimulation experiments, cells were seeded in 6-well plates, cultivated for 24 h and serum-starved in phenol red-free medium for additional 24 h. EGF (Sigma, Steinheim, Germany) and HRG (Biovision, Mountain View, CA, USA) were added to the cells to a final concentration of 5 nM. After times 0, 4, 8, 12, 16, 20, 30, 40, 50 and 60 min, medium was replaced by ice-cold PBS and plates were put on ice. Afterwards, PBS was aspirated and cells were harvested by manual scraping in 40 μl lysis buffer [M-PER (Pierce, Bonn, Germany), Complete Mini, PhosSTOP (Roche, Mannheim, Germany)]. Cells were lysed for 20 min at 4  ○ C. After centrifugation, total protein concentration was determined using the BCA method (Pierce, Bonn, Germany) and all samples were adjusted to the same protein concentration. Prior to printing, samples were mixed with Tween-20 to a final concentration of 0.05%. Three biological replicates were generated at three different days. The samples were printed in triplicate onto nitrocellulose coated glass slides [Oncyte; Grace-Biolabs (Bend, OR, USA)] with a contact spotter [2470 Arrayer; (Aushon Biosystems, billerica, MA, USA)] using 180 μm pins. Slides were blocked in 50% Odyssey Blocking Buffer LI-COR (Lincoln, NE, USA) in PBS containing 5 mM sodium fluoride and 1 mM vanadate. Primary antibodies were diluted 1:300 in antibody diluent with background reducing components (Dako, Glostrup, Denmark). Alexa 680 labelled secondary antibodies (Molecular Probes, Darmstadt, Germany) were diluted 1:5000 in PBS (+0.2% NP-40, 0.02% SDS + 0.5% BSA). After drying, arrays were scanned using the Odyssey Infrared Imaging System (LI-COR, Lincoln, NE, USA) and signal intensities were determined with GenePix Pro 5.0 (Molecular Devices, Sunnyvale, CA, USA). Sample normalisation was done using Fast Green FCF dye (see Loebke  et al. ,  2007 ; Luo  et al. ,  2006 ) to account for different protein concentrations in each spot on the array. Replicate time courses were centred around their common mean to remove systematic shifts in the intensities. Sixteen antibodies for specific phosphorylation sites were used to obtain signal intensities of phosphorylated protein. A list of the proteins and phosphorylation sites is shown in  Table 1 . The antibodies were obtained from the following companies: ERBB4 and GSK3 from Epitomics (Burlingame, CA, USA), ERBB2 from Millipore (billerica, MA, USA), MEK1/2 from Sigma, PKCα from Abcam (Cambrige, UK) all others from Cell Signaling (Beverly, MA, USA).
 Table 1. Proteins and phosphorylation sites used in the RPPA analysis Protein Phosphosite Protein Phosphosite Protein Phosphosite AKT S473 EGFR Y1068 ERBB2 Y1112 ERBB3 Y1289 ERBB4 Y1162 ERK1/2 T202,Y204 GSK3 Y279,Y216 MEK1/2 S217,S221 MTOR S2448 p38 T180,Y182 p70S6K T389 PDK1 S241 PKCα S657,Y658 PLCγ S1248 PRAS T246 SRC Y416 
 3 RESULTS AND DISCUSSION 3.1 Simulations 3.1.1 Generation of simulation data Given a number of nodes and a number of input stimuli, we generated networks as follows: starting at the input stimuli, we sampled outgoing activation edges until all nodes were connected and added 20% of the number of activating edges as inhibitions to retrieve fully connected networks. This ensured that all nodes could be reached by a stimulus signal and that feed forward and feed back loops were included in the network. Given such a sampled network, a data matrix  X  (as defined in  Section 2.2 ) was constructed. We refer to parameters  nstim  as the number of distinct input stimuli and  cstim  as the number of stimulus combinations. Each stimulus gives rise to a separate experiment, so for each stimulus a separate state matrix was constructed by our effect propagation. A state transition matrix for each stimulus was built up by sampling  T  columns with replacement from each state matrix, while the order of the states was preserved. Each column in the state transition matrix was repeated  R  times to generate replicates. Finally, all state matrices were attached to get the total state matrix Γ, and all state transition matrices were attached to generate Γ * . Then, for each time point, replicate and node a measurement  x itr  was sampled from two Gaussian distributions, either from  x itr  ∼ 𝒩(1200, 400) if γ * itr  = 0 or from  x itr  ∼ 𝒩(2000, 1000) if γ * itr  = 1. The parameters for the Gaussians (mean and variance) were chosen similar to the observed measurements in our real data. We chose  T  = 10 and  R  = 9 as number of time points and number of replicates for the simulations. 3.1.2 Recovering the true state sequence We tested how good the HMM from  Section 2.2  is able to recover a true state sequence Γ * . For this purpose we sampled networks for increasing number of nodes and performed the effect propagation from  Section 2.1  for different numbers of input stimuli. Γ *  matrices were sampled 100 times for each network and stimulus combination, and for each Γ *  data was generated as described in  Section 3.1.1 . We performed the HMM state sequence search for all data matrices and compared the resulting state transition matrix   with the corresponding reference Γ *  in terms of sensitivity SN = (TP/(TP+FN)) and specificity SP = (TN/(TN+FP)), counting the true and false occurrences of the entries in  .  Figure 2  shows that the recovery performance stays constantly high at average values of around SN = 0.84 and SP = 0.95 for networks up to 30 nodes, and around SN = 0.83 and SP = 0.97 for increasing the number of input stimuli  nstim . Hence, given an unknown series of system states, the HMM is able to identify the correct states, even for bigger networks with up to 30 nodes.
 Fig. 2. Performance of state recovery for increasing number of nodes  N  ( A ) and number of stimuli  nstim  ( B ). 3.1.3 Performance of structure search We sampled random networks and generated intensity measurements as described in  Section 3.1.1 . For network comparisons, we counted the number of truly inferred edges (TP), truly not inferred edges (TN), erroneously inferred edges (FP) and erroneously not inferred edges (FN). Note that now we counted edges in the network, and not entries in the state matrix as in the previous section. Network reconstructions were done for artificial networks of size  N  = 10 with population sizes from  p  ∈ {100, 250, 500},  q  = 0.3 and  m  = 0.8. Also increasing numbers of different input stimuli were compared. We chose  nstim  ∈ {1, 2} and  cstim  ∈ {0, 1}. To measure the performance, the GA was run for 25 sampled networks, each time with the maximum number of generations set to 1000. The edge inclusion threshold for the final network ( Section 2.4 ) was varied in [0; 1], and the respective final network for each given threshold was compared with the original net, yielding SN and SP values for the generation of receiver operator characteristic (ROC) curves and area under curve (AUC) values. Figure 3  shows that the reconstruction performance was limited for the case of  nstim  = 1, cstim  = 0 and increasing population size  p  = 100,  p  = 250 and  p  = 500 (AUCs 0.57, 0.6, 0.61), while a slight increase could be found for the bigger population size. This is expected because the use of a bigger population ensures broader sampling of the network search space. However, true increase in performance was reached when including two distinct stimuli ( nstim  = 2) and further including one stimulus combination ( cstim  = 1). Here, the AUCs increased to 0.75 and 0.73, respectively. As before, for higher population sizes the AUCs increased (from 0.7 to 0.75). In  Figure 3 B, for a fixed threshold  th  = 0.5, SN and SP were plotted for each simulation test. In the  nstim  = 1 case, SP was high around 0.87, while SN was rather low around 0.17. For  nstim  = 2, SN increased to values around 0.4, while SP improved from 0.78 to 0.83 for growing population sizes. This showed that inclusion of multiple stimuli triggering signalling in the network at different input nodes increased the amount of information that could be used to find the signalling connectivity, and thus resulted in better identification of true edges in the network (apparent in the increasing SN values). However, it was also apparent that SN levels were still rather low, so the reconstruction missed quite a number of edges, that should have been found. On the other hand, the high values for SP ensured that inferred edges were those with strong support from the experimental measurements, and thus could be expected to be meaningful. Summarising this, our method was able to recover parts of the original signalling networks but did this with high specificity, meaning that predominantly true edges were found. This makes the method useful for the generation of new interaction hypotheses.
 Fig. 3. ( A ) ROC curves and AUCs for different settings of input ( nstim ) and combinatorial stimuli ( cstim ) and population sizes ( p ). SN and SP were calculated as average of each 25 network reconstructions with network size of  N  = 10. ( B ) Example SN and SP plot for  th  = 0.5 for all settings. For  p  = 500, SP was high at ∼0.83, while SN increased from ∼0.17 to ∼0.4. This shows, that DDEPN found edges with strong support from the data with low FP rates. The increase in SN for bigger population sizes shows, that broader sampling of the network search space yielded better inference results. 3.2 Comparison with related approaches for network inference We compared our method with the DBN reconstruction approach G1DBN of Lébre ( 2009 ) and to a recent method of Rau  et al.  ( 2010 ), called ebdbNet. For network size  N  = 10, 25 networks were simulated and the reconstruction performed. We repeated each network reconstruction 100 times and calculated ROCs and AUCs as shown in  Section 3.1.3 . The results are depicted in  Figure 4 . For  nstim  = 1,  cstim  = 0, DDEPN performed slightly better than G1DBN and ebdbNet (AUCs 0.61 for DDEPN, 0.58 and 0.55 for G1DBN and ebdbNet, respectively). However, the performance was limited in this case for all methods. Using  nstim  = 2, DDEPN clearly outperformed G1DBN and ebdbNet, for both  cstim  = 0 (AUC = 0.75) and  cstim  = 1 (AUC = 0.73). This highlighted the ability of DDEPN to make use of the additional information gained from multiple perturbations. The better performance had its price in terms of computation time. On average, a 10 node network with three input stimuli was reconstructed in around 7000 s using DDEPN, while G1DBN and ebdbNet completed this task in a few seconds. However, the network inferred in DDEPN was derived from a whole population of candidate networks that covers larger portions of the network search space than the other two approaches. Calculation was done on a Quad-Core AMD Opteron(tm) 2.7 GHz machine with 64 GB memory, on which each 14 cores were used in parallel to optimize the population of networks in the GA. Because of the better performance of the reconstruction and the fact that DDEPN was able to infer both activation and inhibition edges, we think this price is worth paying.
 Fig. 4. ROC curves and AUCs for DDEPN network reconstruction compared with G1DBN and ebdbNet. ( A ) For  nstim  = 1, cstim  = 0, a slight improvement of AUCs was observed, and performances were limited for all approaches. ( B ) For  nstim  = 2, cstim  = {0, 1}, a clear increase in AUC was found for DDEPN, showing the improved quality of the network reconstructions. We also compared our new approach DDEPN with the related approach deterministic effects propagation networks (DEPNs) of Fröhlich  et al.  ( 2009 ), but were not able to infer reasonable networks under the settings applied here. This was for two reasons: first, DEPN was designed for the setting of few time points and many perturbations, i.e. the information on the signal flow is collected through the perturbation of many or even all nodes in the network, so a small number of time points is sufficient. In DDEPN, we only introduced few perturbations and got additional information on the signal flow through a higher number of time points. Therefore, it was not possible to capture the signalling relationships of the components downstream the perturbed nodes with high resolution using DEPN. Second, DEPN cannot model perturbations as stimulation, but only as knockdown. So both methods have specific requirements and cannot be exchanged without care for different datasets. However, DDEPN can also be run with more than two perturbed nodes and has the advantage, that both types of perturbation can be included. 3.3 Signalling networks in HCC1954 breast cancer cell line We used DDEPN to reconstruct a signalling network from our data. Parameters were chosen as population size 500, maximum iterations 1000, crossover rate  q  = 0.3 and mutation rate  m  = 0.8. The inferred network is shown in  Figure 5 . An edge is shown if it was contained in at least 50% of the networks in the final population ( th  = 0.5), allowing only interactions with strong support from the data. We saw several signal cascades in our network that were known from the literature. For example, we inferred the regulation HRG → ERBB1. Olayioye  et al.  ( 1999 ) showed that HRG is an activator of the ERBB-Dimers 1/3 and 1/4, which supported this result. Activation of ERBB2 by EGF or HRG could be found in Jones  et al.  ( 1999 ) (EGF/HRG → ERBB2/3), which also supported activation of PKCα by HRG through the cascade HRG → ERBB2 → PKCα, since crosstalk between ERBB2 and PKCα in ERBB2 overexpressing breast cancer cells was reported by Magnifico  et al.  ( 2007 ). The result was further interesting, since our HCC1954 cells overexpress ERBB2. Kim  et al.  ( 2009 ) reported activation of p38 by ERBB2 in ERBB2 overexpressing breast cancer cells, reflected in our activation EGF → p38. The activations of MEK1/2, ERK1/2 and p70S6K by EGF are key elements in the classical MAPK signalling cascade EGF → ERBB1/1 → GRB2 → SOS1 → RAS → RAF1 → MEK1/2 → ERK1/2 → p70S6K. EGF → ERBB1/1 → PLCγ was shown by Kim  et al.  ( 1990 ), which demonstrated the relevance of the activation EGF → PLCγ in our network. Further EGF → AKT ⊣ GSK3α is found in the cascade EGF → ERBB → GRB2 → GAB1 → PI3K → AKT ⊣ GSK3α.
 Fig. 5. Network reconstructed from HCC1954 data. Interactions found in the literature are marked as thick lines. Dark nodes mark the input stimuli. The numbers at the edges show the proportion of networks in the final GA population, in which the respective edge was contained. More hypothetical interactions included the inferred SRC activation (ERBB3 → SRC), interpreted as activation of SRC by ERBB2 (see e.g. Luttrell  et al. ,  1994 ; Mao  et al. ,  1997 ; Xian  et al. ,  1997 ) through the ERBB2/3 heterodimer. Finally, PDK1 activation by receptor tyrosine kinases was shown by Cohen  et al.  ( 1997 ) in insulin signalling. We found the activation ERBB3 → PDK1, which supported the hypothesis that the cascade ERBB1/3 → PI3K → PIP3 → PDK1 (see Oda  et al. ,  2005 ; Vanhaesebroeck  et al. ,  1997 ) might also play a role in cancer-related signal transduction processes. All of these inferred and literature confirmed interactions had high support by our data (occurrence in &gt;75% of all networks in the final population, see edge labels in  Figure 5 ). Our findings showed that literature knowledge was reproduced well by our method and in addition allowed for discussion of the newly inferred interactions. However, there were cases, where interactions would have been expected, but were not found in the network. For example, in the classical MAPK cascade, MEK1/2 phosphorylates ERK1/2 directly. In our network, the interaction between MEK1/2 and ERK1/2 was not found, but only direct activations of the two proteins by EGF. The reason was that we only measured phosphorylation at time points 8 and 12 min. Activation of MEK1/2 and ERK1/2 is expected around 10 min after stimulation, but in our data we saw peaks for both proteins at the 12 min time point. Thus, we could not resolve this cascade at a higher resolution. Another problem arised when proteins of a signalling cascade were not measured on the array, as seen, for example, for several of the components in the MAPK cascade (e.g. RAS, RAF, etc.). So even if a direct edge between two proteins is found, it has to be carefully assessed whether this edge is a direct influence or an indirect interaction over multiple intermediate steps. Our data only represents the abundance of phosphorylated protein in the cells, which might increase or decrease in response to a ligand. All interactions from such data are abstract influences between two proteins that have to be validated in further experiments. However, considering these kind of caveats and performing careful interpretation of the results makes our method suitable for the generation of reasonable hypotheses on signalling cascades. 4 CONCLUSION In this work, we showed a novel approach for the reconstruction of signalling networks from high-throughput proteomics data generated on RPPAs. The phosphorylation of 16 proteins related to ERBB signalling in human breast cancer was measured after three different stimulations (EGF, HRG, EGF+HRG). We devised a method that describes the signalling dynamics in a discretized way and infers the most likely series of activation states for all proteins at each time point given a candidate network by using an HMM. A likelihood model was set up to describe the goodness of fit of our measurements for a particular candidate network. To find a best fitting network, the space of possible network hypotheses is searched by a GA. We tested our method on simulated data and found good performance for the reconstruction of given networks, and improved performance over two other DBN approaches that were suitable for analysing our kind of data. Finally, we used our method to infer a signalling network from real protein phosphorylation measurements, generated by RPPAs for cell lysates from the human breast cancer cell line HCC1954. We successfully identified parts of signalling cascades, as they could be found in the literature, in particular from the MAPK and AKT signalling cascades, with some interactions originally found in ERBB2 overexpressing breast cancers (e.g. HRG→ PKCα). As new technologies such as RPPAs arise that make parallel measurement of larger numbers of proteins feasible, the need for suitable methods for the analysis of this kind of data is apparent. Our method aims precisely at this niche and gives an example for upcoming systems proteomics methodology. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Prediction of nucleosome positioning by the incorporation of frequencies and distributions of three different nucleotide segment lengths into a general pseudo k-tuple nucleotide composition</Title>
    <Doi>10.1093/bioinformatics/btw562</Doi>
    <Authors>Awazu Akinori, Hancock John</Authors>
    <Abstract/>
    <Body>1 Introduction Nucleosomes are the basic units of eukaryotic chromatin, and each one is formed by 147 DNA base pair (bp) sequences wrapped tightly around a histone octamer. The precise nucleosome formation and its inhibitory effects on promoters ( Choi and Kim, 2009 ;  Jiang and Pugh, 2009 ;  Tirosh and Barkai, 2008 ), enhancers ( Andreu-Vieyra  et al. , 2011 ;  He  et al. , 2010 ;  Maston  et al. , 2012 ;  McPherson  et al. , 1996 ) and insulators ( Bi  et al. , 2004 ;  Takagi  et al. , 2012 ) play crucial roles in the precise regulation of transcription ( West  et al. , 2014 ). The precise nucleosome positioning facilitates DNA replication, DNA repair, and RNA splicing ( Berbenetz  et al. , 2010 ;  Chen  et al. , 2010 ,  2014a ;  Schwartz  et al. , 2009 ;  Yasuda  et al. , 2005 ). Therefore, the elucidation of nucleosome positioning steps may allow an in-depth understanding of various biological processes. Recently, high-resolution genome-wide nucleosome maps were obtained for several model organisms ( Lee  et al. , 2007 ;  Mavrich  et al. , 2008 a,b;  Schones  et al. , 2008 ;  Segal  et al. , 2006 ). In contrast to this, the determinant factors of the nucleosome positioning remained unclear. However, with the increase in the availability of high-quality experimental datasets, various computational methods and tools for the prediction of nucleosome positioning were proposed (reviewed in  Teif, 2015 ), providing valuable insights and allowing the mechanisms determining nucleosome positioning to be unveiled. Furthermore, the construction of the accurate predictors can lead to the possibility of the analysis of single nucleotide polymorphism and gene mutation effects on this process. Many of these predictors were constructed based on the information about the frequencies and distributions of the combinations of polynucleotide sequences as feature vectors ( Field  et al. , 2008 ;  Ioshikhes  et al. , 2006 ;  Kaplan  et al. , 2009 ;  Ogawa  et al. , 2010 ;  Peckham  et al. , 2007 ;  Segal  et al. , 2006 ;  Struhl and Segal, 2013 ;  Yi  et al. , 2012 ;  Zhang  et al. , 2012 ). Sequence-dependent mechanical properties, such as sequence-dependent geometry and DNA fragment flexibility, were also considered for the characterization of nucleosome forming and inhibiting sequences ( Chen  et al. , 2012a ,  2015 ;  Freeman  et al. , 2014 ;  Goñi  et al. , 2008 ;  Guo  et al. , 2014 ;  Isami  et al. , 2015 ;  Nikolaou  et al. , 2010 ;  Tahir and Hayat, 2016 ;  Tolstorukov  et al. , 2008 ;  Stolz and Bishop, 2010 ;  Yuan and Liu, 2008 ). Furthermore, a powerful web-server called Pse-in-One ( Liu  et al. , 2015a ) was developed, where all existing feature vectors for DNA/RNA and protein/peptide sequences can be generated (see references cited in  Chen and Lin 2015 ), together with the generation of the feature vectors for the sequences defined by users themselves. For human ( Homo sapiens ), worm ( Caenorhabditis elegans ) and fly ( Drosophila melanogaster ) genomes,  Guo et al. (2014)  constructed the stringent benchmark datasets of nucleosome forming and inhibiting sequences with low similarities, in order to examine the performance of nucleosome position predictors. Additionally, predictors iNuc-PseKNC and iNuc-PseSTNC (we call iNuc-Pse predictors) were proposed, and they were shown to have better success rates in the prediction of nucleosome positioning than any of the previously developed predictors ( Guo  et al. , 2014 ;  Tahir and Hayat, 2016 ). Furthermore, for yeast ( Saccharomyces cerevisiae ) genomes,  Chen et al. (2015)  constructed a stringent benchmark dataset using the same methodology as  Guo et al. (2014) , and predicted the nucleosome positioning in yeast genome based on the deformation energies of DNA fragments. In order for iNuc-Pse predictors to show the best prediction performance for nucleosome positioning in different organisms, different sets of parameter values must be used ( Guo  et al. , 2014 ;  Tahir and Hayat, 2016 ). The sequence predicted as the nucleosome forming sequence in one organism may be predicted as the nucleosome inhibiting sequence in another organism. This shows that the function of any given DNA sequence in assisting or inhibiting nucleosome formation depends on the investigated organism. However, no key factors and criteria affecting this process could be elucidated using this predictor, because iNuc-Pse predictors are based on support vector machine. Additionally, based on these datasets, some common short motif nucleosome forming and inhibiting sequences were found ( Giancarlo  et al. , 2015 ). However, the nucleosome positioning cannot be predicted sufficiently well using only these motives. In this study, a novel nucleosome positioning predictor was developed based on the linear regression model, consisting of three types of variables with different fragment length scales—the number of five-nucleotide sequences, the number of three-nucleotide combinations in one period of helix, and mono- and di-nucleotides distributions in whole DNA fragments. This predictor exhibited better prediction performance than the recently developed iNuc-Pse predictors for the same benchmark datasets of human and fly genomes and displayed common and organism-dependent key factors of nucleosome positioning explicitly. A series of recent publications ( Jia  et al. , 2015 ,  2016 ;  Lin  et al. , 2014 ,  Liu  et al. , 2016 ;  Qiu  et al. , 2014 ,  2016a ;  Xiao  et al. , 2015 ) demonstrated, in compliance with Chou’s five-step rule ( Chou, 2011 ) that, in order to establish a useful sequence-based statistical predictor for a biological system, the following five guidelines should be observed: (i) how to construct or select a valid benchmark dataset to train and test the predictor; (ii) how to represent the biological sequence samples by catching their key features associated with the target to be predicted; (iii) how to introduce or develop a powerful algorithm to operate the prediction; (iv) how to properly perform cross-validation tests to objectively evaluate the anticipated accuracy; and (v) how to establish a user-friendly web-server for the predictor that is accessible to the public. Below, these steps are further explained. 2 Materials and methods 2.1 Benchmark datasets of nucleosome forming and inhibiting sequences The stringent benchmark datasets of nucleosome forming and inhibiting sequences with low biases constructed by  Guo  et al.  (2014)  and  Chen  et al.  (2015)  were used for the evaluation of the performance of the proposed predictor. These datasets involved human ( H.sapiens : 2273 forming sequences and 2300 inhibiting sequences of 147 bp), worm ( C.elegans : 2567 forming sequences and 2608 inhibiting sequences of 147 bp), fly ( D.melanogaster:  2900 forming sequences and 2850 inhibiting sequences of 147 bp) ( Guo  et al. , 2014 ) and yeast ( S.cerevisiae : 1880 forming sequences and 1740 inhibiting sequences of 150 bp) ( Chen  et al. , 2015 ). In these datasets, none of the sequences has &gt;80% pairwise sequence identity with any other sequence. Note that the benchmark datasets used in previous studies were expected to contain many redundant, highly similar sequences, and these biased datasets lacked statistical representativeness ( Chou, 2011 ), and the predictors may have yielded misleading results if trained and tested using these biased datasets. Therefore, only the low-biased datasets, proposed by  Guo  et al.  (2014)  and  Chen  et al.  (2015)  were employed in this study. 2.2 Model predicting nucleosome positioning 1: three-length scales model In order to predict whether a given 147-bp DNA sequence of human, worm, and fly genomes is involved in the formation or the inhibition of formation of nucleosome, the model included three types of variables: (i) the number of five-nucleotide sequences, (ii) the number of three-nucleotide combinations in one period of a double helix and (iii) mono- and di-nucleotide distributions in DNA fragments. The model was named three length scales (3LS), and it belongs to a class of general PseKNC-based predictors ( Guo  et al. , 2014 ;  Liu  et al. , 2015a ). The model is described by the following equations:
 (1) Q seq   = Q 0 + Σ i { M ( i   |   A   or   T )   S seq 1 ( i   |   A   or   T ) + Σ Di D ( i   |   Di − seq   or   Di − seq * )   S seq 2 ( i   |   Di − seq   or   Di − seq * ) } + Σ Tri Σ 0 &lt; j &lt; k &lt; 11 T ( 0 ,   j ,   k , = 3 − nuc   or   0 ,   k − j ,   k , = 3 − nuc * ) × S seq 3 ( 0 ,   j ,   k , = 3 − nuc   or   0 ,   k − j ,   k , = 3 − nuc * ) + Σ Pent P ( 5 − seq   or   5 − seq * )   S seq 5 ( 5 − seq   or   5 − seq * ) , (2) S seq 1 ( i   |   A   or   T ) = log 2 ( N seq 1 ( i   |   A   or   T ) + 1 ) , (3) S seq 2 ( i ’   |   Di − seq   or   Di − seq * ) = log 2 ( N seq 2 ( i ’   |   Di − seq   or   Di − seq * ) + 1 ) , (4) S seq 3 ( 0 ,   j , k , = 3 − nuc   or   0 ,   k − j ,   k , = 3 − nuc * ) = log 2 ( N seq 3 ( 0 ,   j ,   k , = 3 − nuc   or   0 ,   k − j ,   k , = 3 − nuc * ) + 1 ) , 
and
 (5) S seq 5 ( 5 − seq   or   5 − seq * ) = log 2 ( N seq 5 ( 5 − seq   or   5 − seq * ) + 1 ) . Here, Q seq  is defined as a value of a given sequence, and when Q &gt; Q c  = 0.5, this sequence was considered a nucleosome forming sequence, while it was predicted as an inhibiting sequence otherwise.  N seq 1 ,   N seq 2 ,   N seq 3 , and   N seq 5 are defined as follows: 
 N seq 1 ( i   |   A   or   T ) defines the number of adenine (A) or thymine (T) nucleotides in ith region of the given DNA sequence. Here, region 1 occupies the central 11 bp fragment of the given 147-bp DNA, the regions for 1 &lt; i &lt; 8 occupy 2 10-bp fragments (20 bp) at (i-1)th nearest neighbor of first region, and eighth region occupies the remaining 16-bp fragment ( Fig. 1a ). Fig. 1.  Nucleotide regions and groups analyzed in each 147-bp DNA sequence.  (a)  Each nucleotide belongs to a specific region.  (b)  Each dinucleotide pair belongs to a specific group.  b n  indicates the  n th base of nucleotide, and dinucleotide pairs are underlined red 
 N seq 2 ( i ’   |   Di − seq   or   Di − seq * ) defines the sum of the number of each type of successive dinucleotide sequence and its complementary sequence, named Di-seq and Di-seq*, in the i’th group of the dinucleotide series of a given DNA sequence. Here, first group consists of 10 dinucleotides at the central region of a given 146-dinucleotide series, i’th groups for 1 &lt; i’ &lt; 8 consist of 20 dinucleotides between (5 ± (10 × (i’-2)+1))th to (5 ± 10 × (i’-1))th dinucleotide from the center of a given 146 dinucleotide series, and eight group contains the remaining 16 dinucleotides ( Fig. 1b ). 
 N seq 3 ( 0 ,   j ,   k , = 3 − nuc   or 0 ,   k − j ,   k , = 3 − nuc * ) defines the sum of the number of each type of combination of 3-nucleotide set (3-nuc) that consists of a nucleotide, the second nucleotide located downstream at the distance j, and the third nucleotide located at the distance k in downstream sequence (j &lt; k), together with the number of the complementary nucleotide combinations (3-nuc*) in the given DNA sequence. Here, 5 &lt; k &lt;11 cases were considered. 
 N seq 5 (5-seq or 5-seq*) defines the sum of the number of each type of successive five-nucleotide sequence (5-seq) and that of the complementary sequence (5-seq*) in the given DNA sequence. The coefficients M (), D (), T (), and P () provide the weight of the contributions of  S seq 1 ( ) ,   S seq 2 ( ) ,   S seq 3 ( ) ,   and   S seq 5 ( )  to Q seq  and Q 0  as a constant value. They are organism-dependent values, which reveal the common and organism-specific characteristics of nucleosome forming and inhibiting sequences. 2.3 Variable selection in 3LS model In order to obtain high prediction performances, the 3LS model should contain only the appropriate variables of S seq 1 ( ) ,   S seq 2 ( ) ,   S seq 3 ( ) ,   and   S seq 5 ( ) . The coefficients M (), D (), T () and P () of the appropriate variables should be given as finite values, while the values of redundant variables should be given as zero. The appropriate variables were chosen by the stepwise forward selection method ( Efroymson, 1960 ). Here, in order to avoid multicollinearity ( Farrar and Glauber, 1967 ), the variance inflation factors of all chosen variables were kept below 10 (10.5 for fly genomes, since the prediction performance of the model increased drastically in comparison with the case when 10.0 was used) ( O’brien, 2007 ). The model consists of the linear combination of S seq 1 ( ) ,   S seq 2 ( ) ,   S seq 3 ( ) ,   and   S seq 5 ( ) , instead of that of  N seq 1 ( ) ,   N seq 2 ( ) ,   N seq 3 ( ) , and   N seq 5 ( ) , since this allows a better prediction performance. 2.4 Model predicting nucleosome positioning 2: tri-nucleotide sequence model For the prediction of nucleosome positioning, a simpler model than 3LS, named Tri-nucleotide sequence (TNS) model was introduced:
 Q seq   = Q 0 + Σ tri R ( 3 − seq   or   3 − seq * )   N seq T ( 3 − seq   or   3 − seq * ) , 
where  N seq T ( 3 − seq   or   3 − seq * )  is defined by the sum of the number of each type of successive TNS and that of the complementary sequence in a given DNA sequence. The coefficient R () provide the weight of the contributions of  N seq T ( )  to Q seq  and Q 0  as a constant value. This simple model allows a very high accuracy of the nucleosome positioning prediction for yeast genome. 2.5 Evaluations of the quality of prediction The prediction quality of the present model was evaluated using the jackknife test ( Lachenbruch and Mickey, 1968 ) and relative operating characteristic (ROC) curve. These methods were generally employed for the evaluation of the quality of several previously developed predictors ( Chen  et al. , 2012b ,  2013 ;  Chen and Li, 2013 ,  Chou  et al. , 2012 ;  Esmaeili  et al. , 2010 ;  Gupta  et al. , 2013 ;  Mei, 2012 ,  Mohabatkar et al., 2011 ,  2013 ) and iNuc-Pse predictors ( Guo  et al. , 2014 ;  Tahir and Hayat, 2016 ). Here, N + , N − , N + − , and N − +  were defined as the total number of nucleosome forming sequences, nucleosome inhibiting sequences, nucleosome forming sequences incorrectly predicted as nucleosome inhibiting sequences, and nucleosome inhibiting sequences incorrectly predicted as nucleosome forming sequences. Using the jackknife test, the following metrics were obtained:
 Sn = 1 − N − + / N + Sp = 1 − N + − / N − Acc = 1 − ( N − + + N + − ) / ( N + + N − ) MCC = { 1 − ( N − + / N + + N + − / N − ) } / { ( 1 + ( N + − − N − + − ) / N + ) ( 1 + ( N − + − N + − ) / N − ) } 1 / 2 
where Sn, Sp, Acc and MCC stand for sensitivity, specificity, accuracy, and Mathew’s correlation coefficient, respectively. Note that Sn and (1 − Sp) represent true positive rate (TPR) and false positive rate (FPR), respectively. The conventional formulations of the four metrics are not quite intuitive and it may be difficult for many experimental scientists to understand them, particularly MCC. Fortunately, the more intuitive expressions, presented in this paper, can be derived using the symbols defined in a signal peptide study ( Chou, 2001 ), and elaborated in other studies ( Chen  et al. , 2013 ;  Xu  et al. , 2013 ). The ROC curve can be obtained as the trajectory of TPR–FPR two-dimensional surface for the change in Q c . The area surrounded by TPR = 0, FPR = 0, and ROC curve, called AUROC, was used to estimate the performances of predictors, where AUROC = 0.5 is equivalent to a random prediction, and AUROC = 1 indicates perfect prediction. Note that the following three cross-validation methods are often used to examine the effectiveness of a predictor in practical applications: independent dataset test, subsampling test, and jackknife test ( Chou and Zhang, 1995 ). However, of the three, the jackknife test is deemed the least arbitrary one (most objective) that can always yield a unique result for a given benchmark dataset ( Chou, 2011 ), and therefore, it has been increasingly used for the investigations of the accuracy of various predictors (e.g.  Dehzangi  et al. , 2015 ;  Kabir and Hayat, 2016 ; and references cited in  Chou, 2011 ). Accordingly, the jackknife test was also adopted here for the examination of the quality of the present predictor. 2.6 Construction of nucleosome positioning predictor Based on the 3LS and TNS models, the nucleosome positioning predictors for each organism were constructed. The predictors for human, worm and fly genomes were assumed to consist of the appropriately chosen variables. The coefficients of these chosen variables were determined by the multiple regression analysis, using benchmark datasets for each organism, and the explanatory variables were given by the chosen  S seq 1 ( ) ,   S seq 2 ( ) ,   S seq 3 ( ) ,   and   S seq 5 ( )  for 3LS model, and  N seq T ( )  for TNS model, and the objective variables were given as 1 for nucleosome forming sequences and 0 otherwise. 3 results 3.1 Variable selection for 3LS model using human, worm and fly sequences Variables  S seq 1 ( ) ,   S seq 2 ( ) ,   S seq 3 ( ) ,   and   S seq 5 ( ) , involved in the construction of the nucleosome positioning predictors in 3LS model were chosen by stepwise forward selection method. Here, 403, 392 and 325 variables were chosen for human, worm and fly genomes, respectively ( Supplementary Table S1 ). 3.2 Prediction quality for human, worm and fly genomes Using the jackknife cross-validation tests, Sn, Sp, ACC and MCC of 3LS model based predictor were evaluated for human, worm, and fly genome benchmark datasets ( Table 1 ). The obtained ACCs of the investigated predictor for these datasets (≈ 0.9001, ≈ 0.8786 and ≈ 0.8341, respectively) were shown to be higher than those obtained by iNuc-PseKNC ( Guo  et al. , 2014 ) for all organisms, and higher than those obtained by iNuc-PseSTNC ( Tahir and Hayat, 2016 ) for human and fly genomes. The higher AUROC values were obtained as well (≈ 0.9588, ≈ 0.9505 and ≈ 0.9147 for human, worm, and fly datasets, respectively), compared with those obtained by iNuc-PseKNC (≈ 0.925, ≈ 0.935 and ≈ 0.874) ( Guo  et al. , 2014 ) ( Fig. 2 ). Thus, we expected that 3LS model-based predictor with appropriate coefficients ( Supplementary Table S2a ) can predict the nucleosome positioning more accurately than the recent iNuc-Pse predictors for human and fly genomes. Fig. 2. ROC curves obtained with the jackknife tests using human, worm, and fly genome datasets (Color version of this figure is available at  Bioinformatics  online.) 
 Table 1. The prediction quality of 3LS model-based predictor measured using jackknife tests Human Worm Fly ACC 0.9001 (0.8627 a , 0.8760 b ) 0.8786 (0.8690 a , 0.8862 b ) 0.8341 (0.7997 a , 0.8167 b ) Sn 0.9169 (0.8786 a , 0.8931 b ) 0.8654 (0.9030 a) , 0.9162 b ) 0.8407 (0.7831 a , 0.7976 b ) Sp 0.8835 (0.8470 a , 0.8591 b ) 0.8921 (0.8355  a , 0.8666 b ) 0.8274 (0.8165 a , 0.8361 b ) MCC 0.8006 (0.73 a , 0.75 b ) 0.7576 (0.74  a , 0.77 b ) 0.6682 (0.60 a , 0.63 b ) Sn, sensitivity; Sp, specificity; Acc, accuracy; MCC, Mathew’s correlation coefficient. Values in brackets are those obtained using iNuc-PseKNC a  and iNuc-PseSTNC b . 
 3.3 TNS model for yeast genome The quality of TNS model-based predictor was expected to be lower than that of 3LS model based. ACCs of TNS model were shown to be ≈ 0.8167, ≈ 0.8394 and ≈ 0.7082 for human, worm, and fly genomes, respectively. However, TNS model based predictor exhibited perfect nucleosome positioning prediction (ACC = 1.0) for the benchmark yeast genome dataset, presented in  Chen  et al.  (2015) . For the same benchmark dataset, the predictor based on DNA deformation energy ( Chen  et al. , 2015 ) had ACC of ≈ 0.981. Moreover, we confirmed that the predictor based on the nearest neighbor algorithm ( Yi  et al. , 2012 ) had ACC of ≈ 0.9906 for the same benchmark dataset. These predictors can perform sufficiently well in predicting nucleosome positioning for yeast genome. However, we expected that TNS model-based predictor with appropriate coefficients ( Supplementary Table S2b ) is able to predict the nucleosome positioning more precisely than these recent predictors. 4 Discussion 3LS model-based predictor can predict nucleosome positioning in human and fly genomes more accurately than the recently proposed nucleosome position predictors can. Additionally, the predictor defined here can display the details of organism-dependent key factors for the determination of nucleosome forming and inhibiting sequences. The chosen  S seq 1 ( ) ,   S seq 2 ( ) ,   S seq 3 ( ) ,   and   S seq 5 ( )  in 3LS model differed greatly between human, worm, and fly genomes ( Supplementary Table S1 ). This indicates there are many organism-dependent differences in the features contributing to the nucleosome formation. The coefficients of these variables, M (), D (), T () and P (), and constant value Q 0 , obtained by multiple regression analysis, clearly showed organism-dependent specificities ( Supplementary Table S2 ). These differences are presented in the following examples: (i) In 3LS models of these genomes that contained common variables, their coefficients’ signs often differed between the organisms. (ii) There were only six variables with the same signs of their coefficients between these organisms, and these were: T(0, 1, 6, = CTT or 0, 5, 6, = AAG) &gt; 0, T(0, 1, 10, = TTG or 0, 9, 10, = CAA) &gt; 0, P(TTTTT or AAAAA) &lt; 0, P(GCTTC or GAAGC) &gt; 0, P(GTGTC or GACAC) &gt; 0 and P(GGATC or GATCC) &gt;0 Poly(dA-dT) sequences, such as AAAAA sequence, are known as physically rigid sequences (Brunkner  et al. , 1995;  Nelson  et al. , 1987 ;  Packer  et al. , 2000 ). Therefore, the sequences containing these motives inhibit the nucleosome formation in the genomes of several organisms, which was confirmed by experimental evidence and the use of different nucleosome positioning predictors ( Bi  et al. , 2004 ;  Giancarlo  et al. , 2015 ;  Kunkel and Martinson, 1981 ;  Yi  et al. , 2012 ), which is consistent with the results presented here. The sequences with high GC content were reported to have a nucleosome-forming tendency ( Tillo and Hughes, 2009 ). However, considering the results of the recent studies, 30–50% nucleotides found in the nucleosome forming sequences are A or T nucleotides located at the appropriate positions ( Giancarlo  et al. , 2015 ;  Ioshikhes  et al. , 2006 ;  Ogawa  et al. , 2010 ;  Ohyama 2001 ;  Satchwell  et al. , 1986 ;  Segal  et al. , 2006 ), which seems to agree with the results obtained in this study. (iii) When only the six variables described above were chosen in 3LS model based predictor, ACCs for human, worm, and fly genomes were ACC ≈ 0.7525, ≈ 0.7716 and 0.6438, respectively, which is much lower than the values obtained using the model with suitable variables. However, even when these variables were removed from the 3LS model based predictor with suitable variables, the decrease in ACCs for human, worm and fly genomes was not considerable, and the obtained ACC values were ≈ 0.8974, ≈ 0.8730 and ≈ 0.8290, respectively. This indicates that the organism-specific sequence patterns dominantly contribute to the determination of nucleosome forming abilities. (iv) The weight of the contribution of the set  S seq 3 ( 0 ,   j ,   k , = 3 − nuc   or   0 ,   k − j ,   k , = 3 − nuc * )  for each k is defined as W k = [Number of chosen S seq 3 ( 0 ,   j ,   k , = 3 - nuc   or   0 ,   k − j ,   k , = 3 - nuc * ) ]/[Number of chosen variables] ( Table 2 ). The obtained W k  values were different for different organisms, e.g. W 5  ∼ 0.074, 0.112, 0.080 (k = 5 as the smallest k) and W 10  ∼ 0.159, 0.115, 0.151 (k = 10 as the largest k) were obtained for human, worm, and fly genomes, respectively. This indicates that the length scale of nucleotide combinations required for the characterization of nucleosome forming sequences depends on the organism analyzed.
 Table 2. Weights of the contributions of  S seq 3  (0, j, k, = 3-nuc or 0, k − j, k, = 3-nuc*) for each k (W k ) and  S seq 2  (i’ | Di-seq or Di-seq*) for the positions near and far from the dyad position (W near  and W far ) Human Worm Fly W 5 0.074441687 0.112244898 0.08 W 6 0.094292804 0.068877551 0.089230769 W 7 0.069478908 0.073979592 0.098461538 W 8 0.11662531 0.114795918 0.083076923 W 9 0.1191067 0.135204082 0.12 W 10 0.158808933 0.114795918 0.150769231 W near 0.027295285 0.025510204 0.027692308 W far 0.027295285 0.015306122 0.006153846 (v) The weight of the contribution of the set  S seq 2  (i’ | Di-seq or Di-seq*) near and far from the center of sequence (dyad position) was defined as W near  = [Number of chosen S seq 2  (i’ | Di-seq or Di-seq*) near and far from the center of sequence (dyad position) was defined as W near  = [Number of chosen S seq 2 ( i ’ = 1   ∼   5 |   Di − seq   or   Di − seq * )]/[Number of chosen variables] and W far  = [Number of chosen S seq 2 ( i ’ = 6   ∼   8 |   Di − seq   or   Di − seq * ) ]/[Number of chosen variables] ( Table 2 ). W near  values were similar values in the datasets for the 3 investigated organisms. The values of W far , ≈ 0.027, 0.015 and 0.006, were obtained for human, worm, and fly genomes, respectively, where W far  for fly was shown to be ∼1/2 of that for worm and ∼1/4 for human. This suggests that the contribution of the sequences far from the dyad position to the nucleosome formation depends on the organism type. Using the TNS model-based predictor, the obtained ACC values of nucleosome position predictions for human, worm, and fly genomes were much lower than those obtained using 3LS model-based predictor. while ACC = 1 was obtained for yeast genome. This clearly demonstrates organism-dependent characteristics of nucleosome forming and inhibiting sequences, showing that the nucleosome positioning is much more easily predicted in yeast than in higher organisms. The predictors developed here can predict nucleosome positioning in human, fly and yeast genomes with higher accuracy than the recently proposed predictors and can determine the key factors influencing this positioning in human, worm, fly and yeast genomes. In contrast to the recently proposed iNuc-Pse predictors, 3LS model-based predictor developed in this study is based on the following sequence properties as well: (i) Combinations of nucleotides located further away than those considered by iNuc-Pse predictors; (ii) More detailed distributions of A, T and dinucleotide sequences in a DNA fragment than those in iNuc-Pse predictors. These properties most likely contribute to the exhibited improved performance of the predictor proposed here in comparison with the iNuc-Pse predictors. However, the variable selections and the formalization of the model can be improved, and further modifications are needed for this predictor to perform better than the recent ones. Recent studies suggested that sequence-dependent geometry and flexibility of each DNA fragment may play important roles in the determination of its nucleosome forming ability ( Chen  et al. , 2012a ,  2015 ;  Freeman  et al. , 2014 ;  Goñi  et al. , 2008 ;  Guo  et al. , 2014 ;  Isami  et al. , 2015 ;  Nikolaou  et al. , 2010 ;  Stolz and Bishop, 2010 ;  Tolstorukov  et al. , 2008 ;  Yuan and Liu, 2008 ). Furthermore, the nucleosome forming ability of each sequence may change with intracellular and environmental conditions ( Andreu-Vieyra  et al. , 2011 ;  He  et al. , 2010 ;  Maston  et al. , 2012 ;  McPherson  et al. , 1996 ;  Struhl and Segal, 2013 ;  Zhang  et al. , 2012 ). Because of this, the predictors should be modified in the future by considering these physical and chemical influences. Additionally, as demonstrated in a series of recent publications (e.g.  Chen  et al. , 2014b ,  2016 ;  Jia  et al. , 2015 ;  Lin  et al. , 2014 ;  Liu  et al. , 2015b ;  Qiu  et al. , 2016b ), during the development of new prediction methods, user-friendly and publicly accessible web-servers can significantly enhance the impacts of these tools ( Chou, 2015 ). Therefore, the future efforts will include providing a web-server for the use of the prediction method presented here. Supplementary Material Supplementary Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Repitools: an R package for the analysis of enrichment-based epigenomic data</Title>
    <Doi>10.1093/bioinformatics/btq247</Doi>
    <Authors>Statham Aaron L., Strbenac Dario, Coolen Marcel W., Stirzaker Clare, Clark Susan J., Robinson Mark D.</Authors>
    <Abstract>Summary: Epigenetics, the study of heritable somatic phenotypic changes not related to DNA sequence, has emerged as a critical component of the landscape of gene regulation. The epigenetic layers, such as DNA methylation, histone modifications and nuclear architecture are now being extensively studied in many cell types and disease settings. Few software tools exist to summarize and interpret these datasets. We have created a toolbox of procedures to interrogate and visualize epigenomic data (both array- and sequencing-based) and make available a software package for the cross-platform R language.</Abstract>
    <Body>1 INTRODUCTION Epigenetics is the study of the phenotypic changes unrelated to DNA sequence. Epigenomics is the large-scale study of epigenetics, with various genome-wide assays having been introduced in the past few years and with many epigenome mapping projects on the horizon (Jones  et al. ,  2008 ; Nature editorial,  2010 ). DNA methylation is one of the best studied epigenetic marks and can be assayed genome-wide using restriction enzyme, bisulphite or enrichment-based approaches (reviewed in Laird,  2010 ). Another significant class of epigenetic regulators is histone modifications, typically studied using chromatin immunoprecipitation (ChIP) in combination with microarrays (ChIP-chip) or next-generation sequencing (ChIP-seq). There are limited general tools available for the exploratory analysis and summarization of enrichment-based epigenomics data (see Table 3 of Laird,  2010 ). We present  Repitools , a software package for the R environment that is focused on the analysis of enrichment-based epigenomic data. Examples are shown to illustrate the diversity of tools within the package; many further examples can be found in the comprehensive user's guide. The routines have been tested on Affymetrix and Nimblegen tiling microarrays and Illumina Genome Analyzer sequencing data; generic data types are used so that other platforms can be easily supported. 2 DATA SUMMARIZATION Various procedures for visualization are available within the package. For example,  enrichmentPlot  displays the distribution of enrichment across the whole genome for sequencing-based experiments.  cpgBoxplots  and  cpgDensityPlot  display microarray and sequencing results, respectively, for quality assessment of DNA methylation enrichment experiments.  Figure 1 A illustrates the  cpgDensityPlot  of a successful methylated DNA enrichment experiment using MethylMiner™ (Invitrogen, Carlsbad CA, USA) where, as expected, the CpG density of the enriched DNA population is heavily skewed to the right compared to the input DNA control.
 Fig. 1. Repitools  visualization examples. ( A ) In  cpgDensityPlot , each line is a single experiment's read distribution in terms of CpG density. ( B ) For  binPlots , the middle panel displays a heatmap of summarized signal according to 50 expression level bins (rows), organized into 100 bp locations (columns) within promoters. The left panel gives the enrichment colour scale and the right panel displays the gene expression for each bin. ( C ) For  significancePlots , the purple and red lines illustrate the median signal for the gene sets of interest. The blue line represents median signal of all remaining genes in the genome, while the blue shading illustrates a 95% confidence interval (example data taken from Coolen  et al.  ( 2010 ). 
 We have provided many ways to visualize and summarize promoter-level microarray or genome-wide epigenomic data. For example, given a table of annotation, the  binPlots  function summarizes median signal across points of interest (e.g. transcription start sites). We routinely use  binPlots  as a quality control step of new ChIP experiments where there is a previously known relationship between the interrogated chromatin mark and another metric, commonly gene expression. For example,  Figure 1 B clearly illustrates the positive association between gene expression levels (Affymetrix Gene 1.0 ST data) and the occurrence of H3K9 acetylation in the proximity of the corresponding promoters (Affymetrix Promoter 1.0R data). The routine handles tiling array or sequencing data as inputs, can accept alternative rankings for grouping and the display can be a plot with multiple lines, a heatmap or a 3D visualization. Another useful strategy for summarizing sets of genes of interest is  significancePlots . As illustrated in  Figure 1 C,  significancePlots  shows the distinct methylated DNA enrichment changes associated with genes whose expression is up- or down-regulated &gt;2-fold between two samples, and how the profiles differ between array and high-throughput sequencing readout. For the comparison, a large number of random gene sets are taken to form the profile null distribution; median and confidence intervals are plotted. These plots show evidence that there is a clear enrichment of sequencing reads and hence, DNA methylation surrounding many genes are down-regulated in this comparison. Further data summaries are regularly added. 3 STATISTICAL PROCEDURES The visualization procedures detailed above aggregate signal over a large number of promoters or regions of the genome. Often, it is of interest to focus on specific regions of the genome and summarize the signal observed at these regions (e.g. transcription start sites, exons, etc.). For example, an experimenter may be interested in promoter-level summaries of a particular epigenetic mark. The general purpose  blocksStats  procedure focuses on data for the specified genomic regions of interest. For microarray data, this involves the calculation of a probe-level score and applying a statistical test to the groups of probes within a specified distance from the region of interest. For sequencing data, we calculate statistics on aggregated read counts around the features of interest. Further details are available in the accompanying user's guide. We also have procedures for untargeted analysis of epigenomic tiling array data. The  regionStats  function searches for a persistent change in signal in an untargeted fashion, similar in principle to model-based analysis of tiling arrays (Johnson  et al. ,  2006 ), and therefore not relying upon annotation. Analogous procedures for sequencing data are in development. 4 ACCESSORY TOOLS The package contains a number of useful tools in the spectrum of epigenomics. For example, in the context of CpG methylation, microarray probes or sequence reads are often affected by the local CpG density of the regions being interrogated.  cpgDensityCalc  is a procedure to calculate local CpG density according to a previous definition (Pelizzola  et al. ,  2008 ).  annotationLookup  provides a framework for relating annotation (e.g. transcription start sites) information to probe positions on a tiling array.  multiHeatmap  is a general tool for creating adjacent heatmaps using separate colour scales. Additional included tools exist to access Nimblegen array quickly (e.g.  readPairFile ), access features of aroma.affymetrix objects (e.g.  getProbePositionsDf ) and aggregate sequencing reads according to proximity to annotation (e.g.  annotationCounts ). We expect further tools to be added and encourage others in the epigenomic community to contribute generally useful procedures. 5 DISCUSSION There are relatively few tools currently available for the analysis of epigenomic data. We have developed  Repitools , a software package for the R environment; it contains many useful functions for quality assessment, visualization, summarization and statistical analysis of epigenomics experiments. The package makes use of aroma.affymetrix and several Bioconductor packages for various preprocessing steps (Bengtsson  et al. ,  2008 ; Gentleman  et al. ,  2004 ) and may require an intermediate understanding of R for some features. A comprehensive user manual is available and examples can be run using supplied data. The analysis of large Affymetrix tiling array datasets is facilitated through the memory efficiency afforded by the aroma.affymetrix package (Bengtsson  et al. ,  2008 ). Funding : National Health and Medical Research Council (NH&amp;MRC) project (427614, 481347) (M.D.R., C.S., D.S.) and Fellowship (S.J.C.), Cancer Institute NSW grants (CINSW: S.J.C., M.W.C., A.L.S.), and NBCF Program Grant (S.J.C.). Conflict of Interest : none declared. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>DiscoRhythm: an easy-to-use web application and R package for discovering rhythmicity</Title>
    <Doi>10.1093/bioinformatics/btz834</Doi>
    <Authors>Carlucci Matthew, Kriščiūnas Algimantas, Li Haohan, Gibas Povilas, Koncevičius Karolis, Petronis Art, Oh Gabriel, Kelso Janet</Authors>
    <Abstract/>
    <Body>1 Introduction Biological rhythmicity, including circadian and other cycles ( Schibler and Naef, 2005 ), are fundamentally important to life on Earth ( Bell-Pedersen  et al. , 2005 ). Numerous lines of evidence have indicated that disturbance of biological rhythms is a risk factor for human morbidities, including psychiatric, metabolic and malignant diseases ( Roenneberg and Merrow, 2016 ). Several approaches can be used to estimate the phase, amplitude and statistical significance of these rhythms in time-series data, where each methodology has strengths and weaknesses under various conditions ( Deckard  et al. , 2013 ). Current implementations of these algorithms in R (e.g. JTK Cycle, ARSER and Lomb-Scargle) tend to be slow and difficult to use. Additionally, no unified toolkit exists for performing pre-processing, dimensionality reduction, period detection and visualization of oscillation statistics, all of which require specialized knowledge and expertise. To address these challenges, we developed  DiscoRhythm  ( Disco vering  Rhythm icity), a web application and accompanying R/Bioconductor package for analyzing rhythmicity in temporal biological datasets.  DiscoRhythm  provides a unified interface to execute four methods of rhythm estimation, and heuristically selects suitable approaches for the data being analyzed. By providing interactive modules for outlier detection, analysis of replicates and periodicity profiling,  DiscoRhythm  offers a framework for accessible analysis of periodic datasets in a web browser or in R. 2 Results 
 DiscoRhythm  is implemented as a package in the R programming language (ver. 3.6+) with the web interface based on the R Shiny platform ( Chang  et al. , 2018 ), capable of reproducing findings in transcriptomic ( Li  et al. , 2013 ), epigenomic ( Oh  et al. , 2019 ), metabolomic ( Krishnaiah  et al. , 2017 ), proteomic ( Hurley  et al. , 2018 ) and other similar datasets. A workflow in  DiscoRhythm  begins with a matrix of temporal data ( Fig. 1 ), where two metrics are computed to filter outlier samples, followed by a feature selection procedure based on the ratio of biological signal to technical noise. Dominant periods are determined using dataset-wide period evaluation procedures, and finally, multiple rhythm detection methods are executed on each feature to infer the presence of rhythms. Results of the web session may be emailed or downloaded upon completion as a zip file also containing the R data and code required for future reproducibility. Fig. 1. Overview of the analysis procedures performed by  DiscoRhythm . The illustration shows the step-wise operations being performed on the input circadian data matrix. All procedures are performed on a matrix with the biological features represented by rows and temporal samples by columns. Columns with the same color (e.g. purple and yellow) represent technical replicates, while stripped boxes indicate samples or features removed for downstream analysis. Red and gray oscillating lines show significant and non-significant rhythms, respectively. hr, hours 2.1 Input Input for the web interface is a single table in a CSV (comma separated values) format. Columns contain samples named according to their time of collection, and rows contain values of observed features. Experimental design specifications regarding technical replicates, units of time and the main period of interest are also required. A circadian gene expression dataset simulated using simphony ( Singer  et al. , 2019 ) is provided in order to highlight the available features and demonstrate the sample naming scheme. In addition to the tabular input of the graphical interface, the  DiscoRhythm  R package also accepts SummarizedExperiment objects commonly returned by other R packages in Bioconductor ( Gentleman  et al. , 2004 ). 2.2 Outlier detection and feature selection Sample quality is assessed using two commonly utilized metrics for outlier detection. The first metric is the average inter-sample correlation, computed as a mean pairwise correlation between a given sample and all other samples ( Oldham  et al. , 2008 ), while the second metric(s) is the sample score returned by principal component analysis (PCA). For both metrics, samples that deviate considerably from the rest (beyond a user-defined threshold) are flagged as outliers for removal from further analysis. If present, technical replicates can be used to determine the signal-to-noise ratio for each feature (i.e.  F  statistic of biological versus technical variation). For further analysis, the user is able to only select the features exhibiting high signal-to-noise ratio, determined either by effect size or statistical significance. Technical replicates can then be combined by taking the mean, median or by choosing one replicate at random to prevent inflated sample size stemming from non-independent measurements. 2.3 Period detection Two approaches are available for identifying the dominant period of rhythmicity. First, a goodness of fit can be evaluated for each period using a cosinor model across all selected features, returning the median coefficient of determination ( R 2 ) of the fits. Alternatively, global rhythmic patterns may be investigated using PCA scores. If ‘circular time’ is used for sample collection [e.g. time of day, in hours, is recorded over multiple days as 2, 4, …, 24, 2, 4, …, where samples with the same collection times are assumed to be biological replicates ( Hughes  et al. , 2017 )],  DiscoRhythm  will limit the candidate periods for rhythm detection to  p / k  where  p  is the length of the cycle and  k  is a positive integer ( Cornelissen, 2014 ). 2.4 Estimating rhythm characteristics Oscillations can be detected for each feature using a user-specified period. The period should be chosen by an  a priori  hypothesis or detected by the procedures in Section 2.3. An interface is provided to four commonly used approaches to oscillation detection [Cosinor ( Cornelissen, 2014 ), ARSER ( Yang and Su, 2010 ), JTK Cycle ( Hughes  et al. , 2010 ) and Lomb-Scargle ( Glynn  et al. , 2006 )]. Each is heuristically made available if the input dataset satisfies algorithm-specific criteria, such as: the presence (or absence) of missing values, biological replicates, uneven sampling frequencies or non-integer intervals. To make  DiscoRhythm  suitable for -omic-scale and real-time analysis, high-performance implementations of each algorithm were developed, with runtime improvements of up to 30-fold [ Supplementary Table S1  and  Fig. S1 ; parallelized ARSER, JTK Cycle and Lomb-Scargle were contributed directly to MetaCycle version 1.2 ( Wu  et al. , 2016 )]. Each method returns estimated phases, amplitudes, and  P -values, both raw and adjusted for multiple testing ( Benjamini and Hochberg, 1995 ). These feature-specific rhythm characteristics can be interactively visualized and downloaded for further exploration. 3 Discussion Rhythmicity is a common topic of discussion for most biological researchers. Yet the quantitative analysis is difficult and, therefore, almost exclusively performed by researchers with specialization in statistics and computation. To democratize the field of chronobiology, we developed  DiscoRhythm —a suite of standardized analytical procedures made approachable through interactivity, informative visualizations and key statistics for characterizing rhythmic patterns of temporal datasets. This new tool will enable even non-computational researchers to extract insights on the rhythmicity of biological data in a highly efficient manner. While our workflow is optimized for -omic-scale experiments, future versions of  DiscoRhythm  will also tailor to lower throughput datasets. Lastly, to maintain and extend accessibility to relevant periodic analysis approaches, we plan to adopt new methods as they become available in R/Bioconductor. Supplementary Material btz834_Supplementary_Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>SPRING: a kinetic interface for visualizing high dimensional single-cell expression data</Title>
    <Doi>10.1093/bioinformatics/btx792</Doi>
    <Authors>Weinreb Caleb, Wolock Samuel, Klein Allon M, Berger Bonnie</Authors>
    <Abstract/>
    <Body>1 Introduction Recent advances in single-cell RNA sequencing (scSeq) have made it possible to catalog the expression of every gene in every cell from a given sample with reasonable accuracy. There is now a need for computational tools to explore and visualize this high-dimensional data, and in particular to capture the continuous trajectories of cell in gene expression space. K-nearest-neighbor (knn) graphs have proven useful for analyzing continuous cell topologies ( Bendall  et al. , 2014 ;  Setty  et al. , 2016 ;  Xu and Su, 2015 ), and one study proposed the use of knn graphs for visualization and data clustering ( Islam  et al. , 2011 ). In a knn graph, each cell is a node that extends edges to the k other nodes with most similar gene expression. We have found that interactively exploring graph topology, overlaid with gene expression or other annotations, provides a powerful approach to uncover biological processes emerging from data. However, at present there are no publicly available tools for interactive visualization of scSeq data in a graph format. Here, we present a user-friendly web tool called SPRING. To use the tool, users must supply a table of gene expression measurements for single-cells and can optionally upload additional annotations. SPRING builds a knn graph from this data and displays the graph using a force-directed layout algorithm that renders as a real-time simulation in an interactive viewing window. We include a set of features for open-ended data exploration, including interactive discovery of marker genes; gene expression comparisons between different sub-populations and selection tools for isolating sub-populations of interest. SPRING is compatible with all major web browsers and does not require technical knowledge to operate. 2 Materials and methods To generate the knn graph, SPRING performs the following transformations to the inputted gene expression matrix. All parameters labeled ‘X’ in this section can be adjusted using an interactive web form. (1) Filter all cells with fewer than X reads; (2) cell normalization so that every cell has the same total reads; (3) filter genes with ¡ X mean expression or &lt; X coefficient of variation; (4) Z-score normalize expression values for each gene; (5) perform principal components analysis, keep the top X principal components and (6) compute a distance matrix and output a knn-graph with k = X. One can also conceive of other choices for each step of filtering, normalization, dimensionality reduction and distance metric used. SPRING is demonstrated in two examples in  Figure 1 . The underlying datasets are being published in separate research papers (in submission), and will be available at  https://kleintools.hms.harvard.edu/tools/spring.html .
 Fig. 1. ( A ) SPRING depicts the dynamic trajectories of hematopoietic progenitor cells as they differentiate from stem cells (HSCs; black circle) into each of seven lineages (colored arms; lineage identities are described in a separate publication, in submission). In contrast, tSNE ( B ) and diffusion map ( C ) visualizations of the same data show disconnected clusters of cells or do not capture the full complexity of the data in two dimensions. ( D ) SPRING and tSNE plots of upper airway epithelium cells from three human donors highlight the reproducibility of SPING visualizations. Cells in (A–D) are colored by marker gene scores. Detailed methodology for producing all plots is available in the  Supplementary Material The SPRING GUI is currently configured for datasets up to 10 000 cells and becomes very slow for larger datasets because of poor scalability of the graph rendering method and the computational burden of computing the force layout. In principle, these can be improved, for example by using the ForceAtlas2 algorithm ( Jacomy  et al. , 2014 ). In the meantime, large datasets can be accommodated by coarse-graining cells. A procedure to do so is described in the  Supplementary Material  and shown for an example dataset in  Supplementary Figure S5 . We provide code for coarse-graining on the github page. 3 Advantages over existing methods 3.1 Continuous expression topologies In contrast to the commonly used method tSNE ( Amir  et al. , 2013 ), SPRING captures the long-distance relationships between cells and can, therefore, visualize continuous expression topologies. For example, SPRING accurately maps the branching topology of hematopoietic progenitor cells as they differentiate along seven lineages ( Fig. 1A ). Though a diffusion map ( Haghverdi  et al. , 2015 ) visualization ( Fig. 1C ) can usually capture continuous gene expression trajectories, it often requires more than two diffusion components to distinguish all lineages, preventing a full representation of the data complexity in a single two dimensional plot. 3.2 Graph invariance One drawback of tSNE is that it is stochastic and, therefore, not perfectly reproducible. In contrast, graph construction in SPRING is non-stochastic and, therefore, yields consistent topologies between runs and replicates. In addition, manual interaction with the kinetic SPRING interface allows users to bring plots from separate replicates into register with one other ( Fig. 1D ). 4 Conclusion Single-cell gene expression profiling is becoming a common tool to dissect cellular heterogeneity and characterize dynamic processes such as differentiation. Interactive visualization tools can help researchers exploit this data more fully. Our easy-to-use web tool, SPRING, provides a simple interface for open-ended investigation of gene expression topology. Funding C.W. and S.W. are supported by NIH training grant [5T32GM080177-07] and A.M.K. by NIH Grant 1R33CA212697, a Burroughs-Wellcome Career Award at the Scientific Interface, and by an Edward J Mallinckrodt Foundation Fellowship. 
 Conflict of Interest : none declared. Supplementary Material Supplementary Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>A System for Information Management in BioMedical Studies—SIMBioMS</Title>
    <Doi>10.1093/bioinformatics/btp420</Doi>
    <Authors>Krestyaninova Maria, Zarins Andris, Viksna Juris, Kurbatova Natalja, Rucevskis Peteris, Neogi Sudeshna Guha, Gostev Mike, Perheentupa Teemu, Knuuttila Juha, Barrett Amy, Lappalainen Ilkka, Rung Johan, Podnieks Karlis, Sarkans Ugis, McCarthy Mark I, Brazma Alvis</Authors>
    <Abstract>Summary: SIMBioMS is a web-based open source software system for managing data and information in biomedical studies. It provides a solution for the collection, storage, management and retrieval of information about research subjects and biomedical samples, as well as experimental data obtained using a range of high-throughput technologies, including gene expression, genotyping, proteomics and metabonomics. The system can easily be customized and has proven to be successful in several large-scale multi-site collaborative projects. It is compatible with emerging functional genomics data standards and provides data import and export in accepted standard formats. Protocols for transferring data to durable archives at the European Bioinformatics Institute have been implemented.</Abstract>
    <Body>1 INTRODUCTION The growing use of high-throughput technologies in biomedical studies and the volume and complexity of data generated in such experiments have created a need for dedicated software systems to collect, store and manage these data. Moreover, essential information about biomedical research subjects (patients) and samples have to be recorded and linked to the data. Projects are often collaborative, include many researchers and laboratories and may be spread across different sites. Personal information must be managed in a secure manner, the data access rights should be consistent with ethical requirements. Generic laboratory information management systems are not always appropriate for these purposes. The existing open source software systems (e.g. Reich  et al. ,  2006 ; Saal  et al. ,  2002 ; Saeed  et al. ,  2003 ) have been primarily designed for use in a single laboratory. To address these issues, we have developed a web-based System for Information Management in BioMedical Studies—SIMBioMS. It was originally implemented for needs of a particular multi-site project (MolPAGE,  www.molpage.org ). Since later it proved to be easily customizable and scalable for other applications, including population genomics studies, we generalized the system as open source software. SIMBioMS provides an interface for data entry via web forms, upload facilities of pre-formatted datasets from files, data export facilities (including configurable export definable by XML templates) as well as advanced data access and user rights management. The system can be configured to support the minimum information requirements MIBBI (Taylor  et al. ,  2008 ), data can be imported/exported in accepted standard formats MAGE-TAB (Rayner  et al. ,  2006 ) and ISA-TAB (Sansone  et al. ,  2008 ), as well as custom-made XML and tab-delimited formats, allowing for easy data import and export from users' own tools, and generic tools such as Excel. A simple browsing and customizable data filtering options allow for the essential content exploration and report construction on metadata level. Selected data can be imported into analysis tools, such as Bioconductor. 2 SYSTEMS DESIGN, IMPLEMENTATION AND CUSTAMIZATION The system consists of two components—Sample Information Management System (SIMS) and Assay data and Information Management System (AIMS) ( Fig. 1 ). As the names suggest, SIMS is designed to collect phenotypical, environmental and technical information about samples, while AIMS handles the experimental data from high-throughput assays. SIMS provides a simple solution for data anonymization by creating identifiers linked to person's information in a separate module. SIMS extends a previously published system (PASSIM; Viksna  et al. ,  2007 ). The main new features include customizability and compatibility with data formats MAGE-TAB and ISA-TAB. While, PASSIM was designed to manage patient and sample data, it did not have any means for linking it to data from high-throughput assays.
 Fig. 1. High level class diagram of SIMS and AIMS. 
 AIMS is a new system filling this gap, designed for adoptability for any technological platform, and for easy extraction of captured data for analysis. It is linked to SIMS through a three-level hierarchy: a person can be linked to one or more samples, a sample can have one or more aliquots. Each aliquot can have one or more assays performed on it, and each assay can be linked to one or more data files. Assays are grouped in experiments and studies, each of which can have one or more data files attached. For instance, raw microarray data files would be normally linked to individual assays, while normalized gene expression matrices to experiments. Assays are technology-specific; the current AIMS configurations include genotyping, sequencing, proteomics and metabonomics. The two systems can be installed and used independently, or jointly—if a laboratory already has a local informatics system for sample or assay data, it can be used jointly with AIMS or SIMS, respectively. SIMBioMS run in Apache Tomcat servlet containers, or other application servers. The data are stored in PostgreSQL databases, but other popular database management systems have been tested and can be used with minimal changes. The systems are platform independent, and have been tested on several MS Windows and Linux. Several preconfigured versions, including ones for type 2 diabetes, metabolic syndrome and autoimmune diseases are packed into .war web-application archives. AIMS/SIMS can be installed either as local (e.g. on a laptop) or as centralized databases. Installation for local use is a simple two-step procedure that does not require special database software (java light database h2 is used). Filtering functionality is customizable, for enumerated value fields a drop-down list can be provided, fields are defined as parameters. 3 RESULTS AND DISCUSSION The systems development effort up to now is ∼8 person-years. To the best of our knowledge, this is the only open source web-based system that integrates capturing of rich phenotypic data with management of high-throughput data from multiple platforms for needs of multi-site collaborative projects and that has already proven its usefulness. We are currently running three SIMBioMS instances to support collaborative projects, including an instance containing data from over 25 000 assays on nine different technology platforms, and an instance for population-wide epidemiology studies. We have implemented protocols for data transfer to the permanent data archives: ArrayExpress (Parkinson  et al. ,  2008 ) and European Genotype Archive (EGA) and data from over 6500 assays have been transferred. In the future, the system will be extended to include next-generation sequencing data. Source code, documentation, initialization scripts, templates for metadata configuration, links to demo instances and user guide are available at  http://simbioms.org . </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Information-theoretic evaluation of predicted ontological annotations</Title>
    <Doi>10.1093/bioinformatics/btt228</Doi>
    <Authors>Clark Wyatt T., Radivojac Predrag</Authors>
    <Abstract>Motivation: The development of effective methods for the prediction of ontological annotations is an important goal in computational biology, with protein function prediction and disease gene prioritization gaining wide recognition. Although various algorithms have been proposed for these tasks, evaluating their performance is difficult owing to problems caused both by the structure of biomedical ontologies and biased or incomplete experimental annotations of genes and gene products.</Abstract>
    <Body>1 INTRODUCTION Ontological representations have been widely used in biomedical sciences to standardize knowledge representation and exchange ( Robinson and Bauer, 2011 ). Modern ontologies are typically viewed as graphs in which vertices represent terms or concepts in the domain of interest, and edges represent relational ties between terms (e.g. is-a, part-of). Although, in theory, there are no restrictions on the types of graphs used to implement ontologies, hierarchical organizations, such as trees or directed acyclic graphs, have been frequently used in the systematization of biological experiments, organismal phenotypes or structural and functional descriptions of biological macromolecules. In molecular biology, one of the most frequently used ontologies is the Gene Ontology (GO) ( Ashburner  et al. , 2000 ), which standardizes the functional annotation of genes and gene products. The development of GO was based on the premise that the genomes of all living organisms are composed of genes whose products perform functions derived from a finite molecular repertoire. In addition to knowledge representation, GO has also facilitated large-scale analyses and automated annotation of gene product function ( Radivojac  et al. , 2013 ). As the rate of accumulation of uncharacterized sequences far outpaces the rate at which biological experiments can be carried out to characterize those sequences, computational function prediction has become increasingly useful for the global characterization of genomes and proteomes as well as for guiding biological experiments via prioritization ( Rentzsch and Orengo, 2009 ;  Sharan  et al. , 2007 ). The growing importance of tools for the prediction of GO annotations, especially for proteins, presents the problem of how to accurately evaluate such tools. First, because terms can automatically be associated with their ancestors in the GO graph, the task of an evaluation procedure is to compare the predicted graph with the true experimental annotation. Furthermore, the structure of the ontology introduces dependence between terms, which must be appropriately considered when comparing two graphs. Second, GO, as most current ontologies, is generally unfinished and contains a range of specificities of functional descriptions at the same depth of the ontology ( Alterovitz  et al. , 2010 ). Third, protein function is complex and context dependent; thus, a single biological experiment rarely results in complete characterization of a protein’s function. This is particularly evident in cases when only high-throughput experiments are used for functional characterization, leading to shallow annotation graphs. This poses a problem in evaluation, as the ground truth is incomplete and noisy. Finally, different computational models produce different outputs that must be accounted for. For example, some models simply predict an annotation graph, possibly associating it with a numerical score, whereas others assign a score to potentially each node in the ontology, with an expectation that a good decision threshold would be applied to provide useful annotations. There are two important factors related to the development of evaluation metrics. First, because both the experimental and predicted annotation of genes can be represented as subgraphs of the generally much larger GO graph, it is unlikely that a given computational method will provide an exact prediction of the experimental annotation. Thus, it is necessary to develop metrics that facilitate calculating degrees of similarity between pairs of graphs and appropriately address dependency between nodes. Ideally, such a measure of similarity would be able to characterize not only the level of correct prediction of the true (albeit incomplete) annotation but also the level of misannotation. The second important factor related to the evaluation metric is its interpretability. This is because characterizing the predictor’s performance should be meaningful to a downstream user. Ideally, an evaluation metric would have a simple probabilistic interpretation. In this article, we develop an information-theoretic framework for evaluating the prediction accuracy of computer-generated ontological annotations. We first use the structure of the ontology to probabilistically model, via a Bayesian network, the prior distribution of protein experimental annotation. We then apply our metric to three protein function prediction algorithms selected to highlight the limitations of typically considered evaluation metrics. We show that our metrics provide added value to the current analyses of the strengths and weaknesses of computational tools. Finally, we argue that our framework is probabilistically well founded and show that it can also be used to augment already existing evaluation metrics. 2 BACKGROUND The issue of performance evaluation is closely related to the problems of measuring similarity between pairs of graphs or sets. First, we note that a protein’s annotation (experimental or predicted) is a graph containing a subset of nodes in the ontology together with edges connecting them. We use the term  leaf node  to describe a node that has no descendants in the annotation graph, although it is allowed to have descendants in the ontology. A set of leaf terms completely describes the annotation graph. We roughly group both graph similarity and performance evaluation metrics into topological and probabilistic categories and note that a particular metric may combine aspects from both. More elaborate distinctions are provided by  Guzzi  et al.  (2012)  and  Pesquita  et al.  (2009) . Topological metrics rely on the structure of the ontology to perform evaluation and typically use metrics that operate on sets of nodes and/or edges. A number of topological measures have been used, including the Jaccard and cosine similarity coefficients (the cosine approach initially maps the binary term designations into a vector space), the shortest path-based distances ( Rada  et al. , 1989 ) and so forth. In the context of classifier performance analysis, two common 2D metrics are the precision/recall curve and the Receiver Operating Characteristic (ROC) curve. Both curves are constructed based on the overlap in either edges or nodes between true and predicted terms and have been widely used to evaluate the performance of tools for the inference of GO annotations. They can also be used to provide a single statistic to rank classifiers through the maximum F-measure in the case of precision/recall curve or the area under the ROC curve. The area under the ROC curve has a limitation arising from the fact that the ontology is relatively large, but that the number of terms associated with a typical protein is relatively small. In practice, this results in specificities close to one, regardless of the prediction, as long as the number of predicted terms is relatively small. Although these statistics provide good feedback regarding multiple aspects of a predictor’s performance, they do not always address node dependency or the problem of unequal specificity of functional annotations found at the same depth of the graph. Coupled with a large bias in the distribution of terms among proteins, prediction methods that simply learn the prior distribution of terms in the ontology could appear to have better performance than they actually do. The second class of similarity/performance measures is probabilistic or information-theoretic metrics. Such measures assume an underlying probabilistic model over the ontology and use a database of proteins to learn the model. Similarity is then assessed by measuring the information content of the shared terms in the ontology but can also take into account the information content of the individual annotations. Unlike with topological measures where updates to the ontology affect similarity between objects, information-theoretic measures are also affected by changes in the underlying probabilistic model even if the structure of the ontology remains the same. Probabilistic metrics closely follow and extend the methodology laid out by  Resnik (1995) , which is based on the notion of information content between a pair of individual terms. These measures overcome biases related to the structure of the ontology; however, they have several drawbacks of their own. One that is especially important in the context of analyzing the performance of a predictor is that they only report a single statistic, namely, the similarity or distance between two terms or sets of terms. This ignores the tradeoff between precision and recall that any predictor has to make. In the case of Resnik’s metric, a prediction by any descendant of the true term will be scored as if it is an exact prediction. Similarly, a shallow prediction will be scored the same as a prediction that deviates from the true path at the same point, regardless of how deep the erroneous prediction might be. Although some of these weaknesses have been corrected in subsequent work ( Jiang and Conrath, 1997 ;  Lin, 1998 ;  Schlicker  et al. , 2006 ), there remains the issue that the available probabilistic measures of semantic similarity resort to  ad hoc  solutions to address the common situation where proteins are annotated by graphs that contain multiple leaf terms ( Clark and Radivojac, 2011 ). Various approaches have been taken, including averaging between all pairs of leaf terms ( Lord  et al. , 2003 ), finding the maximum among all pairs ( Resnik, 1999 ) or finding the best-match average, but each such solution lacks strong justification in general. For example, all-pair averaging leads to anomalies where the exact prediction of an annotation containing a single leaf term  u  would be scored higher than the exact prediction of an annotation containing two distinct leaf terms  u  and  v  of equal information content, when it is more natural to think that the latter prediction should be scored higher. Finally, certain semantic similarity metrics that incorporate pairwise matching between leaf terms tacitly assume that the objects to be compared are annotated by similar numbers of leaf terms. As such, they could produce undesirable solutions when applied to a wide range of prediction algorithms such as those outputting a large number of predicted terms. 3 METHODS Our objective here is to introduce information-theoretic metrics for evaluating classification performance in protein function prediction. In this learning scenario, the input space   represents proteins, whereas the output space   contains directed acyclic graphs describing protein function according to GO. Because of the hierarchical nature of GO, both experimental and computational annotations need to satisfy the  consistency requirement , i.e. if an object   is assigned a node (functional term)  v  from the ontology, it must also be assigned all of the ancestors of  v  up to the root(s). Therefore, the task of a classifier is to assign the best consistent subgraph of the ontology to each new protein and output a prediction score for this subgraph and/or each predicted term. We only consider consistent subgraphs as descriptions of function and simplify the exposition by referring to such graphs as prediction or annotation graphs. In addition, we frequently treat consistent graphs as sets of nodes or functional terms and use set operations to manipulate them. We now proceed to provide a definition for the information content of a (consistent) subgraph in the ontology. Then, using this definition, we derive information-theoretic performance evaluation metrics for comparing pairs of graphs. 3.1 Calculating the information content of a graph Let each term in the ontology be a binary random variable and consider a fixed but unknown probability distribution over   and   according to which the quality of a prediction process will be evaluated. We shall assume that the prior distribution of a target can be factorized according to the structure of the ontology, i.e. we assume a Bayesian network as the underlying data generating process for the target variable. According to this assumption, each term is independent of its ancestors, given its parents and, thus, the full joint probability can be factorized as a product of individual terms obtained from the set of conditional probability tables associated with each term ( Koller and Friedman, 2009 ). Here, we are only interested in marginal probabilities that a protein is experimentally associated with a consistent subgraph  T  in the ontology. This probability can be expressed as
 (1) 
where  v  denotes a node in a graph and   is the set of parent nodes of  v . Here,  Equation (1)  can be derived from the full joint factorization by first marginalizing over the leaves of the ontology and then moving towards the root(s) for all nodes not in  T . The information content of a subgraph can be thought of as the number of bits of information one would receive about a protein if it were annotated with that particular subgraph. We calculate the information content of a subgraph  T  in a straightforward manner as
 
and use a base 2 logarithm as a matter of convention. The information content of a subgraph  T  can now be expressed by combining the previous two equations as
 
where, to simplify the notation, we use  ia ( v ) to represent the negative logarithm of  . Term  ia ( v ) can be thought of as the increase, or accretion, of information obtained by adding a child term to a parent term, or set of parent terms, in an annotation. We will refer to  ia ( v ) as  information accretion  (perhaps information gain would be a better term, but because it is frequently used in other applications to describe an expected reduction in entropy, we avoid it in this situation). A simple ontology containing five terms together with a conditional probability table associated with each node is shown in  Figure 1 A. Because of the graph consistency requirement, each conditional probability table is limited to a single number. For example, at node  b  in the graph, the probability   is the only one necessary because   and because   is guaranteed to be 0. In  Figure 1 B, we show a sample dataset of four proteins functionally annotated according to the distribution defined by the Bayesian network. In  Figure 1 C, we show the total information content for each of the four annotation graphs.
 Fig. 1. An example of an ontology, dataset and calculation of information content. ( A ) An ontology viewed as a Bayesian network together with a conditional probability table assigned to each node. Each conditional probability table is limited to a single number owing to the consistency requirement in assignments of protein function. Information accretion calculated for each node, e.g.  , are shown in gray next to each node. ( B ) A dataset containing four proteins whose functional annotations are generated according to the probability distribution from the Bayesian network. ( C ) The total information content associated with each protein found in panel (B); e.g.  
 . Note that   and  , although proteins with such annotation have not been observed in part (B) 3.2 Comparing two annotation graphs We now consider a situation in which a protein’s true and predicted function is represented by graphs  T  and  P , respectively. We define two metrics that can be thought of as the information-theoretic analogs of recall and precision and refer to them as remaining uncertainty and misinformation, respectively.
 Definition 1 The  remaining uncertainty  about the protein’s true annotation corresponds to the information about the protein that is not yet provided by the graph  P . More formally, we express the remaining uncertainty ( ru ) as
 
which is simply the total information content of the nodes in the ontology that are contained in true annotation  T , but not in the predicted annotation  P . In a slight abuse of notation, we apply set operations to graphs to manipulate only the vertices of these graphs. 
 Definition 2 The  misinformation  introduced by the classifier corresponds to the total information content of the nodes along incorrect paths in the prediction graph  P . More formally, the misinformation is expressed as
 
which quantifies how misleading a predicted annotation is. Here, a perfect prediction (one that achieves  P  =  T ) leads to   and  . However, both   and   can be infinite in the limit. In practice, though,   is bounded by the information content of the particular annotation, whereas   is only limited by the particular annotations a predictor chooses to return. To illustrate calculation of remaining uncertainty and misinformation, in  Figure 2 , we show a sample ontology where the true annotation of a protein  T  is determined by the two leaf terms  t 1  and  t 2 , whereas the predicted subgraph  P  is determined by the leaf terms  p 1  and   The remaining uncertainty   and misinformation   can now be calculated by adding the information accretion corresponding to the nodes circled in gray.
 Fig. 2. Illustration of calculating remaining uncertainty and misinformation, given a predicted annotation graph  P  and a graph of true annotations  T . Graphs  P  and  T  are uniquely determined by the leaf nodes  p 1 ,  p 2 ,  t 1 , and  t 2 , respectively. Nodes colored in gray represent graph  T . Nodes circled in gray are used to determine remaining uncertainty ( ru ; right side) and misinformation ( mi ; left side) between  T  and  P Finally, this framework can be used to define the similarity between the protein’s true annotation and the predicted annotation without relying on identifying an individual common ancestor between pairs of leaves (this node is usually referred to as the maximum informative common ancestor;  Guzzi  et al. , 2012 ). The information content of the subgraph shared by  T  and  P  is one such possibility; i.e.  . 
 3.3 Measuring the quality of function prediction A typical predictor of protein function usually outputs scores that indicate the strength (e.g. posterior probabilities) of predictions for each term in the ontology. To address this situation, the concepts of remaining uncertainty and misinformation need to be considered as a function of a decision threshold τ. In such a scenario, predictions with scores greater than or equal to τ are considered positive predictions, whereas the remaining associations are considered negative (if the strength of a prediction is expressed via  P -values or E-values, values lower than the threshold would indicate positive predictions). Regardless of the situation, every decision threshold results in a separate pair of values corresponding to the remaining uncertainty   and misinformation  . The remaining uncertainty and misinformation for a previously unseen protein can be calculated as expectations over the data generating probability distribution. Practically, this can be performed by averaging over the entire set of proteins used in evaluation, i.e.
 (2) 
and
 (3) 
where  n  is the number of proteins in the dataset,  T i  is the true set of terms for protein  x i , and   is the set of predicted terms for protein  x i , given decision threshold τ. Once the set of terms with scores greater than or equal to τ is determined, the set   is composed of the unique union of the ancestors of all predicted terms. As the decision threshold is moved from its minimum to its maximum value, the pairs of   will result in a curve in 2D space. We refer to such a curve using  . Removing the normalizing constant ( ) from the aforementioned equations would result in the total remaining uncertainty and misinformation associated with a database of proteins and a set of predictions. 3.3.1 Weighted metrics One disadvantage of definitions in  Equations (2)  and ( 3 ) is that an equal weight is given to proteins with low and high information content annotations when averaging. To address this, we assign a weight to each protein according to the information content of its experimental annotation. This formulation naturally downweights proteins with less informative annotations compared with proteins with rare, and therefore more informative (surprising), annotations. In biological datasets, frequently seen annotations have a tendency to be incomplete or shallow annotation graphs and arise owing to the limitations or high-throughput nature of some experimental protocols. We define  weighted remaining uncertainty  as
 (4) 
and  weighted misinformation  as
 (5) 
 3.3.2 Semantic distance Finally, to provide a single performance measure, which can be used to rank and evaluate protein function prediction algorithms, we introduce  semantic distance  as the minimum distance from the origin to the curve  . More formally, the semantic distance  S k  is defined as
 (6) 
where  k  is a real number greater than or equal to one. Setting  k  = 2 results in the minimum Euclidean distance from the origin. The preference for Euclidean distance ( k  = 2) over say Manhattan distance ( k  = 1) is to penalize unbalanced predictions with respect to the depth of predicted and experimental annotations. 3.4 Precision and recall To contrast the semantic distance-based evaluation with more conventional performance measures, in this section, we briefly introduce precision and recall for measuring functional similarity. As before, we consider a set of propagated experimental terms  T  and predicted terms   and define precision as the fraction of terms predicted correctly. More specifically,
 
where   is the set cardinality operator. Only proteins for which the prediction set is non-empty can be used to calculate average precision. To address this issue, the root term is counted as a prediction for all proteins. Similarly, recall is defined as the fraction of experimental (true) terms, which were correctly predicted, i.e.
 
As before, precision   and recall   for the entire dataset are calculated as averages over the entire set of proteins [an alternative definition of precision and recall is given by  Verspoor  et al.  (2006) ]. Finally, to provide a single evaluation measure, we use the maximum F-measure over all decision thresholds. For a particular set of terms  T  and  , F-measure is calculated as the harmonic mean of precision and recall. More formally, the final evaluation metric is calculated as
 
where   and   are calculated by averaging over the dataset. 3.4.1 Information-theoretic weighted formulation The definition of information accretion and the use of a probabilistic framework defined by the Bayesian network enables the straightforward application of information accretion to weight each term in the ontology. Therefore, it is easy to generalize the definitions of precision and recall from the previous section into a weighted formulation. Here, weighted precision and weighted recall can be expressed as
 
and
 
Weighted precision   and recall   can then be calculated as weighted averages over the database of proteins, as in  Equations (4)  and ( 5 ). 4 EXPERIMENTS AND RESULTS In this section, we fist analyze the average information content in a dataset of experimentally annotated proteins and then evaluate performance accuracy of different function prediction methods using both topological and probabilistic metrics. Each experiment was conducted on all three categories of the GO: Molecular Function (MFO), Biological Process (BPO) and Cellular Component (CCO) ontologies. To avoid cases where the information content of a term is infinite, a pseudo-count of one was added to each term, and the total number of proteins in the dataset was incremented when calculating term frequencies. 4.1 Data, prediction models and evaluation We first collected all proteins with GO annotations supported by experimental evidence codes (EXP, IDA, IPI, IMP, IGI, IEP, TAS, IC) from the January 2011 version of the Swiss-Prot database (29 699 proteins in MFO, 31 608 in BPO and 30 486 in CCO). We then generated three simple function annotation models: Naive, BLAST and GOtcha, to assess the ability of performance metrics to accurately reflect the quality of a predicted set of annotations. In addition to these three methods, we generated another set of ‘predictions’ by collecting experimental annotations for the same set of proteins from a database generated by the GO Consortium released at about the same time as our version of Swiss-Prot. This was done to quantify the variability of experimental annotation across different databases using the same set of metrics. In addition, this comparison can be used to estimate the empirical upper limit of prediction accuracy because the observed performance is limited by the noise in experimental data. All computational methods were evaluated using 10-fold cross-validation. The Naive model was designed to reflect biases in the distribution of terms in the dataset and was the simplest annotation model we used. It was generated by first calculating the relative frequency of each term in the training dataset. This value was then used as the prediction score for every protein in the test set; thus, every protein in the test partition was assigned an identical set of predictions over all functional terms. The performance of the Naive model reflects what one could expect when annotating a protein with no knowledge about that protein. The BLAST model was generated using local sequence identity scores to annotate proteins. Given a target protein sequence  x , a particular functional term  v  in the ontology, and a set of sequences   annotated with term  v , we determine the BLAST predictor score for function  v  as  
 , where   is the maximum sequence identity returned by the BLAST package ( Altschul  et al. , 1997 ) when the two sequences are aligned. We chose this method to mimic the performance one would expect if they simply used BLAST to transfer annotations between similar sequences. The third method, GOtcha ( Martin  et al. , 2004 ), was selected to incorporate not only sequence identity between protein sequences but also the structure of the ontology (technically, BLAST also incorporates structure of the ontology but in a relatively trivial manner). Specifically, given a target protein  x , a particular functional term  v , and a set of sequences   annotated with function  v , one first determines the r-score for function  v  as  , where   represents the E-value of the alignment between the target sequence  x  and sequence  s , and  c  = 2 is a constant added to the given quantity to ensure all scores were above 0. Given the r-score for function  v , i-scores were then calculated by dividing the r-score of each function by the score for the root term  . As such, GOtcha is an inexpensive and robust predictor of function. 4.2 Average information content of a protein We first examined the distribution of the information content per protein for each of the three ontologies ( Fig. 3 ). We observe a wide range of information contents in all ontologies, reaching over 128 bits in case of BPO (which corresponds to a factor of 128 in the probability of observing particular annotation graphs). The distributions for MFO and CCO show unusual peaks for low information contents, suggesting that a large fraction of annotation graphs in these ontologies are low quality. One such anomaly is created by the term ‘binding’ in MFO that is associated with 72% of proteins. Furthermore, 41% of proteins are annotated with its child ‘protein binding’ as a leaf term, and 26% are annotated with it as their sole leaf term. Such annotations, which are clearly a consequence of high-throughput experiments, present a significant difficulty in method evaluation.
 Fig. 3. Distribution of information content (in bits) over proteins annotated by terms for each of the three ontologies. The average information content of a protein was estimated at 10.9 (std. 10.2), 32.0 (std. 33.6) and 10.4 (std. 9.2) bits for MFO, BPO and CCO, respectively Previously, we showed that the distribution of leaf terms in protein annotation graphs exhibits scale-free tendencies ( Clark and Radivojac, 2011 ). Here, we also analyzed the average number of leaf terms per protein and compared it with the information content of that protein. We estimate the average number of leaf terms to be 1.6 (std. 1.0), 3.0 (std. 3.6) and 1.6 (std. 1.0) for MFO, BPO and CCO, respectively, and calculate Pearson correlation between the information content and the number of leaf terms for a protein (0.80, 0.92 and 0.71). Such high level of correlation suggests that proteins annotated with a small number of leaf terms are generally annotated by shallow graphs. This is particularly evident in the case of ‘protein binding’ annotations that can be derived from yeast-2-hybrid experiments but provide little insight into the functional aspects of these complexes when only viewed as GO annotations. We believe the wide range of information contents coupled with the fact that a large fraction of proteins were essentially uninformative, justifies the weighting proposed in this work. 4.3 2D plots To assess how each metric evaluated the performance of the four prediction methods, we generated 2D plots.  Figure 4  shows the performance of each predictor using precision/recall and ru-mi curves, as well as their weighted variants [additional precision/recall curves using the definition by  Verspoor  et al.  (2006)  as well as additional ru-mi curves are provided in  Supplementary Materials ]. The performance of the GO/Swiss-Prot annotation is represented as a single point because it compares two databases of experimental annotations.
 Fig. 4. The 2D evaluation plots. Each plot shows three prediction methods: Naive (gray, dashed), BLAST (red, solid) and GOtcha (blue, solid) constructed using cross-validation. Green point labeled GO shows the performance evaluation between two databases of experimental annotations, downloaded at the same time. The rows show the performance for different ontologies (MFO, BPO, CCO). The columns show different evaluation metrics:   and  When looking at the precision/recall curves, we first observe an unusually high area under the curve associated with the Naive model. This is a result of a significant fraction of low information content annotations that are relatively easy to predict by simply using prior probabilities of terms as prediction values. In addition, these biases lead to a biologically unexpected result where the predictor based on the BLAST algorithm performs on par with the Naive model, e.g.  (BLAST, MFO)   and  (Naive, MFO)  , whereas  (BLAST, CCO)  (Naive, CCO)  . The largest difference between the BLAST and Naive models was observed for BPO, which has a Gaussian-like distribution of information contents in the logarithmic scale ( Fig. 3 ). The second column of plots in  Figure 4  shows the weighted precision/recall curves. Here, we observe large changes in the performance accuracy, especially for the Naive model, in MFO and CCO categories, whereas the BPO category was, for the most part, not impacted. We believe that the information-theoretic weighting of precision and recall resulted in more meaningful evaluation. The information-theoretic measures are shown in the last two columns of  Figure 4 . One useful property of ru-mi plots is that they explicitly illustrate how many bits of information are yet to be revealed about a protein (on average) as a function of misinformation that is introduced by over-prediction or misannotation. In all three categories, the amount of misinformation being introduced increases rapidly; quickly obtaining a rate that is twice the amount of expected information for an average protein. We believe these plots shed new light into how much information overload a researcher can be presented with by drawing predictions at a particular threshold. Looking from right to left in each plot, we observe an elbow in each of the curves (at ∼3 bits for MFO and CCO and 12 bits for BPO;  Fig. 4 ) after which the remaining uncertainty barely decreases, whereas misinformation grows out of control. 4.4 Comparisons of single statistics Here, we analyze the ability of the single measures to rank predictors and lead to useful evaluation insights. We compare the performance of semantic distance to several other methods that calculate either topological or semantic similarities. For each evaluation method, the decision threshold was varied for each of the prediction methods, and the threshold providing the best performance was selected as optimal. We then analyze and discuss the performance of these metrics at those optimal thresholds. We implemented the semantic similarity metrics of  Jiang and Conrath (1997) ,  Lin (1998) ,  Resnik (1995)  and  Schlicker  et al.  (2006) , as detailed in  Supplementary Materials . Because each of these measures is defined for a pair of terms in the ontology, scores between two protein annotation graphs (true graph  T  versus predicted graph  P ) were obtained by averaging scores over all pairs of leaf terms   such that   and  . We refer to such scoring as all-pair averaging and note that the all-pair averaging using Resnik’s term similarity was implemented by  Lord  et al.  (2003)  in the context of GO annotations. The results for a best-match averaging (also referred to as max-average method) are presented in the  Supplementary Materials . In addition to these semantic measures, we also implemented the Jaccard similarity coefficient between the sets of vertices in the two annotation graphs ( Supplementary Materials ). In terms of precision/recall curve and ru-mi curve, we used   and  S 2  measures to obtain optimal thresholds. Table 1  shows the maximum similarity, or minimum distance in the case of Jiang and Conrath’s and semantic distance, that each metric obtained for each of our classification models. In addition to reporting the maximum similarity, we also report the decision threshold at which that value was obtained along with the associated level of remaining uncertainty and misinformation at that threshold. The first interesting observation is that all metrics, aside from that of Jiang and Conrath, obtain optimal thresholds that result in relatively similar levels of remaining uncertainty and misinformation for the GOtcha model. However, all metrics, aside from semantic distance and Jiang and Conrath’s distance, seem to favor extremely high levels of misinformation at the reported decision thresholds for the BLAST model. For MFO and CCO, the semantic similarity measures of Lord  et al. , Lin and Sclicker  et al.  report misinformation levels that are more than twice the information content of the average protein in that ontology for the BLAST model. In BPO, those are even more extreme. We believe this is a direct consequence of the pairwise term averaging applied in these methods.
 Table 1. Performance evaluation of several information-theoretic and topological metrics Note : For each measure, the decision threshold was varied across the entire range of predictions to obtain the maximum or minimum value (shown in column 1). The threshold at which each method reached the best value is shown in column 2. Columns 3 and 4 show the remaining uncertainty ( ru ) and misinformation ( mi ) calculated according to the Bayesian network. Each semantic similarity metric was calculated according to the relative frequencies of observing each term in the database. It is particularly interesting to analyze the optimal thresholds obtained for the BLAST model. These thresholds can be interpreted as the level of sequence identity above which each metric reports functional transfer can be made. For example, because their optimal BLAST thresholds are relatively low, the levels of misinformation provided by the similarities of Lord  et al. , Lin and Schlicker  et al.  are rather large.   and Jaccard approaches also report low threshold values for all ontologies, whereas Jiang and Conrath’s distance selects the optimal threshold at an overly restrictive 100% sequence identity. We believe that the semantic distance  S 2  provides more reasonable values for functional transfer, finding an optimal distance at 77, 88 and 78% for MFO, BPO and CCO, respectively. 5 DISCUSSION In this work, we propose an information-theoretic framework for evaluating the performance of computational protein function prediction. We frame protein function prediction as a structured-output learning problem in which the output space is represented by consistent subgraphs of the GO graph. We argue that our approach directly addresses evaluation in cases where there are multiple true and predicted (leaf) terms associated with a protein by taking the structure of the ontology and the dependencies between terms induced by a hierarchical ontology into account. Our method also facilitates accounting for the high level of biased and incomplete experimental annotations of proteins by allowing for the weighting of proteins based on the information content of their annotations. Because we maintain an information-theoretic foundation, our approach is relatively immune to the potential dissociation between the depth of a term and its information content, a weakness of often-used topological metrics in this domain such as precision/recall or ROC-based evaluation. At the same time, because we take a holistic approach to considering a protein’s potentially large set of true or predicted functional associations, we resolve many of the problems introduced by the practice of aggregating multiple pairwise similarity comparisons common to existing semantic similarity measures. Although there is a long history ( Resnik, 1999 ) and a significant body of work in the literature regarding the use of semantic similarity measures ( Guzzi  et al. , 2012 ;  Pesquita  et al. , 2009 ), to the best of our knowledge, all such metrics are based on single statistics and are unable to provide insight into the levels of remaining uncertainty and misinformation that every predictor is expected to balance. Therefore, the methods proposed in this work extend, modify and formalize several useful information-theoretic metrics introduced during the past decades. In addition, both remaining uncertainty and misinformation have natural information-theoretic interpretations and can provide meaningful information to the users of computational tools. At the same time, the semantic distance based on these concepts facilitates not only the use of a single performance measure to evaluate and rank predictors but can also be exploited as a loss function during training. One limitation of the proposed approach is grounded in the assumption that a Bayesian network, structured according to the underlying ontology, will perfectly model the prior probability distribution of a target variable. An interesting anomaly with this approach is that the marginal probability, and subsequently the information content, of a single term (i.e. consistent graph with a single leaf term) calculated from a Bayesian network does not necessarily match the relative term frequency in the database (instead, the conditional probability tables are estimated as relative frequencies).  Ad hoc  solutions that maintain the term information content are possible but would result in sacrificed interpretability of the metric itself. One such solution can be obtained via a recursive definition   and  , where  i ( v ) is estimated directly from the database. Finally, rationalizing between evaluation metrics is a difficult task. The literature presents several strategies where protein sequence similarity, protein–protein interactions or other data are used to assess whether a performance metric behaves according to expectations ( Guzzi  et al. , 2012 ). In this work, we took a somewhat different approach and showed that the demonstrably biased protein function data can be shown to provide surprising results with well-understood prediction algorithms and conventional evaluation metrics. Thus, we believe that our experiments provide evidence of the usefulness of the new evaluation metric. 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>CMap 1.01: a comparative mapping application for the Internet</Title>
    <Doi>10.1093/bioinformatics/btp458</Doi>
    <Authors>Youens-Clark Ken, Faga Ben, Yap Immanuel V., Stein Lincoln, Ware Doreen</Authors>
    <Abstract>Summary:CMap is a web-based tool for displaying and comparing maps of any type and from any species. A user can compare an unlimited number of maps, view pair-wise comparisons of known correspondences, and search for maps or for features by name, species, type and accession. CMap is freely available, can run on a variety of database engines and uses only free and open software components.</Abstract>
    <Body>1 INTRODUCTION CMap is a generic and extensible comparative map viewer that runs in standard web browsers and aims to assist biological researchers seeking to extrapolate known map data into unknown areas. Comparison of genetic, physical and sequence maps allows researchers to fill in gaps and extend knowledge both within and across species. For example, comparison of map fragments such as FPC contigs to an assembled sequence map or high-quality genetic map can help order, orient and assemble the fragments. Feature order in one map can help aid the selection of additional markers for mapping in another population of interest. CMap is used by the Gramene project (Liang  et al .,  2008 ) to visualize and compare over 200 map sets of various types from 30 plant species; it is also used by many other projects comparing data from plants and animals. Data providers can extensively customize CMap to suit their tastes, configuring the definitions of species, map types, how maps are grouped into sets, how maps are drawn, the types of features displayed, their positions on the maps, how the features are drawn, what correspondences are made between features, how correspondences between maps are aggregated and colored, the evidence codes supporting these correspondences, and more. CMap relies on a relational database and open-source software. The history of interactive graphical maps includes an early version from AceDB (Stein  et al .,  1999 ) which included ‘Multi-map’ to display comparisons. The similarly named ‘cMap’ application (Fang  et al .,  2003 ) from the MaizeDB was one of the first comparative map viewers to allow cross-species comparisons, but the application appears to be unavailable. National center for genome resources (NCGR) also created the comparative map and trait viewer (CMTV) (Sawkins  et al .,  2004 ) to allow multiple cross-species comparisons. The SOL genomics network (SGN) comparative map (Mueller  et al .,  2008 ) viewer is perhaps the closest in features to CMap, but it allows a user to compare only two maps at a time, a limitation not shared by this software. CMap was originally written in 2001 for Gramene, a comparative mapping resource for crop grasses, and has since been contributed to the generic model organism web site (GMOD) project under the GNU Public License. CMap has been downloaded well over 1000 times and adapted by many other groups working on a range of different organisms, such as the legume information system, CottonDB, GrainGenes, the nematode Pristionchus and BeeBase. This article discusses version 1.01 of CMap released on July 1, 2008. 2 METHODS 2.1 Data concepts The main data components in CMap are species, map types, map sets, maps, features and correspondences. The data administrator decides what constitutes each. Any species or map type is allowed. A map set is simply a collection of maps, and maps are any linear ordered set of features such as a linkage group, chromosome or an FPC contig. A feature is any point or interval positioned on a map such as a genetic marker, an in/del, a centromere, a QTL or a gene. A correspondence can be anything from a shared synonym to a sequence similarity such as a BLAST hit. All data can be loaded from tab-delimited files or manually inserted with tools provided in the CMap distribution. 2.2 Using CMap To use CMap, the researcher will find there are four main paths for entering the comparative map viewer: the map viewer, the feature search, the correspondence matrix and the map search. If the user wishes to view a particular map or map set, then the map viewer is a logical point of entry. After clicking the ‘Maps’ link in the CMap menu, the user is presented with a form to select a species and a map set. The user may choose to view any number of maps in the set to serve as the starting point. Next, the user may select from lists of comparative maps to place on the right and/or left of the reference map(s). The number of correspondences to each comparative map is shown in square brackets to aid the user in deciding which maps to select. After adding comparative maps, the user may choose to add new maps to the right and/or left of the outermost maps; remove, flip or crop maps; change which features and/or labels are displayed; change the size of the image, and more. If the user has a set of features (markers, QTL, BACs, etc.) he wishes to locate on maps, then he can search for features by clicking on the ‘Feature Search’ link in the CMap menu. For each feature found, the feature's name, type, species, map set, map, position and aliases are displayed as well as two links for each feature, one that takes the user to the ‘map details’ page for the map on which the feature occurs with the feature highlighted and another which takes the user to the ‘feature details’ page showing all the known information on the feature and all of its correspondences.
 Fig. 1. Many of the key concepts of CMap are shown. Five maps of varying types from QTL to genetic to sequence and from two species are displayed, and more could be added. Map features range from QTLs to genetic markers to bins to genes, and correspondences based on different types of evidence are show in varying colors ( http://www.gramene.org/db/cmap ). If the user is interested to know the total number of correspondences from any map set to any other map set, then he can click on the ‘Matrix’ link in the CMap menu. A table shows a cross-tabulated comparison of all the correspondences among map sets. By clicking on any map set name, the user can limit the matrix to a particular map set. Clicking on the numbers of correspondences takes the user to the map viewer showing the two maps in relation to each other. The last main method to enter CMap is by choosing the ‘Map Search’ link in the CMap menu. Here, a user can search for particular maps based on the map name or the number of related maps. The results include the map names, the number of related maps, the number of correspondences and the number of features by type present on the map. Clicking a map's name takes the user to the map viewer page so he may add comparative maps. In addition to the above four most direct routes to view comparative maps, other entry points are available through the CMap menu. The ‘Species’ link allows the user to browse the available species and all the map sets from each. From there or from the CMap menu, the user can choose the ‘Map Set Info’ link to learn more about a particular map set, and from there choose to view one or all of the maps in the map viewer or the matrix. In addition, the user may choose the links for ‘Feature Types’, ‘Map Types’ or ‘Evidence Types’. Depending on the questions that the user seeks to answer, other routes may prove more revealing. Lastly, a proactive data provider may choose to present pre-formulated views which he knows will be of interest to his community of users by simply copying and pasting a URL into a web page and making a link with text explaining the view or storing this via the ‘Saved Links’ section of CMap, a function that also allows user to recall previous views. The data provider has the ability to create multiple CMap databases that are entirely separate from each other. If this has been done, the user can switch among these databases using the ‘Data Source’ control in the upper-right corner. Users can download data of maps or whole map sets in GFF or CMap's tab-delimited or XML formats. Links to the download page are available on the ‘Map Set Info’ page and from the map menu buttons in the map viewer. There is also a ‘Help’ section and a ‘Tutorial’ to show users how CMap works and its terminology. 2.3 Implementation CMap is a Perl application that runs on an Apache web server (versions 1 or 2) on Windows and UNIX variants. CMap has a simple relational schema that can be implemented in MySQL, Oracle, PostgreSQL or Sybase. It relies on no proprietary software or SQL extensions, and uses all freely available software. The user interface is a basic HTML/JavaScript page that works with any modern web browser. No registration or permission is required for its use. Administration of CMap is accomplished through the following three methods: simple text configuration files define system settings such as databases and directories for templates, sessions and temporary files as well as how map, features and correspondences are drawn. The ‘cmap_admin.pl’ tool is used for importing and exporting data in various text formats (tab, XML, SQL), creating and deleting maps or correspondences, reloading the correspondence matrix or purging CMap's web data cache after loading new data. Lastly, a browser-based administrative tool allows the data provider a basic CRUD (Create/Read/Update/Delete) interface for all the different objects (map sets, maps, features, etc.) in the database. 2.4 Future plans CMap continues development as a GMOD project. Work is underway on version 2.0 which will feature a major rewrite of the internals, an improved and streamlined database, graphical output in scalable vector graphics, and integration of the Circos circular genome viewer (Krzywinski  et al .,  2009 ). Funding : United State Department of Agriculture (USDA) Initiative for Future Agriculture and Food Systems (IFAFS) (grant number 00-52100-9622); Cooperative State Research and Education Service (CSREES) agreement through the USDA Agricultural Research Service (ARS) (grant number 58-1907-0-041); National Science Foundation (NSF) PGI (grant number 0321685, for the years 2004-2007); NSF Plant Genome Research Resource (grant number 0703908, work from 2004 till now); USDA ARS (grant number 413089). Conflict of Interest : none declared. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Colocalization analyses of genomic elements: approaches, recommendations and challenges</Title>
    <Doi>10.1093/bioinformatics/bty835</Doi>
    <Authors>Kanduri Chakravarthi, Bock Christoph, Gundersen Sveinung, Hovig Eivind, Sandve Geir Kjetil, Wren Jonathan</Authors>
    <Abstract/>
    <Body>1 Introduction The advent of high-throughput sequencing technologies has dramatically increased our understanding of the functional elements that are embedded in the genome and of the biological functions they encode ( Goodwin  et al. , 2016 ). The human genome is no longer an unannotated string of letters with little metadata, as it was in 2001 ( Lander  et al. , 2001 ), but highly annotated sequences with thousands of annotation layers that help us understand which parts of the genome may have which biological functions and cell type specific activity. Over the past decade, maps of genomic features such as protein-coding genes, conserved non-coding elements, transposons, small non-coding ribonucleic acid (RNA), large intergenic non-coding RNAs and epigenomic marks (e.g. chromatin structure, and methylation patterns) have been established ( Lander, 2011 ). An important research direction in biomedical research after the initial characterization, has been the study of the interplay of various functional elements in many biological processes ( Heinz  et al. , 2015 ;  Luco  et al. , 2011 ;  Makova and Hardison, 2015 ;  Portela and Esteller, 2010 ). The search for connections and associations between different types of regulatory regions can provide a deeper understanding of the cellular processes ( Birney  et al. , 2007 ), but it requires suitable tools and tailored analytical strategies. Functionally related genomic features—be it two transcription factors that jointly regulate their target gene, or different epigenomic marks indicative of enhancer elements—often co-occur within a genomic sequence. One important way to detect relevant evolutionary or mechanistic relationships between genomic features is therefore to search for significant overlap or spatial proximity between these features. The commonly employed approaches that search for such significant overlap exploit the fact that the genomic features that are associated either directly or indirectly will not occur independently along the genome. The reference genome enables the detection of spatial proximity by acting as a central entity to interlink the mapped genomic features ( International Human Genome Sequencing Consortium, 2004) . Each genomic feature can be represented as a set of regions on the reference genome map using chromosomal coordinates (e.g. chr1: 1–1000), which are typically a range of numbers denoting the start and end positions of the sequence nucleotides on a specific chromosome. Many high-throughput sequencing experiments result in sets of such  genomic regions  as their main output (often referred to as  genomic intervals ), and the lists/collections of genomic regions are commonly referred to as  genomic tracks  or  region sets . Arithmetic set operations are performed between genomic tracks to determine the amount of overlap or spatial proximity, followed by statistical testing to assess whether or not the observed overlap or spatial proximity is likely to be due to chance. Such analytical approaches are generally referred to as  co-occurrence  or  colocalization  analysis of genomic elements or alternatively as  region set enrichment analysis . Throughout this manuscript, we refer to this methodology as colocalization analysis. Colocalization analyses of genomic features involve computationally intensive genome arithmetic operations and rigorous statistical testing. The analyses may utilize a wide range of curated functional annotations that are often taken from public datasets (e.g.  Supplementary Table S1 ). Several generic and specialized tools are available to perform colocalization analysis, as libraries for specific programming languages, as command line tools, or as web-based tools with varying levels of functionality and comprehensiveness. Specifically, tools are available to (i) generate hypotheses by comparing a region set against public data [e.g.  Bock  et al.  (2009) ,  Halachev  et al.  (2012 ) and  Sheffield and Bock (2016 )], (ii) perform genome arithmetic operations [e.g.  Lawrence  et al.  (2013 ) and  Quinlan and Hall (2010 )], (iii) visualize the intersecting genomic regions [e.g.  Conway  et al.  (2017 ) and  Khan and Mathelier (2017 )] and, (iv) perform statistical testing of colocalization between a pair of tracks [e.g.  Favorov  et al.  (2012)  and  Sandve  et al.  (2010 )] or between multiple tracks [e.g.  Layer  et al.  (2018 ) and  Simovski  et al.  (2017) ]. For an overview of the multitude of tools available for colocalization analysis, see reference ( Dozmorov, 2017 ). The existing tools follow different concepts and workflows, and there is additional variation arising from the setup and parameter choices that the user makes when using these tools. These differences can strongly influence the conclusions. Colocalization analysis is in some cases used for confirmatory analysis, where the establishment of an association is in itself the primary investigational aim. But perhaps more common is the use of colocalization analysis in an explorative fashion, serving to generate hypotheses that are afterwards followed up by tailored experiments and computational analyses. While false positives are less of a scientific problem when colocalization analysis is used in an explorative phase, it may make the analysis indiscriminate and thus invalidate its main purpose of guiding subsequent experimental and computational investigations in fruitful directions. Thus, with a focus on avoiding false findings, in the following sections, we point out the methodological challenges of statistical colocalization analysis, provide runnable examples that highlight the issues, and survey existing ways to handle these challenges. The sections are presented as recommendations on best practices, starting with data representation, continuing through statistical testing and ending with some guidance on the interpretation of results. 2 Make sure that the trivial aspects of data representation are handled correctly The sequence coordinates of any two reference genome builds may differ substantially ( Kanduri  et al. , 2017 ). Similarly, the coordinates of genomic regions differ depending upon the indexing scheme used (0-based indexing or 1-based indexing; see  Fig. 1a ). To avoid erroneous genome arithmetic operations, it is important to make sure that the genome coordinates are compatible.
 Fig. 1. 
 (a)  Examples of coordinates of a sequence of nucleotides on zero-based and one-based genome coordinate systems. The brackets represent closed while parentheses represent open intervals. Being closed at a position represents the inclusion of that position in the genomic interval, whereas being open represents the exclusion of that position.  (b)  Example of discretizing continuous value to call genomic intervals. Here, although both profile-A and profile-B look visually similar, one of the genomic intervals in profile-B was not called because of marginally falling below a chosen threshold or owing to algorithm parameters, resulting in the exclusion of that region from further analyses.  (c)  Example of variations when computing distances from start, midpoint or end coordinates. Here, although both the points are 1 kb upstream of the genomic intervals, because of the differences in the length of genomic intervals the distances largely differ (here almost 2-fold) when computed to the midpoint or end Continuous data associated with genomic sequences is often discretized into a set of high-valued genomic regions before analysis [as in e.g. peak calling of chromatin immunoprecipitation sequencing (ChIP-seq) data ( Zhang  et al. , 2008 )]. Since discretization reduces information (see an example in  Fig. 1b ), using the original continuous signal in statistical testing has the potential to improve statistical power. Both generic ( Stavrovskaya  et al. , 2017 ) and technology-specific [e.g.  Chen  et al.  (2015 ) and  Shao  et al.  (2012 )] methodologies have been proposed to correlate continuous signal of genomic tracks. Another solution to avoid the loss of data due to thresholding, is to incorporate some form of uncertainty associated with genomic regions (e.g. weights,  P -values) into the descriptive measure of colocalization. In certain analysis scenarios (e.g. when computing distances between genomic features), genomic regions may be represented by their start-, end- or mid-point, or may be expanded to include flanks. The choices of a reference point (start, midpoint or end), and flank sizes will provide alternative perspectives about the genomic features of interest (e.g. see  Fig. 1c ). Therefore, if reduction of transformation of data is required, one has to be conscious about their effects and interpretation. 
 Examples: 
 https://hyperbrowser.uio.no/coloc/u/hb-superuser/p/data-representation-1 
 3 Avoid using a single fixed resolution if there is no good biological reason for it Deoxyribonucleic acid (DNA) sequence properties and genomic features are often scale-specific, and they may thus appear differently when measured at different scales ( Supplementary Fig. S1 ). The strength of statistical association observed between genomic features may thus vary when observed across multiple scales. To avoid misconceptions, the choice of scale ought to reflect the intrinsic scale at which the biological phenomena occur. Therefore, analysis of scale-specific events should either be guided by a knowledge-driven choice of the resolution or through rigorous investigation at multiple scales. A common strategy in the analysis of genomic elements is to apply binning of genomic elements into multiple windows of predefined size, in order to obtain window-level statistics. However, it is known that the density of functional elements varies along the chromosomes. For example, focused genetic variation such as single nucleotide polymorphisms (SNPs) and insertions or deletions (InDels) occurs on a scale of a single base or a few bases; transcription factor binding sites determined through ChIP-seq typically span ∼100 bases; RNA transcripts and broad genetic variation, like copy number variations (CNVs), typically occur on a scale of kilo bases; and recombination regions can span several megabases.  A priori  selection of window size is thus a reasonable approach when prior knowledge exists about the resolution of the genomic event of interest. Without prior knowledge, however, using a single fixed window size can lead to a loss of statistical power and misleading conclusions. For many research questions, the biological resolution of a genomic event of interest is not known. Therefore, an alternative to drawing conclusions based on an arbitrary window size could be to analyze the correlations between tracks at multiple scales to identify the scale-specific relationship between biological processes of interest. A few studies have previously tackled this problem by employing techniques routinely used in image processing and segmentation. For example, wavelet-transforms have been used to transform the observed signal intensity in a way that captures the variation in the data at successively broader scales. By correlating the transformed signal at each scale, the scale-specific interactions between biological processes of interest have been evaluated ( Chan  et al. , 2012 ;  Liu  et al. , 2007 ;  Spencer  et al. , 2006 ). With the same objective, multiscale signal representation, which is routinely used in image segmentation, has been applied to genomic signals to convolve the genomic signal into segments at successive scales to capture the unknown scale variations of the signal ( Knijnenburg  et al. , 2014 ). Recently, Gaussian kernel correlation has been proposed to correlate continuous data generated in genomics experiments ( Stavrovskaya  et al. , 2017 ). Although the method was intended for correlating continuous data, the underlying idea is to avoid binning of continuous data into windows of arbitrarily-chosen size. The overarching theme of all these methods is to smoothen the observed signal, capturing the regional variation and subsequently to perform spatial correlation. This is equivalent to assessing correlations at several different scales that are successively broader. Overall, a reasonable choice of window size would depend on the research question, and the choice should be based upon the type of genomic feature under study. Similar ideas are appropriate when choosing a reasonable length of flanking sequences where previous experimental evidence is not available. 
 Examples: 
 https://hyperbrowser.uio.no/coloc/u/hb-superuser/p/predefined-resolution 
 4 Choose an appropriate test statistic and a suitable measure for effect size In colocalization analysis, the pairwise relation of two tracks is summarized using a  test statistic . The test statistics used in colocalization analyses are based on counts, distances or overlap. Examples of these metrics include the total number of intersecting genomic intervals between two tracks ( counts ), the total number of bases  overlapping  between the intersecting elements of two tracks ( overlap ) and some form of  distance  (average or geometric) between the  closest  elements. It has even been proposed to exchange the overlap/distance value of each individual genomic interval with a  P -value that denotes its proximity to the genomic elements of a second track ( Chikina and Troyanskaya, 2012 ). Adding  P -values in this way will in effect scale the per-interval proximity values by what distance would be expected by chance, and allow subsequent direct interpretation of the distribution of computed  P -values. While most of the existing colocalization analysis tools based on exact tests use a count statistic, Monte Carlo (MC) simulations-based tools make use of other test statistics as well. The precise formulation of a research question reflects a specific choice of test statistic in MC simulation-based methods. As an example, let us consider the choice of test statistic when investigating whether genome-wide association study (GWAS) -implicated SNPs preferentially lie in the proximity of annotated genes. One way to formulate this question is as follows: do the SNPs fall inside protein-coding genes more frequently than expected by chance? This formulation would suggest using a count-based test statistic. However, one could also ask whether SNPs are preferentially located in gene-rich regions. One possible test statistic could then be based on expanding the SNP locations with large flanks on both sides, followed by an assessment of whether the overlap between these expanded regions and genes is higher than expected by chance. A third possibility is to ask whether the SNPs are generally close to genes. The test statistic could then be based on determining the closest gene of each SNP, computing the geometric or arithmetic average of these distances (respectively emphasizing immediate or moderate proximity), and assessing whether this average is different from what would be expected by chance. Notably, all the above formulations have an asymmetric aspect, meaning that the observations and the conclusions may change depending upon the direction of analysis. The inverse formulations—whether genes fall inside SNPs, whether genes are located in SNP-rich regions or whether genes are generally close to SNPs—appear less meaningful biologically. In all the above formulations, the test statistic describes the relation of interest. However, in the majority of cases, the test statistic does not serve as an effective metric to understand the size of the effect. For instance, a statistically significant overlap of 1000 base pairs between a pair of tracks does not directly reveal whether or not the observed overlap is of sufficient magnitude that it could plausibly have biological consequences. Therefore, certain descriptive measures can be used to quantify effect size, supporting biological data interpretation. A widely used measure of effect size is the ratio between the observed value of the test statistic compared to the expected value (typically as a ratio of observed to expected). 
 Examples: 
 https://hyperbrowser.uio.no/coloc/u/hb-superuser/p/test-statistic 
 5 Remember that all statistical tests are limited by the realism of the assumed null model Statistical hypothesis testing has been one of the main approaches to assess whether the observed colocalization between two genomic features is likely to have occurred only because of chance. In this approach, it is hypothesized that the genomic features being tested occur independently along the genome (null hypothesis, H 0 ), and the probability of the observed colocalization, or something more extreme, is computed (i.e. the  P -value). The  P -value is computed by comparing the observed test-statistic (e.g. overlap, distance, counts) with the background distribution of a test statistic, which is obtained through a model that assumes that null hypothesis is true (i.e. the null model). The null model should appropriately model the distributional properties and dependence structure of the genomic features along the genomic sequence ( Fig. 2 ). Essentially, the aim is to as closely as possible preserve the characteristics of each genomic track in isolation, while at the same time nullifying any dependence between the two genomic tracks (because they are assumed to be independent in H 0 ).
 Fig. 2. Examples of the distributional properties and dependence structure of genomic features along the genomic sequence. Genomic tracks contain genomic regions that are known to occur in clumps and with variable lengths. Also, genomic sequences could be characterized by the distributional and biological properties of genomic events, where stretches of sequence share similar biological properties (as homogeneous blocks). Furthermore, multiple genomic annotations colocalize with each other and thus any statistical association should disentangle the effect mediated by the colocalization of confounding features All the statistical tests assume some form of null model that can range from being too simplistic to being too cautious. The conclusions of colocalization analysis would vary depending upon the choice of null model, where too simplistic null models give over-optimistic findings ( Ferkingstad  et al. , 2015 ). Therefore, understanding the assumptions of the null model will help the researcher to assess whether the assumptions are appropriate for their data. This will allow the researcher to make an informed choice and to avoid false positives. To grasp the implications of a given null model, it is useful to consider i) which properties of the real data are preserved in the null model, and ii) how the remaining properties are distributed ( Sandve  et al. , 2010 ). A null model could for instance preserve the number of elements in each track, some distributional properties of each track (e.g. clumping tendency), or the tendency of each track to have more occurrences in certain parts of the genome (e.g. certain chromosomes). Consider an example case of a colocalization test between the binding sites of two transcription factors (TFBS). Suppose that the null model of choice preserves the number of TFBS in each track, but assumes that the TFBS are uniformly distributed across the genome. By understanding the assumptions of the null model, the researcher can assess whether this is a reasonable assumption given the known clumping tendencies of TFBS ( Haiminen  et al. , 2008 ). 6 Make an informed choice about the most suitable null model The null models that are routinely used in colocalization analysis can broadly be categorized into two types ( Fig. 3 ). They are (i) the general null models of co-occurrence based either on analytical determination or Monte Carlo (MC) simulations, and (ii) the ones that use an explicit set of background or ‘universe’ regions to contrast the observed level of colocalization between a pair of genomic tracks (using e.g. a hypergeometric distribution). This categorization is described in detail in  Box 1 , together with descriptions on the statistical methodologies and assumptions, and a discussion on the advantages and disadvantages of each method. Below, we briefly provide a set of recommendations to aid the choice of appropriate null models.
 Fig. 3. The statistical methods for colocalization analysis can broadly be categorized into two types depending upon whether they use (a) a general null model of colocalization (b) or a specifically selected set of universe regions to estimate a null model (upper and lower panels). Both methods are further categorized as either (i) analytical tests (ii) or tests based on Monte Carlo simulations (left and right panels). Upper left: analytical tests with a general null model is exemplified by a binomial test on whether Track 1 (T1) positions are located inside Track 2 (T2) regions more than expected by chance. Upper right: General Monte Carlo-based tests provide great flexibility in the choice of test statistics and randomization strategies (null model). Here, exemplified by a simple overlap statistic (bps overlap) and uniform randomization of T2. Lower left: A set of universe regions limits the analysis to (here) the ‘case’ regions of T1 and a set of ‘control’ regions that could have been part of T1. Simple counting of the overlapping regions in T1 and T2 provides the basis of a Fisher's exact test. Lower right: The combination of universe regions and Monte Carlo-based testing is not previously presented in literature, but might be designed to combine advantages of them both. For a detailed overview of the null model categories, see Box 1 6.1 Use a simple analytical test only if the typically simple null model is acceptable Simple analytical tests typically assume a simple null model as exemplified in  Box 1  by Fisher’s exact test. When using simple analytical tests, one should be conscious of whether the null model fits with the data, and if not, how robust the test is to handle such violations. Assuming a too simple null model has been found to result in smaller  P -values and thus more false positives ( Ferkingstad  et al. , 2015 ). A recent study has reported a good correlation between the  P -values obtained through a Fisher’s exact test (contingency table filled with per-interval counts) and MC simulations ( Layer  et al. , 2018 ). However, the MC simulations reported in that study were based on a simple permutation model of uniform distribution of genomic intervals ( Layer  et al. , 2018 ), which can lead to strong over-estimation of statistical significance ( De  et al. , 2014 ;  Sandve  et al. , 2010 ). 6.2 Consider using a MC-based hypothesis testing with a realistic (non-uniform) null model Approaches based on MC simulations are computationally intensive and may require careful customization. As discussed in  Box 1 , the degree of preservation of the data characteristics in a null model affects the conclusions obtained through MC simulations. Previous case studies have shown that the higher the preservation of data characteristics in null models, the lower the statistical significance (larger  P -values) ( Ferkingstad  et al. , 2015 ). However, it is also recommended to assess the consistency of conclusions with different choices of null model to avoid being blindly conservative ( De  et al. , 2014 ). A null model that aptly captures the randomness while mimicking the real complex nature of the genome would be an ideal choice. The development of such a model is far from trivial. The genomic sequence could be perceived as a frozen state of evolution, consisting of a large number of rare events over time. A considerable proportion of such stochastic events may depend on the previous rare events in evolution that might be predictable from a sequence analysis perspective. This points to comparative genomics as a potentially powerful approach to characterize the randomness of genome. 6.3 Use analytical tests based on a set of universe regions, if it flows naturally from the analysis domain, but construct the control set with great care The use of a universe of genomic regions represents both an advantage and a disadvantage. As an example, when analyzing a set of SNPs, it could be useful to define the full set of common SNP locations as universe regions. In settings where such universe regions can be readily constructed, it simplifies the statistical assessment and offers high flexibility in the null model, for instance by supplying a universe set that matches the genomic track in terms of potentially confounding genome characteristics. However, the specification of a control set must be done with great care. Discrepancies between the case and control sets in various properties of the data (such as genomic heterogeneity and clumping) might easily in itself break the assumptions of the analytical test, possibly leading to false positives. 
 Examples: 
 https://hyperbrowser.uio.no/coloc/u/hb-superuser/p/null-models 
 7 To avoid false positives use null models that account for local genome structure The genome organization is complex, with several interdependencies. Various genomic elements and sequence properties occur along the genome in a non-uniform and dependent fashion ( Gagliano  et al. , 2014 ;  Kindt  et al. , 2013 ;  Zhang  et al. , 2007 ), leading to various local heterogeneities in the genomic sequence (see  Fig. 2 ). A track-level similarity measure or summary statistic may conceal such heterogeneity. Some few tools have tried to handle this issue by computing summary statistics in user-defined or fixed-size windows along the genome. However, this approach is again problematic because of the inherent problems of predefining the window sizes (discussed above in Section 2). Statistical testing that does not preserve the local genomic structure may lead to spurious findings of association or enrichment ( Bickel  et al. , 2010 ). As an example, consider the case of assembly gaps in the physical maps of the genomes ( International Human Genome Sequencing Consortium, 2004 ;  Treangen and Salzberg, 2012 ). It has recently been shown that ignoring to account for assembly gaps (spanning around 3–6% of the total genome size) can lead to a higher degree of false findings ( Domanska  et al. , 2018 ). On the other hand, in a typical MC simulations-based approach, too much preservation of the local genomic structure will result in too little randomness and in poor  P -value estimates. A few tools provide the functionality to preserve the local genomic structure either by restricting the randomizations to regions matched by the local genomic properties [e.g.  Heger  et al.  (2013 ),  Quinlan and Hall (2010 ) and  Sandve  et al.  (2010 )], or by defining an explicit background set matched by local genomic properties ( Dozmorov  et al. , 2016 ;  Gel  et al. , 2016 ;  Sheffield and Bock, 2016 ). In addition, a few SNP-centric tools match the genomic locations of SNPs with a selection maintaining properties such as gene density, minor allele frequency, number of SNPs in linkage disequilibrium (LD) and proximity to transcription start and end sites. Although matching of the SNP locations based on the above parameters will not be sufficient to control the false-positive rates, matching at least by the number of SNPs in LD has been shown to be critical for appropriate statistical performance ( Trynka  et al. , 2015 ). A similar bias arises because of the intrinsic nature of some of the technology platforms, like genotyping arrays, which vary greatly in the number of probed markers and the physical distribution of these within the genome. Not accounting for these differences when generating the null distributions could also lead to false interpretations. Nevertheless, one of the main challenges of matching by genomic properties is that it requires prior knowledge about all the genomic properties that would otherwise confound the observations when not appropriately matched. An alternative solution to handle this challenge is to restrict the testing space to the local site (for example restricting the distribution of the information elements being tested to locally homogenous blocks as in  Fig. 4d ). Several approaches have been proposed for handling this issue. The first approach implemented in several tools, allows the users to restrict the analysis space to user-supplied or dynamically-defined genomic regions [e.g. in  Heger  et al.  (2013 ),  Sandve  et al.  (2010) ,  Sheffield and Bock (2016 ) and  Trynka  et al.  (2015 )]. In MC-simulations-based approaches, this functionality could be used to restrict the shuffling of genomic intervals to user-supplied regions that are matched by local genomic properties, whereas approaches that explicitly require a background set of regions could construct the background set to match the local genomic properties. While the simplest and most typical way of restricting the analysis space is based on a discrete decision of whether or not to include a given region, it is also possible to provide a continuous (probabilistic) value for the inclusion of a given region or base pair ( Sandve  et al. , 2010 ). An alternative approach ( Bickel  et al. , 2010 ) uses segmentation to segregate the locally homogeneous regions of the genome. Subsequently, random blocks of homogenous regions are subsampled within the segments to estimate a confidence interval of colocalization. With this method, the biologist has to make essential choices in some aspects like the scale of segmentation, and the subsample size, that would affect the statistical conclusions.
 Fig. 4. Examples of different permutation strategies in Monte Carlo Null models.  (a)  In this illustration, let us assume that the dependence relationship between track-A and track-B is being queried. Note the local heterogeneities within the genomic region, where blue and green segments represent blocks of locally homogenous regions. The red segment represents assembly gaps. Tracks A and B are comprised of points and genomic intervals respectively, and different colors are used to distinguish them.  (b)  The simplest form of permutation model assumes uniformity and independence of genomic locations and thus shuffles either of the track without any restrictions. Note here that one of the points was also shuffled to an assembly gap region.  (c)  Another null model preserves the sequence distance between the points or genomic intervals when shuffling and avoids gaps.  (d)  A more conservative strategy preserves the sequence distance, while also shuffling to regions matched by biological properties (blue and green colors) thus preserving local heterogeneity 
 Examples: 
 https://hyperbrowser.uio.no/coloc/u/hb-superuser/p/local-genomic-structure 
 8 Consider potential confounding features and control for their effects A statistically significant correlation between two functional genomic elements may in fact be driven by colocalization with another (known or unknown) third genomic element or sequence property (in some cases unknown) that was not included in the analysis (see  Fig. 2 ). Spatial dependencies exist for a number of genomic elements. For example, a significant fraction of copy number variation occurs in proximity to segmental duplications ( Sharp  et al. , 2005 ); non-coding variants are concentrated in regulatory regions marked by DNase I hypersensitive sites (DHSs) ( Maurano  et al. , 2012 ); DHS exons are enriched near promoters or distal regulatory elements ( Mercer  et al. , 2013 ); higher gene density is found in GC content-rich regions ( Lander  et al. , 2001 ); and extensive pairwise overlap is often found between the binding sites of transcription factors that co-occur and co-operate ( Zhang  et al. , 2006 ). Not testing for the association of potential confounding factors (e.g. GC content, overlap with repetitive DNA, length of the genomic intervals, genotype and other genetic factors) might thus lead to incomplete or erroneous conclusions. When the colocalization of a pair of genomic features is confounded by a third genomic feature, one could unravel the specific relations by contrasting the pairwise overlap statistics or enrichment scores of all the three features. Below are two examples that handled confounding factors by including them into the null model. Trynka  et al.  ( Trynka  et al. , 2015 ) used stratified sampling where the track to be randomized is divided into two sub-tracks defined by either being inside or outside regions in the confounding track. The sub-tracks are then individually randomized. However, as with any stratified analysis, this may result in the loss of statistical power. Another example based on MC simulations handled the potential confounding relation between two tracks by shuffling the genomic locations according to a non-homogenous Poisson process, where the Poisson parameter depended on the locations defined in a third (or several) co-localizing genomic tracks ( Sandve  et al. , 2010 ). 
 Examples: 
 https://hyperbrowser.uio.no/coloc/u/hb-superuser/p/confounding-features 
 9 Use additional methods to test the validity of the conclusions One of the major harms of false-positive findings in colocalization analysis (or in any genomic analysis) is the triggering of futile follow-up projects ( MacArthur, 2012 ). When relying on the conclusions of colocalization analysis to plan follow-up experiments, one might therefore find it beneficial to have an additional validation step to test whether some form of biases have crept into the analysis. Such validation can be performed by simulating artificial data of pairs of genomic tracks with no significant relationship (i.e. occurring independently of each other), and check whether the devised analytical methodology then results in a uniform distribution of  P -values. [e.g. see  Fig. 1  in  Storey and Tibshirani (2003)  and  Altman and Krzywinski (2017 )]. 10 Summary and outlook In the preceding sections, we provided guidelines for performing statistical colocalization analysis, which is routinely employed to understand the interplay between the genomic features. We highlighted the methodological challenges involved in each step of the colocalization analysis and discussed the existing approaches that can handle those challenges, when available. The state-of-the-art methodology for statistical analyses of colocalization vary in the comprehensiveness of handling such challenges. Moreover,  ad hoc  implementations of project-specific methodologies, which are common in biology-driven collaborative projects, may not necessarily handle all the challenges discussed above to a reasonable extent. Therefore, there is a need for the development of a unified and generic methodology that handles several of the potential shortcomings discussed here. To avoid misinterpretation of the conclusions in colocalization analyses, it is essential to be aware of the pitfalls discussed in this article. In addition, as with any application of statistical hypothesis testing, it is also highly recommended to consider the effect size in addition to  P -values. There has lately been considerable focus on the fallacies of blindly drawing conclusions from a  P -value ( Halsey  et al. , 2015 ;  Nuzzo, 2014 ). This is particularly important in situations with very large datasets, which is often the case in genome analysis, since even a minor deviation from the null hypothesis may be statistically significant (without obvious biological relevance). It is thus recommended to apply measures that combine effect size and precision, such as confidence intervals, or by filtering, ranking or visualizing results based on a combination of  P -values and their corresponding effect sizes (e.g. using volcano plots). False findings in colocalization analysis could be avoided by being aware of the accumulated knowledge on best methodological practices. The accumulated knowledge can be categorized into two layers: (i) First, a generic layer represented by all the guidelines detailed in this manuscript, and (ii) second, a specific layer represented by the data type-specific particularities. One of the main examples of such data type-specific particularities is the genomic properties that are to be matched when drawing samples for the null (e.g. LD for SNPs, chromatin accessibility levels and GC content for transcription factor footprinting and so on). In this example, which properties are to be matched would depend on both the annotations being tested and can also be unknown in several cases. In such cases, simulation experiments as suggested in Section 9 of this article could be performed to test and evaluate the potential biases that could inflate the false findings. While the existing tools are focused on a simple linear sequence model of the reference genome, there are ongoing efforts to represent reference genomes in a graph structure to better represent the sequence variation and diversity ( Church  et al. , 2015 ;  Paten  et al. , 2017 ). Novel coordinate systems are being proposed for the better representation of genomic intervals on a graph structure ( Rand  et al. , 2017 ). As the preliminary evidence suggest that the genome graphs improve read mapping and subsequent operations like variant calling ( Novak  et al. , 2017 ), we anticipate that the accuracy of colocalization analyses would also improve as a consequence. The future tool development in colocalization analyses should be tailored towards handling genome graphs, pending the availability of a universal coordinate system and exchange formats. Another recent development is single-cell sequencing, which is now beginning to extend beyond RNA-seq to also include other -omics assays. It is being increasingly acknowledged that explicit phenotype-genotype associations could be established by integrating multiple omics features from the same cell ( Bock  et al. , 2016 ;  Macaulay  et al. , 2017 ). The inherent limitation of single cell sequencing technology in generating low coverage, sparse and discrete measurements (as of now) however affects the statistical power in detecting true associations between multiple omics features. One way of overcoming the low statistical power is by aggregating the signal across similar functional elements (e.g. aggregating expression levels of functionally similar genes) ( Bock  et al. , 2016 ;  Farlik  et al. , 2015 ). Apart from overcoming the inherent challenges of single cell sequencing technology, colocalization analysis is one of the suitable ways to integrate multiple omics features, especially due to the ripe methodologies that can appropriately model the genomic heterogeneities. Supplementary Material Supplementary Information Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>AGRA: analysis of gene ranking algorithms</Title>
    <Doi>10.1093/bioinformatics/btr097</Doi>
    <Authors>Kocbek Simon, Sætre Rune, Stiglic Gregor, Kim Jin-Dong, Pernek Igor, Tsuruoka Yoshimasa, Kokol Peter, Ananiadou Sophia, Tsujii Jun'ichi</Authors>
    <Abstract>Summary: Often, the most informative genes have to be selected from different gene sets and several computer gene ranking algorithms have been developed to cope with the problem. To help researchers decide which algorithm to use, we developed the analysis of gene ranking algorithms (AGRA) system that offers a novel technique for comparing ranked lists of genes. The most important feature of AGRA is that no previous knowledge of gene ranking algorithms is needed for their comparison. Using the text mining system finding-associated concepts with text analysis. AGRA defines what we call biomedical concept space (BCS) for each gene list and offers a comparison of the gene lists in six different BCS categories. The uploaded gene lists can be compared using two different methods. In the first method, the overlap between each pair of two gene lists of BCSs is calculated. The second method offers a text field where a specific biomedical concept can be entered. AGRA searches for this concept in each gene lists' BCS, highlights the rank of the concept and offers a visual representation of concepts ranked above and below it.</Abstract>
    <Body>1 INTRODUCTION DNA microarray is a technology that can simultaneously measure the expression levels of thousands of genes in a single experiment. The use of microarray chips in gene expression analysis requires an enormous amount of data to be analysed and often, while at the same time, selecting the most informative genes from different gene sets. One of the possible ways to rank the genes is to use a feature selection (FS) method. FS is a machine learning-based technique used to select the most important features for building a robust learning model. The same FS techniques are now widely used in bioinformatics for identification of biomarkers or lists of relevant genes from DNA microarray-based gene expression measurements. There are many FS methods which can be used, but how do researches know which one is the best? Several different methods were proposed to estimate the ‘goodness’ of the ranked gene lists ( Ma, 2006 ;  Qiu  et al. , 2006 ). However, these methods usually need computer experts who know how FS methods and learning algorithms work. We describe a novel system, analysis of gene ranking algorithms (AGRA), which allows biologists and other experts with low or no previous computer knowledge to compare different FS methods with the help of evidence mined from PubMed. AGRA uses finding-associated concepts with text analysis (FACTA), an online text search engine for MEDLINE abstracts that can quickly compute the association strengths between a query and different types of biomedical concepts based on their textual co-occurrence statistics ( Tsuruoka  et al. , 2008 ). While other similar systems exist, such as XplorMed ( Perez-Iratxeta  et al. , 2002 ), MedlineR ( Lin  et al. , 2004 ), LitMiner ( Maier  et al. , 2005 ) and Anii ( Jelier  et al. , 2008 ), FACTA was chosen because of its ability to pre-index words and concepts, which result in fast, real-time responses of the system. AGRA needs to process high amount of data, and fast response of the underlying service is crucial for the efficient delivery of the results. AGRA extracts biomedical concepts using FACTA and thus defines a biomedical concept space (BCS) for each gene list. BCS is defined as six categories (gene/protein, disease, symptom, drug, enzyme and chemical compound) of ranked biomedical concepts. To compare the quality of different FS methods, AGRA calculates the overlap for each pair of two gene list of BCSs. This way, gene lists which are the product of different gene ranking algorithms can be compared with a gold standard list. Finally, experts can use their domain knowledge to search for a specific biomedical concept in the ranked gene lists and decide which FS method outputs the most relevant genes. 2 METHODS Figure 1  shows AGRA's main interface with uploaded gene lists. The application offers a novel way to compare ranked lists of genes with the help of BCS. BCS is a set of ranked biomedical concepts gathered through FACTA where they are grouped into six different categories. FACTA can be queried by inputting a word (e.g. P53), a concept ID (e.g. UNIPROT: P04637) or a combination of these ‘[UNIPROT: P04637 AND (lung OR gastric)]’. AGRA calculates BCS for a single gene list in three steps: (i) calculation of protein BCS; (ii) calculation of gene symbol BCS; and (iii) calculation of gene list BCS.
 Fig. 1. AGRA's main interface. To achieve this, each gene symbol from the gene lists is associated with its protein(s) and their Uniprot IDs are extracted with help of the Affymetrix annotation file (HG-U133 Plus 2 Annotations, Release 31). AGRA then queries FACTA with these Uniprot identifiers and maximum 50 most important biomedical concepts (ranked by their frequencies of appearing in the MEDLINE abstracts) from each category are extracted. Concepts that are gathered in this step represent six BCS categories of each associated protein. Next, BCS categories for the gene symbol are calculated. If the gene symbol is associated with only one protein, its BCS is identical to the protein's one. When the symbol is associated with more than one protein, the average values of the frequencies in each category are calculated. In the final step, the six categories for each gene list BCS are calculated. This is done by summarizing values from all gene symbol BCS categories from the list. Because the order of the gene symbols in the list is crucial, AGRA weights each gene symbol BCS according to the gene symbol position in the list. The weight  w  for single symbol  x i  is defined as  w ( x i ) = ( n  − ( i  − 1))/ n , where  n  is number of all its concepts and  i  represents the rank of the gene that concept belongs to in the gene list (starting from 1). Finally, to avoid sending queries to the FACTA system too often, AGRA saves BCSs in a local database. Whenever a gene symbol, for which BCS has not been defined yet, appears in one of the gene lists, the system queries FACTA, calculates its BCS and saves it locally. When BCSs for all gene lists are extracted, AGRA calculates the overlap values for every combination of two BCSs to evaluate the effectiveness of FS methods. Overlap is a simple method to measure similarity between two BCSs where biomedical concepts that appear in both BCS are counted and divided by the number of concepts in the shorter BCS. Another way to compare FS methods is to search for the position of relevant biomedical concepts in the final gene list BCS. Position of a single biomedical concept is defined as it is ranked number among all the concepts in one of the categories. This way, researchers can decide which FS method selects the most important concepts and ranks them higher compared with other methods. 3 USAGE OF THE APPLICATION The usage of AGRA is simple and only basic computer skills are required. The application consists of three different tabs: main, overlap and position. The main tab is used for uploading the gene lists and starting the analysis. The user should upload the lists in a CSV file where the first row represents gene list names and other rows represent ranked genes with the most important gene on the top and the least important gene on the bottom of the list. Due to the calculation complexity and limitation of the FACTA+ system, the input file should contain maximum 7 different gene lists with maximum 100 genes in each list. When the file is uploaded, the ranked genes for each list are displayed in a table next to each other so they can be visually compared. Then the user can enter a specific concept (e.g. ‘breast cancer’) and select in which BCS category AGRA should look for the concept. The system can be started with the start button which is disabled during the analysis. When finished, the results can be accessed through the overlap and position tabs. The overlap tab offers a visual analysis of overlap values for each pair of uploaded gene lists. Six tables represent six different categories. The first column and the first row of each table contain gene list names and each cell contains an overlap value between two corresponding lists. The value is coloured according to the overlap success rate where dark red colour indicates the lowest and light green indicates the highest overlap. The position tab offers an analysis of the position of the searched concept in each gene list's BCS. With the help of a chart and a table, the user can inspect which concepts were found by AGRA for each gene list and how they were ranked. The position of the searched concept is marked. 4 LIMITATIONS In future work, we will address a number of AGRA's current limitations. Currently, FACTA uses its internal dictionary for associating proteins with their UniProt IDs, thus not every gene is associated with all of its proteins. Newer versions of FACTA will address this issue. Furthermore, some of the biomedical concepts found by the system indicate the same term (e.g. ‘cancer’ and ‘neoplasm’) but they can be ranked in different ways which can affect the quality of the final results. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>mirPub: a database for searching microRNA publications</Title>
    <Doi>10.1093/bioinformatics/btu819</Doi>
    <Authors>Vergoulis Thanasis, Kanellos Ilias, Kostoulas Nikos, Georgakilas Georgios, Sellis Timos, Hatzigeorgiou Artemis, Dalamagas Theodore</Authors>
    <Abstract>Summary: Identifying, amongst millions of publications available in MEDLINE, those that are relevant to specific microRNAs (miRNAs) of interest based on keyword search faces major obstacles. References to miRNA names in the literature often deviate from standard nomenclature for various reasons, since even the official nomenclature evolves. For instance, a single miRNA name may identify two completely different molecules or two different names may refer to the same molecule. mirPub is a database with a powerful and intuitive interface, which facilitates searching for miRNA literature, addressing the aforementioned issues. To provide effective search services, mirPub applies text mining techniques on MEDLINE, integrates data from several curated databases and exploits data from its user community following a crowdsourcing approach. Other key features include an interactive visualization service that illustrates intuitively the evolution of miRNA data, tag clouds summarizing the relevance of publications to particular diseases, cell types or tissues and access to TarBase 6.0 data to oversee genes related to miRNA publications.</Abstract>
    <Body>1 Introduction MicroRNAs (miRNAs) are small, single-stranded endogenous RNAs, which suppress the expression of protein coding genes. They are important regulators of many crucial cellular processes and they are implicated in a multitude of diseases such as rheumatoid arthritis, multiple sclerosis, Alzheimer, cancer, etc. ( Soifer  et al ., 2007 ). The number of miRNA-related publications is growing constantly every year, reaching more than 12,700 papers until 2013. Keyword search to retrieve publications relevant to particular miRNAs faces three obstacles: (i) publications may refer to the same miRNA using different names, (ii) publications may refer to miRNAs with old, obsolete names and (iii) results retrieved as relevant should be carefully examined, since, e.g. sequence changes might have happened in referenced miRNAs, after the publication date. mirPub is a database with a powerful and intuitive interface that addresses the aforementioned issues. During miRNA literature search, mirPub takes into account both inconsistencies in naming miRNAs and the history of miRNA data as being recorded in all available miRBase ( Griffiths-Jones  et al ., 2005 ) versions. mirPub considers miRNA-publication associations based on text mining techniques on MEDLINE articles, integrated data from several curated databases and data provided by its user community. All stored associations are available through mirPub’s Web Interface (see  Supplementary Fig. S1  of  Supplementary Data ). Users are able to search for publications related to particular miRNAs by providing keywords. mirPub matches the keywords to stored miRNA terms. In case of zero or multiple matches for a given keyword, mirPub provides a set of keyword suggestions (string similarity is used for the case of zero matches). Otherwise, the relevant publications are presented as a chronologically ordered list. Along with the article title and year of publication, each mirPub result provides access to TarBase 6.0 ( Vergoulis  et al ., 2012 ) data to oversee gene targets mentioned in the article, as well as links to relevant external resources (e.g. articles abstract and full text) and MeSH metadata. mirPub expands the set of user keywords to also contain the families of the identified miRNAs and miRNA name variants as well (e.g. old or alternative names). The complete set of keywords used for each search is displayed and the user is able to keep only a subset of them filtering out the irrelevant publications (details in Section 1 of  Supplementary Data ). Finally, mirPub provides tag clouds summarizing MeSH diseases, tissues and cells that are relevant to the displayed publications, giving an insight about the role of the queried miRNAs. Regarding the miRNA data history, mirPub also provides an interactive visualization service that intuitively illustrates the timeline of changes for any mature or hairpin miRNA (see  Supplementary Fig. S2  in  Supplementary Data ). In particular, each mature or hairpin miRNA involved in a user query is accompanied by an information button, which activates a pop-up window that visualizes the name and sequence changes related to each particular molecule. This is a key feature of mirPub, since this information enables users to refine their search, excluding, for instance, publications related to a different molecule that had the same name in the past or including publications that refer to miRNAs of interest with different, older names. Preliminary experiments presented in Section 3 evaluate the usefulness of the aforementioned feature. 2 System implementation The majority of miRNA-to-publication associations in mirPub database have been discovered by applying text mining techniques on titles, abstracts and full texts of all available MEDLINE publications. Full-text articles were obtained from PMC only for open-access papers. mirPub’s text mining method is presented in  Figure 1 . In brief, mirPub seeks appearances of terms that describe miRNA molecules or families in MEDLINE publications.
 Fig. 1. mirPub's text mining method. miRBase Parser produces the set of ‘official names’ consisting of all miRNA and family names recorded in miRBase. Term Extractor expands this set to produce the set of ‘miRNA terms’, which also contains variants of the official names based on predefined modification rules (see Section 2). Publication Indexer builds an inverted index on MEDLINE texts and association extractor utilizes this index to retrieve efficiently all appearances of miRNA terms in literature. The aforementioned appearances indicate miRNA publication associations 
 Many of the aforementioned terms are official miRNA molecule or family names collected from all available miRBase versions by mirPub’s ‘miRBase Parser’ component. During its execution, this component captures the whole miRBase history, relating each mature or hairpin name with particular nucleotide sequences, an information needed by mirPub’s miRNA data history visualization feature (for details see Section 2 of  Supplementary Data ). The rest terms are variants of the collected official names produced by mirPub’s ‘Term Extractor’ based on two predefined  modification rules . The first rule consists of omitting the species prefix of the name (e.g. omit ‘hsa’ from ‘hsa-let-7a-5p’). In fact, it is very common in literature that a miRNA name appears without this prefix. The second rule employs replacing some tokens of miRNA names by others. For instance, token ‘mir’ is frequently replaced by tokens ‘mirna’ or ‘microrna’ and token ‘let’ is replaced by ‘mir-let’, ‘mirna-let’ or ‘microrna-let’. Similar rules have been also used in  Xie  et al . (2013) . To support efficient retrieval of publications that contain each miRNA term, mirPub builds a Lucene ( http://lucene.apache.org/core/ ) inverted index for the titles, the abstracts and the full texts of MEDLINE articles. mirPub’s ‘Association Extractor’ probes this index for each miRNA term produced by Term Extractor and stores all discovered miRNA-to-publication associations in mirPub’s relational database. Execution of the above-described text mining process will be performed in a regular basis on the most recent, hitherto, version of MEDLINE files. Nevertheless, this process fails to retrieve some miRNA-to-publication associations for various reasons. For instance, some associations are described in figures, thus optical character recognition must be used to retrieve them. Moreover, many articles are of restricted access, thus their full text is not available for text mining. To capture part of the previous cases, mirPub also incorporates data from several curated databases such as miRBase, TarBase and miR2Disease ( Jiang  et al ., 2009 ). Additionally, mirPub incorporates a User Interface to report new miRNA-to-publication associations, as well as errors (see  Supplementary Figs S1  and  S3  of  Supplementary Data ). A specific protocol (for details refer to Section 1 of  Supplementary Data ) ensures that each newly reported association or error will be examined by a curator prior its inclusion in mirPub’s database. This supervised crowdsourcing approach is expected to guarantee that mirPub contains correct and up-to-date data. 3 Evaluating mirPub’s contribution Currently, mirPub is the largest database that contains miRNA-to- publication associations counting &gt;210,000 distinct associations (involving &gt;19,800 articles). A comparison of mirPub to the most popular databases providing similar information can be found in  Supplementary Table S1  of  Supplementary Data . In brief, mirPub has been found to contain about 14-fold more publications than TarBase. Furthermore, preliminary experiments regarding mirPub’s effectiveness in retrieving miRNA literature are also presented in  Supplementary Data . A first experiment indicates that mirPub has a recall that is more than 2-fold the recall of PubMed in retrieving miRNA-related publications ( Supplementary Table S3  in  Supplementary Data ). This is possible as mirPub incorporates curated associations between miRNAs and publications. Another experiment concludes that taking into account miRNA data history improves search effectiveness ( Supplementary Table S4  in  Supplementary Data ). 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>AlignGraph: algorithm for secondary de novo genome assembly guided by closely related references</Title>
    <Doi>10.1093/bioinformatics/btu291</Doi>
    <Authors>Bao Ergude, Jiang Tao, Girke Thomas</Authors>
    <Abstract>Motivation:
De novo assemblies of genomes remain one of the most challenging applications in next-generation sequencing. Usually, their results are incomplete and fragmented into hundreds of contigs. Repeats in genomes and sequencing errors are the main reasons for these complications. With the rapidly growing number of sequenced genomes, it is now feasible to improve assemblies by guiding them with genomes from related species.</Abstract>
    <Body>1 INTRODUCTION Recent advances in next-generation sequencing (NGS) have made it possible to sequence new genomes at a fraction of the time and cost required only a few years ago. These improvements now allow experimental scientists to integrate genome sequencing approaches into their daily research. In the absence of a close reference genome, whole-genome shotgun NGS sequencing is the most common approach where a  de novo  assembly algorithm is used to join reads into longer continuous contigs and scaffolds. Most NGS  de novo  assemblers create a string or de Bruijn graph representing the connections among the reads and output the paths in the graph as assembled contigs. Examples of these algorithms include Edena ( Hernandez  et al. , 2008 ), Velvet ( Zerbino and Birney, 2008 ), ABySS ( Simpson  et al. , 2009 ), ALLPATHS-LG ( Gnerre  et al. , 2011 ), SOAPdenovo ( Li  et al. , 2010 ;  Luo  et al. , 2012 ), MaSuRCA ( Zimin  et al. , 2013 ), CABOG ( Miller  et al. , 2008 ), Euler-USR ( Chaisson  et al. , 2009 ) and IDBA ( Peng  et al. , 2010 ). This  de novo  assembly approach fundamentally differs from  alignment-based resequencing  approaches, where the NGS reads are not assembled but aligned against a similar reference genome using a variant tolerant short read aligner and structural variant detection algorithms ( Ossowski  et al. , 2008 ;  Zeitouni  et al. , 2010 ). Large parts of the sequence of the target genome can then be reconstructed from the mismatches, indels and rearrangements observed in the alignment results.  De novo  assemblies tend to be computationally more challenging than alignment-based approaches. Additional limitations include (i) the assembly results are often fragmented into large numbers of contigs, (ii) the genome covered by the assembled contigs/scaffolds is commonly incomplete and (iii) the frequency of falsely assembled contigs can be high because of chimeric joins. The most important reasons for these complications are usually sequencing errors, repeat sequences in the target genome, non-uniform sequencing depth and limited read length of NGS data. These error sources result in false-positive, incomplete and branched paths in the assembly graph, and thus greatly limit the lengths and completeness of the final contigs ( Chaisson and Pevzner, 2008 ;  Peng  et al. , 2010 ;  Zerbino and Birney, 2008 ). Combining both  de novo  assembly and alignment-based approaches presents a powerful alternative when a closely related reference genome sequence is available, but its genetic differences relative to the target genome are too pronounced to resolve them with an alignment approach alone ( Phillippy  et al. , 2008 ;  Schatz  et al. , 2013 ;  Schneeberger  et al. , 2011 ). In this case, one can first assemble the reads into contigs and then align them together with the reads to the reference. The much longer contigs facilitate the identification of complex rearrangements, whereas the read alignments are useful for detecting smaller variations in regions that are not covered by contigs. Because of the rapidly increasing number of reference genomes becoming available for most organism groups, this  reference-assisted assembly approach  will soon become the default option for many genome sequencing projects. Compared with  de novo  assemblies, reference-assisted assemblies have many advantages. First, the alignments of the contigs and reads against the close reference provide valuable proximity information that can be used to extend contigs with additional reads and to join contigs even if they overlap only by a few nucleotides. Second, the proximity information in the alignment can also be used to orient and order contigs along the reference to build a scaffold map of the entire assembly. Third, the alignment map can be used to evaluate the quality of contigs and pinpoint potential misassemblies. Previous studies on reference-assisted assemblies include the AMOScmp software ( Pop  et al. , 2004a ), an add-on tool for the ARACHNE assembler ( Gnerre  et al. , 2009 ) and custom workflows largely based on existing assembly software (e.g.  Schneeberger  et al. , 2011 ). The first two were designed primarily for Sanger reads, whereas the latter has been used for NGS genome assembly. Downstream of the primary assembly, scaffolding algorithms, such as RACA ( Kim  et al. , 2013 ), can be used to order and orient preassembled contigs to a connection map by incorporating additional sequence information from mate pair or PE reads and/or from closely related genomes ( Boetzer  et al. , 2011 ;  Dayarian  et al. , 2010 ;  Gao  et al. , 2011 ;  Gritsenko  et al. , 2012 ;  Pop  et al. , 2004b ;  Salmela  et al. , 2011 ). The resulting scaffolds often contain gaps, which are unresolved sequence areas between the original contigs. Dedicated gap-filling algorithms can be used to partially fill these gaps ( Boetzer and Pirovano, 2012 ;  Luo  et al. , 2012 ;  Tsai  et al. , 2010 ). More recently, components of reference-based strategies have also been incorporated into some of the  de novo  assembly suites themselves such as the  cheat  mode option of ALLPATHS-LG ( Gnerre  et al. , 2011 ) and IDBA-hybrid (unpublished). This study proposes a novel algorithm, called AlignGraph, for improving the lengths and completeness of contigs or scaffolds by reassembling them with help provided by a reference genome of a closely related organism. In contrast to existing reference-assisted methods, AlignGraph is a  secondary assembly algorithm  that loads the alignment information of PE reads and preassembled contigs/scaffolds against the reference into a novel assembly graph, called the  PE multi positional de Bruijn graph , that we specifically designed for facilitating secondary assemblies. By traversing this graph, the contigs or scaffolds of the primary assembly can be extended and joined. AlignGraph differs from most scaffolding algorithms by extending contigs exclusively with resolved rather than unresolved bases (Ns) and by acting either upstream and/or downstream of them. AlignGraph’s functionalities are unique by solving several challenges in improving assembly results. As a de Bruijn graph-based method, it solves limitations typical for many heuristic extension methods that are often used in the  de novo  assembly area ( Dohm  et al. , 2007 ;  Jeck  et al. , 2007 ;  Warren  et al. , 2007 ). For instance, if there are multiple solutions for how to extend a contig, then finding the correct one can be challenging with most heuristic methods. Those ambiguous solutions, which correspond to branched paths in the de Bruijn graph, are usually caused by repetitive sequences in genomes and frequently lead to early terminations of the contig extension process. The de Bruijn graph method is often more efficient than heuristic methods in finding the correct solution here because the contextual information, required for resolving these ambiguities, is maintained in the graph ( Chaisson and Pevzner, 2008 ;  Zerbino and Birney, 2008 ). This issue is not as pronounced in assemblies with much longer Sanger reads, as those are more likely to span non-repetitive regions with repetitive regions in between ( Gnerre  et al. , 2009 ). Thus, it is particularly important to address this problem in assemblies with short reads. In comparison with the conventional de Bruijn graph, our PE multipositional de Bruijn graph has several additional advantages. First, many branched paths can be eliminated directly in the graph with the help of the additional PE read and alignment information. This simplifies the identification of correct paths. Second, many false-positive paths, caused by sequencing errors, can be eliminated by correcting erroneous reads with correct reads that align to the same position in the reference genome. Third, guided by the alignment information to the reference genome, the PE multipositional de Bruijn graph is less affected by regionally low read coverage that often gives rise to incomplete paths in the conventional de Bruijn graph. As a result, many incorrect extensions and early terminations can be avoided. 2 METHODS 2.1 AlignGraph algorithm This section describes the AlignGraph algorithm. Its workflow can be divided into the following three major steps.  Figure 1 B illustrates these steps with an example.
 Alignment maps . The PE reads are aligned against both the preassembled contigs and the close reference genome; the contigs are aligned against the reference. Contig reassembly . The alignment mapping results are used to construct a positional variant of the de Bruijn graph, called the  PE multi positional de Bruijn graph . Graph traversal . The resulting graph is edited and traversed to obtain extended contigs. 
Throughout the text, the source genome of the PE reads and the preassembled contigs is referred to as the  target genome , whereas the genome of the closely related species for guiding the contig improvement steps is referred to as the  reference genome . For simplicity, the following description of AlignGraph refers mostly to contigs, but it also applies to scaffolds containing a limited amount gaps.
 Fig. 1. Overview of the AlignGraph algorithm. The outline on the top ( A ) shows AlignGraph in the context of common genome assembly workflows, and the one on the bottom ( B ) illustrates its three main processing steps. (A) In Step 1, the PE reads from a target genome are assembled by a  de novo  assembler into contigs (here  c 1 , 
 c 2  and  c 3 ). Subsequently (Step 2), the contigs can be extended (blue) and joined by AlignGraph ( e 1  and  e 2 ). (B) The workflow of AlignGraph consists of three main steps. (i) The PE reads are aligned to the reference genome and to the contigs, and the contigs are also aligned to the reference genome. (ii) The PE multipositional de Bruijn graph is built from the alignment results, where the red and blue subpaths correspond to the aligned contigs and sequences from PE reads, respectively. (iii) The extended and/or joined contigs (here  e 1  and  e 2 ) are generated by traversing the graph Prerequisites Before the above steps, the user is expected to generate genomic PE reads for the target genome of interest and to assemble them with a  de novo  NGS genome assembler. Because most genome assemblers perform better with PE than single-end data, AlignGraph also depends on this sequence type. A major advantage of AlignGraph is its design to work with most genome assemblers, but the quality of the initial  de novo  assembled contigs is expected to impact the final results (see Section 3.2). For optimal results, it is also important to follow the recommendations of the chosen  de novo  assembler with respect to insert length of the sequencing library, minimum coverage of the target genome with PE reads and other recommendations. If scaffolds are inputted, it is usually beneficial to fill them with a gap-filling algorithm before processing them with AlignGraph (e.g.  Boetzer and Pirovano, 2012 ). Another requirement for AlignGraph is the availability of a closely related reference genome sequence. Nearly complete reference genomes of high quality will yield the best results, but partially sequenced genomes can be used as well. (i) Alignment maps In the initial preprocessing step of AlignGraph, the PE reads, used for the  de novo  assembly in the  Prerequisite  section, are aligned to the contigs and to the reference genome, and the contigs are also aligned to the reference genome. Aligning the reads to the contigs simplifies their alignments to the reference by guiding them with the much longer contigs as backbone (see below). Generating reliable alignments among the PE reads and the contigs is relatively straightforward because both are from the same genetic background, thus requiring a low level of variant tolerance in the alignments. Aligning the contigs to the reference genome demands a higher level of variant tolerance. However, because of the relatively large length of the contigs, their alignments to the reference can also be generated reliably, as long as the evolutionary distance between the target and reference genome is not too large. The current implementation of AlignGraph uses Bowtie2 and BLAT for these two alignment steps, respectively ( Kent, 2002 ;  Langmead and Salzberg, 2012 ). In contrast to this, aligning the relatively short PE reads to the reference genome is a more challenging task owing to the difficulty of generating reliable short alignments containing larger numbers of mismatches and gaps. This problem does not apply to the reads aligning to the contigs because their alignment positions to the reference genome can be inferred from the more robust contig alignments. For the PE read to reference genome alignment, it is important to choose a highly variant tolerant short read aligner that is able to reliably align most of the short reads to their  true  source locations in the reference genome while minimizing the number of false-positive read placements. Clearly, the latter would negatively impact the precision performance of AlignGraph by leading to chimeric joins in the downstream contig extension steps. Although a wide range of short read aligners has been developed over the past years ( Li and Homer, 2010 ), none of them has been specifically designed or optimized for aligning short reads against reference genomes with sequence differences more pronounced than those observed among genomes within the same species. To minimize the above challenges, we have chosen for this critical step the highly tunable Bowtie2 aligner with parameter settings that we optimized for aligning PE reads from a target genome to a reference genome sharing variable degrees of sequence similarity. The use of PE read alignments in this step is also important because the additional sequence information, provided by the second read in a PE, increases the specificity of the alignment process compared with single-end reads, and thus reduces the number of false read placements. To account for rearrangements among the two genomes, we use for the alignments of the PE reads against the reference genome more relaxed insert length variation settings than in the alignments against the contigs. (ii) Contig reassembly with PE multipositional de Bruijn graph The core functionality of AlignGraph is the extension of the contigs by reassembling them using the alignment results obtained in the previous step. To achieve this efficiently, we build from the alignment maps a variant of the de Bruijn graph, here called the  PE multi positional de Bruijn graph . This method combines the PE de Bruijn graph ( Medvedev  et al. , 2011 ) and the positional de Bruijn graph ( Ronen  et al. , 2012 ), where we incorporate both PE read information and alignment positions into the graph ( Pevzner  et al. , 2001 ). The former was designed to generate more complete contigs in  de novo  assemblies and the latter to correct contig errors in secondary assemblies. Our approach solves several problems in improving assembly results that we briefly discussed in Section 1 (see also  Table 1 ). The following describes our modified de Bruijn graph in more details, where we first introduce important concepts of conventional de Bruijn graph-based assembly methods.
 Table 1. Problems the PE multipositional de Bruijn graph solves in comparison with the conventional de Bruijn graph Problem Consequence Solution Repeat sequences Branched paths Distinguishes paths for repetitive regions by incorporating PE read and alignment position information Sequencing errors False-positive paths Corrects paths from erroneous reads with correct reads aligned to the same position Low sequencing depth Incomplete paths Builds paths from reads in low coverage areas supported by reference Background The most widely used method for genome assemblies from short reads is the de Bruijn graph method ( Pevzner  et al. , 2001 ). A de Bruijn graph is a directed graph: two connected nodes represent  k  + 1 bases, where the first node represents the first  k  bases, and the second node the second  k  bases (called  k-mer ). To construct a de Bruijn graph,  l − k + 1  connected nodes are constructed from each read of length  l , and two nodes from different reads are joined if they share the same  k -mers. In theory, the graph contains a walk representing the full sequence of the genome if traversed properly. However, this walk is hard to obtain in practice because of false-positive, incomplete and branched connections in the graph that are caused by errors in the reads and repeats in the genome. The false-positive and incomplete paths are due to false-positive  k -mers with sequencing errors and missing  k -mers from regions of low sequencing depth, respectively. The branched paths are caused by joins of  k -mers from repetitive regions. Several variations of the de Bruijn graph have been proposed to solve these limitations, especially the branched paths, while preserving all of its genome information ( Medvedev  et al. , 2011 ;  Peng  et al. , 2010 ;  Ronen  et al. , 2012 ). The PE de Bruijn graph ( Medvedev  et al. , 2011 ) is built from PE reads, where each  k -mer contains  k  bases from the left pair plus its corresponding  k  bases from the right pair. In contrast to this, the positional de Bruijn graph ( Ronen  et al. , 2012 ) incorporates read alignment information by including in each  k -mer the  k  bases plus its alignment position. With the additional information assigned to the  k -mers,  k -mers from repetitive regions can often be distinguished, and thus the number of branches in the graph can be reduced. In addition, because the positional de Bruijn graph is built from read alignments, false-positive and incomplete paths can be largely avoided. We emphasize that the PE de Bruijn graph requires the left pair forward-strand read and the right pair reverse-strand read or vice versa, but it is difficult to know their orientation. This problem can be resolved if the PE de Bruijn graph is built from aligned reads, where their orientation can be obtained from the alignments. PE multipositional de Bruijn graph We derive the  PE multi positional de Bruijn graph  as a combination of the PE de Bruijn graph and the positional de Bruijn graph. Each  k -mer of the PE multipositional de Bruijn graph is composed of three left/right element pairs: the  k  bases of each the left and the right read pair (called  left or right k bases ), the alignment position of each the left and the right  k  bases to the contigs and the alignment position of each the left and the right  k  bases to the reference genome. Two  k -mers can be joined if they have similar  k  bases and close alignment positions within the constraints defined in the formulas below. Formally, let  s  be the  k  bases from the left read pair and  s ′  the corresponding  k  bases from the right read pair, then the  k -mer of PE multipositional de Bruijn graph is a 6 tuple ( s , 
 s ′ , 
 c , 
 g ,  c ′ , 
 g ′ ), where  c  is the alignment position of  s  to the contigs,  g  is the alignment position of  s  to the reference genome,  c ′  is the alignment position of  s ′  to the contigs and  g ′  is the alignment position of  s ′  to the reference genome. Two  k -mers ( s i , 
 s i ′ , 
 c i , 
 g i , 
 c i ′ , 
 g i ′ ) and ( s j , 
 s j ′ , 
 c j , 
 g j , 
 c j ′ , 
 g j ′ ) can be joined if constrains  (1)–(6)  are met:
 (1) mismatch ( s i , s j ) &lt; δ 
 (2) mismatch ( s i ′ , s j ′ ) &lt; δ 
 (3) | c i − c j | &lt; ϵ or c i = − 1 or c j = − 1 
 (4) | g i − g j | &lt; ϵ 
 (5) | c i ′ − c j ′ | &lt; ϵ + 2 D or c i ′ = − 1 or c j ′ = − 1 
 (6) | g i ′ − g j ′ | &lt; ϵ + 2 D 
where δ and ε are small numbers with the default values 5 and 25, respectively, and  D  is the variability of the insert length  I  of the PE reads. The variability  D  is equal to  max ⁡ { I u − I , I − I l } , where  I u  and  I l  are the upper and lower limits of  I , respectively. The variables in the above formulas are explained below.
 δ: To join two  k -mers and tolerate sequencing errors, we allow a small number of mismatches δ between  s i  and  s j  and between  s i ′  and  s j ′  in  (1)  and  (2) , respectively. ε: We allow a small shift ε between each pair of alignment positions in  (3)–(6)  because the same  k  bases  s i  and  s j  (or  s i ′  and  s j ′ ) from different reads may align to different but close positions in the contigs or genome, as discussed in  Ronen  et al.  (2012) . 2 D : We allow a shift  2 D  of  s i ′  and  s j ′ ’s alignment positions to the contigs in  (5)  and to the reference genome in  (6) . The maximum and minimum alignment distances between a read pair are  I − l + D  and  I − l − D , respectively, where  l  is the read length, assuming the same read length for both members in a pair. Thus, the maximum alignment distance of two right reads with left reads aligned at the same position is  ( I − l + D ) − ( I − l − D ) = 2 D . This distance is equal to the distance between any two  k -mers from the same position in the right read pairs, so the maximum distance between  s i ′  and  s j ′  will be  2 D . −1:  s i  and  s j  (or  s i ′  and  s j ′ ) can be joined if one or both of them are aligned directly to the reference genome rather than guided by the  de novo  contigs. In those cases, we assign  − 1 as alignment position to the contigs. This is important because we allow contig extensions only if the alignable and unalignable bases to contigs can be joined. 
It is important to guarantee that each  k -mer corresponding to an insertion of a read alignment has a position in the reference genome. To achieve this, we append the inserted  k -mer to the end of the genome sequence. In our implementation of the PE multipositional de Bruijn graph, we first iteratively load sections of the reference genome into memory. Then we perform the following operation. We test for each  k -mer in each aligned read at genome position  g , whether there is already a  k -mer at  g  and whether the new  k -mer can be joined with it. If so then we join the two  k -mers; otherwise, we attach the new  k -mer to position  g . The connection between two  k -mers is recorded by using pointers, and the read coverage for each  k -mer is stored along with it.  Figure 2  illustrates the main advantages of the PE multipositional de Bruijn graph compared with the positional de Bruijn graph with several examples (see also  Table 1 ). This includes the contig-guided PE read alignment against the reference genome resulting in a larger number of alignable reads, and thus a more complete de Bruijn graph ( Fig. 2 B), as well as the reduction of branched paths in the graph by distinguishing reads from different repetitive regions ( Fig. 2 C and D). For space reasons, the advantages over the conventional de Bruijn graph in reducing false-positive and incomplete paths are not shown. The PE multipositional de Bruijn graph can converge to the positional de Bruijn graph by relaxing the above constraints  (2) ,  (3) ,  (5)  and  (6) . In our tests, the former shows usually an up to 5% better performance than the latter with respect to several sensitivity measures (see below for definitions). This improvement is considerable because the number of branches that are only resolvable by the PE multipositional de Bruijn graph is usually not large.
 Fig. 2. Advantages of the PE multipositional de Bruijn graph compared with the positional de Bruijn graph. In the target genome given on the top,  A  and  A ′ , 
 C  and  C ′ , 
 E  and  E ′ , 
 G  and  G ′  are repetitive regions. Each PE read of length 2 × 4 bp is sequenced with one pair from region  A B C D A ′ C ′  and the other from the corresponding position of region  E F G H E ′ G ′  (the pair from  E F G H E ′ G ′  is omitted for simplicity). In comparison with the target genome, the reference genome has a repeat-free region  ABC  similar to  A B C D A ′ C ′  and a region  E F G H E ′  similar to  E F G H E ′ G ′ . The reads from region  A B C D A ′ C ′  are assembled with a  de novo  assembler into a contig starting from  C D A ′ C ′ , but regions  A  and  B  are not assembled because of low sequencing depth, repeats or other problems. When aligning the contig to the reference genome, the repetitive regions  C  and  C ′  are both aligned to  C  in the reference genome, and the insertion  D  is assigned to the end of the reference. In ( A ) reads are aligned directly to the reference genome to build the initial positional de Bruijn graph, and in ( B–D ) the reads are aligned to the preassembled contigs and then aligned to the reference to build first the extended positional de Bruijn graph and then the PE multipositional de Bruijn graph. (A) The initial positional de Bruijn graph is built here with 3-mers. Some reads cannot be aligned to the reference genome because of sequence differences in the target genome, as indicated here by 3-mers with −1 as alignment position. The repetitive regions  A  and  A ′  (or  C  and  C ′ ) are collapsed into one path in red in the graph. (B) The initial positional de Bruijn graph is constructed with help from the read-to-contig alignment information. The read-to-reference genome alignment information yields a more complete positional de Bruijn graph, but the repetitive regions  A  and  A ′  (or  C  and  C ′ ) are still collapsed resulting in branch points. (C) An extended positional de Bruijn graph is built by incorporating into each 3-mer the read alignment position to the contig. As a result of this operation, the repetitive regions  C  and  C ′  can be distinguished into two paths, where the 3-mers have different alignment positions in the contig, but  A  and  A ′  are still collapsed. (D) The PE multipositional de Bruijn graph is constructed by incorporating into each 3-mer their PE read alignment positions to the reference genome (the right three bases and their alignment position to the contig is omitted here). With this information the repeats  A  and  A ′  can be distinguished into two paths, as the 3-mers have different PE alignment positions in the reference genome. The final graph contains only one single path allowing to output an extended contig corresponding to the region  A B C D A ′ C ′  in the target genome (iii) Graph traversal returns extended contigs To remove errors, the de Bruijn graph needs to be edited before its traversal. The three major types of errors are tips, bubbles and erroneous connections ( Chaisson and Pevzner, 2008 ;  Peng  et al. , 2010 ;  Zerbino and Birney, 2008 ). Most of them are caused by errors in the reads. A tip is a short path with a dead end, whereas a bubble consists of two short paths sharing the same start and end nodes. Most of the tips and bubbles can be corrected by joining  k -mers with  &lt; δ  mismatches. The remaining errors can be removed by applying a coverage cut-off filter similar to the strategies used by most  de novo  assemblers. Because of the additional information encoded in the modified de Bruijn graph, one can use here a relatively small coverage threshold. After these error removal steps, the PE multipositional de Bruijn graph is traversed, using a breadth-first strategy, to generate the final contigs. Each traversal stops at a branch position and an extended contig is returned. After returning the extended contigs, the remaining unextended contigs (identical with initial  de novo  contigs) are provided to the user in a separate file. Finally, contigs with sufficient PE read connections and a path between them can be joined. Occasionally, those connections can be missed by the above filtering step because of too low read coverage in local areas of the connecting path. 2.2 Software implementation AlignGraph is implemented in C++ for Linux operating systems. Its required input includes the PE reads, the preassembled  de novo  contigs and the reference genome. Its output includes the extended contigs as well as the remaining non-extended contigs. AlignGraph runs the alignment steps with BLAT and Bowtie2 automatically, but both need to be installed on the system. AlignGraph’s run time is currently 23–57 min per million aligned reads. In the performance tests of this study, the memory usage was 36–50 GB, and it stays &lt;100 GB even for entire mammalian genomes. These requirements are more moderate than those of most  de novo  assemblers ( Luo  et al. , 2012 ). 3 EVALUATION 3.1 Experimental design Background To evaluate AlignGraph’s efficiency in improving genome assemblies, we performed a series of systematic performance tests. For this, we downloaded publicly available assemblies and/or assembled genomic PE read sets from organisms of variable genome complexity with seven widely used  de novo  assemblers, extended the resulting contigs with AlignGraph, and then evaluated the improvements with a set of standard metrics for comparing assembly results ( Table 2 ). In these tests, it was important to choose the NGS read samples from organisms where the genome sequence of both the target genome and a close reference genome are known. This way one can evaluate the completeness and correctness of the results against a true result rather than one that is unknown or only partially known. To also assure the improvements obtained by AlignGraph are not simply the result of insufficient optimizations of the upstream  de novo  assembly, we included preassembled contig and scaffold sets that are widely accepted by the community as benchmark datasets for evaluating assembly software. Today’s requirements for assembling genomes from NGS were met by choosing read samples with ≥75 bp and PE read information. In total, we performed assembly tests on the following three sample sets.
 Table 2. Performance evaluation of AlignGraph Upstream assembler Contig set N Contigs 4 N50 5 N covered bases 6 Average length 7 Maximum length 8 MPMB 9 Average identity 10  (%) (a) Contigs of  A.thaliana  genome Velvet All 1 30 037 3515 82 844 417 2668 27 792 22.2 95.2 Extendable 2 8615 4148 28 007 451 3262 27 398 0.3 97.6 Extendable + AlignGraph 3 5751 7876 32 467 110 5521 49 768 1.6 94.8 ABySS All 30 972 2559 69 432 667 2206 29 760 13.4 97.2 Extendable 11 693 2820 28 885 212 2454 16 343 0.5 98.7 Extendable + AlignGraph 8427 5484 35 859 786 4151 25 321 1.1 95.8 (b) Contigs of human chromosome 14 ALLPATHS-LG All 4383 38 590 83 849 397 19 201 240 764 0.3 98.9 Extendable 1674 39 851 35 746 095 20 806 200 495 0.1 98.9 Extendable + AlignGraph 785 71 847 36 441 001 45 358 305 880 0.0 97.5 ALLPATHS-LGc All 3856 43 856 83 860 939 21 818 275 446 0.2 99.3 Extendable 1296 45 719 31 457 201 24 346 275 446 0.1 99.5 Extendable + AlignGraph 608 86 613 34 614 465 54 406 294 615 0.0 96.9 SOAPdenovo All 10 865 16 855 80 135 941 7623 147 494 5.9 94.9 Extendable 5613 17 412 45 246 077 8223 141 981 0.9 96.4 Extendable + AlignGraph 3469 32 881 52 861 640 15 271 219 841 0.5 95.0 MaSuRCA All 19 034 5767 75 497 302 3802 53 837 13.9 98.9 Extendable 9241 6047 38 842 517 4199 51 249 0.2 99.2 Extendable + AlignGraph 5665 11 590 43 930 184 7666 66 758 0.4 98.1 CABOG All 3118 46 523 84 989 190 27 401 296 888 0.3 97.3 Extendable 1692 45 669 46 499 763 27 089 296 888 0.0 98.7 Extendable + AlignGraph 701 101 907 50 527 605 70 362 443 952 0.1 97.6 Bambus2 All 11 219 8378 64 011 072 5764 449 449 3.1 89.9 Extendable 6995 7521 37 857 989 5439 62 798 0.3 97.6 Extendable + AlignGraph 2722 19 989 39 147 357 14 176 86 154 0.5 96.5 (c) Scaffolds of human chromosome 14 SOAPdenovo All 3902 391 693 85 417 248 24 397 1 852 152 1.0 82.9 Extendable 901 387 309 40 296 035 47 526 1 019 659 0.1 84.5 Extendable + AlignGraph 767 544 209 47 823 279 63 525 2 246 638 0.1 81.0 MaSuRCA All 721 580 822 65 433 305 63 876 2 943 966 1.3 57.2 Extendable 101 289 703 5 554 781 52 820 1 516 804 0.0 81.9 Extendable + AlignGraph 78 316 946 6 986 224 86 552 1 573 741 0.0 83.4 CABOG All 471 387 876 81 163 688 176 590 1 944 475 0.1 91.9 Extendable 146 358 688 29 372 033 200 539 1 905 529 0.0 98.2 Extendable + AlignGraph 67 906 407 33 708 925 481 712 2 051 503 0.0 94.1 Bambus2 All 569 319 334 64 378 693 116 582 1 477 847 0.1 77.4 Extendable 66 272 436 6 949 338 119 858 641 463 0.0 92.0 Extendable + AlignGraph 80 377 905 8 963 132 114 852 812 353 0.1 85.4 (a) Genomic PE reads from  A.thaliana  were assembled with Velvet and ABySS. The resulting contigs were extended with AlignGraph using as reference the genome sequence from  A.lyrata . (b–c) The subsequent panels contain assembly results for the human chromosome 14 sample from the GAGE project where the chimpanzee genome served as reference. (b) Contig assembly results are given for the  de novo  assemblers ALLPATHS-LG, ALLPATHS-LGc (in cheat mode), SOAPdenovo, MaSuRCA, CABOG and Bambus2. (c) Scaffolded assembly results are given for SOAPdenovo, MaSuRCA, CABOG and Bambus2. The results are organized row-wise as follows: the number of initial contigs obtained by each  de novo  assembler 1 , the ‘extendable' subset of the initial contigs that AlignGraph was able to improve 2 , and the extension results obtained with AlignGraph 3 . The additional columns give the number of contigs 4 , N50 values 5 , the number of covered bases 6 , the average 7 , and maximum 8  length of the contigs, the number of misassemblies per million base pairs (MPMB) 9 , and the average identity among the true contigs and the target genome 10 . More details on these performance criteria are provided in Section 3.1.5. 3.1.1 Arabidopsis thaliana sample The first sample set was from the model organism  A rabidopsis thaliana , which is a flowering plant with a compact genome of 130 Mb in size. The PE read set chosen for this test is from a genomic Illumina NGS library with a read length of 2 × 75 bp. As  de novo  assemblers, we included in this test Velvet and ABySS, which we chose here as software representatives performing well on single library data and because of their good sensitivity and precision performance ( Lin  et al. , 2011 ). The VelvetOptimiser tool was used to optimize the parameter settings for the Velvet assembly. ABySS was run with the same  k -mer length as Velvet, whereas the remaining parameters were set to their defaults. To extend the preassembled contigs with AlignGraph, we used the publicly available genome sequence from the related  Arabidopsis lyrata  species as reference [ Table 2 (a)]. The latter was chosen because it constitutes a more challenging reference genome for testing AlignGraph’s performance in improving genome assemblies than the references used in the other tests below. This is the case for the following reasons ( Hu  et al. , 2011 ):  A. lyrata  and  A. thaliana  diverged &gt;10 million years ago; their genomes differ by many regional rearrangements; the sequence similarity in the common regions of their genomes is only 80%; and the  A. lyrata  genome sequence is still incomplete and fragmented into many scaffolds. 3.1.2 Human sample from Genome Assembly Gold-standard Evaluations The second sample set is from the community project GAGE (Genome Assembly Gold-standard Evaluations), from which we selected the sample for the human chromosome 14 ( Salzberg  et al. , 2012 ). Its Illumina sequences consist of PE reads with a length of 76–101 bp from three different libraries. We downloaded the preassembled contig sets provided by the GAGE project for the five assemblers that ranked highest in the benchmark tests by  Salzberg  et al.  (2012)  in assemblies from multiple genome libraries with variable insert lengths. Those included ALLPATHS-LG, SOAPdenovo, MaSuRCA, CABOG and Bambus2. As reference sequence for guiding AlignGraph, we used the chimpanzee genome. For ALLPATH-LG in its  cheat  mode, we reassembled the contigs ourselves because this assembler exhibits a better sensitivity and precision performance when providing a closely related reference genome. Here it was important to compare the performance of ALLPATHS-LG with AlignGraph when both are guided by the same reference genome. In addition to contigs, we evaluated AlignGraph’s performance in improving the scaffold sets provided by the GAGE project for the same human sample set. Before their reassembly with AlignGraph, we reduced the number of unresolved sequence regions (gaps filled with ambiguous N bases) in the scaffolds by applying the GapFiller algorithm, which is currently one of the most efficient gap-filling methods ( Boetzer and Pirovano, 2012 ). To also evaluate the influence of the similarity shared among the reference and target genomes on AlignGraph’s performance, we included tests with four reference genomes of variable similarity to the human genome. The reference genomes chosen for this test were from gorilla, orangutan, gibbon and macaque. The genome sequence from gibbon was the only one that is still incomplete containing scaffolds rather than fully assembled chromosomes. 3.1.3 Published genome In addition to the tests above, we were interested in evaluating to what extent AlignGraph can improve the genome sequence generated with another reference assisted assembly approach. For this test, we chose the published genome sequence from Landsberg  erecta  (L er -1;  Schneeberger  et al. , 2011 ). The latter is a strain of  A. thaliana , which is too diverged from the known references to resolve its genome sequence with a simple resequencing approach. Thus,  Schneeberger  et al.  (2011)  assembled its genome with a reference-assisted pipeline approach that included ALLPATHS-LG and several refinement steps. 3.1.4 Data sources The genome sequences used in the above tests were downloaded from the following community sites:  A. thaliana  from TAIR,  A. lyrata  from JGI, Landsberg  erecta  from 1001 Genomes and human and other primates from Ensembl. From the GAGE site, we downloaded the PE read sets and the preassembled contigs and scaffolds for the human chromosome 14 sample ( Salzberg  et al. , 2012 ). The PE read sets from  A. thaliana  and Landsberg  erecta  were downloaded from NCBI’s Sequence Read Archive and the 1001 Genome site, respectively. The  A. thaliana  read set contained 32 million 2 × 75 bp PE reads (accession: SRR073127), the human read set contained 62 million 2 × 76–101 bp PE reads and the L er -1 read set contained 73 million 2 × 40–101 bp PE reads. 3.1.5 Performance measurements Most of the performance measures used by this study are adapted from the GAGE project ( Salzberg  et al. , 2012 ). To evaluate the completeness of the contigs, we aligned them to the target genome with BLAT. If a contig could not be aligned as a single global alignment, it was split according to the local alignment results into the smallest possible number of subcontigs. The resulting contigs are called  true contigs . The precision measures include the number of misassemblies per million base pairs (MPMB) and the average identity between contigs and target genome. Misassemblies caused by misjoin errors result in chimeric contigs. Their number can be calculated as the number of splits necessary to obtain the true contigs. Thus, MPMB  , where  m  is the numbers of misassemblies, and  L  is the cumulative length of the contigs. The average identity between true contigs and the target genome is calculated as   where  t i  is the identity for contig  i , and  l i  is the length of contig  i  (0 &lt; i ≤ n ). In this formula, the identity  t i  of the true contigs  i  is calculated as the number of aligned bases over the length of the alignment. The sensitivity measures include the N50 value and the number of covered bases. The former is the contig size at 50% of the total number of contig bases, and the latter is the total number of genome bases covered by the contigs. Two additional measures are the average length and maximum length of the true contigs. In all tests, we considered only contigs with a length of at least 1000 bp but used the entire set (including the shorter ones) in AlignGraph’s extension steps. 3.2 Results 3.2.1 Extension of A.thaliana contigs The performance test results for the  A. thaliana  dataset are given in  Table 2 (a). In comparison with the initial contig sets assembled by Velvet or ABySS, AlignGraph extends 28.7–37.8% of them when guided by the  A. lyrata  genome as reference. The resulting set of extended contigs contains 27.9–33.2% less sequences because AlignGraph has joined many of the initial contigs. This leads to improvements of the N50, the number of covered bases, average contig length and maximum contig length for the extendable contig set by 89.9–94.5%, 15.9–24.1%, 69.3–69.2% and 54.9–81.6%, respectively. These improvements are accompanied only by minor increases of MPMB errors. The MPMB values of the extendable and extended contigs are usually much lower than for the complete set because of their pre-filtered nature that improves their quality. As expected the average identity also drops slightly (2.8–2.9%) because with increased length of the assembled sequences, internal sequence variations accumulate and complicate the alignment of the extended contigs against the target genome. A similar trend can be seen in the results below for the much longer scaffolds where the average identity is always lower for all of the tested assemblers [ Table 2 (c)]. For all three sample sets (Sections 3.2.1–3.2.4), the AlignGraph results contain a comparable number of sequence variations to the target genomes as the results of  de novo  assemblers (data not shown). This indicates a high sequence quality of the reassembled contigs. 3.2.2 Extension of human contigs and scaffolds from GAGE The test results for the human chromosome 14 contigs are given in  Table 2 (b). Of the contigs assembled by ALLPATHS-LG, 38.2% can be extended and the extension result contains 53.1% less contigs because of the joins generated by AlignGraph. These improvements are more pronounced than in the above experiment with  A. lyrata  as reference because the genomes of human and chimpanzee share a much higher sequence similarity than the genomes of  A. thaliana  and  A. lyrata . The N50, the number of covered bases, average contig length and maximum contig length for the extendable contig set consistently improve by 80.3, 1.9, 118.0 and 52.6%, respectively. Similar results could be obtained with the other  de novo  assemblers SOAPdenovo, MaSuRCA, CABOG and Bambus2. After AlignGraph processing, their extendable contigs improved for the same four evaluation metrics by 88.8–165.8, 3.4–16.8, 82.6–160.6 and 30.3–54.8%, respectively. If ALLPATHS-LG is run in its cheat mode by guiding it with the same reference genome as AlignGraph, then both the sensitivity and precision measures of the ALLPATHS-LGc contigs improve compared with the assembly without a reference. Nevertheless, AlignGraph is still able to extend 33.6% of the ALLPATHS-LGc contigs, and the extension results contain 53.1% less contigs, whereas the four evaluation metrics improve by 89.4, 10.0, 123.5 and 7.0%, respectively. These improvements indicate that the reference-assisted approach used by AlignGraph is more efficient than the one from ALLPATHS-LG in its cheat mode at the contig assembly stage. AlignGraph’s performance results on the scaffolds from the same human chromosome 14 dataset are given in  Table 2 (c). The scaffold sets from SOAPdenovo, MaSuRCA, CABOG and Bambus2 contain much smaller numbers of sequences than their corresponding contig sets. Nevertheless, AlignGraph is able to extend 11.6–31.0% of them and improve the N50 value and the number of covered bases by 9.4–152.7 and 14.8–29.0%, respectively. The extension results for the scaffold set of Bambus2 contain a slightly larger number of sequences (14) than the extendable set. The reason for this is that many of them are short, and AlignGraph extends them often to scaffolds with a length above the 1000 bp requirement, thus increasing the number of reported scaffolds (see Section 3.1.5). This trend also explains the slightly lower average length of the extended scaffold set from Bambus2. 3.2.3 Influence of similarity of reference genome To assess AlignGraph’s performance with reference genomes of variable similarity to the target genome, we post-processed the  de novo  assemblies of the human dataset with AlignGraph using as reference the genome sequences from five different primates. The columns in  Table 3  list these organisms according to their evolutionary distances to the human genome (increasing from left to right). To avoid confusions, exact sequence similarity values to the human genome are not provided because there are many possibilities to calculate them, which can lead to different results. As expected, the performance measures degrade with the evolutionary distance between the target and reference genomes. Although the first four reference genomes show respectable improvements, the macaque genome seems to be too diverged from human to achieve any major improvements. However, this performance drop is mainly due to the difficulty of aligning short reads to a highly diverged reference. Future improvements in NGS read length and alignment algorithms are likely to enhance AlignGraph’s performance in this regard.
 Table 3. Performance with reference genomes of variable similarity Percentage of Chimpanzee Gorilla Orangutan Gibbon Macaque Aligned reads a 94.5% 91.6% 88.9% 49.9% 24.9% Extendable contigs b 51.0% 36.4% 24.9% 6.7% — Improved N50 c 109.9% 84.0% 73.2% 65.3% — The tests were performed on the human chromosome 14 sample where the listed primate genomes served as reference. The results include the percentage values of  a alignable reads,  b Extendable contigs relative to the initial set  c Improvements of the N50 values relative to the extendable contigs. Because of space limitations, the latter two rows contain averaged percentage values for the five assemblers ALLPATHS-LG, SOAPdenovo, MaSuRCA, CABOG and Bambus2. 3.2.4 Improvements to published genome The test results for the published Landsberg  erecta  genome are shown in  Table 4 . The initial scaffold set used in this test consisted of 1676 sequences. AlignGraph extended 27.6% of these scaffolds, whereas the extended set contains 20.3% fewer scaffolds. In addition, AlignGraph improves the N50 value, the number of covered bases, the average contig length and maximum length values for the extendable scaffolds by 86.6, 8.1, 35.7 and 8.1%, respectively. These improvements demonstrate AlignGraph’s usefulness in improving published genome sequences, even for those that have been carefully curated by their authors.
 Table 4. Improvements to published genome Contig set N contigs N50 N total bases a Average length Maximum length All 1676 341 653 112 578 343 67 170 2 930 180 Extendable 462 448 682 57 574 961 124 621 2 930 180 Extendable + AlignGraph 368 837 458 62 216 675 169 067 3 168 537 The published scaffolds from Landsberg  erecta  were extended with AlignGraph using the  A.thaliana  genome as reference. The rows and columns are arranged the same way as in  Table 2 , but several columns are missing here because it is not possible to compute the corresponding performance measures in a meaningful manner without having access to a ‘true’ target genome sequence.  a In addition, we report here the total number of bases in the contigs. In summary, the above performance tests demonstrate AlignGraph’s efficiency in improving the results of a variety of  de novo  assemblers and species with variable genome complexity by taking advantage of closely related reference genomes. 4 CONCLUSIONS AND FUTURE WORK This study introduces a novel de Bruijn graph-based algorithm for improving  de novo  genome assemblies guided by sequence information from a closely related species. The chosen PE multipositional de Bruijn graph approach provides an elegant and efficient solution to this problem. Our performance results demonstrate that the implemented AlignGraph software is able to improve the results of a wide range of  de novo  assemblers for complex genomes even with relatively diverse and suboptimal guide sequences as reference. Moreover, our results demonstrate AlignGraph’s usefulness for improving unfinished genome assemblies. Another advantage is that AlignGraph can be used in combination with most existing  de novo  assemblers. In the future, we will expand AlignGraph in the following areas: (i) we will provide support for additional variant-aware alignment tools for both PE read and contig data, such as GSNAP and GMAP, respectively, (ii)  de novo  assembly functionality will be added to AlignGraph to further optimize assemblies at many stages of the reference-assisted workflow, (iii) utilities will be incorporated for detecting and resolving misassemblies either in the initial contigs or in the extensions and (iv) the processing of scaffolds with large gaps will be improved. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Protein–protein interaction specificity is captured by contact preferences and interface composition</Title>
    <Doi>10.1093/bioinformatics/btx584</Doi>
    <Authors>Nadalin Francesca, Carbone Alessandra, Valencia Alfonso</Authors>
    <Abstract/>
    <Body>1 Introduction The large and constantly increasing number of protein structures highly encourages using protein docking to obtain protein complex structures from unbound conformations. In such experiments, a protein is fixed in the space and the interacting protein is rotated around it and around itself. The space of relative positions is explored through a large sampling of conformations, undergoing several energy minimizations for the computation of the associated free energy. Docking algorithms differ from one another basically upon the energy function used, the degree of flexibility allowed and the resolution chosen for protein representation ( Gray, 2006 ). When a large-scale docking experiment is addressed, with hundreds or thousands of proteins docked against themselves, thousands of millions of poses have to be analysed, thus the computation of each of them should be as fast as possible. In this context, the bottleneck between structure accuracy and computational cost emerges: on the one hand, one seeks an accurate model of the complex structure, and on the other hand, wishes a fast screening of the docking decoys, realized with a scoring scheme that does not suffer from the level of accuracy of the structures. Once a reasonable set of candidate solutions is retrieved, more refined procedures, like flexible docking, can be applied ( Andrusier  et al. , 2008 ). The problem of properly defining a small subset of solutions from docking calculations has been widely studied in the literature ( Moal  et al. , 2013 ). The idea is to  rank  all predicted conformations with  ad hoc  scores, drawn either from the free energy or from statistical potentials. Interface contact propensities, or  pair potentials , capture the signal coming from the type of contacts at the interface. They provide a statistical model estimating how often a contact is expected to occur at the interface at random, given the residue (or contact) frequency observed on a set of experimental structures. Previous studies showed the applicability of contact propensities to the prediction of near-native docking decoys ( Moont  et al. , 1999 ;  Lu  et al. , 2003 ;  Huang and Zou, 2008 ;  Liu and Vakser, 2011 ). The simplest docking score proposed in the literature is based on the sum of propensity values at the interface. Pioneering work is the one proposed by  Moont  et al.  (1999) , who rank the decoys of a small set of 9 complexes after extracting information from datasets of intramolecular (385 domains), homodimer (23 complexes) and heterodimer (11 complexes) interactions.  Lu  et al.  (2003)  define a residue-based propensity matrix on a dataset of true and false interactions and test the ability of their score to discriminate near-native decoys for a set of 21 complexes.  Huang and Zou (2008)  define a procedure iteratively improving the discrimination of near-native decoys by learning pair potentials on bound structures.  Liu and Vakser (2011)  propose five pair potentials, defined either on experimental structures or on decoy sets. More sophisticated approaches propose linear combinations of pair potentials and energy terms ( Feliu  et al. , 2011 ;  Fink  et al. , 2011 ;  Pons  et al. , 2011 ), possibly weighted ( Li  et al. , 2007 ). A common practice is to learn optimal coefficients for each term from docking decoys whose similarity to the native structure is known ( Li  et al. , 2007 ). For example, SIPPER method ( Pons  et al. , 2011 ) combines knowledge-based and desolvation energy terms to pre-screen docking poses for further refinement. Here, we define a new contact propensity matrix, called CIPS for Combined Interface Propensity for decoy Scoring, in which interface propensity and local geometry are explicitly included; our pair potentials are computed from a diversified set of 230 bound structures ( Vreven  et al. , 2015 ). We compare our method to three propensity matrices ( Glaser  et al. , 2001 ;  Pons  et al. , 2011 ;  Mezei, 2015 ) and to two atomic potentials ( Krissinel and Henrick, 2007 ;  Pierce and Weng, 2007 ) on three datasets, very different from one another in composition, size and underlying docking algorithms. A simple combination of CIPS with atomic potentials, not requiring a learning step on the decoy sets, is also analysed. Finally, we study the impact in decoys ranking of four propensity-based descriptors, which provide different characterizations of the interaction either based on propensity values, contact counts or interface layers ( Levy, 2010 ), either computed for the whole interface or between ‘propensity patches’. Supported by a series of computational experiments on both full-atom ( Tovchigrechko and Vakser, 2005 ;  Tovchigrechko and Vakser, 2006 ) and coarse-grain ( Sacquin-Mora  et al. , 2008 ;  Lopes  et al. , 2013 ) decoy sets, we demonstrate that CIPS can be proposed as a fast, accurate and robust method for decoys selection. It is expected to be very useful to discriminate millions of docking conformations quickly. 2 Materials and methods 2.1 Combined interface propensities The bias of contact  〈 i , j 〉  abundance at the interface expresses the preference of amino acids  i  and  j  to be one in front of the other,  provided that  both  i  and  j  lie at the interface. Based on this idea, we model interface propensity as a combination of two terms: the one is contact-based, taking into account the preference of  i  and  j  to be in contact, the other one is residue-based, expressing the preference of  i  and  j  to be located at the interface (one belonging to a surface and the second to the other). We further include, in the definition of the contact-based term, a connectivity factor taking into account the number of per-residue contacts. The final formula expressing the bias of contact  〈 i , j 〉  abundance is given by
 (1) P i , j = c i , j ¯ · r i , j , 
where  c i , j ¯  is the contact-based term and  r i , j  is the residue-based term (see also  Fig. 1A ). Detailed explanation formula (1) is provided below.
 Fig. 1. CIPS  propensity matrix and scoring of docking decoys.  ( A ) Representation of the three components of CIPS propensity: residue–residue contact propensity (left), connectivity level of interface residues (centre), and residue propensity at the interface (right). The contact propensity term is computed with Equation (2) and normalized in [0, 1]; the number of contacts for amino acid  k  is the value  v k  in the expression of  α i , j  in Equation (3); the residue propensity of amino acid  k  is the value from  Negi and Braun (2007) . ( B ) CIPS propensity matrix computed on PPDB v5.0 with Equation (1) by combining the three numerical components illustrated in ( A ). ( C  and  D ) For each complex, the receptor is fixed (grey, at the bottom) and the docking solution (black) and the native conformation (grey) of the ligand are shown on the top. The bar below each structure is coloured according to the propensity value (either from CIPS or Pons definition) of each contact at the interface; namely, for CIPS matrix,  P i , j  values are represented and coloured according to the gradient reported in ( B ). All contacts are represented and ordered from lower to higher propensity values. The number of inter-protein residue contacts is reported below the bars; their number depends on the distance between receptor and ligand (compare structures of C and D). Note that CIPS and Pons values are not directly comparable. To account for this, we report the percentage ranking of the depicted structures within the set of decoys for the same complex. ( C ). Four docking decoys from Dockground decoy benchmark for trypsin (receptor) in complex with trypsin inhibitor (ligand) (PDB code: 2FI4). ( D ) Three docking decoys from CCD benchmark for antigen (receptor) in complex with antibody (ligand) (PDB code: 1QFW). Molecular graphics are performed with the UCSF Chimera package ( Pettersen  et al. , 2004 ) (Color version of this figure is available at  Bioinformatics  online.) 
 The contact propensity term.  The interface propensity of a contact  〈 i , j 〉  between amino acids  i  and  j  is a measure of how often the contact  〈 i , j 〉  occurs at protein interfaces compared to the expected frequency. It is usually defined as
 (2)   log ⁡ 2 f i , j obs f i , j   exp   , 
where  f i , j obs  and  f i , j   exp    are the observed and the expected fraction of contacts of type  〈 i , j 〉 , respectively. Note that the notation  〈 i , j 〉  does not impose any order between  i  and  j . Given the number of contacts  c i , j  of type  〈 i , j 〉  and the total number of inter-protein contacts  n = ∑ 〈 i , j 〉 c i , j , the observed fraction of contacts of type  〈 i , j 〉  is given by  f i , j obs = c i , j / n . We compute the predicted fraction of contacts by means of the residue–residue contact frequency as follows. The amount of contacts involving amino acid  i  is given by  c i = ∑ j c i , j  and the contact frequency of  i  is defined as  f i = c i / n . The expected fraction of contacts between  i  and  j  is given by  f i , j   exp   = f i f j . 
 The connectivity factor.  We introduce a weight for  f i , j   exp    (see above). The connectivity factor  α i , j  associated to  〈 i , j 〉  is a function of the average number of contacts for residue types  i  and  j , respectively. It is defined as  α i , j = 1 − a · ( v i · v j ) , where  v k  is the average contact frequency of  k , rescaled in [0, 1] (see  Supplementary Table S1 , third row), and  a  is a fixed parameter (see  Supplementary Section S1.2 ). We obtain the following new expression for the interface contact propensity [compare to  Equation (2) ]
 (3) c i , j = log ⁡ 2 f i , j obs α i , j f i , j   exp   . 
The term  c i , j ¯  of (1) is the value  c i , j  in  Equation (3)  rescaled in [0, 1]. 
 The residue propensity term.  The amino acid preference at the interface is modelled by the term  r i , j  associated to the pair  〈 i , j 〉 . It is defined as
 (4) r i , j = 1 + b · ( w i + w j ) , 
where the  w k  is the residue propensity of  k  from  Negi and Braun (2007)  rescaled in  [ − 1 , 1 ] , and  b  is a fixed parameter (see  Supplementary Section S1.2 ). 2.1.1 Interface residues and residue–residue contacts The accessible surface area (ASA) is computed with NACCESS ( Hubbard and Thornton, 1993 ) using a probe radius of  1.4 Å . The interface is identified by the residues that lose ASA upon binding (i.e.  Δ ASA &gt; 0 ) ( Levy, 2010 ). Inter-protein contacts are computed between interface residues. Two residues are assumed to be in contact if the distance within any heavy atom pair is below  5 Å . We consider the support-core-rim model proposed in  Levy (2010)  defined according to the degree of exposure to the solvent; the surface is identified by the residues whose ASA is at least 25% of the total residue surface. A residue belongs to the  support  if it is buried both in the bound and in the unbound form, to the  core  if it is exposed in the unbound form and buried in the bound form, to the  rim  if it is exposed both in the unbound and in the bound form. Note that the  Δ ASA  cut-off of 25% was chosen so that the number of residues on each region is approximately the same (see  Levy, 2010 ). 2.1.2 Experimental structures for propensity calculations Properties of interface contacts are drawn from the analysis of the 230 bound structures of the Protein-Protein Docking Benchmark (PPDB) v5.0 ( Vreven  et al. , 2015 ). The dataset consists of 39 antibody-antigen, 46 enzyme-inhibitor, 26 enzyme complexes with a regulatory or accessory chain, 17 enzyme-substrate, 23 G-protein containing, 24 receptor containing and 55 unclassified complexes (others). PDB structures are downloaded from  https://zlab.umassmed.edu/benchmark/ . 2.2 Benchmark sets of docking decoys The Dockground decoy benchmark ( Liu  et al. , 2008 ) and the Complete Cross Docking (CCD) benchmark ( Lopes  et al. , 2013 ) are used for testing. Both contain decoys issued from docking unbound structures. The former is the result of rigid-docking experiments performed with GRAMM-X ( Tovchigrechko and Vakser, 2006 ) on Dockground ( Douguet  et al. , 2006 ). The decoy sets of the CCD were computed with the coarse-grain model MAXDo ( Sacquin-Mora  et al. , 2008 ) on the PPDB v2.0 ( Mintseris  et al , 2005 ) ( http://www.lcqb.upmc.fr/CCDMintseris /). For each complex, we considered two instances, where a protein is either a receptor or a ligand in the docking procedure. In this work, we use the 168 decoy sets for the 84 true protein complexes. 2.2.1 Comparison with other pair potentials Comparison of the results is performed against the propensity matrices proposed in ( Glaser  et al. , 2001 ), ( Pons  et al. , 2011 ) and ( Mezei, 2015 ). ( Glaser  et al. , 2001 ) apply amino acid volume normalization to residue frequencies;  Mezei (2015)  normalizes interface propensity by surface propensity;  Pons  et al.  (2011)  compute residue frequencies bias either at the surface or at the interface (we use the latter for comparison). The matrix by Glaser  et al.  is computed on a set of 621 complexes, where a high fraction is constituted by homodimer interfaces. The matrix by Pons  et al.  is computed on the 70 complexes of the benchmark in ( Chakrabarti and Janin, 2002 ). Mezei matrix is computed on 1172 high-resolution structures. 2.2.2 Preparation of training sets and testing sets The docking decoy benchmarks described above are used to build two testing sets, as follows. To eliminate overfitting, all complexes used in either Glaser or Pons matrices (PDB codes for Mezei matrix are not available) are removed from both Dockground decoy benchmark and CCD benchmark. Note that we did not exclude the complexes used for computing CIPS matrix (i.e. the PPDB v5.0, see Section 2.1.2) from the testing sets, because they fully cover the structures used to build the CCD benchmark (i.e. the PPDB v2.0). The reduced testing set  Dockground_ decoy_ filtered  ( DF ) contains 51 of 61 decoy sets. Of note, 419 of 5523 decoys are classified as near-native. The reduced testing set  CCDMintseris_ filtered  ( CF ) contains 66 of 84 complexes, namely, 132 of 168 decoy sets (see above). It contains 611 (182) acceptable (medium quality) decoys of 247, 258; 105 (51) decoy sets contain an acceptable (medium quality) solution. See  Supplementary Section S1.6  for decoys classification. The PDB codes of the testing sets are reported on  Supplementary Tables S7  ( DF ) and S8 ( CF ). Two training sets are defined, consisting of crystallographic structures from PPDB v5.0 (see Section 2.1.2). Redundancy between training sets and testing sets is removed as follows. Within PPDB v5.0, 19 complexes of 230 are shared with  DF , thus they are removed from PPDB v5.0 and the remaining 211 are referred to as PPDB5\ DF . Similarly, the 65 complexes of PPDB v5.0 already present in  CF  are removed and the remaining 165 are referred to as PPDB5\ CF . We further check the structural similarity between training and testing sets. For this purpose, we use PIFACE ( Cukuroglu  et al. , 2014 ), a database of clusters of redundant protein–protein interfaces. The redundancy is very low: 20 of 211 complexes in PPDB5\ DF  are structurally redundant with some complexes of  DF  (see  Supplementary Table S9 ); 8 of 165 complexes in PPDB5\ CF  are structurally redundant with some complexes of  CF  (see  Supplementary Table S10 ). Training sets are filtered further: each complex of PPDB5\ DF  (of PPDB5\ CF ) belonging to the same structural cluster of  DF  ( CF ) is removed. The training sets obtained after this two-step filtering procedure are referred to as PPDB5\ DF (nr)  (91 complexes) and PPDB5\ CF (nr)  (157 complexes). Summarizing, we built two testing sets:  DF  and  CF , and four training sets: PPDB5\ DF , PPDB5\ CF , PPDB5\ DF (nr)  and PPDB5\ CF (nr) . Testing on  DF  is done with a model computed on either PPDB5\ DF  or PPDB5\ DF (nr) , and testing on  CF  is done with a model computed on either PPDB5\ CF  or PPDB5\ CF (nr)  (see also Section 3.3). 2.3 The CAPRI targets set We select top predicted models for all protein–protein CAPRI targets from Round 7 on ( http://www.ebi.ac.uk/msd-srv/capri ). Targets with no acceptable models are discarded, resulting in 28 targets (see  Supplementary Table S11 ) and 8213 models (1214 acceptable, 796 medium, and 225 high quality). Based on CAPRI evaluation ( Lensink  et al. , 2007 ;  Lensink and Wodak, 2010 ,  2013 ;  Lensink  et al. , 2016 ), we could identify 10 easy (2857 models) and 9 difficult (2405 models) targets (we could not find enough information to classify the remaining 9 targets). The easy class contains 834 (598) acceptable (medium quality) models; the difficult class 88 (15) acceptable (medium quality) models. 2.4 Scoring structural models We will refer to a  decoy set  for a complex  C , denoted  X C , as the set of docking solutions for  C .  X  denotes the union of  X C  over all complexes  C  of a given dataset. Interface residues and contacts are defined in the same way for both experimental structures and docking decoys (see Section 2.1.1). A score  S ( x ) for a decoy  x  is defined in two steps. First, each interface contact of type  〈 i , j 〉  is assigned the value  P i , j  computed with  Equation (1)  (as illustrated on  Fig. 1C and D ). Second, the score for the whole interface is computed according to one of the following three scoring categories: (i) the sum of all  P i , j s at the interface; (ii) the sum of  P i , j s between specific interface regions ( Levy, 2010 ): core/core (C–C), core/support + core (C–CS), core/core + rim (C–CR) and core/interface (C–SCR); (iii) the propensity-based descriptors (see  Supplementary Table S2 ), which are computed either on the whole interface or on  propensity patches , and expressed as either the sum of  P i , j s or as contact counts (see  Supplementary Section S1.3  for details). The final score assigned to  x  is averaged over the interface [scoring category (i)] or its regions [scoring categories (ii) and (iii)]. Unless specified otherwise, the score  S ( x ) of  x  is computed according to (i), namely
 (5) S ( x ) = 1 n ∑ 〈 i , j 〉 n i , j P i , j ,   with   n = ∑ 〈 i , j 〉 n i , j , 
where  P i , j  is defined in  Equation (1)  and  n i , j  is the number of  〈 i , j 〉  contacts. 2.5 Availability and implementation CIPS is available at  http://www.lcqb.upmc.fr/CIPS . It is written in C++ and Perl and is supported on GNU/Linux and Mac OS X. Instructions to install and use the package are provided in  Supplementary Section S1.11 . In this work, the ASA is computed with NACCESS ( Hubbard and Thornton, 1993 ), but this is not a requirement; the user can force CIPS to use the open-access library FreeSASA ( Mitternacht, 2016 ). Experiments were run on an Intel Xeon CPU with 3.50 GHz speed. The computational bottleneck is interface calculation: it took 1 s (small complex &lt; 500 residues) to 7 s (large complex &gt; 2000 residues). The computation of the contacts and of CIPS scores [see  Equation (5) ] does not add significant overhead (&lt; 0.1 s). 3 Results The prediction of protein binding sites, successfully realized by several methods based on the observation that interface residues exhibit a compositional bias and undergo slower evolution ( Lichtarge  et al. , 1996 ;  Jones and Thornton, 1997 ;  Zhou and Shan, 2001 ;  Armon  et al. , 2001 ;  Lichtarge and Sowa, 2002 ;  Pupko  et al. , 2002 ;  Caffrey  et al. , 2004 ;  Neuvirth  et al. , 2004 ;  Fernandez-Recio  et al. , 2005 ;  Liang  et al. , 2006 ;  Innis, 2007 ;  Leis  et al. , 2010 ;  Zhang  et al. , 2010 ;  Segura  et al. , 2011 ;  Jordan  et al. , 2012 ;  Andreani  et al. , 2012 ;  Laine and Carbone, 2015 ;  Maheshwari and Brylinski, 2015 ;  Aumentado-Armstrong  et al. , 2015 ), is not sufficient to correctly discriminate near-native conformations from a set of docking decoys ( Lopes  et al. , 2013 ). This task requires joint information coming from both protein partners. A test on the Dockground decoy benchmark ( Liu  et al. , 2008 ) shows that pair potentials pinpoint good quality models with higher precision with respect to residue propensities (see  Supplementary Fig. S7 ). Docking scores realized with existing knowledge-based methods take into account the distribution of amino acid pairings, rather than residue distribution, at the interface. Nevertheless, both pieces of information might be used jointly to accurately select docking decoys. To corroborate this idea, we compare CIPS model with other pair potentials ( Moont  et al. , 1999 ;  Glaser  et al. , 2001 ;  Pons  et al. , 2011 ;  Mezei, 2015 ), on two decoy sets and on selected models from CAPRI competition. The importance of re-ranking structural models after docking calculations has been explicitly addressed in CAPRI competition ( Lensink  et al. , 2016 ), where several groups specifically focused on models scoring could contribute to. Among the best performing ones, we mention HADDOCK, CLUSPRO and SWARMDOCK. Notice that energy-based terms are often coupled with scores derived from knowledge-based potentials. Combination of CIPS with atomic potentials ( Krissinel and Henrick, 2007 ;  Pierce and Weng, 2007 ) highlights the complementarity of the two approaches (statistical and energy-based) in capturing correct binding modes. 3.1 Local geometry of interface contacts and the propensity of residues to be at the interface Contact propensities allow to capture the tendency of two amino acids to set up inter-protein contacts. They account for the variability in the number of contacts, which depends on the number of residues at the interface and, possibly, on their volume and degree of exposure to the solvent. The contact propensity we define on  〈 i , j 〉  is the contribution of two terms [see  Equation (1 )]. The first one is contact-based [ c i , j ¯ , see  Equation (3) ] and expresses the tendency of  i  and  j  to make a contact (see  Fig. 1A , left); this factor regards two proteins as paired upon complex formation. The second term is residue-based [ r i , j , see  Equation (4) ] and expresses the likelihood of  i  and  j  to be located at the interface rather than on the rest of the protein surface (see  Fig. 1A , right); it is defined on single proteins, without knowing the partner. These two factors constitute the key ingredients modelling protein–protein interactions. Other important observations help us to refine the model. Namely, our definition of contact propensity takes into account the local connectivity of  i  and  j  at the interface (see  Fig. 1A , centre). The number of per-residue contacts changes according to the amino acid (see  Supplementary Fig. S1 ). Based on the observation that residues making a lot of contacts tend to have lower contact propensity, a connectivity factor is included [see  Equation (3) ] to correct for this bias (see Section 2.1 and  Supplementary SectionS1.1 ). Notice that most interfaces contain at least one residue making at least five contacts (see  Supplementary Fig. S2 ). 3.2 A matrix of combined interface propensities Propensity values  P i , j  computed with  Equation (1)  on PPDB v5.0 result in the propensity matrix reported on  Figure 1B  (see  Supplementary Table S4 ). It will be referred to as CIPS: Combined Interface Propensity for decoy Scoring. The pattern shown by CIPS matrix essentially points out that: (i) there is a strong tendency of hydrophobic residues to be paired in the interaction; (ii) aromatic residues (i.e. W and Y) play a crucial role in binding, with both hydrophobic and positively charged residues, and (iii) oppositely charged residues tend to be in contact, whereas residues with the same charge are not. Detailed inspection of the matrix shows that R-R is more favourable than K-K: this is due to their different interface propensity and connectivity factor  α i , j  (see also  Negi and Braun, 2007  and  Supplementary Table S1 ). We study the distribution of the values associated to the amino acid pairs to test whether our statistical model for the interface contact frequencies well describes the data. We observe that contact propensities (i.e. the values  c i , j  for the amino acid pairs  〈 i , j 〉 ) are roughly partitioned in two halves, representing contacts occurring at the interface respectively less often ( c i , j &lt; 0 ) or more often ( c i , j &gt; 0 ) than expected (see  Supplementary Fig. S3 ). This observation supports the consistency of our definition. This is not the case for  Glaser  et al.  (2001) . The above test highlights that all-atom structures, implicitly considered in our computation of inter-protein contacts, are more accurate compared to methods approximating a residue with a single atom ( C β  or  C α ) ( Moont  et al. , 1999 ), possibly weighted by the side chain volume ( Glaser  et al. , 2001 ). To test the robustness of CIPS further, we replace residue–residue with atom–atom contact counts in  Equation (1) , by keeping the same distance threshold (see Section 2.1.1). The matrix obtained conserves the qualitative behaviour of CIPS, but the signal is less sharp (see  Supplementary Fig. S4 ). Then, we test the effect of including hydrogen atoms on the structures [the Open Babel toolbox ( O’Boyle  et al. , 2011 ) is used]. The change in the matrix pattern is negligible (see  Supplementary Fig. S5 ). We compare the performances among the matrix built with the contact-based term alone, the one where the expected contact frequency is weighted by the connectivity factor and the one where the residue term is included (i.e. CIPS). Note that the first case corresponds to a standard pair potential, defined on a given set of experimental complex structures, where the amino acid pairing at the interface is modelled according to the number of contacts per amino acid. The inclusion of  α i , j  allows to better discriminate, with respect to the other matrices, near-native decoys on  DF  and medium quality decoys on  CF . The inclusion of  r i , j  boosts CIPS discriminative power, which turns out to outperform all existing methods (see  Supplementary Fig. S8  and compare  Supplementary Table S12  with Tables S15–17). 3.3 Scoring protein docking decoys at large scale The ability of contact propensities to recognize docking structures close to the experimental one relies on capturing the relative orientation of protein partners. To discriminate such conformations, we make use of  Equation (5)  to score a docking structure, i.e. the average propensity value at the interface. As described in Section 2.2.2, two distinct training sets are used, named PPDB5\ DF  and PPDB5\ CF , for testing on  Dockground_ decoy_ filtered  ( DF ) and  CCDMintseris_ filtered  ( CF ), respectively. Note that they guarantee unbiased testing (see Section 2). Thus, two new matrices are computed: CIPS DF  on PPDB5\ DF  and CIPS CF  on PPDB5\ CF . These matrices highlight the robustness of CIPS. Indeed, the correlation between CIPS and CIPS DF  and between CIPS and CIPS CF  is very high, with  ρ = 0.99  and  ρ = 0.95 , respectively. We observe that removing structural redundancy at the interface does not considerably affect the results (see below). CIPS matrices and the pair potentials proposed by  Glaser  et al.  (2001) ,  Pons  et al.  (2011)  and  Mezei (2015)  are used to score  DF  and  CF . Correlations between the four propensity matrices are reported in  Supplementary Table S5 . We also consider an ‘ideal matrix’, describing pair potentials fitting the characteristics of the docking decoys we want to test (see  Supplementary Section S1.8 ). We compute it [with Equation (S1)] in three scenarios ( DF ,  CF  with acceptable decoys and  CF  with medium quality decoys), the idea being to identify possible biases in near-native structures towards specific amino acid pairings. The discriminative power of this ideal matrix provides upper limits to the results obtainable with predictions. Note that a similar study was already proposed with DECK method ( Liu and Vakser, 2011 ). In the following, we measure the accuracy of the scoring schemes we tested with the criteria described in  Supplementary Section S1.9 . 3.3.1 Dissecting how different parts of the interface contribute to binding specificity We ask whether either all interface contacts play a role in the specificity of the interaction, only the “propensity-favourable” part of them, or those connecting specific interface layers. We show that (i) the whole interface is important for the interaction specificity, (ii) all contacts, propensity-favourable or not, contribute to the recognition of true interfaces and (iii) propensity values provide more accurate discrimination compared to contact counts. To answer (i), we check whether focusing on specific interface layers allows to capture enough, possibly stronger, information for the discrimination of true interactions. We employ the support-core-rim model ( Levy, 2010 ) (see Section 2). Contacts involving core residues are the most abundant at the interface (see  Supplementary Fig. S9 ). Hence, we evaluate the effect of considering only core–core contacts, by progressively adding contacts involving support and rim. The analysis with CIPS shows that contacts involving core residues explain only part of the interaction, and that the best performance is obtained by including all interface contacts (see  Supplementary Fig. S10 ). This effect remains consistent across different pair potentials (see  Supplementary Table S14 ). To answer (ii) and (iii), we apply four propensity-based descriptors to the discrimination of true interactions (see  Supplementary Section S1.3  and  Supplementary Table S2 ). Two of them are computed on the whole interface; the other two are computed on patches connected to the protein partner by high propensity contacts (see  Supplementary Section S1.4  for their definition). We consider either propensity values or the number of contacts with very high propensity value. We use normalized scores because they are invariant upon re-scaling, which is essential for our definition [see  Equation (1) ], and do not depend on interface size.  Supplementary Figure S13  and  Supplementary Tables S15–17  show the results obtained on  DF  and  CF  using the descriptors. “Total propensity” outperforms the other three. On  DF , its predictive power reaches the ‘ideal’ result when patches are used (compare CIPS DF  and “decoy-based” on  Supplementary Fig. S13A ). 3.3.2 Scoring of all-atom docking decoys:  Dockground_ decoy_ filtered To evaluate CIPS DF  (see Section 2.2.2) on structures obtained with a full-atom model, we consider the Dockground decoy benchmark ( Liu  et al. , 2008 ). Comparison of the discriminative power of the propensity matrices is reported in  Figure 2A  (top). CIPS DF  shows to be a well-designed pair potential, able to dissect native from non-native interfaces and to assign the highest scores to docking conformations that are close to the correct structure. CIPS DF  performs better than Glaser, Mezei and Pons matrices. Note that the correlation with the decoy-based propensity is higher for CIPS DF  matrix compared to the other three (see  Supplementary Table S6 ). CIPS DF  performance (AUC 0.87) is very close to the ideal result (AUC 0.90) and the elimination of 19 complexes from the dataset of experimental structures does not affect much the outcome (compare CIPS and CIPS DF  curves in  Fig. 2A ). The discriminative power of Pons matrix is remarkable too (AUC 0.83). The results are coherent when ranking of near-native decoys is measured (compare  Supplementary Tables S15 and S20 ). CIPS DF  assigns top 10% rank to 233 (56%) near-native decoys, Pons to 205 (49%), Mezei to 174 (42%) and Glaser 130 (31%), the ideal performance attaining 74% of near-native decoys ranked on top 10% (see  Supplementary Table S15 ). By removing even structural redundancy at the interface, as described in Section 2.2.2, the training set PPDB5\ DF (nr)  is used to compute a new matrix called CIPS DF(nr) . The correlation between CIPS DF(nr)  and CIPS DF  is high ( ρ = 0.95 ). We observe that the effect of structural redundancy is negligible (compare CIPS DF  and CIPS DF(nr)  performances in  Supplementary Fig. S11A  and  Supplementary Table S18 ).
 Fig. 2. 
 Discriminative power of pair potentials, of atomic potentials, and of the combination of the two on all-atom versus coarse-grain decoys datasets.  ROC curves are computed on the union of the decoy sets and AUC values are reported for each method. ( A–C ) The score used is the average contact propensity at the interface [see Equation (5)]. Dotted lines refer to the performances of the  decoy-based potential , a propensity defined as the log-ratio between the contact frequency on near-native and non–near-native decoys (see  Supplementary Section S1.8 ). ( D–F ) The best performing pair potentials and atomic potentials on  Dockground_ decoy_ filtered  ( DF ) and  CCDMintseris_ filtered  ( CF ), and their combination. PISA + CIPS DF  and ZRANK + CIPS CF  are defined as in Equation (S1). (A and D) Analysis of  DF . (B and E) Analysis of  CF , with acceptable decoys labelled as near native. (C and F) Analysis of  CF , with medium quality decoys labelled as near native. ROC curves and AUC values are computed with the R packages ROCR ( Sing  et al. , 2005  and  Tuszynski, 2014 ) (Color version of this figure is available at  Bioinformatics  online.) 3.3.3 Scoring of coarse-grain docking decoys:  CCDMintseris_ filtered Despite the optimal parameters for CIPS have been tuned on  DF  (see  Supplementary Section S1.2  and  Supplementary Table S3 ), its relative performance compared to other propensity matrices is robust. We show this with CIPS CF  (see Section 2.2.2) on  CF ; this control allows us to demonstrate that the propensity we define is actually able to dissect good signal from noise, even for coarse-grain conformations. We perform two separate evaluations by assuming in turn that a “near-native” decoy is either an acceptable or a medium quality solution according to CAPRI definition ( Lensink  et al. , 2007 ) (see  Supplementary Section S1.6 ). The results obtained by the four propensity matrices are reported in  Figure 2B and C  (top). As expected, compared to  DF , all matrices exhibit worse performances on  CF , due to a docking schema applied to a coarse-grain resolution of the structures. A general improvement is observed when going from acceptable to medium quality decoys. CIPS CF  is very precise in discriminating near-native structures from the whole set of decoys, especially for medium quality ones (AUC 0.83). CIPS CF  and Pons assign the highest number of near-native structures to the top 10% ranking; CIPS CF  outperforms all other methods on top 20% ranks and below (see  Supplementary Tables S16 and S17 ). The results are in agreement with the fact that CIPS highly correlates with the decoy-based matrices (see  Supplementary Table S6 ). The discriminative power of the decoy-based matrix is strikingly higher than statistical potentials, especially for medium quality structures (see  Supplementary Fig. S13C  and compare  Supplementary Tables S16 and S17  with  Supplementary Tables S21 and S22 ). Conversely, CIPS is robust with respect to the set of complexes used: inclusion of structures of  CF  in the computation of the propensity matrix lead to a very slight difference in performance (compare CIPS and CIPS CF  curves in  Fig. 2B and C ). Similar to  DF , we test the elimination of structural redundancy at the interface (see Section 2.2.2) and use the training set PPDB5\ CF (nr)  to compute a new matrix called CIPS CF(nr) . The correlation between CIPS CF(nr)  and CIPS CF  is very high ( ρ = 0.98 ). Structural redundancy does not have perceivable effects on the results (compare CIPS CF  and CIPS CF(nr)  on  Supplementary Fig. S11  and  Supplementary Table S18 ). 3.3.4 Comparison between CIPS and atomic potentials Docking decoys are often ranked according to energy functions or  atomic potentials . To corroborate the performances we obtained on  DF  and  CF , we compare the above results with the predictive power of atomic potentials. We select two tools widely used in the scientific community for this purpose: PISA ( Krissinel and Henrick, 2007 ), combining energetic and entropic terms weighted by the number of contacts; and ZRANK ( Pierce and Weng, 2007 ), a sum of energy terms, where weights are learnt from decoy sets built on 15 complexes of PPDB v1.0 ( Chen  et al. , 2003 ). PISA outperforms ZRANK on  DF , yielding 0.19 higher AUC (see  Supplementary Fig. S12A ) and ranking 25% more near-native decoys on top 10% (see  Supplementary Table S19 ). On  CF , atomic potentials show good performances (see  Supplementary Fig. S12B ). ZRANK is more precise in discriminating medium quality structures (see  Supplementary Table S19 ); this improved behaviour might be due to the learning step performed on docking decoys. Note that the training set used in ZRANK does not contain complexes from PPDB v2.0. By comparing CIPS DF  with PISA and CIPS CF  with ZRANK, we observe that propensity matrix and atomic potential performances are close to each other. On  DF , CIPS DF  outperforms atomic potentials both in global decoy classification (see  Fig. 2A ) and in the ranking within each decoy set (see  Table 1 ). In contrast, CIPS CF  displays better decoys classification than ZRANK, but the latter better ranks near-native decoys on  CF . This is particularly true on medium quality structure classification where for the top 20% ranks, ZRANK outperforms CIPS CF .
 Table 1. Ranking of near-native decoys on all-atom versus coarse-grain decoys datasets Top percentage rank Dockground_ decoy_ filtered CCDMintseris_ filtered  (acceptable) CCDMintseris_ filtered  (medium quality) PISA CIPS DF PISA + CIPS DF ZRANK CIPS CF ZRANK + CIPS CF ZRANK CIPS CF ZRANK + CIPS CF Top 1% 5 6 8  (63) 8 3 5 (18) 14 2 7 (22) Top 10% 44 56 65  (90) 36 32 42  (66) 57 40 57  (78) Top 20% 64 73 79  (94) 48 53 62  (90) 71 64 77  (90) Top 30% 74 82 87  (98) 60 69 76  (96) 80 80 88  (94) Top 40% 81 89 90  (98) 67 78 81  (96) 86 90 93  (98) Top 50% 87 94 95  (100) 74 85 88  (98) 86 93 96  (98) Entries represent the percentage of near-native decoys placed on each top percentage rank. The number reported in parentheses is the percentage of complexes having at least one near-native decoy placed on each top % rank. Bold values indicate best performance. 3.3.5 Combination of CIPS with atomic potentials reaches optimal discrimination of near-native decoys We show how a combination of pair potentials and energy functions can improve both decoy classification and ranking. The ideal situation is the one where the two approaches capture distinct interface features. To check for this, we compute the correlation between the ranking assigned by pair potentials and atomic potentials, respectively, to all decoys of  DF  and  CF  (see  Supplementary Table S23 ). Correlations are positive but low: 0.35 between PISA and CIPS DF  and 0.12 between ZRANK and CIPS CF , hinting that the two approaches might use complementary information. We combined pair potentials and atomic potentials (see  Supplementary Section S1.5 ) and evaluate the discrimination of near-native decoys on  DF  and  CF  (see  Fig. 2 ). PISA + CIPS DF  performs better on  DF  than either PISA or CIPS DF  alone; ZRANK + CIPS CF  performs better on  CF  than ZRANK and CIPS CF  alone. PISA is always the best choice on  DF  when combined to any pair potential (see  Supplementary Figs S15A and S16 A and  Supplementary Table S24 ). Conversely, on  CF , ZRANK outperforms PISA when coupled to any pair potential (see  Supplementary Figs S15B and C  and S16B and C and  Supplementary Tables S25 and S26 ). The relative performance of pair potentials remains consistent on combined scores, showing that CIPS provides the best discrimination compared to the other matrices. Combined scores always perform better, compared to atomic potentials and pair potentials used alone, and almost reach the theoretical limit set by decoy-based propensities, especially on  DF  (see  Fig. 2A ). On  CF , the difference in performance between the decoy-based matrix and pair potentials is more evident (see  Fig. 2B and C ). However, ZRANK + CIPS CF  discriminates medium quality decoys consistently better than ZRANK + Pons. By combining atomic potentials and CIPS as above, selection of near-native docking solutions is possible with high precision (see  Table 1 ). On  DF , picking the top 10% ranked decoys allows to retain 65% of near-native solutions and to have at least one near-native for 46 out of 51 complexes (90%). On  CF , top 10% ranked decoys contain 42% and 57% of acceptable and medium quality structures, respectively. They represent 69 out of 105 decoy sets with at least one acceptable solution, 40 of them having also one medium quality solution. 3.4 Scoring high-quality structural models: CAPRI targets CIPS performance is assessed on the top 10 models produced by CAPRI participants, for 28 targets (see Section 2.3). We use the near-native solution classification provided on CAPRI rounds. Evaluation on this testing set is reported on  Figure 3  and  Table 2  (see  Supplementary Fig. S17  and  Supplementary Table S27  for details). CIPS outperforms the other three propensity matrices, yielding at least 0.08 higher AUC (when acceptable or better decoys are considered as near native). We identify 10 easy and 9 difficult targets (see  Supplementary Table S11 ). The performances of the four propensity matrices are reported on  Supplementary Figure S18  and  Supplementary Table S28 . CIPS equals or outperforms existing pair potentials, irrespective of the quality of the solutions and on the conformational change upon binding.
 Table 2. Ranking of near-native models of 28 CAPRI targets Acceptable Medium quality Top percentage rank Glaser Mezei Pons CIPS Glaser Mezei Pons CIPS Top 1% 0 0 1 1 0 0 0 0 Top 10% 8 11 9 11 6 10 8 9 Top 20% 18 23 22 27 14 21 19 24 Top 30% 32 35 35 42 26 33 32 38 Top 40% 44 47 49 56 38 43 46 52 Top 50% 57 58 62 69 52 56 58 65 Entries represent the percentage of near-native decoys placed on each top percentage rank. Bold values indicate best performance. Fig. 3. 
 Discriminative power of pair potentials on 28 CAPRI targets.  Scores are defined as for Figure 2. Acceptable (left) and medium quality (right) decoys are labelled as near native (Color version of this figure is available at  Bioinformatics  online.) 3.5 Classes of complexes where CIPS fails CIPS almost always assigns high score to near-native structures. When it fails, the best rank assigned by CIPS to a near-native is not far from the best rank assigned by other methods. In  Supplementary Figures S21–25 , we illustrated this observation on each decoy set of the  CF  dataset. CIPS behaves better on  DF  rather than on  CF . Namely, on  DF , for 50 (for 44) out of 51 decoy sets, 75% of the near-native poses are ranked on top 50% (on top 20%). We recall that  DF  poses have been generated by full-atom docking while coarse-grain docking generated  CF  poses. This means that the quality of the poses might influence CIPS ranking. In the attempt at characterising the decoy sets where CIPS does not perform well in  CF , we defined three classes of decoy sets based on CIPS CF  ranking of near-native poses (see  Supplementary Section S1.10 ). For each class, we computed the physico-chemical properties of the experimental interface and concluded that CIPS fails to rank near-native solutions on top when the interface is enriched in charged residues and in contacts between opposite charges (see  Supplementary Fig. S26 , top left). Note that the same observation holds true for the decoy set in  DF  that was not ranked properly by CIPS, also presenting a highly charged experimental interface. Finally, we observed that the interface size does not affect CIPS CF  performance. 4 Discussion It has been shown that protein–protein interfaces span a wide range of sizes, which are not related to binding strength, and that the apolar tendency is variable ( Nooren and Thornton, 2003 ). We analyse in depth the characteristics of the interface by identifying groups of contacts that are susceptible to carry the strength of the interaction, we study which part of it is the most specific and we find that all contacts are important for recognition. Focusing only on sub-regions of the interface hinders the discriminative power of knowledge-based methods for decoy scoring. We demonstrate that capturing information coming from both amino acid specificity at the interface and their coupling in the interaction allows to discriminate high-quality structural models, among thousands of decoys, with high precision. Taking into account the level of connectivity of each residue type further refines the method. Previous studies ( Pons  et al. , 2011 ) defined a performing propensity matrix based on the interface random model instead of the contact random model used in CIPS. Interface residues and contacts are detected with the same criterion for the two approaches and their difference in performance is mainly due to the introduction of the interface residue propensity in CIPS. We find out that CIPS is more powerful than three other propensity matrices available in the literature and two widely used atomic potentials: PISA ( Krissinel and Henrick, 2007 ) and ZRANK ( Pierce and Weng, 2007 ). On Dockground decoy benchmark, CIPS alone performs far better than a learning method previously proposed ( Fink  et al. , 2011 ). A decoy-based potential, directly using experimental information on the complexes to be evaluated and representing the physico-chemical properties of the interaction contacts, allowed us to check how our method behaves with respect to a reference. We observe that a score defined as the combination of CIPS and atomic potentials almost reaches the “ideal” performance. This effect is more evident on the decoys obtained with a full-atom model. Note that no refinement is applied on coarse-grain structures, thus clashes might occur; moreover, on some structures of  CF , the conformational change is not negligible (see  Supplementary Section S1.7 ), but CIPS copes well with it (see  Supplementary Fig. S14  and  Supplementary Table S13 ).  DF  is an easier testing set compared to  CF . On  DF , decoys are selected by knowing the experimental structure; on  CF , they are filtered according to a coarse-grain energy function. We observe that the total number of interface contacts well discriminates near-native solutions in  DF  (AUC 0.82), suggesting that normalized scores are needed to partially get rid of the bias associated to decoys pre-selection. The construction of ‘ideal matrices’ for  DF  and  CF  (see  Supplementary Fig. S6 ) leads us to observe that amino acid pairings involving aromatic residues and hydrophobic contacts are more abundant at near-native interfaces compared to non-near-native ones. Surprisingly, contacts between oppositely charged residues do not discriminate true interactions. This might be due to the inclusion of a Coulomb energy term in the docking procedure, which might drive the search towards wrong conformations with lots of positive/negative contacts. Instead, interaction specificity might be the result of a complex combination of signals at the interface, as suggested by our analysis performed with propensity-based descriptors. Note that the low specificity associated to positive/negative contacts in near-native conformations is in contrast with the pattern observed on most of the propensity matrices found in the literature, including CIPS (compare  Fig. 1  and  Supplementary Fig. S6B ). Consistently, we observed that CIPS predictions are worse on highly charged interfaces (see Section 3.5), so alternative methods might be designed in this case. 5 Conclusion Protein partner prediction is a fundamental problem in biology that needs to be tackled with robust computational pipelines, the first steps being docking calculations and filtering out incorrect conformations. CIPS is a valuable method for systematically screening the large amount of poses returned by large-scale cross-docking. It is fast, accurate and robust upon decrease of protein structure resolution; thus, it can be safely employed for scoring structural models obtained with coarse-grain docking methods. Further applications of the method might be CIPS direct inclusion in the docking procedure, similarly to  Mintseris  et al.  (2007) , to guide and speed up the search within the conformational space. From a broader point of view, CIPS can be seen as a method able to add constraints to the degrees of freedom of the quaternary structure. The usage of statistical methods have been successfully employed in the protein folding problem ( Süel  et al. , 2003 ;  Weigt  et al. , 2009 ;  Marks  et al. , 2012 ). An emerging field of research focuses on the application of co-evolution methods to inter-protein contacts, but the problem has been shown to be much harder to solve ( Wilkins  et al. , 2013 ;  Hopf  et al. , 2014 ). Pair potentials could contribute in a completely novel manner to protein complex design. Supplementary Material Supplementary Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>ThreaDom: extracting protein domain boundary information from multiple threading alignments</Title>
    <Doi>10.1093/bioinformatics/btt209</Doi>
    <Authors>Xue Zhidong, Xu Dong, Wang Yan, Zhang Yang</Authors>
    <Abstract>Motivation: Protein domains are subunits that can fold and evolve independently. Identification of domain boundary locations is often the first step in protein folding and function annotations. Most of the current methods deduce domain boundaries by sequence-based analysis, which has low accuracy. There is no efficient method for predicting discontinuous domains that consist of segments from separated sequence regions. As template-based methods are most efficient for protein 3D structure modeling, combining multiple threading alignment information should increase the accuracy and reliability of computational domain predictions.</Abstract>
    <Body>1 INTRODUCTION Protein molecules are composed of domains that fold, function and evolve autonomously. The definition of protein domains is, however, not absolute. Recent studies have shown that protein domains within the same family or superfamily can vary significantly in both structure and function ( Dessailly  et al. , 2010 ;  Reeves  et al. , 2006 ). Nevertheless, correct assignment of boundaries of the protein domains is essential for the efficient elucidations of protein function and evolutionary mechanisms. The most accurate characterization of protein domains is through the analysis of the 3D structures. However, the experimental determination of protein structures is often time and manpower expensive and some proteins are even impossible to solve currently. The computational domain prediction from the amino acid sequence is, therefore, highly demanded. A variety of methods have been recently developed in this regard that can be roughly categorized into three groups: statistical and machine-learning based, homology-based and 3D model-based methods. The statistical and machine-learning-based methods are probably the most frequently used approaches to protein domain predictions, with examples including DGS ( Wheelan  et al. , 2000 ), DomCut ( Suyama and Ohara, 2003 ), Armadillo ( Dumontier  et al. , 2005 ), PPRODO ( Sim  et al. , 2005 ), DOMPro ( Cheng  et al. , 2006 ), DomNet ( Yoo  et al. , 2008 ), DROP ( Ebina  et al. , 2011 ), DOBO ( Eickholt  et al. , 2011 ), PRODOM ( Servant  et al. , 2002 ), ADDA ( Heger  et al. , 2005 ) and EVEREST ( Portugaly  et al. , 2006 ). In the DGS, DomCut and Armadillo programs, the statistical regularities seen in the Protein Data Bank (PDB) structures, including domain size distribution and residue propensities, are used to deduce the domain linker and boundary predictions. In PRODOM, ADDA and EVEREST, the domain boundaries are derived by large-scale sequence comparisons followed by clustering analyses. In the rest examples (PPRODO, DOMPro, DomNet, DROP and DOBO), the residue-based statistical features, together with the position-specific scoring matrix from PSI-BLAST search, are trained by machine-learning techniques, including neural network, support vector machine and random forest classifiers. These methods deduce boundary information from sequence only, which can in principle be applied to any proteins. But the overall accuracy is low compared with the homology-based approaches. In the homology-based methods, e.g. Pfam ( Finn  et al. , 2010 ), CHOP ( Liu and Rost, 2004 ) and FIEFDOM ( Bondugula  et al. , 2009 ), target sequences are searched through known protein structure or family libraries by hidden Markov model (HMM) or PSI-BLAST programs. The domain boundary information is then obtained by mapping the domain information from the homologous templates or families following the sequence alignments. The homology-based methods can have a high accuracy of predictions when close templates are identified, but the accuracy decreases sharply when the sequence identity of target and template is low (e.g. &lt;30%). In the 3D model-based methods, e.g. SnapDRAGON ( George and Heringa, 2002 ), RosettaDom ( Kim  et al. , 2005 ) and OPUS-DOM ( Wu  et al. , 2009 ), the authors first construct tertiary structure models of the target by either  ab initio  folding or knowledge-based coarse-grained modeling simulations. Domain parser tools are then used to assign the domain information on the predicted 3D models. The accuracy of domain assignments relies on the quality of the tertiary models, which usually decreases with the size of the target proteins because of the limited ability of  ab initio  folding simulations ( Zhang, 2008 ). Having in mind the improved power of the template-based protein structure predictions and the increasing size of PDB, we propose a new algorithm ThreaDom based on multiple threading algorithms, which aims to significantly improve the reliability of domain predictions in the category of distantly homologous protein targets. Although the threading-based algorithms have been successfully used in the CASP experiments for modeling multiple-domain protein structures where the domain boundaries are usually decided by human-intervened views and interpretations of multiple-threading alignment profiles ( Zhang, 2007 ,  2009 ), this is the first time to integrate the multiple threading algorithms into an automated pipeline for domain boundary determinations. The key to the algorithm is the development of a sensitive domain boundary profile that can calibrate composite structural and sequence alignment information from the multiple threading templates for precise domain assignment. The method will be systematically benchmarked on large-scale proteins, to examine the weaknesses and strengths in comparison with other widely used domain prediction approaches. 2 METHODS 2.1 Data sets We collect a non-redundant protein set from PISCES ( Wang and Dunbrack, 2003 ), with the sequence identity cut-off at 25%, resolution &lt;3.0 Å and R-factor &lt;1.0. The domain definitions of these proteins are taken from CATH 3.4 ( Orengo  et al. , 1997 ). If the template protein does not exist in the CATH, the domain structure is defined by DomainParser ( Xu  et al. , 2000 ). All proteins with a chain length &lt;80 residues or domain length &lt;40 residues are removed, which results in a protein set containing 715 multi-domain and 2524 single-domain chains. The 715 multi-domain proteins are divided into training and testing sets, including 400 and 315 chains, respectively. The 2524 single-domain chains are randomly divided into two sets, paired with multi-domain proteins in the training and testing sets. Based on the significance of threading alignments, the protein chains are categorized into ‘Easy’, ‘Medium’ and ‘Hard’ proteins (see later in the text). Thus, our training set includes 331 × 2 ‘Easy’, 46 × 2 ‘Medium’ and 23 × 2 ‘Hard’ protein sequences, and the testing set contains 261 × 2 ‘Easy’, 36 × 2 ‘Medium’ and 18 × 2 ‘Hard’ non-redundant sequences, where ‘×2’ refers to two sets of single and multiple-domain proteins. 2.2 Multiple template identification by LOMETS In ThreaDom, LOMETS ( Wu and Zhang, 2007 ) will be used to thread the target sequences through the PDB for structural template identifications. LOMETS contains eight threading programs of complementary approaches, including HHSEARCH ( Söding, 2005 ), MUSTER ( Wu and Zhang, 2008 ), PROSPECT2 ( Xu and Xu, 2000 ), PPA-I ( Wu  et al. , 2007 ), SAM-T02 ( Karplus  et al. , 1998 ), SPARKS2 and SP3 ( Zhou and Zhou, 2005 ). In HHSEARCH, we implemented two versions of global and local HMMs, HHSEARCH-1 and HHSEARCH-2. These eight threading programs are displayed in  Figure 1  as the LOMETS-based threading server layer.
 Fig. 1. Architecture of ThreaDom. It consists of a library layer, a meta-server threading layer and a domain decision layer from the bottom. Eight threading programs in the meta-server layer access three structure libraries in the library layer to provide alignments for the domain decision layer. Threading Library Map-Adapter calls libraries in the library layer and provides a unified order map. In the decision layer, data flow from A to F Three independent template libraries are used. First, MUSTER, PPI-1, SAM-T02 and PROSPECT2 use an internal I-TASSER template library with sequence identity &lt;70% from  http://zhanglab.ccmb.med.umich.edu/library/ ; SPARKS2 and SP3 use another internal library of sequence identity &lt;40%; HHSEARCH uses the library downloaded from  ftp://ftp.tuebingen.mpg.de/pub/protevo/HHsearch/databases , which has also a 70% sequence identity cut-off. The domain boundaries for all templates are pre-calculated based on CATH3.4 or DomainParser. As residues in the template structures were re-ordered in threading libraries, a map-adapter is established to map all the template libraries into the original entries in the PDB library so that CATH domain definitions can be exploited. For each LOMETS program, a  Z -score cut-off ( Z 0 ) is assigned, based on the threading results data of 1190 independent training proteins, so that the well-defined alignments with an average TM-score &gt;0.6 can be achieved when  Z -score &gt;  Z 0 . Here,  Z -score is a measure of the significance of the target-template alignment, defined as the score difference to the mean in the unit of standard deviation. A protein target is categorized as ‘Easy’ if each of the LOMETS threading programs have at least one template with  Z -score &gt;  Z 0 ; it is a ‘Hard’ target if there is no template hit with  Z -score &gt;  Z 0  by any programs. Otherwise, it is assigned as a ‘Medium’ target. 2.3 Outline of ThreaDom procedure Domain predictions in ThreaDom are based on two assumptions: (i) homologous proteins have similar domain structures; (ii) residues in the core regions of domain structures are evolutionally more conserved than that in the boundary (or linker) regions between domains. Following the assumptions, the ThreaDom procedure contains three steps as displayed in  Figure 1 .
 Target sequences are threaded through the PDB by eight LOMETS programs, and a multiple sequence alignment is constructed based on the target sequence (with external inserts/gaps shaved). A domain conservation score (DCS) is calculated for each residue position based on the LOMETS multiple sequence alignments, which counts for the balance of conservation and gap penalty scores. Domain boundaries are assigned based on the DCS profile using a target-specific scoring cut-off. 
 2.4 Domain conservation score In ThreaDom, the domains of the target sequence are specified by the location and size of linker regions between two domains (e.g. A and B):
 (1) 
where  i  is the residue number,  start ( L )  is the residue position of the last residue in domain A along the sequence,  end 
 ( L )  is the position of the first residue in domain B and  Start ( L ) &lt;end 
 ( L ) . In ThreaDom, we consider two contributions of template domain boundary structure and target-template alignment gaps, which are used to decide the location and size of the domain linkers of the target sequences. 2.4.1 Template domain linker score ThreaDom considers  T  template alignments obtained by LOMETS, where  T good  is the number of templates with a  Z -score ≥ Z 0 , and  T bad  is that with a  Z -score &lt;  Z 0 . Following CATH (or DomainParser) domain definition, the  j th template has a domain split specified by the linker structure  :
 (2) 
where   and   are the starting and ending positions of the linkers on  j th template. Considering the alignment error that may result in linker shift, we introduce a distance allowance  d  to increase the size of template linkers. For residue  i , the template domain linker score from the  T  template alignments is calculated by
 (3) 
where  w 1  and  w 2  are the weight parameters on ‘good’ or ‘bad’ templates.  a ij  counts for whether the  i th residue on the target is aligned with the linker regions of  j th template, i.e.
 (4) 
Here,  m  counts for the confidence of domain assignment of template structures. In our benchmark test, there is an agreement between DomainParser and CATH for ∼80% proteins. We set  m =  0.8 if the template domain is assigned by DomainParser, and  m  = 1.0 if by CATH, as the latter is assisted by human intervention and of a higher accuracy in domain assignment. 2.4.2 Gap penalty score ThreaDom specifies two types of gap penalties from threading alignments: terminal gap  G term  and internal gap  G int . The terminal gaps on  j th template are defined by
 (5) 
where  j N  and  j C  are the N- and C-terminal positions of the first and last aligned residues on  j th template. The internal gap is defined by
 (6) 
where   denotes the middle point of the internal gaps, and  start G ( j ) and  end G ( j ) are the starting and ending locations of the gaps. To rule out alignment noise, we only consider the gaps with a size longer than  l , i.e.  . The total gap penalty score for the residue  i  is calculated by:
 (7) 
where  w 3  and  w 4  are the weight parameters;  b ij  and  c ij  are the binary values representing whether the  i th residue locates in the gap regions of the  j th template alignment:
 (8) 
 2.4.3 Domain conservation score The template domain linker score and gap penalty score indicate the degree of variations of multiple threading alignments at the position  i . Accordingly, the domain conservation score,  S ( i ), is calculated by
 (9) 
where  1 &lt; i &lt; N ,  N  is the length of the target sequence. To reduce noise in the DCS assignment that may result in artificial domains with very short length, we smooth the domain conservation score using a 19-residue window:
 (10) 
Meanwhile,  S ( k ) is set to 0 if  S ( k ) &lt;0. Thus, the smoothed domain conservation score  S ’( i ) has a value in (0,1). 2.4.4 Deciding domain linkers by DCS profiles A putative domain linker  L ( k )  in ThreaDom is an aggregation of the continuous residues that have the conservation score below a certain cut-off  S c , i.e.
 (11) 
where   represents the number of linkers. The middle point of the linker,  , is noted as the predicted boundary to the linker  L ( k )  in the ThreaDom program. As the majority of protein domains in the PDB have a length longer than 40 residues, we consider two length-based domain filters. First, if the distance from   to the terminal of sequence is &lt;40, the  L ( k )  is removed from the putative linker list (i.e. set  L ( k )  = 0). Second, if the distance between two neighboring linkers is too close, i.e.  , the linkers will be merged into one linker. The boundary position of the merged linkers is calculated by
 (12) 
where   is associated with the confidence of the linker assignment on  . Based on  Equation 12 , if the linkers have similar confidence, i.e.  , the boundary position of the merged linker is at the middle of the two linkers,  . Otherwise, the boundary of the merged linkers will be biased to the position of the linker with a higher confidence score. The final continuous domain assignment in the ThreaDom is represented in the form of  , and the residue range in each pair of parenthesis represents an individual domain. One example of the ThreaDom protein domain prediction is given in  Figure 2 , where four individual domains are separated by three linkers  L (1) ,  L (2)  and  L (3) , with the cutoff  S c  = 0.60.
 Fig. 2. An illustration of the domain decision in ThreaDom based on the domain conservation score profile. Four individual domains are separated by three linkers defined by the valleys of the DCS distribution. The vertical dotted lines indicate the start and end locations of each putative linker. The vertical solid lines denote the predicted boundary at the middle of the linkers ( ,   and  ) Equations 1–12  have eight free parameters ( w 1 ,  w 2 ,  w 3 ,  w 4 ,  d ,  l ,  T  and  S c ), which will be trained on our training proteins of various classes (see later in the text). 2.5 Strategy for detecting discontinuous domains The term discontinuous domain refers to a domain that contains two or more segments from separated regions of target sequence. ThreaDom detects discontinuous domains based on the DCS profile and the pre-defined domain boundaries of the threading templates, which contains three steps:
 Step I: Detecting discontinuous domain sequence. A target is considered to have discontinuous domains if it has &gt;30% templates that have discontinuous domain structure in the LOMETS template collection. Step II: Clustering the discontinuous domain templates. The discontinuous domain templates are clustered based on their domain boundary locations and domain assignments. The discontinuous templates are clustered into one category if they have the same number of domains with same domain segments number and similar boundaries, where ‘similar boundary’ means that the difference in boundary positions is within ±5 residues after structure alignment of the two templates. Step III: Boundary refinement and discontinuous domain substitution. After clustering, the DCS-based domain prediction and the domain structure in the first template cluster are combined, i.e. if the domain boundary difference between the DCS prediction and the first template cluster is within ±20 residues, the separated domains in the DCS prediction will be merged into a single domain following the assignment in the first template cluster. Meanwhile, if the number of domains in the DCS prediction is &gt;3 but less than that in the first cluster, we substitute the DCS prediction with the domain information of the first cluster when the domain boundaries in &gt;50% of templates are consistent (i.e. differences are ±20 residues). 
 2.6 Evaluation criteria We evaluate the ability of ThreaDom on both the domain number and the domain boundary predictions. The domain number prediction is assessed by counting the accuracy of single- or multi-domain protein classifications. We use specificity, sensitivity and Matthew’s correlation coefficient (MCC) to assess the domain number prediction:
 (13) 
 (14) 
 (15) 
where  TP ,  FP ,  TN  and  FN  are true positive, false positive, true negative and false negative rates of the classifications, respectively. To assess the quality of domain boundary predictions, we calculate precision and recall rates, the normalized domain overlap (NDO)-score ( Tai  et al. , 2005 ) and the domain boundary distance (DBD) score ( Tress  et al. , 2007 ). The precision has a similar definition to specificity as defined in  Equation 13 , but boundary prediction is designated as ‘TP’ if it is within ±20 residues of the true boundary; otherwise it is an ‘FP’ prediction. Recall is similar to the sensitivity in  Equation 14 , which represents the fraction of the target boundaries that are correctly retrieved in the domain predictions. The NDO-score is defined as the normalized overlap rate of all predicted domain and linker regions with the true assignments of the target structure. The DBD-score measures the distance of the predicted boundaries from the true target domain boundaries. To avoid the contamination of homologous templates that are easy to predict in ThreaDom, we exclude all the templates that have a sequence identity &gt;30% to the target protein or that are detectable by PSI-BLAST with an E-value &lt;0.05. As a control, we implemented five publicly available domain predictors, including FIEFDom ( Bondugula  et al. , 2009 ), Pfam ( Finn  et al. , 2010 ), DomPro ( Cheng  et al. , 2006 ), DROP ( Ebina  et al. , 2011 ) and PPRODO ( Sim  et al. , 2005 ), which represent different type of homology- and machine-learning-based methods. These methods are run on the same test set of proteins. 2.7 Parameter training There are eight free parameters in the ThreaDom scoring  Equations (2–11) , including four weight parameters ( w 1 ,  w 2 ,  w 3  and  w 4 ), the linker shift  d , the length cut-off of internal gaps  l , the number of threading templates  T  and the DCS cut-off  S c . We trained the parameters by maximizing the precision, recall and NDO scores on the 800 training proteins (400 single-domain + 400 multi-domain proteins, see  Section 2.1 ). The parameters are tuned separately for Easy and Medium/Hard proteins. To increase the efficiency, we projected the parameter values on an 8D system and enumerate various values on the lattices. Parameter values corresponding to the optimal results were selected with results summarized in  Table 1 . For the 331 × 2 ‘Easy’ targets in our training set, the optimal NDO, precision and recall scores are 0.919, 0.836 and 0.77, respectively, and those for the 69 × 2 ‘Medium/Hard’ targets are 0.821, 0.476 and 0.32, respectively.
 Table 1. The optimized parameters in ThreaDom Parameters Easy Medium/ Hard Weight of good templates ( w 1 ) 2.0 2.0 Weight of bad templates ( w 2 ) 0.6 0.5 Weight of terminal gaps ( w 3 ) 0.8 1.4 Weight of internal gaps ( w 4 ) 0.1 0.5 Shift of linkers or gaps ( d ) 12 10 Minimum length of internal gaps ( l ) 15 15 Number of used templates ( T ) 50 50 DCS cut-off ( S c ) 0.6 0.76 3 RESULTS AND DISCUSSION 3.1 Domain classification prediction A sequence is considered to be a multi-domain protein if it includes one or more domain linkers. In the test on the 315 × 2 non-homologous proteins that are also non-homologous to the training protein set, ThreaDom correctly classifies proteins as being either single- or multi-domain proteins in 81% of the cases. For the ‘Easy’ protein set, the accuracy is 84.7%, and for ‘Medium/Hard’ test set, the accuracy is 68.5%. Table 2  shows a summary of ThreaDom performance in control with the other five methods on the single-domain or multi-domain classification. For all the three categories of ‘All’, ‘Easy’ and ‘Medium/Hard’, ThreaDom produces the highest MCC among the five predictors. The MCC values are 54, 60 and 41%, respectively, higher than that by the second best predictor FIEFDom, which is a homologous method-based on PSI-BLAST search. Pfam, a standard HMM-based domain assignment program, has a slightly lower average MCC than FIEFDom.
 Table 2. Single- or multi-domain classifications on CATH domains Type Predictor MCC Single-domain Multi-domain Spec. Sens. Spec. Sens. All ThreaDom 0.682 0.800 0.902 0.887 0.775 FIEFDom 0.443 0.724 0.683 0.700 0.740 Pfam 0.378 0.645 0.813 0.747 0.552 DROP −0.019 0.491 0.517 0.490 0.463 DomPro 0.287 0.571 0.917 0.790 0.311 PPRODO 0.076 0.800 0.025 0.505 0.994 Easy ThreaDom 0.734 0.837 0.908 0.900 0.824 FIEFDom 0.458 0.768 0.648 0.695 0.805 Pfam 0.420 0.676 0.793 0.750 0.621 DROP −0.019 0.490 0.479 0.491 0.502 DomPro 0.304 0.579 0.912 0.793 0.337 PPRODO 0.076 1.000 0.011 0.503 1.000 Medium/Hard ThreaDom 0.432 0.653 0.870 0.806 0.537 FIEFDom 0.307 0.597 0.852 0.742 0.426 Pfam 0.178 0.538 0.907 0.706 0.222 DROP −0.020 0.494 0.704 0.484 0.278 DomPro 0.199 0.537 0.944 0.769 0.185 PPRODO 0.113 0.714 0.093 0.515 0.963 Note : Bold values denote the best performance in each category. MCC, Matthew’s correlation coefficient; Spec., specificity; Sens., sensitivity. Interestingly, the two machine-learning-based methods, DomPro and PPRODO, have top specificity or sensitive values in some categories, but they have a low MCC because of unbalanced classifications. For example, in the ‘Easy’ set, PPRODO has a sensitivity of 100% for multi-domain classification but only 1.1% for single-domain, whereas the corresponding specificity values were 50.3 and 100%, respectively. These data imply that PPRODO classifies almost all sequences as multi-domain protein, which, therefore, leads to a low MCC value of 0.076. Similarly, DomPro tends to classify most chains as being single-domain chains that also results in a modest MCC value, although it has a better balance than PPRODO. In other words, DomPro is an underpredicting method for multiple-domain, whereas PPRODO is overpredicting. DROP, another machine-learning method, has a negative MCC in all three categories of targets because of the low assignment accuracy. 3.2 Domain boundary prediction For the proteins in the ‘All’, ‘Easy’ and ‘Medium/Hard’ sets, the domain boundary predictions by ThreaDom has the NDO-scores of 0.893, 0.905 and 0.832, DBD-scores of 0.861, 0.887 and 0.737, precisions of 0.784, 0.814 and 0.562 and recalls of 0.670, 0.708 and 0.425, respectively.  Figure 3  presents the ThreaDom prediction results together with that by other five control methods, where the  y -axis is the value of NDO, DBD, boundary precision and recall scores, and  x -axis denotes the categories of protein sets in ‘All’, ‘Easy’ or ‘Medium/Hard’. For proteins in all the three categories, ThreaDom achieves the highest value in NDO-score, DBD-score and boundary precision. ThreaDom also has the highest score in boundary recall for ‘All’ and ‘Easy’ categories, and the second highest score in boundary recall for the ‘Medium/Hard’ proteins.
 Fig. 3. Summary of domain boundary predictions by ThreaDom and the control predictors. ( A ) Normalized Domain Overlap score; ( B ) Domain Boundary Distance Score; ( C ) Precision of predicted boundaries; ( D ) Recall of pre-defined boundaries As shown in  Figure 3 D, PPRODO has a slightly higher recall value than ThreaDom for the ‘Medium/Hard’ targets. This is partly because of the overprediction of PPRODO that predicts most of the single-domain proteins as multi-domain and has, therefore, on average more boundary linkers assigned. This leads to a worse precision value (26.7%) in comparison with that by ThreaDom (56.2%). Because of the unbalanced recall and precision, PPRODO has overall a poor performance when assessed by the NDO- and DBD-scores ( Fig. 3 A and B). Different methods have different sensitivities on the category of protein targets. As shown in  Figure 3 , the predictions by the statistical and machine-learning-based methods (DomPro, DROP and PPRODO) have similar performances in both ‘Easy’ and ‘Medium/Hard’ categories, as these predictions are from sequence only. The HMM-based method, Pfam, also does not show difference between Easy and Hard targets because the domains in Pfam were retrieved from UniProt and ADDA sequence clustering ( Heger and Holm, 2003 ), which are not directly associated with template structures in the PDB library. However, the two template-based methods, ThreaDom and FIEFDom, have an obvious difference between ‘Easy’ and ‘Medium/Hard’ proteins because of the different availability of the template hits in the two category of proteins. Nevertheless, ThreaDom identified much more accurate boundary predictions than FIEFDom in both ‘Easy’ and ‘Medium/Hard’ categories. Particularly in the ‘Medium/Hard’ protein set, the precision and recall are three and five times higher than that in FIEFDom. These improvements are mainly because of (i) the better identification of templates by LOMETS than that by PSI-BLAST or HMM searches and (ii) the sensitive calibration of gaps and alignments by the domain conservation score as designed by ThreaDom. These advantages are essential for ThreaDom to detect efficient domain structures for the weakly- or non-homologous proteins. In general, for the ‘Easy’ proteins, there are a large number of ‘good’ templates with a high  Z -score as detected by various threading programs in ThreaDom. The consensus domain assignments of the template structures dominate the boundary predictions. For the ‘Medium/Hard’ targets, however, there are few consensus ‘good’ templates, and the identification of consensus terminal and internal alignment gaps becomes sensitive. This explains the reason that the weighting parameters of gap penalty score (w 3  and w 4  in  Equation 7 ) become larger for the ‘Medium/Hard’ targets than that for the ‘Easy’ targets ( Table 1 ). In  Supplementary Table S1  of  Supplementary Materials , we separated the contributions of template alignment and gap penalty scores in ThreaDom. Although the gap penalty score tends to make more important contribution for Hard targets, a combination of template and gap penalty scores outperforms individual scores in all categories of targets. Thus, using a balanced consensus of template domain assignment and the internal and terminal gaps from multiple template alignments, the DCS system helps erase the incident errors from single template alignment by individual threading programs that are often less reliable. In  Figure 4 , we show an illustrative example from the GTP cyclohydrolase I (PDB ID: 1wurA), which is a hard target for which none of the LOMETS threading program has a strong template alignment with  Z -score &gt; Z 0 . The chain is classified as a two-domain protein with boundary structure of (1–55) (56–185) in the CATH database ( Fig. 4 E). In the control programs, both FIEFDom and DROP incorrectly predicted the protein as a single-domain chain, whereas PPRODO and DomPro correctly assigned it as a multi-domain protein, but the assigned domain boundary is 35 and 45 residues away from the CATH assignment, respectively.
 Fig. 4. An illustrative example of ThreaDom prediction on ‘Hard’ target from the GTP cyclohydrolase I (PDB ID: 1wurA). ( A ) DCS score distribution. ( B–D ) Counts of templates with terminal gap, internal gap and template domain assignment along the sequence in the total 50 selected templates. ( E ) X-ray structure of the target protein with CATH and ThreaDom domain boundaries labeled As shown in  Figure 4 D, the alignments among the 50 selected threading templates are divergent and not conclusive: four templates at residue 46, two at residues 65, 69 and 102 and others with domains almost evenly distributed along the sequence. Similarly in  Figure 4 B and C, the internal and terminal alignment gaps are nearly even-distributed. However, when we combine the contributions from the domain and gap assignments as described in  Equations 9  and  10 , the overall DCS profile has an obvious valley around residue 60 ( Fig. 4 A), which is due to the weak but consistent tendency of gap and domain assignments in the multiple threading alignments. Although there are two other valleys in the N- and C-terminals, the locations are &lt;40 residues away from the sequence ends and are ruled out by the default length filter. Finally, the ThreaDom boundary prediction is (1–60) (61–185), which shift by only five residues from the CATH assignment. This example highlights the power of ThreaDom in extracting correct domain information from distantly homologous threading alignments by combining multiple domain and alignment gap/insertion information. 3.3 Domain prediction assessed by alternative domain definitions One concern of the aforementioned data analyses is on the possible bias of distinctive domain definitions of the training and test proteins, as some methods (e.g. FIEFDom) were trained by domains defined in the SCOP database ( Murzin  et al. , 1995 ), but the analyses are mainly on CATH definitions, which is what ThreaDom was trained on. In  Supplementary Table S2 , we present a quantitative analysis of the domain predictions on the 315 test protein pairs with the domains defined by SCOP1.75. Similarly, if a protein cannot be seen in the SCOP library, a definition from DomainParser is used instead. Although some small variations are seen in specific score values, there is no qualitative difference between  Supplementary Table S2  and the data shown in  Table 1  and  Figure 3 . These results demonstrate that the distinctive domain definitions of different databases have no impact on the training and testing procedures of domain predictions. 3.4 Discontinuous domain prediction Domain assignment for the proteins that have domains consisting of segments from separated locations, called discontinuous domains, is a long-standing unsolved problem. Despite the significant importance of discontinuous domains in protein structural determination and function annotations, there is so far no efficient method available for discontinuous domain prediction. To test the ability of ThreaDom in discontinuous domain prediction, we collect 486 non-homologous multi-domain proteins from CATH 3.4 that include at least one domain with discontinuous segments. These protein domains/segments have &gt;40 residues with a pairwise sequence identity &lt;40%. Overall, the automated ThreaDom procedure correctly identified 88.9% of the proteins as multi-domain proteins. For the domain boundary prediction, the precision and recall are 83.9 and 64.5%, respectively, which are comparable with that for the continuous domain protein samples (78.4 and 67.0% in precision and recall), although we did not separately train ThreaDom on the discontinuous domain proteins. The success rates of the predictions demonstrate that the segment assembly procedure has efficiently combined the identified domain linkers from separated positions into the discontinuous domains. To illustrate the procedure, we present in  Figure 5  an example of a discontinuous domain protein from the aminopeptidase I in  Clostridium acetobutylicum  (PDB ID: 2gljE). The domain structure in CATH assignment is (1–105;244–455) (106–243), where the first domain  D 1  (1–105;244–455) contains two segments  S 11  (1–105) and  S 12  (244–455). The second domain  D 2  is a continuous domain containing one segment  S 21  from 105 to 243. In  Figure 5 , the domain boundary residues PRO105 and LYS244 are labeled in blue, ARG106 and GLY243 in magenta. P 1  and P 2  indicate the positions that split the sequence into the two domains.
 Fig. 5. Illustration of ThreaDom on discontinuous domain prediction for the aminopeptidase I protein (PDB ID: 2gljE). The segments assigned by ThreaDom,  s 1  (1–103),  s 2  (104–243) and  s 3  (244–455), are marked in red, green and yellow, respectively. The separated segments ( s 1  and  s 2 ) are merged into a single domain following the clusters of the discontinuous domain templates. P 1  and P 2  denote the domain boundary position according to CATH 3.4, and P 2  and P 3  are that predicted by ThreaDom As most of the top templates by LOMETS have discontinuous domain structure, ThreaDom classified the target as a discontinuous domain protein. Following the multiple template alignments, the sequence is split into three segments of  s 1  (1–103),  s 2  (104–243) and  s 3  (244–455), which are marked in red, green and yellow, respectively, in  Figure 5 , where P3 and P2 indicate the splitting positions of the segments. These segments are highly consistent with the first cluster of discontinuous-domain templates that have a domain structure of (27–104;247–454) (105–246). Therefore, the segments of  s 1  and  s 3  were merged in a single domain with  s 2  assigned as the second domain. As a result, there is only a two-residue shift in the domain boundary by the ThreaDom prediction compared with the CATH assignment in this example. 3.5 Benchmark on CASP targets As publicly available domain predictors are limited, to have a more extensive benchmark with the state-of-the-art methods, we test ThreaDom on the protein targets in CASP8 ( Ezkurdia  et al. , 2009 ), which is the last community-wide blind experiment on protein domain prediction (DP). The DP section in CASP8 contains seven multi-domain Free Modeling (FM) targets, 29 Multi-Domain Hard (MD-Hard) targets and 20 DisContinuous Domain (DCD) targets. To mimic the CASP procedure, all template proteins, which were solved after the CASP8 experiment, were excluded from the LOMETS threading library when implementing ThreaDom. In  Figure 6 , we present the average NDO-score of ThreaDom predictions in the three categories, in control with the seven severs in the CASP8 DP section, which submitted predictions for all the targets (five other servers, which submitted only partial targets, were not shown in the figure). Overall, ThreaDom has an average NDO-score of 0.738, 0.868 and 0.854 for the FM, MD-Hard and DCD targets, respectively. For the entire set of 56 targets, the average NDO-score is 0.847, which is higher than all the predictors in the CASP8 experiment.
 Fig. 6. Average normalized domain overlap (NDO)-score of ThreaDom predictions on the CASP8 domain prediction targets, in comparison with the CASP8 servers that submitted prediction for all 56 DP targets In  Figure 7 , we present three typical examples of ThreaDom predictions for the CASP8 targets. First, T0496 and T0397 in  Figure 7 A and B have the domain boundary defined as (4–123) (124–178) and (1–82) (83–150), respectively, based on the experimental structures. Both targets are FM targets that have no obvious template hit by LOMETS. ThreaDom combined the consensus of template alignments and gap penalty scores, which generated a domain prediction as (1–113) (114–178) and (1–75) (76–150) for the two targets; these correspond to NDO-scores of 0.889 and 0.907, respectively. All the predicted boundaries are within ±10 residues from the native domain definition.
 Fig. 7. Illustrative examples of domain prediction by ThreaDom on CASP8 targets: ( A ) FM target T0496; ( B ) FM target T0397; ( C ) DCD target T0490. The domain boundary from the native structures is labeled by the cutting pars P1–P6 with the adjacent residues marked in blue and magenta. The domain segments predicted by ThreaDom are marked in different colors. In T0490, the neighboring segments are correctly merged into two individual domains T0490 in  Figure 7 C is also an FM target but with DCD structure, (5–87|143–227|319–368) (88–142|228–318). The first step of the DCS scan split the target sequence into five segments, i.e. (1–84) (85–142) (143–223) (224–319) (320–369), shown in red, green, orange, cyan and yellow, respectively. After the template domain structure clustering and boundary refinement, the first, third and fifth segments are merged into the first domain and the rest into the second domain. The final prediction (1–85|143–225|319–368) (86–142|226–318) has a DNO-score of 0.97, which is close to the native structure not only in the domain number and boundary but also in the DCD components ( Fig. 7 C). Domain prediction tests were not included in the most recent CASP experiments (CASP9 and 10). The FM/Hard targets in the experiments, however, represent a set of well-defined real-time proteins free of homologous contaminations. In  Supplementary Table S3 , we list the performance of ThreaDom, in control with FIEFDom, Pfam, DROP, Dompro and PPRODO, on 46 FM/MD-Hard targets with 22 from CASP9 and 24 from CASP10. Similarly, all protein templates solved after the CASP experiments were excluded. To examine the impact of different steps of procedures, we implemented two version of ThreaDom, i.e. ThreaDom1 used the Domain Conservation score without linker refinement and DCD detection procedures, whereas ThreaDom2 is a complete ThreaDom implementation, including both procedures. As shown in  Supplementary Table S3 , ThreaDom2 obviously outperforms ThreaDom1 in all criteria of precision, recall, DNO- and DBD-scores, which demonstrates the importance of the refinement procedures. Overall, the two ThreaDom programs are ranked as the top two methods in most of the assessments for the CASP9 and CASP10 targets, except for that the DBD-score, and the boundary recall of the ThreaDom programs are slightly lower than that of a few other methods for the CASP9 targets. 3.6 Drawbacks of ThreaDom ThreaDom is a threading-based method, and the quality of the threading template alignments has a major impact on the performance of the domain predictions. Generally, the success rate for Easy targets with a strong hit is higher than that of Hard/Medium targets. However, there are also cases that strong templates hits can result in incorrect domain assignments. The major sources of errors in ThreaDom come from (i) inconsistent domain order of homologous proteins; (ii) non-specific DCS cut-offs; and (iii) unmatched sequence size between target and templates. Supplementary Figure S1  shows two examples of the incorrect ThreaDom predictions because of inconsistent domain orders, one from the Talin-1 (PDBID: 3dyjA) and one from the DNA polymerase III subunit β (PDBID: 3d1gA). Target 3dyjA is a two-domain protein with boundary at 164 according to CATH ( Supplementary Fig. S1A ). LOMETS identified the top template from 2 × 0cA, which has the same architecture to the target but with domains containing swapped segments ( Szilagyi  et al. , 2012 ) ( Supplementary Fig. S1B ); this results in an incorrect split (1–98) (99–162) (163–241) (242–311) by ThreaDom ( Supplementary Fig. S1C ). ThreaDom did not merge the separate segments, as the fraction of hits on 2 × 0cA is below the cut-off (30%) in this example. Target 3d1gA consists of three domains: (1–123) (124–247) (248–366) in CATH ( Supplementary Fig. S1D ). It has the dominant template alignments from 2awaB with a high TM-score to the target ( Supplementary Fig. S1E ). However, 2awaB contains discontinuous domain structures (1–138|205–244) (139–204|245–375) in CATH, which results in an incorrect domain assignment (1–123|195–241) (124–194|242–366) following threading mapping ( Supplementary Fig. S1F ), despite the fact that the overall topology of the two proteins is close. The domain linker regions in ThreaDom are decided by the interplay of DCS profile and threshold cut-offs. To increase the specificity, the DCS cutoff parameter has been trained in two sets of Easy and Medium/Hard proteins. Nevertheless, a single cut-off score might be still too general, which can result in over- or underprediction of protein domains.  Figure 8 A is an example of overprediction on 3-methyladenine DNA glycosylase I (PDBID: 2ofkA), which is a single-domain protein target. LOMETS considers it as a Hard target, as no significant template was identified. Among the top 35 template hits, 19 are multiple-domain proteins and 10 have the terminal gap near the residue 120. The DCS profile has, therefore, an artificial valley lower than the DCS threshold 0.76 for Hard/Medium target, which results in an overprediction of (1–120) (121–182) for this target. If the DCS threshold for Easy target (0.6) was taken, this artificial valley could have been ignored.
 Fig. 8. Inappropriate DCS thresholds and template sizes can result in incorrect domain predictions. ( A–C ) DCS score for 2ofkA, 1sauA and 1e8yA, respectively; ( D ) histogram of template alignment coverages for 1e8yA Figure 8 B is an example of underprediction because of the inappropriate DCS cut-offs. This protein is from the γ subunit of the dissimilatory sulfite reductase (DsrC) (PDBID: 1sauA), which is a two-domain Hard protein with domain assignment as (1–44) (45–114) in CATH. Although the DCS profile has a well-shaped valley at the correct domain boundary region, ThreaDom mis-predicted it as a single-domain protein because the N-terminal peak of DCS-score is lower than the threshold cut-off 0.76, and the N-terminal domain boundary is, therefore, overseen by ThreaDom. Again, if the DCS threshold for Easy target (0.6) was taken, this valley could have been picked up. The major reason for the low DCS peak is that the domain segment is short, and nearly all the residues undergo a gap penalty because they are too close to the two terminal and internal gaps. As shown in Equations ( 2 ,  5  and  6 ), a distance allowance  d  (=10/12) was introduced to tolerate the alignment/gap uncertainty; but it also introduces overpenalty for small domains. Generally, ThreaDom is unable to predict small domains with size &lt;20 residues. To enhance the sensitivity for small domains, a size-dependent threshold cut-off might be needed for ThreaDom. As ThreaDom derives domain information from templates, insufficient coverage of template alignments is another source of errors in ThreaDom prediction. This is particularly a problem for big proteins, such as the armadillo and heat repeats and β-propellers etc, when the solved template proteins cover only part of the repeats.  Figure 8 C shows the DCS profile of an example of such big proteins from the phosphatidylinositol 3-kinase catalytic subunit (PDBID: 1e8yA), which contains 841 residues with five domains assigned in CATH: (1–166) (168–302) (303–488) (489–646) (647–841). There are overall 31 strong template hits in LOMETS but most of them have a length &lt;550 residues as seen in  Figure 8 D. As a result, ThreaDom generates a prediction of three domains (1–165) (166–303) (304–841) with only the first two domains correctly assigned. We did notice that there are weak valleys in the third and fourth domain boundary linkers that are from the alignment shifts in the C-terminal region, but they are too weak to pick-up by the current DCS cut-offs (0.6 for Easy proteins). This example highlights on one hand the importance of fine-tuning DCS threshold parameters. On the other hand, the domain prediction for big proteins may be further improved by an iterative threading procedure, i.e. repeating threading on the large single-domain sequences. In this example, if we run LOMETS on the remaining big domain (304–841) recursively, correct domain assignment can be obtained for the third, fourth and fifth domains. 4 CONCLUSION We developed a multiple-threading-based method, ThreaDom, for protein domain boundary prediction. For a given target, it first threads the sequence through the PDB library to identify homologous and analogous templates. The profile distribution of the DCS, which combines the composite information of template domain structure and terminal/internal alignment gaps, is then derived for identifying the domain boundary locations. If DCDs are detected in the threading alignments, segments from separated sequences will be merged into single domains under the guide of the top template domain clusters and the target-template alignments. There are several distinct advantages of ThreaDom over the current domain methods in literature. First, for the proteins of homologous templates, the domain assignment from threading alignments achieves a significantly higher accuracy than that from  ab initio  statistical or machine-learning approaches ( Fig. 3 ). For proteins without close homologies, the LOMETS threading programs often identify multiple alignments or super-secondary structure segments from weakly homologous templates, where the DCS profile can help pull out consensus information between domain structure and alignment gaps. This enables ThreaDom to generate useful domain information for the targets that traditional homology-based approaches have difficulty with. It has also the advantage over the structural modeling-based approaches, as no lengthy modeling simulations are needed, and the approach has basically no limit on the size of protein targets. ThreaDom was tested on three independent sets of proteins. For the first set of 315 single- and multi-domain protein pairs, ThreaDom achieves MCCs of 0.734 and 0.432 in single-/multi-domain classification compared with the CATH definition for ‘Easy’ and ‘Medium/Hard’ targets, respectively, which are significantly higher than the control methods from homology and machine-learning-based approaches. Similar results are obtained when using an alternative domain definition from SCOP, which demonstrates the reliability of the data analysis. Second, in the test of 486 DCD proteins, ThreaDom has a similar domain assignment accuracy as that in continuous domains with a precision and recall 83.9 and 64.5%, respectively, in the domain boundary prediction. Finally, when tested on the 56 CASP8 targets, ThreaDom has NDO-scores 0.761, 0.868 and 0.854 for the FM, MD-Hard and DCD targets, respectively. The average NDO-score for all targets is 0.847 that is the highest among all CASP8 servers from different categories of homology, machine-learning and  ab initio  folding-based approaches. Similar achievements are obtained for targets in the CASP9 and CASP10 experiments. Overall, these data demonstrate a new promising approach that fills up the gaps between the sequence-based and the homology-based methods, which can achieve reliable domain assignments in all categories of template-based and template-free modeling protein targets. Nevertheless, fine-tuning on DCS profile cut-offs and iterative threading are needed for further improvement on small domain recognition and long sequence covering, respectively. Although ThreaDom uses template-based modeling approach, it is much faster than the normal protein folding simulations, as the threading procedure involves only the sequence alignment search through a subset of the PDB library, which takes ∼20 min for one target protein. This speed makes it fairly feasible to genome-wide applications, as a single threading scan for a middle-size genome of 5000 genes takes &lt;1 day on a 100-core cluster and that using multiple threading programs, such as LOMETS, should take &lt;1 week. An online server, as well as the source code package of ThreaDom, is freely available for academic users at  http://zhanglab.ccmb.med.umich.edu/ThreaDom/ . 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>BioJava: an open-source framework for bioinformatics</Title>
    <Doi>10.1093/bioinformatics/btn397</Doi>
    <Authors>Holland R. C. G., Down T. A., Pocock M., Prlić A., Huen D., James K., Foisy S., Dräger A., Yates A., Heuer M., Schreiber M. J.</Authors>
    <Abstract>Summary: BioJava is a mature open-source project that provides a framework for processing of biological data. BioJava contains powerful analysis and statistical routines, tools for parsing common file formats and packages for manipulating sequences and 3D structures. It enables rapid bioinformatics application development in the Java programming language.</Abstract>
    <Body>1 INTRODUCTION BioJava was conceived in 1999 by Thomas Down and Matthew Pocock as an Application Programming Interface (API) to simplify bioinformatics software development using Java (Pocock,  2003 ; Pocock  et al. ,  2000 ). It has since then evolved to become a fully featured framework with modules for performing many common bioinformatics tasks. The goal of BioJava is to facilitate code reuse and to provide standard implementations that are easy to link to external scripts and applications. BioJava is an open-source project that is developed by volunteers and coordinated by the Open Bioinformatics Foundation (OBF). It is one of several Bio* toolkits (Mangalam,  2002 ). All code is distributed under the LGPL license and can be freely used and reused in any form. BioJava is a mature project and has been employed in a number of real-world applications and over 50 published studies. A list of these can be found on the BioJava website. According to the project tracking web site Ohloh ( http://www.ohloh.net/projects/biojava ), the BioJava code-base represents an estimated 47 person-years worth of effort. 2 FEATURES BioJava contains a number of mature APIs. The 10 most frequently used are: (1) nucleotide and amino acid alphabets, (2) BLAST parser, (3) sequence I/O, (4) dynamic programming, (5) structure I/O and manipulation, (6) sequence manipulation, (7) genetic algorithms, (8) statistical distributions, (9) graphical user interfaces and (10) serialization to databases. Below follows a short discussion of some of these modules. At the core of BioJava is a symbolic alphabet API which represents sequences as a list of references to singleton symbol objects that are derived from an alphabet. Lists of symbols are stored whenever possible in a compressed form of up to four symbols per byte of memory. In addition to the fundamental symbols of a given alphabet (A, C, G and T in the case of DNA), all BioJava alphabets implicitly contain extra symbol objects representing all possible combinations of the fundamental symbols. The symbol approach allows the construction of higher order alphabets and symbols that represent the multiplication of one or more alphabets. An example is the codon ‘alphabet’ which is the cubed product of the DNA alphabet, each codon ‘symbol’ comprising three DNA symbols. Such an alphabet allows construction of views over sequences without modifying the underlying sequence which is useful for tasks such as translation. Other complex alphabets which can be described include conditional alphabets for the construction of conditional probability distributions, and heterogeneous alphabets such as the combination of the codon and protein alphabets for use with a DNA–protein aligning hidden Markov model (HMM). Other interesting applications of the alphabet API include chromosomes for genetic algorithms using, but not limited to, integer or binary symbol lists, and the representation of Phred quality scores (Ewing  et al. ,  1998 ) as a multiplication of the DNA and integer alphabets. The typical user would most likely start out by using the sequence input/output API and the sequence/feature object model. These allow sequences to be loaded from a number of common file formats such as FASTA, GenBank and EMBL, optionally manipulated in memory, then saved again or converted into a different format. The simplicity of this process is demonstrated in  Figure 1 .
 Fig. 1. Loading a GenBank file with BioJava and writing it out as FASTA. The example demonstrates the use of several convenience methods that hide the bulk of the implementation. If the developer desires a more flexible parser it is possible to make use of the interfaces hidden behind the convenience methods to expose a fully customizable, multi-component, event-based parsing model. Another useful API is the feature/annotation object model which associates sequences with located features and unlocated annotations. Features can be found either by keyword or by defining a location query from which all overlapping or contained features are returned, while annotations can be retrieved by keyword. The location model handles circular and stranded locations, split locations and multi-sequence locations allowing features to span complex sets of coordinates. The protein structure API contains tools for parsing and manipulating PDB files (Berman  et al. ,  2000 ). It contains utility methods to perform linear algebra calculations on atomic coordinates and can calculate 3D structure alignments. A simple interface to the 3D visualization library Jmol ( http://www.jmol.org ) is contained as well. An add-on allows the serialization of the content of a PDB file to a database using Hibernate ( http://www.hibernate.org ). Other APIs include those for working with chromatograms, sequence alignments, proteomics and ontologies. Parsers are provided for reading, amongst others, Blast reports (Altschul  et al. ,  1997 ), ABI chromatograms and NCBI taxonomy definitions. Recently the BioJavaX module was added which provides more detailed parsing of the common file formats and improved storing of sequence data into BioSQL databases ( http://www.biosql.org ). This allows to incorporate BioJava into existing data processing pipelines which use alternative OBF toolkits such as BioPerl (Stajich  et al. ,  2002 ). The BioJava web site provides detailed manuals on how to use the different components. In particular, the ‘CookBook’ section provides a quick introduction into solving many problems by demonstrating solutions with documented source code. There is also a section to demonstrate the performance of a few selected tasks via Java WebStart examples. To mention just one: the FASTA-formatted release 4 Drosophila genome sequence can be parsed in &lt;20 s on a 1.80 GHz Core Duo processor. 3 FUTURE DEVELOPMENT BioJava aims to provide an API that is of use to anyone using Java to develop bioinformatics software, regardless of which specialization they may work in. Genomic features currently must be manipulated with reference to the underlying genomic sequence, which can make working with post-genomic datasets, such as microarray results, overly complex. Phylogenetics tools are already in development which will allow users to work with NEXUS tree files (Maddison  et al. ,  1997 ). Although the Blast parsing API is widely used, it does not support all of the existing blast-family output formats. We will continue the ongoing effort to add parsers for PSI-Blast and other currently unsupported formats. Users are welcome to identify further areas of need and their suggestions will be incorporated into future developments. BioJava is written entirely in the Java programming language, and will run on any platform for which a Java 1.5 run-time environment is available. Java 5 and 6 provide advanced language features, and we shall be taking advantage of these in the next major release, both to aid in maintenance of the library and to make it even easier for novice Java developers to make use of the BioJava APIs. 4 CONCLUSIONS BioJava is one of the largest open-source APIs for bioinformatics software development. It is a mature project with a large user and support community. It offers a wide range of tools for common bioinformatics tasks. The BioJava homepage provides access to the source code and detailed documentation. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>baobabLUNA: the solution space of sorting by reversals</Title>
    <Doi>10.1093/bioinformatics/btp285</Doi>
    <Authors>Braga Marília D. V.</Authors>
    <Abstract>Summary: Computing the reversal distance and searching for an optimal sequence of reversals to transform a unichromosomal genome into another are useful algorithmic tools to analyse real evolutionary scenarios. Currently, these problems can be solved by at least two available softwares, the prominent of which are GRAPPA and GRIMM. However, the number of different optimal sequences is usually huge and taking only the distance and/or one example is often insufficient to do a proper analysis. Here, we offer an alternative and present baobabLUNA, a framework that contains an algorithm to give a compact representation of the whole space of solutions for the sorting by reversals problem.</Abstract>
    <Body>1 INTRODUCTION Computing the reversal distance between two unichromosomal genomes without duplications, insertions and deletions and finding one optimal sequence of reversals (that is, a sequence with a minimum number of reversals) that transforms one genome into the other can be solved in polynomial time, thanks to Hannenhalli and Pevzner ( 1999 ). These two problems have been the topic of several works, such as Tannier  et al.  ( 2007 ), and their solutions are valuable tools to analyse evolutionary scenarios. Currently, there are at least two available softwares to solve these problems. One is the package  GRAPPA  and the other is the software  GRIMM , described respectively, in Moret  et al.  ( 2001 ) and Tesler ( 2002 ). Nevertheless, there are many different solutions, with each solution representing an optimal sequence of reversals that sort one genome into another, and finding only one is often insufficient. Exploring the whole set of solutions is thus an interesting strategy to do a more realistic analysis. The first step in this direction was the enumeration of all solutions, thanks to an algorithm proposed by Siepel ( 2003 ). However, since the number of solutions is usually huge, the whole set is very hard to handle and this could be as useless as finding one of them. Bergeron  et al.  ( 2002 ) then proposed a model to represent the solutions in a compact way, grouping them into classes of equivalence. This allows to reduce the set to be handled and an algorithm to directly enumerate the classes was given by Braga  et al.  ( 2008 ). The number of non-equivalent solutions can be still too large, therefore, a method was proposed for filtering solutions using constraints (Braga,  2009 ). In this work, we describe  baobabLUNA , a framework that contains the implementation of the algorithm developed by Braga  et al.  ( 2008 ) to directly enumerate all the classes of equivalent solutions and also the further use of biological constraints to filter the classes. 2 DESCRIPTION 2.1 Permutations, reversals and sorting sequences Genomes are represented by the list of homologous markers between them. These markers correspond to the integers 1, 2,…,  n , with a plus or minus sign to indicate the strand they lie on. The order and orientation of the markers of one genome in relation to the other is represented by a  signed permutation  π = (π 1 , π 2 ,…, π n −1 , π n ) of size  n  over {− n ,…, −1, 1,…,  n }, such that, for each value  i  from 1 to  n , either  i  or − i  is mandatorily represented, but not both. The  identity permutation  (1, 2, 3,…,  n ) is denoted by I n . A subset of numbers ρ⊆{1, 2,…,  n −1,  n } is said to be an  interval  of a permutation π if there exist  i , j ∈{1,…,  n }, 1≤ i ≤ j ≤ n , such that ρ={|π i |, |π i +1 |,…, |π j −1 |, |π j |}. Given a permutation π and an interval ρ of π, we can apply a  reversal  on the interval ρ of π, that is, the operation which reverses the order and flips the signs of the elements of ρ, that results in the permutation (π 1 ,…, π i −1 , −π j ,…, −π i , π j +1 ,…, π n ). If  s =ρ 1 ρ 2 …ρ i  is a  sequence of reversals  for a permutation π, we say that  s sorts  π into π T  if the result of the consecutive application of the reversals ρ 1 , ρ 2 , …ρ i  on π is π T . The length of a shortest sequence sorting π into π T  is called the  reversal distance  of π and π T , denoted by  d (π, π T ). Let  s =ρ 1 ρ 2 …ρ i  be a sequence of reversals sorting π into π T . If  d (π, π T )= i , then  s  is said to be an  optimal sorting sequence . As an example, the sequence {1}{2}{4}{1, 2, 3} sorts (−3, 2, 1, −4) into I 4  and is optimal. 2.2 Main functionalities 2.2.1 Computing traces Given two permutations π and π T , the enumeration of all solutions (sequences) that sort π into π T  can be done by iterating an algorithm given by Siepel ( 2003 ). However, the number of solutions is huge and the complexity of enumerating all of them is  O ( n 2 n +3 ) (Braga,  2009 ). Bergeron  et al.  ( 2002 ) introduced a more compact representation of the space of solutions, grouping them into equivalence classes called  traces . All equivalent solutions in a trace are composed by the same reversals but in different orders. Observe however that this is not the formal definition of a trace, which can be obtained in Braga ( 2009 ). Braga  et al.  ( 2008 ) later proposed an algorithm to directly give one representative solution and the number of solutions in each trace. The complexity of this algorithm is also exponential in a property of the traces called  width  (Braga,  2009 ), but, as the number of traces is usually much smaller than the number of solutions, enumerating traces runs considerably faster. The framework  baobabLUNA  contains the implementation of the algorithm developed by Braga  et al.  ( 2008 ). As a simple example of the gain represented by this algorithm with respect to the enumeration of all solutions, the 28 solutions that sort (−3, 2, 1, −4) into I 4 , can be grouped in only two traces, one is represented by {1}{1, 2, 3}{2}{4} and has 24 solutions, while the other is {1, 2, 4}{3}{1, 3, 4}{2, 3, 4} and has 4 solutions. More details on how the algorithm generates directly the traces and also counts the number of solutions in each trace can be obtained in Braga ( 2009 ). 2.2.2 Filtering traces with constraints Biological constraints can be used to filter the traces of optimal sequences, as described in Braga ( 2009 ). Besides the two signed permutations π and π T , this approach requires a list  C  of compatible constraints for selecting the sequences that sort π into π T  and respect the given constraints. Frequently, only a subset of the sorting sequences of a trace is in agreement with the constraints in  C , and this subset is called  C -induced subtrace. The result of applying this method is the complete set of non-empty  C -induced subtraces of sequences sorting π into π T . Generally, we have no guarantee that a sorting sequence that respects all constraints exists, thus this approach can lead to an empty result. One of the considered constraints is the list of common intervals detected between the two initial permutations, that may correspond to the clusters of co-localized genes between the considered genomes—an optimal sequence of reversals that does not break the common intervals may be more realistic than one that does break. This approach was previously used in several studies [see for instance, Diekmann  et al.  ( 2007 )]. We used the common intervals initially detected and also a variation of this approach, described in Braga ( 2009 ), that is the list of common intervals progressively detected when sorting one permutation into another by reversals. Another constraint implemented in  baobabLUNA  is called  strata  and is specific to the evolution of sexual X and Y chromosomes in mammals and some other organisms. Although X and Y are usually very different, they still share an identical region (called ‘pseudo-autosomal’ region) at one of their extremities and are believed to have evolved from an identical pair of chromosomes. This process is at the origin of sexual differentiation: the female XX and the male XY pairs. Current theories suggest that the pseudo-autosomal region, which originally covered the whole chromosomes, was successively pruned by a few big reversals on the Y chromosome (Lahn and Page,  1999 ). The successive limits of the pseudo-autosomal region on the X chromosome represent the limits of what have been called the ‘evolutionary strata’ of X chromosome and a sequence of reversals that could have created the strata on human X chromosome is given by Ross  et al.  ( 2005 ). The use of the strata as a constraint to filter the space of solutions of the sorting by reversals problem is described in Braga  et al.  ( 2008 ) and is used by Lemaitre  et al.  ( 2009 ) to evaluate the scenario of reversals given by Ross  et al.  ( 2005 ). 2.3 Experiments In order to evaluate the performance of the algorithm that computes directly the traces, named  traces , we used the algorithm  enumSol  that enumerates all solutions. We also tested the filters  perfTrcs , that selects traces whose solutions do not break common intervals initially detected,  prgSubt , which selects subtraces whose solutions do not break common intervals progressively detected and  strSubt  that selects subtraces whose solutions produce a given strata in the origin permutation. The analysed permutations are π A =(−12,11,−10,6,13,−5,2,7,8,−9,3,4,1) and π B =(−12,11,−10,−1,16,−4,−3,15,−14,9,−8,−7,−2,−13,5,−6) (both fictitious),  Rfe =(1,3,−2,−11,5,−9,−10,8,6,−7,−4,12) and  R 2=I 12  [the bacterium  Rickettsia felis  and its ancestor  R 2, reconstructed in Blanc  et al.  ( 2007 )],  X =I 12  and  Y =(−12,11,−2,−1,−10,−9,8,−5,7,6,−4,3), [human X and Y chromosomes, as the scenario proposed in Ross  et al.  ( 2005 )]. The results are in  Table 1  and show that computing traces directly indeed runs much faster than computing solutions. Moreover, the variants that take constraints in consideration usually run faster than computing all traces. Additional analyses and experimental results can be found in Braga ( 2009 ).
 Table 1. Computation results for each pair of permutations (the number of elements and reversal distance of each pair is given in the first column). PERMUT. Algorithm N S N T Execution time π A , I 12 enumSol 8 278 540 − ≃  13.5 min n =12,  d =10 traces 8 278 540 2151 ≃  27 sec perfTrcs 1 698 480 12 ≃  4 sec prgSubt 453 600 3 ≃  2 sec π B , I 16 enumSol 505 634 256 − ≃  16 h n =16,  d =12 traces 505 634 256 21 902 ≃  7.3 min perfTrcs 122 862 960 171 ≃  27 sec prgSubt 5 963 760 6 ≃  14 sec Rfe ,  R 2 enumSol 546 840 − ≃  42 sec n =12,  d =9 traces 546 840 13 ≃  3 sec prgSubt 263 088 6 ≃  2 sec X ,  Y enumSol 31 752 - ≃  5 sec n =12,  d =8 traces 31 752 6 ≃  1.3 sec strSubt 420 1 ≃  0.5 sec The columns  N S  and  N T  give, respectively, the number of sorting sequences and traces computed by each algorithm. Experiments were made on a 64 bit personal computer with two 3 GHz CPUs and 2 GB of RAM. 
 2.4 Download, setup and tutorial Download and setup instructions, interface description and tutorial for computing traces (including the versions that take constraints in consideration) are available in  http://pbil.univ-lyon1.fr/software/luna . 3 FINAL REMARKS The framework  baobabLUNA  contains the implementation of a method proposed by Braga  et al.  ( 2008 ), that gives a compact representation of the solution space of the sorting by reversals problem, grouping solutions into traces. This is an interesting alternative to most of the previous methods that give either only one or all solutions, and are provided by tools such as  GRIMM  (Tesler,  2002 ) and  GRAPPA  (Moret  et al. ,  2001 ). However, although the number of traces is much smaller than the number of solutions, it may be still too big to be interpreted, and in some cases, too big to be computed. Indeed, currently we are unable to compute traces for permutations with a reversal distance of about 20 or higher. Different biological constraints can be used to filter the traces and reduce the universe to be handled. Nevertheless, there is no guarantee that a solution that respects the given constraints exists, thus this approach may lead to empty results. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>ncIDP-assign: a SPARKY extension for the effective NMR assignment of intrinsically disordered proteins</Title>
    <Doi>10.1093/bioinformatics/btr054</Doi>
    <Authors>Tamiola Kamil, Mulder Frans A. A.</Authors>
    <Abstract>Summary: We describe here the ncIDP-assign extension for the popular NMR assignment program SPARKY, which aids in the sequence-specific resonance assignment of intrinsically disordered proteins (IDPs). The assignment plugin greatly facilitates the effective matching of a set of connected resonances to the correct position in the sequence by making use of IDP random coil chemical shifts.</Abstract>
    <Body>1 INTRODUCTION The structural characterization of intrinsically disordered proteins (IDPs) is a rapidly growing field in structural molecular biology. Over the past decade NMR spectroscopy has proven to be singular in its capacity to provide detailed structural characterization of these dynamic entities ( Eliezer, 2009 ). Although diverse experimental approaches have been developed ( Bermel  et al. , 2006 ;  Mäntylahti  et al. , 2010 ;  O'Hare  et al. , 2009 ), the sequential resonance assignment of &gt; 10 kDa natively unfolded polypeptides is still not a trivial task. Here, we describe an enhanced version of the SPARKY ( Goddard and Kneller, 2006 ) sequence repositioning extension, which assist in matching, connecting and assigning consecutive residues, and is specifically designed for intrinsically disordered proteins:  ncIDP-assign  (neighbor-corrected IDP chemical shift assignment). This tool makes use of a novel random-coil chemical shift library, enabling the accurate prediction of the chemical shifts of a queried protein on a basis of tripeptides ( Tamiola  et al. , 2010 ). Predicted sequence-specific chemical shifts are used as a template for re-assignment and validation of existing resonance assignments. The newly designed  ncIDP-assign  greatly accelerates the process of sequential resonance assignment of large intrinsically disordered proteins by drastically reducing the level of assignment ambiguities. 2 IMPLEMENTATION The process of assignment validation and repositioning begins with the computation of the sequence-specific random-coil chemical shifts for the intrinsically disordered protein under study, based on its primary sequence.The random-coil chemical shift for a nucleus  n  ∈ { 1 H α ,  1 H N ,  13 C α ,  13 C β ,  13 C O ,  15 N} of amino acid residue  a , within a tripeptide  x  −  a  −  y , can be expressed as,
 (1) where δ rc n ( a ) is the ‘random−coil’ chemical shift in the reference peptide  Gly  −  a  −  Gly  , Δ −1 n ( x ) and Δ +1 n ( y ), are the neighbor corrections due to the preceding and the sequential residue, respectively. Subsequently, a collection of experimentally assigned resonances in a range  k  …  k  +  l  specified by the user is retrieved and compared against the predicted chemical shifts from Equation ( 1 ). All plausible locations for the fragment along the sequence of a protein are considered. Each investigated position is characterized with a chemical shift deviation score  S ,
 (2) where  , δ exp n  and σ pred n  are experimental chemical shifts and the standard deviations of expected chemical shieldings for  k  …  k  +  l  sequence combination, respectively. A score of 1.0 implies that at the queried sequence position, the experimental shifts are one standard deviation from the computed values, on average. Hence, the ‘best-fit’ solutions are characterized by the lowest  S  values. 3 RESULTS AND CONCLUSION To assess the performance of  ncIDP-assign  we used the NMR chemical shift assignments for the 140-residue intrinsically disordered, cytoplasmic domain of human neuroligin-3 (hNLG3cyt) ( Paz  et al. , 2008 ). The robustness of the assignment procedure was established using the data obtained in  1 H- 15 N HSQC ( Bodenhausen and Ruben, 1980 ), COCAHA ( Dijkstra  et al. , 1994 ) and HNCACB ( Wittekind and Mueller, 1993 ) experiments, providing access to backbone and  13 C β  chemical shifts.  Table 1  displays the level of completeness of the resonance assignment for hNLG3cyt, and the accuracy of the chemical shift back computation modules in the standard and ncIDP versions of the SPARKY sequence repositioning plugins. As borne out by  Table 1 ,  ncIDP-assign  offers an almost two-fold improvement in the estimation of chemical shifts for hNLG3cyt. The superior predictive power of  ncIDP-assign  translates into detection sensitivity of chemical shift deviations from the sequence-specific ‘random-coil’ values due to resonance misassignment. This point is demonstrated by  Fig. 1  where the repositioning performance of the two methods is compared against known resonance assignments for hNLG3cyt.  ncIDP-assign  identifies correct solutions much more readily, and already at the level of dipeptides for the considered experiments ( Fig. 1 A). Further expansion of the length of a query fragment to tripeptide rapidly shifts the probability of assigning the correct solution. Consequently, the information content contained within a combination of resonance frequencies in short peptides (length ≥2) is unique enough to make the correct position guess in most cases.
 Fig. 1. Comparative analysis of accuracy of standard and ncIDP SPARKY sequence repositioning extensions, using resonance assignments in:  1 H- 15 N HSQC, COCAHA and HNCACB experiments, for the intrinsically disordered protein hNLG3cyt. ( A ) Normalized frequency of correct repositioning solutions as a function of fragment length in: standard ( blue ) and ncIDP-enhanced ( green ) repositioning, respectively. ( B ) Sensitivity as a function of sequence length for: standard ( blue ) and ncIDP ( green ) repositioning extensions. 
 Table 1. Comparative analysis of chemical shift back-computation for hNLG3cyt using standard chemical shift computation module available in SPARKY, and  ncIDP-assign Nucleus (n) Standard a ncIDP a Assignments 1 H α 0.122 0.044 120 1 H N 0.148 0.111 128 13 C α 0.810 0.324 128 13 C β 0.410 0.212 120 13 C O 0.717 0.393 131 15 N 1.314 0.664 129 a Chemical shift RMSD computed after removal of mean systematic offsets between the computed and experimental resonance assignments for hNLG3cyt in order to minimize chemical shift referencing errors. Root mean square difference (RMSD) values are given in ppm. 
 Another critical parameter in the assignment process is the relative separation of the ‘best-fit’ score  S best  with respect to the second-best scoring suggestion  S second-best , expressed here as the sensitivity  S second-best / S best  ( Fig. 1 B). Values for the sensitivity close to 1.0 indicate ambiguity. Given a sequentially assigned dipeptide,  ncIDP-assign  generates a list of solutions in which the ‘best-fit’ scenario scores appreciably better than the next considered option. Already significant improvements are observed in the analysis of  1 H- 15 N HSQC resonance lists, clearly demonstrating that information content of  1 H- 15 N resonance pairs in sequentially connected dipeptides can be effectively used in the assignment process of an intrinsically disordered protein. In conclusion, we have shown here that  ncIDP-assign  is an effective tool to aid the sequential NMR resonance assignment of (intrinsically) disordered proteins. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>RVD2: an ultra-sensitive variant detection model for low-depth heterogeneous next-generation sequencing data</Title>
    <Doi>10.1093/bioinformatics/btv275</Doi>
    <Authors>He Yuting, Zhang Fan, Flaherty Patrick</Authors>
    <Abstract>Motivation: Next-generation sequencing technology is increasingly being used for clinical diagnostic tests. Clinical samples are often genomically heterogeneous due to low sample purity or the presence of genetic subpopulations. Therefore, a variant calling algorithm for calling low-frequency polymorphisms in heterogeneous samples is needed.</Abstract>
    <Body>1 Introduction Next-generation sequencing (NGS) technology has enabled the systematic interrogation of the genome for a fraction of the cost of traditional assays ( Koboldt  et 
 al. , 2013 ). Protocol and platform engineering improvements have enabled the generation of  1 × 10 9  bases of sequence data in 27 h for ∼$1000 ( Quail  et 
 al. , 2012 ). As a result, NGS is increasingly being used as a general platform for research assays for methylation state ( Laird, 2010 ), DNA mutations ( 1000 Genomes Project Consortium  et 
 al. , 2012 ), copy number variation ( Alkan  et 
 al. , 2009 ), promoter occupancy ( Ouyang  et 
 al. , 2009 ) and others ( Rivera and Ren, 2013 ). NGS diagnostics are being translated to clinical applications including non-invasive fetal diagnostics ( Kitzman  et 
 al. , 2012 ), infectious disease diagnostics ( Capobianchi  et 
 al. , 2012 ), cancer diagnostics ( Navin  et 
 al. , 2010 ), and human microbiome analysis ( The Human Microbiome Project Consortium, 2013 ). Increasingly, NGS is being used to interrogate mutations in heterogeneous clinical samples. For example, NGS-based non-invasive fetal DNA testing uses maternal blood sample to sequence the minority fraction of cell-free fetal DNA ( Fan  et 
 al. , 2008 ). Infectious diseases such as HIV and influenza may contain many genetically heterogeneous sub-populations ( Flaherty  et 
 al. , 2011 ;  Ghedin  et 
 al. , 2010 ). DNA sequencing of individual regions of a solid tumor has revealed genetic heterogeneous within an individual sample ( Navin  et 
 al. , 2010 ). Importantly, accounting for technical errors can drastically improve performance ( Zagordi  et 
 al. , 2010 ). However, the primary statistical tools for calling variants from NGS data are optimized for homogeneous samples. Samtools and GATK use a naive Bayesian decision rule to call variants ( DePristo  et 
 al. , 2011 ;  Li, 2011 ). GATK involves more sophisticate pre- and post-processing steps wherein the genotype prior is fixed and constant across all loci and the likelihood of an allele at a locus is a function of the Phred score ( McKenna  et 
 al. , 2010 ). Recently, some have developed algorithms to call low-frequency or rare variants in heterogeneous samples.  Yau  et 
 al.  (2010)  developed a Bayesian framework which can model the normal DNA contamination and intra-tumor heterogeneity by parameterizing the normal genotype cell proportion at each SNP. VarScan2 combines algorithmic heuristics to call genotypes in the tumor and normal sample pileup data and then applies a Fisher’s exact test on the read count data to detect a significant difference in the genotype calls ( Koboldt  et 
 al. , 2012 ). Strelka uses a hierarchical Bayesian approach to model the joint distribution of the allele frequency in the tumor and normal samples at each locus ( Saunders  et 
 al. , 2012 ). With the joint distribution available, one is able to identify locations with dissimilar allele frequencies. muTect uses a Bayesian posterior probability in its decision rule to evaluate the likelihood of a mutation ( Cibulskis  et 
 al. , 2013 ). RVD uses a hierarchical Bayesian model to capture the error structure of the data and call variants ( Cushing  et al. , 2013 ;  Flaherty  et 
 al. , 2011 ). That algorithm requires a very high read depth to estimate the sequencing error rate and call variants. Several studies have compared the relative performance of these algorithms.  Spencer  et 
 al.  (2013)  demonstrated that VarScan-somatic performed the best when comparing SAMtools, GATK and SPLINTER for detecting minor allele fractions (MAFs) of 1–8%, with &gt;500 coverage required for optimal performance. However,  Spencer  et 
 al.  (2013)  also highlighted the fact that VarScan2 yielded more false positives at high read depth.  Stead  et 
 al.  (2013)  showed that VarScan-somatic outperformed Strelka and had performance on-par with muTect in detecting a 5% MAF for read depths between 100 and 1000. The remainder of this article is organized as follows. In the next section we describe the statistical model structure of our new algorithm, RVD2. Then, we derive a sampling algorithm for computing the posterior distribution over latent variables in the model and use those samples in a Bayesian posterior distribution hypothesis test to call variants. We compare the performance of RVD2 to several other variant calling algorithms for a range of read depths and minor allele fractions. Finally, we show that RVD2 is able to call variants on a heterogeneous clinical sample and identify two novel loss-of-heterozygosity events. 2 Model Structure RVD2 uses a two-stage approach for detecting rare variants. First, it estimates the parameters of a hierarchical Bayesian model under two sequencing datasets: one from the sample of interest (case) and one from a known reference sample (control). Then, it tests for a significant difference between key model parameters in the case and control samples and returns called variant positions. Figure 1  shows a graphical representation of the RVD2 statistical model. In this graphical model framework, a shaded node represents an observed random variable, an unshaded node represents an unobserved or latent random variable and a directed edge represents a functional dependency between the two connected nodes ( Jordan, 2004 ). A rounded box or ‘plate’ represents replication of the nodes within the plate. The graphical model framework connects graph theory and probability theory in a way that facilitates algorithmic methods for statistical inference.
 Fig. 1. RVD2 graphical model 
 For a given sample, the observed data consist of two matrices  r ∈ R J × N  and  n ∈ R J × N , where  r ji  is the number of reads with a non-reference base at location  j  in experimental replicate  i  and  n ji  is the total number of reads at location  j  in replicate  i. 
 J  is the region of interest length and  N  is the number of technical replicates in the sample. Technical replicates are used to establish experimental variability in NGS procedure ( Robasky  et 
 al. , 2013 ), though multiple replicates are not necessary for RVD2. The model generative process given hyperparameters  μ 0 , M 0  and  M  is as follows:
 For each location  j :
 Draw an error rate  μ j ∼ Beta ( μ 0 , M 0 ) For each replicate  i :
 Draw  θ j i ∼ Beta ( μ j , M j ) Draw  r j i | n j i ∼ Binomial ( θ j i , n j i ) The generative process involves several hyperparameters:  μ 0 , a global error rate;  M 0 , a global precision;  μ j , a local error rate; and  M j , a local precision. The global error rate,  μ 0 , estimates the expected error rate across all locations. The global precision,  M 0 , estimates the variation in the error rate across locations. The local error rate,  μ j , estimates the expected error rate across replicates at location  j.  The local precision,  M j , estimates the variation in the error rate across replicates at location  j. RVD2 has three levels of sampling. First, a global error rate and global precision are chosen once for the entire dataset. Then, at each location, a local precision is chosen and a local error rate is sampled from a Beta distribution. Finally, the error rate for replicate  i  at location  j  is drawn from a Beta distribution and the number of non-reference reads is drawn from a binomial. RVD2 hierarchically partitions sources of variation in the data. The distribution  r j i | n j i ∼ Binomial ( θ j i , n j i )  models the variation due to sampling the pool of DNA molecules on the sequencer. The distribution  θ j i ∼ Beta ( μ j , M j )  models the variation due to experimental reproducibility. The variation in error rate due to sequence context is modeled by  μ j ∼ Beta ( μ 0 , M 0 ) . Importantly, increasing the read depth  n ji  only reduces the sampling error, but does nothing to reduce experimental variation or variation due to sequence context. The joint distribution over the latent and observed variables for data at location  j  in replicate  i  given the parameters can be factorized as
 (1) p ( r j i , θ j i , μ j | n j i ; μ 0 , M 0 , M j ) = p ( r j i | θ j i , n j i ) p ( θ j i | μ j ; M j ) p ( μ j ; μ 0 , M 0 ) , p ( r j i | θ j i , n j i ) = Γ ( n j i + 1 ) Γ ( r j i + 1 ) Γ ( n j i − r j i + 1 ) · θ j i r j i ( 1 − θ j i ) n j i − r j i , p ( θ j i | μ j ; M j ) = θ j i M j μ j − 1 ( 1 − θ j i ) M j ( 1 − μ j ) − 1 B ( M j μ j , M j ( 1 − μ j ) ) , p ( μ j ; μ 0 , M 0 ) = μ j M 0 μ 0 − 1 ( 1 − μ j ) M 0 ( 1 − μ 0 ) − 1 B ( M 0 μ 0 , M 0 ( 1 − μ 0 ) ) , 
where  B ( · , · )  denotes the beta function. The log-likelihood of the dataset is
 (2) log ⁡ p ( r | n ; μ 0 , M 0 , M ) = ∑ j = 1 J ∑ i = 1 N log ⁡ ∫ μ j ∫ θ j i p ( r j i | θ j i , n j i ) p ( θ j i | μ j ; M j ) p ( μ j ; μ 0 , M 0 ) d θ j i d μ j . 
RVD2 improves on RVD in three ways. First, RVD2 has a  Beta ( μ 0 , M 0 )  prior on local error rate  μ j , which captures the global across-position error rate. The prior distribution allows  μ j  to share information across adjacent positions and allows RVD2 to handle low read depths. Second, RVD2 handles multiple replicates in case samples. Third, RVD2 has a more accurate Bayesian hypothesis testing method compared with the normal  z -test in RVD. We show a performance comparison between RVD and RVD2 in Section 5.2. 3 Inference and Hypothesis Testing The primary object of inference in this model is the joint posterior distribution function over the latent variables,
 (3) p ( μ , θ | r , n ; ϕ ) = p ( μ , θ , r | n ; ϕ ) p ( r | n ; ϕ ) , 
where the parameters are  ϕ = Δ { μ 0 , M 0 , M } . The Beta distribution over  μ j  is conjugate to the Binomial distribution over  θ ji , so we can write the posterior distribution as a Beta distribution. However, there is not a closed form for the product of a Beta distribution with another Beta distribution, so exact inference is intractable. Instead, we have developed a Metropolis-within-Gibbs (MwG) approximate inference algorithm shown in Algorithm 1. First, the hyperparameters are initialized using method-of-moments (MoM). Given those hyperparameter estimates, we sample from the marginal posterior distribution for  μ j  given its Markov blanket using a Metropolis–Hasting (M–H) rejection sampling rule. Finally, we sample from the marginal posterior distribution for  θ ji  given its Markov blanket. Samples from  θ ji  can be drawn from the posterior distribution directly because the prior and likelihood form a conjugate pair. This sampling procedure is repeated until the chain converges to a stationary distribution and then we draw samples from the posterior distribution over latent variables. Algorithm 1  Metropolis-within-Gibbs Algorithm 1: Initialize  θ ,  μ ,  M ,  μ 0 ,  M 0 2:  repeat 3:  for  each location j  do 4: Draw T samples from  p ( μ j | θ i j , μ 0 , M 0 )  using M–H 5: Set  μ j  to the sample median for the T samples 6:   for  each replicate i  do 7:   Sample from  p ( θ i j | r i j , n i j , μ j , M ) 8:   end 
 for 9:  end 
 for 10:  until  sample size sufficient 3.1 Initialization The initial values for the model parameters and latent variables are obtained by a MoM procedure. MoM works by setting the population moment equal to the sample moment. A system of equations is formed such that the number of moment equations is equal to the number of unknown parameters and the equations are solved simultaneously to give the parameter estimates. We simply start with the data matrices  r  and  n  and work up the hierarchy of the graphical model solving for the parameters of each conditional distribution in turn. We present the initial parameter estimates here and provide the derivations in  Supplementary Information . The MoM estimate for replicate-level parameters are  θ ^ j i = r j i n j i . The estimates for the local parameters are  μ ^ j = 1 N ∑ i = 1 N θ ^ j i  and  M ^ j ^ = μ ^ j ( 1 − μ ^ j ) 1 N ∑ i = 1 N θ ^ j i 2 − 1 . The estimates for the global parameters are  μ ^ 0 = 1 J ∑ j = 1 J μ ^ j  and  M ^ 0 = μ ^ 0 ( 1 − μ ^ 0 ) 1 J ∑ j = 1 J μ ^ j 2 − 1 . 3.2 Sampling from  p ( θ i j | r i j , n i j , μ j , M ) Samples from the posterior distribution  p ( θ j i | r j i , n j i , μ j , M j )  are drawn analytically because of the Bayesian conjugacy between the prior  p ( θ j i | μ j , M j ) ∼ Beta ( μ j , M j )  and the likelihood  p ( r j i | n j i , θ j i ) ∼ Binomial ( θ j i , n j i ) . The posterior distribution is
 (4) p ( θ j i | r j i , n j i , μ j , M j ) ∼ Beta ( r j i + M j μ j , n j i − r j i + M j ( 1 − μ j ) ) . 
 3.3 Sampling from  p ( μ j | θ j i , M j , μ 0 , M 0 ) The posterior distribution over  μ j  given its Markov blanket is
 (5) p ( μ j | θ j i , M j , μ 0 , M 0 ) ∝ p ( μ j | μ 0 , M 0 ) p ( θ j i | μ j , M j ) 
Since the prior,  p ( μ j | μ 0 , M 0 ) , is not conjugate to the likelihood,  p ( θ j i | μ j , M j ) , we cannot write an analytical form for the posterior distribution. Instead, we sample from the posterior distribution using the M–H algorithm. A candidate sample is generated from the symmetric proposal distribution  Q ( μ j * | μ j ( p ) ) ∼ N ( μ j ( p ) , σ j ( p ) ) , where  μ j ( p )  is the  p th from the posterior distribution. The acceptance probability is then
 (6) a = p ( μ j * | μ 0 , M 0 ) p ( θ j i ( p + 1 ) | μ j * , M j ) p ( μ j ( p ) | μ 0 , M 0 ) p ( θ j i ( p + 1 ) | μ j ( p ) , M j ) . 
We fixed the proposal distribution variance for all the M–H steps within a Gibbs iteration to  σ j = 0.1 · μ ^ j · ( 1 − μ ^ j )  if  μ ^ j ∈ ( 10 − 3 , 1 − 10 − 3 )  and  σ j = 10 − 4  otherwise, where  μ ^ j  is the MoM estimator of  μ j .  Though it is not theoretically necessary, we have found that the algorithm performance improves when we take the median of five or more M–H samples in single Gibbs step for each position. We resample from the proposal if the sample is outside of the support of the posterior distribution. We typically discard 20% of the sample for burn-in and thin the chain by a factor of 2 to reduce autocorrelation among samples. Since, each position  j  is exchangeable given the global hyperparameters,  μ 0  and  M 0 , this sampling step can be distributed across up to  J  processors. 3.4 Posterior distribution test 3.4.1 Posterior difference test MwG provides samples from the posterior distribution of  μ j  given the case or control data. For notational simplicity, we define the random variables associated with these two distributions  μ j case  and  μ j control  and the associated samples as  μ ˜ j case  and  μ ˜ j control . A variant is called if  μ j case &gt; μ j control  with high confidence,
 (7) Pr ⁡ ( μ j case − μ j control &gt; τ ) ≈ 1 N MwG ∑ k = 1 N MwG   1 μ ˜ j k case − μ ˜ j k control &gt; τ &gt; 1 − α , 
where  τ  is a detection threshold and  1 − α  is a confidence level. We draw a sample from the posterior distribution  μ ˜ j Δ = Δ μ ˜ j case − μ ˜ j control  by simple random sampling with replacement from  μ ˜ j case  and  μ ˜ j control . The threshold,  τ , may be set to 0 or optimized for a given median depth and desired MAF detection limit. The optimal  τ  maximizes the Matthews Correlation Coefficient (MCC),
 (8) τ * = arg ⁡ max ⁡ τ { MCC ( τ ) } . 
While we are able to compute the optimal  τ  threshold for a test dataset, in general we would not have access to  τ * . With sufficient training data, one would be able to develop a lookup table or calibration curve to set  τ  based on read depth and MAF level of interest. Absent this information we set  τ  = 0. 3.4.2 Posterior somatic test We use a two-sided posterior difference test with control and case paired samples to identify somatic mutations. We consider scenarios when the case(tumor) error rate is lower than the control(germline) error rate (e.g. loss-of-heterozygosity) as well as scenarios when the case(tumor) error rate is higher than the control(germline) error rate (e.g. homozygous somatic mutation). The two hypothesis tests are then  Pr ⁡ ( μ j case − μ j control &gt; τ ) &gt; 1 − α  and  Pr ⁡ ( μ j case − μ j control &lt; − τ ) &gt; 1 − α . We typically set the threshold  τ  to 0. 3.4.3 Posterior germline test We use a one-sided posterior distribution test with a single control sample to identify germline mutations. We call a germline mutation if  μ j control ≥ τ  with high confidence,
 (9) Pr ⁡ ( μ j control ≥ τ ) ≈ 1 N MwG ∑ k = 1 N MwG   1 μ ˜ j k control ≥ τ &gt; 1 − α . 
 3.5  χ 2  test for non-uniform base distribution An abundance of non-reference bases at a position called by the posterior density test may be due to a true mutation or due to a random sequencing error; we would like to differentiate these two scenarios. We assume non-reference read counts caused by a non-biological mechanism results in a uniform distribution over three non-reference bases. In contrast, the distribution of counts among three non-reference bases caused by biological mutation would not be uniform. We use a  χ 2  goodness-of-fit test on a multinomial distribution over the non-reference bases to distinguish these two possible scenarios. The null hypothesis is  H 0 : p = ( p 1 , p 2 , p 3 )  where  p 1 = p 2 = p 3 = 1 / 3 .  Cressie and Read (1984)  identified a power-divergence family of statistics, indexed by  λ , that includes as special cases Pearson’s  χ 2 ( λ = 1 )  statistic, the log likelihood ratio statistic  ( λ = 0 ) , the Freeman–Tukey statistic  ( λ = − 1 / 2 ) , and the Neyman modified statistic  X 2 ( λ = − 2 ) . The test statistic is
 (10) 2 n I λ = 2 λ ( λ + 1 ) ∑ k = 1 3 r j i ( k ) [ ( r j i ( k ) E j i ( k ) ) λ − 1 ] ; λ ∈ R , 
where  r j i ( k )  is the observed frequency for non-reference base  k  at position  j  in replicate  i  and  E j i ( k )  is the corresponding expected frequency under the null hypothesis.  Cressie and Read (1984)  recommended  λ = 2 / 3  when no knowledge of the alternative distribution is available and we choose that value. We control for multiple hypothesis testing in two ways. We use Fisher’s combined probability test ( Fisher  et 
 al. , 1970 ) to combine the  P -values for  N  replicates into a single  P -value at position  j ,
 (11) χ j 2 = − 2 ∑ i = 1 N ln ⁡ ( p j i ) . 
 Equation (11)  gives a test statistic that follows a  χ 2  distribution with 2 N  degrees of freedom when the null hypothesis is true. If the sample average depth is higher than 500, we use the Benjamini–Hochberg method to control the family-wise error rate over positions that have been called by the posterior distribution test ( Benjamini and Hochberg, 1995 ;  Efron, 2010 ). The average depth threshold is set because Benjamini–Hochberg method is a highly conservative method and will reject many true calls when the read depth is not high enough. 4 Datasets We used two independent datasets to evaluate the performance of RVD2 and compare it with other variant calling algorithms. The synthetic DNA sequence data provide true positive and true negative positions as well as define minor allele fractions. The HCC1187 data is used to test the performance on a sequenced cancer genome with less than 100% tumor purity. 4.1 Synthetic DNA sequence data 4.1.1 Experimental methods Two 400 bp DNA sequences (including linkers) that are identical except at 14 loci with variant bases were synthesized and clonally isolated. The samples with the mutations are taken as the case sample and the sample without the mutations is taken as the control. Aliquots of the case and control DNA were mixed at defined fractions to yield defined minor allele fractions (MAFs) of 0.1, 0.3, 1, 10 and 100%. Paired-end sequencing was performed on an Illumina GAIIx sequencer (Illumina SCS 2.8) with real-time image analysis and base calling (Illumina RTA 2.8). Eland II (from Illumina pipeline version 1.6) was used with the default parameters to perform sequence alignment to the 300-bp synthetic DNA construct. More details of the experimental protocol are available from the original publication ( Flaherty  et 
 al. , 2011 ). As shown in  Supplementary Table S1 , each sample has ∼1 000 000 35 bp paired end reads. 4.1.2 Pre-processing methods The reads were aligned with Eland as described previously. We then ran samtools mpileup with the -C50 option to filter for high mapping quality reads. To simulate lower coverage data while retaining the error structure of real NGS data, BAM files for the synthetic DNA data were downsampled  10 × ,   100 × ,   1000 ×  and  10   000 ×  using Picard v1.96. The final dataset contains read pairs for three replicates of each case and pairs of reads three replicates for the control sample giving  N  = 6 replicates for the control and each MAF level. 4.2 HCC1187 sequence data 4.2.1 Experimental methods The HCC1187 dataset is a well-recognized baseline dataset from Illumina for evaluating sequence analysis algorithms ( Howarth  et 
 al. , 2011 ,  2007 ;  Newman  et al. , 2013 ). The HCC1187 cell line was derived from epithelial cells from primary breast tissue from a 41–year-old adult with TNM stage IIA primary ductal carcinoma. The estimated tumor purity was reported to be 0.8. Matched normal cells were derived from lymphoblastoid cells from peripheral blood. Sequencing libraries were prepared according to the protocol described in the original technical report ( Allen, 2013 ). 4.2.2 Pre-processing methods The raw FASTQ read files were aligned to hg19 using the Isaac aligner to generate BAM files ( Raczy  et 
 al. , 2013 ). The aligned data had an average read depth of 40× for the normal sample and 90× for the tumor sample with about 96% coverage with 10 or more reads. We used samtools mpileup to generate pileup files using hg19 as reference sequence ( Navin  et 
 al. , 2010 ). 5 Results We tested RVD2 using synthetic DNA and data from the HCC1187 primary ductal carcinoma sample. The inference algorithm parameters were set to yield 4000 Gibbs samples with a 20% burn-in and  2 ×  thinning rate for a final total of 1600 samples. We drew 1000 samples from  μ ˜ Δ = μ ˜ j case − μ ˜ j control  to estimate the posterior probability of a variant. We performed the posterior difference test to identify mutations in the haploid synthetic data. We set the threshold  τ  = 0 and the size of the test  α = 0.05 . For the HCC1187 dataset, we identified both somatic and germline mutations. In the posterior somatic test, we set the threshold  τ  = 0 and the size of the test  α = 0.05 . In the posterior germline test, we set the threshold  τ = 0.05  considering the low average coverage (40×). The size of the test is set at  α = 0.15 . We performed the  χ 2  non-uniformity test after the posterior density tests. 5.1 Performance by read depth We generated receiver-operating characteristic curves (ROCs) for a range of median read depth and a range of MAFs. For these ROCs, we used the posterior density test without the  χ 2  test to evaluate the performance of posterior density test individually.  Figure 2  shows ROCs generated by varying the threshold  τ  with a fixed  α = 0.05 .  Figure 2 a shows ROC curves for a true 0.1% MAF for a range of median coverage depths. At the lowest depth the sensitivity and specificity is no better than random. However, we would not expect to be able to call a 1 in 1000 variant base with a coverage of only 43. The performance improves monotonically with read depth.  Figure 2 b and c shows a similar relationship between coverage depth and accuracy for higher MAFs.
 Fig. 2. ROCs varying read depth showing detection performance 
 We measured the computational time for RVD2 varying the number of Gibbs sampling steps and the median read depth for the 400 bp synthetic dataset. In brief, on a 2.4-GHz processor it took ∼13 min per 1000 Gibbs samples to fit the model. The computational time is independent of the median read depth due to the model structure; the same performance was observed for a median read depth of 130 and 40 000. As stated previously, due to the independence structure of the model, we are able to perform the sampling step for each location in parallel greatly decreasing the computational time. The memory requirement is roughly the size of the gene sequence times the number of Gibbs samples. Complete timing results without parallelization are shown in  Supplementary Section 8 . 5.2 Empirical performance compared with other algorithms We compare the empirical performance of RVD2 to other variant calling algorithms using the synthetic DNA dataset by the false discovery rate (FDR) and sensitivity/specificity. Among these algorithms, Samtools and GATK are optimized for homogeneous samples, while RVD, VarScan2-somatic, Strelka and muTect have good performance to call variants in heterogeneous samples. In research applications, the FDR is a more relevant performance metric because the aim is generally to identify interesting variants for follow-up. The sensitivity/specificity metric is more relevant in clinical applications where one is more interested in correctly calling all of the positive variants and none of the negatives. GATK, Varscan2, Strelka and muTect are only able to make use of one case and one control sample, so we provide results of RVD2 with the same data set ( N  = 1) for comparison. We compare the empirical performance across a wide range of median read depth (∼40× to ∼40 000×). In typical whole genome applications, the read depth is between 10× and 100×. For targeted cancer sequencing, the median read depth is higher at 100× to 1000×. For microbial or viral sequencing for rare variants, the median read depth is even higher at 1000× to 100  000×. 5.2.1 Sensitivity/specificity comparison Figure 3  shows that samtools, GATK and VarScan2-mpileup all have similar performance. They call the 100% MAF experiment well even at low depth, but are unable to identify true variants in mixed samples. VarScan2-somatic is able to call more mixed samples. However, as the read depth increases the specificity degrades. Strelka is able to call 10% MAF variants with good performance, but is limited at 1% MAF and below. muTect has good performance across a wide range of MAF levels. But even at the highest depth only has around 0.5 sensitivity for low MAF levels.
 Fig. 3. Sensitivity/specificity comparison of RVD2 with other variant calling algorithms using synthetic DNA data 
 The performance statistics for RVD are an average of three sets of pair-end case replicates. RVD performed the best among all algorithms when the read depth is near 40 000. RVD called all the mutated positions across all MAF levels with no false positives when MAF level is 0.3% or lower. However, RVD cannot call any mutations when the depth is too low to measure the baseline error rate and therefore is not useful for low-depth data. RVD2 has a high sensitivity and specificity for a broad range of read depths and MAFs. The sensitivity increases considerably with read depth at a slight expense to specificity. For the most difficult test with a low read depth and low MAF, RVD2 performs on-par with muTect. With  τ *  the performance is much better with high sensitivity and specificity across a wide range of read depths and MAFs. However, in practice one may not know the optimal  τ * 
 a 
 priori.  With  N  = 6 replicates, the sensitivity increases considerably for low MAF variants with a slight degradation in specificity due to false positives. When the median read depth is at least  10 ×  the MAF, RVD2 has higher specificity than all of the other algorithms tested and has a lower sensitivity in only three cases. 5.2.2 FDR comparison Figure 4  shows the FDR for RVD2 compared with samtools, GATK, varscan, Strelka and muTect. Blank cells indicate no positive calls were made.
 Fig. 4. FDR comparison of RVD2 with other variant calling algorithms using synthetic DNA data. Blank cells indicate no locations were called variant 
 Samtools performs well on 100% MAF sample and performance improves for read depths 3089 and 30 590. GATK performs well on both the 10 and 100% variants, but makes a false positive call at the 100% MAF level for all read depth levels. VarScan2-pileup performs perfectly for all but the lowest depth for the 100% MAF. VarScan2-somatic is able to make calls for all but the lowest MAF and coverage level. However, the FDR is high due to many false positives. Interestingly, at a MAF of 100% the FDR is zero for lowest read depth and over 0.9 for the highest read depth. Strelka has a better FDR than the samtools, GATK or Varscan2-somatic algorithms for almost all read depths at the 10 and 100% MAF. However, it does not call any variants at or below 1% MAF. muTect has the best FDR performance of the other algorithms we tested over a wide range of MAF and depths. But the FDR level is relatively high at around 0.7 for 0.1–1% MAF and 0.3 for 10–100% MAF. RVD has best FDR performance in the high read depth for 0.1–1% MAF levels. The FDR increases to around 0.3 for 10–100% MAF in the high read depth. RVD2 has a lower FDR than other algorithms when the read depth is greater than  10 ×  the inverse MAF with  N  = 1 and  τ  set to the default value of zero or to the optimal value. The FDR is higher when  N  = 6 because the variance of the control error rate distribution  P ( μ j control | r control )  is smaller. The smaller variance yields improvements in sensitivity at the expense of more false positives. Since the FDR only considers positive calls, the performance by that measure degrades. 5.3 HCC1187 primary ductal carcinoma sample RVD2 identified 15 variant locations in the 59 kbp PAXIP1 gene from chr7:154735400 to chr7:154794682. There were 11 germline variants and 10 somatic mutations.  Figure 5  shows the estimated MAFs for the normal and tumor samples at the called locations. Interestingly, positions chr7:154754371 and chr7:154758813 appear to be loss-of-heterozygosity events. Some of these mutations are also found to be common population SNPs according to dbSNPv138. The corresponding identities are shown in  Figure 5 . The read depth distribution for positions called by RVD2 is provided in  Supplementary Table S1 . Karyotyping indicates that chromosome 7 in HCC1187 is tetraploid  http://www.path.cam.ac.uk/pawefish/BreastCellLineDescriptions/HCC1187.html .
 Fig. 5. Estimated minor allele fraction for germline and somatic mutations called by RVD2 in the 59kbp PAXIP1 gene. Blue diamonds indicate germline mutations, where  μ control  is significantly different from the reference sequence. Red stars indicate somatic mutations, where  μ case  is significantly different from  μ control . The vertical lines represent 95% credible interval around posterior mean MAF. Ten positions are common population SNPs according to dbSNPv138, and the identities are shown below the positions 
 5.3.1 Performance comparison with other algorithms We ran muTect and VarScan2-somatic to call mutations in the PAXIP1 gene in HCC1187 sample. We also compared with the result shown in original research report where Strelka was used to identify mutations in the same sample ( Allen, 2013 ).  Figure 6 a shows mutation detection result from Strelka, RVD2, muTect and VarScan2-somatic. For notation simplicity, we use position index to present actual positions in  Figure 6  (the corresponding genomic positions are provided in  Supplementary Table S1 ).
 Fig. 6. ( a ) Positions called by VarScan2-somatic, muTect, RVD2 and Strelka in the 59kbp PAXIP1 gene from chr7:154735400 to chr7:154794682. The positions are sorted by index (correspondence to genomic positions shown in  Supplementary Table S1 ). ( b ) Read counts for each base for positions called by RVD2 and muTect from raw pileup data 
 The mutations called by RVD2 and muTect are the most consistent among all the techniques. RVD2 detected 15 germline mutations and 10 somatic mutations, while muTect reported 16 mutations; 11 were called by both. In the disagreements, RVD2 did not call positions 1, 39, 54 and 84 while muTect did not call positions 41 75 and 77. Referring to the depth distribution shown in  Figure 6 b, it can be seen that positions 41, 75 and 77 are more likely mutated while positions 1, 39, 54 and 84 are less likely mutated. Strelka was the least sensitive algorithm among all the algorithms. According to the technical report, Strelka identified position 26 (chr7:154760439) as variant, but did not call any other variants. In particular Strelka missed the two LOH events called by RVD2. VarScan2-somatic called most positions among all algorithms, 84 positions as shown in  Figure 6 a. VarScan2-somatic detected all the positions called by RVD2 except position 39, which turns out to be a very likely mutation given the depth distribution in  Figure 6 b. VarScan2-somatic reported 65 positions which were not called by any other three algorithms. The read depth in  Supplementary Table S1  suggests that these positions are very likely to be false positives. As shown in  Figure 4 , the FDR for VarScan2-somatic at read depth 53 MAF level 1.0% is as high as 1.00.  Spencer  et 
 al.  (2013)  also mentioned that VarScan2 has tendency to call many false positives at high read depth. 6 Discussion We describe here a novel algorithm for model estimation and hypothesis testing for identifying single-nucleotide variants in heterogeneous samples using NGS data. Our algorithm has a higher sensitivity and specificity than many other approaches for a range of read depths and MAFs. Our inference algorithm uses Gibbs sampling to do inference in the RVD2 hierarchical empirical Bayes model. This sampling procedure provides a guarantee to identify the global optimal parameter settings asymptotically. However, it may require many samples to achieve that guarantee causing the algorithm to be slower than other deterministic approaches. We opted for this balance of speed and accuracy because computational time is often not limiting and the cost of a false positive or false negative greatly outweighs the cost of more computation. Another factor that can affect the speed of RVD2 is the number of M–H sample within one Gibbs sampling run. RVD2 is able to use multiple cores in parallel, which can significantly improve time efficiency. In future studies, we plan to reduce the computational cost by using more sophisticated MCMC sampling methods or deterministic approximation methods such as variational EM or stochastic variational EM. We have focused on the statistical model and hypothesis test in this study and our results do not include any pre-filtration of erroneous reads or post-filtration of mutation calls beyond a simple quality score threshold. Incorporation of such data-cleaning steps will likely improve the accuracy of the algorithm. Our approach does not address identification of indels, structural variants or copy number variants. Those mutations typically require specific data analysis models and tests that are different than those for single-nucleotide variants. Furthermore, analysis of RNA-seq data or other data generated on the NGS platform may require different models that are more appropriately tuned to the particular noise feature of that data. The availability of clinical sequence data is increasing as the technical capability to sequence clinical samples at low-cost improves. Consequently, we require statistically accurate algorithms that are able to call germline and somatic point mutations in heterogeneous samples with low purity. Such accurate algorithms are a step toward greater access to genomics for clinical diagnostics. 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>High-throughput analysis of epistasis in genome-wide association studies with BiForce</Title>
    <Doi>10.1093/bioinformatics/bts304</Doi>
    <Authors>Gyenesei Attila, Moody Jonathan, Semple Colin A.M., Haley Chris S., Wei Wen-Hua</Authors>
    <Abstract>Motivation: Gene–gene interactions (epistasis) are thought to be important in shaping complex traits, but they have been under-explored in genome-wide association studies (GWAS) due to the computational challenge of enumerating billions of single nucleotide polymorphism (SNP) combinations. Fast screening tools are needed to make epistasis analysis routinely available in GWAS.</Abstract>
    <Body>1 INTRODUCTION Genome-wide association studies (GWAS) have been successful in identifying a large number of trait-associated genetic loci ( Hindorff  et al. , 2009 ), but are less successful in identifying much of the genetic variation ( Maher, 2008 ). Gene–gene interactions (epistasis) are thought to be a potential source of unexplained genetic variation ( Eichler  et al. , 2010 ;  Gibson, 2010 ;  Manolio  et al. , 2009 ;  Zuk  et al. , 2012 ), but they remain largely unexplored in GWAS conducted so far. A major hurdle for studying epistasis in GWAS is the lack of widely accepted algorithms that are fast enough to effectively handle high-density single nucleotide polymorphism (SNPs) and can map different forms of epistasis while keeping false-positive rates (FPRs) under control ( Wei  et al. , 2010 ). High-throughput tools are needed to make epistasis analyses routinely available in GWAS and ultimately improve our understanding of the role of epistasis in the genetic regulation of complex traits. Significant efforts have been made to develop new tools and algorithms for epistasis detection in GWAS ( Cordell, 2009 ) using either deterministic or stochastic methods, such as regression ( Hemani  et al. , 2011 ;  Kam-Thong  et al. , 2011 ;  Liu  et al. , 2011 ;  Schupbach  et al. , 2010 ;  Wan  et al. , 2010 ), machine learning ( Cattaert  et al. , 2010 ;  Greene  et al. , 2010 ;  Motsinger-Reif  et al. , 2010 ) and Bayesian-based approaches ( Tang  et al. , 2009 ;  Zhang and Liu, 2007 ). Most of these algorithms concern only GWAS for case–control (binary) disorders and still require substantial computing time to analyse epistasis in one trait in real GWAS data ( Schupbach  et al. , 2010 ;  Yung  et al. , 2011 ). Partial search strategies, based on biological knowledge ( Emily  et al. , 2009 ) or filtering unimportant SNPs prior to analysis ( Kam-Thong  et al. , 2011 ), are adopted in some studies in order to reduce excessive computing burden but risk missing some types of variation. Fast but comprehensive methods to analyse epistasis in GWAS conducted for many complex (i.e. continuous and quantitative) traits are lacking. Previously, we showed that high-throughput analysis of epistasis in quantitative traits in GWAS was feasible using computers with general purpose graphics processing units ( Hemani  et al. , 2011 ). We also suggested a search algorithm using the information of pre-identified loci in a full pair-wise genome scan to increase the power of detection of epistasis ( Lam  et al. , 2009 ;  Wei  et al. , 2010 ), which was applied successfully in recent studies for binary ( Liu  et al. , 2011 ) and complex traits ( Wei  et al. , 2011 ). These ideas are combined in a unique tool, BiForce, to support high-throughput epistasis analysis for either binary or quantitative traits on commonly used computer systems. Herein, we describe the algorithm and essential features of BiForce and compare the performance of BiForce with that of BOOST ( Wan  et al. , 2010 ) in binary traits and PLINK ( Purcell  et al. , 2007 ) in quantitative traits through simulation, both of which perform exhaustive pair-wise search based on regression-based statistics over commonly used computer systems as BiForce. We also demonstrate BiForce analysis of epistasis in real GWAS data, i.e. eight metabolic traits in the Northern Finland Birth Cohort 1966 (NFBC1966) ( Sabatti  et al. , 2009 ) provided by dbGaP ( http://www.ncbi.nlm.nih.gov/gap ) and two disease traits provided by the Wellcome Trust Case Control Consortium (WTCCC) ( https://www.wtccc.org.uk/ccc1/ ). It is hoped that with BiForce, the analysis of epistasis in GWAS will become a routine exercise thus facilitating accumulation of information on epistasis hence improving our understanding its role in the genetic regulation of complex traits. 2 METHODS 2.1 BiForce BiForce is a multi-threaded Java implementation of a combined search algorithm ( Lam  et al. , 2009 ;  Wei  et al. , 2010 ) and it can be used on a single workstation or computer clusters through a friendly graphical user interface or the command line. It uses contingency table-based methods to calculate pair-wise SNP interactions, which makes BiForce applicable to either binary or quantitative traits ( Supplementary Note 1 ). BiForce can perform full pair-wise genome scans very rapidly because of three computational achievements:
 Bitwise data structures: SNP genotype data are converted into Boolean bit values and stored in memory-efficient Java BitSet arrays allowing missing genotypes to be handled easily. Boolean bitwise operations: logical operations (e.g. AND) over the arrays of bit values make the calculation of SNP interactions (see below) extremely fast. Multithread (and/or multi-core) parallelization: this makes full pair-wise genome scans feasible on a single workstation and portable across computer clusters. 
The combined search algorithm includes two consecutive genome scans: single SNP-based genome-wide association tests (i.e. conventional GWAS) and pair-wise interaction tests of all SNP combinations. Single SNPs with marginal effects that are genome-wide significant (marginal SNPs) are identified in the first scan and used to detect interactions involving marginal SNPs ( Wei  et al. , 2010 ). The 5% genome-wide significance thresholds are derived based on the Bonferroni correction for total number of tests performed. Given  N  to be the total number of SNPs in a study with  K  ( K &gt;0) marginal SNPs being identified, the thresholds are  P =0.05/ N  for marginal SNPs,  P =0.05/(( N −1)× K ) for interactions involving marginal SNPs (because each marginal SNP is tested against the full genome, and hence the total test is ( N −1)× K ) and  P =0.05/( N ×( N −1)/2) for a pair-wise genome scan ( Evans  et al. , 2006 ;  Lam  et al. , 2009 ). Considering a pair of SNPs denoted as SNP 1  and SNP 2 , the following genetic models are used to detect epistasis where genotypes of each SNP (i.e. homozygote of the minor allele, homozygote of the major allele and heterozygote) were fitted as fixed factors:
 Model 1:  y  = μ + SNP 1  + SNP 2  + SNP 1  × SNP 2  +  e Model 2:  y  = μ + SNP 1  + SNP 2  +  e , 
where  y  is the trait of interest, μ is the model constant, SNP 1  (or SNP 2 ) is a fixed factor with three levels, SNP 1 ×SNP 2  is the interaction term and  e  is the random error term. The test of Model 1 against Model 2 ( F  ratio for quantitative traits and log-likelihood ratio for binary traits) is for the interaction between the two SNPs (i.e. four degrees of freedom).  P -values were computed based on specific test statistic distributions and actual degrees of freedom (assumed fixed four degrees of freedom in disease traits). BiForce is designed to provide fast screening of epistasis without pre-filtering of SNPs in GWAS. For disease traits, BiForce adopts the approximation step implemented in BOOST ( Wan  et al. , 2010 ) as a default option to accelerate the exhaustive genome scan, which can be dismissed when necessary (i.e. using only log-likelihood ratio tests in the exhaustive scan). Quality control procedures applied in GWAS are required before using BiForce to analyse epistasis. Currently, BiForce can only work with SNPs located on autosomal chromosomes. 2.2 Experiments on simulation data Simulation was used to test the performance of BiForce in binary traits in comparison with BOOST ( Wan  et al. , 2010 ) and quantitative traits in comparison with PLINK ( Purcell  et al. , 2007 ) using 500 replicates for every simulation scenario ( Supplementary Note 2 ). For simplicity, in both comparison cases, we adopted the simulation design used in the BOOST paper ( Wan  et al. , 2010 ), where BOOST was compared against PLINK using simulation generated by the program gs ( Li and Chen, 2008 ) to measure the power of detection and the program genomeSIMLA ( Dudek  et al. , 2006 ) for FPR estimates. The gs program generated SNP genotypes using HapMap data under the assumption of Hardy–Weinberg equilibrium, whereas the genomeSIMLA program generated genotype data based on the Affymetrix 500k SNP array to accommodate linkage disequilibrium in real GWAS. The simulation design used four two-locus interaction models each with marginal effects of the disease loci to generate epistatic scenarios. Briefly, considering two loci  A  (disease risk allele  a ) and  B  (disease risk allele  b ), Model 1 is a multiplicative model ( Marchini  et al. , 2005 ); both Models 2 and 3 have the missing lethal genotype (i.e. the double homozygote of disease alleles  aabb  does not lead to disease) ( Li and Reich, 2000 ); Model 2 differs from Model 3 mainly in the double heterozygous genotype  AaBb  that does not lead to disease and has been used to describe the genetics of handedness ( Levy and Nagylaki, 1972 ;  Neuman and Rice, 1992 ); Model 4 is a well known XOR (exclusive OR) model where only four single heterozygous genotypes ( AABb ,  AaBB ,  Aabb  and  aaBb ) lead to disease ( Li and Reich, 2000 ;  Moore and Williams, 2009 ) ( Supplementary Note 2 ). Following the design, the four epistatic models were used to generate epistatic scenarios for binary traits each with a fixed disease prevalence of 0.1, 1000 SNPs, a sample size of 800 or 1600 (with balanced design) and a minor allele frequency (MAF) of 0.1 or 0.2 or 0.4 for disease SNPs (assumed equal MAF for both loci). Disease heritability was set as 0.03 for Model 1 and 0.02 for Models 2–4. The gs program simulated SNP genotypes and samples (either 800 or 1600) for each epistatic scenario. To apply the simulation design to quantitative traits without the disease prevalence parameter while maintaining the interaction pattern, genotypes in the contingency table derived for each epistatic scenario for binary traits were scaled down to concern only MAF and heritability ( Supplementary Note 2 ). The gs program simulated SNP genotypes and an R script was used to simulate samples for quantitative traits with a standardized distribution (mean of 0 and variance of 1) for each epistatic scenario ( Supplementary Note 2 ). We randomly chose the chromosome 11 HapMap data for gs to simulate the epistatic scenarios. Power was calculated as the percentage of replicates with the simulated epistatic SNP pair detected as a significant signal out of the total of 500 replicates. In addition, following the simulation design, the NULL scenarios were generated using the genomeSIMLA program to simulate 38 836 SNPs based on the SNP information of chromosome 1 from the Affymetrix 500k SNP array as the BOOST paper ( Wan  et al. , 2010 ), with 1000 samples simulated by randomly sampling from a Bernoulli distribution for binary traits or from a Gaussian distribution (mean of 0 and variance of 1) for quantitative traits. An additional NULL scenario was used to examine FPRs where 1000 SNPs were randomly generated with MAFs uniformly distributed in [0.05, 0.5]. FPR was calculated as the percentage of the total number of significant SNP pairs detected out of 500 replicates. 2.3 High-throughput analyses of epistasis BiForce was used to analyse epistasis in eight metabolic traits in the NFBC1966 cohort: C-reactive protein (CRP), diastolic blood pressure (DBP), glucose (GLU), high-density lipoprotein (HDL), insulin (INS), low-density lipoprotein (LDL), systolic blood protein (SBP) and triglycerides (TRI). Following the instructions given in the original GWAS ( Sabatti  et al. , 2009 ), we firstly excluded individuals according to the phenotypic exclusion criteria and then undertook the procedures of quality control of genotype data and corrected each trait for the SexCOPG covariate (recoded according to gender, status of taking oral contraception and pregnancy) ( Supplementary Note 3 ). Furthermore, each trait was normalized (instead of log transformed in the original GWAS ( Sabatti  et al. , 2009 )) using the ‘rntransform’ function and then corrected for relatedness using the ‘polygenic’ function both available in the GenABEL package ( Aulchenko  et al. , 2007a ) implemented in R ( http://www.r-project.org/ ) and the resultant environmental residuals (i.e. pgresidualY) were used as the new trait values to test for association ( Aulchenko  et al. , 2007b ). After the quality control and phenotype pre-processing, the NFBC1966 cohort had 323 697 autosomal SNPs and &gt;4500 individuals (ranged from 4579 in INS to 5255 in CRP) in different traits. The consensus GWAS threshold ( P  = 5.0E-08) ( McCarthy  et al. , 2008 ) was applied to identify marginal SNPs. Following the definitions in  Section 2.1 , with  N  as 323 697, the 5% genome-wide threshold  P -values were derived as 9.54E-13 for the full pair-wise genome scan and 1.5E-07 for interactions with marginal SNPs if only one marginal SNP was detected (or 7.7E-07, 5.1E-08, 3.9E-08, 3.1E-08, 2.6E-08, 2.2E-08, 1.9E-08, 1.7E-08 and 1.5E-08, if 2–10 marginal SNPs were detected, respectively). BiForce was also used to analyse two WTCCC datasets: bipolar disorder (BD) and Crohn's disease (CD) that were obtained initially for BiForce development with 1500 shared control individuals from the UK Blood Services ( Consortium, 2007 ). All individuals were genotyped with the Affymetrix GeneChip 500K mapping array set. After excluding 153 individuals with non-European ancestry and quality control ( Supplementary Note 3 ), in total 1458 shared controls, 1868 BD cases (347 004 SNPs) and 1752 CD cases (349 056 SNPs) were analysed for epistasis using BiForce. The 5% genome-wide threshold  P -values were derived for the two traits similarly as for the metabolic traits. 3 RESULTS 3.1 Power and FPR on simulation experiments Figure 1  shows the comparison of power of detection of epistasis between BiForce and BOOST for binary traits. BiForce exhibited higher or similar power across all epistatic scenarios simulated. The BiForce power advantage over BOOST became more evident when the sample size was 1600, e.g. power more than doubled when MAF was 0.1 in Models 2–4, MAF was 0.2 and 0.4 in Model 1 and MAF was 0.2 in Model 3. The power gains in BiForce are attributable to the use of the combined search algorithm through detection of interactions involving marginal SNPs. If interactions involving marginal SNPs are ignored or when no marginal SNPs were involved in interactions, the power values from BiForce were almost identical to those from BOOST since both use log-likelihood ratio tests and a pair-wise genome scan.
 Fig. 1. Comparison of power of detection of epistasis in binary traits between BiForce and BOOST. Model 1: multiplicative model, Models 2 and 3: missing lethal genotype model ( aabb  does not lead to disease,  AaBb  does in Model 3 but not in Model 2), Model 4: exclusive OR model The power of detection of epistasis in quantitative traits was generally low across simulation scenarios ( Fig. 2 ). With a sample size of 800, neither BiForce nor PLINK could detect epistatic signals. BiForce was clearly more powerful than PLINK in all the scenarios with a sample size of 1600. The reasons for the BiForce power gains include the use of the combined search algorithm as before as well as the genotype models. In contrast to the allelic models used in PLINK that can detect only additive–additive interactions, the genotype models used in BiForce can detect additional interaction components not captured by the allelic model (e.g. additive dominance).
 Fig. 2. Comparison of power of detection of epistasis in quantitative traits between BiForce and PLINK. Model 1: multiplicative model, Models 2 and 3: missing lethal genotype model ( aabb  does not lead to disease,  AaBb  does in Model 3 but not Model 2), Model 4: exclusive OR model The simulation results of the NULL scenario generated by genomeSIMLA showed that BiForce could control FPR at the 5% genome-wide significance level ( Fig. 3 ). The FPR values became slightly lower than the expected values as the Bonferroni-adjusted thresholds became liberal (i.e. 20%). In the NULL simulation scenario using random SNPs, the FPR of BiForce at the 5% genome-wide significance level was similar to that of BOOST and PLINK and close to 5%, with slightly inflation in quantitative traits when the Bonferroni-adjusted thresholds became liberal ( Supplementary Note 4 ). Using thresholds adjusted to the same level of FPR of 5% made little differences to power profiles as shown in  Figures 2  and  3  ( Supplementary Note 4 ).
 Fig. 3. FPR profiles of BiForce in detection of epistasis in binary and quantitative traits 3.2 BiForce computational efficiency We tested BiForce and BOOST in analysing datasets with 1000 samples and different numbers of SNPs on a single workstation (2.8 GHz Intel Core iMAC with 4 GB RAM and four CPU cores each with two threads) to give a fair comparison. In addition, the same tests were performed on a computer cluster of 32 nodes each with four CPU cores (two threads per core). BiForce was found to be about 30% faster than BOOST when using a single thread and 4–5-fold faster when using eight threads ( Table 1 ). Using the computer cluster, BiForce was 315–330-fold faster than BOOST.
 Table 1. Computing performance (in h) comparison between BiForce and BOOST in analysing different GWAS datasets (1000 samples) a SNPs BOOST BiForce (1 thread) BiForce (8 threads) BiForce (cluster) 100 k 2.90 2.29 0.60 0.01 200 k 11.61 8.89 2.29 0.29 300 k 26.11 20.08 5.11 0.65 400 k 46.36 35.97 8.83 1.16 500 k 72.64 55.68 14.03 1.82 1000 k 295.97 221.98 55.96 7.40 a BOOST, BiForce (one thread) and BiForce (eight threads) each ran on an iMAC workstation with 4 GB RAM and 4 Intel Cores each with two threads running at 2.8 GHz. BiForce (cluster) used a 32-node computer cluster each with 4 CPU cores (two threads per core). 
 The above tests were not feasible for quantitative traits because of very long PLINK computing times. Instead, we measured the number of pair-wise tests computed per second by BiForce and PLINK in the quantitative trait situation using 10 000 SNPs and 200 samples on the same workstation as before. BiForce computed 505 000 pair-wise tests per second when using a single thread (2 380 714 using eight threads), whereas PLINK computed only 2990 pair-wise tests per second, i.e. a 168- and 796-fold speed increase using one and eight threads, respectively. We also tested BiForce on a large GWAS dataset with 500 000 SNPs and 5000 samples to give an idea of computing time in real GWAS data. In the binary trait case, BiForce took 118.18, 30.8 and 0.46 h using one thread and eight threads on the workstation and 256 threads on the computer cluster, respectively. In the quantitative trait case, BiForce took 293.24 and 6.81 h using eight threads on the workstation and 256 threads on the cluster, respectively. In contrast, FastEpistasis—a parallel extension of PLINK took 29, 4 or 0.5 days to analyse a GWAS dataset of the same size using 8, 64 or 512 MPI-bound processors, respectively ( Schupbach  et al. , 2010 ); GBOOST—a graphical processing unit version of BOOST took 1.34 h to compute a smaller GWAS dataset of 351 542 SNPs and 5003 samples on a computer with Nvidia GeForce GTX 285 display card (i.e. 240 CPU cores) ( Yung  et al. , 2011 ). 3.3 Epistasis in eight metabolic traits in NFBC1966 BiForce was used to analyse the eight metabolic traits in the NFBC1966 cohort over a local (MRC Human Genetics Unit) cluster of 32 computer nodes each with two Quad-cores (four threads per core) giving a total of 256 threads running at 2.53 GHz per thread. For each trait, BiForce splits the search into 32 small tasks each of which was analysed using two threads and took on average 10.5 h to complete. The whole analysis of eight traits was completed within a day (&lt;24 h). Using the threshold of  P  = 5.0E-08, we found 10, 4, 7, 4, 1 and 2 marginal SNPs associated with CRP, GLU, HDL, LDL, SBP and TRI, respectively, and none associated with DBP or INS ( Supplementary Table S1 ). These results mostly agreed with the original GWAS ( Sabatti  et al. , 2009 ) although we used genotype models (instead of allelic models) and the more rigorous rank transform of the data to normality (instead of the log-transform). BiForce discovered nine genome-wide significant epistatic SNP pairs of which five were for CRP (essentially two epistatic signals on chromosomes 1 and 12, respectively) and one was for each of GLU, HDL, LDL and TRI ( Table 2 ). All the nine epistatic pairs were discovered as marginal SNP interactions (the first seven were between two marginal SNPs), while the rs3764261–rs1532624 pair in HDL was also detected through the pair-wise genome scan. All the epistatic SNPs had a relatively common MAF between 0.28 and 0.41. Interestingly, the interacting SNPs in each of the nine epistatic pairs are located very closely together (&lt;1 Mb) with linkage disequilibrium (LD, in  r 2 ) in a range between 0.35 and 0.99. No significant epistatic signals were detected in DBP, INS and SBP.
 Table 2. Genome-wide significant epistatic pairs identified from the NFBC199 cohort a Trait SNP 1 SNP 2 P int Distance LD ( r 2 ) CRP rs1811472 b  (1q23.2; 0.41) rs2592887 b  (1q23.2; 0.40) 3.0E-12 10 590 0.86 CRP rs1811472 b  (1q23.2; 0.41) rs2794520 b  (1q23.2; 0.36) 3.5E-11 36 467 0.62 CRP rs2592887 b  (1q23.2; 0.40) rs2794520 b  (1q23.2; 0.36) 2.9E-12 25 877 0.70 CRP rs2650000 b  (12q24.31; 0.45) rs7953249 b  (12q24.31; 0.48) 2.6E-09 14 762 0.76 CRP rs1169300 b  (12q24.31; 0.32) rs2464196 b  (12q24.31; 0.32) 3.4E-10 4202 0.99 GLU rs560887 b  (2q31.1; 0.30) rs563694 b  (2q31.1; 0.34) 1.3E-08 10 923 0.81 HDL rs3764261 b  (16q13; 0.28) rs1532624 b  (16q13; 0.41) 2.0E-14 12 155 0.53 LDL rs157580 b  (19q13.32; 0.29) rs405509 (19q13.32; 0.46) 6.9E-10 13 570 0.35 TRI rs1260326 b  (2p23.3; 0.36) rs780094 (2p23.3; 0.36) 5.8E-08 10 297 0.95 a All SNP pairs listed were detected as marginal SNP interactions, with the threshold of 1.5E-08 for CRP, 2.2E-08 for HDL, 3.9E-08 for GLU and LDL, 7.7E-07 for TRI; SNP 1  (SNP 2 )—name, genomic location and MAF (the latter two in bracket) of the first (second) SNP;  P int — P -value of the interaction test; distance—the distance in base pairs between two SNPs; LD—linkage disequilibrium (in  r 2 ) between a pair of SNPs; the SNP pair in HDL was also detected via the pair-wise genome scan ( P  &lt; 9.54E-13). b The marginal SNP. 
 3.4 Epistasis in two disease traits in WTCCC BiForce was used to analyse two disease traits BD and CD in the WTCCC data over the same local computer cluster above. Using pre-defined genome-wide thresholds, we identified 3 and 25 marginal SNPs ( Supplementary Table S2 ), 5 and 12 genome-wide significant SNP pairs ( Table 3 ) in BD and CD, respectively. Two of these identified SNP pairs were detected as marginal SNP (rs4246045) interactions, and the remaining were detected only through full pair-wise genome scans. The SNP pairs of rs11162341–rs6658302, rs11096892–rs6531531 and rs2747436–rs29254 were identified in both BD and CD suggesting pleiotropic effects in these signals. Again, all the identified SNP pairs were local interactions in five loci: 1p31.1, 3q21.3, 4p15.1, 5q33.1 and 6p22.1, with SNP MAF ranged from 0.05 to 0.29 and LD ranged between 0.02 and 0.82. Similar observations were previously reported in the BOOST paper (in  Table 3  without detailed information of epistatic pairs of SNPs) ( Wan  et al. , 2010 ) where the total number of SNP pairs detected was slightly different possibly due to a doubled number of control samples and slight differences in the quality control procedure used.
 Table 3. Genme-wide significant epistatic pairs identified from the WTCCC datasets a Trait SNP 1 SNP 2 P int Distance LD ( r 2 ) BD rs11162341 (1p31.1; 0.13) rs6658302 (1p31.1; 0.25) 1.7E-14 11 272 0.02 BD rs11096892 (4p15.1; 0.05) rs6531531 (4p15.1; 0.28) 9.1E-15 2196 0.03 BD rs4246045 b  (5q33.1; 0.14) rs4958847 (5q33.1; 0.12) 5.5E-09 62 490 0.82 BD rs11949556 (5q33.1; 0.12) rs4246045 b  (5q33.1; 0.14) 4.7E-09 52 704 0.82 BD rs2747436 (6p22.1; 0.29) rs29254 (6p22.1; 0.06) 4.2E-13 28 341 0.03 CD rs11162341 (1p31.1; 0.13) rs6658302 (1p31.1; 0.25) 3.1E-14 11 272 0.02 CD rs1735558 (3q21.3; 0.15) rs6439119 (3q21.3; 0.24) 2.2E-16 136 423 0.47 CD rs2248668 (3q21.3; 0.14) rs6439119 (3q21.3; 0.24) 8.9E-19 32 438 0.49 CD rs2811472 (3q21.3; 0.15) rs6439119 (3q21.3; 0.24) 8.0E-16 44 316 0.48 CD rs2811483 (3q21.3; 0.14) rs6439119 (3q21.3; 0.24) 4.8E-19 8514 0.50 CD rs2811484 (3q21.3; 0.14) rs6439119 (3q21.3; 0.24) 6.0E-19 8334 0.50 CD rs2811510 (3q21.3; 0.14) rs6439119 (3q21.3; 0.24) 6.3E-19 8675 0.50 CD rs2955125 (3q21.3; 0.14) rs6439119 (3q21.3; 0.24) 6.8E-19 2005 0.50 CD rs2955132 (3q21.3; 0.14) rs6439119 (3q21.3; 0.24) 1.1E-18 17 039 0.50 CD rs1554534 (3q21.3; 0.15) rs6439119 (3q21.3; 0.24) 8.8E-15 123 518 0.47 CD rs11096892 (4p15.1; 0.05) rs6531531 (4p15.1; 0.28) 2.8E-14 2196 0.03 CD rs2747436 (6p22.1; 0.29) rs29254 (6p22.1; 0.06) 3.4E-14 28 341 0.03 a Threshold for the pair-wise genome scan was 8.3E-13 for BD and 8.2E-13 for CD; threshold for marginal SNP interactions was 4.8E-08 in BD (three marginal SNPs were detected); SNP 1  (SNP 2 )—name, genomic location and MAF (the latter two in bracket) of the first (second) SNP;  P int — P -value of the interaction test; distance—the distance in base pairs between two SNPs; LD—linkage disequilibrium (in  r 2 ) between a pair of SNPs. b The marginal SNP. 
 4 DISCUSSION We have shown that BiForce is a unique tool that can support high-throughput analysis of epistasis in GWAS for either binary or quantitative traits. BiForce achieves great computational efficiency by integrating three major advances in computing (i.e. bitwise data structures, Boolean bitwise operations and multithreaded parallelization) with fast calculation of pair-wise interactions. The implementation of the combined search algorithm (i.e. using a less stringent threshold to detect marginal SNP interactions) increases the power of detection of epistasis. Through a series of performance tests, we showed that BiForce was more powerful and significantly faster than BOOST in binary traits and PLINK in quantitative traits. Using real GWAS datasets from the NFBC1966 and WTCCC cohorts, we demonstrated that BiForce could analyse multiple traits in a short time period and identified genome-wide significant epistasis signals. The strategy of using a less stringent threshold for marginal SNP interactions is statistically justifiable and has been validated in simulations elsewhere ( Kooperberg and Leblanc, 2008 ;  Wei  et al. , 2010 ) and successfully applied in real data analyses ( Evans  et al. , 2011 ;  Strange  et al. , 2010 ;  Wei  et al. , 2011 ). Our simulation results support the strategy and justify that BiForce is working as expected. BiForce differs from BOOST mainly in the strategy of detection of marginal SNP interactions, thus in binary traits the power of detection by BiForce should be higher than or equal to that from BOOST as shown in our simulation results ( Fig. 1 ). BiForce differs from PLINK in the strategy of detection of marginal SNP interactions as well as the use of genotype models, and hence it is not straightforward to assess the individual impact of the strategy on the BiForce power gains in quantitative traits, especially when the power of detection was generally low ( Fig. 2 ). However, with reference to the simulation results in the BOOST study ( Wan  et al. , 2010 ) comparing BOOST (genotype models) against PLINK (allelic models), it is clear that the power advantage in BiForce over PLINK in scenarios using Model 1 with MAF of 0.2 and 0.4 ( Fig. 2 ) can be mostly attributed to the strategy of detection of marginal SNP interactions because the model simulated favours the allelic models in these scenarios. Whereas in the scenario using Model 4 with MAF of 0.4, the BiForce power advantage ( Fig. 2 ) can be attributed to the use of genotype models because as MAF increases towards 0.5 the model simulated generates less marginal effects and thus marginal SNP interactions are more difficult to be detected. Nevertheless, the power gains in BiForce are not surprising because all the four interaction models favour marginal effects to some extent. It is worth noting that allelic models implemented in PLINK may have some advantage over BiForce in situations where empty cells (i.e. no samples available in certain genotypes) in the nine-cell contingency table ( Supplementary Note 1 ) are prevalent. The NULL scenario simulation results ( Fig. 3 ) further justify BiForce. Using 38 836 SNPs simulated from a subset of human genome (i.e. chromosome 1), the results suggest that BiForce has a good control of FPR at the 5% level when LD is present. Surprisingly, the FPR values became more deflated (i.e. lower than the expected nominal error rates of 10 and 20%) when using less stringent thresholds indicating the Bonferroni correction could be conservative. This phenomenon was also observed in the BOOST paper where it was suggested to be due to the LD among the SNPs simulated (i.e. correlated tests) because no FPR deflation was observed in the NULL simulation scenario using 1000 random SNPs ( Wan  et al. , 2010 ). Our results of the NULL scenarios using 1000 random SNPs were in line with the BOOST results in binary traits but showed certain FPR inflation in quantitative traits using either BiForce or PLINK ( Supplementary Note 4 ). Our results suggest that in addition to LD, the ratio of number of SNPs to number of samples in a study could be critical for the relevance of the significance thresholds based on Bonferroni correction under the assumption that all pair-wise tests are independent. One may expect that in real GWAS concerning the full genome, the FPR at the 5% level may be further deflated because of a much increased ratio of SNPs to samples and the power of detection may be reduced owing to likely over-stringent Bonferroni-corrected thresholds. The problem can become severe as more and more SNPs are becoming available to GWAS. Therefore, the simulation results in this study may be taken as evidence for software comparisons but are not encouraged to be interpreted at the genome-wide level. A good alternative way to derive the genome-wide significance thresholds is to use permutation. Unfortunately, genome-wide permutation in real GWAS of epistasis would be computational prohibitive even for BiForce. Before the threshold issue is resolved, it is reasonable to use Bonferroni-corrected thresholds so that significant interactions identified from BiForce would contain less false positives than expected, which may be important to GWAS epistasis studies at the early stage. Nevertheless, considering that the Bonferroni-corrected thresholds may be over-stringent, BiForce allows user specified thresholds to be used in epistasis detection. The generally low power of detection of epistasis in quantitative traits in simulation ( Fig. 2 ) may be slightly discouraging. One possible reason for the low power could be that the scaling applied to genotype values ( Section 2.2 ,  Supplementary Note 2 ) might have reduced contrasts among them. However, it becomes obvious that a large proportion of existing GWAS have limited power to detect epistasis through pair-wise genome scans due to relatively small sample sizes used ( Cordell, 2009 ;  Gauderman, 2002 ;  Wei  et al. , 2012 ;  Zuk  et al. , 2012 ), particularly in quantitative traits ( Yang  et al. , 2010 ). Indeed, the epistasis results of the NFNC1966 cohort ( Table 2 ) suggest that excluding marginal SNP interactions, we could identify only one epistatic pair in HDL through pair-wise genome scans of eight metabolic traits despite that the sample size (5000) in the NFBC1966 cohort is reasonably big. In contrast, 3 and 12 SNP pairs were identified through pair-wise genome scans of the WTCCC BD and CD, respectively (sample size &lt;3500;  Table 3 ) suggesting WTCCC is slightly more powerful than NFBC1966. Considering that nine SNP pairs in CD could be regarded as one epistatic signal because they were mapped to the same genomic location (3q21.3), the power of detection in CD may not be much higher than that in BD ( Table 3 ). After BiForce analyses, the identified statistical significant interactions need to be tested for replication in other GWAS populations to avoid false positives ( Wei  et al. , 2011 ,  2012 ). Nonetheless, statistical replication of the identified interactions and further understanding their underlying biology are beyond the focus of this article of presenting the computational efficiency of BiForce as a fast screening tool. BiForce users are recommended to firstly evaluate the epistasis results in the original GWAS population by re-testing the statistical significant epistatic signals jointly in models considering various covariates and potential population stratification if necessary and then identify independent and important epistatic SNP pairs for statistical replication and further analyses. Such re-tests are essential for binary traits because BiForce in its current form (based on contingency tables) is unable to accommodate covariates in binary traits. Covariates can be approximately dealt with in quantitative traits prior to BiForce analysis as demonstrated in the epistasis analyses of the eight metabolic traits in the NFBC1966 cohort. Interestingly, all the significant epistatic pairs identified from the NFBC1966 and WTCCC cohorts reflect interactions between two closely located SNPs with a wide range of LD ( Tables 2  and  3 ). The observation of rich local interactions could be taken as supporting the hypothesis that some genetic variation in complex traits may hide in epistasis between linked SNPs ( Haig, 2011 ). However, one immediate question is whether these epistatic pairs are true interactions or mirroring marginal effects captured by haplotypes. The epistatic pairs identified in quantitative traits involve at least one marginal SNP ( Table 2 ) whereas most of those identified in the two disease traits involving no marginal SNPs ( Table 3 ). These results demonstrate that local interactions are not necessarily associated with marginal SNPs (e.g. none of the 25 marginal SNPs in CD listed in  Supplementary Table S2  was involved in local interactions) or driven by high LD between SNPs. A haplotype effect could create an apparent statistical interaction when there is only a single causative variant segregating. However, it may be more likely to find an apparent local interaction caused by a haplotype effect when each SNP is in LD with a single causative variant, but LD between SNPs is low. This arises because the correlation between two SNPs in high LD means that fitting the two SNPs together may explain little additional variation over fitting just one SNP. Unfortunately, statistically distinguishing a haplotype effect from a genuine local interaction is likely to be very difficult especially when only a limited sample of the variants available in a region. A detailed study of genetic variation in the region and other approaches such as functional genetic studies may be needed to help disentangle local interactions and understand the underlying mechanisms, e.g. intragenic and extragenic regulation mechanisms ( Rokop and Grossman, 2009 ) and interactions between coding and regulatory variants within a gene ( Lappalainen  et al. , 2011 ). With the performance presented, BiForce can remove the computational bottleneck in analysing epistasis in single GWAS populations. Indeed, BiForce has been used to analyse several other dbGaP GWAS datasets with different numbers of SNPs and samples (300–800k SNPs, 1800–6000 samples) in separate studies of epistasis, including those from the GAIN Collaborative Association Study of Psoriasis, GoKinD and GENEVA Diabetes studies. However, routine high-throughput analysis of epistasis with BiForce presents many new challenges. For example, we need fast pipelines to interpret epistatic signals identified from high-throughput analyses, perhaps making use of functional annotation to include biological meaning. We also need methods to make good use of sub-significant epistatic signals given that many GWAS may have low power to detect genome-wide significant signals ( Gauderman, 2002 ). In this case, a method to support meta-analysis of epistasis in GWAS will be needed as BiForce is not applicable to imputed SNP genotype data in its current form. 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Large-scale inference of competing endogenous RNA networks with sparse partial correlation</Title>
    <Doi>10.1093/bioinformatics/btz314</Doi>
    <Authors>List Markus, Dehghani Amirabad Azim, Kostka Dennis, Schulz Marcel H</Authors>
    <Abstract/>
    <Body>1 Introduction MicroRNAs (miRNAs) are ∼23 nt long RNAs that play an important role in the regulation of transcript abundance in mammalian cells. They are estimated to regulate at least half of the genes in the human genome ( Friedman  et al. , 2009 ) and thus affect important biological processes and show deregulation in many diseases ( Jiang  et al. , 2009 ). miRNAs regulate their target mRNAs by either degrading them or by preventing their translation ( Bartel, 2009 ). Target recognition is initiated by sequence complementarity of the target transcripts to the seed sequence of the miRNA at position 2–8. To predict miRNA target interactions, a number of sequence-based approaches have been proposed (e.g.  Agarwal  et al. , 2015 ;  John  et al. , 2004 ). However, a large fraction of these predictions are false positives, since condition-specific attributes of the cell, such as miRNA abundance, number of miRNA targets and their expression, are not known ( Pinzón  et al. , 2017 ). Although experimental techniques exist that measure condition-specific miRNA–gene interactions (e.g.  Jaskiewicz  et al. , 2012 ), these experiments are laborious and costly and often not available for the condition of interest. This motivates the development of computational methods that quantify condition-specific miRNA–gene interaction potential using widely available gene and miRNA expression datasets, reviewed by  Muniategui  et al.  (2013) . Notably, genes sharing binding sites for the same miRNA(s) compete over a limited pool of miRNA molecules, giving rise to a complex gene-regulatory network of competing endogenous RNAs (ceRNAs) ( Tsang  et al. , 2010 ). A number of cancer-associated genes have been shown to act as ceRNAs ( Arvey  et al. , 2010 ;  Salmena  et al. , 2011 ;  Tay  et al. , 2014 ), including  PTEN  ( Poliseno  et al. , 2010 ),  CD44  ( Jeyapalan  et al. , 2011 ),  ESR1  ( Chiu  et al. , 2015 ),  BRAF  ( Karreth  et al. , 2015 ),  KRAS  ( Poliseno  et al. , 2010 ),  MYCN  ( Powers  et al. , 2016 ) and  HULC  ( Wang  et al. , 2010 ). These findings motivated the development of computational methods for inferring ceRNA interactions systematically from gene and miRNA expression data, reviewed by  Le  et al.  (2016) . The different methods can be broadly categorized into methods that use (i) only static information, such as the number of miRNA binding sites or binding energy or (ii) methods that use condition-specific information in addition such as expression or Clip-data. One of the most commonly used methods in category (i) is based on the idea to assess the probability that two mRNAs share miRNAs and their binding sites, and to then highlight cases where this probability is much higher than expected by chance, for example by using the hypergeometric test ( Li  et al. , 2014 ). With the emergence of large-scale studies providing gene and miRNA expression data for hundreds of samples, a number of methods of category (ii) have been developed ( Le  et al. , 2016 ).  Sumazin  et al.  (2011)  proposed the use of conditional mutual information (CMI) for estimating the effect of a miRNA on a gene–gene interaction in their method HERMES. The advantage of this approach is that it measures non-linear associations, but estimation of significance is done using permutations, later implemented as part of the CUPID software (CUPID step III) ( Chiu  et al. , 2015 ). Recently the JAMI software has improved the runtime of the extensive CMI computation compared to CUPID ( Hornakova  et al. , 2018 ), but runtime is still a limiting factor for this approach in applications to very large datasets. This issue has motivated the use of conceptually simpler and fast linear correlation-based methods, for instance, restricting to only gene–gene correlation values ( Du  et al. , 2016 ;  Xu  et al. , 2015 ), gene–miRNA correlation ( Zhang  et al. , 2017 ) or correlations within triplets of two genes and one miRNA ( Liu  et al. , 2017 ;  Wang  et al. , 2015 ). However, in contrast to CUPID, these approaches do not quantify the contribution of the miRNA to the ceRNA interaction in a unified model. 
 Paci  et al.  (2014)  overcame this issue with the definition of sensitivity correlation ( scor ), which has similarities to the CMI-based approach. Linear partial correlation can be used to quantify the remaining correlation between two genes after accounting for the effect of one miRNA.  scor  is then defined as the difference between gene–gene correlation and partial correlation and thus quantifies the contribution of the miRNA in the regulation of two genes. Similar to CMI,  scor  considers the impact of miRNA regulation on both genes in a single mathematical model and is thus more powerful than the methods proposed in  Wang  et al.  (2015 ),  Zhang  et al.  (2017 ),  Du  et al.  (2016 ),  Xu  et al.  (2015 ) and  Liu  et al.  (2017) . Unlike CMI, however,  scor  computation is based on efficient estimators of covariance matrices for computing partial correlation and thus allows large-scale ceRNA network inference as demonstrated by  Zhang  et al.  (2016) , who inferred lncRNA–mRNA related ceRNA networks for 12 different cancer types. While the estimation of the  scor  coefficients is efficient, no theory for the computation of the null distribution of these values exists. Therefore, previous work relied on  ad hoc  approaches.  Paci  et al.  (2014)  selected the top 5% of  scor  coefficients for downstream analysis, disregarding significance testing.  Zhang  et al.  (2016)  addressed this issue by generating a null distribution using permutations based on randomly selected lncRNA–miRNA–mRNA triplets. This null distribution was then used to obtain empirical  P -values. We have identified a number of issues with the current approaches that use  scor . First, current correlation-based approaches assume independence between  scor  values and the gene–gene correlation. However, as we show in this work, the distribution of  scor  coefficients is strongly affected by gene–gene correlation ( Fig. 1A  and  Supplementary Fig. S1 ). Thus, previous studies that have used  scor  values have been biased.
 Fig. 1. Overview of the SPONGE workflow. ( A ) Predicted and/or experimentally validated gene–miRNA interactions are subjected to regularized regression on gene and miRNA expression data. Interactions with negative coefficients are retained since they indicate miRNA induced inhibition of gene expression. ( B ) We compute sensitivity correlation coefficients for gene pairs based on shared miRNAs identified in (A). ( C ) Given the sample number, we compute empirical null models for various gene–gene correlation coefficients (k) and number of miRNAs (m). Sensitivity correlations coefficients are assigned to the best matching null model and a  P -value is inferred. ( D ) After multiple testing correction, significant ceRNA interactions can be used to construct a genome-wide, disease or dataset-specific ceRNA interaction network Second, we note that many ceRNAs are regulated by several miRNAs. Neglecting joint contributions, many significant ceRNA interactions may be missed. The CUPID approach considers that ceRNA interactions may be mediated by several miRNAs in conjunction. To accommodate this, CUPID pools  P -values obtained from individual ceRNA triplets ( Chiu  et al. , 2015 ). We propose that the contributions of multiple miRNAs should be part of the ceRNA inference model to optimally account for miRNA covariance effects. Here we present a unified mathematical approach that addresses the above issues. We have developed a Bioconductor/R package called Sparse Partial correlation ON Gene Expression (SPONGE). At the core of SPONGE is a new mathematical framework that is a generalization of  scor  values for more than one miRNA, which we call multiple miRNA sensitivity correlation ( mscor ). Assessing the significance of  mscor  coefficients is difficult due to biases of gene–gene correlation, number of samples and the number of miRNAs. Therefore, we have developed a novel strategy for simulating background distributions that accommodate the aforementioned factors and for inferring  P -values for  mscor  coefficients efficiently. Due to SPONGE’s efficiency, we were able to perform an analysis of the complete human transcriptome across 31 different cancer types combining over 10 000 paired gene and miRNA expression samples using data from The Cancer Genome Atlas (TCGA). Our analysis highlights the potential of ceRNA network inference for hypothesis generation by revealing extensive ceRNA cross-talk. Some of the key regulators have already been reported as ceRNAs while others potentially represent novel biomarkers and drug target candidates. 2 Materials and methods 2.1 SPONGE overview The objective of SPONGE is to infer a ceRNA interaction network from gene and miRNA expression data of paired samples. In theory, inferring a genome-wide ceRNA network with  n  genes entails considering  ( n 2 )  interactions for all pairwise combinations. In practice, only gene pairs with shared miRNAs need to be considered. First, SPONGE identifies for each gene those miRNAs that are likely to have a regulatory effect ( Fig. 1A ). Second, we filter for gene pairs with shared miRNAs and determine their ceRNA interaction scores ( Fig. 1B ). Third, we assess the significance of each ceRNA interaction using a series of null models ( Fig. 1C ) adjusting for confounders. Finally, significant interactions are retained for constructing a ceRNA interaction network ( Fig. 1D ). In the following, we describe each of these steps in detail. Step 1: Identifying relevant miRNA–gene interactions SPONGE identifies relevant gene–miRNA interactions in two stages. First, we retain only miRNA–gene pairs for which we have general evidence from external predictive or experimental sources. SPONGE allows for an arbitrary number of data sources to be combined. Second, we test if the gene and miRNA expression data provides support for these interactions, since we expect many of the putative miRNA–gene interactions in particular to be false positives ( Pinzón  et al. , 2017 ). Negative correlation of gene and miRNA expression can provide evidence for a miRNA–gene regulation. However, many miRNAs might target a single gene. To take this into account, and to identify the most likely miRNA regulators of each gene, we use regularized regression. We build an Elastic net regularized linear regression model with the expression of gene  g  as the dependent variable and the expression of miRNAs  Z ′ ∈ Z  as explanatory variables, where  Z ′  are miRNAs predicted or experimentally shown to target  g . Elastic net balances lasso (L1) and ridge (L2) penalties using a linear combination of both denoted as a weight factor  α . We build a range of Elastic net models to optimize the parameters for  α  = 0.1, 0.2, …, 1.0 and the optimal shrinkage parameter  λ  via 10-fold cross validation using the  glmnet  package ( Friedman  et al. , 2010 ). We select the best model based on the residual sum of squares. Since miRNAs with positive coefficients are likely caused by effects other than miRNA repression, we retain only miRNAs with negative coefficients, which was previously shown to work well ( Muniategui  et al. , 2012 ;  Schulz  et al. , 2013 ). Moreover, SPONGE offers a user-definable coefficient threshold for discarding miRNAs with negligible impact on gene expression (default &lt; −0.05). In summary, we identify for each gene condition-specific miRNA regulators. This leads to a dramatic reduction of gene pairs that share miRNAs ( Fig. 1B ) compared to using all predicted miRNA–gene interactions and reduces the runtime of SPONGE. In the next step, we determine the effect strength of ceRNA interactions. Step 2: Computing sensitivity correlation coefficients In general, the partial correlation  pco r x , y | Z  describes to what extent two variables  x  and  y  are correlated when controlling for one or up to  i  additional variables  Z = z 1 , … , z i .  Paci  et al.  (2014)  proposed to quantify the regulatory contribution of a miRNA in a ceRNA interaction between two genes  g 1  and  g 2  by subtracting the partial correlation achieved when controlling for a single miRNA  m  and refered to this as sensitivity correlation ( scor ):
 (1) scor ( g 1 , g 2 , m ) = cor ( g 1 , g 2 ) − pcor ( g 1 , g 2 | m ) . 
Note that this approach does not account for a combinatorial effect of several miRNAs. Consequently, strong ceRNA interactions mediated by several moderate miRNA regulators cannot be detected. We thus propose to extend the definition of sensitivity correlation considering the effect of multiple miRNAs  M  for the computation of the partial correlation. In this way, we implicitly incorporate the effect of miRNA-miRNA cross-correlation. We call this multiple miRNAs sensitivity correlation ( mscor ):
 (2) mscor ( g 1 , g 2 , M ) = cor ( g 1 , g 2 ) − pcor ( g 1 , g 2 | M ) , 
where  M = m 1 , … , m i  and  i  is the number of shared miRNAs between  g 1  and  g 2 .We compute  mscor  coefficients efficiently using the R package  ppcor  ( Kim, 2015 ). In the next step, we establish the significance of each  mscor  coefficient. Step 3: Sampling from the  mscor  null distribution with respect to important parameters 
 Zhang  et al.  (2016)  proposed to establish the significance of  scor  coefficients by means of sampling a background distribution from random triplets. This approach, however, disregards that correlation coefficients have smaller variance when the coefficient is high ( Fisher, 1915 ). Moreover, it can be expected that the significance of sensitivity correlation values is linked to the number of samples and the number of miRNAs involved. To accommodate these biases, we propose a novel algorithm to study the null distribution of  mscor  coefficients. Our null hypothesis is that the shared miRNAs  M  do not affect the correlation of two genes  g 1 and  g 2. Hence,
 (3) mscor ( g 1 , g 2 , M ) = cor ( g 1 , g 2 ) − pcor ( g 1 , g 2 | M ) (4) 0 = cor ( g 1 , g 2 ) − pcor ( g 1 , g 2 | M ) . 
To be able to sample random  mscor  coefficients under this null hypothesis, we first need to construct random covariance matrices that fulfill these conditions. Briefly, we consider a partitioned expression vector  Z = g 1 , g 2 , m 1 , … , m i  with  m 1 , … , m i ∈ M . The correlation matrix of Z can be expressed as:
 (5) R = ( R 11 R 12 R 21 R 22 ) 
where
 (6) R 11 = ( 1 r 12 r 21 1 ) 
is the correlation matrix between the first two entries of  Z . In order to compute the conditional covariance between the first two entries ( g 1 ,  g 2 ) of  Z  given  ( m 1 , … , m i ) ∈ Z , we compute the Schur complement of  R / R 22  as follows:
 (7) R / R 22 = R 11 − R 12 R 22 − 1 R 12 T = ( 1 − a r 12 − b r 12 − b 1 − c ) . 
where
 (8) a = v 1 T R 22 − 1 v 1 b = v 1 T R 22 − 1 v 2 c = v 2 T R 22 − 1 v 2 
We can obtain the partial correlation  r 12. m  from the conditional covariance as follows:
 (9) r 12. m = ( r 12 − b ) ( 1 − a ) − 1 / 2 ( 1 − c ) − 1 / 2 . 
Our null hypothesis is that  r 12. m = r 12 . Thus
 (10) 0 = ( r 12 − b ) ( 1 − a ) − 1 / 2 ( 1 − c ) − 1 / 2 . 
We have devised sampling strategies that enable us to find values  a ,  b  and  c  such that these conditions are fulfilled, allowing us to construct random covariance matrices under the null. Most importantly, we can control the gene–gene correlation  r 12  and the number of miRNAs (via the dimensions of  R 22 ) to construct a series of covariance matrices with respect to these important parameters. SPONGE uses these covariance matrices to draw random samples which are subsequently used to estimate empirical  P -values for  mscor  values computed on experimental data. The details of this approach and of our sampling strategy can be found in the  Supplementary Material . The SPONGE R package provides precomputed covariance matrices for a range of gene–gene correlations and number of miRNAs. Given the number of samples in the expression data, SPONGE can efficiently construct a series of null distributions from these covariance matrices. Next, we assign each  mscor  coefficient to the closest matching null model and infer its  P -value via its rank in the random distribution ( Fig. 1D ). The number of data points sampled for the null distribution determines the maximal precision of this  P -value ( P &gt; 1 e − 6  by default). Finally,  P -values are adjusted for multiple testing within each null model using the method by  Benjamini and Hochberg (1995) . Step 4: Constructing a ceRNA network We filter ceRNA interactions returned by SPONGE by a user-defined significance threshold (FDR &lt; 0.01 by default) and subsequently construct a ceRNA interaction network  N  = ( V ,  E ), where nodes  V  correspond to genes participating in significant ceRNA interactions and edges correspond to significant ceRNA interactions between two genes. 2.2 Using SPONGE to construct a pan-cancer ceRNA network We downloaded reprocessed TCGA pan-cancer data from the TOIL project ( Vivian  et al. , 2017 ) via the UCSC Xena Browser ( Goldman  et al. , 2018 ). We identified 10 019 samples for which both gene and miRNA expression data were available. Next, we performed log2 transformation and discarded genes and miRNAs not expressed in more than 80% of samples as well as genes and miRNAs with expression variance &lt;0.5. To consider both coding and non-coding miRNA–gene interactions, we downloaded sequence-based predictions of two methods, namely TargetScan ( Agarwal  et al. , 2015 ) (v.7.1, downloaded 10/03/2017) and miRcode ( Jeggari  et al. , 2012 ) (v.11, downloaded 10/03/2017). We included the latter since it also considers non-coding RNAs which have been shown to act as ceRNAs. TargetScan and miRcode predict target genes for miRNA families. We thus downloaded suitable miRNA family definitions for both datasets (available at the TargetScan website  http://www.TargetScan.org/ ). Note that miRcode uses the miRNA family definitions corresponding to TargetScan v.6. After mapping family ids to miRBase mature miRNA ids (MIMATs) we generated integer matrices in which genes are listed as rows and miRNAs are listed as columns. Each entry of the matrix represents the number of binding sites for the corresponding interaction. To consider experimental evidence for miRNA–gene interactions, we obtained datasets from miRTarBase (v.6, downloaded 13/03/2017) ( Chou  et al. , 2016 ) for coding and lncBase (v.2, downloaded 13/03/2017) ( Paraskevopoulou  et al. , 2016 ) for non-coding genes and generated input matrices as described above. Matrices, for gene and miRNA expression and miRNA–gene interactions were analyzed with SPONGE. Significant ceRNA interactions were used to construct the pan-cancer ceRNA network. 2.3 Runtime analysis In order to compare against the runtime of the CMI-based approach of CUPID ( Chiu  et al. , 2015 ), which similarly uses paired gene and miRNA expression to estimate gene–miRNA–gene triplets, we used the JAMI software ( Hornakova  et al. , 2018 ). JAMI is a fast reimplementation of CUPID step III that leverages parallelization. We compared the runtime of JAMI to that of SPONGE (without step1: the regression filter) for a fair comparison. We used a subset of the pan-cancer dataset with 200 genes, which form ca. 80 000 gene–miRNA–gene triplets. We ran both tools with default parameters in parallel mode with 16 cores with varying number of samples and genes. 2.4 Survival analysis For assessing the impact of gene or miRNA expression on the survival probability, we downloaded right-censored TCGA survival data of TCGA patients from the UCSC Xena Browser ( Goldman  et al. , 2018 ). We divided patients into two groups based on the 50% quantile of the expression vector. Survival probability was computed in R using the survfit function in the R package  survival  ( Therneau and Grambsch, 2000 ).  P -values were computed using the function survdiff in the same package. survdiff tests for significant differences of survival curves using the  χ 2  statistic. Kaplan Meier plots were generated using the ggsurv function of package  survminer . To test for the enrichment of survival genes in a list of top candidates ranked by degree, we used the following strategy. First, we computed survival  P -values based on expression data for all genes as outlined above using the survdiff function. Second, we classified genes into survival-associated and -unassociated (background) genes (FDR &lt; 0.001;  Benjamini and Hochberg, 1995 ) for the purpose of enrichment analysis. Third, we computed enrichment of the candidate gene set in survival-associated compared to background using the hypergeometric test in R. 3 Results We have devised a method for the statistical evaluation of condition-specific ceRNA interactions from paired miRNA and gene expression data considering contributions for multiple miRNAs: called multiple miRNA sensitivity correlation ( mscor ).  mscor  is a generalization of  scor  previously defined for one miRNA by ( Paci  et al. , 2014 ) (see Section 2 for details). 3.1 Simulated data reveals dependency of sensitivity correlation on several factors As mentioned above, no theory existed to describe the distribution of sensitivity correlation values ( Paci  et al. , 2014 ). However, we wanted to understand how the  mscor  measure is influenced by confounding factors present in ceRNA relationships: (i) the correlation of two genes, (ii) the number of miRNAs involved in the ceRNA interaction and (iii) the number of samples that are available for estimation. We developed an efficient simulation approach to explore null models in which miRNAs have no effect on the correlation of two genes, hence  mscor  is zero (see Section 2 and  Supplementary Material  for details). Our method is able to compute random covariance matrices that fulfill this null hypothesis. This allowed us to simulate datasets for a range of gene–gene correlation coefficients (0.2–0.9 in steps of 0.1), shared miRNAs (1–8) and number of samples (50, 200, 800) and thus to approximate the random distribution of the  mscor  coefficients under the null hypothesis that  mscor  is zero. 
 Figure 1  and  Supplementary Figure S1  show our simulation results, which reveal that the null distribution is strongly affected by all three tested parameters. Our findings indicate that large  mscor  coefficients are more likely to occur by chance when the gene–gene correlation is low and when the number of miRNAs increases. As expected, it is more difficult to obtain significant  mscor  coefficients with few samples as higher  mscor  values are obtained with smaller samples sizes by chance. Thus, comparing  mscor  values without proper adjustment for these parameters would prioritize low gene–gene correlation pairs, interactions with many miRNAs and lead to a bias when tests between studies with different sample numbers are compared. The above insights led us to develop SPONGE, an R/Bioconductor package to infer ceRNA interactions between pairs of genes. We briefly outline how SPONGE facilitates this in two steps (see Section 2 and  Fig. 1 ). First, we estimate condition-specific miRNA–gene associations from a large set of putative miRNA–gene interactions. This is done using sparse regression of paired gene and miRNA expression data obtained from many samples. Second, ceRNA interactions are predicted using  mscor  values estimated for all gene–gene pairs that share at least one miRNA from the first step. Statistical significance of  mscor  values is efficiently computed using the simulation approach described above. 3.2 Considering multiple miRNAs leads to information gain To demonstrate the advantages of  mscor  measure over  scor , we selected a subset of the TCGA data with 364 liver cancer samples and 1000 randomly selected genes.  mscor  allows us to incorporate multiple miRNAs in the model and thus to detect ceRNA interactions that only become significant when several miRNAs act in concert.  Figure 2A  shows that considering all miRNAs lead in most but not all cases to a higher  mscor  coefficient compared to the individual miRNA with highest  scor . However, when also considering significance (FDR &lt; 0.01), the signal to noise ratio increased and led to a clear gain in information, namely consistently higher  mscor  coefficients for multiple miRNAs. Consequently, SPONGE is able to assess the joint regulatory effect of several miRNAs in a ceRNA relationship in a condition-specific way.
 Fig. 2. Comparison of sensitivity correlation and SPONGE FDR control on liver cancer data. ( A )  mscor  values (y-axis) compared to maximal  scor  values (x-axis) for the same gene–gene interaction. ( B )  mscor P -values obtained from sampling compared to  P -value summarization of  scor  values using Fisher’s method. ( C ) Boxplot of gene–gene correlations for gene–miRNA–gene triplets obtained after selecting the top 5% ceRNA interactions according to the raw  scor  values (orange) or based on FDR corrected  P -values from SPONGE (blue).  t -test  P -value between both distributions is shown on top Our approach correctly adjusts the  P -value to the number of miRNAs involved (see  Fig. 1C ). As CUPID uses a meta-analysis strategy on individual gene–miRNA–gene triplets ( Chiu  et al. , 2015 ) to obtain one  P -value for a set of miRNAs per gene–gene interaction, we sought to compare to such an approach for our measure. We used Fisher’s popular meta-analysis approach to combine  P -values ( Fischer, 1925 ) of individual miRNA triplets.  Figure 2B  shows that aggregated  P -values tend to be considerably higher in meta-analysis, illustrating the loss of information and sensitivity compared to assessing significance in a joint model via  mscor . Our simulation suggested that ranking ceRNA interactions by the  scor  or  mscor  values would introduce a bias towards interactions with low gene–gene correlation (see  Fig. 1C ). In  Figure 2C , we compared the gene–gene correlation values of the top 5% ceRNA interactions sorted according to  mscor  with our FDR corrected set of ceRNA interactions.  Paci  et al.  (2014)  used 5% as an arbitrary cutoff. We observed that SPONGE selected ceRNA interactions showed significantly higher gene–gene correlation values on average ( t -test  P -value &lt; 2.2 e – 16 ) underlining that sorting without proper correction leads to a bias. 3.3 Runtime comparison with a conditional mutual information-based approach CMI is an alternative to partial correlation for estimating the effect and significance of a gene–miRNA–gene interaction. We compared the performance of JAMI ( Hornakova  et al. , 2018 ), a fast implementation of the CMI-based approach of CUPID ( Chiu  et al. , 2015 ), and SPONGE on a subset of the pan-cancer dataset.  Figure 3  illustrates that the SPONGE workflow can be computed fast even for large sample numbers and large number of triplets, while the runtime of JAMI increases dramatically due to the need to rank expression values and due to computationally intensive permutations that are needed for assessing the significance of CMI values. In addition, SPONGE does normally not evaluate each triplet individually, but considers all shared miRNAs in a joint model, giving rise to an additional speedup. SPONGE is thus uniquely suited to infer a genome-wide ceRNA network even on large-scale datasets such as the TCGA pan-cancer data.
 Fig. 3. Runtime comparison between SPONGE and JAMI, a fast method for computing ceRNA interactions based on CMI. ( A ) Runtime for varying number of samples on a fixed set of ca. 80 000 triplets. ( B ) Runtime for varying number of triplets on a fixed number of samples. Time was measured in CPU hours (y-axis) 3.4 The empirical null model allows strict control over the false positive rate To study ceRNA interactions in a pan-cancer setting, we applied SPONGE to paired miRNA and gene expression data for 10 019 samples from TCGA (see Section 2) combining data from 31 cancer types. A comprehensive set of putative miRNA–gene interactions was obtained by combining several sources: sequence-based predictions from TargetScan ( Agarwal  et al. , 2015 ) and miRcode ( Jeggari  et al. , 2012 ) as well as experimentally validated miRNA–gene interactions from mirTarBase ( Chou  et al. , 2016 ) and LncBase ( Paraskevopoulou  et al. , 2016 ). Considering all possible pairwise combinations of genes, ca. 10 9  putative ceRNA interactions can be formed.  Figure 4  shows how the three-step approach of SPONGE reduces this large set of putative interactions. In the first step, condition-specific gene–miRNA interactions are inferred, which reduces the set of considered ceRNA interactions to 10 8 . However, many of these denote spurious ceRNA interactions that do not pass our selected significance threshold ( FDR  &lt; 1 e –5) in the second filter step. Finally, ca. 10 6  significant ceRNA interactions are predicted by SPONGE and used to construct a pan-cancer ceRNA interaction network.
 Fig. 4. Analysis of SPONGE ceRNA interactions on the pan-cancer dataset. Barplots show the number of interactions (y-axis) that are initially analyzed (grey), obtained after the regression filter (Step 1) and after computing  mscor  values and FDR correction of empirical  P -values (Step 3). The analysis is shown for miRNA–gene relationships for which miRNA binding sites (seeds) have been predicted (orange bars) and for a large set of true-negative miRNA–gene relationships, investigating miRNAs without seed matches in a given gene (blue bars) SPONGE estimates ceRNA interaction significance based on simulated null distributions. To determine if this estimation is accurate when applied to real data, we devised a random scenario in which SPONGE should not be able to find significant interactions. We devised a true-negative setting by using only miRNAs as features for a particular gene, which do not have a predicted miRNA binding site in the target gene in any of our considered databases, i.e. miRNAs that have no seed match in the gene (blue bars,  Fig. 4 ). Here only 66 interactions remained significant. Thus, our assumed  FDR  &lt; 1 e –5 appears conservative, which demonstrates the efficacy of SPONGE in filtering for significant miRNA-mediated interactions between genes. 3.5 Pan-cancer ceRNA network analysis After demonstrating that most of the ceRNA interactions in the pan-cancer network are statistically sound, we proceeded with a more detailed analysis. After processing expression data from 60 498 genes and 2463 mature miRNAs, SPONGE reported 95 541 095 gene–gene interactions after step one from which we retained 914 165 at an FDR threshold of 1e−5 ( Fig. 4 ). 16 935 genes participated in ceRNA cross-talk with a median of 29 interactions per gene and a median of six miRNAs per ceRNA interaction with a maximum of 36 miRNAs per interaction.  Table 1  shows the number of genes in different Ensembl gene categories, highlighting that ceRNA interaction is not limited to protein-coding genes with a 3’ UTR. Interestingly, we found a large number of pseudogenes in this pan-cancer analysis, including the two previously reported pseudogenes PTENP1 and BRAFP1 ( Sanchez-Mejias and Tay, 2015 ). Table 1. Number of genes participating in significant ceRNA pan-cancer interactions (FDR &lt; 1e−5) divided by Ensembl gene type Gene type Number of genes Protein coding 12 776 Pseudogenes 1529 lincRNA 1086 Antisense 1025 Processed transcript 207 Sense intronic 69 Sense overlapping 67 We further investigated which microRNAs facilitate ceRNA cross-talk by counting how many interactions they participate in. These results are shown in  Supplementary Figure S2 . Our results highlight that a few miRNAs mediate most of the ceRNA interactions in the network. We observe that these miRNAs have comparably high expression levels, which is in line with what we would expect since ceRNA competition only plays a role if sufficient miRNA copies are present in a cell. The ceRNA network is based on the pan-cancer TCGA dataset which contains cancer as well as tumor-adjacent samples. To identify which of the key ceRNA regulators are associated with cancer, we filtered for genes which showed high mean expression levels (TPM &gt; 100) and were differentially expressed between cancer and tumor-adjacent samples [ t -test, FDR &lt; 0.01 ( Benjamini and Hochberg, 1995 ) and  log 2  fold change &gt; 1]. Our rationale was to determine genes that are present at sufficient copy numbers to exert cell-relevant ceRNA effects concentrated on genes that are overexpressed in the pan-cancer samples, thus likely mediating oncogenic effects. A total of 141 unique genes were obtained using these criteria ( Supplementary Table S1 ). The 10 genes with the highest number of interactions are shown in  Table 2  and in  Figure 5 . Table 2. Top 10 ceRNA regulating genes with highest node degree among genes differentially expressed between cancer and tumor-adjacent samples Ensembl gene id HGNC gene symbol Degree 1 ENSG00000038427 VCAN 1135 2 ENSG00000113810 SMC4 923 3 ENSG00000166851 PLK1 812 4 ENSG00000115414 FN1 698 5 ENSG00000142945 KIF2C 519 6 ENSG00000134013 LOXL2 513 7 ENSG00000141756 FKBP10 481 8 ENSG00000227036 LINC00511 478 9 ENSG00000258947 TUBB3 433 10 ENSG00000106089 STX1A 391 
 Note : The full table with 141 differentially expressed genes is shown in  Supplementary Table S1 . Fig. 5. ( A ) Degree of ceRNA genes with mean expression (TPM &gt; 100) and differential expression between cancer and tumor-adjacent samples (FDR &lt; 0.01 and log fold change &gt; 1). Number of ceRNA interactions (y-axis) is compared to mean expression (x-axis). Differential expression magnitude is shown as color code in the plot. ( B ) The 10 genes with highest degree ranked by their survival analysis  P -value. ( C ) Kaplan Meier survival plot of the non-coding RNA LINC00511 The gene with the largest number of significant ceRNA interactions is  VCAN , which is an established ceRNA ( Sanchez-Mejias and Tay, 2015 ;  Tay  et al. , 2014 ). In fact, previous work has shown that overexpression of the  VCAN  3’UTR sequence alone is able to induce cancer growth in liver cancer cells ( Fang  et al. , 2013 ). Similarly  FN1  is a known ceRNA ( Sanchez-Mejias and Tay, 2015 ). We used clinical data from TCGA to assess if the expression of the genes identified here is significantly associated with survival probability.  Figure 5B  shows that among the top 10 genes, 8 are significant ( P  &lt; 0.05, see Section 2). Among all 141 genes in this analysis ( Supplementary Table S1 ) we find a significant enrichment for survival related genes according to a hypergeometric test ( P   =    3.75e−10) comparing against the background of other genes. An intriguing candidate in this list is the linc-RNA  LINC00511 , which has the highest expression of all non-coding genes in this set and is associated with survival ( Fig. 5C ). Interestingly, a recent paper has shown that  LINC00511  is an oncogenic ceRNA and regulates VEGFA gene expression in pancreatic adenocarcinoma ( Zhao  et al. , 2018 ). Further, it was found that  LINC00511  is a ceRNA for E2F1 and is involved in breast cancer tumourigenesis ( Lu  et al. , 2018 ). Also, it was found that  LINC00511  drives tumourigenesis in non-small-cell lung cancer ( Sun  et al. , 2016 ). This suggests that  LINC00511  is an oncogenic ceRNA that plays an important role in diverse cancer types, as experimental evidence for  LINC00511  mediated ceRNA regulation in two cancers already exists. Thus,  LINC00511  qualifies as an interesting pan-cancer drug target. 4 Discussion We identified two major obstacles that prevent the efficient inference of a comprehensive genome-wide ceRNA interaction network. One of the first approaches, CUPID ( Chiu  et al. , 2015 ;  Hornakova  et al. , 2018 ;  Sumazin  et al. , 2011 ), does not scale to the genome-wide level (see  Fig.3 ) due to the use of permutation-based empirical  P -value computation for establishing significance and the complexity of estimating CMI. Partial correlation-based approaches employing  scor  ( Paci  et al. , 2014 ), on the other hand, are fast but do not accurately determine significance of the estimated effects. To overcome these two issues, we designed an efficient empirical  P -value computation approach by sampling from null models that describe the random distribution of  mscor  values. Moreover, this approach enabled us to accommodate possible biases introduced by several parameters, namely the number of samples, the gene–gene correlation and the number of shared miRNAs. Our results highlight that the current practice of ranking ceRNA interactions by  scor  coefficients introduces a bias towards gene pairs with low correlation, which are also more abundant. Furthermore, it became evident that  scor  cannot be directly compared across datasets with different sample numbers, suggesting that previous studies on unbalanced datasets, where ceRNA network comparisons between cancer and related normal samples were conducted, have likely been biased. We note that null model-based significance analysis is fast, which entails that SPONGE can compute  P -values at high numerical precision ( P  &gt;   1 e −6) compared to permutation-based approaches that often limit precision ( P  &gt;   1 e −3) due to excessive runtime. In this work we have presented a statistical approach to jointly estimate the significance of multiple miRNAs in a ceRNA interaction between two genes. Most genes are regulated by several miRNAs and it can thus be expected that potential ceRNA interaction partners share more than one miRNA between them. This suggests that there is an advantage in considering joint effects of several miRNAs. As far as we are aware only CUPID considers such combinatorial effects when using paired expression data. However, CUPID integrates these effects at the level of triplets, where  P -values of triplets involving the same genes are pooled ( Chiu  et al. , 2015 ), which, as we have shown, results in a loss of sensitivity ( Fig. 2B ). In contrast, our approach captures the contribution of several miRNAs and their co-expression in a single mathematical model. To this end, we extended the concept of sensitivity correlation to multiple miRNA sensitivity correlation ( mscor ). To make this approach broadly available, we developed SPONGE, a R/Bioconductor package which provides a general framework for analyzing sensitivity correlation beyond its current application in ceRNA network inference. SPONGE enabled us to construct the first pan-cancer ceRNA network that systematically infers interactions between all genes within a few days on a typical compute cluster. Notably, close to 16 000 genes are involved in ceRNA regulation. Roughly 12 000 of these are protein-coding genes highlighting that this is a genome-wide phenomenon as proposed by  Salmena  et al.  (2011)  and not limited to non-coding RNAs. However, association may not be confused with causation. We cannot rule out that some of the effects we observe are caused by the activity of the proteins encoded by the tested ceRNA genes. For instance, transcription factors or RNA binding proteins may affect the expression of ceRNA interaction partners directly or indirectly. To further investigate to what extend our results are biased by non-miRNA-mediated regulatory effects, we conducted an  in silico  control experiment where we observed that almost no significant ceRNA interactions remained when miRNAs were tested for which an actual regulation is unlikely as they have no seed match in either of the genes. This suggests that the majority of SPONGE reported ceRNA interactions can be attributed to miRNA-based association. Network analysis in which we focused on genes that show moderate to high average expression and that are differentially expressed between cancer and tumor-adjacent samples revealed ceRNA genes with hundreds of interactions, many of which also show a significant association with survival probability. Our findings suggest that many protein-coding genes auch as VCAN and FN1 have an additional regulatory function as a ceRNA. Moreover, SPONGE suggests ceRNA regulation as a potential mechanism to explain why non-coding RNAs such as LINC00511 have a significant impact on survival. This straight-forward analysis thus illustrates the potential of ceRNA networks for hypothesis generation and biomarker discovery. We note that results might vary depending on the choice and quality of miRNA target interaction databases. To alleviate this issue, we selected datasets based on sequence-based predictions as well as experimentally validated miRNA target interactions. Most of the sequence-based prediction methods focus exclusively on the 3’ UTR of protein-coding genes for detecting miRNA binding sites. Our results indicate that non-coding RNAs make a substantial contribution to miRNA cross-talk such that future miRNA-target annotations should be adapted. It is important to emphasize that statistical significance does not equal biological relevance. While we have ensured that the pan-cancer ceRNA interactions predicted in this work are likely true associations with respect to our model and its assumptions, understanding which of those individual interactions are of physiological relevance, is another important problem. Large-scale validation of ceRNA interactions is challenging and new methods are needed. One interesting approach is the work by Rzepiela et al., in which miRNA target sensitivity values were estimated using mathematical modelling of miRNA overexpression coupled to single cell expression analyses and may provide a way to prioritize ceRNA targets of functional biological relevance ( Rzepiela  et al. , 2018 ). 5 Conclusion and outlook The TCGA pan-cancer analysis performed here provides unique insights into global ceRNA cross-talk in cancer. However, cancer-specific networks will be needed to draw a more comprehensive map of ceRNA regulation where sophisticated network alignment methods are employed to reveal commonalities and differences between cancer types. Generating paired gene and miRNA expression data for healthy tissues in databases like GTEx ( Lonsdale  et al. , 2013 ) will become crucial for gaining an understanding of tissue-specific ceRNA cross-talk which will in turn present a baseline for detecting cancer-specific aberrations in the network presented here. Recently, single cell protocols that facilitate measurements of multi-omics have become available ( Macaulay  et al. , 2017 ). We envision that a protocol supporting parallel measurement of microRNA and gene expression will particularly benefit from fast correlation-based approaches like SPONGE for celltype-specific ceRNA network inference. In many genes, alternative splicing gives rise to a large number of transcripts, many of which differ strongly in their expression. Some of these transcripts are not translated and vary in the miRNA binding sites they carry. Thus, similar to transcripts originating from non-coding genes, they have no apparent biological role but may potentially contribute to ceRNA cross-talk. Considering transcript-level expression data will improve the quality of ceRNA network inference and allow for identifying disease-relevant changes in alternative splicing that act through ceRNA effects. Note that, to our knowledge, we have devised the first generalized algorithm for sampling covariance matrices in which the partial correlation is equal to the correlation. We envision that this might be relevant beyond the inference of ceRNA interaction networks with possible applications in other scientific disciplines. Supplementary Material btz314_Supplementary_Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>appreci8: a pipeline for precise variant calling integrating 8 tools</Title>
    <Doi>10.1093/bioinformatics/bty518</Doi>
    <Authors>Sandmann Sarah, Karimi Mohsen, de Graaf Aniek O, Rohde Christian, Göllner Stefanie, Varghese Julian, Ernsting Jan, Walldin Gunilla, van der Reijden Bert A, Müller-Tidow Carsten, Malcovati Luca, Hellström-Lindberg Eva, Jansen Joop H, Dugas Martin, Hancock John</Authors>
    <Abstract/>
    <Body>1 Introduction Precision medicine is on its way to revolutionizing patient care. Individual therapeutic strategies are increasingly applied to provide every patient with the most suitable treatment. An important aspect for realizing personalized medicine with respect to genetically related diseases, including cancer, is the correct determination and interpretation of mutations ( Ashley, 2016 ;  Dey  et al. , 2017 ). In the course of the last years, this is increasingly done by next-generation sequencing (NGS) ( Park  et al ., 2013 ). Different from Sanger sequencing ( Sanger  et al ., 1977 ), NGS provides a solution for detecting variants with variant allele frequencies (VAFs) below 20% ( Mohamed  et al ., 2014 ). Furthermore, sequencing can be performed consuming only a fraction of time and costs ( Loman  et al ., 2012 ), which enables the analysis of selected target regions as well as a patient’s whole exome or even whole genome. When using NGS it is essential to be able to rely on variant calling results that are valid. Ideally, the analysis pipeline applied in research and particularly also in clinical routine has to feature both high sensitivity and high positive predictive value (PPV). However, all NGS platforms more or less suffer from systematic as well as random sequencing errors ( Bragg  et al ., 2013 ;  Hoffman  et al. , 2009 ;  Liu  et al. , 2012 ;  Yeo  et al. , 2014 ). Previously, we performed re-sequencing experiments involving several patients with myelodysplastic syndrome (MDS) that were sequenced on Illumina NextSeq, Ion Torrent PGM and Roche 454 platforms. These experiments revealed considerable differences in the number of true variants and artifacts reported per sample ( Sandmann  et al ., 2017 ). These differences could be observed when comparing different sequencing platforms as well as when comparing two runs on the same platform. The analysis of two Illumina data sets (HiSeq and NextSeq), covering altogether more than 150 patients with MDS indicated that additional differences in variant calling results can be expected when considering different variant calling tools ( Sandmann  et al ., 2017 ). We considered all currently available open-source variant calling tools for NGS data. However, only 8 out of 43 tools were applicable on our sets of non-matched targeted sequencing data. Evaluation of these eight tools revealed that not a single tool succeeded in detecting all mutations present in the two data sets. Furthermore, no tool showed sensitivity and PPV  ≥ 0.95  for both data sets. Our observations are conform to the results of other studies comparing variant calling tools ( Cornish and Guda, 2015 ;  Hwang  et al. , 2015 ;  Zook  et al ., 2014 ). These studies point out the necessity for a variant calling pipeline that is able to detect variants with both high sensitivity and high PPV—even at low allelic frequencies. Additionally, the pipeline’s performance should be independent of the analyzed data set, not involve re-calibration with new training data in case of new experiments and not include validation of each variant call by Sanger sequencing as proposed for current pipelines by  Mu  et al.  (2016) . Furthermore, application should be possible even in the absence of normal controls, which is a common scenario as pointed out by  Kalatskaya  et al.  (2017) . In this paper, we present  ‘ appreci8 ’ —a Pipeline for PREcise variant Calling Integrating 8 tools. The pipeline automatically performs variant calling of single nucleotide variants (SNVs) and short indels integrating eight open-source variant calling tools. The calls are automatically normalized, combined and filtered on the basis of a novel artifact- and polymorphism score. The scores categorize a variant as either likely pathogenic mutation, polymorphism or artifact. Our tool is applicable to any type of NGS data. To train our pipeline, we analyzed two sets of non-matched targeted sequencing data, covering 165 MDS patients sequenced on Illumina HiSeq, resp. Illumina NextSeq. An intersecting target region of 42 322 bp was considered. Performance of our pipeline was tested analyzing five independent sets of targeted sequencing data, differing from the training sets in varying degrees [sequencing platforms: Illumina HiSeq, HiScanSQ, NextSeq and Roche 454; target region: 42 322–958 547 bp; disease entity: MDS and acute myeloid leukemia (AML)]. Appreci8’s ability to separate true variants from artifacts with allelic frequencies down to 1% was evaluated. We compared our pipeline’s performance to every individual tools’ performance, all possible combined approaches and an alternative version of our pipeline. 2 Materials and methods 2.1 Variant calling pipeline Appreci8 is a completely automatic pipeline for performing SNV and indel calling. An overview of the pipeline is provided in  Figure 1 .
 Fig. 1. Overview of the analysis performed by appreci8 BAM files containing the raw aligned reads per sample form the input for our variant calling pipeline (see  Supplementary Section 3  for information on sequence alignment). Variant calling is automatically performed on eight different tools: GATK 3.3-0 HaplotypeCaller ( DePristo  et al ., 2011 ), Platypus 0.8.1 ( Rimmer  et al ., 2014 ), VarScan 2.3.9 ( Koboldt  et al ., 2012 ), LoFreq 2.1.2 ( Wilm  et al ., 2012 ), FreeBayes 1.0.2 ( Garrison and Marth, 2012 ), SNVer 0.5.3 ( Wei  et al ., 2011 ), SAMtools 1.3 ( Li  et al ., 2009 ) and VarDict ( Lai  et al ., 2016 ). For each caller, the default recommended options are used. The only exception is the VAF threshold in case of FreeBayes and SNVer, which is lowered to 0.01 (default 0.20, resp. 0.25). The resulting raw output per caller is filtered to remove all off-target calls. Subsequently, the remaining calls are combined (see  Supplementary Section 4 ) and annotated using SnpEff ( Cingolani  et al ., 2012 ). The user can choose between an annotation using ENSEMBL ( Aken  et al ., 2016 ) or RefSeq ( O’Leary et al., 2016 ). Furthermore, it is optional to report the annotation of all possible transcripts or just the annotation for selected transcripts. For our training and evaluation of appreci8, we removed all calls that are according to SnpEff located in the 3′-UTR, 5′-UTR, downstream, upstream, intron, intergenic, intragenic, protein–protein contact and in the splice site region (intron_variant+splice_region_variant). Furthermore, silent mutations were removed. By concentrating on coding, non-synonymous variants, we focus our analysis on those variants that are best characterized with respect to biological truth for all data sets considered. However, this filtration is not fixed, but can be adjusted. For all remaining calls, appreci8 determines a first set of characteristics: the number of reference reads (#REF), the number of alternate reads (#ALT), the depth (DP) and the VAF. These characteristics are determined for all reads and for the forward- and reverse reads separately. Furthermore, the mean base quality (PHRED value) for the reference- ( B Q _ r e f ) and alternate allele ( B Q _ a l t ) are determined. As some of the tools apply specific steps of local realignment, all parameters are determined on the basis of the raw alignment data that have already been used for variant calling. Assuming that a decision on a call—whether or not it is true—is only possible in case of sufficient coverage, we remove all calls with number of alternate reads ( # A L T ) &lt;20, depth (DP) &lt;50 or  V A F &lt; 1 % . Furthermore, we remove all calls with  B Q _ a l t &lt; 15  or  B Q _ d i f f = B Q _ r e f − B Q _ a l t &gt; 7  (for details see  Supplementary Section 5.1 ). All parameters may be easily adjusted depending on the sequencing data that are analyzed, e.g. in case of low-coverage whole-genome sequencing (WGS) data. Finally, a second set of characteristics is determined for the remaining calls. These include the results of an automatic check of the databases ESP6500 ( http://evs.gs.washington.edu/EVS/ ), 1000 Genomes ( The 1000 Genomes Project Consortium, 2015 ), dbSNP ( Sherry  et al ., 2001 ) (build 138 and build 138 excluding sites after 129), ExAC ( Lek  et al ., 2016 ), Cosmic ( Forbes  et al ., 2015 ) (CodingMuts, NonCodingVariants, CompleteExport and CompleteExport.fail, 17.02.2016) and ClinVar ( Landrum  et al ., 2016 ) (common and clinical, 03.02.2016; common no known medical impact, 03.02.2016). Additionally, Provean 1.1.5 ( Choi  et al ., 2012 ) is used to determine the influence of every variant on the corresponding protein. Integrating all information characterizing a call, an artifact score—separating true from false positive calls—is calculated. Furthermore, a polymorphism score—identifying likely polymorphisms—is calculated. The general principle of filtration with appreci8 based on these two scores is displayed in  Figure 2 .
 Fig. 2. General principle of filtration with appreci8. Calls are classified as ‘Mutations’, ‘Polymorphism’ or ‘Artifact’ on the basis of an artifact- and a polymorphism score The artifact score separates all calls into two initial categories:  ‘ Potential variants ’  and  ‘ Potential artifacts ’  (see  Supplementary Fig. S3  for details). Subsequently, the polymorphism score is evaluated. It allows for separating  ‘ Possible mutations ’  from  ‘ Possible polymorphisms ’ . However, for the final decision on these calls, the artifact score is reconsidered. It is adjusted on the basis of the polymorphism score as well as call characteristics. This enables the final classification of  ‘ Possible mutations ’  and  ‘ Possible polymorphisms ’ . In addition to separating mutations from polymorphisms, the polymorphism score enables the identification of  ‘ Polymorphisms ’  in the initial set of  ‘ Potential artifacts ’  (see  Supplementary Fig. S4  for details). As some polymorphisms feature characteristics that are typical for artifacts, these calls would be misclassified on the basis of the artifact score alone, but are correctly classified by the combination of both scores. The optimal weighting and combination of the different call characteristics for the calculation of the artifact- and polymorphism score is determined using two training sets (see  Supplementary Sections 5.2 and 5.3  for details). The performance of appreci8 is evaluated analyzing five independent test sets. 2.2 Data sets analyzed To train our variant calling pipeline—appreci8—two well characterized sets of amplicon-based targeted sequencing data are investigated. Both data sets result from MDS patients, covering an intersecting target region of 42 322 bp (19 genes).  ‘ Training set 1 ’  comprises 54 samples sequenced on Illumina HiSeq, using HaloPlex for target enrichment.  ‘ Training set 2 ’  comprises 111 samples sequenced on Illumina NextSeq, using TruSight DNA Amplicon Sequencing Panel Library Prep Kit (see  Table 1 ).
 Table 1. Main characteristics of the training- and test sets analyzed with appreci8 Set n Sequencer Disease Target [bp] Coverage Background all Coding &gt;50x (%) noise Training 1 54 Illumina HiSeq MDS 42 322 23 162 95 5.39 · 10 − 3 2 111 Illumina NextSeq MDS 42 322 23 162 97 6.26 · 10 − 3 Test 1 237 Illumina HiSeq MDS 42 322 23 162 92 4.15 · 10 − 3 2 46 Illumina HiSeq MDS 42 322 23 162 93 5.02 · 10 − 3 3 89 Roche 454 MDS 42 322 23 162 84 3.63 · 10 − 3 4 22 Illumina NextSeq AML 125 459 78 866 99 6.63 · 10 − 3 5 119 Illumina HiScanSQ AML 958 547 218 179 94 1.56 · 10 − 3 To test the performance of appreci8 on independent data, we consider five additional sets of amplicon-based targeted sequencing data:  ‘ Test set 1 ’  covers Illumina HiSeq sequencing data (using HaloPlex for target enrichment) from 237 MDS patients.  ‘ Test set 2 ’  covers Illumina HiSeq sequencing data (using HaloPlex for target enrichment) from 46 MDS patients.  ‘ Test set 3 ’  covers Roche 454 ( Janitz, 2008 ) sequencing data (using GS FLX Titanium SV emPCR Kit Lib-A) from 89 MDS patients. In case of these three test sets the same target region is analyzed as in the two training sets.  ‘ Test set 4 ’  covers Illumina NextSeq sequencing data (using TruSight DNA Amplicon Sequencing Panel Library Prep Kit) from 22 patients with acute myeloid leukemia (AML). Different from the first three test sets, a larger target region comprising 125 459 bp is analyzed in this case.  ‘ Test set 5 ’  covers Illumina HiScanSQ sequencing data (using HaloPlex for target enrichment) from 119 patients with AML. Again, a larger target region comprising 958 547 bp is analyzed. In case of all data sets, patient material was collected and analyzed in accordance with the relevant ethical guidelines and regulations. Informed consent was obtained from all subjects. Sequencing data (BAM files) of the 678 patients have been deposited into the NCBI Sequence Read Archive (BioProjectID: 388411;  https://www.ncbi.nlm.nih.gov/bioproject/PRJNA388411 ). We decided to choose these data sets for training and testing, as they are all well characterized with respect to biological truth. Furthermore, the sets allowed us to investigate if appreci8’s performance is dependent on the sequencing technique, the target region and the disease that is considered. 2.3 Validation For the initial training of appreci8 and its subsequent evaluation, we only consider data sets with validated mutations. Validation was achieved using three different approaches: (i) a selected set of calls (mutations, polymorphisms and artifacts) was validated using Sanger sequencing. However, as variants with a VAF below 20% are difficult to confirm with this sequencing technique, (ii) we re-analyzed a subset samples by the same or another technique as validation. Six samples were re-analyzed on Illumina NextSeq. Nine samples were re-analyzed on Ion Torrent PGM ( Rothberg  et al ., 2011 ). Twenty-two samples were analyzed on Roche 454 and Illumina NextSeq. NPM1 mutations were validated by LightCycler (Roche, Mannheim, Germany) based melting curve analysis ( Schnittger  et al ., 2005 ). As additional validation (iii), all calls reported in case of the two training sets were manually investigated by two independent experts. The variant-specific characteristics as well as the calls themselves in the IGV ( Robinson  et al ., 2011 ) were considered. In case of the five test sets, all calls categorized as true were manually investigated. Furthermore, variant-specific characteristics of all calls categorized as polymorphisms and artifacts were investigated. 3 Results A variant calling pipeline’s main task is successfully calling true variants with high sensitivity and automatically discarding artifacts. Variants themselves can be subdivided into benign variants that are present in the general population, i.e. germline single nucleotide polymorphisms (SNPs) and indel polymorphisms, and possibly pathogenic variants, i.e. SNVs and indels. In cancer, most pathogenic mutations are somatically acquired and tumor cell specific. While the correct classification of polymorphisms versus pathogenic mutations is in some cases straight forward, it may often prove to be challenging due to lack of germline material, variants of uncertain clinical significance or subclonal variants. When considering a complete ClinVar ( Landrum  et al ., 2016 ) export (August 2, 2016), the list contains 130 097 variants in total. About 32.63% are classified as variants of  ‘ uncertain clinical significance ’  (24.02%) or variants with information on clinical significance  ‘ not provided ’  (8.61%). Only 9.66% are classified as  ‘ benign ’ , 20.31% are classified as  ‘ pathogenic ’ . For these reasons, we consider the automatic separation between artifacts and true variants as the main task of our pipeline. The automatic separation between benign and pathogenic variants is considered an add-on and is presented in the supplement (see  Supplementary Section 10 ). 3.1 Training appreci8 To train our variant calling pipeline, we use two well characterized NGS data sets. Although both sets are derived from patients with the same disease—MDS—and the same target region is analyzed, they differ in the enrichment technologies and sequencing platforms. Therefore, we expect to find different characteristics for both data and also variant calls. 
 Supplementary Table S1  shows that both training sets differ considerably in their main data characteristics. While training set 2 features higher mean coverage, the set is also characterized by 16% more background noise compared to training set 1. This characteristic is expected to have negative influence on PPV (see  Supplementary Section 9  for information on how background noise was calculated). Variant calling results with respect to sensitivity and PPV are displayed in  Figure 3  and summed up in  Table 2  (for details see  Supplementary Tables S3 and S4  and  Supplementary Data S1  and  S2 ). The performance of every single tool—GATK, Platypus, VarScan, LoFreq, FreeBayes, SNVer, SAMtools and VarDict—is compared to appreci8. Additionally, we evaluate two alternative approaches in detail:  ‘ 8 tools ’  considers all variants that have been reported by at least one out of eight tools and no further steps of filtration.  ‘ Single-appreci8 ’  is an experimental variant of our algorithm. Every sample is evaluated independently. Any information on other samples analyzed in the same run is disregarded (see  Supplementary Sections 6 and 7  for details on the two alternative approaches). In all cases, we only consider calls with sufficient coverage and  V A F ≥ 1 % , as we assume that validation of variants with lower VAFs or insufficient coverage is not feasible without further sequencing experiments.
 Table 2. Positive predictive value and sensitivity in case of GATK, Platypus, VarScan, LoFreq, FreeBayes, SNVer, SAMtools, VarDict, the combined output of all tools (eight tools), single-appreci8 and appreci8 in training sets 1 and 2 Approach Training set 1 Training set 2 Sens PPV Sens PPV GATK 0.92 0.85 0.82 0.71 Platypus 0.93 0.80 0.83 0.42 VarScan 0.89 0.97 0.47 0.73 LoFreq 0.91 0.35 0.78 0.23 FreeBayes 1.00 0.03 0.99 0.02 SNVer 0.93 0.92 0.55 0.07 SAMtools 0.85 0.87 0.64 0.77 VarDict 0.97 0.96 0.94 0.15 8 tools 1.00 0.03 1.00 0.02 single-appreci8 0.98 0.98 0.99 0.35 appreci8 0.98 0.99 0.98 0.94 Fig. 3. Relation between positive predictive value and sensitivity in case of GATK, Platypus, VarScan, LoFreq, FreeBayes, SNVer, SAMtools, VarDict, the combined output of all tools (eight tools), single-appreci8 and appreci8 in training sets 1 and 2 
 Figure 3  illustrates the data-dependent performance of the eight individual variant calling tools. While all tools succeed in calling variants with sensitivity above 0.80 in case of training set 1, only four tools—GATK, Platypus, FreeBayes and VarDict do so in case of training set 2. However, when aiming for sensitivity of at least 0.95, which appears to be a more apt threshold for usage of NGS in clinical routine, only two tools—FreeBayes (0.99) and VarDict (0.97)—succeed in case of training set 1 and only one tool—FreeBayes (0.99)—succeeds in case of training set 2. While sensitivity is an essential aspect of any variant calling pipeline, so is PPV. False positive mutations can have serious consequences for the treatment of a patient. Furthermore, using a tool that reports thousands of calls per patient increases the risk of overlooking the actual true mutations among the many artifacts. Regarding training set 1, six out of eight tools feature PPV above 0.80 (VarDict performs best with  PPV  = 0.96). FreeBayes, however, shows the lowest PPV (0.03). The tool reports 290 true variants and 8040 artifacts. Regarding training set 2, not a single tool reaches a value of 0.80 or above. FreeBayes reports 627 out of 631 true variants and 40,159 artifacts ( PPV  = 0.02). When combining the output of all tools, we succeed in calling all variants present in both training sets ( sens  = 1.00). This is not possible with any of the considered individual tools. However, as PPV is 0.03 in case of training set 1 and 0.02 in case of training set 2, the need for filtration is obvious. Application of our appreci8 pipeline leads to a considerable increase in PPV—to values higher than any of the individual tools—while sensitivity is only marginally reduced. Comparing single-appreci8 to appreci8, only minor differences can be observed in case of training set 1. As regards training set 2, the application of appreci8 leads to a considerable improvement in the results ( PPV  = 0.35 versus  PPV  = 0.94). Evaluation of the artifact- and polymorphism score removes 1% of the artifacts in training set 1 and 21% in training set 2. 3.2 Testing appreci8 To test appreci8, we consider five independent, well characterized data sets (for data characteristics see  Supplementary Table S2 ). Variant calling results with respect to sensitivity and PPV are displayed in  Figure 4  and  Table 3  (for details see  Supplementary Tables S5–S9  and  Supplementary Data S3 – S7 ).
 Table 3. Positive predictive value and sensitivity in case of GATK, Platypus, VarScan, LoFreq, FreeBayes, SNVer, SAMtools, VarDict, the combined output of all tools (eight tools), single-appreci8 and appreci8 in test sets 1–5 Approach Test set 1 Test set 2 Test set 3 Test set 4 Test set 5 Sens PPV Sens PPV Sens PPV Sens PPV Sens PPV GATK 0.92 0.82 0.90 0.73 0.95 0.31 0.86 0.73 0.92 0.94 Platypus 0.93 0.81 0.93 0.83 0.97 0.13 0.91 0.30 0.90 0.94 VarScan 0.84 0.84 0.82 0.73 0.91 0.51 0.73 0.55 0.71 0.94 LoFreq 0.98 0.29 0.98 0.71 0.94 0.31 0.73 0.01 0.79 0.88 FreeBayes 0.99 0.02 1.00 0.01 0.99 0.07 0.95 0.01 0.96 0.25 SNVer 0.91 0.91 0.94 0.81 0.97 0.10 0.64 0.04 0.73 0.98 SAMtools 0.82 0.83 0.81 0.75 0.93 0.68 0.70 0.53 0.86 0.99 VarDict 0.96 0.78 0.99 0.30 0.99 0.25 0.91 0.09 0.97 0.83 8 tools 1.00 0.02 1.00 0.01 0.99 0.05 0.99 0.01 1.00 0.25 single-appreci8 0.99 0.88 1.00 0.76 0.99 0.36 0.93 0.19 0.96 0.97 appreci8 0.98 0.99 1.00 0.99 0.99 0.76 0.93 0.65 1.00 1.00 Fig. 4. Relation between positive predictive value and sensitivity in case of GATK, Platypus, VarScan, LoFreq, FreeBayes, SNVer, SAMtools, VarDict, the combined output of all tools (eight tools), single-appreci8 and appreci8 in test sets 1–5 Test sets 1 and 2 result from the same sequencing platform as training set 1. Furthermore, the same target region and disease is considered as is the case for both training sets. Therefore, we expect all tools as well as our pipeline to show results comparable to training set 1. When comparing  Figures 1  and  2 , it is clear that the results are indeed comparable. Sensitivity of the individual variant calling tools is above 0.80. FreeBayes, VarDict and LoFreq succeed in calling variants with sensitivity above 0.95 (test set 1: FreeBayes: 0.99, VarDict: 0.96, LoFreq: 0.98; test set 2: FreeBayes: 0.99, VarDict: 0.99, LoFreq: 0.98). Six out of eight tools feature PPV above or close to 0.80. Again, FreeBayes shows the lowest PPV (test set 1: 0.02; test set 2: 0.01). In contrast to training set 1, VarDict’s PPV is only 0.78 in test set 1 and 0.30 in test set 2. The combined output of all tools leads to  sens  = 1.00 in case of both test sets.  PPV  = 0.02 for test set 1 and  PPV  = 0.01 for test set 2. Application of single-appreci8 leads to a minor decrease in sensitivity and to a considerable increase in PPV. Both test sets show a further clear improvement of the results when applying appreci8 in its actual functionality (test set 1:  sens  = 0.98,  PPV  = 0.99; test set 2:  sens  = 1.00,  PPV  = 0.99). Evaluation of the artifact- and polymorphism score removes 2% of the artifacts in test set 1 and 9% in test set 2. To test appreci8’s robustness with respect to variation in the sequencing technique, we analyzed Roche 454 data (test set 3), although the pipeline was exclusively trained on Illumina data. Regarding the individual tools, sensitivity ranges between 0.91 and 0.99, while PPV ranges between 0.07 and 0.68. By combining the output of all variant calling tools, sensitivity increases to 0.99, while PPV is 0.05. Application of single-appreci8 leads to an improvement in the results. The overall best results can, however, be observed when applying appreci8 ( sens  = 0.99,  PPV  = 0.76). Remarkably, 97% of the artifacts are filtered because of their artifact- and polymorphism score. To test appreci8’s robustness when considering a bigger target region and a different disease entity, we analyzed test sets 4 and 5. Both data sets result from patients with AML and cover a considerably bigger target region (test set 4: 125 459 bp, test set 5: 958 547 bp in comparison to 42 322 bp for the MDS training sets). Test set 4 underlines the data-dependent performance of individual variant calling tools. Low sensitivity (ranging between 0.64 and 0.95) and low PPV (ranging between 0.01 and 0.73) is observed for the eight tools. Combining the output of all tools leads to sensitivity of 0.99 and a PPV of 0.01. Application of single-appreci8 leads to a minor decrease in sensitivity and an increase in PPV. Application of appreci8 leads to further improvement of the variant calling results ( sens  = 0.93,  PPV  = 0.65). Considering test set 5, FreeBayes and VarDict feature again highest sensitivity of all individual variant calling tools (FreeBayes: 0.95, VarDict: 0.96, other tools: 0.71–0.91). Higher values of PPV can be observed in comparison to all previously analyzed data sets. Even FreeBayes features PPV of 0.25 (other tools: 0.81–0.99). The combined output of all tools leads to  sens  = 0.99 and  PPV  = 0.25. Application of single-appreci8 results in a minor decrease in sensitivity and to a considerable increase in PPV. Application of appreci8 leads to the best variant calling results that can be observed for this data set ( sens  = 0.99,  PPV  = 0.99). Analyzing the influence of reoccurring variants, it can be observed that appreci8 performs equally well—detecting variants known from the training sets as well as unknown variants (see  Supplementary Section 13 ). 4 Discussion In the context of high-throughput research sequencing and personalized medicine, NGS provides a powerful tool. When considering variant calling results, it is therefore essential to be able to rely on a tool with stable and high sensitivity as well as high PPV. However, the analysis of seven data sets of non-matched amplicon-based targeted sequencing data, covering 678 samples from patients with hematological malignancies, shows that no individual tool meets these requirements. We developed a pipeline, appreci8, that automatically combines and filters the variant calling results of eight different tools. Appreci8 succeeds in separating true calls from artifacts in all analyzed data sets with sensitivity ranging between 0.93 and 1.00 and PPV ranging between 0.65 and 1.00. Appreci8’s performance is in all cases superior to the best individual tool as well as to alternative combined approaches. Application of appreci8 on additional Illumina data sets (HiSeq, MiSeq and NextSeq), which were not part of this study (data available on request) as well as Ion Torrent data (see  Supplementary Section 11 ), shows comparable results. Application of appreci8 on a public targeted sequencing data set (Sequence Read Archive, project PRJEB14077) shows comparable results as well (see  Supplementary Section 12 ). Our results indicate that appreci8 is a pipeline that can generally be applied on NGS data, independent of the sequencing technique, the disease entity or the genes that are studied. It should also be noted that appreci8 is able to automatically exclude polymorphisms, while all the other individual tools require manual filtration of polymorphisms or—in case of GATK—filtration on the basis of predefined polymorphism files. It can be discussed, why we chose exactly this set of variant calling tools. Altogether, we used all available open-source tools that could be applied on our sets of non-matched targeted NGS data. However, performing variant calling with eight instead of one tool has negative influence on run-time if a high-performance server is not available. Furthermore, post-processing—which is inevitable—is time-consuming as well. We cannot exclude the possibility that seven or even less tools might also lead to acceptable results—dependent on the analyzed data set. However, for the calculation of the artifact- and the polymorphism score, the number of callers and even the specific callers that detect a variant are important characteristics. Decreasing the number of tools would thus have negative influence on appreci8’s overall performance. Analysis of the overlapping calls, reported by two to eight tools, indicates that a mere combination of tools—even those with different variant detection algorithms—would not be beneficial (see  Supplementary Section 8 ). It can also be questioned, why we chose our set of 41 conditions that are evaluated to calculate the artifact- and the polymorphism score. On the one hand, our selection covers all classical characteristics that are considered when separating true from false positive calls, e.g. coverage, base quality and strand bias. On the other hand, we additionally consider novel characteristics, e.g. the number of tools calling a variant or the Provean score. Altogether, our categories represent the joined experience of biological and bioinformatical experts. Instead of being a black box, our algorithm aims at comprehensibly reproducing a biologist’s manual work when investigating a raw list of calls. The weights we assigned to the different conditions were determined exploratively to optimize performance of appreci8 in case of the two training sets. The results we observe regarding the independent test sets are comparable. Still it is possible that another weighting or evaluation of the conditions might have led to even better results. Alternatively, we could have used decision tree learning or estimated a model, e.g. a generalized linear model. The parameters—the conditions in our case—and their weights would have been selected with the help of a model selection approach and an information criterion ( Sandmann  et al ., 2017 ). However, the determined model or decision tree would be exclusively based on the two training sets. On the contrary, our approach additionally considers long-time experience of molecular biology experts. Another approach would be to apply deep learning to estimate the best model, like  Esteva  et al.  (2017)  did. However, this approach has the disadvantage of being another black box. Manual adaptation of the parameters or their weighting based on e.g. new experience or updates of the data bases is not possible. On purpose, we did not include any platform specific characteristics or filtration steps. Appreci8 was developed as a thoroughly automatic pipeline that does not require data-dependent re-calibration. Still—if desired—a user has the possibility to adjust thresholds, e.g. regarding coverage if low-coverage WGS instead of high-coverage targeted sequencing data is analyzed. Furthermore, we did not consider any tool-specific filtration steps or changes in the tool-specific parameters for variant calling. These might have significant influence on the different tools’ sensitivities and PPVs. However, GATK is the only tool that proposes precise thresholds for filtration. All the other tools do not. Additionally, no tool provides data-dependent recommended configurations for variant calling. Due to the high number of possible configurations, we decided to treat all callers equally, not apply any tool-specific filtration steps and stick to the default options for variant calling. An essential aspect of our analysis is the correct classification of all variant calls. We did not validate every single call by re-sequencing the corresponding sample on the same or another platform. Instead, a majority of calls were validated by expert-based review. It could be argued that this approach may have led to mistakes. However, we were facing more than 180 000 calls in total, i.e. on average almost 270 calls per patient. By performing re-sequencing experiments in case of exemplary mutations, polymorphisms and artifacts, we showed that our classification was indeed correct. Furthermore, expert-based review involved evaluation of various databases, base qualities, coverage, allelic frequencies, the predicted effect on protein level and manual inspection using IGV, considering the sample in question and other samples in the same run. For these reasons, we estimate the risk of mistakes in the classification to be low and not to influence our overall results. It can be observed that even when using appreci8, sensitivity and PPV are still lower than 1.00. Detailed analyses of the false positive- and false negative calls reveal that many feature an artifact score between −1 and 1 (threshold for true positive calls: −1). This observation suggests that additional analyses of calls with an artifact score close to the threshold could be useful. We are currently testing an approach evaluating the signal-to-noise ratio as described by  Kockan  et al.  (2017)  in combination with base qualities. Regarding run-time, we are currently testing a speed-up version of our pipeline that improves the analysis of whole-exome sequencing and WGS data. Previous analyses have shown that it is not advisable to use the multi-threading modes of the variant calling tools ( Sandmann  et al ., 2017 ). While germline samples can already be analyzed with appreci8, automatic filtration of germline calls by matched sample analysis is not yet available. We are currently investigating an algorithm for automatic filtration of germline calls from matching, as well as pooled control samples with our appreci8 pipeline. As soon as this approach is available, it will be interesting to compare appreci8 with popular tools for matched sample analysis, e.g. MuTect2 ( Cibulskis  et al ., 2013 ). Considering the ongoing development in the field of variant calling, it appears useful to make appreci8 more flexible with respect to the variant calling tools that are considered. We are currently testing an extension of appreci8 that allows the user to select his own set of variant calling tools to consider. Tool versions and configurations used for variant calling will be user-definable. A graphical user interface will be provided to facilitate application of appreci8. 5 Conclusion To consider variant calling results in research and in clinical routine, it is necessary to have a tool with stable, high sensitivity as well as high PPV. However, the analysis of seven data sets, covering 678 samples from patients with hematological malignancies, shows that no individual tool meets these requirements. We developed a pipeline, appreci8, that combines and filters the variant calling results of eight different tools. Appreci8 succeeds in separating true calls from artifacts in all analyzed data sets with sensitivity ranging between 0.93 and 1.00 and PPV ranging between 0.65 and 1.00. Appreci8’s performance is in all cases superior to the best individual tool. Supplementary Material Supplementary Data S1 Click here for additional data file. Supplementary Data S2 Click here for additional data file. Supplementary Data S3 Click here for additional data file. Supplementary Data S4 Click here for additional data file. Supplementary Data S5 Click here for additional data file. Supplementary Data S6 Click here for additional data file. Supplementary Data S7 Click here for additional data file. Supplementary Data S8 Click here for additional data file. Supplementary Data S9 Click here for additional data file. Supplementary Data S10 Click here for additional data file. Supplementary Data S11 Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>SciApps: a cloud-based platform for reproducible bioinformatics workflows</Title>
    <Doi>10.1093/bioinformatics/bty439</Doi>
    <Authors>Wang Liya, Lu Zhenyuan, Van Buren Peter, Ware Doreen, Hancock John</Authors>
    <Abstract/>
    <Body>1 Introduction Key challenges in big data analysis include, but are not limited to, access to sufficient computing resources, massive data transfer, standardized analysis workflows and reproducibility. To address these challenges, we built a workflow platform by using infrastructure components of the CyVerse project ( Goff  et al. , 2011 ). The CyVerse project, funded by the National Science Foundation of United States, aims to design, deploy, and expand a national Cyber-Infrastructure (CI) for life science researchers and to train scientists in its use. Two major components of the CyVerse CI, the Data Store built on top of iRODS (rule-oriented data system) ( Moore and Rajsekar, 2010 ) and the Agave platform ( Dooley  et al. , 2012 ), were used in building the SciApps platform. Specifically, on the computing side, we registered a local CSHL cluster as an execution system in the Agave platform. The Agave platform or API was designed to be an open-source, platform-as-a-service solution for hybrid cloud computing. It provides a full suite of services to handle the complete cycle of analysis job management, including defining the interface between the scientific application (or app) and the execution system on which the app is executed through an app JSON file, job submission and monitoring, data transfer, archiving of analysis results, and many other aspects. Through the Agave API, SciApps applications that execute on the CSHL cluster as defined by the app JSON are available automatically in the Discovery Environment of CyVerse (or DE). Simply updating the execution system in the app JSON file switches from the CSHL cluster to a cloud or vice versa. SciApps workflows can be constructed with apps that execute on a local cluster or a cloud like XSEDE ( Towns  et al. , 2014 ). For both cases, to avoid transferring massive amount of data across different sites, the workflow engine keeps all intermediate results close to the cluster and copies them to the final location/site only after the entire workflow is completed. On the data side, SciApps supports retrieval of data from the CyVerse Data Store. The CyVerse Data Store provides researchers with reliable access to secure cloud-based storage. CyVerse users get 100 GB storage space at registration, and this limit can be increased upon request. 2 Architecture SciApps consists of a web interface for executing apps and building workflows, a workflow engine for execution of workflow, a storage server, a computing cluster, and a web server for various visualization services ( Supplementary Fig. S1 ). It is designed for leveraging cloud-based storage and computing systems through Agave Science API. SciApps’s local storage and computing systems are set up as a remote CyVerse system for handling the analysis of large-scale datasets locally and more efficiently ( Wang  et al. , 2015a ). 2.1 Web interface The SciApps web interface has four areas ( Fig. 1 ). Apps in the left panel are searchable by names and categorized according to the EDAM ontology ( Ison  et al. , 2013 ). The app search function is interactive: when any letters are typed into the search box, categories with a matched app or apps will be expanded with matched app names. Clicking on an app will bring up the app form in the main panel, along with a short description of the app below the form. The app form is loaded in the main panel with default (or previously used) inputs and parameters (if reloaded from the history panel), and once submitted, the analysis/job history is displayed in the history panel, and can be selected for building a workflow. The history panel only displays outputs predefined in the app JSON file, as shown for Step 5 ( Fig. 1 ). If the user aims to build a workflow, these predefined outputs are the only ones that should be used as inputs for subsequent analysis tasks. More details on how to use SciApps interface to build and run workflows are provided in the SciApps platform guide ( https://www.sciapps.org/page/help ). Fig. 1. SciApps web interface. The interface, in which users perform data analyses and build workflows, has four areas: the navigation bar, containing workflow functionalities (building, loading, public and private workflows), a link to example data, help, and login for CyVerse authentication; the app panel (left column) for categorized apps (with the Clustering category clicked and expanded); the main panel (middle column) for app form(s) or workflow builder form; and the history panel (right column) for job name followed by three icons: checkbox for building a workflow from executed jobs, job history ( i ), and job re-launch. Here, a seven-step association workflow is loaded in the history panel, the app form of the fourth step is re-loaded in the main panel, and the results from Step 5 are clicked and expanded in the history panel. As an example, Steps 4–6 are checked (for building a new workflow) 2.2 Authentication For access to multiple cloud-based systems, SciApps adopts the CyVerse Central Authentication Service (CAS). When a user logs in, they are directed to the CyVerse portal, where they enter their username and password; upon successful authentication, they are redirected back to SciApps. The CyVerse username is captured by SciApps for two uses: First, it directs SciApps to user’s sci_data folder in CyVerse Data Store, as shown in  Supplementary Figure S2  for user lwang; Second, once an analysis job is submitted, the job is shared with the user through the Agave API so that, if needed, user can check the detailed job information through Agave’s web interface ( https://togo.agaveapi.co/ ). Sharing is required since SciApps uses a designated CyVerse user account (a ‘superuser’) to execute apps and workflows. The superuser also gains full access to each user’s sci_data folder once the folder is automatically created, immediately after the user enables the SciApps service in the CyVerse user portal ( https://user.cyverse.org ). In other words, SciApps adopts a platform-centric approach by designating a superuser for managing the entire analysis cycle. 2.3 Workflow To create a workflow, each analysis job is submitted, recorded and accessed through the web portal. Part or all of a series of recorded jobs can be saved as reproducible, sharable workflows for future execution with the original or modified inputs and parameters. When a user submits an analysis task, the job is recorded in the history panel, and a workflow is built by selecting two or more jobs using the check boxes ( Fig. 1 ). The input/output relationships among individual tasks are built by tracing the origin of intermediate output, which is available in the job history metadata of the Agave API. Once built, a graphic workflow diagram is shown for verification (e.g. see  Supplementary Fig. S3 ). After visual inspection, users can save the workflow from the diagram, and then download the workflow JSON file from ‘My workflows’ page. The workflow diagram is interactive. Users can mouse over both data and app nodes to check related metadata and full names (long names are truncated in the diagram), and click for more metadata (input nodes), opening or downloading results (other data nodes), and full documentations (app nodes). For a running workflow, the diagram provides real-time job status updates by automatically updating the color of the app node ( Supplementary Fig. S4 ). Alternatively, the status of individual jobs can also be obtained by clicking on the info icon. As shown in both  Supplementary Figures S3  and  S4 , SciApps workflows are implemented as directed acyclic graphs. The execution of a step is only dependent on the availability of its required input(s), making it possible to exploit parallelism. For example, in  Supplementary Figure S4 , two jobs are running at the same time (app nodes in blue) because their required inputs are available simultaneously. For one User (A) to share an analysis with another User (B), User A can build a workflow from the analysis jobs, download a lightweight workflow JSON from SciApps, and send it to User B. User B can then load the workflow JSON on SciApps to check all inputs, intermediate data, parameters, final results and related metadata. Through SciApps’ platform-centric approach, User B can also run the same workflow with modified inputs and parameters without any permission issues because jobs are submitted by the superuser, who has gained access to all of the workflow components either through sharing (apps, systems, sci_data folder) or being the owner (analysis jobs). If User B just wants to re-run a subset of the analysis, a new workflow can be built by selecting a subset of analysis jobs ( Fig. 1 ). Alternatively, User B can also add more steps to the workflow by launching new jobs with any outputs from the workflow. For example, it is straightforward to build a three-pass workflow from a two-pass annotation workflow. In other words, any SciApps workflows can serve as a template for building new workflows. 3 Use cases Two example use cases, one for association and one for annotation analysis, demonstrate the major features of SciApps. 3.1 Annotation The workflow presented in this use case study implements a three-pass iterative annotation pipeline with two apps, MAKER ( Holt and Yandell, 2011 ), a portable genome annotation pipeline with an integrated suite of gene prediction tools, and SNAP ( Korf, 2004 ), a Semi-HMM-based Nucleic Acid Parser. In this workflow, MAKER annotation results for novel genomes without much prior knowledge of gene models are used to estimate HMM parameters with SNAP. The estimated parameters are then supplied to MAKER for re-annotation. The pipeline iterates MAKER and SNAP several times for a three-pass annotation ( Supplementary Fig. S3 ) to improve the final gene models. The MAKER app has two outputs, annotation result in GFF format (my.all.gff.gz), which is also the input of SNAP for HMM parameter estimation, and maker_output.jbrowse, which points to a JBrowse ( Skinner  et al. , 2009 ) view of the annotation result and evidences ( Supplementary Fig. S5 ). The user can visualize annotation results from each pass on JBrowse and compare them to determine whether annotation results have been improved by additional passes. 3.2 Association A genome-wide association study is an examination of a genome-wide set of genetic variants in a population of individuals aimed at determining whether any variant is associated with a trait.  Supplementary Figure S4  shows an association workflow with individual components previously built in the DE ( Wang  et al. , 2015b ). From left to right, trait data and marker data are intersected by accession ID (or sample ID) with the MergeG2P app; missing markers in merged marker data (m_marker.txt) are imputed with NPUTE, an app based on the nearest-neighbor algorithm ( Roberts  et al. , 2007 ); and then the imputed marker file (imputed.txt) is used by the PCA app to estimate population structure or converted for use in association studies with three mixed-model analysis methods, EMMAX ( Zhou and Stephens, 2012 ), MLM-TASSEL ( Zhang  et al. , 2010 ), and MLMM ( Segura  et al. , 2012 ). For direct visualization of the association results from each mixed model, a web-based interactive Manhattan plot ( Supplementary Fig. S6 ) was built using the Shiny framework ( Chang  et al. , 2015 ). List of nearby genes can be retrieved from Gramene ( Ware  et al. , 2002 ) by clicking on the plot. 4 Implementation The back end of SciApps was built using Perl and the MySQL database engine, and the front-end was built with React, an open-source JavaScript library. The workflow engine uses the database to track job status and perform the submission of a subsequent job once its inputs are ready. Most components of the SciApps web interface are rendered from JSON data, including the app category and app list in the left panel, app form in the main panel, history in the right panel, and the workflow forms and diagram. The schemas of all JSON data are custom-designed for fast rendering, with the exception of the app JSON schema, which is adopted from the Agave API. In addition to defining inputs and parameters for rendering the app form, the app JSON specifies the system where the app will be executed. This makes it possible for SciApps to leverage the Agave API for job management on both local and cloud-based systems. To add a new app, storage, or execution system, users can follow the CyVerse tutorial ( https://github.com/cyverse/cyverse-sdk ). The diagram is built with mermaid ( https://knsv.github.io/mermaid/ ) and modified for interactivity on metadata and real-time job status. The latter is acquired through Agave API’s Webhook notification for jobs, which is also used for automatic updating of the MySQL database and automated execution of a workflow. In regard to genome browsers, both JBrowse and Biodalliance ( https://www.biodalliance.org/ ) are supported for visualizing alignments, variants, and genome annotation results. h5ai ( https://larsjung.de/h5ai/ ) is used to display the directory of example data and folders containing complete lists of jobs. 5 Discussion SciApps provides a web-based workflow platform, accessible to all CyVerse users (currently ∼50 000), for automating the execution of modular Agave apps. Such automation is currently not supported by the DE. SciApps workflows can be used to process large amount of data either remotely on a cloud or locally. Local processing has the benefit of reducing cross-country data transfers. For the 12 jobs used in the Association or Annotation workflows, a comparison of the total time required when launching from SciApps or DE shows that SciApps always outperforms DE by avoiding such transfers ( Supplementary Fig. S7 ). SciApps’ front-end interface is designed with three panels aligned similarly to those of the Galaxy ( Giardine  et al. , 2005 ) and GenePattern ( Reich  et al. , 2006 ) platforms. On the back end, SciApps is designed to be flexible in regard to data placement and mixing of local and cloud-based computing resources. In other words, SciApps workflows can work with data located on a local server, a cloud, or a mix of both. Similarly, the computation of each step can happen in either location, depending on the execution system definition in the app JSON. This flexibility is achieved via adoption of the Agave API. Specifically, the Agave job JSON specifies where to retrieve the data and where to archive the results. The Agave app JSON, in addition to describing each step in the Common Workflow Language (CWL) format ( Amstutz  et al. , 2016 ), specifies where to execute an app and how many processors and how much RAM are needed. Therefore, the SciApps workflow JSON is not interoperable with CWL. However, for single-node jobs, it would be possible to convert SciApps workflows into CWL format by combining the workflow JSON with the Agave app JSON and job JSON, although this would eliminate the flexibility regarding where and how to execute an app and where the data can be placed. Another challenge is that CWL does not yet support the Singularity containers ( Kurtzer  et al. , 2017 ), which are used by SciApps applications to simplify workflow switching between local and cloud-based computing resources. Future work will be focused on two goals: first, to continuously optimize SciApps for more efficient processing of local data with local computing resources in order to support data processing and management for project like MaizeCode; and second, to configure and optimize a community version to simplify local installation of SciApps. The community version will pull data directly from the CyVerse Data Store to XSEDE clusters for computing, and will be able to be launched from a laptop or virtual machine. Consequently, it will no longer be necessary to set up a local cluster and storage server, in order to use SciApps. Supplementary Material Supplementary Materials Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Integrating regulatory DNA sequence and gene expression to predict genome-wide chromatin accessibility across cellular contexts</Title>
    <Doi>10.1093/bioinformatics/btz352</Doi>
    <Authors>Nair Surag, Kim Daniel S, Perricone Jacob, Kundaje Anshul</Authors>
    <Abstract/>
    <Body>1 Introduction Cost-effective, sequencing-based functional genomics assays such as RNA-seq, ChIP-seq, DNase-seq and ATAC-seq have enabled large-scale profiling of epigenomes and transcriptomes across diverse cellular contexts ( Consortium, 2012 ;  Kundaje  et al. , 2015 ). These datasets provide a unique resource to understand the relationship between regulatory DNA sequence, chromatin state and gene expression. DNase-seq ( Boyle  et al. , 2008 ;  Thurman  et al. , 2012 ) or ATAC-seq ( Buenrostro  et al. , 2013 ) experiments profile the accessible chromatin landscape typically bound by regulatory DNA binding proteins such as transcription factors (TFs). Chromatin accessibility is highly dynamic across cellular contexts ( Thurman  et al. , 2012 ). Chromatin accessibility of a regulatory element is largely a function of the combinatorial  cis -regulatory code of TF binding sequence motifs embedded in its DNA as well as the availability and activity of the  trans -regulatory proteins such as TFs that bind them. A large body of literature has focused on developing computational models to decipher the  cis -regulatory sequence code of cell-type specific chromatin accessibility landscapes. Recently, convolutional neural networks (CNNs) have been used to learn the  cis -regulatory grammars encoded in regulatory DNA sequences associated with cell-type specific  in vivo  TF binding and chromatin accessibility ( Alipanahi  et al. , 2015 ;  Kelley  et al. , 2016 ;  Quang and Xie, 2016 ;  Zeng  et al. , 2016 ;  Zhou and Troyanskaya, 2015 ). By learning a series of de-novo motif-like pattern detectors (called convolutional filters) and non-linear activation transformations, CNNs are able to map raw DNA sequence across the genome to binary or continuous measures of associated regulatory activity profiles without explicit feature engineering. The Basset model ( Kelley  et al. , 2016 ) is a state-of-the-art CNN architecture that predicts binary chromatin accessibility in a specific cellular context across the genome as a function of local 600 bp DNA sequence context around each bin. The Basset model is also a multi-task architecture trained simultaneously on binary chromatin accessibility profiles from multiple cellular contexts (each context is a prediction task) and produces a vector of outputs for any genomic position containing the probability of accessible chromatin state at that position in each of the cellular contexts (task). The input DNA sequences represented using a one-hot encoding is transformed by three convolution layers. A rectified linear unit (ReLU) non-linear transformation is applied to the output of the final convolution layer and a pooling operation takes the maximum across a window of adjacent positions. These transformations are then passed to three fully connected layers followed by a logistic non-linearity for each task (cellular context) that outputs the probability of accessibility. The convolutional filters learned by Basset were visualized and interpreted to infer putative  cis -regulatory sequence drivers of context-specific chromatin accessibility. The model was also used to score putative regulatory genetic variants using an  in-silico  mutagenesis approach. The Basset model was recently enhanced by factorizing the convolution layers ( Wnuk  et al. , 2017 ) (Factorized model). The Factorized model increases the model depth—the three convolution layers in Basset are replaced by nine convolution layers. Further, the first two convolution layers in Basset which contain convolutional filters (motif-like pattern detectors) of widths 19 and 11, respectively, are factorized into multiple convolution layers with smaller widths. The authors note that these modifications enhance prediction performance and reduce learning time. While these and other sequence-only models ( Kelley  et al. , 2018 ;  Zhou and Troyanskaya, 2015 ) have provided useful insight into context-specific  cis -regulatory sequence features and the context-specific impact of regulatory genetic variants, these models cannot be used to predict chromatin accessibility or other regulatory profiles in cellular contexts not present in the training set. This is largely because these sequence-only models do not model the regulatory activity of  trans -factors that vary across cellular contexts. Gene expression levels of  trans -factors as measured by RNA-seq provide a useful, albeit indirect surrogate for their availability and activity in different cellular contexts. Models that can integrate  cis -regulatory DNA sequence and  trans -regulator expression should in principle be able to generalize to predict chromatin accessibility landscapes across cellular contexts. Such a model would be very valuable because it would enable prediction of chromatin accessibility profiles in large collections of cellular contexts that are currently characterized only by RNA-seq ( Collado-Torres  et al. , 2017 ). Moreover, interpreting such an integrative model would also provide insights into  cis -regulatory sequence features and  trans -regulators that are predictive of chromatin dynamics across cellular contexts. Deep-learning architectures allow this kind of flexibility to integrate multi-modal data i.e. DNA sequence coupled with RNA expression profiles. Hence, we expand upon previous work to predict genome-wide maps of chromatin accessibility using sequence and gene expression data ( Kelley  et al. , 2016 ;  Wnuk  et al. , 2017 ). We introduce multi-modal, residual neural network (ResNet) architectures ( He  et al. , 2016 ) that integrate  cis -regulatory sequence and context-specific expression of  trans -regulators to predict genome-wide chromatin accessibility profiles across cellular contexts. We show that the average accessibility of a genomic region across training contexts can be a powerful baseline predictor. We leverage this feature and employ novel strategies for training models to enhance prediction performance of shared and context-specific chromatin accessible sites across cell types. Further, we show that we can interpret these cross-cell type models to reveal insights into  cis - and  trans -regulators of chromatin dynamics across 123 diverse cellular contexts. 2 Materials and methods 2.1 Chromatin accessibility data DNase-seq datasets profiling genome-wide chromatin accessibility were downloaded from the Roadmap Epigenomics Project ( http://www.ncbi.nlm.nih.gov/geo/roadmap/epigenomics/ ) and ENCODE ( https://www.encodeproject.org/ ). The complete list of DNase-seq datasets and their identifiers is provided in  Supplementary Table S1 . The fastq files were aligned with BWA aln (v0.7.10), where all datasets were treated as single-end, with ENCODE default alignment parameters. After mapping, reads were filtered to remove unmapped reads and mates, non-primary alignments, reads failing platform/vendor quality checks and PCR/optical duplicates (-F 1804). Low quality reads (MAPQ &lt; 30) were also removed. Duplicates were then marked with Picard MarkDuplicates and removed. The final filtered file was then converted to tagAlign format (BED 3 + 3) using bedtools bamtobed. Cross-correlation scores were then obtained for each file using phantompeakqualtools (v1.1). All files were checked to have cross-correlation with a quality tag above 0 and discarded if not. For the ENCODE data generated from the Stam Lab protocol, all datasets were trimmed to 36 bp and then technical replicates were combined. Read depths were considered, and a standardized depth of 50 million reads was set for the final datasets. As such, the files were filtered to remove mitochondrial reads, filtered for mappability (MAPQ &gt; 30) and then subsampled to 50 million reads. For the ENCODE data generated from the Crawford Lab protocol, the same procedure as above was performed, except reads were trimmed to 20 bp due to the different library generation protocol. For the Roadmap data, which was all generated by the Stam Lab protocol, the same procedure as above was performed with trimming to 36 bp, and files were only combined to give a minimum read depth of 50 million reads, since each file came from a different developmental time point. These trimmed, filtered, subsampled tagAlign files were then used to generate signal tracks and call peaks. Signal tracks and peaks were called with a loose threshold ( P  &lt; 0.01) with MACS2 to generate bigwig files (fold enrichment and  P -value) and narrowPeak files, respectively. To identify reproducible peaks, we performed pseudoreplicate subsampling on the pooled reads across all replicates (taking all reads from the final tagAligns and splitting in half by random assignment to two replicates) and retaining reproducible peaks passing an Irreproducible Discovery Rate (v2.0.3) ( Li  et al. , 2011 ) ( https://github.com/kundajelab/idr ) threshold of 0.1 to get a reproducible peaks for each DNase experiment. The pipeline is available in a Zenodo record  https://doi.org/10.5281/zenodo.156534 . We bin the human genome (GRCh37 assembly) into 200 bp bins ( i ) every 50 bp. For each of the 123 cellular contexts ( j = { 1 … 123 } ), all bins are assigned binary labels ( y i , j ∈ { 0 , 1 } ) corresponding to accessible (+1) or inaccessible (0) state based on whether they overlap (&gt;50% overlap) context-specific reproducible DNase-seq peaks or not. The genome-wide binary labels for each task  j  (cellular context) are highly imbalanced (Proportion of positive bins: min = 3%, median = 7%, max = 10% across cell types). The complete binary label matrix is available via a Zenodo archive  https://doi.org/10.5281/zenodo.2603199 . The  cis -regulatory sequence context (S i ) for each bin  i  is represented using 1000 bp of genomic DNA sequence centered at the bin. We use a 1000 bp sequence context since previous work showed performance gains using contexts up to 1000 bp ( Avsec  et al. , 2018 ;  Zhou and Troyanskaya, 2015 ). 2.2 Gene expression data RNA-seq fastq files (no subsampling, no filtering, no trimming) from Roadmap and ENCODE were mapped using the STAR aligner (version 2.4.1d), using ENCODE default parameters. GENCODE release 19 (GRCh37.p13) transcriptome annotations were used. To determine the strandedness of the file (which is needed for RSEM quantification), the infer_experiment.py script from RSeQC (version 2.6.4) was used in conjunction with the STAR output that was sorted by coordinate. The strandedness and the pairedness (paired-end or single-end) of the experiment were passed on to RSEM (version 1.2.21). For RSEM, we used ‘–estimate-rspd’ to estimate read start position distribution, and we did not calculate confidence bounds. If the experiment was stranded, we set ‘–forward-prob’ to be 0, and unstranded experiments were left at default. The transcriptome aligned file from STAR was used in the RSEM run. The complete list of RNA-seq datasets and their identifiers is provided in  Supplementary Table S3 . The pipeline is available at  https://github.com/ENCODE-DCC/rna-seq-pipeline  (v1.0). The final dataset includes RNA-seq data associated with each of the 123 cell types. We extract the transcripts per million (TPM) values and use the log transformed TPM values. The  trans -regulatory feature space  R j  for each cellular context  j = { 1 … 123 }  is represented by the log(TPM) expression levels of a list of 1630 putative TFs as curated by the FANTOM5 consortium ( http://fantom.gsc.riken.jp/5/sstar/Browse_Transcription_Factors_hg19 ) of human TFs. The TF gene expression feature matrix is available via a Zenodo archive  https://doi.org/10.5281/zenodo.2603199 . 2.3 ChromDragoNN neural network architecture Our goal is to learn a model  F ( S i , R j )  that can predict the binary chromatin accessibility state  y i , j  at any bin  i  in genome in any cellular context  j  as a function of the one-hot encoded 1 kb  cis -regulatory sequence context Si of bin  i  and the expression of 1630 TFs  R j  in cellular context  j . We use a multi-modal neural network model to integrate the  cis -sequence and  trans -expression modalities and optionally the mean accessibility of the bin across cell types. The one-hot encoded sequence  S i  for each bin  i  in the genome is fed into a residual convolutional neural network (ResNet) model ( Fig. 1A ). The ResNet architecture includes hierarchically arranged convolution layers that are able to map one-hot encodings of raw DNA sequence input data to learn complex representations without explicit feature engineering. Each convolution layer learns and scans a set of weight matrix pattern detectors (convolutional filters) across its input and detects patterns in the input sequence. ResNets ( He  et al. , 2016 ) have been shown to be more effective for training CNNs with a large number of layers by introducing skip connections between blocks of convolution layers to optimize gradient flow and improve learning. Utilizing these concepts, we use a ResNet architecture to extend previous models ( Kelley  et al. , 2016 ;  Wnuk  et al. , 2017 ). The residual network ( He  et al. , 2016 ) consists of blocks in which the input is transformed through one or more convolutional layers to an intermediate output to which the input is added back. In our model, the convolution layers within a block preserve the input dimensions. Fig. 1. Improved training methods and new architecture design enhances model performance ( A ) Model architecture for the ResNet model. The RNA-seq inputs and mean accessibility (if used) are concatenated after the convolutional layers. ( B ) The validation set loss over training steps for a model (Basset architecture for sequence mode) with and without two-stage learning (without mean accessibility as an input feature). In two-stage learning the weights of the convolutional layer of the model are initialized from a model first trained to map sequence to chromatin accessibility for all training cell types. ( C ) The test set AUPRC of the original Basset model, Factorized model and ResNet model under 4 training paradigms: with and without mean accessibility as an input feature, and with (Tune) and without (Freeze) fine-tuning convolution layers in second stage. Numbers reported on a fixed training, validation and test split with 103 training, 10 validation and 10 test cell types. Models using mean accessibility as an input feature significantly outperform models without mean accessibility. ( D ) Five-fold cross-validation performance of the ResNet model compared to the Factorized model with and without mean locus accessibility as an input to the model. Each fold contains a split over 123 cell types in the dataset. All models trained using 2-stage scheme with all weights tunable in second stage. Wilcoxon signed rank test (single-tailed) was performed with  n  = 5, n.s. not significant, * P &lt;0.05. ( E ) Binned AUPRC of Factorized model without mean accessibility, ResNet model without mean accessibility and ResNet model with mean accessibility. Loci are binned by the fraction of training cell types that are accessible, and AUPRC is computed for predictions on test cell types for each bin. Note that AUPRC is computed for the minority class—when fraction of accessible cell types &gt;0.5, AUPRC is computed on non-accessible regions. Gray bars indicate the fraction of loci having a certain fraction of accessible cell types. Numbers reported on a training, validation and test split same as for (C) To provide the model with quantitative information on the availability of  trans -regulator TFs, we follow recent work ( Wnuk  et al. , 2017 ) that extended the Basset model to predict chromatin accessibility in held-out cellular contexts, using RNA-seq profiles as surrogates of cell-type specific availability and activity of  trans -regulators. RNA-seq profiles have been shown to uniquely identify individual cell types while preserving biological similarity between cell types ( Sudmant  et al. , 2015 ). We use log(TPM) RNA expression levels of 1630 TFs as a meaningful representation of  trans -regulatory cell state, as TFs are the DNA binding proteins that would affect chromatin accessibility by binding  cis -regulatory sequence patterns. The sequence ResNet-CNN component of the model learns  cis -regulatory sequence patterns and returns a transformed sequence-based feature space as intermediate representation. The TF RNA-seq vector  R j  for cellular context  j  is concatenated with this intermediate sequence representation, which is then passed through fully connected neural network layers and a logistic non-linearity to produce an output  F ( S i , R j )  representing the predicted probability that the bin  i  is accessible in the cellular context  j . The mean accessibility for bin  i  across all training cell types, if used, is concatenated at the final fully connected layer. The complete sequential network is as follows: One-hot input sequence of dimension 1000. Two convolutional layers with 48 and 64 channels, respectively, filter size (3, 1). Two residual blocks, each with three convolution layers with 64 channels and filter size (3, 1). Two residual blocks, each with two convolution layers with 128 channels and filter size (7, 1). Two residual blocks, each with three convolution layers with 200 channels and filter sizes (7, 1), (3, 1), (3, 1) respectively. Two residual blocks, each with two convolution layers with 200 channels and filter size (7, 1). The output is flattened and concatenated with gene expression. In case of mean accessibility models, the mean is concatenated. Fully connected layer with 1000 dimension output. Fully connected layer with 1000 dimension output. Fully connected layer with one output dimension. A single convolution layer is present after each residual block (except the third) to transform the number of channels. Batch normalization ( Ioffe and Szegedy, 2015 ) layers are present after each layer. A max pool is applied after the last three residual blocks. We use the ReLU non-linearity transform. We use a fixed dropout of 0.3 on the fully connected layers. 2.4 Multi-stage training We randomly split our 123 cellular contexts into training, validation and test sets across 5 folds ( Supplementary Table S2 ). For each fold, we train models genome-wide across the training cell types. The validation set cell types are used for hyperparameter tuning. The models are evaluated based on their genome-wide predictions in the held-out cell types in the test sets. The shift from a multi-task, cell-type specific sequence-only model to a single-task, cross-cell type, multi-modal model increases the number of training examples by a factor of  C , equal to the number of cell types in the training data. The increased size of the training data has implications for training. A naive training setup could potentially take up to a factor  C  longer to train compared to a fixed cell type model. To improve efficiency, performance and interpretability, we train our models in two steps: the first stage pre-trains a multi-task sequence-only model that maps sequence of each genomic bin to accessibility labels in each of the cellular contexts in the training set as individual tasks. The second stage trains the multi-modal model across all genomic bins and cellular contexts in the training set by initializing the sequence-mode’s convolutional layer weights using the pre-trained model. The two-stage training scheme provides added flexibility in that during the second stage of training, the convolutional layer weights may or may not be frozen while the fully connected layers are trained. 2.5 Model training and testing We use the Adam optimizer ( Kingma and Ba, 2014 ) on binary cross entropy loss to update our network’s weights, along with batch normalization on the convolution and fully connected layers. We use the default PyTorch v0.4 parameter initialization method ( LeCun  et al. , 2012 ). We perform hyperparameter searches for all stage 1 models with batch sizes (128, 256) and learning rates (2e-2, 2e-3, 2e-4), and for stage 2 models with batch sizes (256, 512, 1024) and learning rates (1e-3, 1e-4). To mitigate the class imbalance, we maintained a 1:3 ratio of positives to negatives per batch by upsampling accessible regions in the second stage of training. Given the significant class imbalance in the labels, we use the area under precision–recall curve (AUPRC) as our primary performance evaluation measure. 2.6 Motif extraction The dynamics of chromatin accessibility of regulatory elements across cellular contexts is a result of distinct subsets of context-specific TFs binding combinations of motifs encoded in the sequence of the regulatory elements ( Sherwood  et al. , 2014 ;  Voss and Hager, 2014 ). Deep neural network models of regulatory DNA sequence implicitly learn these motifs as distributed representations across the convolutional filters. Hence, valuable insights on predictive regulatory sequence features can be obtained by interpreting the model. A commonly used approach involves directly visualizing the convolutional filters or deriving position weight matrices from subsequences that maximally activate filters ( Kelley  et al. , 2016 ). However, this approach has the drawback that the motifs obtained from individual filters are often redundant or incomplete since the models learn distributed representations ( Shrikumar  et al. , 2018 ). An alternative approach is to use feature attribution methods to interpret predictive patterns in specific input DNA sequences. These feature attribution methods ( Shrikumar  et al. , 2017 ;  Simonyan  et al. , 2013 ;  Sundararajan  et al. , 2017 ) decompose the output prediction of a model for a specific input sequence of interest in the form of contribution scores of individual nucleotides in the sequence. Nucleotides with high positive scores can be interpreted as driving the prediction for the sequence. Feature attribution methods allow for instance-by-instance interpretation of predictive patterns but do not provide a global summary of predictive motifs across all accessible sites within and across cellular contexts. Hence, we used a new method we recently developed called TF-MoDISco (v0.2.1) ( Shrikumar  et al. , 2018 ) that (i) identifies predictive sequence patterns within the sequences of each accessible site across the genome in a cell context of interest as subsequences (called seqlets) with significant contribution scores derived using a feature attribution method (specified below); (ii) computes a similarity matrix between all predictive seqlets across the accessible landscape and (iii) clusters the seqlets into non-redundant motifs. To obtain nucleotide-resolution contribution scores for each input sequence corresponding to accessible bins in the genome in a specific cellular context, we used the gradient of the logit of the output probability of the model (predicted probability of site being accessible in the specific cellular context) with respect to the one-hot DNA sequence, gated by the observed nucleotides in the input sequence. To focus on motifs associated with dynamic chromatin accessible sites, for each cellular context, we extracted the contribution score profiles from the ResNet model (that does not use mean accessibility as an input feature) for subsets of 20 000 bins that are accessible in the given cellular context and in &lt;30% of all the cell types. Contribution score profiles computed for these 20 K sequences in each cellular context were passed to TF-MoDISco to learn context-specific globally predictive motifs. The TF-MoDISco motifs were matched against a database of known TF motifs using Tomtom ( Gupta  et al. , 2007 ). 3 Results 3.1 Accurate prediction of chromatin accessibility across cellular contexts from DNA sequence and gene expression with multi-stage training We developed multi-modal neural network architectures to predict the binary chromatin accessibility state at each bin in the genome in any cellular context by integrating 1 kb  cis -regulatory sequence context around each genomic bin and gene expression levels of 1630 TFs in the specific cellular context. Models were trained on a subset of training cell types and their performance was reported based on genome-wide predictions in held-out test cell types. We developed a two-stage learning strategy to improve efficiency, performance and interpretability of the models. In the first stage, we pre-trained a multi-task sequence-only model across all training cell types. In the second stage, we trained a multi-modal model integrating sequence and expression, where we initialized the convolutional layer weights of the sequence model from the first stage. We found that tuning the convolution layers in the second stage consistently improved performance over freezing the weights of the layers at an increased computational cost. Further, pre-training the sequence mode consistently improved training time and performance ( Fig. 1B ). We experimented with different CNN architectures, training strategies and tested the impact of adding an additional feature—the mean accessibility of a genomic bin across training cell types. After evaluating the various models on our validation datasets, our best model architecture achieves an average AUPRC=0.76 and area under Receiver Operating Characteristic curve (AUROC)=0.954 across five folds, outperforming previously published model architectures trained and tested on matching data (average AUPRC=0.69, AUROC=0.937 across five folds) ( Fig. 1C ). 3.2 Using mean accessibility as an input feature boosts performance A key difference between cell-type specific models and cross-cell type models is that cross-cell type models can make use of statistics based on the accessibility state of each genomic bin (locus) across the training cell types. For each bin in the genome, we computed the mean of the binary accessibility values across all cell types in the training set. Since binary accessibility is 0 if the locus is closed and 1 if open, the mean accessibility is a value in  [ 0 , 1 ]  that is equivalent to the fraction of cell types in which the bin is accessible. We observed that mean accessibility is a strong baseline predictor of chromatin accessibility across cell types [also recently reported by  Schreiber  et al.  (2019) ]. Setting the predicted accessibility of a locus equal to its mean accessibility across training cell types yielded an AUPRC of 0.579 and an AUROC of 0.902 on the test set. This method is oblivious to the test cell type and in fact assigns the same values to all test cell types for a given bin. A stronger baseline is achieved by computing a weighted average of accessibility across training cell types, where the weight is proportional to the similarity between RNA-seq profiles of the training and test cell types. The resulting predictions yield an AUPRC of 0.587 and AUROC of 0.903 which are marginally better than the unweighted version. All our multi-modal models that use sequence and expression substantially outperform this strong baseline predictor (mean baseline AUPRC = 0.579, weighted-mean baseline AUPRC = 0.587, Basset+expr AUPRC = 0.656, FactorizedBasset+expr AUPRC = 0.692, ResNet+expr AUPRC = 0.700). However, we decided to capitalize on the strong mean baseline and decided to use it as an auxiliary input feature to the multi-modal model. The single scalar mean accessibility value for each bin is concatenated with the output of first feed forward layer. We observe substantial improvements when the mean accessibility feature is provided as an input to the model ( Fig. 1C and D ). Across three different types of architectures that we trained, incorporating the mean as an input feature improves the performance of the model by as much as 0.09 AUPRC. 3.3 Residual network architecture outperforms previous architectures ResNets ( He  et al. , 2016 ) have been shown to be highly effective for training deeper CNNs with a large number of layers. ResNets provide added flexibility to CNNs by introducing skip connections between blocks of convolution layers. In practice, while the performance of ordinary CNNs saturates or even drops with increasing layers ( Srivastava  et al. , 2015 ), ResNets have made possible training of CNNs often having &gt;100 convolution layers. ResNets have also recently been used to train high performance deep learning sequence models of splicing ( Jaganathan  et al. , 2019 ). We implemented a ResNet architecture that uses 23 convolution layers across eight residual blocks. Following the Factorized model, we used convolution filters with shorter widths.  Figure 1D  shows the results of a 5-fold cross-validation performed on our dataset. We compared the performance of the model with the Factorized model with and without passing mean locus accessibility as an input to the model. In both cases, the ResNet architecture improved upon the performance of the Factorized model. Overall, our best performing ResNet(+mean accessibility) model achieves a mean AUPRC of 0.76 while the previous best published model in the literature i.e. the Factorized Basset model ( Wnuk  et al. , 2017 ) achieves 0.69 ( Fig. 1C ) on a matched training/validation/test data split. Next, in order to understand performance variation as a function of cell-type specificity of accessible sites, we grouped genomic bins based on the fraction of cell types in which bins exhibit accessibility. For each group, we compared the AUPRC of our best ResNet model that included mean accessibility as auxiliary input with the previous best published model i.e. Factorized Basset without mean accessibility ( Fig. 1E ). Our models consistently outperform the previous state-of-the-art across all groups. 3.4 Model interpretation reveals cell-type specific  cis -regulatory sequence features and associated  trans -regulators Understanding what the model is utilizing in the DNA sequence input is of interest, and previous work has successfully shown that CNNs learn predictive motif-like patterns of cell-type relevant TFs from regulatory DNA sequences ( Kelley  et al. , 2016 ). However, the model learns a distributed representation of the sequence features. Hence, interpreting individual convolutional filters results in redundant and partially complete motifs. Instead, we use TF-MoDISco, a new method we recently developed, for distilling consolidated motifs from sequence-based deep learning models ( Shrikumar  et al. , 2018 ). First, we use a feature attribution approach (gradient×input) to infer contribution scores attributed by the model to each nucleotide in chromatin accessible sequences with respect to the output prediction in each cellular context. Predictive nucleotides and motif instances get highlighted with high positive contribution scores. The same sequence can have different contribution score profiles across different cellular contexts representing dynamic regulation of the region by different sequence motifs ( Fig. 2A ). For each cellular context, we sample a subset of bins that are labeled accessible, obtain contribution scores for corresponding input sequences and extract motifs using TF-MoDISco with default parameters. The motifs are matched against a database of known motifs of TFs using Tomtom ( Gupta  et al. , 2007 ). The sets of motifs retrieved for each cellular context reflect the globally predictive TF motif patterns learned by the model for that context ( Fig. 2B ). Fig. 2. Cell-type specific TF motifs distilled from the ResNet model ( A ) Gradient × input contribution scores of each nucleotide (columns) in an example genomic sequence (chr8: 128929715–128931715) across different cellular contexts (tasks shown as rows). The obtained nucleotide-resolution contribution scores for the same genomic sequence can differ between cell-types reflecting differential chromatin accessibility and differences in regulation of the sequence, as shown in this example locus. ( B ) Summary of motifs learned by the model for individual cell types. The TF-MoDISco method is used to distill consolidated motifs learned by the model for each cell type using a subset of sequences which are accessible in the respective cell type. The returned motifs are then matched to known motifs of TFs using Tomtom ( Gupta  et al. , 2007 ) The model learned known DNA motifs of ubiquitous as well as cell-type specific TFs that match the canonical roles of TFs in different lineages ( Fig. 2B ). As reported in  Kelley  et al.  (2016) , the model learns the CTCF motif as a widely important sequence element for accessible regions across cellular contexts ( Ong and Corces, 2014 ). The HNF1A and HNF4A motifs are more narrowly predictive of accessibility in hepatocyte-related, large and small intestinal contexts ( D’Angelo  et al. , 2010 ). The model discovers SIX2 motif as a key predictor in kidney-related contexts ( Kobayashi  et al. , 2008 ). TWIST1 motif is retrieved for contexts of mesenchymal origin ( Qin  et al. , 2012 ), while RUNX1, ETS1 and IRF1 motifs are mainly discovered only in specific hematopoietic cell types ( Brien  et al. , 2011 ). GRHL2 motif is discovered in the lung, epithelial cells and kidneys, which matches known differential expression patterns of GRHL family TFs across cell types ( Aue  et al. , 2015 ). No prior information about sequence motifs is provided to the model, suggesting that the model is effective at extracting cell context relevant  cis -regulatory features from the DNA sequence input. Many of the discovered motifs are cell-type specific, which suggested that intersecting these results with the dynamics of RNA expression profiles of  trans -regulators could potentially lead us to the TFs that potentially bind these discovered motifs. For each discovered motif, we determined all the TFs (often from the same family) that could potentially bind the motif. We computed the binary vector of dynamic motif activity for each motif across cell types (whether that motif was discovered by TF-MoDISco in the cell type or not). We computed the Pearson correlation between the motif activity vector and the vector of expression levels of matching TFs across those cell types. We show the top 15 most correlated TFs in  Fig. 3A . This analysis highlighted several known key regulators, both universal and cell-type specific, across a variety of cell types. TWIST1 is a known regulator in mesenchymal cell types and is highlighted as important in muscle cell types and fibroblasts ( Qin  et al. , 2012 ). RUNX3 and IRF1 are important regulators in blood cell types ( Brien  et al. , 2011 ), while HNF4A is a master regulator in intestinal development ( Babeu and Boudreau, 2014 ). HNF1A, GRHL2, SIX2 and HOXA9 are all regulators known to be important in kidney development ( Aue  et al. , 2015 ;  Kobayashi  et al. , 2008 ;  Martovetsky  et al. , 2013 ), and are highlighted here as important specifically in kidney cell types. Interestingly, ASCL1 is highlighted as important in thymus and spleen cell types, where the expression is also very specifically high in these cell types—this suggests a role for ASCL1 in these cell types that was not elucidated before, though further work is required to fully validate this hypothesis. This analysis thus uncovers possible  trans -regulators that modulate cell context-specific chromatin accessibility profiles through predictive  cis -regulatory motifs. Fig. 3. Predictive  cis -sequence features and  trans -regulators inferred from the models: ( A ) TF-MoDISco motifs (left) and row-normalized log TPM RNA-expression values (right) for each training cell type, where each row is a matching motif and TF. ( B ) t-SNE embedding of 250 additional cell types (points) based on the RNA-seq profiles of 1630 TFs (left) compared to t-SNE embedding of the same cell types based on the predicted chromatin accessibility profiles 3.5 Biologically relevant segregation of cell types based on predicted chromatin accessibility We used our cross-cell type, multi-modal models to impute genome-wide binary chromatin accessibility profiles in 250 additional cellular contexts (see  Supplementary Table S3 ) that were not seen in our original dataset and were profiled only using RNA-seq. These new imputed samples were then embedded into a 2D visualization using t-SNE ( Maaten and Hinton, 2008 ) to determine how well the imputed accessibility profiles group distinct and related cell types. Comparing an equivalent t-SNE visualization in RNA-seq expression space (using the 1630 TFs as features) to the predicted chromatin accessibility ( Fig. 3B ), we find that the t-SNE map from imputed accessibility shows improved separation of distinct clusters of samples grouped by cell type and disease state. E.g. the carcinoma cell types and the adenocarcinoma cell types are embedded near each other in the t-SNE from predicted accessibility. Further, the predicted accessibility t-SNE embeds the adenocarcinomas as slightly offset from the carcinomas. While t-SNE embeddings can be unstable and difficult to interpret, our visualizations do suggest that the imputed accessibility profiles do capture biologically meaningful differences and similarities between cell types and that these differences are not simply reflecting differences in expression of the TFs that were used as predictors. This ability to distinguish cell types through imputed accessibility profiles is important because it suggests that given a new expression profile, these models can produce distinct accessibility profiles that may be granular enough to potentially reveal subtypes and finer grained structure beyond the expression profile. 4 Discussion We present an optimized multi-modal residual network architecture that can integrate  cis -regulatory DNA sequence and expression of  trans -regulators to predict genome-wide binary chromatin accessibility profiles across cellular contexts. The model can be used to predict genome-wide chromatin accessibility in cellular contexts that are only profiled with RNA-seq. This is particularly useful given the large number of profiled transcriptomes that do not have corresponding experimentally profiled epigenomes. We demonstrate that accessibility profiles predicted from sequence and TF expression do not simply recapitulate the landscape of expression profiles across cell types but rather provides a complementary feature space that can discriminate between related and distinct cellular contexts. Using enhanced training strategies, we achieve a new state-of-the-art in terms of prediction performance across cellular contexts. We show that a two-stage training strategy that pre-trains using only sequence before integrating the expression data improves performance and training time. This method of transfer learning is common in applications in computer vision and natural language processing ( Chen  et al. , 2015 ;  Oquab  et al. , 2014 ). In two-stage model learning, we show that tuning the convolution layers in the second stage offers a benefit over freezing the weights of the layers, however, at an increased computational cost. Mean accessibility of a given locus across contexts is a surprisingly strong predictor of chromatin accessibility. Combining the mean accessibility with  cis -regulatory sequence and  trans -regulator RNA expression allows improved prediction performance. Notably, we find that adding mean accessibility as a feature improves performance across all types of accessible sites including the cell-type specific and ubiquitously active. We demonstrate that using a residual CNN architecture for chromatin accessibility prediction results in superior performance compared to previous architectures. Recent related work ( Wnuk  et al. , 2017 ) showed that increasing the number of convolution layers while reducing the width of each convolution layer increases the model performance. ResNets ( He  et al. , 2016 ) allows for connections between non-adjacent layers and have been shown to confer performance gains in deep networks. We observe and confirm similar improvements in model performance for predicting chromatin accessibility models. Recently developed imputation methods such as ChromImpute ( Ernst and Kellis, 2015 ), BIRD ( Zhou  et al.,  2017 ), PREDICTD ( Durham  et al. , 2018 ) and Avocado ( Schreiber  et al. , 2018 ) also tackle the problem of predicting regulatory profiles in new cellular contexts. In systematic comparisons on matched data, our models outperform the BIRD method in predicting genome-wide binary chromatin accessibility profiles from RNA expression data ( Supplementary Table S4 ). The imputation methods are based on capturing and modeling the local correlation structure between profiles of multiple biochemical markers such as RNA, histone modifications and chromatin accessibility within and across diverse cell types. In our framework, we instead use only one widely available auxiliary modality, the gene expression of  trans -regulators. Moreover, the above mentioned imputation methods do not model  cis -regulatory DNA sequence and hence lack the ability to interpret biologically meaningful predictive sequence features from the models. Our models enable interpretation of predictive  cis -sequence features learned by the models. Using model interpretation methods, we show that our models learn motifs of ubiquitous and lineage specific TFs. Correlating the RNA profiles of TFs with the dynamic predictive activity of motifs discovered by the model provides insights into the TFs that might bind these motifs and the relationship between  cis - and  trans -regulatory features. Our current models predict genome-wide binary chromatin accessibility profiles instead of continuous, quantitative profiles. However, our models can be easily adapted to predict continuous, quantitative profiles at finer resolutions by using regression loss functions ( Kelley  et al. , 2018 ). Our models can also be extended to include additional input data modalities or predict other types of genome-wide regulatory profiles such as histone modification profiles. Finally, improved approaches for interpreting multi-modal neural networks will provide significantly more nuanced insights into the complex interactions between  cis -regulatory sequence features and  trans -regulatory features. More transparent encodings of the gene expression space (e.g. using latent variables that directly model modules of functionally related genes or pathway annotations) would also improve interpretability. Our study highlights the promise of integrative multi-modal deep learning models for learning predictive models that generalize across cellular contexts and obtaining insight into the dynamics of gene regulation. Supplementary Material btz352_Supplementary_Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>TRStalker: an efficient heuristic for finding fuzzy tandem repeats</Title>
    <Doi>10.1093/bioinformatics/btq209</Doi>
    <Authors>Pellegrini Marco, Renda M. Elena, Vecchio Alessio</Authors>
    <Abstract>Motivation: Genomes in higher eukaryotic organisms contain a substantial amount of repeated sequences. Tandem Repeats (TRs) constitute a large class of repetitive sequences that are originated via phenomena such as replication slippage and are characterized by close spatial contiguity. They play an important role in several molecular regulatory mechanisms, and also in several diseases (e.g. in the group of trinucleotide repeat disorders). While for TRs with a low or medium level of divergence the current methods are rather effective, the problem of detecting TRs with higher divergence (fuzzy TRs) is still open. The detection of fuzzy TRs is propaedeutic to enriching our view of their role in regulatory mechanisms and diseases. Fuzzy TRs are also important as tools to shed light on the evolutionary history of the genome, where higher divergence correlates with more remote duplication events.</Abstract>
    <Body>1 INTRODUCTION Tandem Repeats (TRs) are multiple (two or more) duplications of substrings in the DNA that occur contiguously, and may involve some base mutations (such as substitutions, insertions and deletions). TRs of several forms (satellites, microsatellites, minisatellites and others) have been studied extensively because of their role in several biological processes. In fact, TRs are privileged targets in activities such as fingerprinting or tracing the evolution of populations (Kelkar  et al. ,  2008 ; Vogler  et al. ,  2006 ). Several diseases, disorders and addictive behaviors are linked to specific TR loci (Wooster  et al. ,  1994 ). The role of TRs has been studied also within coding regions (O'Dushlaine  et al. ,  2005 ) and in relation to gene functions (Legendre  et al. ,  2007 ). Large scale comparative studies on TRs of the human genome are described in Ames  et al.  ( 2008 ) and Warburton  et al.  ( 2008 ). Data Bases of repetitive elements such as RepBase (Jurka  et al. ,  2005 ) and Tandem Repeats Database (TRDB) (Gelfand  et al. ,  2007 ) are now available; and the detection of repetitive elements via library-based similarity matching, for example by using the tool Repeatmasker (Smit  et al. , 2004), is a popular practice. However, tools for  ab initio  detection of repetitive elements that are not based on prior knowledge accumulated in data bases are still important in order to extend our comprehension of the role of TRs in biological mechanisms. Existing  ab initio  tools are successful when the TR exhibits a moderate amount of divergence and when the TR is easily validated. However, there is an emerging need for new tools that are able to cope with higher levels of sequence divergence and/or TR computationally more difficult to validate. For example, Boeva  et al.  ( 2006 ) study so called  Fuzzy TRs  and their role in gene expression. The technique in Boeva  et al.  ( 2006 ) works well for the Hamming metric (only substitutions and no insertions/deletions allowed) and for short repeat units (from 3 to 24 bp) that are common in micro- and mini-satellite families. Some of the most successful  ab initio  tools, such as TRF (Benson,  1999 ) and ATRHunter (Wexler  et al. ,  2005 ), are based on a multi-stage filtering approach [see also (Peterlongo  et al. ,  2009 )]. In the first stage the input sequence is analyzed to detect, via statistical criteria, likely position and length of candidate subsequences. The final stage is the validation one in which a more expensive test is applied to candidate substrings passing the first stages, so to determine an output that matches the implicit definition of TR and the user-defined filtering parameters. 1.1 Our contribution Our contribution is a novel multi-stage filtering algorithm, called  TRStalker , for finding long fuzzy TRs under the edit distance, that introduces new techniques (w.r.t. previous TR finding algorithms) in all stages. For the first stage, where over-represented distances between probes are sought, we employ  gapped q-grams  (Burkhardt and Kärkkäinen,  2003 ) in place of the standard  ungapped q-grams  in order to collect evidence on the candidate substrings. Gapped q-grams have been used before in the context of textual and biological database searching, but less so in the area of TRs detection [with the exception of the system TEIRESIAS (Stolovitzky  et al. ,  1999 )]. Because of errors due to insertion/deletions, the  period  of a TR is subject to fluctuations, thus we employ a weighting scheme with exponential decay so to reinforce the signal even in presence of this smearing effect. Finally, we use  ranking  instead of  thresholds  when deciding the substrings to pass to the next phases, in order to concentrate the computational effort on the zones with candidates with higher weight. For the final validation stage we employ an NP-complete definition of TR involving the concept of  generalized median string  under edit distance (de la Higuera and Casacuberta,  2000 ; Sim and Park,  2003 ), together with an efficient heuristic for computing an approximation of such median string (Jiang  et al. ,  2003 ) previously not used in a biological context. By extensive experimental comparisons of  TRStalker  with two state-of-the-art tools, namely TRF and ATRHunter, we did find out that TRStalker has consistently better performance for a large range of error and length parameters for the class of fuzzy TRs under edit distance, with a recall ranging from 100 to 60%. Thus TRStalker improves the capability of TR detection for classes of TRs for which existing methods do not perform well. Tests performed on standard evolutionary TRs definitions (verifiable in polynomial time) also show recall performance close to 100%. Incidentally, this result confirms of the power of the new techniques developed for the initial filtering phase. 1.2 State of the art We will briefly survey the state of the art in finding tandem repeats. First we will describe methods that for a given definition of TR are able to find all maximal substrings in the input that match the definition ( exhaustive algorithms ). Often exhaustive algorithms may not be available, or when available they may be too slow in practice. Thus, several  heuristic algorithms  have been developed which are shown experimentally to be able to detect a large fraction of TRs efficiently. Note that the time/precision trade-off is severely influenced by the allowed error thresholds. Performance often degrades quickly with increasing error levels. 1.2.1 Exhaustive algorithms When we allow no error, it is possible to find all maximal exact TRs in a string of length  n  in time  O ( n ) (Gusfield and Stoye,  2004 ; Kolpakov and Kucherov,  1999 ). When we allow two consecutive repeats to differ by an amount at most  k  (either in Hamming or in edit distance) Landau  et al.  ( 2001 ) give exhaustive algorithms running in time  O ( nk log( n / k )) for Hamming distance, and  O ( nk log  k log( n / k )) for edit distance. A simpler algorithm with the same asymptotic complexity for the edit distance is proposed by Sokol  et al.  ( 2007 ). Kolpakov and Kucherov ( 2003 ) improved the bound for the Hamming distance to  O ( nk log k  +  s ) where  s  is the number of TRs found. For the Hamming distance, Krishnan and Tang ( 2004 ) give an exhaustive method running sequentially in time  O ( n 3 ), that can be easily implemented onto a parallel architecture, since every possible pattern length is searched independently. 1.2.2 Heuristic algorithms The algorithmic techniques in Kolpakov and Kucherov ( 1999 ,  2003 ) have been extended in the tool  mreps  (Kolpakov  et al. ,  2003 ) so to be able to handle approximate TRs (ATRs) under edit distance, with some additional heuristic filtering steps. The tool TRF (Tandem Repeat Finder) developed by Benson ( 1998 ,  1999 ), based on statistical filtering of zones of DNA likely to contain TRs, is currently one of the standard heuristic methods. ATRHunter by Wexler  et al.  ( 2004 ) is also based on a statistical filtering approach, placing greater emphasis in techniques for designing thresholds for the quantities of interest. Other proposed heuristics for finding TRs are REPuter (Kurtz and Schleiermacher,  1999 ; Kurtz  et al. ,  2001 ), STRING (Parisi  et al. ,  2003 ), TEIRESIAS (Stolovitzky  et al. ,  1999 ) and TandemSWAN (Boeva  et al. ,  2006 ). A class of papers (see e.g. Brodzik,  2007 ; Buchner and Janjarasjitt,  2003 ; Gupta  et al. ,  2007 ; Sharma  et al. ,  2004 ) tackle the problem of finding TRs as a problem in signal processing theory and usually map the input string into a time-signal in a suitable numerical domain for which several spectral techniques can be used, such as the  Periodicity Transform  or the  Fourier Transform . Other methods use data compression techniques to detect repetitive elements (Rivals  et al. ,  1997 ). The methods cited above are rather general since they aim at treating efficiently TRs in a wide range of length values. There is also a large class of methods that are aimed at handling particular or special classes of TRs such as: microsatellites [e.g. IMEx (Mudunuri and Nagarajaram,  2007 )], palindromic repeats [e.g. CRISPFinder (Grissa  et al. ,  2007 )], Variable Length TRs (VLTR) and Multi-period TRs (MPTR) (Hauth and Joseph,  2002 ) and Variable Number TRs (VNTR) (Sammeth and Stoye,  2006 ). Since the focus of our research on TRs at present is on the more classical forms of TRs, we do not dwell longer on them. However, we just note that often methods for MPTR, VNTR, VLTR use standard TR finding as a subroutine, thus our proposed algorithm can increase also the ability to detect such higher order structures. Systematic comparison among TR finding tools and algorithms operating  ab initio , that is without support of specific biological data bases has been tackled in recent years (Leclercq  et al. ,  2007 ; Saha  et al. ,  2008 ). A survey of problems on TRs in the context of evolutionary mechanisms, such as the construction of TR Evolutionary Trees, is proposed in Rivals ( 2004 ); see also Elemento and Gascuel ( 2002 ). 1.3 Organization of the article The article is organized as follows: in  Section 2  we describe at a high level the principles guiding the different phases of the TRStalker algorithm.  Section 3  gives a more technical description of key ingredients of TRStalker and discusses the formal definition of fuzzy TR employed.  Section 4  describes the experiments devised to demonstrate the capacity of TRStalker in detecting fuzzy TRs, and a few interesting fuzzy TRs found in sequences of biological significance. 2 APPROACH An example: To focus on the main ideas, let us consider the very simple case of Exact TR. Consider an alphabet Σ = { A ,  C ,  G ,  T } of four symbols, and a string  X  =  x 1 x 2  … x t  formed by the concatenation of  t  strings  x i , embedded in a random string  Y , where  x i  =  x 1  for all  i  and | x 1 | =  k , thus all replicas of  x 1  are of the same length. An  ungapped q-gram  is a string of  q  symbols from Σ that appears as a consecutive sequence of  q  symbols in  Y . We aim at discovering  k  just by looking at the distances between occurrences of homologous (i.e. identical)  q -grams in  Y . For q-grams in  X , the period  k  will appear at least ( k  −  q  + 1)( t  − 1) times as the distance between homologous probes. More generally the distance  hk , an integer multiple of  k , will appear at least ( k  −  q  + 1)( t  −  h ) times for each value  h  = 1 … t  − 1. A  gapped q-gram  is a sequence of  q  characters from Σ with additional ‘don't care’ symbols, also called ‘gaps’, that appears as a consecutive sequence in  Y . For gapped  q -grams similar formulae hold. For values of  k  and  t  large enough, the period  k  and its integer multiples will occur more frequently than the expected number of occurrences of any distance of homologous  q -grams in a random string, thus the empirical number of occurrences of the value  k  and its multiples will tend to be in the higher part of a ranking by frequency. This observation holds true as long as the length of the super-string  Y  is sufficiently limited so that the frequencies generated by the random portion of  Y  do not overrun the frequencies generated by  X . An exact characterization of such a distribution in terms of the parameters  k ,  t ,  q  and | Y | is complex since it can be characterized as the sum of  non-independent  random variables each with a negative binomial distribution. However we avoid the issue of characterizing exactly such a distribution by: (i) splitting the input string into blocks of predefined length and limiting the analysis to each block separately, providing mechanisms to deal with TRs stranded across the block boundaries; (ii) ranking the periods by  weighted frequency  and exploring only the top  L  positions (for  L  = 50 in our experiments). Note that in most cases the top ranking periods not corresponding to TRs will be discarded quickly when the positional density is considered, thus we can be very slack in choosing  L  without incurring in a computational burden. The choice of block length could be critical too, but experimental results showed that blocks of length within a factor of up to 40 of the length of the TR do work well. For long input, string occurrences of the same  q -gram that are too distant are unlikely to be related to a TR, thus we limit the number of pairs of homologous q-grams considered. While scanning each block of the input  Y  we record for each occurrence of a gapped q-gram in  Y  its distance to the five preceding and the five following homologous occurrences (10 in total). The high-level pseudocode of TRStalker is shown in the  Supplementary Materials  while we expose next the key algorithmic choices. Gapped q-grams: The presence of substitutions/insertions/deletions in  X  has the effect that many instances of q-grams will be affected by error and a match will be missed, thus reducing the frequency counts for the period  k . To cope with this effect, we use  gapped q-grams  (Burkhardt and Kärkkäinen,  2002 ,  2003 ) that are more resilient to the presence of substitutions/insertions/deletions. As suggested by experiments in Burkhardt and Kärkkäinen ( 2002 ), just few gaps are sufficient to be effective, thus we will use the family of all gapped q-grams with three alphabet symbols and at most two gaps. Anti-smear weighting: If  q 1  and  q 2  are occurrences of homologous  q -grams in  X  at distance  k , before the implant of mutations, the effect of insertion and deletions on the positions of the string  X  between  q 1  and  q 2  is to alter their distance so that a different period  k ′ is detected. The difference  k  −  k ′ is equal to the algebraic sum of number of insertions and deletions in the positions between  q 1  and  q 2 . Assuming that any such position can be an insertion or a deletion independently with the same probability, the random variable  k  −  k ′ is distributed as a sum of independent random variables with values in {+1, −1, 0} with mean value 0, thus, by a Chernoff bound argument, its tail distribution decays exponentially (Motwani and Raghavan,  1995 ; Mulmuley,  1993 ). Also near-by probes in  X  have small variations in the value of the shift  k  −  k ′. Inspired by the above observation, we devise a weighting scheme that increments the total weight of period  k  if another period of value   is discovered in a near-by position, with weights that decay exponentially with  . The final weight  w 0 ( k ) for a given period  k  is the sum of the individual anti-smear weights computed above for probes at distance  k . Multiplicity weighting: Let  w 0 ( k ) be the weight of the period  k  as assigned by the anti-smear weighting procedure. As observed before, for a TR with a large number of copies we will find also integer multiples of  k  with a relatively high frequency. We take advantage of this fact and compute new weights:
 
The candidate periods are then sorted by the weight  w 1 (.), and processed in decreasing order. Positional density: We further exploit the property of TRs that the same period is detected by probes in near-by positions. We define a notion of positional  k -density, that is the density of probes that contribute to the counter for the candidate period  k . We search for position in  Y  of high  k -density as candidates for the starting point of a TR. Validation: In the third phase we take each candidate pair ( p ,  i ) and we test explicitly whether there is a TR of period  p  starting in position  i  according to the definition ( Section 3 ). In particular when using the definition of a  Steiner-STR  ( Section 3.2 ) we use a double filtering. The fist filter uses a  wraparound dynamic programming  technique (WDP; Fischetti  et al. ,  1993 ). The second filter computes an approximation to the  generalized media string  [inspired by an algorithm proposed in Jiang  et al.  ( 2003 )]. In this phase, besides validating the TRs, we discover the (fractional) repetition number of the TRs eventually extracted. Post-processing: As a post-processing, we check for inclusion the TRs found and we filter out those TRs completely enclosed in another one. For TRs in the same position and length but different period we report the TR with shorter period. Finally we align the approximate generalized median string with the TR units so to give a graphical compact output of the TR. 3 METHODS 3.1 Basic definitions A TR in a DNA sequence is the repetition of two or more contiguous exact or approximate copies of a substring (called the  motif ) of the TR. 3.1.1 Exact TR Formally, given an alphabet Σ, and a set of strings  x i  ∈ Σ * , consider the concatenation  X  =  x 1 x 2  … x t . The string  X  is an  exact TR  (ETR) of  period k  and  repeat number t , when | x i | =  k  and  x i  =  x 1 , for each  i  ∈ [1 … t ]. In general we may suppose there is a longer string  Y  of which  X  is a substring. The string  x 1  that is repeated exactly is called the  motif  of the ETR. A TR  X  is called maximal if it cannot be extended in  Y  while still being a TR. 3.1.2 ATR ETRs are sometimes found in biological sequences, but they tell us only part of the story, thus several notions of an  ATR  have been developed. Denote with  D H ( a ,  b ) the hamming distance of two strings with equal length. If the length of  a  and  b  is different, we consider the smallest possible mismatch in an alignment of the two strings without gaps. Denote with  D E ( a ,  b ) the edit distance of the two strings  a  and  b . 3.2 Our definitions of TR We used two different definitions of TRs:
 Neighboring TR (NTR) : a string  X , so that for each  i  ∈ [1 … t  − 1],  D E ( x i ,  x i +1 )≤μ| x i |, for a user defined parameter 0≤μ≤1 Steiner-STR with sum : a string  X  =  x 1 x 2  … x t  for which two conditions hold for a user defined error parameter 0≤μ≤1, and constant  c  with 1≤ c ≤2:
 for each  i  ∈ [1 … t  − 1],  D E ( x i ,  x i +1 )≤ c μ| x i |. there exists a Steiner string   so that  
 
 Intuitively, in a Steiner-STR the TR consists of  t  duplications of a single Steiner consensus string   with   mutations on average in each copy, such that consecutive copies do not diverge too much w.r.t. the average. Note that condition (a) is vacuous for μ ≥ 1/ c . The choice for the constant  c  depends also on the level of divergence. For low divergence  c  = 2 is a sensible choice since two copies at distance   from   are also at distance at most   from each other by the triangular inequality. Thus (a) is a necessary condition for (b). For higher level of divergence above 30%, the value  c  = 2 is too loose and we use a lower value  c  = 1.5, so as to maintain a good filtering ability of condition (a) and to avoid having as a possible solution a TR where the consecutive pairs may have a very irregular divergence. 3.3 Output of TRStalker The aim of TRStalker is to produce a ranked list of  all maximal tandem repeat sub-sequences  present in the input string that satisfy the definition above (NTR or Steiner-STR), where maximality means that it is not possible to extend the TR (as a substring of the input) to the left or to the right without violating the definition within the given user-defined parameters. We avoid producing meaningless TRs by imposing also a lower bound on the TRs length. 3.4 Other definitions In Sokol  et al.  ( 2007 ) it is used the following definition:  X  is called a  k-edit ATR  when ∑ i =1 t −1   D E ( x i ,  x i +1 )≤ k , where the last repeat  x t  might be incomplete so  D E ( x t −1 ,  x t ) is computed as the minimum edit distance of  x t  and the prefixes of  x t −1 . This definition is inspired by the evolutionary model of TRs in which it is assumed that TRs are generated by duplicating the last copy of a previous TR, possibly with duplication errors that truncate it. A k-edit repeat is  maximal  if it cannot be extended either to the left or to the right without violating its definition. In Wexler  et al.  ( 2004 ), for a similarity function ϕ that measures the alignment score of two sequences, it is defined a η -Simple ATR  (η-SATR) a string  X  =  x 1  … x t  such that: there exists a motif   so that for every  i  ∈ [1, …,  t ],  . In other words, the TR consists of  t  duplications of a single consensus string   with mutations. Such string   is also called a  Steiner motif  if   is not constrained to be equal to some repeat  x j . Often in practice   is chosen as the repeat  x j  that minimizes the error function, and is called a  pivot motif . The distinction is critical since, as mentioned before, Steiner motifs lead to NP-complete recognition problems, while pivot motifs do not. The η -Neighboring ATR  (η-NATR) is a string  X , so that for each  i  ∈ [1,..,  t  − 1], ϕ( x i ,  x i + )≥η (Wexler  et al. ,  2004 ). The  Pairwise ATR  (PATR) is a string  X , such that for every pair of indices  i ,  j ∈[1,..  t ] 2  with  i  ≠  j  we have ϕ( x i ,  x j )≥η ij , where η ij  is set to be a monotonically decreasing function of | i  −  j |, thus allowing more slackness when comparing distant copies of the basic motif. In Krishnan and Tang ( 2004 ) it is used a definition similar to that of the NATR, except that the Hamming distance is used and that the threshold is not absolute but relative to the length. A γ-Hamming ATR (γ-HATR) is a string  X  such that: for each  i  ∈ [1, … t  − 1],  D H ( x i ,  x i +1 )≤| x i |γ. In Stolovitzky  et al.  ( 1999 ), a more complex definition is given that takes into account the substring alignment score density function for pairs of random substrings of a given length. Here, the definition of a TR  X  depends on the properties of the longer string  Y  into which  X  is embedded. In particular, a (μ,  p )-TR must comply to two conditions: (i) ∑ i =0 t −1  ϕ( x i ,  x i +1 )≥( t  − 1)μ, that imposes an average high similarity score for adjacent repeats, and (ii) define α( p ,  k ) as the value of similarity such that there is probability  p  that two random substrings of length  k  in  Y  have similarity above α( p ,  k ). There must be an index  q  ∈ [1, … t ] such that ϕ( x q ,  x j )≥α( p ,  k ) for all  j  ∈ [1, … t ]. Note that this condition limits the dispersion of the similarity with respect to one of the copies (called the  pivot ). TRF (Benson,  1999 ) uses as final validation algorithm the WDP that tests efficiently the alignments of a given candidate motif with the surrounding portions of the input sequence, so as to determine the maximum number of adjacent repetitions within a user-defined score bound. This implies a notion of TR akin to that of SATRs with pivot motif. Classical results on string alignments (Gusfield,  1997 , p. 351) ensures that, for the metric score given by the sum of motif-repeats distances, the solution found using the optimal pivot motif has a score within a factor (2 − 1/ t ) of the score induced by the optimal Steiner motif. For low levels of errors one could use a pivot-SATR definition doubling the error threshold to capture a Steiner-SATR, however for higher error levels (say, above 25%), doubling the error threshold forces the existing systems to work in a range of values (say, above 50%) where most methods do not perform well. 3.5 Gapped q-grams Let  I  be a finite subset of non-negative integers. We call  I  an  index set . The  span  of  I  is  span ( I ) =  max { i  −  j | i ,  j  ∈  I }, the position of  I  is  pos ( I ) = min i  ∈  I  and the  shape  of  I  is  shape ( I ) = { i  −  pos ( I )| i  ∈  I }. When set  I  has | I | =  q  and  span ( I ) =  s , its shape belongs to the class of ( q ,  s )-shapes. Any set of non-negative integers  Q  containing 0 is a shape. For an alphabet Σ={ A ,  C ,  G ,  T }, a string  S  ∈ Σ *  of length  n  can be seen as a function defined over [0, …,  n  − 1] with values in Σ, and for any subset  I  ⊂ [0, …,  n  − 1] the restriction of  S  to  I , denoted by  S [ I ] a substring of  S . Given any shape  Q  in the class of ( q ,  s )-shapes, all sets  I  ⊂ [0, … n  − 1] such that  shape ( I ) =  Q , form the set of  Indexes ( Q ,  n ). We can use elements from the  Indexes ( Q ,  n ) to generate restrictions for the string  S . Given two index sets  I 1 ,  I 2  ∈  Indexes ( Q ,  n ), we call them  matching  (or  homologous  in  S , if  S [ I 1 ] =  S [ I 2 ]. The value | pos ( I 1 ) −  pos ( I 2 )| is called the  period  of the match. An index set  I  with | I | =  q  and  span ( I ) =  q  − 1 is called an  ungapped q-gram  since its shape is  shape ( I ) = [0, … q  − 1]. If we have an index set  J  with | J | =  q  and  span ( J ) =  s ≥ q  we have a  gapped q-gram  since its shape is formed of non-consecutive integers. In order to generate a population of candidate periods we consider now all possible ( q ,  s )-shapes with  q  = 3 and  s  = 4, 3 ,2. Denoting with – the gaps and with # symbols from Σ, (the first and last positions must be always #), we have the (3, 4)-shapes ##− −#, #−#−# and #−−##; the (3, 3)-shapes #−##, ##−#; and the (3, 2)-shape ###. As noted in Burkhardt and Kärkkäinen ( 2003 ) and Burkhardt and Kärkkäinen ( 2002 ), if we fix an ungapped shape and an error level in Hamming distance, there are error patterns for which every corresponding ungapped q-gram is affected by error. In contrast with the same Hamming error level, for some gapped shapes, there are always some gapped q-grams unaffected by the injected error. Thus using a small complete family of gapped q-grams we can detect the correct period in situations where ungapped q-grams cannot. 1 3.6 Anti-smear weighting Let  P  be a  q -gram in the input string  Y  at position  i . Let  j 1 ,…,  j h  be the next  h  occurrences of  P  in  Y  following the occurrence at position  i . The  h  corresponding detected distances are  x g  =  j g  −  i , for  g  ∈ [1, … h ]. For the period  x g , we increment its weight:
 
where  Q  is a queue holding the last  H  detected distances in the sequential scan of the input string  Y . After the weight update, we enqueue all  h  values  x g  in the queue  Q , and we dequeue an equal number  h  of items. In line with other constants fixed in TRStalker, we have chosen  h  = 5 and  H  = 20 since they do work well in our synthetic experiments for a large range of TR error and length values. A fine tuning of these parameters as a function of the characteristics of the TR sought is possible, but beyond the focus of this article. 3.7 Positional density Let  k  be the period under investigation. Consider the set  K k  of the positions of those  q -grams (i.e. substrings of  Y ) that contribute to the weighting of  k  through the multiplicity weighting. In order to avoid double counting, we always take the position of the first of the two matching probes. Note that, if a position is shared by several pairs of probes it will be counted only once. Let  f  :[1, …, | Y |]→{0, 1} the characteristic function that for each position in  Y  denote the membership of that position to  K k . Consider the  k -window smoothing of  f :  F ( i ) = ∑ j = i i + k   f ( j ) that computes the  k -smoothed density of the function  f , for  i  ∈ [1, …, | Y | −  k ]. Finally, we define a threshold  t ( k ) proportional to the average  k -density by a user-defined constant, and we consider as a candidate position set  CP ( Y ,  k ) = { i  ∈ [1, …, | Y | −  k ]| F ( i )≥ t ( k )}. The output of this positional density computation is a sequence of pairs ( k ,  i ) where  k  is a candidate period and  i  a candidate position. 3.8 Validation The definition of Steiner-STR is composed of two conditions that will be tested in cascade starting from the one less computationally demanding. 3.8.1 Testing condition (a) The WDP technique in Fischetti  et al.  ( 1993 ) solves the following problem. Given a string  P  of length  m  and a text  T  of length  n , with  m  ≪  n , find the best alignment of  P n  (concatenation of  n  copies of  P  in  T ), in time and storage  O ( nm ). Note that a naive application of the standard dynamic programming based optimal alignment of two strings would require  O ( n 2 m ) time/storage. We modify the WDP approach in order to (i) work with edit distance instead of similarity matrices, (ii) take as pattern the candidate initial tandem copy in positions [ i ,  i  +  k  − 1] and as text an adjacent portion of the input string of size  O ( m ), (iii) we iteratively expand the the text length till the termination condition is met and (iv) we stop the matching as soon as the next adjacent copy of the TR differ from the previous one by more than  c μ m  in edit distance. 3.8.2 Testing condition (b) Let  x 1 , … x t  be the candidate TR to test for property (b) that passed the test for property (a). We incrementally compute an approximate generalized median  , using  x i  and the previously computed approximate generalized median string  . Initially  . Let  k  and  h  be two positive integers and  K  = { j / k | j  ∈ [0,  k ]} be the set formed by  k  + 1 equally spaced real values between 0 and 1. For each value α ∈  K  , we determine up to  h median strings  between  x i  and   with weight α. This set of at most  hk  candidates is then searched for the string  a  that minimizes the function ∑ j =1 i   D E ( a ,  x j ). So we set   and start the next iteration. A  median string  of weight α ∈ [0, …, 1] of two strings  a  and  b  is obtained as follows. Compute the edit distance  e = D E ( a ,  b ) and record the set  A ( a ,  b ) of edit operations that transform  a  into  b . Pick any subset of size ⌊α e ⌋ in  A ( a ,  b ). The median weighted string  c  is obtained by applying those operations to the string  a . It is not difficult to show that it holds that  D E ( a ,  c )=α D E ( a ,  b ) and  D E ( b ,  c ) = (1 − α) D E ( a ,  b ). Note that depending on the value of  e  we have   different subsets of  A ( a ,  b ) we can choose. In our algorithm we randomly select   of them. 3.9 Evaluation of recall in synthetic sequences In order to measure the quality of the TRs reported by TRSTalker and by other benchmark algorithms in our synthetic experiments, we need to give a score to a pair of TRs. The higher the similarity of the two TRs, the higher should be the score. Since perfect equality is rare we need a more flexible score function. A TR can be characterized by the triple: ( b ,  p ,  r ), where  b  is the initial position,  p  the period,  r  the repetition number. Also, the same TR covers the positions in  Y  from index  b  to  b + rp  − 1. We identify the TR with the set of positions  Seg ( TR )=[ b ,  b  +  rp  − 1]. Given two TRs  TR 1  and  TR 2  represented as sets of positions, the classical Jaccard coefficient measure of set similarity  JC  is:
 3.9.1 Modified Jaccard coefficient Let  t 0  be a TR embedded in  Y . Even if  t 0  is a TR according to the definition, when we embed  t 0  in a string  Y , it is well possible that  t 0  is not maximal in  Y , thus if an algorithm reports correctly  t ′ ⊃  t 0  there will be a slight penalization in the JC measure. This phenomenon arose a number of times, thus we decided to use a modified version of the Jaccard coefficient, called JC2, where the denominator is changed. The resulting measure is thus more robust w.r.t. this penalization:
 
Given a TR  t 0  and a set of TRs:  T  = { t 1  … t s } we define the best-match  BM ( t 0 ,  T ):
 
and the best-match-score (BMS):
 
In our controlled experiments, the evaluation module knows the embedded TR  t 0  and receives the output of an algorithm  T , giving back the BMS. For a series of experiments, we will report the average of the BMS. Note that BMS has values in the range [0, …, 1], and higher values correspond to better quality. At first sight one might consider this metric as overly generous. However, since we cannot rule out the existence of other TRs in  Y  besides the embedded ones, we do not want to penalize the presence in  T  of valid TRs different from  t 0 . Also, the set  T  will not contain nested TRs. 3.10 Evaluation of recall on biological sequences The evaluation has been carried out according to the following procedure. Let  T TRS ,  T TRF ,  T ATR  be the set of TRs found by TRStalker, TRF and ATRHunter respectively. First, we removed from every set all the TRs that have a Jaccard coefficient greater than a threshold  J  when compared with another TR in the same set. In other words, we removed TR duplicates from every set of results, where two TRs are considered as duplicates when they cover the same region with an approximation  J . Since TRF and ATRHunter have been executed with options that discard all TRs having a score lower than a given threshold, we filtered  T TRS  by removing all the TRs with a score under such value (this has been done to not penalize TRF and ATRHunter with respect to TRStalker). More in detail, TRF has been executed with match, mismatch and indel score equal to 2, 3 and 3, respectively, maximum motif length equal to 2000bp 2 , and threshold equal to 30. ATRHunter has been executed with match, mismatch, gap and terminal gap score equal to 1 0 -1 0, maximum motif length equal to 500bp 3  and threshold equal to 30. For the TRs found by TRStalker, the score is computed by using the same weights used by TRF and ATRHunter then we filtered the results using the same threshold. After the filtering phase, we computed the union of the TRs found by all algorithms,  . The removal of duplicates with threshold  J  is also applied to  U . Naturally the higher the value of  J  less filtering will be performed. 4 DISCUSSION We have performed comparative experiments both with synthetic and with biological sequences. Here we describe the experimental set up, how the synthetic sequences are generated and the outcome of the comparison. For biological data, we briefly indicate the reason why that sequence has been selected, and the new TRs found by the application of TRStalker. 4.1 Synthetic data 4.1.1 Generation of synthetic data We carried out a first set of experiments by using synthetic data. This allows a fine grained control on the amount of mutations introduced within the regions covered by the TRs. The sequences we gave as input to the programs have been built according to the following steps:
 the background sequence is generated by selecting the four bases A,C,G and T with equal probability; a perfect TR is embedded within the previous sequence, the TR is generated as  r  repetitions of a motif with length  l ; the region covered by the TR is mutated according to substitution, insertion and deletion probabilities ( p s ,  p i  and  p d ); the number of substitutions, insertions and deletion for every repetition of the motif is exactly equal to  lp s ,  lp i  and  lp d ; and if the TR is a Steiner-STR, mutations are introduced in every repeat with respect to the consensus motif; if the TR is a NTR, mutations are introduced with respect to the previous repeat. 
 The experiments have been carried out running ATRHunter with these parameters: match, mismatch, gap and terminal gap score equal to 1 0 -1 0 (the most permissive setting on the website); maximum motif length equal to 500 bp (the maximum allowed by the tool). In order to select the definition of TRs among those allowed by ATRHunter, we performed a preliminary set of experiments: the definition that gave the best results was the third one (minimum alignment score). In this case, ATRHunter reports only the TRs that have a score higher than a given threshold. The value of the threshold has been set to 30. For the web-based version of TRF, all the experiments have been carried out with these parameters: match, mismatch and indel score equal to 2, 3 and 5, respectively; maximum period equal to 500; minimum score equal to 30. For the binary version, we used the following ones: match, mismatch and indel score equal to 2, 3 and 3, respectively; match and indel probability equal to 0.75 and 0.20; maximum period equal to 500; minimum score equal to 30. The parameters of the experiments have been set so as to make sure that the minimum allowed score for all the tools tested is attained on the input data. TRStalker is run with the error parameter μ = 0.3 and the constant  c  = 1.5. 4.1.2 Discussion of the comparative experiments For the experiments on NTR ( Fig. 1 ), we tested TRs with motifs of length from 60 to 300, and a number of repeats from 2 to 8. TRStalker has recall always above 95%. TRF (binary) has always a recall above 80% except for TR with repeat number 2 for which the recall drops to 60%. ATRHunter has recall of a about 60%. These experiments confirm the effectiveness of the new techniques for the initial filtering steps.
 Fig. 1. BMS as a function of copy number for NTR. Motif lengths 60 ( a ), 100 ( b ), 100 ( c ) and 300 ( d ). The total length of the input sequence is 10 000 bp; the amount of substitutions, insertions and deletions are equal to 10% of the motif length each (thus with total error allowed of 30%). Every point is the average of 30 measurements and the 95% confidence intervals are shown. Results on Steiner-STR with motifs of length from 60 to 300, and a number of repeats from 2 to 8 are shown in  Figure 2 . Here we notice that all methods have degraded performance for longer motifs (&gt;200 bases) while TRStalker still manages to have recall above 60%. For shorter motifs (of &lt;100 bases) TRF (binary) is able to match TRStalker only when the repeat number is above 6. Thus for a large range of values, TRStalker attains the best performance in recall, or a matching one, always above 80%.
 Fig. 2. BMS as a function of copy number for Steiner-STR. Motif lengths 60 ( a ), 100 ( b ), 200 ( c ) and 300 ( d ). The total length of the input sequence is 10 000 bp; the amount of substitutions, insertions and deletions are equal to 10% of the motif length each (thus with total error allowed of 30%). Every point is the average of 30 measurements and the 95% confidence intervals are shown. The time performance of TRStalker has not been yet optimized. At the moment it is within an order of magnitude of TRF and ATRHunter. More details on the running time are in the  Supplementary Materials . 4.2 Biological sequences Testing of TRStalker on biological sequences has confirmed the potential of our method for finding very fuzzy TRs not detected by TRF and ATRHunter, and, to the best of our knowledge, not reported in literature. We tested the following sequences:
 U43748  Homo sapiens  frataxin gene, promoter region and exon—2465 bp long (FRDA). L3609  Homo sapiens  germline T-cell receptor βchain, complete gene—684 973 bp long (HSBT). NC_001133.8  Saccharomyces cerevisiae  Chromosome I—230 208 bp long (YCh1). 
 4.2.1 Experimental settings The three algorithms have been run with the setting used in the synthetic experiments 4  (thus with a very permissive acceptance policy). In general, none of the three algorithms generates all TRs found by the two others, and in  Table 1  we show the percentage of the TRs found by each algorithm with respect to the union of the TRs found. In  Table 2 , we report some very long TRs that were detected by TRStalker but missed by the other two methods. We check the motif/repeat alignments using the tool  jaligner  ( http://jaligner.sourceforge.net/ ) using the BLOSUM62 score matrix, that confirms the good quality of the motifs found ( Table 3 ).
 Table 1. Evaluation of recall for the three methods under evaluation Algorithm Filter 90% Filter 70% Frataxin TRStalker (TRF filter) 59 (56.2) 43 (56.5) TRStalker (ATR filter) 43 (41.0) 30 (39.4) TRF 24 (22.9) 18 (23.6) ATRHunter 24 (22.9) 23 (30.2) Union 105 (100.0) 76 (100.0) Homo sapiens  T-cell receptor β chain TRStalker (TRF Filter) 22 557 (59.1) 14 137 (60.2) TRStalker (ATR Filter) 18 124 (47.5) 11 427 (48.7) TRF 9977 (26.1) 8521 (36.0) ATRHunter 7392 (19.3) 7034 (29.6) Union 38218 (100.0) 23743 (100.0) Saccharomyces cerevisiae  chromosome I TRStalker (TRF Filter) 7168 (61.8) 4656 (63.5) TRStalker (ATR Filter) 5621 (48.4) 3655 (49.9) TRF 2892 (24.9) 2518 (34.1) ATRHunter 2037 (17.6) 1958 (26.4) Union 11 616 (100.0) 7407 (100.0) Each entry in the table gives the absolute number of unique TR found, and in the percentage of unique TR w.r.t the union of the three methods. For TRSTalker, we used both a TRF-like and an ATRHunter-like filtering (more restrictive) on the TRs found. 
 Table 2. Examples of TRs found by TRStalker and missed by TRF and ATRHunter No. Sequence Seq. length TR start TR end TR length Consensus Repetitions Score Norm. score 1 HSBT 684 973 411 000 413127 2127 1061 2.00 2868 1.384 2 HSBT 684 973 448 001 449687 1686 842 2.00 2310 1.370 3 HSBT 684 973 636 116 638622 2506 1253 2.00 3323 1.326 4 YCh1 230 208 186 168 188347 2179 1089 2.00 3053 1.401 5 FRDA 2465 2029 2407 378 188 2.011 501 1.325 We report the original sequence name and length, the TR starting and ending positions, the TR length and the TR repeating unit length and copy number. The score is computed by assigning +2 to matches and −1 to mismatches and gaps w.r.t the consensus string. The normalized score is the score divided the TR length. 
 Table 3. Motif/repeats alignment scores computed by  jaligner  using the BLOSUM62 score matrix with gap open penalty set to 10.0 and gap extend penalty set to 0.5 for the TRs reported in  Table 2 Seq. No. Repeat Length,  N Identity, n(%) Gaps, n(%) Score HSBT 1 1 1107 805(72.72) 91 (8.22) 3657.00 - 1 2 1093 895(81.88) 70 (6.40) 4291.00 HSBT 2 1 878 638(72.67) 85 (9.68) 3045.50 - 2 2 866 716(82.68) 52 (6.00) 3568.00 HSBT 3 1 1300 1000(76.92) 94 (7.23) 5206.00 - 3 2 1313 1004(76.47) 120 (9.14) 5176.50 YCh1 4 1 1130 895(79.20) 83 (7.35) 4280.50 - 4 2 1123 901(80.23) 77 (6.86) 4345.50 FRDA 5 1 193 149(77.20) 10 (5.18) 723.50 - 5 2 191 146(76.44) 5 (2.62) 765.00 
 4.2.2 Frederich's ataxia Frederich's ataxia is an autosomal recessive degenerative disease involving the central and peripheral nervous system and the heart, that roughly affects one person in 50 000 (Wells,  2008 ). In 1996, it was shown (Campuzano  et al. ,  1996 ) that in 98% of the cases this disease was caused by an abnormal expansion in the copy number of a triplet TR in the first intron of the Frataxin coding sequence. It belongs to the family of  trinucleotide repeat disorders . Very recently (Vissers  et al. ,  2009 ) it has been shown that the local repetitive structure of DNA may play a role in  variable copy number  genomic disorders. Applying TRStalker to the frataxin sequence we detected a divergent TR in positions [2036–2414], of period 188 and copy number 2, to the best of our knowledge not previously reported, that includes the breakpoint region of the repeat disorder ( Table 2 ). Experimental data reported in Brodzik ( 2007 ) on the Frataxin sequence did find a number of short TRs (of period up to 10/13) that are completely covered by the longer fuzzy TR reported by TRStalker. 4.2.3 Human  beta  T-cell receptor locus The cellular immune system detects the presence of pathogens largely through the activation of  T-cell receptor  proteins (TCR) (Glusman  et al. ,  2001 ), which come in four different families α, β, γ and δ. The complete DNA sequence of the human β T-cell receptor locus has been determined (Rowen  et al. ,  1996 ) and it has been found that a large fraction of the locus sequence (about 47%) is formed by locus-specific repeats (Rowen  et al. ,  1996 ). This sequence was selected as a test case for TRStalker because of its richness in repeating elements with the aim of highlighting the ability of TRStalker in finding repeats with high divergence among adjacent copies. Here, ( Table 2 ) we could find a few such repeats apparently not recorded in the GenBank: L36092.2 record, nor found by TRF and ATRHunter (still set with very loose parameters). 4.2.4 Yeast chromosome I Saccharomyces cerevisiae  (baker's yeast) has been the focus of intensive study as the first eukaryotic organism whose genome was completely sequenced (Dujon,  1996 ), and serves as a model organism in basic genomic investigations. Chromosome I (Bussey  et al. ,  1995 ) is the smallest of the 16 chromosomes present in yeast. It has been noticed that the yeast genome is remarkably poor in repeated elements (Dujon,  1996 ), thus finding new TRs in such organism is a challenging task for any algorithm. In  Table 2  we report a TR in position [186168, 188347] of copy number 2 and motif length 1089. This TR is not reported in the TRDB database, while ATRHunter in the same region finds 15 shorter TR of length ranging from 50 to 180. This region, according to the NCBI record, is rich in genes of the DUP240 gene family (encoding membrane proteins). The presence of a fuzzy repeat in this region thus suggests a possible remote gene duplication event. 4.2.5 Performance on biological sequences Reporting interesting single new TRs, as in  Table 2 , is useful to demonstrate that biological relevant TRs are still unknown. We give also an evaluation of the overall behavior of the three different methods on biological sequences. Thus, we compared TRStalker, TRF and ATRHunter by estimating their recall on the three biological sequences with the methodology described in  Section 3.10 .  Table 1  reports (i) the number of unique TRs found by the different algorithms and (ii) the percentage of the union reported by a given algorithm, with two filtering thresholds at  J  = 90% and  J  = 70%. For all the three sequences, TRStalker is able to find a large number of TRs that are not discovered by using the other methods. In practice, a better overall coverage can be attained by using all three methods and merging their results. Although lower  J  values imply a more aggressive filtering, the percentage of the union attained by TRStalker is almost constant. 5 CONCLUSION TRStalker is a novel efficient heuristic algorithm for finding Fuzzy TRs in biological sequences. TRStalker aims at improving the capability of TR detection for a class of fuzzy TRs for which existing methods do not perform well. Initial testing on biological data show that fuzzy TRs not previously reported are present in biologically relevant sequences. In the case of the Frataxin sequence, the fuzzy TR reported is associated with the known variable copy number breakpoint of Frederich's ataxia. Future work will involve testing TRStalker on relevant families of repetitive elements such as centromeric α-satellites. An extension of TRStalker to handle amino acid sequences is under development. Funding : European Community's Seventh Framework Programme FP7/2007-2013 under grant agreement N 223920 (Virtual Physiological Human Network of Excellence) partially. Conflict of Interest : none declared. 
          Supplementary Material 
          
             
                [Supplementary Data] 
             
             
             
          
       </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Java bioinformatics analysis web services for multiple sequence alignment—JABAWS:MSA</Title>
    <Doi>10.1093/bioinformatics/btr304</Doi>
    <Authors>Troshin Peter V., Procter James B., Barton Geoffrey J.</Authors>
    <Abstract>Summary: JABAWS is a web services framework that simplifies the deployment of web services for bioinformatics. JABAWS:MSA provides services for five multiple sequence alignment (MSA) methods (Probcons, T-coffee, Muscle, Mafft and ClustalW), and is the system employed by the Jalview multiple sequence analysis workbench since version 2.6. A fully functional, easy to set up server is provided as a Virtual Appliance (VA), which can be run on most operating systems that support a virtualization environment such as VMware or Oracle VirtualBox. JABAWS is also distributed as a Web Application aRchive (WAR) and can be configured to run on a single computer and/or a cluster managed by Grid Engine, LSF or other queuing systems that support DRMAA. JABAWS:MSA provides clients full access to each application's parameters, allows administrators to specify named parameter preset combinations and execution limits for each application through simple configuration files. The JABAWS command-line client allows integration of JABAWS services into conventional scripts.</Abstract>
    <Body>1 INTRODUCTION The explosion in biological data volume, diversity and complexity, has been paralleled by the development of computational methods to aid data analysis. Locally installed copies of programs give full access to their capabilities, and the ability to run long CPU or memory intensive jobs. However, configuring software to install easily on a range of operating systems is challenging. Instead, many developers make their techniques available through a custom web page, or increasingly as a web service, which provides a flexible, programmable and scriptable ( Hull  et al. , 2006 ) middle ground between installing software locally and accessing via a web browser. There is now a rich ecosystem of public web services ( Bhagat  et al. , 2010 ), but they do not always meet the needs of users. Their interfaces may have limited functionality and even the biggest service providers, such as EBI and NCBI, limit the computational resources (memory/CPU) available to an individual so that total demand can be met. Web services at the University of Dundee were created in 2005 as part of the Jalview project ( Waterhouse  et al. , 2009 ) to provide its users access to ClustalW ( Larkin  et al. , 2007 ), Mafft ( Katoh and Toh, 2008 ) and Muscle ( Edgar, 2004 ) multiple alignment programs and the JPred secondary structure predictor ( Cole  et al. , 2008 ). Although convenient, these suffer from the same public service limitations, and many of the &gt; 20 000 users of Jalview requested the ability to run alignments locally on their workstation or to deploy the same services on their institution's cluster. Although locally installable web service systems such as Opal 2 ( Krishnan  et al. , 2009 ) and SoapLab 2 ( Senger  et al. , 2008 ) are available, they require significant investment in time or expertise to install. In contrast, the JABAWS:MSA system introduced here, is an easy to install web services solution for multiple alignment that can run on a desktop or laptop computer, local server or computer cluster. JABAWS:MSA's rich interface lets the user track execution progress, retrieve results as a data structure instead of file(s) and provides SOAP services conforming to WS-I Basic Profile 1.2/2.0. While developed primarily for the Jalview Desktop, these can be scripted against, and a command-line client for this purpose is distributed with the system. 2 THE SERVER The JABAWS:MSA server and command-line client are both implemented in Java, but the server exploits native programs to provide ClustalW ( Larkin  et al. , 2007 ), Muscle ( Edgar, 2004 ), Probcons ( Do  et al. , 2005 ), T-coffee ( Notredame  et al. , 2000 ) and Mafft ( Katoh and Toh, 2008 ) alignment services. These programs can be difficult to compile or incompatible with some operating systems, so a range of JABAWS:MSA distributions have been created to suit most environments. The basic JABAWS:MSA distribution is a pre-configured Java Web Application aRchive (WAR) conforming to the Servlet 2.4 specification, optionally bundled with the source and prebuilt binaries of its dependent bioinformatics programs. This option is most appropriate for experienced users wishing to set up multiple alignment services for their group or institution, or those working on Linux or Mac operating systems. For most other users, a ‘Virtual Appliance’ (VA) ( http://en.wikipedia.org/wiki/Virtual_appliance ) is supplied that is built on TurnKey Linux and contains a fully operational JABAWS:MSA server. This can be run on a freely available virtualization product such as VMware player ( http://www.vmware.com/products/player ) or Oracle VirtualBox ( http://www.virtualbox.org ). 3 JABAWS:MSA CONFIGURATION OPTIONS A native JABAWS:MSA installation can be tightly integrated with the local computing infrastructure. It allows fine-grained control over the calculations to be run on the server through three user-editable configuration files: ‘Parameters’, ‘Limits’ and ‘Presets’, for each alignment method. The Parameters file defines command-line parameters that are supported by the method, its valid ranges and which parameters are mandatory, while the Presets file allows combinations of parameter settings to be given a name and brief description to explain their use. For example, Mafft has six alternative presets, enabling a range of accuracy or speed-oriented options. The Limits file specifies the maximum length and number of sequences allowed for a method, and separate limits may be set for calculations to be run on the same server as Tomcat, or  via  a cluster queuing system. Further configuration files centralize the definition of application-specific directories, and any command-line flags needed to enable JABAWS to interact correctly with a queuing system. Together, these permit JABAWS to operate efficiently in nearly any compute environment, by executing jobs on the local machine, on a cluster through a queuing system or on whichever resource is most appropriate according to a job's size and the availability of local CPUs. 4 THE CLIENTS Web services provided by a JABAWS:MSA installation can be accessed from any programming language, providing libraries are available for the consumption of SOAP web services. The service interface allows a client to discover the execution limits, command line parameters and named presets for a program, and once a calculation is submitted, check on its progress by retrieving any text normally output to the console by the program. Importantly, data structures and formats for all services are standardized, so the same client can submit data to, and handle the results from any of the alignment programs provided by any JABAWS:MSA server. A lightweight, ready to use JABAWS:MSA command-line client is provided which allows access to JABAWS alignment services. The client is Java based, and is also a useful guide for those who wish to develop or adapt their own clients or test new JABAWS:MSA installations. For non-programmers, a graphical client is available within Jalview 2.6. This client connects to a JABAWS:MSA server to discover which services are available, their parameters and presets, and then populates the user interface with these options. 5 CONCLUSION A collection of portable web services, initially for multiple sequence alignment, has been developed. It is designed to interact with other software or services and be deployed easily on a variety of computing infrastructures, either directly or through the use of virtualization. JABAWS:MSA provides an in-house web services solution that is limited only by locally available computing resources. Furthermore, its client provides fault tolerance by allowing access to multiple server instances, while the service interface gives a high level of control over individual tasks. Comprehensive metadata provided by each service enables their integration with Jalview, to allow novices and experts alike to utilize their local compute resources efficiently. Finally, although JABAWS:MSA is focused on multiple alignment, the JABAWS framework is flexible, and will soon be extended to include analytical methods such as secondary structure and disorder prediction. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Towards reliable named entity recognition in the biomedical domain</Title>
    <Doi>10.1093/bioinformatics/btz504</Doi>
    <Authors>Giorgi John M, Bader Gary D, Wren Jonathan</Authors>
    <Abstract/>
    <Body>1 Introduction PubMed contains over 30 million publications and is growing rapidly. Accurate, automated text mining tools are needed to maximize discovery and unlock structured information from this massive volume of text ( Cohen and Hunter, 2008 ;  Rzhetsky  et al. , 2009 ). Biomedical named entity recognition (BioNER) is the task of identifying biomedical named entities—such as genes and gene products, diseases, chemicals and species—in raw text. Biomedical named entities have several characteristics that make their recognition in text challenging ( Zhou  et al. , 2004 ), including the use of descriptive entity names (e.g. ‘normal thymic epithelial cells’) leading to ambiguous term boundaries, and several spelling forms for the same entity (e.g. ‘ N -acetylcysteine’, ‘ N -acetyl-cysteine’ and ‘NAcetylCysteine’). Many solutions for reliable BioNER have been proposed (e.g.  Delėger  et al. , 2016 ;  Kim  et al. , 2004 ) and current state-of-the-art approaches employ a domain-independent approach, based on deep learning and statistical word embeddings, called bidirectional long short-term memory network-conditional random field (BiLSTM-CRF;  Habibi  et al. , 2017 ;  Lample  et al. , 2016 ;  Ma and Hovy, 2016 ) or modifications to this approach [e.g. transfer learning ( Giorgi and Bader, 2018 ) and multi-task learning ( Wang  et al. , 2018 )]. While BiLSTM-CRF paired with pre-trained word embeddings as inputs is a powerful approach to sequence labeling ( Huang  et al. , 2015 ), it has many trainable parameters which could lead to a reduction in generalizability. Here, we are concerned with the ability of a model to generalize from training on one corpus to testing on another for the same biomedical entity. This simulates a real-world scenario where the model is used to annotate text which is outside the corpus it was trained on, but still within the biomedical domain. Generalization across biomedical corpora appears to be a problem even for less-parameterized machine learning methods. For example, Gimli, an open-source tool for BioNER based on a CRF classifier, achieved an  F 1  score of 87.17% when trained and tested on the GENETAG corpus ( Campos  et al. , 2013a ), but only a 45–55%  F 1  score when trained on the GENETAG corpus and tested on the CRAFT corpus for genes and proteins ( Campos  et al. , 2013b ).  Galea  et al.  (2018)  explore this further by demonstrating that performance of a CRF model for BioNER trained on individual corpora decreases substantially for recognition of the same biomedical entity in independent corpora. They conclude that bias in the available BioNER evaluation corpora is partly to blame. In a simple orthographic feature analysis (i.e. what does a word look like?), they find that features which significantly predict biomedical named entities in one corpus (e.g. number of digits or capital letters, text span length) are not necessarily useful to predict those same entities in a different corpus. We need to address this problem if BiLSTM-CRF models are to be useful in real-world scenarios involving the large-scale annotation of diverse articles. To address this challenge, we evaluate successful ideas from recent work on BiLSTM-CRF models for BioNER ( Giorgi and Bader, 2018 ;  Habibi  et al. , 2017 ;  Wang  et al. , 2018 ) and sequence labeling with BiLSTM-CRFs in general ( Reimers and Gurevych, 2017 ) and propose several model-based strategies to improve generalization, namely: additional regularization via variational dropout, transfer learning and multi-task learning. We assessed the performance of the model on the same corpus it was trained on (‘in-corpus’ performance) and when trained on one corpus and tested on another corpus annotated for the same entity class (‘out-of-corpus’ performance). All proposed strategies achieved an improvement in out-of-corpus performance without degrading the average in-corpus performance. The best improvement resulted from a combination of multi-task learning and variational dropout, which boosts out-of-corpus performance by 10.75%. We make available to the community a user-friendly, open-source tool for BioNLP (‘Saber’) which incorporates these successful model features ( https://github.com/BaderLab/saber ). 2 Materials and methods We evaluated three modifications to a BiLSTM-CRF model’s architecture and training strategy aimed at improving generalization, described briefly below (details in  Supplementary Material ). We use the BiLSTM-CRF neural network architecture introduced by  Lample  et al.  (2016)  as our baseline (BL) model ( Supplementary Fig. S1 ). 2.1 Variational dropout As neural networks have many parameters, model regularization is critical for generalization performance. Traditionally, the ‘dropout’ technique, which randomly drops units during training, is used for this purpose ( Srivastava  et al. , 2014 ). Previous applications of BiLSTM-CRF models for BioNER ( Giorgi and Bader, 2018 ;  Habibi  et al. , 2017 ) have only applied dropout to the character-enhanced word embeddings, the final inputs to the word-level BiLSTM layers, as originally proposed by  Lample  et al.  (2016) . However, no regularization technique is applied to the recurrent layers of the model, which contain the majority of the model’s trainable parameters. Part of the reason for this is likely that the standard dropout implementation is ineffective for regularizing recurrent connections, as it disrupts the recurrent neural networks ability to retain long-term dependencies ( Bayer  et al. , 2013 ;  Pachitariu and Sahani, 2013 ;  Zaremba  et al. , 2014 ). Variational dropout, where the same units are dropped across multiple time steps (in our case, tokens of an input sentence), has been proposed to overcome this ( Gal and Ghahramani, 2016 ) ( Supplementary Fig. S2 ). We hypothesize that regularizing the recurrent layers of the BiLSTM-CRF model via variational dropout will improve out-of-corpus generalization. To test this, we compare the performance of two BiLSTM-CRF models for the task of BioNER: one with the standard dropout strategy ( Giorgi and Bader, 2018 ;  Habibi  et al. , 2017 ;  Lample  et al. , 2016 ), and a second where variational dropout is additionally applied to the input, recurrent and output connections of all recurrent layers. 2.2 Transfer learning Transfer learning is a machine learning research problem which aims to perform a task on a ‘target’ dataset using knowledge learned from a ‘source’ dataset ( Li, 2012 ;  Pan and Yang, 2010 ;  Weiss  et al. , 2016 ). Ideally, transfer learning reduces training time on the target dataset and the amount of labeled data needed to obtain high performance. It can be used to improve model generalization by training on very large, but usually lower quality silver standard corpus (SSC), and then using the learned parameters to initialize training on a smaller, manually generated and more reliable gold-standard corpus (GSC). We recently showed that transfer learning for BioNER ( Giorgi and Bader, 2018 ) reduces the amount of labeled data needed to achieve high performance from a BiLSTM-CRF model, but we did not assess its impact on generalizability. As in  Giorgi and Bader (2018) , here we apply transfer learning by first training on a large, semi-automatically generated and lower quality SSC and then transfer to continued training on a GSC. Like  Giorgi and Bader (2018) , we use CALBC-SSC-III (Collaborative Annotation of a Large Biomedical Corpus) as our SSC ( Kafkas  et al. , 2012 ;  Rebholz-Schuhmann  et al. , 2010 ). 2.3 Multi-task learning Multi-task learning ( Caruana, 1993 ) is a machine learning method in which multiple learning tasks are solved at the same time. The idea is that by sharing representations between tasks, we can exploit commonalities, leading to improved learning efficiency, prediction accuracy and generalizability for the task-specific models, when compared to training the models separately ( Baxter  et al. , 2000 ;  Caruana, 1997 ;  Thrun, 1996 ). Recently,  Crichton  et al.  (2017)  demonstrated that a neural network multi-task convolutional neural network (CNN) model outperforms a comparable single-task model, on average, for the task of BioNER. Similarly,  Wang  et al.  (2018)  found that multi-task learning outperforms single-task learning for BioNER with a BiLSTM-CRF. Neither study explored the effect of multi-task learning on the model’s ability to generalize across corpora. We hypothesize that multi-task learning will improve across-corpora generalization, potentially as a result of exposing the model to more training data. To test this, we compare the performance of a single-task and multi-task BiLSTM-CRF model for the task of BioNER. Our multi-task model (MTM) builds off this BiLSTM-CRF architecture. The MTM is a global model comprised of distinct, task-specific input and output layers, while the hidden layers (and their parameters) are shared across all tasks ( Supplementary Fig. S3 ). We follow  Wang  et al.  (2018)  who found it best to share all hidden layers of the BiLSTM-CRF model for BioNER. During training, all corpora are used to update the parameters of the hidden layers of the model but the output ‘task’ layers are trained using only their corresponding corpus. In our multi-task experiments, two corpora of the same entity type are used to train the model. The ‘train’ corpus is used to train the output layer that is then evaluated on the ‘test’ corpus. The ‘partner’ corpus contributes to hidden layer training and is used to train its output layer, but the corresponding output is not used for performance evaluation. During training, the model optimizes the log-probability of the correct tag sequence for each corpus. In practice, the model is trained in an end-to-end fashion. 3 Results 3.1 Establishing a baseline To establish a baseline for each corpus used in this study, we performed 5-fold cross-validation using the hyperparameters presented in  Supplementary Material , Section 3.5. We use the dropout strategy proposed by  Lample  et al.  (2016)  and employed by  Habibi  et al.  (2017)  and  Giorgi and Bader (2018) , which applies a single dropout layer (with a dropout rate of 0.3) to the character-enhanced word embeddings, which is the final input to the word-level BiLSTM layers. In  Supplementary Table S3 , we present our baseline  F 1  scores along with the best reported  F 1  scores from  Habibi  et al.  (2017) ,  Wang  et al.  (2018)  and  Giorgi and Bader (2018) , all of whom use nearly identical BiLSTM-CRF models and the same word embeddings as used in this study, and  Crichton  et al.  (2017) , who used a CNN-based model for BioNER. Our baseline significantly outperforms previous results obtained with BiLSTM-CRF models for 7 out of the 12 corpora evaluated and was comparable to the best method in the remaining 5 cases. Since our architecture is nearly identical and paired with the same word embeddings, this is likely due to our hyperparameter choice as presented by  Reimers and Gurevych (2017) . For the remainder of the study, we compare all performance scores to our baseline. 3.2 Can BiLSTM-CRF generalize for BioNER? To test the out-of-corpus generalizability of a BiLSTM-CRF model for BioNER, we trained the model on various corpora and evaluated its performance on independent corpora annotated for the same entity type. Even when corpora are annotated for the same entity type, they may have different biases, given that they are typically independently developed with different annotation guidelines. To control for the expected drop in performance due to differing annotation guidelines, we used a relaxed matching criterion in our evaluation (described in  Supplementary Material , Section 3.7). We found that, even with a relaxed matching criterion, performance of the model as measured by  F 1  score falls by an average of 31.16% when the model is evaluated on a corpus other than the one it was trained on ( Supplementary Table S4 ). Out-of-corpus performance was worst for chemicals, falling by an average of 33.45%, followed by genes/proteins (29.27%), species (28.47%) and disease (26.09%). These results demonstrate the dramatically poor out-of-corpus generalizability of BiLSTM-CRF for BioNER, even though the model obtains state-of-the-art results when trained and tested on the same corpus ( Supplementary Table S3 ). 3.3 Improved regularization We next explored the effect that additional regularization of the BiLSTM-CRF model via variational dropout (see Section 2.1) has on both in-corpus and out-of-corpus performance. In  Table 1 , we compare the in-corpus performance of the baseline (BL) BiLSTM-CRF model employing a simple dropout strategy—proposed by  Lample  et al.  (2016)  and used in previous applications of BiLSTM-CRF to BioNER ( Giorgi and Bader, 2018 ;  Habibi  et al. , 2017 )—compared to a model in which variational dropout has been additionally applied to the recurrent layers. We use dropout ratios of 0.3, 0.3 and 0.1 for the input, output and recurrent connections, respectively. In  Table 2 , we measure the effect that variational dropout has on the generalizability of the model, by comparing out-of-corpus performance to the baseline.
 Table 1. In-corpus (IC) performance, measured by  F 1 -score, of the baseline (BL) BiLSTM-CRF compared to a BiLSTM-CRF with variational dropout (VD)  BL VD Entity Corpus Average σ Average σ Chemicals BC4CHEMD 88.46 0.61 
 88.71 
 0.76 BC5CDR 92.82 0.80 
 93.08 
 0.82 CRAFT 84.98 1.98 
 85.22 
 1.37 Disease BC5CDR 84.49 0.33 
 85.10 
 0.56 NCBI-disease 87.01 1.17 
 87.60 
 1.50 Variome 
 85.75 
 2.83 85.69 3.81 Species CRAFT 96.28 2.21 
 96.38 
 2.26 Linnaeus 89.44 3.91 
 89.66 
 7.47 S800 72.75 2.42 
 77.39 
 4.17 Genes/proteins BC2GM 81.48 0.48 
 83.10 
 ** 
 0.50 CRAFT 84.46 6.08 
 86.09 
 5.19 JNLPBA 80.92 2.50 
 81.95 
 2.62 
 Note : In the BL model, dropout is applied only to the character-enhanced word embeddings. In the VD model, dropout is additionally applied to the input, recurrent and output connections of all LSTM layers. IC performance is derived from 5-fold cross-validation, using exact matching criteria. Statistical significance is measured through a two-tailed  t -test. Bold, best scores, σ, standard deviation. ** Significantly different than the BL ( P  ≤ 0.01). Table 2. Out-of-corpus (OOC) performance, measured by  F 1  score, of the baseline (BL) BiLSTM-CRF compared to a BiLSTM-CRF with variational dropout (VD) Entity Train Test BL VD 
 Δ F 1 
 Chemicals BC4CHEMD BC5CDR 
 90.90 
 90.61 −0.29 CRAFT 47.44 
 47.67 
 0.23 BC5CDR BC4CHEMD 71.81 
 72.41 
 0.60 CRAFT 39.55 
 41.30 
 ** 
 1.74 CRAFT BC4CHEMD 40.50 
 42.65 
 2.14 BC5CDR 41.59 
 56.64 
 ** 
 15.05 Diseases BC5CDR NCBI-disease 76.67 
 80.86 
 * 
 4.19 Variome 74.03 
 74.83 
 0.81 NCBI-disease BC5CDR 69.62 
 74.96 
 ** 
 5.33 Variome 74.98 
 75.69 
 0.72 Variome BC5CDR 22.45 
 30.38 
 * 
 7.93 NCBI-disease 40.17 
 45.16 
 ** 
 4.99 Species CRAFT Linnaeus 45.32 
 53.25 
 * 
 7.93 S800 36.88 
 46.10 
 ** 
 9.21 Linnaeus CRAFT 82.49 
 82.85 
 0.36 S800 62.90 
 66.93 
 * 
 4.02 S800 CRAFT 57.09 
 76.44 
 ** 
 19.34 Linnaeus 61.43 
 67.05 
 * 
 5.62 Genes/proteins BC2GM CRAFT 56.04 
 58.17 
 2.12 JNLPBA 69.77 
 70.79 
 ** 
 1.02 CRAFT BC2GM 44.11 
 49.12 
 * 
 5.01 JNLPBA 52.88 
 56.30 
 3.42 JNLPBA BC2GM 51.03 
 55.61 
 4.57 CRAFT 44.29 
 49.08 
 4.79 
 Note : In the BL model, dropout is applied only to the character-enhanced word embeddings. In the VD model, dropout is additionally applied to the input, recurrent and output connections of all LSTM layers. OOC performance is derived by training on one corpus (train) and testing on another annotated for the same entity type (test) using a relaxed, right-boundary matching criteria. Bold, best scores. * Significantly different than the BL ( P  ≤ 0.05). ** Significantly different than the BL ( P  ≤ 0.01). In general, variational dropout has a small positive impact on in-corpus performance. For at least two corpora, S800 and BC2GM, variational dropout leads to a large improvement in performance, but only the latter case is statistically significant. For out-of-corpus performance, variational dropout improves performance for nearly every train/test corpus pair we evaluated, with an average  F 1  score improvement of 4.62%. In some cases, variational dropout leads to sizable improvements in out-of-corpus performance, such as when the model was trained on S800 and tested on CRAFT (19.34%), or trained on CRAFT and tested on BC5CDR (15.05%). In one case—when trained to recognize chemicals on BC4CHEMD and tested on BC5CDR—variational dropout reduced out-of-corpus performance, although the performance difference was minimal (&lt;0.5%). Thus, variational dropout improves the out-of-corpus performance of the model, without degrading in-corpus performance. Regularization of the recurrent layers of a BiLSTM-CRF model via variational dropout, therefore, can improve model generalizability for BioNER. 3.4 Transfer learning In this experiment, we measure the effect that a transfer learning strategy for BiLSTM-CRF has on both in-corpus and out-of-corpus performance. In  Table 3 , we compare the in-corpus performance of the baseline BiLSTM-CRF model (BL) to that of the model trained with transfer learning and in  Table 4  we similarly compare out-of-corpus performance of the two models. In the transfer learning setting, the model was pre-trained on the CALBC-SSC-III corpus, which is annotated for chemicals, diseases, species and genes/proteins, before being trained on one of the 12 GSCs, each annotated for a single entity class. The state of the optimizer is reset during this transfer, but model weights for all layers beside the final CRF layer are retained (see Section 2.2 and  Supplementary Material , Section 3.5).
 Table 3. In-corpus (IC) performance, measured by  F 1 -score, of the baseline (BL) BiLSTM-CRF compared to a BiLSTM-CRF trained with transfer learning (TL)  BL TL Entity Corpus Average σ Average σ Chemicals BC4CHEMD 88.46 0.61 
 88.98 
 0.63 BC5CDR 
 92.82 
 0.80 92.20 0.86 CRAFT 84.98 1.98 
 85.80 
 1.74 Disease BC5CDR 
 84.49 
 0.33 84.41 0.24 NCBI-disease 87.01 1.17 
 87.66 
 0.86 Variome 85.75 2.83 
 86.69 
 3.03 Species CRAFT 96.28 2.21 
 96.55 
 1.72 Linnaeus 89.44 3.91 
 90.72 
 4.90 S800 72.75 2.42 
 74.93 
 3.27 Genes/proteins BC2GM 
 81.48 
 0.48 80.65 * 0.57 CRAFT 84.46 6.08 
 85.50 
 4.59 JNLPBA 80.92 2.50 
 81.56 
 2.73 
 Note : The TL model was pre-trained on the CALBC-Small-III corpus. IC performance is derived from 5-fold cross-validation, using exact matching criteria. Statistical significance is measured through a two-tailed  t -test. Bold, best scores, σ, standard deviation. * Significantly different than the BL ( P  ≤ 0.05). Table 4. Out-of-corpus (OOC) performance, measured by  F 1  score, of the baseline (BL) BiLSTM-CRF compared to a BiLSTM-CRF trained with transfer learning (TL) Entity Train Test BL TL 
 Δ F 1 
 Chemicals BC4CHEMD BC5CDR 
 90.90 
 90.73 −0.17 CRAFT 
 47.44 
 47.02 −0.42 BC5CDR BC4CHEMD 71.81 
 74.27 
 * 
 2.46 CRAFT 39.55 
 41.20 
 * 
 1.64 CRAFT BC4CHEMD 40.50 
 46.15 
 * 
 5.64 BC5CDR 41.59 
 58.57 
 ** 
 16.98 Diseases BC5CDR NCBI-disease 76.67 
 78.51 
 * 
 1.83 Variome 74.03 
 77.18 
 3.16 NCBI-disease BC5CDR 69.62 
 73.19 
 3.56 Variome 74.98 
 76.95 
 * 
 1.97 Variome BC5CDR 22.45 
 50.28 
 ** 
 27.83 NCBI-disease 40.17 
 58.64 
 ** 
 18.47 Species CRAFT Linnaeus 45.32 
 53.37 
 8.04 S800 36.88 
 46.46 
 ** 
 9.57 Linnaeus CRAFT 82.49 
 83.07 
 0.57 S800 62.90 
 67.64 
 * 
 4.73 S800 CRAFT 57.09 
 69.56 
 ** 
 12.47 Linnaeus 61.43 
 67.21 
 * 
 5.78 Genes/proteins BC2GM CRAFT 56.04 
 56.84 
 0.79 JNLPBA 69.77 
 70.27 
 0.50 CRAFT BC2GM 44.11 
 49.69 
 * 
 5.58 JNLPBA 52.88 
 57.91 
 * 
 5.03 JNLPBA BC2GM 51.03 
 57.81 
 * 
 6.78 CRAFT 44.29 
 56.90 
 ** 
 12.61 
 Note : The TL model was pre-trained on the CALBC-Small-III corpus. OOC performance is derived by training on one corpus (train) and testing on another annotated for the same entity type (test) using a relaxed, right-boundary matching criteria. Statistical significance is measured through a two-tailed  t -test. Bold, best scores. * Significantly different than the BL ( P  ≤ 0.05). ** Significantly different than the BL ( P  ≤ 0.01). In general, transfer learning had a small positive effect on in-corpus performance, boosting the average  F 1  score by ∼1%. In contrast, transfer learning had a large positive effect on out-of-corpus performance, improving performance for nearly every train/test pair we evaluated for an average improvement of 6.48%. In a handful of cases, such as when the model was trained on the CRAFT corpus and tested on the BC5CDR corpus, performance improved by over 10%. In a single case (i.e. when the model was trained on Variome and tested on BC5CDR) transfer learning doubled out-of-corpus performance over the baseline. Thus, the use of transfer learning improves the generalizability of BiLSTM-CRF models for BioNER, in some cases dramatically, while preserving in-corpus performance. 3.5 Multi-task learning To assess the effect that a multi-task learning strategy for BiLSTM-CRF has on both in-corpus and out-of-corpus performance, we evaluate a model trained on all corpus pairs within an entity class. Each model is simultaneously trained on a ‘train’ and ‘partner’ corpus, each defined as a separate task, and a ‘test’ corpus is used to evaluate performance on the ‘train’ task (see Section 2.3 and  Supplementary Material , Section 3.5). In  Table 5  we compare the in-corpus performance of the single-task, baseline BiLSTM-CRF model, to that of the MTM. In  Table 6  we similarly compare out-of-corpus performance of the BL and MTM.
 Table 5. In-corpus (IC) performance, measured by  F 1 -score, of the baseline (BL) BiLSTM-CRF compared to the multi-task model (MTM) BL MTM Entity Train Partner Average σ Average σ Chemicals BC4CH. BC5CDR 88.46 0.61 
 88.81 
 0.60 CRAFT — — 88.67 0.50 BC5CDR BC4CH. 92.82 0.80 
 93.00 
 0.55 CRAFT — — 91.52 * 0.68 CRAFT BC4CH. 84.98 1.98 
 85.06 
 1.49 BC5CDR — — 84.74 1.33 Diseases BC5CDR NCBI-disease 
 84.49 
 0.33 83.85 0.64 Variome — — 83.29 * 0.80 NCBI-disease BC5CDR 
 87.01 
 1.17 86.89 1.74 Variome — — 86.27 1.44 Variome BC5CDR 85.75 2.83 
 86.13 
 2.49 NCBI-disease — — 85.73 2.46 Species CRAFT Linnaeus 96.28 2.21 96.82 1.51 S800 — — 
 96.90 
 1.31 Linnaeus CRAFT 89.44 3.91 89.72 4.51 S800 — — 
 92.18 
 3.42 S800 CRAFT 72.75 2.42 
 74.80 
 2.98 Linnaeus — — 74.43 1.90 Genes/proteins BC2GM CRAFT 
 81.48 
 0.48 79.41 ** 0.14 JNLPBA — — 79.60 ** 0.53 CRAFT BC2GM 84.46 6.08 
 87.76 
 2.65 JNLPBA — — 85.36 4.74 JNLPBA BC2GM 80.92 2.50 
 81.61 
 2.53 CRAFT — — 81.15 2.04 
 Note : The MTM is trained on pairs of corpora (train, partner), where each corpus is used during training to update the parameters of all hidden layers. IC performance is derived from 5-fold cross-validation, using exact matching criteria. Statistical significance is measured through a two-tailed  t -test. Bold, best scores, σ, standard deviation. * Significantly different than the BL ( P  ≤ 0.05). ** Significantly different than the BL ( P  ≤ 0.01). Table 6. Out-of-corpus (OOC) performance, measured by  F 1  score, of the baseline (BL) BiLSTM-CRF compared to the multi-task model (MTM) Entity Train Partner Test BL MTM 
 Δ F 1 
 Chemicals BC4CHEMD BC5CDR CRAFT 47.44 
 47.74 
 0.29 CRAFT BC5CDR 90.90 
 90.97 
 0.07 BC5CDR BC4CHEMD CRAFT 39.55 
 44.79 
 ** 
 5.24 CRAFT BC4CHEMD 71.81 
 72.54 
 0.73 CRAFT BC4CHEMD BC5CDR 41.59 
 71.68 
 ** 
 30.09 BC5CDR BC4CHEMD 40.50 
 49.80 
 ** 
 9.30 Diseases BC5CDR NCBI-disease Variome 74.03 
 76.84 
 * 
 2.81 Variome NCBI-disease 76.67 
 77.33 
 0.66 NCBI-disease BC5CDR Variome 74.98 
 76.32 
 * 
 1.34 Variome BC5CDR 69.62 
 70.72 
 1.10 Variome BC5CDR NCBI-disease 40.17 
 69.35 
 ** 
 29.18 NCBI-disease BC5CDR 22.45 
 60.06 
 ** 
 37.61 Species CRAFT Linnaeus S800 36.88 
 50.26 
 13.38 S800 Linnaeus 45.32 
 57.80 
 ** 
 12.48 Linnaeus CRAFT S800 62.90 
 67.90 
 4.99 S800 CRAFT 82.49 
 82.69 
 0.20 S800 CRAFT Linnaeus 61.43 
 67.90 
 ** 
 6.46 Linnaeus CRAFT 57.09 
 80.04 
 ** 
 22.94 Genes/proteins BC2GM CRAFT JNLPBA 69.77 
 70.26 
 0.49 JNLPBA CRAFT 56.04 
 57.17 
 1.12 CRAFT BC2GM JNLPBA 52.88 
 58.78 
 * 
 5.89 JNLPBA BC2GM 44.11 
 45.12 
 1.01 JNLPBA BC2GM CRAFT 44.29 
 52.78 
 * 
 8.49 CRAFT BC2GM 51.03 
 57.35 
 ** 
 6.32 
 Note : The MTM is trained on pairs of corpora (train, partner), where each corpus is used during training to update the parameters of all hidden layers, but only the train corpus task is used for evaluation on another corpus annotated for the same entity type (test) using a relaxed, right-boundary matching criteria. Bold, best scores. * Significantly different than the BL ( P  ≤ 0.05). ** Significantly different than the BL ( P  ≤ 0.01). Multi-task learning, as applied here, appears to have little impact on in-corpus performance. Average performance of the BL and the MTM were nearly identical, at 85.74 and 85.99% respectively, though in a few cases, such as when the model was trained on BC2GM alongside CRAFT or JNLPBA, the MTM significantly underperforms the baseline. On the other hand, multi-task learning improved out-of-corpus performance for every train/partner/test corpus set we evaluated, with an average improvement of 8.42%. In some cases, this improvement was substantial, such as when the model was trained on the Variome and NCBI-Disease corpora and tested on the BC5CDR corpus (37.61%). However, we do observe significant variability overall in the degree of improvement, suggesting that multi-task learning is sensitive to the choice of train/partner pairs. Thus, a multi-task learning strategy of simultaneously training on multiple corpora can substantially boost out-of-corpus performance of BiLSTM-CRF models for BioNER. 3.6 Combining modifications We next evaluate if combinations of the above modifications improve BiLSTM-CRFs model performance above individual modifications ( Fig. 1 ). In general, all combinations of the proposed modifications improve average out-of-corpus performance without degrading in-corpus performance. However, not all combinations are additive. For example, multi-task learning improves out-of-corpus performance by 8.42%, transfer learning by 6.48% but together only by 5.61%. The biggest boost to out-of-corpus performance is achieved by the MTM paired with additional regularization of the recurrent layers via variational dropout, improving average performance by 10.75%. Therefore, we recommend using this combination to produce a model with the highest expected out-of-corpus performance.
 Fig. 1. Violin plot of the average in-corpus (IC) and out-of-corpus (OOC) performance, measured by  F 1  score, of the BiLSTM-CRF model. IC performance is derived from 5-fold cross-validation, using exact matching criteria. OOC performance is derived by training on one corpus (train) and testing on another corpus annotated for the same entity type (test) using a relaxed, right-boundary matching criterion. The average performance of a model employing one of each of the proposed modifications: variational dropout (VD), transfer learning (TL) and multi-task learning (MTL) independently as well as models which employ all combinations of these methods are shown 4 Discussion We demonstrate that BiLSTM-CRF, a popular deep learning-based approach to BioNER and sequence labeling in general, does a poor job generalizing to corpora other than the one it was trained on. While some drop in performance is expected whenever a model is evaluated on data outside the train set, the magnitude of decrease we observed was substantial, falling by an average of over 30%  F 1  score. This was true even though we used a liberal scoring criterion and any two corpora in our experiments were annotated for the same entity type (and sometimes even used similar annotator guidelines, such as the case with BC5CDR and NCBI-Disease). There are two possible explanations for the poor out-of-corpus generalization we observed—either the corpora are biased or insufficient or the model is prone to training in a corpora specific manner. Likely both reasons contribute to the problem. We elected to explore improving the model because corpus creation is extremely laborious and modification of existing corpora would make it difficult to compare our methods against existing solutions. The three modifications we evaluated—variational dropout, transfer learning and multi-task learning—are straightforward to implement and improve across-corpora generalizability without degrading average in-corpus performance. On average, variational dropout improves out-of-corpus performance by 4.62%, transfer learning by 6.48% and multi-task learning by 8.42%, and the best combination of these (multi-task learning and variational dropout) improves average out-of-corpus performance by 10.75%. We provide our model to the community as an easy-to-use tool for BioNLP under a permissive MIT license ( https://github.com/BaderLab/saber ). We note three limitations of our work. First, the most promising modification to BiLSTM-CRF, multi-task learning, appears to be sensitive to the choice of train/partner corpus pairs. A user should, therefore, evaluate multiple train/partner corpora to determine the most beneficial combination for their use case. Second, while transfer learning is not part of our recommended combination, we previously found it to significantly improve performance on smaller corpora (&lt;6000 labels) ( Giorgi and Bader, 2018 ). While we did not evaluate the effect of corpus size, presumably, transfer learning would still be useful and should be considered for small corpora. Third, in our multi-task experiments, we restricted ourselves to training only on two corpora, which were annotated for the same entity type. These restrictions were made only to keep the number and the training time of the multi-task experiments within a reasonable range and not because of inherent limitations in the model’s architecture. Previous work has suggested that multi-task learning with deep neural networks for the task of BioNER may increase performance even when trained on corpora that do not annotate the same entity class.  Crichton  et al.  (2017)  report, for example, that the best partner for Linnaeus (Species) out of 15 corpora was NCBI-Disease (Disease), not another Species corpus. Additionally, previous work ( Wang  et al. , 2018 ) has suggested that training a BiLSTM-CRF on many corpora (i.e. &gt;2) in a multi-task setting leads to sizable improvements in BioNER. Thus, a further direction for our work could be to explore performance improvements as increasing numbers of corpora annotated for different entity types are used for training. We suspect that this could significantly boost out-of-corpus performance. Further, these results suggest that a single model trained on many corpora (and even additionally pre-trained on an extremely large SSC) may produce a robust and reliable tagger suitable for deployment on massive literature databases (such as PubMed). Our strategy for transfer learning involves pre-training a model on a large SSC and transferring the learned weights to initialize training on a smaller, but typically much higher quality, GSC. As we were writing this paper, a novel transfer learning strategy for NLP demonstrated state-of-the-art performance on many benchmark corpora ( https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf ;  Devlin  et al. , 2018 ;  Howard and Ruder, 2018 ). This transfer learning strategy involves first training a language model on a massive corpus of unlabeled text. The task of a language model is to predict the next most probable word given a sequence of words (or more recently, to predict randomly masked words). By learning this task, the model is required to capture both syntax and semantics and is also required to encode something akin to common sense. This is followed by the addition of task-specific layers, which take the output of the language model as input and are trained on labeled data in order for the model to learn a specific classification task such as NER. This transfer learning strategy has already been applied to BioNER with some success ( Lee  et al. , 2019 ;  Sachan  et al. , 2018 ). In the future, we plan to explore this transfer learning strategy for BioNER, and also for other tasks in the biomedical text mining pipeline, such as relation and event extraction. 5 Conclusion While BioNER has recently made substantial advances in performance with the application of deep learning, current applications suffer from poor generalizability in real-world scenarios. We show that out-of-corpus performance of a BiLSTM-CRF model for BioNER (training on one corpus and testing on another annotated for the same entity type) suffers when using a current state-of-the-art model. Straightforward model modifications (variational dropout, transfer learning, and multi-task learning and their combinations) substantially improve across-corpora generalization performance. We propose that our model will significantly outperform previous applications of BiLSTM-CRF models to BioNER when deployed for the large-scale annotation of widely diverse articles, such as the articles found in databases like PubMed. We make our model accessible as an easy-to-use BioNLP tool, Saber ( https://github.com/BaderLab/saber ). Supplementary Material btz504_Supplementary_Materials Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>COSMOS: Python library for massively parallel workflows</Title>
    <Doi>10.1093/bioinformatics/btu385</Doi>
    <Authors>Gafni Erik, Luquette Lovelace J., Lancaster Alex K., Hawkins Jared B., Jung Jae-Yoon, Souilmi Yassine, Wall Dennis P., Tonellato Peter J.</Authors>
    <Abstract>Summary: Efficient workflows to shepherd clinically generated genomic data through the multiple stages of a next-generation sequencing pipeline are of critical importance in translational biomedical science. Here we present COSMOS, a Python library for workflow management that allows formal description of pipelines and partitioning of jobs. In addition, it includes a user interface for tracking the progress of jobs, abstraction of the queuing system and fine-grained control over the workflow. Workflows can be created on traditional computing clusters as well as cloud-based services.</Abstract>
    <Body>1 INTRODUCTION The growing deluge of data from next-generation sequencers leads to analyses lasting hundreds or thousands of compute hours per specimen, requiring massive computing clusters or cloud infrastructure. Existing computational tools like Pegasus ( Deelman  et al. , 2005 ) and more recent efforts like Galaxy ( Goecks  et al. , 2010 ) and Bpipe ( Sadedin  et al. , 2012 ) allow the creation and execution of complex workflows. However, few projects have succeeded in describing complicated workflows in a simple, but powerful, language that generalizes to thousands of input files; fewer still are able to deploy workflows onto distributed resource management systems (DRMs) such as Platform Load Sharing Facility (LSF) or Sun Grid Engine that stitch together clusters of thousands of compute cores. Here we describe COSMOS, a Python library developed to address these and other needs. 2 FEATURES AND METHODS An essential challenge for a workflow definition language is to separate the definition of tools (which represent individual analyses) from the definition of the dependencies between them. Several workflow libraries require each tool to expect specifically named input files and produce similarly specific output files; however, in COSMOS, tool I/O is instead controlled by specifying file  types . For example, the BWA alignment tool ( Fig. 1 a) can expect FASTQ-typed inputs and produce a SAM-typed output, but does not depend on any specific file names or locations. Additionally, tool definitions do not require knowledge of the controlling DRM.
 Fig. 1. ( a ) Tools are defined in COSMOS by specifying input and output  types , not files, and a  cmd()  function returning a string to be executed in a shell.  cpu_req  and other parameters may be inspected by a programmer-defined Python function to set DRM parameters or redirect jobs to queues. ( b ) Workflows are defined using map-reduce primitives:  sequence_, map_  (execute the  align  tool from (a) on each ‘chunk’ in parallel) and  reduce_  (group the aligned outputs by  sample  tag). ( c ) Directed acyclic graph of jobs generated by the workflow in (b) to be executed via the DRM for four input FASTQ files (with  sample  tags s1 and s2, and  chunk  tags of c1 and c2) Once tools have been defined, their dependencies can be formalized via a COSMOS  workflow , which is defined using Python functions that support the map-reduce paradigm ( Dean and Ghemawat, 2004 ) ( Fig. 1 b). Sequential workflows are defined primarily by the  sequence_  primitive, which runs tools in series. The  apply_  primitive is provided to describe workflows with potentially unrelated branching by executing tools in parallel. To facilitate map-reduce in large and branching workflows, COSMOS introduces a tagging system that associates a set of key-value tags (e.g. a sample ID, chunk ID, sequencer ID or other job parameter) with specific job instances. This tagging feature enables users to formalize reductions over existing tag sets or to split by creating new combinations of tags ( Supplementary Fig. S1 ). To execute a workflow, COSMOS creates a directed acyclic graph (DAG) of tool dependencies at runtime ( Fig. 1 c) and automatically links the inputs and outputs between tools by recognizing file extensions as types. All file paths generated by tool connections are managed by COSMOS, automatically assigning intermediate file names. Another major challenge in workflow management is execution on large compute clusters, where transient errors are commonplace and must be handled gracefully. If errors cannot be automatically resolved, the framework should record exactly which jobs have failed and allow the restart of an analysis after error resolution. COSMOS uses the DRMAA library ( Troger  et al.  2007 ) to manage job submission, status checking and error handling. DRMAA supports most DRM platforms, including Condor, although our efforts used LSF and Sun Grid Engine. Users may control DRM submission parameters by overriding a Python function that is called on every job control event. COSMOS’ internal data structures are stored in an SQL database using the Django framework ( https://djangoproject.com ) and is distributed with a Web application for monitoring the state of both running and completed workflows, querying individual job states, visualizing DAGs and debugging failed jobs ( Supplementary Figs S2–S5 ). Each COSMOS job is continuously monitored for resource usage, and a summary of these statistics and standard output and error streams are stored in the database. This allows users to fine-tune estimated DRM parameters such as CPU and memory usage for more efficient cluster usage. Pipeline restarts are also facilitated by the persistent database, as it records both success and failure using job exit codes. 3 COMPARISON AND DISCUSSION Projects such as Galaxy and Taverna ( Wolstencroft  et al. , 2013 ) are aimed at users without programming expertize and offer graphical user interfaces (GUIs) to create workflows, but come at the expense of power. For example, it is straightforward to describe task dependencies in Galaxy’s drag-and-drop workflow creator; however, to parallelize alignment by breaking the input FASTQ into several smaller chunks to be aligned independently, input stages must be manually created for each chunk or the workflow must be applied to each chunk manually. In addition, the user must fix the number of input chunks  a priori . COSMOS resolves this tedious process for the programmer by dynamically building its DAG at runtime. Such limitations may not be a major concern for small-scale experiments where massive parallelization to reduce runtime is not critical; however, when regularly analyzing terabytes of raw data, the logistics of parallelization and job management play a central role. Snakemake ( Köster and Rahmann, 2012 ) looks to the proven design of GNU Make to describe DAGs for complicated workflows, whereas the Ruffus project ( Goodstadt, 2010 ) aims to create a DAG by providing a library of Python decorators. However, neither of these projects directly supports integration with a DRM. The Pegasus system offers excellent integration with DRMs and even the assembly of several independent DRMs using the Globus software; however, the description of some simple workflows can require considerably more code than the equivalent COSMOS code ( Supplementary Fig. S6 ), and the DAG is not determined at runtime, so cannot depend on the input. Bpipe offers an elegant syntax for defining the DAG, but does not include a graphic user interface for monitoring and runtime statistics. Additionally, COSMOS’ persistent database and Web front end allow rapid diagnosis of errors in data input or workflow execution (see  Supplementary Table S1  for a detailed feature comparison). COSMOS has been tested on the Ubuntu, Debian and Fedora Linux distributions. The only dependency is Python 2.6 or newer and the ability to install Python packages; we recommend a DRMAA-compatible DRM for intensive workloads. Funding : This work was supported by the  National Institutes of Health  [ 1R01MH090611-01A1  to D.P.W,  1R01LM011566  to P.J.T., and  5T15LM007092  to P.J.T. and J.B.H.]; and a Fulbright Fellowship [to Y.S.]. Conflict of Interest : L.J.L. is also an employee with Claritas Genomics Inc., a licensee of COSMOS. 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>EWAS: epigenome-wide association study software 2.0</Title>
    <Doi>10.1093/bioinformatics/bty163</Doi>
    <Authors>Xu Jing, Zhao Linna, Liu Di, Hu Simeng, Song Xiuling, Li Jin, Lv Hongchao, Duan Lian, Zhang Mingming, Jiang Qinghua, Liu Guiyou, Jin Shuilin, Liao Mingzhi, Zhang Meng, Feng Rennan, Kong Fanwu, Xu Liangde, Jiang Yongshuai, Valencia Alfonso</Authors>
    <Abstract/>
    <Body>1 Introduction Epigenome-wide association study (EWAS) is an effective tool to identify the association between epigenetic variation and common disease/phenotype ( Rakyan  et al. , 2011 ;  Wahl  et al. , 2017 ). Compared with genome-wide association study (GWAS), the analysis tools of EWAS have lagged behind. To fill this gap, we developed novel and unique features, and improved upon the previous version EWAS1.0 ( Xu  et al. , 2016 ). EWAS1.0 was originally designed only for identifying the association between combinations of methylation levels (beta-value) and diseases. EWAS2.0 ( http://www.ewas.org.cn ) is a fully functional software. 2 Features EWAS2.0 software can perform: (i) epigenome-wide single marker association study; (ii) epigenome-wide methylation haplotype (meplotype) association study and (iii) epigenome-wide association meta-analysis. The methylation data should be cleaned and normalized. For each DNA methylation loci, EWAS2.0 can carry out t-test or logistic regression analysis to identify the significant associations with case/control or binomial phenotype, perform linear regression analysis to identify the significant results associated with continuous phenotype, and calculate the Pearson's correlation coefficients between beta-value and continuous phenotype. According to our ‘population epigenetic framework’ ( Zhao  et al. , 2016 ), EWAS2.0 can analyze the methylation genotypes (menotypes: MM, MU and UU, where M is methylation epi-allele and U is unmethylation epi-allele) data, calculate the epi-allele frequency and identify risk epi-allele (calculate Chi-square,  P -value, odd ratio and 95% confidential interval). EWAS2.0 can also analyze the association between two epi-alleles (M and U) in the same locus, and label the type of the relationships: synergic (two members of homologous chromosomes tend to be methylated simultaneously) or exclusive (one member of homologous chromosomes is methylated, the other member of homologous chromosomes tends to be unmethylated) ( Zhao  et al. , 2016 ). For multiple DNA methylation loci that are physically close to each other, there are non-random associations of epi-alleles between these loci, which we call methylation disequilibrium (MD) ( Zhao  et al. , 2016 ). EWAS2.0 can calculate the MD coefficients ( Zhao  et al. , 2016 ), identify the MD blocks using Gabriel  et al .’s algorithm ( Barrett  et al. , 2005 ;  Gabriel  et al. , 2002 ) and estimate the frequency of meplotype (a group of specific epi-alleles on a chromosome) using Excoffier  et al .’s Maximum Likelihood Estimate method ( Excoffier and Slatkin, 1995 ). For case/control data, EWAS2.0 can scan the whole epigenome and identify the disease-related meplotype (calculate Chi-square,  P -value, odd ratio and 95% confidential interval). We suggest that users perform meplotype analysis to identify the combinations of some SMP loci related to diseases/phenotypes after performing the single SMP analysis. Since the results of the similar EWAS studies are often inconsistent, we developed an epigenome-wide meta-analysis module. At first, EWAS2.0 test the heterogeneity between individual studies using Cochran’s Q-statistics. Then, the fixed effects model (all studies share a common effect size) and a random effects model (each study has a specific effect size) were used to evaluate the association between marker and disease/phenotype. We suggest that users select fixed effects model for low heterogeneity, and random effects model for high heterogeneity. EWAS2.0 software is a JAVA application based on JAVA 1.7 and is freely available at:  http://www.ewas.org.cn . The current status of EWAS2.0 is depicted in  Table 1 . More functions will be added in the future version (such as EWAS for gene region, KEGG pathway, GO categories, network, interacting with genetic marker, regulation of gene expression, RNA modification, histone modification). Some comparisons between different methods can be found in a supplement ( http://www.bioapp.org/ewas/supplement.html ). We expect it to become a useful tool.
 Table 1. Overview of novel functions in EWAS2.0 Category Description -t.test T-test for case/control or binomial phenotype -linear Linear regression analysis for continuous phenotype -logistic Logistic regression analysis for case/control or binomial phenotype -cor The Pearson's correlation coefficients for continuous phenotype -SMP.allele_chisq Chisq-square test for epi-allele: 2 (phenotype)*2 (M vs. U) table -SMP.aa Identify the type of epi-allele association: synergic or exclusive -meplotype Epigenome-wide meplotype association analysis -MD Calculate the MD coefficient -block Identify the MD blocks and calculate the frequency of meplotype -meta Epigenome-wide association meta-analysis </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>SoS Notebook: an interactive multi-language data analysis environment</Title>
    <Doi>10.1093/bioinformatics/bty405</Doi>
    <Authors>Peng Bo, Wang Gao, Ma Jun, Leong Man Chong, Wakefield Chris, Melott James, Chiu Yulun, Du Di, Weinstein John N, Kelso Janet</Authors>
    <Abstract/>
    <Body>1 Introduction Due to complications and limitations of cross-language interfaces (e.g. the R and MATLAB interfaces for Python), most bioinformaticians write separate scripts when tools in different languages are needed for parts of the analysis. Adding to all of the difficulties in interfacing between languages and producing reports from multiple scripts, it is challenging to keep track of separate script files, share fragmented workflows with colleagues, and archive the workflows properly for future reference. A multi-language data analysis environment provides a unified platform on which workflows composed in mixed languages can be easily executed, shared and reproduced. A few such environments under active development include RStudio Notebook, Apache Zeppelin and Beaker Notebook (now BeakerX). RStudio has a clear root in R with limited support for other languages; Zeppelin is designed for data analytics and visualization in large-scale data exploration; Beaker Notebook has now been replaced by BeakerX which supports only Java-based languages. None of those environments support SAS or MATLAB, and that limits their applications in the bioinformatics and biostatistics community. With the goal of developing a versatile environment for daily bioinformatic data analysis, we developed a multi-language notebook environment powered by a Python3-based workflow engine entitled Script of Scripts (SoS), hence its name SoS Notebook. 2 Features SoS Notebook consists of a new kernel and a number of front-end extensions of the Jupyter Notebook platform ( Kluyver  et al. , 2016 ). The SoS kernel acts as a proxy to the SoS workflow engine and a hub between Jupyter and more than 60 existing Jupyter kernels. It executes scripts in multiple languages and coordinates data exchange among the kernels. The SoS front-end extends the single-kernel notebook interface of Jupyter to a multi-language notebook interface with global and cell-level language selectors and a multi-purpose side panel. Using a set of ‘magics’ (special commands prefixed with  % ) and keyboard shortcuts, SoS Notebook provides an interactive notebook environment with explicit and automatic kernel switch and data exchange, line-by-line code execution, and other useful features such as preview of variables and files. An SoS notebook consists of markdown cells and code cells with content executed by either SoS (based on Python 3) or one of the subkernels. Cell kernels can be changed either interactively or using magics  %with  and  %use . Magic  %expand  interpolates cell contents with variables in the SoS kernel, allowing the composition of scripts in different languages using shared notebook variables (e.g. filenames, parameters). Magic  %capture  captures cell outputs as SoS variables so results from one kernel can be passed to and used by other kernels. Along with the main notebook, a side panel is provided to perform a variety of nonpermanent actions, such as executing scripts that will not be saved with the main notebook and showing results of line-by-line execution ( Fig. 1 ).
 Fig. 1. An example SoS notebook that uses SoS (Python) to read data from an excel file, preview it in the side panel as a searchable and sortable table, transfer data to R to annotate ensembl gene id with HGNC names using the Bioconductor package biomaRt, then plot the annotated data in Julia A more powerful data exchange method is provided to exchange variables among kernels for supported languages (Bash, JavaScript, Julia, MATLAB, Octave, Python2 and 3, R, SAS and TypeScript). Because of large differences in datatypes among scripting languages, SoS transfers variables through the creation of independent homonymous variables of the most similar datatypes in the destination language. For example, although  3  and  c(3, 5)  are both numeric arrays in R, they are transferred to Python as integer 3 and numpy  array([3, 5])  respectively. Similarly, Julia’s  Char  and  str  types are both converted to and from Python as  str , while Python pandas  DataFrame  is converted to  data.frame  in R, dataset in SAS,  table  in MATLAB,  dataframe  in Octave and nested dictionaries in JavaScript. Data exchanges are performed automatically for variables with names starting with  sos  and explicitly with magics such as  %put  and  %get . Interestingly, magic ‘% with language –in v1 –out v2 ’ evaluates the contents of a cell in another language with input and output variables as if calling a function in the current kernel. SoS magics provide functions for smooth, interactive data analysis. Of particular interest is a  %preview  magic that previews variables and expressions from any kernel, and local or remote files in many different formats. The preview magic could be triggered explicitly or automatically (e.g. during line-by-line execution) and can preview dataframes in scrollable, sortable and searchable tabular format, or in interactive scatterplots with tooltips. In addition, magic  %render  renders headers, tables and figures from texts produced by any kernel, allowing the generation of a single report from results obtained using multiple languages. SoS Notebook facilitates the generation of reports from multi-language data analysis by providing magics to capture and render results from multiple kernels, and tags, shortcuts, and templates to generate reports in HTML format. For example, the  sos_report  template allows generation of HTML reports that display only selected material. A reader can therefore focus immediately on the core messages of a report and display more details if needed. Output from the  %preview  magic can be included to report large datasets as dynamic tables and plots. 3 Discussions SoS Notebook provides an interactive working environment in which users can apply the most suitable languages and tools for different parts of a data analysis. Resulting notebooks contain detailed descriptions, complete source codes and results. They can be readily shared with others and re-executed to reproduce prior analyses. SoS Notebook also serves as an execution and management console for the SoS workflow system, as described in a companion report (in preparation). This unique combination of interactive environment and workflow engine makes it easy to convert scripts developed for interactive data analysis to a workflow for non-interactive use in high-performance computing environments, further distinguishing SoS Notebook from other multi-language environments. Although SoS currently focuses on commonly-used languages for bioinformatic applications under a Jupyter front end, we are working with the open source community, especially the JupyterLab (successor of Jupyter) team, to support more languages and to port SoS Notebook to JupyterLab. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>aRNApipe: a balanced, efficient and distributed pipeline for processing RNA-seq data in high-performance computing environments</Title>
    <Doi>10.1093/bioinformatics/btx023</Doi>
    <Authors>Alonso Arnald, Lasseigne Brittany N, Williams Kelly, Nielsen Josh, Ramaker Ryne C, Hardigan Andrew A, Johnston Bobbi, Roberts Brian S, Cooper Sara J, Marsal Sara, Myers Richard M</Authors>
    <Abstract/>
    <Body>1 Introduction Quantification of RNA transcripts by next-generation sequencing technologies continues to increase in both throughput and capabilities as sequencing becomes more affordable and accessible ( McGettigan, 2013 ). Unlike gene expression microarrays, RNA-seq not only quantifies gene expression levels but also measures alternative splicing, transcript fusions, and RNA sequence variants ( Finotello and Di Camillo, 2015 ;  Koboldt  et al. , 2012 ;  Maher  et al. , 2009 ). This broad spectrum of applications has fostered development of a rich set of bioinformatics methods focused on each processing stage ( Conesa  et al. , 2016 ). Current RNA-seq data primary analysis applications usually apply a single processing step, involve complex dependencies between processing stages, and depend on the sequencing protocol performed (see Supplementary Section S1). Consequently, there is an increasing need for tools orchestrating the analysis workflow to ensure repeatability of RNA-seq data processing. In addition to the need for data processing integration, the computational requirements of some RNA-seq analysis steps are a bottleneck ( Scholz  et al. , 2012 ) and, the use of high-performance computing (HPC) clusters is unavoidable. Because HPC clusters are a valuable and often limited resource, tools integrating RNA-seq processing stages must be carefully designed and optimized. Considering these challenges, we developed a balanced, efficient and distributed pipeline for RNA-seq data analysis: aRNApipe ( a utomated  R NA -seq  pipe line). This pipeline was optimized to efficiently exploit HPC clusters, to scale from tens to thousands of RNA-seq libraries, and includes modules yielding complete RNA-seq primary analysis. 2 Methods aRNApipe is designed to overcome the challenges of integration, synchronization and reporting of RNA-seq data analysis by using a project-oriented and balanced design optimized for HPC clusters ( Fig. 1 ). Fig. 1. aRNApipe workflow for primary analysis of RNA-seq data The core application of aRNApipe (Supplementary Section S1) includes six operating modes: (i) executing a new analysis, (ii) updating a previous analysis to include new samples or enable new modules, (iii) showing analysis progress, (iv) building a project skeleton, (v) showing available genome builds, and (vi) stopping an ongoing analysis. 
 Input data:  aRNApipe requires two input files: (i) analysis configuration and (ii) samples to include in the analysis. In the configuration file, the user can set the executing parameters, including enabling/designating arguments of processing modules, assigning computational resources to modules, and selecting the reference genome build. 
 Current applications:  aRNApipe currently includes applications covering the main variations of RNA-seq data generation (Supplementary Section S2). Throughout the workflow, a main daemon process manages pipeline execution (i.e. inter-dependencies between applications) and monitors analysis of each sample at each stage ( Fig. 1 ). First, low-quality reads/bases and adapter sequences can be filtered. Then, a second stack of applications is run in parallel, including assessment of raw data quality, transcript alignment and quantification and identification of gene fusions. The main process launches a third stack of analyses including quantification of genes and exons, conversion of SAM files to BAM sorted files, and alignment quality. Finally, a fourth stack of applications including variant calling and alternative splicing modules are run. 
 Report generation:  The Spider is an aRNApipe add-on module that generates interactive web reports summarizing an aRNApipe analysis. These reports review each module, including sample quality control metrics ( Supplementary Figs S1 and S2 ). The user can also observe the computational resources used by each module and access all logs generated during analysis ( Supplementary Fig. S3 ). Additionally, the Spider generates matrix-like count data files of raw counts, RPKMs and corresponding annotation files (i.e. gene identifier and length). Supplementary Section S3 provides a list of generated outputs. 
 Reference builder:  The programs used for RNA-seq data processing often use different formats and standards. To address this problem and to provide a centralized repository of available genome builds, aRNApipe includes a reference builder that generates all required files for a genome build based on initial files obtained from sources like NCBI and Ensembl repositories (Supplementary Section S4 and Fig. S4). 
 Implementation:  aRNApipe has been developed using Python 2.7 ( Supplementary Fig. S5 ). When running on an HPC cluster, aRNApipe relies on the workload management application to submit jobs for each processing stage, taking into account cross-stage dependencies and custom resource requirements for each stage. aRNApipe has been implemented with the workload management system IBM Platform LSF, but its design allows quick migration to any other workload manager by editing one Python library (Supplementary Section S5). Additionally, a single-machine version is also provided. A configuration library provides supply paths to all applications used by aRNApipe. 3 Results We have extensively tested aRNApipe and used it to analyze hundreds of RNA-seq libraries with multiple configurations, including different species, different genome builds and different RNA-seq protocols. The reports generated for four example datasets can be accessed online ( http://arnapipe.bitbucket.org ): (i) Strand-specific paired-end RNA-seq data from nine human samples of different tissues (GSE69241), (ii) unstranded single-end data from two paired normal and colorectal tumor tissues (GSE29580), (iii) unstranded paired-end data from 11 melanoma samples (GSE20156) and 1 prostate cancer cell line (NCIH660), and (iv) in-house unstranded paired-end data from 20 zebrafish libraries. 4 Conclusions aRNApipe provides an integrated and efficient workflow for analyzing single-end and stranded or unstranded paired-end RNA-seq data. Unlike previous pipelines, aRNApipe is focused on HPC environments and the independent designation of computational resources at each stage allows optimization of HPC resources (see Supplementary Section S1). This application is highly flexible because its project configuration and management options. The Spider provides functional reports for the user at all analytical stages and the reference builder is a valuable genome build manager. Implementation of this pipeline allows users to quickly and efficiently complete primary RNA-seq analysis. Supplementary Material Supplementary Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Multi-netclust: an efficient tool for finding connected clusters in multi-parametric networks</Title>
    <Doi>10.1093/bioinformatics/btv479</Doi>
    <Authors>Kuzniar Arnold, Dhir Somdutta, Nijveen Harm, Pongor Sándor, Leunissen Jack A.M.</Authors>
    <Abstract/>
    <Body>Bioinformatics  (2010)  26 (19)  2482–2483 The authors of the above article would like it to be known that the principle of Multi-netclust is incorrectly illustrated in Figure 1. The connected clusters shown in B and C do not correspond with the aggregation rules/formulae indicated below the illustrations. Swapping the clusters with one another restores this correspondence. The authors apologize for this error. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>ISMB/ECCB 2017 proceedings</Title>
    <Doi>10.1093/bioinformatics/btx321</Doi>
    <Authors>Beerenwinkel Niko, Bromberg Yana</Authors>
    <Abstract/>
    <Body>The 25th Annual Conference Intelligent Systems for Molecular Biology (ISMB), held jointly with the 16th annual European Conference on Computational Biology (ECCB), took place in Prague, Czech Republic, July 21–25, 2017 ( http://www.iscb.org/ ismbeccb2017). This special issue of Bioinformatics serves as the proceedings for this official conference of the International Society for Computational Biology (ISCB,  http://www.iscb.org/ ). The year 2017 was inaugural for a new program format introducing the ISCB Communities of Special Interest (COSIs) as scientific themes within the conference. The 12 COSIs presenting at ISMB/ECCB 2017 ( Table 1 ) had previously organized the pre-conference Special Interest Group meetings. This year, the program incorporated COSI sessions including proceedings, highlights (previously published research), and late-breaking research talks in addition to special sessions, workshop, posters and other scientific talks submitted directly to ISMB/ECCB.
 Table 1. Thematic areas of ISMB/ECCB 2017. Areas have been defined as Communities of Special Interest (COSIs), except ‘Other Topics’ COSI/Area Short name Area chairs No. of reviewers No. of submissions No. of accepted papers 3Dsig: Structural Bioinformatics and Computational Biophysics 3Dsig Jianlin Cheng, Charlotte Deane, Rafael Najmanovich 30 18 4 Automated Function Prediction Function Simon Kasif 15 10 2 Bio-Ontologies Bio-Ontologies Michel Dumontier, Philippe Rocca-Serra, Karin Verspoor 27 9 2 Biological Data Visualization BioVis Alexander Lex, Michel Westenberg 5 2 0 Computational Modeling of Biological Systems SysMod Laurence Calzone, Andreas Dräger, Tomas Helikar, Jonathan Karr, Nicolas Le Novere, Julio Saez-Rodriguez 5 24 4 Computational RNA Biology RNA Rolf Backofen, Jerome Waldispuhl 24 26 3 Critical Assessment of Massive Data Analysis CAMDA David P. Kreil 10 4 2 High-Throughput Sequencing Algorithms and Applications HitSeq Can Alkan, Valentina Boeva, Ana Conesa, Francisco De La Vega, Dirk Evers, Kjong Lehman, Gunnar Rätsch 91 55 8 Network Biology NetBio Alexander Pico, Natasa Przulj 34 39 6 Regulatory Genomics RegGen Finn Drablos, Jason Ernst, Saurabh Sinha, Lonnie Welch 45 31 5 Translational Medicine Informatics and Applications TransMed Bissan Al-Lazikani, Irina Balaur, Wei Gu, Winston Hide, Roland Krause, Mansoor Saqi, Venkata Satagopam 39 33 4 Variant Interpretation VarI Yana Bromberg, Emidio Capriotti, Hannah Carter 21 13 2 Other Topics Other (no COSI) Yana Bromberg, Niko Beerenwinkel 24 15 3 Total 370 (208 unique) 279 45 ISMB/ECCB is the leading international forum in the field for presenting new methods and research results and for facilitating discussions at all levels of expertise. The 45 papers in this volume were selected from 279 original submissions, resulting in an acceptance rate of 16%. Submitted papers accumulated 3.2 reviews on average. The review process included 208 expert reviewers (with some reviewers working for more than one COSI) led by 42 Area Chairs ( Table 1 ) to produce 899 reviews. The Area Chairs were appointed by the individual COSIs, and included a mix of experienced individuals intimately familiar with the field as well as with the review process. Manuscripts submitted to the proceedings track were assigned to COSIs based on authors’ suggestions and on the judgement of Area and Proceedings Chairs. An additional track was created to house all submissions unrelated to any of the existing COSI topics. The submissions in this ‘Other Topics’ area mostly addressed questions in microbiome analysis and evolutionary modeling. With the COSI-based structure, proceedings areas have become more community-driven and dynamic. We believe that going forward in this manner new emerging scientific themes can be detected quicker and represented better. After initial screening, submissions were sent out to expert reviewers selected by the Area Chairs from the program committee. Each paper and its reviews were then discussed online among reviewers and Area Chairs. Finally, Area and Proceedings Chairs discussed the ranked papers to arrive at a subset of 46 papers, which were conditionally accepted. In continuation of the two-tier review process employed at ISMB 2016, Area Chairs, Proceedings Chairs and sometimes the corresponding reviewers inspected the revised papers again. All but one paper were re-submitted. These 45 revised papers were judged to have satisfactorily addressed the reviewers’ comments and were finally accepted into the proceedings. We would like to thank all authors of the 279 submissions for submitting their work. The excellent scientific quality of these papers is pivotal for the impact and the success of the conference. We believe that we have identified 45 outstanding pieces of bioinformatics research for the proceedings track. We note, however, that the review process was subject to some re-organization largely due to the new COSI-based conference format. It is, therefore, possible that excellent papers have not been fully recognized as such. Nevertheless, we know that submissions were evaluated fairly and diligently and hope that all authors have received useful feedback. Like the conference itself, the review and selection process of the proceedings papers is a community effort. We would like to express our deep gratitude to all Area Chairs, members of the program committee, and external subreviewers for their outstanding efforts and rigorous focus on scientific quality. We would like to express deep gratitude to Steven Leard for all help with the review process—without his organizational support our work would not be possible. We would also like to thank the team at Oxford University Press for consistently handling this ISMB/ECCB special proceedings issue. Last, but certainly not least, we thank all members of the ISMB Steering Committee for their expert advice and support. 
 Conflict of Interest : none declared. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>checkMyIndex: a web-based R/Shiny interface for choosing compatible sequencing indexes</Title>
    <Doi>10.1093/bioinformatics/bty706</Doi>
    <Authors>Varet Hugo, Coppée Jean-Yves, Wren Jonathan</Authors>
    <Abstract/>
    <Body>1 Introduction The sequencing of RNA and DNA libraries has become a standard experimental tool in the life sciences. It can notably be used to characterize and understand biological mechanisms ( Goodwin  et al. , 2016 ). For instance, knowledge of DNA sequences can help researchers to discover disease-specific single nucleotide polymorphisms, whereas transcriptome analyses can reveal differentially expressed genes or provide information for genome annotation. As sequencing capacities increase, some biological applications do not require all the reads produced by a single sequencing lane. Hence, in order to optimize costs, researchers often choose to sequence several samples simultaneously by adding short (typically 6- to 8-base-pair) sample-specific barcodes to their libraries. The indexes are then read by the sequencer, and each read is assigned to its original sample via an a posteriori demultiplexing procedure. In this context, the selection of compatible indexes is a crucial step; if the indexes are not compatible, the sequencer will not be able to read each index entirely. Indeed, the compatibility of a set of indexes depends on (i) the indexes’ sequence composition at each position, and (ii) the sequencer’s chemistry (e.g. Illumina HiSeq or NextSeq) [Illumina Index Adapters Pooling Guide, Document # 1000000041074 v02 (2018)]. By way of an example, two- or four-channel chemistry requires each color-transformed nucleotide to be present at least once at each position. Using incompatible indexes would cause the demultiplexing procedure to fail, and so the sequencing reads would not be assigned to the corresponding biological samples. Accordingly, the reads could not be used as input for any future bioinformatics analyses. Although simultaneously sequencing a pool of libraries can lower costs, this strategy must therefore be carefully planned. Several tools have been developed to check index compatibility. For instance, Illumina’s Experiment Manager (IEM) software checks the compatibility of indexes provided by the user. However, IEM is available only for computers running Windows operating systems, and cannot build combinations that fulfil complex criteria (such as the use of single indexes in several lanes). sicTool ( http://amaryllisnucleics.com/sicTool/ ) also provides a web interface with advanced options but does not handle the different Illumina chemistries. Here, we describe the checkMyIndex web-based application. When given a set of available indexes, checkMyIndex generates compatible combinations that meet the idiosyncratic experimental conditions imposed by both the research objective and the user’s preferences. Moreover, checkMyIndex can help staff in core facilities to generate good-quality, ready-to-analyze sequencing data. 2 Results We used the popular R package shiny ( Chang  et al. , 2017 ) to develop a user-friendly application (available free of charge at checkmyindex.pasteur.fr) dedicated to searching for compatible combinations of indexes. The corresponding R source code is hosted on GitHub ( https://github.com/PF2-pasteur-fr/checkMyIndex ) so that researchers can also run the application on their own computer. Note that checkMyIndex uses only native R code, and shiny is the only additional package required. An R script is also available for advanced users willing to run checkMyIndex from the command line. Our application supports single- and dual-indexing in order to achieve higher multiplexing rates and decrease error rates ( Kircher  et al. , 2012 ). checkMyIndex is also compatible with the various chemistries used by the Illumina HiSeq, MiSeq, NextSeq and iSeq devices. With the four-channel chemistry (HiSeq and MiSeq), for instance, a red laser detects A/C bases and a green laser detects G/T bases; the indexes are compatible if there is at least one red light and one green light at each position. We refer the reader to both the checkMyIndex help page and the Illumina documentation for more detailed information on the different chemistries. In practice, the user only needs to provide his/her available indexes as a simple, two-column, tab-separated text format file. The first column contains the index identifiers, and the second contains the corresponding short sequences (see either the GitHub repository or the checkMyIndex help page for examples). When the indexes are loaded, a dissimilarity score is attributed to each one. If the  N  indexes are labelled from  I 1  to  I N , the score for index  k  is defined as  min j ≠ k d H ( I k , I j ) , where  d H  is the Hamming distance (the number of mismatches). This enables the user to check that each index differs sufficiently from all the others, so that the demultiplexing procedure can tolerate sequencing errors. Next, several constraints have to be defined via the interface: the total number of samples, the multiplexing rate (i.e. the number of samples per pool/lane) and whether the same index or the same combination of indexes can be used several times. The latter constraint has been designed to ensure a certain level of diversity and thus avoid the same indexes from being used all the time. Moreover, using unique indexes allows one to perform a small sequencing test on a single lane by pooling all the libraries. From a technical standpoint, it is sometimes impossible or excessively length to find a solution that fulfils all the constraints. In fact, the number of index combinations to be tested can increase rapidly as the multiplexing rate rises. If  n  is the number of available indexes and  m  is the multiplexing rate, the number of possible combinations is defined by  n ! / ( m ! ( n − m ) ! ) . For instance, when looking for combinations of  m  =   12 indexes in a list of  n  =   48, the program may have to generate and test the compatibility of almost  7 × 10 10  combinations. Since adding an index to an already compatible set does not revoke compatibility, our trick in this situation is to find a partial solution with a smaller number of samples per pool/lane and then to complete it to obtain the desired multiplexing rate. In practice, the algorithm decreases the desired multiplexing rate step by step until the number of possible combinations falls to below 2 00 000. This quickly provides an intermediate, partial solution that can then be completed with some of the remaining indexes. Using the figures above, a partial solution can be easily found with  m  =   4 (i.e. 1 94 580 possible combinations) and completed to reach  m  =   12. In most situations, checkMyIndex therefore returns a solution in a few seconds. It should be noted that a new score is computed for each index in the proposed solution, in order to assess its dissimilarity vs. the other indexes in the pool/lane. Lastly, the user can visualize a heat-map-like plot of the solution in the corresponding tab ( Fig. 1 ). Furthermore, the table containing the proposed flow cell design can be downloaded and saved for future experiments or processing.
 Fig. 1. A heat-map-like plot of a proposed flow cell design for sequencing 18 samples using 3 lanes and unique indexes. Samples (in rows) are grouped by pool/lane, and each nucleotide in each index is color-coded as a function of the chosen Illumina chemistry. Sample IDs are printed on the left, and index IDs are printed on the right 3 Conclusion checkMyIndex is a user-friendly, easy-to-use web application that facilitates searching for compatible indexes in sequencing experiments. Solutions are returned quickly (in a few seconds) by the underlying algorithm, and satisfy the constraints imposed by the user (such as not using the same index several times). Moreover, the structure of the R code will allow new features to be added (e.g. when new chemistries are developed). </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Predicting phenotypes from microarrays using amplified, initially marginal, eigenvector regression</Title>
    <Doi>10.1093/bioinformatics/btx265</Doi>
    <Authors>Ding Lei, McDonald Daniel J</Authors>
    <Abstract/>
    <Body>1 Introduction A typical scenario in genomics is to obtain expression measurements for thousands of genes from microarrays or RNA-Seq which may be relevant for predicting a particular phenotype. Such studies have been useful in relating specific genetic variations to a wide variety of outcomes such as disease specific indicators ( Lesage and Brice, 2009 ;  Barrett  et al. , 2008 ;  Burton  et al. , 2007 ;  Sladek  et al. , 2007 ); drug or vaccine response ( Saito  et al. , 2016 ;  Kennedy  et al. , 2012 ); and individual traits like motion sickness ( Hromatka  et al. , 2015 ) or age at menarche ( Elks  et al. , 2010 ;  Perry  et al. , 2014 ). In these scenarios, researchers are interested in the accurate prediction of the phenotype and the identification of a handful of relevant genes with a reasonable computational expense. With these goals in mind, supervised linear regression techniques such as ridge regression ( Hoerl and Kennard, 1970 ), the lasso ( Tibshirani, 1996 ), the Dantzig selector ( Candes and Tao, 2007 ) or other penalized methods are often employed. However, because phenotypes tend to be the result of groups of genes, which perhaps together describe more complicated biomechanical processes, rather than individual polymorphisms, recent approaches have tried to account for this group structure. Techniques such as the group lasso ( Yuan and Lin, 2006 ) can predict the response with sparse groupings of coefficients as long as the groups are partially understood ahead of time. In contrast, unsupervised methods such as principal components analysis ( Hotelling, 1957 ;  Jolliffe, 2002 ;  Pearson, 1901 ) are often used directly on the genes when no phenotype is being examined ( Alter  et al. , 2000 ;  Sladek  et al. , 2007 ;  Wall  et al. , 2003 ). Finally, modern approaches developed specifically for the genomics context such as supervised gene shaving ( Hastie  et al. , 2000 ), tree harvesting ( Hastie  et al. , 2001 ), and supervised principal components ( Bair and Tibshirani, 2004 ;  Bair  et al. , 2006 ) have sought to combine the presence of a response with the structure estimation properties of eigendecompositions from unsupervised techniques to obtain the best of both. It is this last set of techniques that most closely resemble the approach we present here. We give a more detailed discussion of supervised principal components next, before motivating our method with an example. 
 Notation:  We will use bolded letters  M  to indicate matrices, capital letters to denote column vectors, such that  M j  is the  j th  column of the matrix  M , and lower case letters  m i  to denote row vectors (a single subscript) or scalars ( m ij  being the  i ,  j  element of  M ). We will use the notation  M A  to mean the columns of  M  whose indices are in the set  A  and  [ k ] = { 1 , … , k } . Finally, for a matrix  M , we write the singular value decomposition (SVD) of  M = U ( M ) Λ ( M ) V ( M ) ⊤  and define  M †  to be the Moore-Penrose inverse of  M . In the case only of the design matrix  X  discussed below, we will use the more compact decomposition  X = U Λ V ⊤ . 1.1 Supervised eigenstructure techniques The first technique for extending unsupervised principal components analysis to the case where a response is available is principal components regression (PCR,  Hotelling, 1957 ;  Kendall, 1965 ). Instead of regressing the response on all the available covariates as in ordinary least squares (OLS), PCR first performs an eigendecomposition of the empirical covariance matrix and then regresses the response on the subset of principal components corresponding to the largest variances. Defining  Y ∈ ℝ n  to be the centered response vector, and  X  to be the  n  ×  p  centered design matrix, write the (reduced) SVD of  X  as  X = U Λ V ⊤ .  For some integer  d ≤ p , the principal components regression estimator is given as the solution to
 Γ ^ P C R = argmin Γ | | Y − U [ d ] Λ [ d ] Γ | | 2 2 , 
which has the closed form representation
 Γ ^ P C R = ( ( U [ d ] Λ [ d ] ) ⊤ U [ d ] Λ [ d ] ) − 1 ( U [ d ] Λ [ d ] ) ⊤ Y = Λ [ d ] − 1 U [ d ] T Y . 
Since this solution is in the space spanned by the principal components, it is easy to rotate the estimate back onto the span of  X: β ^ P C R : = V [ d ] Γ ^ P C R = V [ d ] Λ [ d ] − 1 U [ d ] T Y . Then any elements of  β ^ P C R  which are identically zero imply the irrelevance of those genes for predicting the phenotype while the columns of  V [ d ] ⊤  can be interpreted as indicating groupings of individual genes. Principal components regression performs well under certain conditions when we believe that there are natural groupings of covariates (linear combinations) which are useful for predicting the response. However,  Lu (2002)  and  Johnstone and Lu (2009)  show that the empirical singular vectors  U [ d ]  are poor estimates of the associated population quantity (the left singular vectors of the expected value of  X ) unless  p / n → 0  as  n → ∞ . In particular, when  p ≫ n , as is common in genomics where the number of gene expression measurements is much larger than the number of patients, PCR will suffer. To avoid this flaw in PCR, various approaches have been proposed.  Hastie  et al.  (2000)  proposed a method called ‘gene shaving’ that is applicable to both supervised (given a phenotype) and unsupervised (only gene expressions) settings. In the supervised setting, it works by computing the first principal component and ranking the genes using a combined measure that balances the principal component scores and the marginal relationship with the response. Those genes with lowest combined scores are removed and the process is repeated until only one gene remains, resulting in a nested sequence of clusters containing fewer and fewer genes. Then one chooses a cluster along this sequence, orthogonalizes the data with respect to the genes in that cluster, and repeats the entire process again, iterating until the desired number of clusters has been recovered. This procedure is somewhat computationally expensive as well as requiring both the cluster sizes and the number of clusters to be chosen. An alternative with somewhat similar behavior is supervised principal components (SPC,  Bair and Tibshirani, 2004 ;  Bair  et al. , 2006 ). SPC avoids the high-dimensional regression problem by first selecting a much smaller subset of useful genes which have high marginal correlation with the phenotype (in contrast to gene shaving, which uses the marginal correlation and the covariance between genes). By screening out most of the hopefully irrelevant genes, we can return to the scenario where  p  &lt;  n . In follow-up work,  Paul  et al.  (2008)  show that, if a small marginal correlation with the response implies irrelevance for prediction, then SPC will find any truly relevant genes and predict the phenotype accurately. They also suggest using lasso or forward stepwise selection after SPC to further reduce the number of genes. However, if some genes have small marginal relationship with the response but large conditional relationship, they will be erroneously ignored by SPC. It is this last property that our method attempts to correct. We now illustrate that the screening step of SPC is likely to remove important genes in typical applications before discussing how our procedure avoids suffering the same fate. 1.2 A motivating example To motivate our methodology in relation to previous approaches, we examine a dataset consisting of 240 patients with diffuse large B-cell lymphoma (DLBCL,  Rosenwald  et al. , 2002 ) in some detail. Each patient is measured on 7399 genes, and her survival time is recorded. Previous approaches rely on the assumption that a small marginal correlation between the response variable, in this case patient survival time, and the vector of expression measurements for a particular gene is sufficient for guaranteeing the irrelevance of that particular gene for prediction. To make this assumption mathematically precise, suppose  y = x ⊤ β + ϵ ,  where  y  is the response,  x  is a vector of gene expression measurements, and  ϵ  is a mean-zero error. Then, the assumption can be stated mathematically as  Cov ( x j , y ) = 0 ⇒ β j = 0 . While reasonable under some conditions, this assumption is perhaps too strong for many gene expression datasets. Very often, individual gene expressions are only predictive of phenotype in the presence of other genes. We can rewrite this assumption using the population covariance matrix between genes,  Cov ( x , x ) = Σ x x , and the vector-valued covariance between gene expressions and phenotype,  Cov ( x , y ) = Σ x y . Then, using the population equation for  β  allows us to rewrite the assumption as
 (1) ( Σ x y ) j = 0 ⇒ β j = ( Σ x x − 1 Σ x y ) j = 0. 
In words, we are assuming that the dot product of the  j th  row of the inverse covariance matrix with the covariance between  x  and  y  is zero whenever the  j th  element of Σ xy  is zero. To examine whether this assumption holds, we can estimate both  Σ x x − 1  and Σ xy  using the DLBCL data and imagine that these estimates are the population quantities for illustration. To estimate Σ xy , we use the standard covariance estimate, but set all but the largest 120 values equal to zero, corresponding to a sparse solution. For the case of  Σ x x − 1 , estimating large inverse covariance matrices accurately is impossible when  p ≫ n  unless we assume some additional structure. If most of the entries are 0 [a necessary condition for (1) to hold], methods like the graphical lasso (glasso,  Friedman  et al. , 2008 ) or graph estimation ( Meinshausen and Bühlmann, 2006 ) have been shown to work well. We use the graph estimation technique for all 7399 genes in the dataset at 10 different sparsity levels ranging from 100% to 99.2%. For visualization purposes,  Figure 1  shows the first 250 genes for one estimate of the inverse covariance that is 97.5% sparse. Fig. 1 A sparse estimate of the inverse covariance of gene expression measurements for the first 250 genes from the DLBCL dataset. The estimate has 97.5% of the off-diagonal elements equal to 0. Darker colors represent inverse co-variances of larger magnitude To assess the validity of (1),  Table 1  shows the sparsity of the full inverse covariance matrix, the percentage of non-zero regression coefficients, and the percentage of non-zero regression coefficients which are incorrectly ignored by the assumption (the false negative rate). In all cases, Σ xy  is about 98% sparse. Even with an extremely sparse inverse covariance matrix, the false negative rate is at least 25% meaning that 25% of possibly relevant genes are ignored by the analysis. If the sparsity of  Σ x x − 1  is allowed to increase only slightly, the false negative rate increases to over 95%.
 Table 1 This table shows properties of the coefficients of the linear model corresponding to 10 different estimates of the inverse covariance matrix, from complete sparsity on the left (a diagonal matrix) to still more than 99% sparsity on the right Sparsity of  Σ x x − 1 1.0000 0.9999 0.9998 0.9995 0.9991 0.9984 0.9975 0.9963 0.9946 0.9922 % Non-zero  β ’s 0.0162 0.0216 0.0287 0.0418 0.0618 0.0843 0.1193 0.1803 0.2645 0.3699 False Negative Rate 0.0000 0.2500 0.4340 0.6117 0.7374 0.8077 0.8641 0.9100 0.9387 0.9562 Note:  The second row is the number of non-zero population regression coefficients corresponding to each inverse covariance matrix. The bottom row shows the percentage of non-zero regression coefficients which are incorrectly ignored under the assumption on the relationship between marginal correlations and regression coefficients. 1.3 Our contribution For a similar computational budget, our method outperforms existing approaches by taking advantage of all the data. Our method does not require that the set of non-zero regression coefficients be a subset of the non-zero marginal correlations. Suppose that  M ∈ ℝ p × p  is a symmetric, non-negative definite matrix; that is, for all vectors  a ∈ ℝ p ,  a ⊤ M a ≥ 0  and  M ⊤ = M . To approximate the matrix  M , we fix an integer  ℓ ≪ p  and form a sketching matrix  S ∈ ℝ p × ℓ . Then, we report the following approximation:  M ≈ ( M S ) ( S ⊤ M S ) † ( M S ) ⊤ . The details behind the formation of the matrix  S  control the type of approximation. In the simplest case, which we employ here, we take  S = π τ ,  where  π ∈ ℝ p × p  is a permutation of the identity matrix and  τ = [ I ℓ , 0 ] ⊤ ∈ ℝ p × ℓ  is a truncation matrix. While many alternative sketching matrices, mostly based on random projections, have been proposed, this method is the only one necessary to develop our results. Without loss of generality, divide the matrix  M  into blocks
 M = [ M 11 M 21 ⊤ M 21 M 22 ] 
so that we can (implicitly) construct the matrix  F ( M ) ∈ ℝ p × ℓ  as
 F ( M ) : = M S = [ M 11 M 21 ] . 
Because
 M ≈ ( M S ) ( S ⊤ M S ) † ( M S ) ⊤ = F ( M ) ( S ⊤ M S ) † F ( M ) ⊤ , 
we can approximate the eigendecomposition of  M  using the SVD of  F ( M ) . If we decompose  F = U ( F ) Λ ( F ) V ( F ) ⊤ ,  where we have suppressed the dependence of  F  on  M  when  F  is an argument for clarity, then the resulting approximation to the eigenvectors of  M  is  V ( M ) ≈ F V ( F ) Λ ( F ) † = U ( F ) .  Likewise, the approximate eigenvalues of  M  are given the singular values  Λ ( F ) . 
 Homrighausen and McDonald (2016)  show that this approximation is more accurate than the one based on  M 11  for performing a principal components analysis. As previous techniques for principal components regression (like SPC) are based on  M 11  rather than  F , it is possible that by using  F , we will have better results. As we will see, this intuition turns out to be true under some conditions which were suggested in Section 1.2. In particular, for essentially the same computational budget, our procedure outperforms previous procedures if some genes have small marginal correlations with the phenotype but are, nonetheless, important for predicting the phenotype conditional on the presence of other genes. Furthermore, even if the assumption in (1) is true, our procedure is not much worse than existing approaches. In Section 2, we discuss exactly how to implement our methodology. We examine the behavior of our procedure in Section 3. In Section 3.1, we state an explicit model for the data-generating mechanism in order to be clear about the conditions under which our procedure works well. Section 3.2 uses a number of carefully constructed simulations to show when our technique works well, and when it doesn’t. In Section 4, we examine our procedure on four genetics datasets, including the one discussed above. We find that our methods slightly outperform existing techniques on three of them, suggesting that the motivation is sound. Finally, in Section 5, we give conclusions and discuss some avenues for future work. 2 Methods and computations We now give the details of our methodology. For clarity, we assume that the design matrix  X  and the response  Y  are already centered. Let  T  be a  p -dimensional vector denoting standardized regression coefficient estimates i.e. for any  j ∈ { 1 , 2 , … , p } ,  t j  is the coefficient estimate of standardized univariate regression between response  Y  and covariate  X j . We use standardized regression so that the coefficient estimates are comparable across disparate covariates. Note that  t j  is also the marginal correlation between the response  Y  and covariate  X j . For some threshold  t * , we separate  X  into two matrices  X A  and  X A c , where  A = { j : | t j | &gt; t * } . We assume  | A | = ℓ . The hope is that  X A  contains many of the genes that are most predictive of the phenotype under study. Ideally, high marginal correlations will suggest relevant predictors to be emphasized in the decomposition, but unlike other methods, we will also use those genes in the set  A c . We now focus on  X new = [ X A ,   X A c ]  and note that it has the same range as  X . Therefore, we will use the approximation technique discussed in Section 1.3 to try to estimate the eigendecomposition of  Σ x x  using sample quantities. Because  X new ⊤ X new  is symmetric and positive definite, write
 F = X new ⊤ X A = ( X A ⊤ X A X A c ⊤ X A ) , 
and decompose  F = U ( F ) Λ ( F ) V ( F ) . For some integer  d ∈ { 1 , … , ℓ } , we define
 V ^ [ d ] = U [ d ] ( F ) , Λ ^ [ d ] = Λ [ d ] ( F ) 1 / 2 ,     and U ^ [ d ] = X n e w V ^ [ d ] Λ ^ [ d ] − 1 . Now, we have estimates for the principal components  U ^ [ d ] Λ ^ [ d ] . Therefore, just as with principal components regression, we can regress  Y  on the estimated principal components to produce estimated coefficients in principal component space:
 Γ ^ A I M E R = argmin Γ | | Y − U ^ [ d ] Λ ^ [ d ] Γ | | 2 2 = Λ ^ [ d ] − 1 U ^ [ d ] T Y . 
Then the coefficient estimates for linear regression in the space spanned by  X n e w  are given by
 (2) β ^ A I M E R = V ^ [ d ] Γ ^ A I M E R = V ^ [ d ] Λ ^ [ d ] − 1 U ^ [ d ] T Y . 
Because our methodology uses marginal regression to select a small number of hopefully relevant predictors before ‘amplifying’ their eigenstructure information with the  F  matrix, we refer to our technique as ‘Amplified, Initially Marginal, Eigenvector Regression’ (AIMER). Unlike previous approaches, the solution given by (2) is not sparse: with probability 1,  ( β ^ A I M E R ) j ≠ 0 ,   ∀ j . However, most of the coefficients will be small. We therefore threshold the estimates to produce our final estimator:
 (3) β ^ A I M E R ( b ) : = β ^ A I M E R 1 ( b , ∞ ) ( | β ^ A I M E R | ) , 
where  b ≥ 0 , and  1 A ( w )  is the indicator function, which returns the value one for every element of  w ∈ A  and zero otherwise. We summarize this procedure in Algorithm 1. As with SPC, the computational burden of our method is dominated by the SVD. We use an SVD of  F  while SPC uses the SVD of  X A . However, since the SVD is cubic in the smaller dimension, in both cases the computation is  O ( | A | 3 ) . Thus, to leading order, both methods require the same amount of computation.  Algorithm 1:  Amplified, Initially Marginal, Eigenvector Regression (AIMER) Input:  centered design matrix  X , centered response  Y , thresholds  t * , b * ≥ 0 , integer  d 1  Compute marginal correlation  t j  between  X j  and  Y  for all  j ; 2 Set A = { j : | t j | &gt; t * } ; 3 Set X n e w = [ X A ,   X A c ] ; ; 4  Define  F = X n e w ⊤ X A ; 5  Decompose  F = U ( F ) Λ ( F ) V ( F ) ⊤ ; 6 Set V ^ [ d ] = U [ d ] ( F ) ; 7 Set Λ ^ [ d ] = Λ [ d ] ( F ) 1 / 2 ; 8 Set U ^ [ d ] = X n e w V ^ [ d ] Λ ^ [ d ] − 1 ; 9  Calculate  β ^ = V ^ [ d ] Λ ^ [ d ] − 1 U ^ [ d ] T Y ; 10 Set β ^ ( b * ) : = β ^ 1 ( b * , ∞ ) ( | β ^ | ) ; Output:  coefficient estimates  β ^ ( b * ) To make predictions given a new observation  x * , we simply center it using the mean of the original data, reorder its entries to conform to  X n e w , multiply by the coefficient vector in (3), and add the mean of the original response vector. 3 Experimental analysis To examine the performance of our method, we set up a number of carefully constructed simulations under various conditions. We first discuss the generic data model we assume, a latent factor model, which is amenable to analysis via SPC or AIMER. 3.1 Data model Consider the multivariate Gaussian linear regression model
 (4) y = x ⊤ β + σ 1 ϵ 
with  y  the response,  x ∈ ℝ p  a column vector of gene expression measurements,  β = ( β 1 , … , β p ) ⊤  the coefficients,  ϵ  a random Gaussian distributed error with zero mean and variance 1, and  σ 1 &gt; 0 . We further assume that  x ∼ N p ( 0 , Σ x x )  has a Gaussian distribution with mean vector  0  and covariance matrix  Σ x x . We will assume that  β  is sparse, in that most of its elements are exactly 0 indicating no linear relationship between the associated gene and the response. Finally, the design matrix  X  and the response vector  Y  include  n  independent observations of  x  and  y , respectively. 
 Model for X. 
 As  Σ x x  is symmetric and positive (semi-) definite, we can decompose it as
 Σ x x = V ( Σ x x ) L ( Σ x x ) V T ( Σ x x ) = ( V 1 ⋯ V p ) ( l 1 0 ⋱ 0 l p ) ( V 1 ⊤ ⋮ V p ⊤ ) , 
where  V 1 , … , V p  are orthonormal eigenvectors on  ℝ p  and  l 1 ≥ ⋯ ≥ l p ≥ 0  are eigenvalues. We assume that there is some  1 ≤ G ≤ p  such that the eigenvalues can be separated into two groups, one of which includes relatively large eigenvalues and the other relatively small eigenvalues, that is,  l k = λ k + σ 0 2  for  1 ≤ k ≤ G  and  l k = σ 0 2  for  k  &gt;  G  where  λ 1 ≥ ⋯ ≥ λ G &gt; 0 , and  σ 0 2 &gt; 0. Then, because  X  is multivariate Gaussian, we can write  X  as
 X = U G Λ G V G T + σ 0 E = ( U 1 ⋯ U G ) ( λ 1 0 ⋱ 0 λ G ) ( V 1 ⊤ ⋮ V G ⊤ ) + σ 0 E 
where latent factors  U 1 , … , U G  are independent and identically distributed (i.i.d.)  N n ( 0 , I )  vectors, and the noise matrix  E  is  n  ×  p  with i.i.d.  N (0, 1) entries independent of  U 1 , … , U G .  
 Model for  Y . 
 We assume that  Y  is a linear function of the first  K ≤ G  latent factors in  U G  plus additive Gaussian noise:  Y = U K Θ + σ 1 Z ,  where  Θ  is the coefficient vector,  σ 1 &gt; 0  is a constant, and  Z  is distributed  N n ( 0 , I ) , independent of  X . Note that the expectation of  Y  is zero and that this is a specific form of (4).  
 Implication of the model. 
 Under this model for  X  and  Y , the population marginal covariance between each gene  X j  and the response  Y  can be written as
 Σ x y = ( Cov ( X 1 , Y ) ⋮ Cov ( X p , Y ) ) = V K Λ K Θ . 
Therefore, the population ordinary least squares coefficients of regressing  Y  on  X  ( β  in (4)) can be written as
 (6) β = Σ x x − 1 Σ x y = V K L K − 1 Λ K Θ 
We will define the set  B : = { j : ( Σ x y ) j ≠ 0 }  and the set  A : = { j : β j ≠ 0 } . We note that for  K  = 1, it is always the case that  A = B . By manipulating the parameters in  Θ ,  L , and  Λ , we can create a number of scenarios for testing AIMER against alternative methods. 3.2 Experiments We present results under five different experiments. For each of the simulations which follow, we generate datasets with  n  = 200 and  p  = 1000. We use half ( n  = 100) to estimate the model and test our predictions on the other half. We repeat this process 100 times for each combination of parameters. Throughout, we use  σ 0 = .1 ≈ .3  and  σ 1 = .1 . The matrix  U  is generated with i.i.d. standard Gaussian entries, while the matrix  V  is constructed by hand to have the correct number of orthogonal components. The first experiment is designed to be favorable to AIMER. The second is designed to be favorable to SPC. The third examines the extent to which the assumption that  A = B  is beneficial to SPC over AIMER. The fourth examines the impact of using incorrect numbers of components, while the fifth uses cross validation on all the tuning parameters. 
 Simulation 1: Favorable conditions for AIMER. 
 In this simulation, we create data which is amenable to AIMER at the expense of the conditions for SPC, that is we use  B ⊂ A . We set parameters in the data model as  K = G = 3  and choose  λ 1 = 10 , λ 2 = 5 ,  and  λ 3 = 1 . In order to achieve  B ⊂ A , we set  θ 1 = θ 2 = 1  and solve (5) for  θ 3  so that some corresponding elements of Σ xy  will be zero. We make the first 15 elements of  β  non-zero, five corresponding to each of the three principal components. Thus, the first 10 genes have non-zero population marginal correlation and the remaining 990 have zero marginal correlation. In this scenario, SPC should find the first 10 important genes, but AIMER will find the remaining five important genes as well. In order to focus on the relationship between performance and the condition  B ⊂ A , we examine the methods for a fixed computational budget and choose  t *  to select the same 50 most predictive genes. We examine SPC, SPC with lasso, AIMER( b  = 0), and AIMER. We use the first three principal components for regression in all the methods. For SPC with lasso and AIMER, we choose the remaining tuning parameters via 10-fold cross-validation. We also give results for OLS on the first 15 genes. This is the oracle estimator, the best one could hope to do with foreknowledge of the predictive genes. 
 Figure 2  shows the classification performance using a receiver operating characteristic (ROC) curve for SPC with lasso and AIMER in the left panel (the remaining panels are for the next two simulations). Examining the figure, it is easy to see that SPC + lasso identifies the first 10 genes easily, but AIMER is able to capture all 15 predictive genes at a low cost of false positive identifications. A more detailed analysis is given in the first row of  Figure 3 . Panel 1a shows the ability of each method to estimate the  β  coefficients of three different factors. Coefficient estimates for the five genes in factor 1 by AIMER are slightly more accurate, and no more variable, than SPC + lasso. Furthermore, AIMER is better at estimating those  β ’s associated with factor 2, and much better at those associated with factor 3 (these are assumed zero in SPC). Panel 1b examines the mean square error (MSE) of estimation as the average squared difference between the true coefficients and their estimates for all 1000 genes. The overall estimation accuracy of AIMER ( b  = 0) is worse because of the inclusion of so many useless genes (it estimates all 1000), however, by thresholding with AIMER, accuracy is improved and exceeds that of SPC with and without lasso. In panel 1c, we show the MSE for prediction, the average squared difference between predicted values and the actual observations, for a test set. This MSE is smaller for AIMER than for SPC much of the time, but the variance across simulations is large.  Fig. 2 Receiver operating characteristic (ROC) Curve for Simulations 1–3. The  x -axis is the false positive rate while the  y -axis is the true positive rate. The curves present averages across 100 replications. SPC is limited to only 50 selected genes, and so its false positive rate is bounded. The dashed line indicates its best case theoretical performance were it allowed to continue to select further genes Fig. 3 Estimation and prediction performance of SPC and AIMER in the first three simulations. The left panel shows the estimates of the regression coefficients, the middle panel shows the mean squared error (MSE) of estimation for all 1000 genes, and the right panel shows prediction MSE on the held-out data. The boxes indicate variability across 100 replications. The dashed black horizontal lines indicate the true values of  β 
 Simulation 2: Favorable conditions for SPC. 
 This simulation compares the performance of SPC and AIMER under conditions which are more favorable to SPC. In particular, we choose parameters such that  A = B . While AIMER is likely to perform worse because it will tend to include irrelevant genes, it is not too much worse. Most of the parameters are the same as in Simulation 1, except that  K = G = 2 ,   λ 1 = 10 ,  λ 2 = 1 ,   θ 1 = θ 2 = 1 , and we use the first two principal components to do regression. Therefore, 10 out of 1000 genes are truly predictive of the response, and all 10 have non-zero marginal correlation with the response (the rest have  Σ x y = 0 ). Looking again at  Figure 2 , both SPC + lasso and AIMER can identify all 10 predictive genes at a small price of false positives. Examining  Figure 3 , we see that the estimation accuracy of SPC/SPC + lasso is better than that of AIMER as expected, and the MSE of prediction for AIMER is about twice that of SPC/SPC + lasso. The estimation MSE (panel 2b) of AIMER is comparable to that of SPC.  
 Simulation 3: Slight perturbations. 
 In this simulation, we adjust only  θ 2 = 3 , rather than 1 as in simulation 2, thereby maintaining the condition that  A = B . However, in this case AIMER works much better than SPC/SPC + lasso.  Figures 2  and  3  show that AIMER can easily identify all the predictive genes, has more precise coefficient estimates, and has much smaller MSE for prediction. The reason is that, even though  A = B , the marginal correlations for some predictive genes are very small. Therefore, those genes are more difficult for SPC to identify, but AIMER can compensate. For one further comparison,  Table 2  shows the average (standard deviation in parentheses) number of predictive genes selected in each of the first three simulations. AIMER selects the smallest number of coefficients in most cases. 
 Table 2 Average final number of predictive genes in Simulations 1, 2 and 3 Simulation 1 2 3 True # 15 10 10 SPC 50 (0) 50 (0) 50 (0) SPC+lasso 31 (9.011) 39 (3.636) 46 (2.665) AIMER ( b  = 0) 1000 (0) 1000 (0) 1000 (0) AIMER 39 (9.225) 21 (12.750) 16 (7.558) Note : The standard deviation is shown in parentheses. 
 Simulation 4: Choosing the number of components. 
 In the previous simulations, we used the correct number of principal components, though such a choice is unlikely to be possible given real data. In this simulation, we examine the impact choosing the number of components has on estimation accuracy. We use similar parameter settings as Simulation 1 except with  K = G = 2  rather than 3 (we maintain the condition that  B ⊂ A ). We then use all the methods with 1, 2 and 3 components. We also adjust the values of  λ 1  in a range from 5 to 50. As we can see in  Figure 4 , using two components reduces MSE for AIMER( b  = 0) and AIMER across all values of  λ 1  relative to using only one component, while using more than two components has little impact. With only one component, SPC performs better than AIMER, likely due to smaller variance for a similar bias, but using two or three components leads to large gains for AIMER. In practice, it is worthwhile to try several numbers of components and use cross-validation to decide which works best.  Fig. 4 Prediction MSE averaged across 100 replications for each method for different numbers of components (Simulation 4). We also allow  λ 1  to vary between 5 and 50 
 Simulation 5: The screening threshold. 
 In previous simulations, we choose  t *  so that variable screening by the marginal correlation would always select exactly 50 genes. Thus, we could compare methods based on their ability to use the same amount of information. In reality, it may be better to choose the threshold  t *  using cross validation. In this simulation, we use the same conditions as in the previous simulation with  λ 1 = 10 . It is still not appropriate to have more genes than patients, so we allow the number of selected genes to be anything less than the number of patients (100). We further use 10-fold cross-validation to choose the best threshold. As shown in  Figure 5 , allowing  t *  to be chosen rather than fixed leads to improved results for AIMER relative to SPC/SPC + lasso. The prediction MSE decreases and fewer genes are selected. Fig. 5 Performance of each method when we allow  t *  to be chosen by cross validation rather than fixed to choose 50 genes (Simulation 5) 4 Performance on real data We now illustrate our methods on four empirical datasets in genomics that record the censored survival time and gene expression measurements from DNA microarrays of patients with four different types of cancer. The first dataset comes from  Rosenwald  et al.  (2002)  and contains 240 patients with diffuse large B-cell lymphoma (DLBCL) and 7399 genes. The second dataset has 4751 gene expression measurements of 78 breast cancer patients ( Van’t Veer  et al. , 2002 ). The third consists of 86 lung cancer patients measured on 7129 genes ( Beer  et al. , 2002 ), and finally, we analyze a dataset consisting of 116 patients with acute myeloid leukemia (AML,  Bullinger  et al. , 2004 ) and 6283 genes. Since the survival times for some patients are censored and right-skewed, we use  log   ⁡ ( survival   time + 1 )  as the response. A Cox model would be more appropriate, but this transformation is enough to illustrate our methodology. In order to assess our method using limited data, we randomly select half of the data as the training set and let the rest be in the testing set, then estimate each model using the training half and predict the held out data. We repeat this procedure for 10 random splits and report the average error. We use 10-fold cross-validation on the training set to choose all tuning parameters ( t * ,   b * ,  d  and  λ  where appropriate), mimicking the procedure of a real data analysis. We apply seven methods on each dataset: (i) PCR; (ii) lasso; (iii) ridge regression; (iv) SPC; (v) SPC + lasso; (vi) AIMER( b  = 0) and (vii) AIMER. We use the R packages pls ( Mevik and Wehrens, 2007 ) to perform PCR and glmnet ( Friedman  et al. , 2010 ) to perform lasso and ridge. For PCR, SPC, SPC + lasso, AIMER( b  = 0) and AIMER, we allow the number of components  d  to be chosen between 1 and 5. Our results are shown in  Table 3 . For each dataset, we show the MSE on the testing set, the number of selected genes, and the number of principal components used (if relevant), averaged across the 10 random training-testing splits. We do not show results for PCR because it is uniformly awful. The results in  Table 3  are largely consistent with the conclusions we derive from simulations. AIMER and SPC + lasso tend to select a similar number of genes, though AIMER has better prediction error on three of the four datasets. Interestingly, the genes selected by SPC + lasso, lasso and AIMER rarely overlap, suggesting that to identify genes for further study, one should try all three methods. The online  Supplementary Material  lists the genes identified by AIMER for each dataset. In the case of DLBCL, we also list any previous research relating the selected genes to lymphoma.
 Table 3 The MSE on the test set, the number of selected genes and the number of principal components used ( d  if relevant), each averaged across the 10 random training-testing splits DLBCL Breast cancer Lung cancer AML Methods MSE # genes d MSE # genes d MSE # genes d MSE # genes d lasso 0.6805 20 0.6285 9 0.8159 22 1.9564 6 ridge 0.6485 7399 0.6407 4751 0.7713 7129 1.9234 6283 SPC 0.6828 41 3 0.6066 16 2 0.8344 19 3 2.4214 24 2 SPC+lasso 0.6780 31 3 0.6029 14 2 0.8436 9 4 2.3980 22 2 AIMER( b  = 0) 1.1896 7399 2 2.6531 4751 1 0.9444 7129 1 12.4014 6283 1 AIMER 0.6518 28 4 0.6004 31 3 1.0203 13 1 1.8746 36 4 Note:  Bolded values indicate the best predictive performance for each type of method (with and without structure learning) for each dataset. The Lung Cancer data is rather odd in that AIMER( b  = 0) has better performance than AIMER. This anomaly is likely because, in contrast with the other datasets, the lung cancer expression measurements have not been scaled relative to a control group. We tried two transformations using only the treatment group to approximate such a scaling, but, while the performance of our method becomes comparable to SPC following transformations, it remains slightly worse. Without a control group, it is difficult to explain this outcome with any certainty. A comparison of these alternative transformations with our results in  Table 3  is contained in the online  Supplementary Material . As seen in the table, ridge regression is sometimes the best of all the methods. Previous experience suggests that ridge regression is dominant if the genes are highly correlated or when there is not a particularly predictive set of genes. However, the fact that ridge does not screen out unimportant genes is a barrier to its applications in genomics. On the other hand, AIMER approaches or exceeds the small prediction error of ridge regression while also selecting a small number of predictive genes, making it a better candidate for solving these types of problems. 5 Discussion High-dimensional regression methods help in predicting future survival time and identifying possibly predictive genes for diseases. However, the large number of genes, the limited access to patients, and the complex covariance structure between genes make the problem both computationally and statistically difficult. In both simulations and analysis of actual gene expression datasets, AIMER has comparable or slightly improved prediction accuracy relative to existing methods and finds small numbers of actually predictive genes, all while having a similar computational burden. On the other hand, there are some issues which warrant further exploration. A major benefit of SPC is that it comes with theoretical guarantees under certain assumptions. While our methodology is intended to work when these assumptions don’t hold, we do not yet have comparable guarantees. However, the simulated experiments in this paper have suggested how we might derive such results in a more general setting. For the real data examples in this paper, we applied a simple monotonic transformation to the response variable, however, extending our methods to Cox models, which are more appropriate, and other generalized linear models for predicting discrete traits is highly desirable. It may also be useful to examine other eigenstructure techniques such as Locally Linear Embeddings or Laplacian Eigenmaps to produce non-linear predictors. Finally, using other matrix approximation techniques may yield improved performance or be more amenable to theoretical analysis. Funding This work is supported by the National Science Foundation [grant number DMS–14-07439 to D.J.M.]. 
 Conflict of Interest : none declared. Supplementary Material Supplementary Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>LinkinPath: from sequence to interconnected pathway</Title>
    <Doi>10.1093/bioinformatics/btr328</Doi>
    <Authors>Ingsriswang Supawadee, Yokwai Sunai, Wichadakul Duangdao</Authors>
    <Abstract>Summary: LinkinPath is a pathway mapping and analysis tool that enables users to explore and visualize the list of gene/protein sequences through various Flash-driven interactive web interfaces including KEGG pathway maps, functional composition maps (TreeMaps), molecular interaction/reaction networks and pathway-to-pathway networks. Users can submit single or multiple datasets of gene/protein sequences to LinkinPath to (i) determine the co-occurrence and co-absence of genes/proteins on animated KEGG pathway maps; (ii) compare functional compositions within and among the datasets using TreeMaps; (iii) analyze the statistically enriched pathways across the datasets; (iv) build the pathway-to-pathway networks for each dataset; (v) explore potential interaction/reaction paths between pathways; and (vi) identify common pathway-to-pathway networks across the datasets.</Abstract>
    <Body>1 INTRODUCTION Pathways are functional units resulted from the interplay of interacting genes, RNAs, proteins and small molecules. Mapping genes or proteins into the context of pathways can help gain more insights into their functions and interactions in an organism. Although sequence similarity-based methods [e.g. NCBI BLAST ( Altschul  et al. , 1990 )] have been commonly used for identification of pathways to genes/proteins based on their orthologous genes/proteins annotated in the well-characterized pathways, these methods have limitations such that some best hits (such as those annotated as ‘hypothetical’ or ‘unknown’ genes/proteins) may not necessarily be annotated in any pathway. The incorporation of additional data such as protein–protein interactions and enzymatic reactions can help infer the pathways and their interconnection and uncover the biological function of genes/proteins. However, the information regarding the connections between pathways through molecular interactions and reactions are not included and adequately represented to support the exploratory analysis in most pathway mapping tools (reviewed in  Gehlenborg  et al. , 2010 ), for example, GenMAPP ( Salomonis  et al. , 2007 ), Pathway Explorer ( Mlecnik  et al. , 2005 ) and Pathway Projector ( Kono  et al. , 2009 ). To overcome these limitations, a web-based interactive tool, so-called LinkinPath, was developed to analyze, map and visualize the gene/protein lists in the context of interconnected pathways, which provides a valuable resource for not only comprehensive studies of gene–gene interaction, but also functional genomics in virtually all organisms. 2 METHOD LinkinPath has been developed as a web-based interactive exploration tool for pathway analysis. Users can upload the datasets of DNA or protein sequences in FASTA format and submit to the LinkinPath's job queue ( Supplementary Fig. S1 ), which could support the analyses of genome-scale inputs. Although, many jobs may be submitted at the same time, the LinkinPath web server accepts a total maximum of 5000 sequences for each job. When jobs are finished, users can retrieve the result using the bookmarked URL during the submission or the web link from the notification email. LinkinPath processes each job in five main steps: (i) sequence and protein domain search; (ii) pathway mapping and annotation; (iii) identification and comparison of enriched pathways; (iv) construction of interconnected pathway networks and (v) identification of common pathway subnetworks ( Supplementary Fig. S2 ). First, input sequences are searched against KEGG ( Kanehisa and Goto, 2000 ), NR ( Benson  et al. , 2007 ), PFAM ( Finn  et al. , 2008 ) and RFAM ( Griffiths-Jones  et al. , 2003 ) databases. Second, if significant similarities are found to match with an enzyme class (EC) and/or KEGG Orthology (KO), genes/proteins will be annotated with the corresponding pathways. Third, to support comparison of functional composition, Treemaps and statistical methods are employed to examine the pathways enriched in the dataset. Fourth, information of molecular interactions and reactions from BIND ( Bader  et al.,  2003 ) and KEGG will be used for inference of pathways and networks. In case that an input sequence cannot be annotated in any pathway, LinkinPath will use its interacting partners to infer its related pathways. The pathways will thus be linked together via interaction or reaction paths to form the network in this step. Lastly, to identify the commonalities across multiple datasets, LinkinPath includes an algorithm to extract the frequent subnetworks from the pathway networks built in previous step. 3 RESULT Using the method described above, LinkinPath automatically maps and annotates genes/proteins in the datasets into the context of interconnected pathways and presents the results to users via a Flash-driven interactive web interfaces. The results are organized into five analysis steps (see following subsections), which can easily be browsed, searched and downloaded in any of supported formats. Summary charts and an interactive Venn diagram are also provided to illustrate of how a dataset differs from others according to their annotated sequences with EC numbers, interactions and pathways ( Supplementary Fig. S3 ). The annotation results of input sequences can be interactively explored in different visualization perspectives including KEGG-pathway maps, TreeMaps, pathway-to-pathway networks and interaction and reaction paths. 3.1 Detecting changes in animated pathway maps In most tools, the KEGG pathway image is statically displayed with positioned genes/proteins in the pathway. To enable the dynamicity, LinkinPath allows users to run the pathway map animation to depict changes of expressed genes/proteins annotated in each pathway over multiple and time-series datasets ( Supplementary Figs 4  and  5 ). Detecting changes between the same pathway maps taken from different times/experiments/organisms can help reveal the functional characterization of genes/proteins expressed under different conditions. In addition, the co-occurrence and co-absence of genes/proteins in a series of pathways can suggest the functional relationships between these molecules. Alternatively, users can interactively explore the annotated pathways across datasets on KEGG Atlas, in which connected paths are colored according to the datasets. 3.2 Visualizing functional composition via TreeMaps Many classification schemes such as enzyme classification, KO and KEGG pathways have tree-based structures that are difficult to simultaneously display and compare the information contained in the trees. To circumvent this problem, Treemaps are included in LinkinPath to facilitate the visualization and comparison of those hierarchical data via a set of nested rectangular maps. A base rectangle represents the root of the hierarchy and is divided into rectangular subareas proportional to data size and colored by data type. As a result, Treemaps enable users to compare sizes of nodes and subtrees, and are helpful in revealing patterns. To disclose the functional composition among datasets, LinkinPath provides users with three different Treemaps: (i) the enzyme compositions using top-level EC numbers; (ii) the pathway compositions using KEGG pathway classification; and (iii) and KO compositions using KO numbers ( Supplementary Fig. S6 ). The Treemap of enzyme composition, for example, helps users visually examine enzyme enrichment in lists of genes/proteins, where each rectangle represents the top-level EC numbers with different colors and indicates the proportion of genes/proteins annotated with the EC group. Alternatively, LinkinPath allows users to compare and examine the enrichments of the EC and KO annotations of entire input data across pathways. 3.3 Identification of statistically enriched pathways Since pathways that are highly enriched with the list of annotated genes/proteins are more likely to be biologically relevant, LinkinPath employs KOBAS ( Wu  et al. , 2006 ) to help identify significantly enriched pathways in a dataset. KOBAS uses KO number to link genes/proteins to KEGG pathways and calculates the statistical significance of each pathway in a queried dataset against all pathways in the referenced datasets. There are three statistical tests available including binominal, chi-square and hyper-geometric distribution tests to assess the enrichment of the found pathways. For each pathway found in the input datasets, LinkinPath calculates these statistics by comparing the number of sequences involved in the pathway for each dataset with the total number of sequences involved in the same pathway for all datasets. 3.4 Inferring pathway-to-pathway interconnections Despite their complexity, the interconnections between pathways can help unravel novel regulation mechanisms including metabolism and signaling in organisms. LinkinPath infers the pathway-to-pathway connections using molecular interactions and reactions. Pathways will be connected to each other and represented in a form of interactive networks. Each dataset could have several pathway networks with different sizes. Each node represents either a pathway or an input sequence. The pathway node contains the information on the number of input sequences annotated in that pathway. An edge between nodes indicates the pathway connection types with different colors and line styles. Two pathway nodes are connected with a solid line if a path between them exists in KEGG database. The dash line indicates an inferred path from an input sequence or a query node to a known pathway. With its interactive network browser, LinkinPath allows users to browse and access the network characterization and the associated information such as the number of input sequences and interactions involved in a pathway node and the list of input sequences that appear in single or multiple pathways. Exploring putative functions of genes/proteins : LinkinPath utilizes molecular interactions to infer the pathway and putative function of an un-annotated sequence on the basis of its interacting partner's function. The shortest interaction paths connecting from a query node to a pathway node in the network are identified using all paths breadth first search. In addition, the putative function of an input sequence might be inferred by its proximity to functionally annotated genes/proteins within the context of interconnected pathways. In the network browser, users can traverse the interaction paths to the inferred pathways of the nodes with dashed edge ( Supplementary Fig. S7 ). Exploring reactions between metabolic pathways:  LinkinPath searches the reaction paths from an input sequence mapped in a metabolic pathway to the compounds linking to other metabolic pathways. On a solid blue edge in the pathway-to-pathway network, if the reaction paths exist, users can explore what enzymes and compounds are essential to a metabolic process via the reaction paths between two pathways ( Supplementary Fig. S8 ). 3.5 Discovering the common pathway subnetworks LinkinPath extracts the frequently occurred subgraphs /subnetworks from the pathway networks to discover the commonalities across the datasets ( Supplementary Fig. S9 ). Users can navigate and visualize frequent subnetworks of varied sizes that occur in a number of different datasets. 4 CONCLUSION LinkinPath is a web-based tool that was applied the state of the art visualization techniques for original aspects of pathway mapping and analyses. Its novel contributions such as functional composition using Treemaps, pathway-to-pathway interconnections and the global metabolic map with highlighting mechanisms add value over comparable existing tools. 
          Supplementary Material 
          
             
                Supplementary Data 
             
             
             
          
       </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Modeling interactions between adjacent nucleosomes improves genome-wide predictions of nucleosome occupancy</Title>
    <Doi>10.1093/bioinformatics/btp216</Doi>
    <Authors>Lubliner Shai, Segal Eran</Authors>
    <Abstract>Motivation: Understanding the mechanisms that govern nucleosome positioning over genomes in vivo is essential for unraveling the role of chromatin organization in transcriptional regulation. Until now, models for predicting genome-wide nucleosome occupancy have assumed that the DNA associations of neighboring nucleosomes on the genome are independent. We present a new model that relaxes this independence assumption by modeling interactions between adjacent nucleosomes.</Abstract>
    <Body>1 INTRODUCTION Eukaryotic DNA is highly compacted within the cell nucleus by the wrapping of 147-bp-long DNA stretches around histone protein octamers, forming nucleosomes (Kornberg and Lorch,  1999 ). Adjacent nucleosomes are separated by short DNA sequences, called linkers. The positioning of nucleosomes along genomic DNA is the first order of chromatin organization. Past analyses of nucleosomal DNA and linker sequences have revealed specific sequences that are enriched within the nucleosome or within linkers (Ioshikhes  et al. ,  1996 ; Kaplan  et al. ,  2009 ; Lee  et al. ,  2007 ; Satchwell  et al. ,  1986 ; Segal  et al. ,  2006 ; Yuan and Liu,  2008 ). Based on these nucleosome sequence preferences, several models for predicting nucleosome occupancy were suggested (Ioshikhes  et al. ,  2006 ; Kaplan  et al. ,  2009 ; Lee  et al. ,  2007 ; Peckham  et al. ,  2007 ; Segal  et al. ,  2006 ; Yuan and Liu,  2008 ). Two of these were incorporated into a thermodynamic model (Kaplan  et al. ,  2009 ; Segal  et al. ,  2006 ) that was shown to predict  in vitro  and  in vivo  genome-wide nucleosome occupancy with high accuracy. The thermodynamic model assigns a statistical weight for each possible configuration of nucleosomes that are placed along a genomic sequence, such that no two nucleosomes overlap. In this model, the association of each nucleosome to a 147-bp-long sub-sequence within a configuration is weighted according to the nucleosome sequence preferences, and is independent of associations of other nucleosomes elsewhere on the DNA. However, given the several factors that are known to affect chromatin folding and higher order chromatin organization, this independence assumption does not hold. First, different linker lengths allow different relative conformations between neighboring nucleosomes, resulting from steric hindrance constraints and the helical turns of the DNA (Schalch  et al. ,  2005 ; Widom  1992 ). Second, many experiments and analyses have suggested that linker length distributions demonstrate a preference for quantized length patterns, of the form  d  +  r  ·  n , where  n  is a running integer,  r  is a repeat length, and  d  is a length offset ( d  &lt;  r ) (Cohanim  et al. ,  2006 ]; Kato  et al. ,  2003 ; Wang  et al. ,  2008 ). In most cases,  r  was found to be ∼10, in accordance with the DNA helical repeat, while the value of  d  varied. Third, the binding of the linker histone H1 to linker DNA greatly affects chromatin folding and condensation. Long linker lengths enable H1 binding, giving condensed chromatin, while short ones disable H1 binding, resulting in open chromatin (Routh  et al. ,  2008 ). Finally, electrostatic interactions may occur between two nucleosomes that are spatially close (Chodaparambil  et al. ,  2007 ; Dorigo  et al. ,  2004 ; Luger  et al. ,  1997 ), and may contribute to chromatin folding. Here, we model interactions between adjacent nucleosomes using a nucleosome cooperativity function (NCF), resulting in a new thermodynamic model for predicting nucleosome occupancy. We consider several types of functions as NCF candidates, based on an analysis of  in vivo  linker length distributions in yeast, and devise an algorithm to estimate these functions from data measurements of nucleosome occupancy. All of the functions we consider are simple and defined by a small number of parameters (between two and five parameters). When applied to synthetic data, we show that our model can accurately reconstruct NCF parameters, even in the presence of large degrees of noise in the input data. Our results suggest that reported preferences for quantized linker lengths result from the previously observed periodic sequence preferences of the single nucleosome (Satchwell  et al. ,  1986 ; Segal  et al. ,  2006 ). We show that modeling interactions between adjacent nucleosomes significantly improves nucleosome occupancy predictions in an  in vitro  system consisting of purified histones assembled on naked yeast genomic DNA, demonstrating that the preferred interactions that we find are intrinsic to nucleosome-DNA associations. The interactions that we learn introduce a preference for short linkers of less than 20 bp in length. Finally, modeling these intrinsic interactions also significantly improves predictions of nucleosome occupancy  in vivo  in both yeast and in  Caenorhabditis elegans , showing that they also play a role in nucleosome positioning  in vivo , and suggesting that they may be universal to all eukaryotes. 2 METHODS 2.1 New thermodynamic model for predicting nucleosome occupancy A thermodynamic model for the genome-wide prediction of nucleosome occupancy has been published by our lab (Field  et al. ,  2008 ; Segal  et al. ,  2006 ). This model assigns a statistical weight for each possible configuration of nucleosomes that are placed along a genomic sequence. The association of each nucleosome to a 147 bp long sub-sequence within a configuration is weighted by a probabilistic model that represents the nucleosome sequence preferences, assigning different statistical weights to different 147 bp long sequences. The association of a nucleosome to DNA at a certain genomic region is independent of the associations of other nucleosomes elsewhere, other than the fact that no two nucleosomes can overlap in the same configuration. Our new thermodynamic model relaxes the above independence assumption and models interactions between adjacent nucleosomes by incorporating a nucleosome cooperativity function (NCF). An NCF, denoted  L ( x ), is a positive function that assigns different statistical weights to different linker lengths. These weights are used as multiplicative factors, with 1 being a neutral weight. In the Results section we refer to the actual types of functions selected to represent NCFs. The probabilistic model that we use to describe the nucleosome sequence preferences was learned from  in vitro  bound sequences that we previously published (Kaplan  et al. ,  2009 ). We will denote this model of single nucleosome sequence preferences by  Nuc , where  Nuc(i)  is the statistical weight that the  Nuc  model assigns to a nucleosome being positioned on the input sequence,  S , starting at position  i . By  S i , j  we denote the sub-sequence of  S  starting at position  i  and ending at position  j . By  Bg(i ,  j)  we denote the statistical weight given by a background model to an unoccupied sub-sequence  S i , j . Since the  Nuc  model includes a background component that is used to normalize statistical weights, we used a simple uniform 0-order Markov model (i.e. P(A) =  P ( C ) =  P ( G ) =  P ( T ) = 0.25) as the  Bg  model. Using the above definitions, we compute the distribution over nucleosome configurations on an input sequence  S  of length  N . We take the partition function to be the space of all legal nucleosome configurations on  S , denoted by  C . A legal configuration  c  ∈  C  is defined by a set of nucleosome start positions on  S ,  c [1],…,  c [k], such that no two nucleosomes overlap. Assuming thermodynamic equilibrium, its statistical weight (its Boltzmann factor) is:
 
where τ represents an apparent nucleosome concentration, and β is a temperature parameter. For conciseness of representation, we assume that if  i  &gt;  j  then  Bg(i , j)  = 1. The probability of configuration  c  is given by:
 
where  c ′ traverses over the space  C  of all legal configurations. The probability of placing a nucleosome at start position  i  on  S , denoted  P ( i ), can be computed as follows:
 
where  c ″ traverses over the space  C i  of all legal configurations in which a nucleosome starts at position  i . To efficiently compute  P ( i ) for all positions  i  on  S  we employ a dynamic programming procedure (Rabiner,  1989 ). This demands that we limit the effect of any NCF to a window of reasonable length  M L , such that its contribution will only be added for linker lengths shorter than  M L . In this work we used  M L  = 100. For any NCF  L  this is equivalent to transforming  L  to a new function  L ′ such that:
 
The first part of our dynamic program is a  forward step , in which we compute two sets of random variables: { F i Nuc } and { F i Bg } (1 ≤  i  ≤  N ).  F i Nuc  represents the sum of the statistical weight of all legal configurations over the prefix  S 1 ,…,  S i  of  S , that end with a nucleosome (the last nucleosome end position is  i ).  F i Bg  is similarly defined, where position  i  is not covered by a nucleosome. The forward step computation is as follows:
 
 
 This concise representation is assisted by extending the definition of  F i Nuc  and  F i Bg  also over negative positions. The second part of the dynamic program is a  backward step , in which we compute two more sets of random variables: { R i Nuc } and { R i Bg } (1 ≤  i  ≤  N ).  R i Nuc  represents the sum of the statistical weight of all legal configurations over the suffix  S i ,…,  S N  of  S , in the event where a nucleosome ends at position  i  − 1 (exactly before the suffix  S i ,…,  S N ).  R i Bg  is similarly defined, where position  i  − 1 is not covered by a nucleosome. The backward step computation is as follows:
 
 
 This concise representation is assisted by extending the definition of  R i Nuc  and  R i Bg  also over positions  i  &gt;  N  + 1, and by defining:  Bg ( i ,  i  − 1) = 1. Having computed the above, we can now compute  P ( i ) for any position  i  in  S :
 
The probability of position  i  in  S  being covered by a nucleosome, also referred to as the average nucleosome occupancy over position  i , is predicted by our model to be:
 
 2.2 Learning the parameters of a nucleosome cooperativity function Having chosen a type of function as our NCF, we want to learn an optimal choice of its parameter values. Our model produces a vector   of predicted average nucleosome occupancy, per position of an input sequence  S . Therefore, for the purpose of learning NCF parameters, we require as input a vector   of the measured cell population average nucleosome occupancy per position of  S .   and   after normalization (by subtracting the mean and dividing by the SD) to mean 0 and SD 1 are denoted Ô and  , respectively. We define our objective function to be the  L 2 -distance between Ô and  :
 
and our learning algorithm searches for NCF parameters assignments for which the model generates a prediction   that minimizes  . We chose the (Nelder-Mead) simplex method for the function optimization task at hand, since it requires only the computation of the objective function at each point in the space of NCF parameter values. We refrained from using methods, such as conjugate gradient, that require computing the partial derivatives of the objective function according to the NCF parameters (see  Supplementary Methods ), as such computations are quite costly, and as they limit the choices of NCFs to differentiable ones. 3 RESULTS Previous approaches for predicting nucleosome occupancy (Ioshikhes  et al. ,  2006 ; Kaplan  et al. ,  2009 ; Lee  et al. ,  2007 ; Peckham  et al. ,  2007 ; Segal  et al. ,  2006 ; Yuan and Liu,  2008 ) relied on modeling the nucleosome sequence preferences, and used them to generate nucleosome occupancy predictions assuming that the association of one nucleosome to the DNA is independent of the associations of other nucleosomes. We relax this independence assumption by modeling interactions between adjacent nucleosomes through a nucleosome cooperativity function (NCF). In the previous section we presented how an NCF is incorporated into our model, and how we can learn its parameters. In this section we use our model to learn NCFs from synthetic data, as well as  in vitro  and  in vivo  measurements of nucleosome occupancy. 3.1 Selecting the types of nucleosome cooperativity functions A good candidate for an NCF would be the organism's linker lengths distribution. This distribution can be easily derived from single cell data of mono-nucleosome sequences that are uniquely mapped to the organism's genome, as linker lengths are simply the distances between any two mapped nucleosomes. However, existing experimental methods cannot measure genome-wide nucleosomes from single cells. Rather, existing nucleosome data comes from cell populations. We therefore resort to an approximation of the linker lengths distribution, derived from cell population data of mapped nucleosome sequence reads, similar to that used in (Valouev  et al. ,  2008 ). Instead of counting appearances of true linker lengths, we count appearances of putative linker lengths. For any pair of nucleosomes that are  d  bps apart, such that  d  &lt; 100, we count a single occurrence of a (putative) linker of length  d . We smooth the resulting linker lengths distribution with a moving average window of 5 bps. Using this procedure, whenever we encounter a pair of nucleosomes that were adjacent within a single cell then we count a true linker length appearance. In all other cases, we falsely add appearance counts, adding noise to the distribution. We used  in vivo  mono-nucleosome data, extracted from wild-type S. cerevisiae that were grown in rich medium and uniquely mapped to the S. cerevisiae genome (Kaplan  et al. ,  2009 ). The linker lengths distributions that we computed from cell population data of five different experiment replicates are shown in  Figure 1 . These five distributions are highly similar, and share several main features. First, they all exhibit an apparent disfavoring of linker lengths shorter than ∼15 bps. Second, a single prominent peak exists at 11–16 bp, and seems to decay exponentially at longer linker lengths (see exponential fits in  Fig. 1 ). Third, with this dominant decaying pattern, a periodic pattern of subtle peaks that are approximately 10 bps apart is combined. This pattern concurs with past analyses that revealed a preferentially quantized linker lengths pattern in yeast (Cohanim  et al. ,  2006 ; Wang  et al. ,  2008 ). The above linker lengths distributions derived from yeast cell populations are approximations of the unknown true linker lengths distribution in yeast. We assume that the above three features that appear in the approximate distributions reflect features of the true one. This suggests that biologically relevant NCFs will also include them. We therefore selected simple functions that represent at least one of the above three features, and are defined by a small number (between 2 and 5) of parameters. These functions are: an exponentially decaying function ( Exp , two parameters), a right-shifted exponentially decaying function ( S-Exp , three parameters), a right-shifted exponentially decaying sinusoid ( S-ES , five parameters) and a step function ( Step , two parameters, may represent both an  Off Step  or an  On Step ). The function formulas are presented in the  Supplementary Data  section. All functions have a parameter assignment for which they are equivalent to the constant 1 function ( No Coop ) that represents no nucleosome cooperating interactions. Examples of the selected functions for specific parameter assignments are shown in  Figure 2 .
 Fig. 1. Linker lengths distributions derived from mono-nucleosome genome-wide positioning data, extracted from  in vivo  yeast cell populations. The data includes five different replicates, all for wild-type yeasts grown in rich medium. For each replicate, the distribution of linker lengths in the range 0–100 is shown (divided by its mean value), along with an exponential curve that was fit to its decaying part (starting at the main peak). 
 Fig. 2. Nucleosome cooperativity functions and their linker length distributions. The figure is organized in a table-like fashion, with columns per NCF and rows per graph type. In the first row (in blue) are the NCFs, along with their formulas (after parameters were assigned). In the second row (in red) are the sampled linker lengths distributions derived from sampled nucleosome configurations that represent data at single cell resolution. In the third row (in green) are the sampled linker lengths distributions derived from sampled mono-nucleosome data that represents data at cell population resolution. 
 3.2 Using the model to explore linker length preferences in yeast Having selected the types of NCFs to examine, we sought to compare the  in vivo  linker length distributions to linker length distributions that are sampled using our model with each of the chosen NCF types. For this purpose, we selected particular parameter assignments for each NCF type (see  Supplementary Data ). The resulting NCFs are plotted in  Figure 2 . For each NCF, we sampled 5000 nucleosome configurations over a 500 000 bp long sub-sequence of yeast chromosome 4 using our model with that NCF (denoted  Model NCF ), with the temperature and nucleosome concentration parameters set to 1. Each sampled configuration represents sampled nucleosome positioning data in single cell resolution. Thus, by counting linker lengths appearances in the 5000 sampled configurations we derived the sampled linker lengths distribution, plotted in  Figure 2 . Next, we collected all mono-nucleosome reads out of the sampled configurations, generating the sampled mono-nucleosome positioning data of the cell population. Following the same procedure described in  Section 3.1  we further produced the sampled linker lengths distribution derived from cell population data, also plotted in  Figure 2 . Examining properties of the sampled linker lengths distributions, we find a high similarity between the shape of the NCF functions themselves ( Fig. 2 , blue graphs) and their respective sampled single cell linker lengths distributions ( Fig. 2 , red graphs). Similarities are also evident between the shape of the NCFs and their respective sampled cell population linker lengths distributions ( Fig. 2 , green graphs). This supports our approach in  Section 3.1  of selecting NCF types reflecting features that appear in the yeast  in vivo  cell population linker lengths distributions. Second, all sampled linker lengths distributions ( Fig. 2 , red graphs) show an exponential decay as linker lengths get longer, even for NCFs that do not represent such a decay, in particular the  No Coop  NCF. Thus, any sampled linker lengths distribution can be decomposed to an exponentially decaying component that is NCF-independent, and other components that depend on the particular NCF type. Third, all sampled cell population linker lengths distributions ( Fig. 2 , green graphs), except in the  S-ES  case, demonstrate a periodic pattern of subtle peaks. In the  S-ES  case, a periodic pattern of high peaks appears, concurring with the 10 n  ( n =1, 2,…) peak pattern of the  S-ES  NCF. The periodic pattern of subtle peaks apparent in all other cases starts around linker length 5, with a period slightly longer than 10 bp. The periodic pattern of subtle peaks observed in the sampled cell population linker lengths distributions is similar in all NCF cases except  S-ES , and does not depend on the NCF type. Therefore, other elements that the model accounts for produced this periodic pattern. Genomic sequences are known to encode periodic signals (Cohanim  et al. ,  2005 ,  2006 ; Widom,  1996 ) that follow a ∼10-bp periodic pattern. One possibility is that the periodic pattern of subtle peaks is mainly a result of these periodic signals. Alternatively, these peaks may result from the nucleosome sequence preferences, since aligned nucleosome sequences exhibit a ∼10bp periodic dinucleotide pattern (Ioshikhes  et al. ,  1996 ; Satchwell  et al. ,  1986 ; Segal  et al. ,  2006 ), and since the model we use (the  Nuc  model, see  Section 2.1 ) includes these periodic dinucleotide preferences. If the latter possibility is true, then using a non-uniform and non-periodic model of nucleosome sequence preferences would not produce a periodic pattern of subtle peaks. To examine this, we created an alternative model of the nucleosome sequence preferences, denoted  Nuc U , which replaces the  Nuc  model (see  Section 2.1 ), with a model in which the periodic dinucleotide preferences are removed (see  Supplementary Methods ). We repeated the above process of generating sampled linker lengths distributions from cell population data for several of the above NCFs using the  Nuc U  model, and compared them with the ones generated using the  Nuc  model. The results of this comparison appear in  Figure 3 , where for each NCF we present both sampled cell population linker lengths distributions, with the original  Nuc  model (in green), and with the  Nuc U  model (in orange). Notably, whereas the general theme of the distribution is similar for both cases, the periodic pattern of subtle peaks is abolished as a result of the removal of the periodic component of the nucleosome sequence preferences model. This demonstrates that the periodic subtle peaks pattern is mainly a result of the periodicity of the nucleosome sequence preferences. This suggests that the previously reported preferentially quantized linker lengths distribution (Cohanim  et al. ,  2006 ; Wang  et al. ,  2008 ) results mainly from the periodic sequence preferences of the nucleosome itself, rather than from periodicity of certain signals encoded in genomic sequences.
 Fig. 3. A comparison of sampled linker lengths distributions derived from cell population data that was sampled by one of two models: a model that recognizes nucleosome periodic sequence preferences (using the  Nuc  model, in green) and a model that does not (using the  Nuc U  model, in orange). The comparison was performed for four different NCFs. For each NCF, the distribution was similar in both cases, but the preference for quantized linker lengths was abolished when periodic nucleosome sequence preferences were removed. This demonstrates that preferentially quantized linker lengths distributions are mainly the result of the periodic sequence preferences of the nucleosome itself. 
 3.3 Learning nucleosome cooperativity functions from synthetic data Before trying to learn NCFs from real nucleosome positioning data, we sought to test our ability to learn NCFs from the controlled setting of synthetic data. For each of the six NCFs presented in  Figure 2  we used the sampled mono-nucleosome reads cell population data described in  Section 3.2  as six synthetic data sets. Due to experimental limitations of the nucleosome mapping experiments, in the real yeast data that we use, each nucleosome read is mapped to the genome with an estimated inaccuracy of up to 20 bp shifts from its true location. To reflect that in the synthetic setting, we randomly shifted the location of each sampled read by a number of  P noise  bp, sampled from a Normal distribution of mean 0 and SD  Std noise  (we varied  Std noise  between 0, 10, 20, 50 and 100). After adding noise to the sets, we counted for each position on the sequence the number of sampled reads that cover it. The vector of counts per position was normalized to have mean 0 and SD 1, resulting in the normalized nucleosome occupancy data required for learning NCF parameters (the Ô vector, see  Section 2.2 ). For each of the 30 synthetic sets (five noise levels for each of the six NCFs that we use), we partitioned the data into training data and test data, in a 5-fold cross validation (CV) manner. For each of the five CV groups, we tried to learn parameters for the  Exp ,  S-Exp ,  S-ES ,  Step  and  No Coop  NCFs that minimize the  L 2 -distance between the normalized training data and the normalized model predictions (see  Section 2.2 ). Along with the NCF parameters, we learned the model's temperature and nucleosome concentration parameters. For the  No Coop  NCF we learned only the last two. In all cases, a small number of parameters were learned (between 2 and 7). In the  Supplementary Methods  we address the issue of choosing an initial parameters assignment. Let   be the normalized nucleosome average occupancy predicted by the model with a learned NCF  L  over the sequence positions that correspond to the normalized test data Ô. We use the  R 2  statistic as a test of the learned NCF  L :
 
This measure quantifies the fraction of the variance in the test data that the model learned from the training data explains. The same score can be applied on the training data itself to produce a training score. The results over the different synthetic sets appear in  Figure 4 . In all cases, when no noise is introduced, we are able to reconstruct the original model (when learning parameters of an NCF of the same type that was used to sample the data) with high accuracy. One exception is in the  S-Exp * synthetic case, where we do not reconstruct the exact “shift” of the function. At high noise levels ( Std noise  50 and 100), using the original model yields worse results than other models with learned NCFs, showing that the task of learning the ‘true’ NCF parameters is hard. However, at noise levels that correspond to the estimated noise in the real yeast data that we use (when  Std noise  is up to 20, see above) we are still able to learn models that fit the data well. In the  S-Exp * and  S-ES * synthetic cases, as more noise is introduced, learning the parameters that determine the ‘shift’ (of  S-Exp  and  S-ES ) and the ‘preferred quantized lengths’ (of  S-ES ) becomes harder, and the  Exp  and  Step  functions yield better results. This shows that if an  Exp  or a  Step  function scores slightly better than an  S-Exp  or an  S-ES  function on real noisy data, we cannot rule out the possibility that the ‘true’ function is one of the latter two. Taken together, we conclude that we are able to learn NCFs in a synthetic setting, even when a realistic level of noise is introduced.
 Fig. 4. A summary of results of learning NCFs from synthetic datasets. Synthetic sets were sampled over a 500 000-bp-long sub-sequence of yeast chromosome 4, using the model with each of the NCFs:  Exp *,  S-Exp *,  S-ES *,  Off Step *,  On Step * and  No Coop * (shown in  Fig. 2 ). To each sampled set different levels of noise (different SDs for Gaussian perturbations of sampled nucleosome locations, denoted  Std noise ) were introduced. On each resulting synthetic set, parameters of five types of NCFs were learned ( Exp ,  S-Exp ,  S-ES ,  Step  and  No Coop ), together with the model's temperature and nucleosome concentration parameters, in a 5-fold cross validation manner. The results are organized in a table-like fashion, with rows per synthetic data type and columns per noise level introduced into the synthetic set. Each cell shows results attained for each of the learned NCFs, along with results attained for the original NCF (with original temperature and nucleosome concentration) used for sampling the synthetic data. Results per learned NCF are color coded according to a color legend appearing in the left part of the respective row. For each learned NCF shown are: in the bar plot, the cross validation mean (bar) and SD (blue error bar) of the test  R 2  statistic (quantifying the fraction of the variance in the test data that is explained by the model with the learned NCF), as well as the cross validation mean and SD of the train  R 2  statistic (light green error bar). In the graphs plot, shown are the cross validation mean and SD (per linker length) of the linker lengths distribution (over linker lengths 0–50) sampled using the model with the learned NCF. 
 3.4 Learning nucleosome cooperativity functions from yeast in vitro and in vivo data We now turn to learning NCFs from real data. First, we learned NCFs from yeast nucleosome mapping data taken from two  in vitro  experiment replicates that we previously measured (Kaplan  et al. ,  2009 ). Since  in vitro  there are only purified histones and naked DNA, NCFs learned from this data can represent only interactions that are intrinsic to the association of nucleosomes and DNA, and that do not depend on other factors such as transcription factors and chromatin remodeleres that are present in living cells. From the  in vitro  data, we produced  in vitro  normalized nucleosome occupancy over the yeast genome (see  Supplementary Methods ). We randomly chose a 1M bp long sub-sequence of yeast chromosome 4 and used the normalized nucleosome occupancy data over it in a 5-fold CV manner, similar to the synthetic cases in  Section 3.3 , learning parameters of the  Exp ,  S-Exp ,  S-ES ,  Step  and  No Coop  NCFs. We repeated this procedure twice more over randomly chosen 1M bp long sub-sequences of yeast chromosomes 7 and 12. The results are presented in  Figure 5 A–C, and are similar for all three chromosomes. We find that the learned  Model Exp  and  Model Step  models explain ∼74% of the variance in the test data, significantly better (Wilcoxon signed-rank test  P -values 6 × 10 −5  and 3 × 10 −4 , respectively) than the learned  Model NoCoop  model that explains ∼69.5% of the variance in the test data. This result demonstrates that modeling intrinsic interactions between adjacent nucleosomes improves the accuracy of yeast  in vitro  nucleosome occupancy predictions. The learned intrinsic interactions display a preference for short linkers, evident in the linker lengths distributions sampled by the  Model Exp  and  Model Step  models. The  Model S−Exp  and  Model S − ES  models that were learned were highly similar, and explained ∼66.5% of the variance in the test data, significantly worse (each with a Wilcoxon signed-rank test  P -value 6 × 10 −5 ) than the  Model NoCoop  model. The reason for this may be that the learned  S-Exp  and  S-ES  NCFs show a very strong disfavoring of linkers longer than 10 bp that may be too extreme.
 Fig. 5. ( A ) Parameters of five NCF types (together with the model's temperature and nucleosome concentration parameters) were learned from yeast  in vitro  data of nucleosome mapping over a 1M-bp-long sub-sequence of chromosome 4, in a 5-fold cross validation manner. Results for each NCF type are color coded according to a color legend that appears at the center of the figure. For each learned NCF shown are: in the top bar plot, the cross validation mean (bar) and SD (blue error bar) of the test  R 2  statistic (quantifying the fraction of the variance in the test data that is explained by the model with the learned NCF), as well as the cross validation mean and SD of the train  R 2  statistic (light green error bar). In the bottom bar plot, shown is the cross validation mean (bar) and SD (blue error bar) of the correlation between the test data and the model predicted average occupancy. In the graphs plot, shown is the cross validation mean and SD (per linker length) of the linker lengths distribution (over linker lengths 0–50) sampled using the model with the learned NCF. ( B ) Same as in (A), for chromosome 7. ( C ) Same as in (A), for chromosome 12. ( D – F ) Same as in (A–C), respectively, for yeast  in vivo  data. ( G ) Same as A, for  in vivo  data of  C.elegans  chromosome I. ( H ) Same as (G), for chromosome II. ( I ) Same as (G), for chromosome III. 
 Next, we examined whether interactions between adjacent nucleosomes play a similar role  in vivo . We repeated the above procedure for learning NCFs over the same three sub-sequences of chromosomes 4, 7 and 12, this time using the yeast  in vivo  data that was analyzed in  Section 3.1 . From this data we produced  in vivo  normalized nucleosome occupancy over the yeast genome (see  Supplementary Methods ). The results are presented in  Figure 5 D–F, and are again similar for all three chromosomes. The learned  Model Exp ,  Model Step ,  Model S − ES  and  Model S−Exp  models explain ∼37.5%, ∼37%, ∼34.5% and ∼34% of the variance in the test data, respectively, all significantly better (Wilcoxon signed-rank test  P -values 6 × 10 −5 , 6 × 10 −5 , 10 −3  and 10 −3 , respectively) than the learned  Model NoCoop  model that explained ∼30.5% of the variance in the test data. Importantly, the linker lengths distributions sampled using all these models are highly similar to those sampled using the models that were learned from the  in vitro  data, with the exception that in the  in vivo  case the learned  S-Exp  and  S-ES  NCFs show a weaker disfavoring of linkers longer than 10 bp. Thus, we find that modeling intrinsic interactions between adjacent nucleosomes also improves the accuracy of yeast  in vivo  nucleosome occupancy predictions. 3.5 Learning nucleosome cooperativity functions from c.elegans in vivo data To examine whether the intrinsic interactions between adjacent nucleosomes that we find in yeast play similar roles in higher eukaryotes, we applied our approach for learning parameters of the  Exp  and  No Coop  NCFs from  in vivo  nucleosome positioning data of  C.elegans . We randomly chose 1M bp long sub-sequences of  C.elegans  chromosomes I, II and III, and used published  in vivo  nucleosome occupancy data over these sub-sequences (Valouev  et al. ,  2008 ). The results are presented in  Figure 5 G–I. The results are qualitatively similar over the three chromosomes. The  Model Exp  model explained ∼13% more of the variance in the test data than the  Model NoCoop  model, and this improvement was significant (Wilcoxon signed-rank test  P -value 6 × 10 −5 ). Moreover, the resulting linker length distributions sampled by the two models are highly similar to those sampled for yeast, with the one sampled using the learned  Exp  NCF demonstrating the same preference for short linkers. This shows that, as in yeast, modeling intrinsic interactions between adjacent nucleosomes improves the accuracy of nucleosome occupancy predictions of  C.elegans in vivo . 4 DISCUSSION We presented a new thermodynamic model for genome-wide prediction of nucleosome occupancy, extending a model previously published by our lab (Field  et al. ,  2008 ; Segal  et al. ,  2006 ). The model assigns a statistical weight for each possible configuration of nucleosomes that are placed along a genomic sequence, such that no two nucleosomes overlap. The previous model assumed that the association of a nucleosome to the DNA at one place is independent of the associations of other nucleosomes elsewhere. Our new model relaxes this independence assumption by modeling interactions between adjacent nucleosomes through a nucleosome cooperativity function (NCF). Based on an analysis that involves our model we suggest that the previously reported preference for quantized linker lengths in yeast (Cohanim  et al. ,  2006 ; Wang  et al. ,  2008 ) results mainly from the periodic sequence preferences of the nucleosome itself. Our results show that by modeling interactions between adjacent nucleosomes, such that short linkers (less than 20 bp long) are preferred, we improve the accuracy of predictions of yeast  in vitro  nucleosome occupancy. The  in vitro  system contains only nucleosomes and naked yeast genomic DNA. Thus, the modeled interactions are intrinsic to the association of nucleosomes and DNA and are independent of other factors such as transcription factors and chromatin remodelers that affect chromatin organization in living cells. Notably, modeling these same interactions also improves the accuracy of nucleosome occupancy predictions of yeast  in vivo . Moreover, these intrinsic interactions also improve the accuracy of nucleosome occupancy predictions of  C.elegans in vivo , suggesting that these interactions may be universal across eukaryotes. It will be interesting to understand the mechanistic basis for the preferred nucleosome interactions that we find. One possibility is that such interactions results from direct interaction between spatially close nucleosomes, which are known to occur (Chodaparambil  et al. ,  2007 ; Dorigo  et al. ,  2004 ; Luger  et al. ,  1997 ). The fact that the modeled interactions are accompanied by a preference for short linkers may hint at that direction. Direct interaction between two adjacent nucleosomes (that may involve their histone tails) may also assist with the chromatin fiber folding, energetically justifying a shift of nucleosomes away from positions that would have been otherwise favored according to the single nucleosome sequence preferences. Funding : European Research Council (to E.S.). E.S. is the incumbent of the Soretta and Henry Shapiro career development chair. Conflict of Interest : none declared. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Network-principled deep generative models for designing drug combinations as graph sets</Title>
    <Doi>10.1093/bioinformatics/btaa317</Doi>
    <Authors>Karimi Mostafa, Hasanzadeh Arman, Shen Yang</Authors>
    <Abstract/>
    <Body>1 Introduction Drug resistance is a fundamental barrier to developing robust antimicrobial and anticancer therapies ( Housman  et al. , 2014 ;  Taubes, 2008 ). Its first sign was observed in 1940s soon after the discovery of penicillin ( Abraham and Chain, 1940 ), the first modern antibiotic. Since then, drug resistance has surfaced and progressed in infectious diseases such as HIV ( Clavel and Hance, 2004 ), tuberculosis (TB) ( Dooley  et al. , 1992 ) and hepatitis ( Ghany and Liang, 2007 ) as well as cancers ( Holohan  et al. , 2013 ). Mechanistically, it can emerge through drug efflux ( Chang and Roth, 2001 ), activation of alternative pathways ( Lovly and Shaw, 2014 ) and protein mutations ( Balbas  et al. , 2013 ;  Toy  et al. , 2013 ) while decreasing the efficacy of drugs. Combination therapy is a resistance-overcoming strategy that has found success in combating HIV ( Shafer and Vuitton, 1999 ), TB ( Ramón-García  et al. , 2011 ), cancers ( Bozic  et al. , 2013 ;  Sharma and Allison, 2015 ) and so on. Considering that most diseases and their resistances are multifactorial ( Kaplan and Junien, 2000 ;  Keith  et al. , 2005 ), multiple drugs targeting multiple components simultaneously could confer less resistance than individual drugs targeting components separately. Examples include targeting both mitogen-activated protein kinase kinase (MEK) and B-Raf proto-oncogene serine/threonine-protein kinase (BRAF) in patients with BRAF V600-mutant melanoma rather than targeting MEK or BRAF alone ( Flaherty  et al. , 2012 ;  Madani Tonekaboni  et al. , 2018 ). The effect of drug combination is usually categorized as synergistic, additive or antagonistic depending on whether it is greater than, equal to or less than the sum of individual drug effects ( Chou, 2006 ). Synergistic combinations are effective at delaying the beginning of the resistance; however, antagonistic combinations are effective at suppressing expansion of resistance ( Saputra  et al. , 2018 ;  Singh and Yeh, 2017 ), representing offensive and defensive strategies to overcome drug resistance. In particular, offensive strategies cause huge early causalities but defensive ones anticipate and develop protection against future threats ( Saputra  et al. , 2018 ). Discovering a drug combination to overcome resistance is, however, extremely challenging, even more so than discovering a drug which is already a costly (∼billions of USD) ( DiMasi  et al. , 2016 ) and lengthy (∼12 years) ( Van Norman, 2016 ) process with low success rates (3.4% phase-1 oncology compounds make it to approval and market) ( Wong  et al. , 2019 ). An apparent challenge, a combinatorial one, is in the scale of chemical space, which is estimated to be 10 60  for single compounds ( Bohacek  et al. , 1996 ) and can ‘explode’ to  10 60 K  for  K -compound combinations. Even if the space is restricted to around 10 3  U.S. Food and Drug Administration (FDA)-approved human drugs, there are 10 5 –10 6  pairwise combinations. Another challenge, a conceptual one, is in the complexity of systems biology. On top of on-target efficacy and off-target side effects or even toxicity that need to be considered for individual drugs, network-based design principles are much needed for drug combinations that effectively target multiple proteins in a disease module and have low toxicity or even resistance profiles (Billur  Engin  et al. , 2014 ;  Martínez-Jiménez and Marti-Renom, 2016 ). Current computational models in drug discovery, especially those for predicting pharmacokinetic and pharmacodynamic properties of individual drugs/compounds, can be categorized into discriminative and generative models. Discriminative models predict the distribution of a property for a given molecule, whereas generative models would learn the joint distribution on the property and molecules. For instance, discriminative models have been developed for predicting single compounds’ toxicities, based on support vector machines ( Darnag  et al. , 2010 ), random forest ( Svetnik  et al. , 2003 ) and deep learning ( Mayr  et al. , 2016 ). Whereas discriminative models are useful for evaluating given compounds or even searching compound libraries, generative models can effectively design compounds of desired properties in chemical space. Recent advance in inverse molecular design has seen deep generative models such as SMILES representation-based reinforcement learning (RL) ( Popova  et al. , 2018 ) or recurrent neural networks as well as graph representation-based generative adversarial networks (GANs), RL ( You  et al. , 2018 ) and generative tensorial RL ( Zhavoronkov  et al. , 2019 ). Unlike single-drug design, current computational efforts for drug combinations are exclusively focused on discriminative models and lack generative models. The main focus for drug combination is to use discriminate models to identify synergistic or antagonistic drugs for a given specific disease. Examples include the Chou–Talalay method ( Chou, 2010 ), integer linear programming ( Pang  et al. , 2014 ) and deep learning ( Preuer  et al. , 2018 ). However, it is daunting if not infeasible to enumerate all cases in the enormous chemical combinatorial space and evaluate their combination effects using a discriminative model. Not to mention that such methods often lack explainability. Directly addressing aforementioned combinatorial and conceptual challenges and filling the void of generative models for drug combinations, in this study, we develop network-based representation learning for diseases and deep generative models for accelerated and principled drug combination design (the general case of  K  drugs). Recently, by analyzing the network-based relationships between disease proteins and drug targets in the human protein–protein interactome, Cheng  et al.  proposed an elegant principle for FDA-approved drug combinations that targets of two drugs both hit the disease module but cover different neighborhoods. Our methods allow for examining and following the proposed network-based principle ( Cheng  et al. , 2019 ) to efficiently generate disease-specific drug combinations in a vast chemical combinatorial space. They will also help meet a critical need of computational tools in a battle against quickly evolving bacterial, viral and tumor populations with accumulating resistance. To tackle the problem, we have developed a network principle-based deep generative model for faster, broader and deeper exploration of drug combination space by following the principle underling FDA-approved drug combinations. First, we have developed hierarchical variational graph auto-encoders (HVGAE) for jointly embedding disease–disease network and gene–gene network. Through end-to-end training, we embed genes in a way that they can represent the human interactome. Then, we utilize their embeddings with novel attentional pooling to create features for each disease so that we can embed diseases more accurately. Second, we have also developed a reinforcement-learning-based graph-set generator for drug combination design by utilizing both gene/disease embedding and network principles. Besides those for chemical validity and properties, our rewards also include (i) a novel adversarial reward, generalized sliced Wasserstein distance (GSWD), that fosters generated molecules to be diverse yet similar in distribution to known compounds (ZINC database and FDA-approved drugs); and (ii) a network principle-based reward for drug combinations that are feasible for online calculations. The overall schematics are shown in  Figure 1 , and they are detailed in Section 3.
 Fig. 1. Overall schematics of the proposed approach for generating disease-specific drug combinations 2 Data 2.1 Human interactome and its features We used the human interactome data (a gene–gene network) from  Menche  et al.  (2015)  that feature 13 460 proteins interconnected by 141 296 interactions. We introduced edge features for the human interactome based on the biological nature of edges (interactions). The interactome was compiled by combining experimental support from various sources/databases including (i) regulatory interactions from TRANSFAC ( Matys  et al. , 2003 ); (ii) binary interactions from high-throughput (including  Rolland  et al.  2014 ) and literature-curated datasets [including IntAct ( Aranda  et al. , 2010 ) and MINT ( Ceol  et al. , 2010 )] as well as literature-curated interactions from low-throughput experiments [IntAct, MINT, BioGRID ( Stark  et al. , 2011 ), and HPRD ( Keshava Prasad  et al. , 2009 )]; (iii) metabolic enzyme-coupled interactions from  Lee  et al.  (2008) ; (iv) protein complexes from CORUM ( Ruepp  et al. , 2010 ); (v) kinase-substrate pairs from PhosphositePlus ( Hornbeck  et al. , 2012 ); and (vi) signaling interactions. In summary, an edge could correspond to one or multiple physical interaction types. So we used a six-hot encoding for edge features, based on whether an edge corresponds to regulatory, binary, metabolic, complex, kinase and signaling interactions. We also introduced features for nodes (genes) in the human interactome based on (i) Kyoto Encyclopedia of Genes and Genomes (KEGG) pathways ( Kanehisa  et al. , 2002 ) (336 features) queried through Biopython ( Cock  et al. , 2009 ); (ii) Gene Ontology (GO) terms ( Ashburner  et al. , 2000 ) including biological process (30 769 features), molecular function (12 183 features) and cellular component (4451 features), mapped using the NCBI Gene2Go dataset; (iii) disease–gene associations from the database OMIM (Mendelian Inheritance in Man) ( Hamosh et al., 2005 ) and the results from genome-wide association studies ( Mottaz  et al. , 2008 ;  Ramos  et al. , 2014 ) (299 features). The last 299 features correspond to 299 diseases represented by the Medical Subject Headings (MeSH) vocabulary ( Rogers, 1963 ). After removing those genes without KEGG pathway information, the human interactome used in this study has 13 119 genes and 352 464 physical interactions. 2.2 Disease–disease network We used a disease–disease network from  Menche  et al.  (2015)  with 299 nodes (diseases), created based on human interactome data (as detailed earlier), gene expression data ( Su  et al. , 2004 ), disease–gene associations ( Hamosh  et al. , 2005 ;  Mottaz  et al. , 2008 ;  Ramos  et al. , 2014 ), GO ( Ashburner  et al. , 2000 ), symptom similarity ( Zhou  et al. , 2014 ) and comorbidity ( Hidalgo  et al. , 2009 ). The original disease–disease network is a complete graph with real-valued edges. The edge value between two diseases shows how much they are topologically separated from each other. A positive/negative edge weight indicates that that two disease modules are topologically separated/overlapped. Therefore, we used zero-weight as the threshold and pruned positive-valued edges, which results in a disease–disease network of 299 nodes and 5986 edges (without weights). 2.3 Disease–gene associations We used disease–gene associations from the database OMIM ( Hamosh  et al. , 2005 ). These associations bridge aforementioned gene–gene and disease–disease networks into a hierarchical graph of genes and diseases, based on which gene and disease representations will be learned. 2.4 Disease classification For the purpose of assessment, we used the Comparative Toxicogenomics Database (CTD) ( Davis  et al. , 2019 ) to classify diseases into eight classes based on their Disease Ontology (DO) terms ( Schriml  et al. , 2012 ) where diseases are represented in the MeSH vocabulary ( Rogers, 1963 ). In the CTD database only 201 of the 299 diseases have a corresponding DO term. Therefore, for the 98 diseases with missing DO terms we considered the majority of their parents’ DO terms, if applicable, as their DO terms. With this approach, we assigned DO terms to 66 such diseases and classified 267 of the 299 diseases. The 32 diseases with DO terms still missing are usually at the top layers of the MeSH tree. 2.5 FDA-approved drugs and drug combinations To assess our deep generative model for drug combination design (to be detailed in Section 3.2), we consider a comprehensive list of US FDA-approved combination drugs (1940–2018.9) ( Das  et al. , 2019 ). The dataset contains 419 drug combinations consisting of 328 unique drugs, including 341 (81%), 67 (16%) and 11 (3%) of double, triple and quadruple drug combinations. We also utilized the curated drug–disease association from CTD database ( Davis  et al. , 2019 ). 3 Materials and Methods We have developed a network-based drug combination generator which can be utilized in overcoming drug resistance. Representing drugs through their molecular graphs, we recast the problem of drug combination generation into network-principled graph-set generation by incorporating prior knowledge such as human interactome (gene–gene), disease–gene, disease–disease, gene pathway and gene–GO relationships. Furthermore, we formulate the graph-set generation problem as learning an RL agent that iteratively adds substructures and edges to each molecular graph in a chemistry- and system-aware environment. To that end, the RL model is trained to maximize a desired property  Q  (e.g. therapeutic efficacy for drug combinations) while following the valency (chemical validity) rules and being similar in distribution to the prior set of graphs. As shown in  Figure 1 , the proposed approach consists of (i) embedding prior knowledge (different network relationships) through HVGAE and (ii) generating drug combinations as graph sets through an RL algorithm, which will be detailed next. 
 Notations : As both gene–gene and disease–disease networks can be represented as graphs, notations are differentiated by superscripts ‘g’ and ‘d’ to indicate gene–gene and disease–disease networks, respectively. Drugs (compounds) are also represented as graphs and notations with ‘ k ’ in the superscript indicate the  k th drug (graph) in the drug combination (graph set). 3.1 HVGAE for representation learning Suppose that a gene–gene network is represented as a graph  G ( g ) = ( A ( g ) , { F ( g , m ) } m = 1 M ) , where  A ( g ) = [ A ( g , 1 ) , … , A ( g , n e ) ] ∈ { 0 , 1 } n g × n g × n e  is the adjacency tensor of the gene–gene network with  n g  nodes and  n e  edge types ( k -hot encoding of six types of aforementioned physical interactions such as regulatory, binary, metabolic, complex, kinase and signaling interactions). We also define  A ˜ ( g ) ∈ { 0 , 1 } n g × n g  to be elementwise  OR  of  { A ( g , 1 ) , … , A ( g , n e ) } . Furthermore,  F ( g , m )  denotes the  m th set of node features for gene–gene network where  M  (5 in the study) represents different types of node features such as pathways, three GO terms and gene–disease relationship. We also suppose that the disease–disease network is represented as graph  G ( d ) = ( A ( d ) , F ( d ) ) , where  A ( d ) ∈ { 0 , 1 } n d × n d  is the adjacency matrix of the disease–disease network with  n d  nodes; and  F ( d )  represents the set of node features for the disease–disease network. We have developed a hierarchical embedding with two levels. In the first level, we embed the gene–gene network to get the features related to each disease and then we incorporate the disease features within the disease–disease network to embed their relationship. We infer the embedding for each gene and disease jointly through end-to-end training. The proposed HVGAE perform probabilistic auto-encoding to capture uncertainty of representations which is in the same spirit as the variational graph auto-encoder models introduced in ( Hajiramezanali  et al. , 2019 ;  Hasanzadeh  et al. , 2019 ;  Kipf and Welling, 2016 ). 3.1.1 First level: gene–gene embedding The inference model for variational embedding of the gene–gene network is formulated as follows. We first use  M  graph neural networks (GNNs) to transform individual nodes’ features in  M  types and then concatenate the  M  sets of results  F ^ ( g , m )  ( m = 1 , … , M ) into  F ^ ( g ) :
 (1) F ^ ( g , m ) = AGG ( { GNN j ( A ( g , j ) , F ( g , m ) ) } , j = 1 , ⋯ , n e ) F ^ ( g , m ) ∈ R n g × L g ,   m = 1 , ⋯ , M F ^ ( g ) = CONCAT ( { F ^ ( g , m ) } m = 1 M ) ∈ R n g × M L g , where  AGG  is an aggregation function combining output features of  GNN j s for each node. We used a two layer fully connected neural network with ReLU activation functions followed by a single linear layer in our implementation. We then approximate the posterior distribution of stochastic latent variables  Z ( g )  [containing  z i ( g ) ∈ ℝ L g  for  i = 1 , … , n g  where  L g  (32 in this study) is the latent space dimensionality for the  i th gene], with a multivariate Gaussian distribution  q ( · )  given the gene–gene network’s aggregated node features  F ^ ( g )  and adjacency tensor  A ( g ) :
 (2) q ( Z ( g ) | F ^ ( g ) , A ( g ) ) = ∏ i = 1 n g q ( z i ( g ) | F ^ ( g ) , A ( g ) ) , where q ( z i ( g ) | F ^ ( g ) , A ( g ) ) = N ( μ i ( g ) , diag ( σ i 2 , ( g ) ) ) , μ ( g ) = AGG ( { GNN μ , g , j ( A ( g , j ) , F ^ ( g ) ) } ,   j = 1 , … , n e ) ,   log   ⁡ ( σ ( g ) ) = AGG ( { GNN σ , g , j ( A ( g , j ) , F ^ ( g ) ) } ,   j = 1 , … , n e ) , μ ( g ) ∈ ℝ n g × L g ,     log   ⁡ ( σ ( g ) ) ∈ ℝ n g × L g , where  Z ( g ) ∈ ℝ n g × L g ;  μ ( g )  is the matrix of mean vectors  μ i ( g ) ; and  σ ( g )  is the matrix of standard deviation vectors  σ i ( g )  ( i = 1 , … , n g ). The generative model for the gene–gene network is formulated as:
 (3) p ( A ˜ ( g ) | Z ( g ) ) = ∏ i = 1 n ∏ j = 1 n   p ( A ˜ i j ( g ) | z i ( g ) , z j ( g ) ) , where p ( A ˜ i j ( g ) | z i ( g ) , z j ( g ) ) = σ ( z i ( g ) z j ( g ) T ) , and  σ ( · )  is the logistic sigmoid function. The loss for gene–gene variational embedding is represented as a variational lower bound (ELBO):
 (4) L ( g ) =   E q ( Z ( g ) | F ^ ( g ) , A ( g ) ) [ log   ⁡ p ( A ˜ ( g ) | Z ( g ) ) ] − KL ( q ( Z ( g ) | F ^ ( g ) , A ( g ) ) | | p ( Z ( g ) ) ) , where  KL ( q ( · ) | | p ( · ) )  is the Kullback–Leibler divergence between  q ( · )  and  p ( · ) . We take the Gaussian prior for  p ( Z ( g ) )  and make use of the reparameterization trick ( Kipf and Welling, 2016 ) for training. 3.1.2 Second level: disease–disease embedding The inference model for variational embedding of the disease–disease network is similar to that of the gene–gene network except that the disease–disease network’s aggregated node features,  F ^ ( d ) , are derived through parameterized attentional pooling of  Z ^ r ( g ) , latent variables of genes associated with the  r th disease (a subset of  Z ( g ) ):
 (5) e r = v   tanh ( Z ^ r ( g ) W + b ) ,   r = 1 , ⋯ , n d α r = softmax ( e r ) ,   r = 1 , ⋯ , n d F ^ r ( d ) = ∑ i α r , i Z ^ r , i ( g ) ,   r = 1 , ⋯ , n d F ^ ( d ) = CONCAT ( { F ^ r ( d ) } r = 1 n d ) ∈ R n d × L d , where  α m  capture the importance of genes related to the  r th disease for calculating its latent representations and  L d  is the latent space dimensionality of a disease. Once  F ^ ( d ) , the disease–disease network’s aggregated node features for all diseases, are derived; we again define  q ( Z ( d ) | F ^ ( d ) , A ( d ) )  for the posterior distribution of stochastic latent variables  Z ( d )  similarly to what we did in  Equation (2)  except that AGG functions are removed since disease–disease network has one binary adjacency matrix; give the generative decoder  p ( A ( d ) | Z ( d ) )  for embedding the disease–disease network similarly to what we did in  Equation (3) ; and calculate the variational lowerbound (ELBO) loss  L ( d )  for the disease–disease network similarly to what we did in  Equation (4) . Details can be found in  Supplementary Section S1.1 . Both levels of our proposed HVGAE, i.e. gene–gene and disease–disease variational graph representation learning, are jointly trained in an end-to-end fashion using the following overall loss:
 (6) L HVGAE = L ( d ) + L ( g ) . 3.2 RL-based graph-set generator for drug combinations In this section, we introduce the RL-based drug combination generator. We will detail (i) the state space of graph sets ( K  compounds) and the action space of graph-set growth; (ii) multi-objective rewards including chemical validity and our generalized sliced Wasserstein reward for individual drugs as well as our newly designed network principle-based reward for drug combinations; and (iii) policy network that learns to take actions in the rewarding environment. 3.2.1 State and action space We represent a graph set (drug combination) with  K  graphs as  G = { G ( k ) } k = 1 K . Each graph  G ( k ) = ( A ( k ) , E ( k ) , F ( k ) )  where  A ( k ) ∈ { 0 , 1 } n k × n k  is the adjacency matrix,  F ( k ) ∈ ℝ n k × ϕ  the node feature matrix,  E ( k ) ∈ { 0 , 1 } ϵ × n k × n k  the edge-conditioned adjacency tensor and  n k  the number of vertices for the  k th graph; and  ϕ  is the number of features per nodes and  ϵ  the number of edge types. The state space  G  is the set of all  K  graphs with different numbers and types of nodes or edges. Specifically, the state of the environment  s t  at iteration  t  is defined as the intermediate graph set  G t = { G t ( k ) } k = 1 K  generated so far which is fully observable by the RL agent. The action space is the set of edges that can be added to the graph set. An action  a t  at iteration  t  is analogous to link prediction in each graph in the set. More specifically, a link can either connect a new subgraph (a single node/atom or a subgraph/drug-substructure) to a node in  G t ( k )  or connect existing nodes within graph  G t ( k ) . The actions can be interpreted as connecting the current graph with a member of scaffold subgraphs set  C . Mathematically, for  G t ( k ) , graph  k  at step  t , the action  a t ( k )  is the quadruple of  a t ( k ) = concat ( a first , t ( k ) , a second , t ( k ) , a edge , t ( k ) , a stop , t ( k ) ) . 3.2.2 Multi-objective reward We have defined a multi-objective reward  R t  to satisfy certain requirements in drug combination therapy. First, a chemical validity reward maintains that individual compounds are chemically valid. Second, a novel adversarial reward, generalized sliced Wasserstein GAN (GS-WGAN), enforces that generated compounds are synthesizable and ‘drug-like’ by following the distribution of synthesizable compounds in the ZINC database ( Irwin and Shoichet, 2005 ) or FDA-approved drugs. Third, a network principle-based award would encourage individual drugs to target the desired disease module but not to overlap in their target sets. Toxicity due to drug–drug interactions (DDIs) can also be included as a reward. It is intentionally left out in this study so that toxicity can be evaluated for drug combinations designed to follow the network principle. When training the RL agent, we use different reward combinations in different stages. We first only use the weighted combination of chemical validity and GS-WGAN awards learning over drug combinations for all diseases; then we remove the penalized logP (Pen-logP) portion of chemical validity and add adversarial loss again while learning over drug combinations for all diseases; and finally use the combination of the three rewards as in the second stage but focusing on a target disease and possibly on restricted actions/scaffolds (in a spirit similar to transfer learning). The three types of rewards are detailed as follows. 
 Chemical validity reward for individual drugs.  A small positive reward is assigned if the action does not violate valency rules. Otherwise a small negative reward is assigned. This is an intermediate reward added at each step. Another reward is on penalized logP (lipophilicity where P is the octanol–water partition coefficient) or Pen-logP values. The design and the parameters of this reward are adopted from  You  et al.  (2018)  without optimization. 
 Adversarial reward using GSWD.  To ensure that the generated molecules resemble a given set of molecules (such as those in ZINC or FDA-approved), we deploy GANs. GANs are very successful at modeling high-dimensional distributions from given samples. However, they are known to suffer from training instability and cannot generate diverse samples (a phenomenon known as  mode collapse ). Wasserstein GANs (WGAN) have shown to improve stability and mode collapse by replacing the Jenson–Shannon divergence in original GAN formulation with the Wasserstein distance (WD) ( Arjovsky  et al. , 2017 ). More specifically, the objective function in WGAN with gradient penalty ( Gulrajani  et al. , 2017 ) is defined as follows:
 (7) min ⁡ θ max ⁡ ϕ V W ( π θ , D ϕ ) + λ R ( D ϕ ) , with   V W ( π θ , D ϕ ) = E x ∼ p r [ log   ⁡ D ϕ ( x ) ] − E y ∼ π θ [ log   ⁡ D ϕ ( y ) ] , where  p r  is the data distribution,  λ  is a hyper-parameter,  R  is the Lipschitz continuity regularization term,  D ϕ  is the critic with parameters  ϕ , and  π θ  is the policy (generator) with parameters  θ . Despite theoretical advantages of WGANs, solving  Equation (7)  is computationally expensive and intractable for high-dimensional data. To overcome this problem, we propose and formulate a novel GS-WGAN which deploys GSWD ( Kolouri  et al. , 2019 ). GSWD, first, factorizes high-dimensional probabilities into multiple marginal 1D distributions with generalized Radon transform (GRT). Then, by taking advantage of closed form solution of WD in 1D, the distance between two distributions is approximated by the sum of WDs of marginal 1D distributions. More specifically, let  R  represent GRT operator. The GRT of a probability distribution  P ( · )  which is defined as follows:
 (8) R P ( t , ψ ) = ∫ ℝ d P ( x )   δ ( t − f ( x , ψ ) )   d x , where  δ ( · )  is the 1D Dirac delta function,  t ∈ ℝ  is a scalar,  ψ  is a unit vector in the unit hyper-sphere in a  d -dimensional space ( S d − 1 ), and  f  is a projection function whose parameters will be learned in training. Injectivity of the GRT ( Beylkin, 1984 ) is the requirement for the GSWD to be a valid distance. We use linear project  f ( x , ψ )  here and can easily extend to two nonlinear cases that maintain the GRT-injectivity (circular nonlinear projections or homogeneous polynomials with an odd degree). GSWD between two d-dimensional distributions  P X  and  P Y  is therefore defined as:
 (9) GSWD ( P X ,   P Y ) = ∫ S d − 1 WD ( R P X ( · , ψ ) ,   R P Y ( · , ψ ) )   d ψ   . The integral in the above equation can be approximated with a Riemann sum. Knowing the definition of GSWD, we define the objective function of GS-WGAN as follows:
 (10) min ⁡ θ max ⁡ ϕ V GSW ( π θ , D ϕ ) + λ R ( D ϕ ) , (11) s . t .   V GSW ( π θ , D ϕ ) = ∫ ψ ∈ S d - 1 E x ∼ p r [ log   ⁡ D ϕ ( x ) ] - E y ∼ π θ [ log   ⁡ D ϕ ( y ) ]   d ψ   , where the parameters and notations are the same as defined in  Equation (7) . We note that  x  and  y  in  Equation (10)  are random variables in  ℝ d , which is not a reasonable assumption for graphs. To that end, we use an embedding function  g  that maps each graph to a vector in  ℝ d . We use graph convolutional (GC) layers followed by fully connected layers to implement  g . We deploy the same type of neural network architecture for  D ϕ . We use  R advers = − V GSW ( π θ , D ϕ )  as the adversarial reward used together with other rewards and optimize the total rewards with a policy gradient method (Section 3.2.3). 
 Network principle-based reward for drug combinations.  Proteins or genes associated with a disease tend to form a localized neighborhood disease module rather than scattering randomly in the interactome ( Cheng  et al. , 2019 ). A network-based score has been introduced ( Menche  et al. , 2015 ) to efficiently capture the network proximity of a drug ( X ) and disease ( Y ) based on the shortest-path length  d ( x ,  y ) between a drug target ( x ) and a disease protein ( y ):
 (12) Z = d ( X , Y ) − d ¯ σ d d ( X , Y ) = 1 | | Y | | ∑ y ∈ Y min ⁡ x ∈ X d ( x , y ) , where  d ( · , · )  is the shortest path distance;  d ¯  and  σ d  are the mean and standard deviation of the reference distribution which is corresponding to the expected network topological distance between two randomly selected groups of proteins matched to size and degree (connectivity) distribution as the original disease proteins and drug targets in the human interactome. Z-score being negative ( Z  &lt;   0) implies network proximity of disease module and drug targets which is desirable. From the drug combination perspective, it has been shown that the drug–drug relationship of complementary exposure has the least side effect and the most clinical efficacy ( Cheng  et al. , 2019 ). Complementary exposure for two drugs ( X 1  and  X 2 ) means that the  X 1  drug targets ( x 1 ) and  X 2  drug targets ( x 2 ) are not in the same neighborhood and have the least overlapping. Therefore,  Cheng  et al.  (2019)  have proposed a network-separation score which is formulated as follows:
 (13) s X 1 , X 2 = d ( X 1 , X 2 ) − d ( X 1 , X 1 ) + d ( X 2 , X 2 ) 2 , where  d ( X 1 , X 2 )  is the mean shortest path distance between drugs  X 1  and  X 2 ;  d ( X 1 , X 1 )  and  d ( X 2 , X 2 )  are the mean shortest path distance within drug targets  X 1  and  X 2 , respectively. The separation score being positive ( s  &gt;   0) implies that two networks are separated from each other, which is desirable. We have extended and combined these scores for general drug combination therapy where we have a set of  k  drugs  { X 1 , … , X k }  and disease  Y :
 (14) R network = λ 1 ∑ i = 1 k ∑ j &gt; i s ( X i , X j ) − λ 2 ∑ i = 1 k Z ( X i , Y ) . However, the exact online calculation of the reward  R network  is infeasible while training across all the diseases and the whole human interactome with more than 13K nodes and 352K edges. Therefore, we have developed a relaxed version of the reward which is feasible for online calculation and correlates with the actual reward. Specifically, we consider the normalized exclusive or (XOR) of intersections of disease modules with drug targets:
 (15) R ^ network = Y ∩ ( X 1 ⊕ ⋯ ⊕ X k ) | Y | = ( X 1 ∩ Y ) ⊕ ⋯ ⊕ ( X k ∩ Y ) | Y | . The relaxed network principle-based reward is penalizing a drug combination if the overlap between drug targets in the disease module is high; therefore, it will prevent the adverse DDIs. We scaled the network score by a constant (equals 10) such that the score would be in the same range as Pen-logP and can use the same weight in the total reward as Pen-logP did in  You  et al.  (2018) . For a generated compound, we predict its protein targets by DeepAffinity ( Karimi  et al. , 2019 ), judging by whether the predicted IC 50  is below 1  μ M. 3.2.3 Policy network Having explained the graph generation environment (various rewards), we outline the architecture of our proposed policy network. Our method takes the intermediate graph set  G t  and the collection of scaffold subgraphs  C  as inputs, and outputs the action  a t , which predicts a new link for each of the graphs in  G t  ( You  et al. , 2018 ). Since the input to our policy network is a set of  K  compounds or graphs  { G t ( k ) ∪ C } k = 1 K , we first deploy some layers of graph neural network to process each of the graphs. More specifically,
 (16) X ( k ) = GN N ( k ) ( G t ( k ) ∪ C ) ,           for       k = 1 , … , K , where  GN N ( k )  is a multilayer-graph neural network. The link prediction-based action at iteration  t  is a concatenation of four components for each of the  K  graphs: selection of two nodes, prediction of edge type, and prediction of termination. Each component is sampled according to a predicted distribution ( You  et al. , 2018 ). Details are included in the  Supplementary Section S1.2 . We note that the first node is always chosen from  G t  while the next node is chosen from  { G t ( k ) ∪ C } k = 1 K . We also note that infeasible actions (i.e. actions that do not pass valency check) proposed by the policy network are rejected and the state remains unchanged. We adopt proximal policy optimization (PPO) ( Schulman  et al. , 2017 ), one of the state-of-the-art policy gradient methods, to train the model. 4 Results To assess the performance of our proposed model, we have designed a series of experiments. In Section 4.1, we first compare HVGAE to state-of-the-art graph embedding methods in disease–disease network representation learning and further include several variants of HVGAE for ablation studies. We then assess the performance of the proposed RL method in two aspects. In a landscape assessment in Section 4.2, we examine designed pairwise compound combinations for 299 diseases in quantitative scores of following a network-based principle ( Cheng  et al. , 2019 ). In Section 4.3, we focus on four case studies involving multiple diseases of various systems pharmacology strategies. Our method is capable of generating higher-order combinations of  K  drugs. As FDA-approved drug combinations are often pairs, here we design compound pairs from the scaffolds of FDA-approved drug pairs. We further delve into designed compound pairs to understand the benefit of following network principles in lowering toxicity from DDIs. We also do so to understand their systems pharmacology strategies in comparison to the FDA-approved drug combinations. 4.1 HVGAE representation compares favorably to baselines 4.1.1 Experiment setup To assess the performance of our proposed embedding method HVGAE, we compare its performance in (disease–disease) network reconstruction with Node2Vec ( Grover and Leskovec, 2016 ), DeepWalk ( Perozzi  et al. , 2014 ) and VGAE ( Kipf and Welling, 2016 ), as well as some variants of our own model for ablation study. Node2Vec and DeepWalk are random walk-based models that do not capture node attributes; hence, we only used the disease–disease graph structure. For VGAE, we used identity matrix as node attributes as suggested by the authors. For our HVGAE described in Section 3.1, we also considered two variants for ablation study: HVGAE-disjoint does not jointly embed gene–gene and disease–disease networks and does not use attentional pooling for disease embedding, whereas HVGAE-noAtt just does not use attentional pooling. Specifically, in HVGAE-disjoint, we, first, learned an embedding for gene–gene network, then used the sum of mean of the node representations of genes affected by a disease as its node attributes. In HVGAE-noAtt, we jointly learned the representations while using sum of mean of the node representations of genes as node attributes for disease–disease network. In node2vec and DeepWalk, the walk length was set to 80, the number of walks starting at each node was set to 10, and the nodes were embedded to a 16D space. The window size was 10 for node2vec while it is set to 10 in DeepWalk. All models were trained using Adam optimizer. In VGAE, a 32D GC layer followed by two 16D layers was used for mean and variance inference. The learning rate was set to 0.01. For HVGAE and its variants (for ablation study), we embed gene networks in 32D space using a single GC layer with 32 filters for each of the five types of input followed by a 64D GC layer and two 32D GC layer to infer mean and variance of the representation. We used a single 32D fully connected layer for attention layer. For disease–disease network embedding, we deployed a single 32D GC layer followed by two 16D layer for mean and variance inference resulting in 16D embedding for disease–disease network. Learning rates were set to 0.001. The models were trained for 1000 epochs choosing the best representation based on their reconstruction performance at each epoch. 4.1.2 Numerical analysis and ablation study for network embedding 
 Table 1  summarizes the reconstruction performance of the aforementioned methods. Compared to all baselines, our HVGAE showed the best performance in all metrics considered. Node2Vec and DeepWalk showed the worst performance as they only use the graph structure. The performance of VGAE was very close to DeepWalk. This is due to the fact that no attributes have been provided to VGAE despite having the capability of capturing attributes.
 Table 1. Graph reconstruction performances (unit: %) in the disease–disease network using our proposed HVGAE and baselines Method AUC-ROC AP F1-Macro F1-Micro Node2Vec 79.01 72.82 35.73 51.10 DeepWalk 79.32 73.77 40.28 53.30 VGAE 88.12 85.71 60.19 64.98 
 HVGAE-disjoint 
 91.45 90.72 73.45 74.77 
 HVGAE-noAtt 
 92.83 92.34 73.81 75.14 
 HVGAE 
 
 96.11 
 
 95.89 
 
 79.77 
 
 80.45 
 
 Note : F1 scores are based on 50% threshold. Method names in bold are our HVGAE and its variants. Numbers in bold correspond to the best performances. Compared to VGAE, HVGAE-disjoint without joint embedding or attentional pooling still saw better performance, which suggests that the attribute generated by the gene–gene network contains meaningful features about the disease–disease network. The slight performance gain from HVGAE-disjoint to HVGAE-noAtt shows that joint learning of both networks hierarchically helps to render more informative features for the disease–disease network. Finally, HVGAE had another performance boost compared to HVGAE-noAtt and outperformed all competing methods, which shows the benefit of attentional pooling. Specifically, the attention layer of HVGAE allows the model to produce features that are specifically informative for the disease–disease network representation learning. 4.2 Our model generates drug combinations following network principles across diseases 4.2.1 Experiment setup We have trained the proposed reinforcement model in three stages using different rewards, disease sets and action spaces to increasingly focus on a target disease while exploiting all diseases whose representations already jointly embed gene–gene, disease–disease and gene–disease networks. In the first stage, we train the model to only generate drug-like small molecules which follow the chemistry valency reward, lipophilicity reward (Pen-logP where P is the octanol–water partition coefficient) ( You  et al. , 2018 ), and our novel adversarial reward for individual compounds. In this study, we trained the model for 3 days (4800 iterations) to learn to follow the valency conditions and promote high logP for generated compounds. In the second stage, we start from the trained model at the end of the first stage (‘warm-start’ or ‘pre-training’). And we continue to train the model to generate good drug combinations across all diseases. We do so by adding the network principle-based reward for compound combinations and sequentially generating drug combinations for each disease one by one. Then, we calculate the network-based score for the generated drug combinations at the last epoch across disease ontologies and compare them with the FDA-approved melanoma drug combinations’ network-based score. In this study, we trained the model for 1500 iterations to generate drug combinations across all 299 diseases. In each iteration, we generated eight drug combinations for a given disease. We adopted PPO ( Schulman  et al. , 2017 ) with a learning rate of 0.001 to train the proposed RL for both stages. The last stage is disease-specific and is detailed in Section 4.3. 4.2.2 Numerical analysis Across disease ontologies we quantify the performance of the proposed RL (stage 2 model first) using quantitative scores of compound combinations following a network-based principle ( Cheng  et al. , 2019 ). We consider the generated combinations in the last epoch (the last 299 iterations) and calculate the network score  R ^ network  based on disease ontologies. We asses our model based on two versions of disease classification, original disease ontology and its extension, explained in Section 2.4.  Table 2  summarizes the network-based scores for our model. Specifically, suppose that the set of targets for drugs 1 and 2 are represented by  A  and  B  whereas the disease module is the universal set Ω, we report the portion exclusively covered by drug 1 ( η A − B ), exclusively covered by drug 2 ( η B − A ), overlapped by both ( η A ∩ B ) and collectively by both ( η A ∪ B ). As a reference, we calculated the corresponding network scores for three FDA-approved drug combinations for melanoma.
 Table 2. Network-based score for the generated drug combinations based on disease ontology classifications Disease Ontology Disease Ontology extended 
 η A − B 
 
 η B − A 
 
 η A ∩ B 
 
 η A ∪ B 
 
 η A − B 
 
 η B − A 
 
 η A ∩ B 
 
 η A ∪ B 
 Infectious disease 0.25 0.10 0.06 0.41 0.20 0.07 0.05 0.33 Disease of anatomical entity 0.27 0.12 0.10 0.49 0.26 0.11 0.09 0.48 Disease of cellular proliferation 0.25 0.09 0.07 0.42 0.25 0.10 0.08 0.44 Disease of mental health 0.22 0.11 0.10 0.43 0.22 0.11 0.10 0.43 Disease of metabolism 0.22 0.13 0.10 0.46 0.23 0.14 0.11 0.48 Genetic disease 0.23 0.15 0.11 0.4 0.23 0.15 0.11 0.49 Syndrome 0.22 0.11 0.11 0.44 0.22 0.11 0.11 0.44 Based on the results shown in  Table 2 , we note that across all disease classes, the designed compound combinations learned in an environment, where the network principle( Cheng  et al. , 2019 ) was rewarded, did achieve the desired performances. Specifically, their overlaps in disease modules were low as  η A ∩ B  fractions were around 0.1; whereas their joint coverage in disease modules was high as  η A ∪ B  fractions were in the range of 0.4–0.5 for all diseases. Compared to a few FDA-approved drugs for melanoma in  Table 3 , we notice that the designed compound combinations had similar exclusive coverage ( η A − B  and  η B − A ) as the drug combinations. However, the overlapping and overall coverage ( η A ∩ B  and  η A ∪ B ) were both much higher in FDA-approved drug combinations than the designed. Improvements could be made by training the RL agent longer, as these scores had already been improving during the limited training process under computational restrictions. More improvement can be made by adjusting the network-based reward as well.
 Table 3. Network-based scores for FDA-approved melanoma drug combinations 
 η A − B 
 
 η B − A 
 
 η A ∩ B 
 
 η A ∪ B 
 Dabrafenib + Trametinib 0.05 0.21 0.55 0.81 Encorafenib + Binimetinib 0.21 0.05 0.53 0.86 Vemurafenib + Cobimetinib 0.05 0.27 0.36 0.68 4.3 Case studies for specific diseases 4.3.1 Experiment setup In the third and last stage of RL model training, we start from the stage 2 model and generate drug combinations for a fixed target disease and can choose scaffold libraries specific to the disease. In parallel, we trained the model for 500 iterations (roughly 1 day) to generate 4000 drug combinations specifically for each of four diseases featuring various drug-combination strategies: melanoma, lung cancer, ovarian cancer and breast cancer. In all cases, we started with the Murcko scaffolds of specific FDA-approved drug combinations to be detailed next. 
 Melanoma: Different targets in the same pathway . Resistance to BRAF kinase inhibitors is associated with reactivation of the mitogen-activated protein kinase (MAPK) pathway. There is, thus, a phase 1 and 2 trial of combined treatment with Dabrafenib, a selective BRAF inhibitor, and Trametinib, a selective MAPK kinase (MEK) inhibitor. As melanoma is not one of the 299 diseases, we chose broader neoplasm as an alternative. To compensate the loss of focus on target disease, we design compound pairs from Murcko scaffolds of Dabrafenib + Trametinib. 
 Lung and ovarian cancers: targeting parallel pathways.  MAPK and phosphoinositide 3-kinase (PI3K) signaling pathways are parallels important for treating many cancers, including lung and ovarian cancers ( Bedard  et al. , 2015 ;  Day and Siu, 2016 ). Clinical data suggest that dual blockade of these parallel pathways has synergistic effects. Buparlisib (BKM120) and Trametinib (GSK1120212; Mekinist) are used as a drug combination therapy for the purpose. Specifically, Buparlisib is a potent and highly specific PI3K inhibitor, whereas Trametinib is a highly selective, allosteric inhibitor of MEK1/MEK2 activation and kinase activity ( Bedard  et al. , 2015 ). 
 Breast cancer: reverse resistance.  Endocrine therapies, including Fulvestrant, are the main treatment for hormone receptor-positive breast cancers (80% of breast cancers) ( Turner  et al. , 2015 ). However, they could confer resistance to patients during or after the treatment. A Phase 3 study is using Fulvestrant and Palbociclib as a combination therapy to reverse the resistance. Fulvestrant and Palbociclib are targeting different genes in different pathways. Specifically, Fulvestrant targets estrogen receptor  α  in estrogen signaling pathway and Palbociclib targets cyclin-dependent kinases 4 and 6 (CDK4 and CDK6) in cell cycle pathway ( Turner  et al. , 2015 ). 4.3.2 Baseline methods for drug pair combination Since our proposed method is the first to generate drug combinations for specific diseases, we consider the following baseline methods to compare with: (i) random selection of 1000 pairs from 8724 small-molecule drugs in DrugBank ( Wishart  et al. , 2018 ); (ii) 628 FDA-approved drug combinations curated by  Cheng  et al.  (2019)  for hypertension and cancers (our case studies are on four types of cancers); (iii) random selection of 1000 pairs of FDA-approved drugs for the given disease, based on drug–disease dataset ‘SCMFDD-L’ ( Zhang  et al. , 2018 ). 4.3.3 Designed pairs follow network principles and improve toxicity We first compare the compound combinations designed by our model and those from the baselines using the network score that reflects the network-based principle.  Figure 2(a–d)  shows that our designed combinations in all four cases, with higher network scores in distribution, respected the network principle more than the baselines (including the FDA-approved pairs not necessarily specific for the target disease). The observation is statistically significant with  P -values ranging from 6E-74 to 7E-7 (one-sided Kolmogorov–Smirnov test; see more details in the  Supplementary Tables S2 and S3 ). Such a result is thanks to the network-principled reward we introduced.
 Fig. 2. Comparison of network score and toxicity of RL-generated pairs of compounds (our proposed method) with three baselines, i.e. random pairs of DrugBank compounds, FDA-approved drug pairs and random pairs of FDA-approved drugs for four case-study diseases. Panels (a-h) are combinations of row measure (network score or toxicity) and column diseases (melanoma, lung cancer, ovarian cancer, and breast cancer) We also examine whether drug combinations designed to follow the network principle could reduce toxicity from DDIs. DDIs are crucial when using drug combinations since they may trigger unexpected pharmacological effects, including adverse drug events. We used a deep-learning model DeepDDI ( Ryu  et al. , 2018 ) with a mean accuracy of 92.4% to predict for each combination the probabilities of 86 types of DDIs (we manually split them into 16 positives and 70 negatives; see details in the  Supplementary Section S1.3 ). To summarize over the DDIs, we considered both maximum and mean probabilities of positive or negative ones. And we compared those distributions between our designed pairs and baselines in each disease. 
 Figure 2(e–h) , using the mean probability among negative DDIs, shows that our compound pairs designed for all four diseases were predicted to have less chances of toxicity compared to the baselines. One-sided Kolmogorov–Smirnov tests attested to the statistical significance of the observation as  P -values ranged between 2E-166 and 2E-53. More analyses can be found in the  Supplementary Section S3 . Taken together,  Figure 2  suggested that following the network principle in designing drug combinations would help reduce toxicity due to DDIs. 4.3.4 Designed pairs reproduce approved polypharmacology strategies We next examine the DeepAffinity-predicted target genes of our designed pairs and compare them to the polypharmacology strategies outlined in Section 4.3.1 for each disease. Since improved network scores have been shown to correlate with lower toxicity, we used the scores to filter the 4000 combinations designed for each disease. Specifically, we retained combinations with network scores above 0.5 and  η A ∩ B  below 0.1. These designs are shared along with the codes. For melanoma, out of 69 combination designs retained, 26% were predicted to jointly cover BRAF and MEK genes in a complementary way. In other words, one molecule only targets BRAF and the other only targets MEK, according to our DeepAffinity ( Karimi  et al. , 2019 )-predicted IC 50 , echoing the systems pharmacology strategy of the drug combination of Dabrafenib and Trametinib. There were also other designs which demand further examination and potentially contain novel strategies. All retained designs were predicted to target the MAPK pathway to which BRAF and MEK belong. For lung and ovarian cancers, the same filtering criteria retained 204 (896) compound combinations designed for ovarian (lung) cancer. As disease modules can be limited, MEK1/2 does not exist in the used modules for ovarian or lung cancer so a gene-level analysis cannot be performed as the melanoma case. Instead, we performed the pathway-level analysis and found that 50.9% (45.2%) of combination designs for ovarian (lung) cancer were predicted to jointly and complementarily cover the MAPK and PI3K signaling pathways, which echoes the combination of Buparlisib and Trametinib. Moreover, 99.5% (100%) of these retained designs were predicted to jointly target both pathways for ovarian (lung) cancer. For breast cancer, 77 designed compound-combinations passed the filters. As CDK4/6 does not belong to the breast-cancer module due to the limitation of disease modules used, we again only performed a pathway-level analysis. Nine percent of the combinations were predicted to jointly and complementarily cover estrogen receptor-signaling and cell cycle pathways as Fulvestrant and Palbociclib do. Also, 74% of the retained combinations jointly cover these pathways. These two portions suggest that many designed combinations were predicted to simultaneously target both pathways (with possible overlapping genes). If we consider PI3K signaling rather than cell cycle pathway for CDK4/6, 15.5% of retained drug combinations were predicted to jointly and complementarily cover estrogen and PI3K signaling pathways and all of them did jointly. 4.3.5 Ablation study for RL-based drug-combination generation Besides HVGAE for network and disease embedding, two of our novel contributions in RL-based drug set generations were network-principled reward and adversarial reward through GS-WGAN. To assess the effects of these contributions to our model, we performed ablation study for Stage 3 using the case of melanoma. We ablated the originally proposed model in two ways: removing the network-principled reward or replacing the GS-WGAN adversarial reward with the previously used GAN reward based on Jenson–Shannon divergence. Results in  Figure 3  suggested that both rewards led to faster initial growth and higher saturation values in network-based scores.
 Fig. 3. Ablation study for RL: best network scores achieved by three variants of the proposed method over training iterations 5 Conclusion In response to the need of accelerated and principled drug-combination design, we have recast the problem as graph-set generation in a chemically and net-biologically valid environment and developed the first deep generative model with novel adversarial award and drug-combination award in RL for the purpose. We have also designed hierarchical variation graph auto-encoders (HGVAE) to jointly embed domain knowledge such as gene–gene, disease–disease, gene–disease networks and learn disease representations to be conditioned on in the generative model for disease-specific drug combination. Our results indicate that HGVAE learns integrative gene and disease representations that are much more generalizable and informative than state-of-the-art graph unsupervised-learning methods. The results also indicate that the RL model learns to generate drug combinations following a network-based principle thanks to our adversarial and drug-combination rewards. Case studies involving four diseases indicate that drug combinations designed to follow network principles tend to have low toxicity from DDIs. These designs also encode systems pharmacology strategies echoing FDA-approved drug combinations as well as other potentially promising strategies. As the first generative model for disease-specific drug-combination design, our study allows for assessing and following network-based mechanistic hypotheses in efficiently searching the chemical combinatorial space and effectively designing drug combinations. Supplementary Material btaa317_Supplementary_Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Heavy-tailed prior distributions for sequence count data: removing the noise and preserving large differences</Title>
    <Doi>10.1093/bioinformatics/bty895</Doi>
    <Authors>Zhu Anqi, Ibrahim Joseph G, Love Michael I, Stegle Oliver</Authors>
    <Abstract/>
    <Body>1 Introduction RNA sequencing (RNA-seq) is a widely used assay for measuring the expression of transcripts from the genome. One common goal is to identify which genes are differentially expressed (DE) between experimental conditions, and to estimate the strength of the difference. The difference is usually defined in terms of the logarithmic fold change (LFC) between average expression levels of different conditions. The expression level of a gene in an RNA-Seq experiment is proportional across samples to a scaled count, representing the number of observed single- or paired-end reads that could be assigned to a given gene at a given library size. Scaling for the library size of the experiment is necessary, and other scaling factors can be included as well ( Leek, 2014 ;  Risso  et al. , 2014 ;  Soneson  et al. , 2015 ). Many variations on the standard RNA-seq protocol exist, as well as other sequencing-based assays such as chromatin immunoprecipitation followed by sequencing (ChIP-seq), and to the degree that these other experiments assess differences in scaled counts using estimated LFCs, the methods described here are generally applicable to these other assays as well. Many statistical methods have been developed for DE analysis of RNA-seq ( Anders and Huber, 2010 ;  Hardcastle and Kelly, 2010 ;  Law  et al. , 2014 ;  Leng  et al. , 2013 ;  Li and Tibshirani, 2013 ;  Love  et al. , 2014 ;  McCarthy  et al. , 2012 ;  Robinson  et al. , 2010 ;  Trapnell  et al. , 2012 ;  van de Wiel  et al. , 2013 ). Their common approach in detecting DE genes is to find sets of genes such that the null hypothesis of no difference in expression between conditions can be rejected, usually targeting the false discovery rate (FDR) for the set. However, a gene can be found significantly different, and the null rejected, even if the size of difference is very small ( Love  et al. , 2014 ). For further research interests, rather than only considering the order of genes according to adjusted or unadjusted  P -values, it is also of interest to order genes by the estimated effect size itself (the LFC). It is challenging to accurately estimate the LFCs for genes with low expression levels, or genes with a high coefficient of variation. Due to experimental costs and time, RNA-seq experiments designed for hypothesis generation typically have a small number of biological replicates ( n  of 3–5) for each condition group ( McCarthy  et al. , 2012 ). When the counts of sequenced reads are small or have a high coefficient of variation in one or a subset of the conditions, the estimated LFCs will have high variance, leading to some large estimated LFCs, which do not represent true large differences in expression. One approach that reduces the problem of these noisy LFC estimates is to filter out low count genes. Filtering also has the benefit of removing genes that may not have enough power for detecting DE, and so reducing the multiple test correction burden. Setting the threshold requires careful consideration of which genes may be lost. The authors of  edgeR  ( McCarthy  et al. , 2012 ) and  limma-voom  ( Law  et al. , 2014 ) suggest using a filtering rule that removes genes with low scaled counts before statistical analysis ( Chen  et al. , 2016 ). Other methods take a Bayesian modeling approach, including  ShrinkBayes  ( van de Wiel  et al. , 2013 ) and  DESeq2  ( Love  et al. , 2014 ).  DESeq2  applies an adaptive  Normally  distributed prior, to produce a shrinkage estimator for the LFC for each gene. However, in our analysis, we found that filtering or application of Normal priors each can have drawbacks, either leading to loss of genes with sufficient signal, or overly aggressive shrinkage of true, large LFCs. In this article, we present an empirical Bayes procedure that stabilizes the estimator of LFCs, without overly shrinking large LFCs, and uses the posterior distribution for point estimates and posterior probabilities, such as the aggregated  s -value ( Stephens, 2017 ) and the false-sign-or-smaller (FSOS) rate. We extend the basic framework of  DESeq2 , a Negative Binomial (NB) generalized linear model (GLM) ( McCullagh and Nelder, 1989 ) with moderated dispersion parameter, by exchanging the Normal distribution as a prior on LFCs with a heavy-tailed Cauchy distribution (a  t  distribution with 1 degree of freedom). We use various approximation techniques to provide Approximate Posterior Estimation for the GLM ( apeglm ). We compare  apeglm  to four existing methods on two benchmarking RNA-seq datasets. We demonstrate the advantages of  apeglm’ s shrunken estimates in reducing variance while preserving the true large effect sizes. We also show that  apeglm  shrunken estimates improve gene rankings by LFCs, relative to methods which do not apply Bayesian shrinkage on the LFCs.  apeglm  is available as an open-source R package on Bioconductor, and can be easily called from within the  DESeq2  software. 2 Materials and methods 2.1 NB model for RNA-seq counts We start with summarized measures of gene expression for the experiment, represented by a matrix of read or fragment counts. The rows of the matrix represents genes,  ( g = 1 , … , G ) , and columns represent samples,  ( i = 1 , … , m ) . Let  Y gi  denote the count of RNA-seq fragments assigned to gene  g  in sample  i . We assume that  Y gi  follows a NB distribution with mean  μ gi  and dispersion  α g , such that  Var ( Y g i ) = μ g i + α g μ g i 2 . The mean  μ gi  is a product of a scaling factor  s gi  and a quantity  q gi  that is proportional to the expression level of the gene  g . We follow the methods of  Love  et al.  (2014)  to estimate  α g  and  s gi  sharing information across  G  genes, and consider estimates as fixed for the following. We fit a GLM to the count  Y gi  for gene  g  and sample  i ,
 (1) Y g i ∼ NB ( μ g i , α g ) μ g i = s g i q g i   log   q g i = X i , * β g 
where  X  is the standard design matrix and  β g  is the vector of regression coefficients specific to gene  g . Usually  X  has one intercept column, and columns for covariates, e.g. indicators of the experimental conditions other than the reference condition, continuous covariates, or interaction terms. We consider design matrices where the first element of  β g  is the intercept. For clarity, we partition the  β g  into  β g = ( β g 0 , β g 1 , … , β g K ) , where  β g 0  is the intercept and  β gk ,  k = 1 , … , K  is for  k th covariate. The scaling factor  s gi  accounts for the differences in library sizes, gene length ( Soneson  et al. , 2015 ) or sample-specific experimental biases ( Patro  et al. , 2017 ) between samples, and is used as an offset in our model. In the GLM, we use the logarithmic link function. In the  apeglm  software, the estimated coefficients and corresponding SD estimates are reported on the same  natural log    scale. The  apeglm  method can be easily called from  DESeq2 ’s  lfcShrink  function, which provides LFC estimates on the    log   2  scale. The  apeglm  method and software is generic for GLMs and can be used with other likelihoods. For example, it can be used for the Beta Binomial or zero-inflated NB model, as long as estimates for the additional parameters, e.g. dispersion or the zero component parameters, are provided. An example of  apeglm  applied to Beta Binomial counts, as could be used to detect differential allele-specific expression, is provided in the software package vignette. 2.2 Adaptive shrinkage estimator for  β gk We shrink coefficients representing differences between groups, continuous covariates, or interaction terms, but not the intercept. We propose a Cauchy distribution as the prior for the coefficients that the user wants to shrink. Therefore  β gk  in the model (1) has the prior
 (2) β g k ∼ Cauchy ( 0 , S k ) 
where the first parameter of the Cauchy gives the location and the second parameter is the scale,  S k . For simplicity, as  apeglm  shrinks only a single coefficient at a time, we will write  S  in place  S k . A similar default prior for coefficients associated with non-intercept covariates has been proposed by  Gelman  et al.  (2008)  in the  bayesglm  R package, which uses a zero-centered Cauchy distribution with a scale of 2.5. The proposed prior distribution assumes that the distribution of LFCs across genes is unimodal and symmetric. We assessed robustness to violations of this assumption and found  apeglm  still performed well (detailed in a later section). However, if most of the genes are differentially expressed in the same direction, the global normalization method used by all methods discussed here would break down, thus affecting any effect size estimation. This situation can be detected by histogram, MA-plot or more rigorously with  quantro  ( Hicks and Irizarry, 2015 ). For setting the scale of the prior  S , we use the maximum likelihood estimates (MLEs)  β ^ g k  and their standard errors  e gk . When making use of the set of MLEs for a coefficient, we shrink only a single coefficient at a time, and adapt the scale of the prior to the MLE by solving the following equation for  S 2 = A  ( Efron and Morris, 1975 ).
 (3) A = ∑ g = 1 G ( β ^ g k 2 − e g k 2 ) I g ( A ) / ∑ g = 1 G I g ( A ) (4) I g ( a ) ≡ 1 / [ 2 ( a + e g k 2 ) 2 ] This equation is motivated by assuming that the MLE  β ^ g k  follows a Normal distribution around the true value  β gk  with variance  e g k 2 , and that the  β gk  themselves follow a Normal distribution with mean zero.  A  is an empirical Bayes estimate of the variance of the  generating  Normal distribution, and  S = A  gives the scale. Although we use a Cauchy prior in  apeglm  in order to accommodate large effect sizes from potentially only a few genes, we found that setting the scale of the prior by assuming the  β gk  follow a zero-centered Normal distribution performed well in practice. The equations above for estimating  A  are given by  Efron and Morris (1975) , as a generalization of empirical Bayes estimators for the situation of many parameters each distributed with unequal variances.  Equation (3)  is solved for  A  using Brent’s line search implemented in R ( Brent, 1972 ). Although the method above for estimating  A  proposed by  Efron and Morris (1975)  requires that the  e gk  be known, here we have instead substituted an estimated quantity, the standard errors. We assessed the practical consequence of this substitution when the standard errors are unstable because the counts were very low. We found that the resulting estimate of  A  is only slightly biased, even when counts are very low ( Supplementary Fig. S1 ). If the MLEs of the coefficients are not supplied, we use a scale  S  = 1 for all non-intercept coefficients. The unscaled posterior for  β gk  is the product of the prior density and the NB likelihood. We use the posterior mode, or  maximum a posteriori  (MAP), as the shrinkage estimator for the coefficient. The posterior mode is found using the L-BFGS algorithm ( Nocedal, 1980 ) implemented in C++ using the  RcppNumerical  and  L-BFGS++  libraries. Running  apeglm  is efficient: for the simulation dataset modeled on the  Pickrell  data (10 000 genes and 5 versus 5 samples), running  DESeq2  to obtain dispersion estimates and MLE coefficients takes 4.7 s on a laptop with one core, running LFC shrinkage with the  DESeq2  Normal prior takes 2.9 s, and running LFC shrinkage with the  apeglm  Cauchy prior takes 4.1 s. Running  apeglm  to only produce the MAP estimates (without calculating the posterior SD) takes 0.5 s. We derive the posterior distribution for  β gk  using the Laplace approximation: we estimate the covariance of the posterior distribution as the negative inverse of the Hessian matrix obtained from numeric optimization of the log posterior. We also attempted an alternate method for approximating the posterior by integrating the un-normalized posterior over a fine grid, but we found the Laplace approximation was consistently more accurate. Using the approximate posterior, we compute local false sign rate (FSR) and credible intervals. Following  Stephens (2017) , the local FSR is defined as the posterior probability that the posterior mode (MAP) is of the false sign, that is for gene  g ,
 lfsr g = { p ( β g k &lt; 0 )   MAP   of   β g k ≥ 0 p ( β g k ≥ 0 )   MAP   of   β g k &lt; 0. We also provide the local FSOS rate, relative to a given  θ &gt; 0  representing a biologically significant effect size,
 lfsosr g θ = { p ( β g k &lt; θ )   MAP   of   β g k ≥ 0 p ( β g k &gt; − θ )   MAP   of   β g k &lt; 0. Analogous to the  q -value ( Storey, 2003 ), the  s -value ( Stephens, 2017 ) provides a statistic for thresholding, in order to produce a gene list satisfying a certain bound in expectation. The  s -value can be computed as
 s g = 1 | Γ | ∑ j ∈ Γ lfsr j ,   Γ = { j : lfsr j ≤ lfsr g } , 
and likewise for the local FSOS rate. Other methods that have suggested using the cumulative average or the cumulative maximum of posterior probabilities for defining the set of interesting features in high-throughput experiments include  Choi  et al.  (2008) ,  Kall  et al.  (2008)  and  Leng  et al.  (2013 ). 3 Results 3.1 Strong filtering thresholds may result in loss of DE genes It is difficult to accurately estimate the LFCs for genes with low read count; MLEs of LFCs for genes with low read count have high variance due to the dominance of sampling variance over any detectable biological differences. The MLEs of LFCs for these genes may not reflect the true biological difference of gene expression between conditions, and thus are not reliable for plotting or ranking genes by effect size ( Love  et al. , 2014 ).  Chen  et al.  (2016)  suggested to remove from analysis the genes that have low scaled counts across samples. They define a scaled quantity, the  counts per million  (CPM), which is the counts  Y gi  divided by a robust estimator for the library size, multiplied by one million. The filtering rule is to keep only those genes with  n  or more samples with CPM greater than the CPM value for a raw count of 10 for the least sequenced sample. The suggested value for  n  is the sample size of the smallest group. CPM filtering occurs prior to any statistical analysis. Other data-independent thresholds, such as requiring a CPM of 0.5 or 1 from  n  or more samples can be even more aggressive at removing genes with potential signal when the sequencing depth is high. We illustrate how filtering can lead to loss of DE genes using the dataset by  Bottomly  et al.  (2011) , which contains 10 and 11 samples of RNA-seq data for mouses from two strains, C57BL/6J(B6) and DBA/2J(D2), respectively. We repeatedly randomly picked three samples from each strain, balancing across the three experimental batches. We then applied a CPM filtering rule to each random subset, repeating the process 100 times. For all genes in the full dataset, we used  DESeq2  ( Love  et al. , 2014 ) to test for DE across strains controlling for batch, defining a set of genes with a nominal FDR threshold of 5%.  Supplementary Figure S2  shows four example genes that were removed &gt;50% of the time across random subsets, but were reported as differentially expressed by  DESeq2  on the full dataset. There were 207 such genes, which are shown in  Supplementary Figure S3 . These genes did have information to contribute: for example, they had on average the same sign of estimated LFCs 99% of the time when comparing to the LFCs from the full dataset. These genes, despite having low gene expression, may still be biologically relevant, so we considered statistical methods that produce LFC estimates with low variance for relatively low count genes as well. To be clear, we do not argue against  any filtering , only against strong filtering for the purposes of obtaining precise LFCs which may discard genes with a relevant signal. Besides filtering, an additional approach to produce precise effect sizes is to use scaled pseudocounts, or  prior counts , to obtain shrinkage estimates of LFCs. The prior count approach is employed by  edgeR  and  limma-voom . However, setting a prior count does not make use of the statistical information contained in the data for estimating the LFCs, such that the optimal prior count needs to be adapted per dataset. For example, as the sample size increases, the optimal prior count should go to zero, and so a fixed prior count may be sub-optimal. Furthermore, the prior count approach, while helping with high LFC variance from genes with low counts, helps less for high variance genes. Finally, we note that the prior count approach does not provide a posterior distribution for effect sizes, which may be useful for certain analyses discussed below. 3.2 Overview of the  apeglm  method Following the basic framework of GLMs, we propose an adaptive Bayesian shrinkage estimator ( Fig. 1 ). We employed a heavy-tailed prior distribution on the effect sizes, where the shape of the prior distribution is fixed, and the scale is adapted to the distribution of observed MLE of effect size for all genes (see Section 2). For each gene, the method uses a Laplace approximation to provide the mode of the posterior distribution as a shrinkage estimate, the posterior SD as a measure of uncertainty, and posterior probabilities of interest described below. Our method obviates the need for filtering rules or prior counts, and takes advantage of the statistical information in the data for estimating the effect size. The method is general for various likelihoods, but here we apply it to RNA-seq using a NB GLM, where the effect size is a particular LFC (log fold change between groups or an interaction term in a complex design). For genes that have low counts or high variance, this method shrinks the LFCs towards zero thus alleviating the problem of unreliably large LFC estimates.
 Fig. 1. An overview of the method.  apeglm  takes the MLE estimates and corresponding standard errors of a GLM model as input. In  apeglm  we provide a heavy-tailed prior distribution on the coefficients, and compute the shrinkage estimators and corresponding SDs. Users can also define a likelihood function that describes the data and feed to  apeglm .  apeglm  also provides the local FSRs and  s -values ( Stephens, 2017 ) as part of the output The local FSR ( Stephens, 2017 ) is defined as the posterior probability for a gene that the sign of the estimated effect size is wrong. Similar to the FSR, we also make use of a local  FSOS  rate: the posterior probability of having mis-estimated the sign of an effect size,  or the effect size being smaller than a pre-specified value . For the FSR and FSOS rates,  apeglm  provides an aggregate quantity, the  s -value proposed by  Stephens (2017) , which can be used for generating lists of genes. The  s -value for a gene is defined as the average of local FSR over the set of genes that have smaller local FSR than this one (likewise for FSOS, see Section 2). 3.3 An adaptive prior controls the FSR We performed an initial assessment of our approach on simulated data, to confirm that the adaptive prior would control the aggregate FSR, when thresholding on  s -values, for datasets with varying spread of true LFCs. Using a  fixed , non-adaptive prior scale leads to loss of control of FSR when the true LFCs were drawn from a Normal distribution with small variance; matching the scale of the prior to the scale of the true distribution of LFCs regained control of FSR ( Supplementary Fig. S4 ). Although a prior  smaller  in scale than the true distribution of LFCs also controlled the FSR, it lead to an increase in the relative error of point estimates ( Supplementary Figs S5 and S6 ). Therefore we chose to set the scale of the prior to the estimated scale of the true LFCs using the MLEs and their standard error (Section 2). 3.4 Evaluation on highly replicated yeast dataset To investigate the precision of various estimates of LFCs, we used a highly replicated RNA-seq dataset designed for benchmarking ( Schurch  et al. , 2016 ). This dataset consists of RNA-seq data of  Saccharomyces cerevisiae  from two experimental conditions: 42 replicates in  wild-type  and 44 replicates in a  Δ snf2 mutant. We randomly picked three samples from each experimental condition to form a test dataset, and applied differential gene expression methods to estimate the LFCs. We compared the LFCs estimates against the    log   2  ratio of mean scaled counts in the full dataset, which was taken as ‘gold standard’ LFCs. We repeated the random sampling 100 times. We also performed this same experiment using a sample size of 5 versus 5. For this evaluation and all others, we minimized the influence of genes with no signal for estimating the LFCs, by only evaluating the methods over genes with an average of more than one scaled count per sample. This minimal filtering does not advantage  apeglm . We compared the performance of  apeglm  with four other methods for estimation of effect size in RNA-seq,  DESeq2 ,  edgeR ,  limma-voom , as well as  ashr  ( Stephens, 2017 ). In comparing to  DESeq2 , we compared  apeglm  to the LFC shrinkage estimator produced using a Normal prior, as described in  Love  et al.  (2014) .  ashr  provides generic methods for adaptive shrinkage estimation, taking as input a vector of estimated  β g  i.e.  β ^ g , and the corresponding estimates of standard errors. For the Bayesian shrinkage estimation methods that we compared, the unimodal assumption of the true LFCs was checked for all the examples we considered below, and the assumption was met in all the examples ( Supplementary Fig. S7 ). We also found that the Bayesian methods were robust to some degree of violation of the unimodal assumption, discussed in a later section. For  ashr , we input  β ^ g  and corresponding standard error using the MLE from  DESeq2  (‘ ashr DESeq2  input’), and the estimated coefficient from  limma-voom , plus a standard error calculated using the moderated variance estimate (‘ ashr limma  input’). We also included  edgeR  with a prior count of 5, which helps to moderate the variance of the estimated LFCs from genes with low counts, ( edgeR-PC5 ). Stratifying genes by the absolute value of true LFCs allows us to see where the different methods excel and fall short systematically, across 100 iterations of sub-sampling ( Fig. 2a and c ).  limma  and  edgeR  had the lowest mean absolute error (MAE) for both sample sizes.  DESeq2  had the highest error for the largest bin of true LFCs, meaning that  DESeq2 ’s Normal prior could not accommodate the top 0.5% of effect sizes for this dataset and resulted in too much shrinkage. The other shrinkage estimators  apeglm  and  ashr  (with either input) maintained a middle range of MAE.  edgeR-PC5  had low error for the small true LFCs, but then increased to higher error for the largest bin of true LFCs, especially when the sample size increased to 5 versus 5, where the bias approaches that of  DESeq2 .
 Fig. 2. 
 (a)  MAE of estimates for 3 versus 3 samples, defined as the mean of the absolute value of the differences between the estimated and reference LFCs, stratified by absolute value of reference LFCs. The mean of MAE over 100 iterations is plotted for each method. The x-axis label gives the upper bound of the bin on absolute value of LFCs.  (b)  Concordance At the Top (CAT) plot ( Irizarry  et al. , 2005 ) comparing ranked gene lists from each method against the reference ranked gene list for 3 versus 3 samples. Number of top genes ranked by the absolute value of the LFCs is on the x-axis, and the proportion of concordance between the two rankings is on the y-axis. For example, if the ranked gene list from  apeglm  estimated and reference LFCs share 85 of top 100 genes, then the  apeglm  point would fall at (100, 0.85).  (c)  MAE plot of estimates for 5 versus 5 samples.  (d)  CAT plot for 5 versus 5 samples Ranking genes by estimated LFCs can assist with further investigation into the genes most affected in their expression by changes in condition. We compared the concordance of the top ranked genes by absolute LFC estimates ( Fig. 2b and d ). We examined, for the top  w  genes ranked by absolute value of estimated LFCs, the proportion which were among the top  w  genes by absolute value of reference LFCs ( w ∈ { 100 , 150 , 200 , … , 400 } ). Although  limma  and  edgeR  had lowest MAE when binning by quantile of the true LFC, they meanwhile had the lowest concordance when ranking genes by LFC, while the shrinkage estimators tended to perform better (always the case past  w  = 250 genes).  apeglm ,  ashr  (with either input), and  edgeR-PC5  had the highest concordance of top ranked genes by absolute LFC overall, for 3 versus 3.  apeglm  and  ashr  (with either input) had the highest concordance for the 5 versus 5 sub-sampling experiment.  DESeq2  had relatively low concordance among the shrinkage estimators for the smallest  w , due to over-shrinkage of the very largest effect size genes. In one iteration of random sampling, much of the behavior that was seen systematically over all iterations can be observed ( Supplementary Fig. S8 ).  apeglm ,  ashr  with  DESeq2  or  limma  input, and  edgeR-PC5  did well in estimating LFCs, with LFC estimates close to reference LFCs for most of the genes.  DESeq2  and  edgeR-PC5  had similar performance to  apeglm , but were too aggressive in shrinking LFC for genes with large reference LFCs, for example    log   2  fold change near 4. We show the counts of such example genes in  Supplementary Figure S9 , where  DESeq2  or  edgeR-PC5  give small LFC estimates to genes with large reference LFC, while the  apeglm  method allows for large estimated LFCs.  edgeR  and  limma  returned large estimated LFCs for some genes with reference LFCs around zero, which is problematic for ranking genes by effect size without first applying some form of count filtering. In summary, considering both aggregate error ( Fig. 2a and c ) and concordance in ranking of genes by effect size ( Fig. 2b and d ),  apeglm ,  ashr  and  edgeR-PC5  were top performers for the 3 versus 3 experiment, and  apeglm  and  ashr  were top performers for the 5 versus 5 experiment.  limma  and  edgeR  were low performers for ranking genes by effect size, and  DESeq2  and  edgeR-PC5  had high error for the top effect size genes ( DESeq2  for both experiments, while  edgeR-PC5  only for the 5 versus 5 experiment). We therefore conclude that shrinkage estimation is useful for ranking genes by effect size, and does not necessarily come at the expense of much bias, depending on the design of the shrinkage method. Among the methods using shrinkage estimation, an advantage of  apeglm  is that it preserves true, large differences across conditions in the estimated LFCs. To demonstrate this, we calculated the average estimated LFCs for the methods that perform shrinkage ( apeglm ,  DESeq2 ,  ashr  and  edgeR-PC5 ), averaging over the 100 iterations. Comparing the average estimated LFCs to the reference LFCs demonstrates the extent of  bias  of the estimators, where it is expected that shrinkage estimators would have bias toward zero. We then constructed an MA plot, as typically used to visualize DE gene expression results, but where overshrinkage across many iterations i.e. biased estimation, is highlighted ( Supplementary Fig. S10 ). All of the methods exhibited shrinkage of LFCs more than 0.5 for many genes with mean scaled counts &lt;10, but  apeglm  preserved the most large LFCs for genes with larger mean scaled counts.  DESeq2  and  ashr  with  limma  input tended to shrink the LFCs by &gt;0.5 for genes with mean expression levels &gt;10, including genes with absolute value of reference LFCs &gt;2, thus representing large differences across condition. 3.5 Rank comparison with  P -values Many RNA-seq workflows use adjusted  P -values from a statistical test of the null hypothesis of no difference in expression in order to rank the genes by importance.  limma  by default ranks by log odds of DE. However, adjusted  P -values or log odds do not capture the magnitude of LFCs, unless the standard null of  LFC = 0  is replaced by a threshold test ( Love  et al. , 2014 ;  McCarthy and Smyth, 2009 ), wherein a positive threshold of biological importance is specified by the analyst. Using Bayesian methods, we can directly rank genes by their effect size, as unreliable LFCs from genes with low counts or high variability are moderated toward zero. We assessed whether our ranking by  apeglm  effect size provided substantially different output than a typical ranking of genes by  P -values or Independent Hypothesis Weighting ( IHW ) adjusted  P -values ( Ignatiadis  et al. , 2016 ). We compared against the ranking of genes by the reference LFC—while making an important caveat that  P -values are not designed to provide reliably rankings by effect size ( Fig. 3 ). Comparing the percentage of concordance at top with the rank by reference LFCs, the ranking from  apeglm  estimated LFCs had over 80% concordance, while the ranking from  DESeq2 P -values and  IHW  adjusted  P -values had about 60% concordance. Furthermore, some of the genes with low rank (top of the gene list) from  IHW  adjusted  P -values had high rank by  apeglm , potentially indicating that the effect size was significantly different than zero but nevertheless small. This comparison revealed that  apeglm  does in fact give substantially different output in terms of gene lists, and the previous analysis reveals that the  apeglm  output is accurate on a highly replicated RNA-seq dataset.
 Fig. 3. 
 (a)  CAT plot comparing ranked gene lists from  apeglm  estimated LFCs,  DESeq2  p values and  IHW  adjusted  P  values for 3 versus 3 samples.  (b)  CAT plot comparing ranked gene lists from  apeglm  estimated LFCs,  DESeq2 P  values and  IHW  adjusted  P  values for 5 versus 5 samples.  (c)  Rank plot comparing the ranks of genes from  apeglm  estimated LFCs and  IHW  adjusted  P  values for 3 versus 3 samples.  (d)  Rank plot comparing the ranks of genes from  apeglm  estimated LFCs and  IHW  adjusted p values for 5 versus 5 samples 3.6 Evaluation on simulation modeled on experimental data We also checked whether  apeglm  provides accurate estimates of LFCs in simulated data modeled on experimental datasets. We generated the ‘true’ LFCs from a mixture of zero-centered Normal distributions. The mean counts and NB dispersion estimates were drawn from the joint distribution of the estimated parameters over the  Bottomly  et al.  (2011)  and  Pickrell  et al.  (2010 ) datasets, as was performed in  Love  et al.  (2014) . We simulated 10 000 genes with a sample size of 5 versus 5, and repeated the whole simulation 10 times per experimental dataset. We also doubled the sample size to 10 versus 10 to see if the methods provided consistent relative performance at higher sample size. For the  Pickrell  dataset, which has higher within-group variation, we used a mixture of Normal distributions with SDs of 1, 2, 3 (with mixing proportions 90, 5 and 5%, respectively). The  Bottomly  dataset has lower within-group variation, and so to make the simulation equally challenging, we used SDs of 0.25, 0.5 and 1 (90, 5 and 5%). We constructed the simulation such that the expected count for all simulated samples was always &gt;10, to avoid overemphasizing the smallest count genes (this simulation choice does not advantage  apeglm ). The simulation results for the  Pickrell  dataset ( Fig. 4 ) and the  Bottomly  dataset ( Supplementary Fig. S11 ) were mostly consistent with the previous result on the highly replicated yeast dataset.  limma ,  edgeR ,  edgeR-PC5  and  apeglm  tended to have the lowest error when stratifying by true LFCs, although  limma  and  edgeR  had the lowest concordance when ranking genes by LFCs. The methods which do not shrink tended to produce large estimates for genes where the true LFCs are near 0 ( Supplementary Figs S12 and S13 ). With one iteration of random sampling, we showed two genes that had true LFCs near zero for  Pickrell  dataset, for which  edgeR  and  limma  greatly overestimated the LFCs, but  apeglm  provided LFCs near 0 ( Supplementary Fig. S14 ). As in the yeast dataset, as the sample size increased,  apeglm  had lower error compared with  edgeR-PC5  for the largest LFCs.  DESeq2  had the highest error for the largest LFCs, as was expected. Unlike the results from the highly replicated yeast dataset, here  ashr  with both inputs had higher error for the middle range of LFCs. With one iteration of random sampling, we showed two genes with middle range of LFCs for the  Bottomly  dataset, for which  ashr  estimated LFCs were much smaller than the true LFCs, while  apeglm  gave more accurate, large LFCs ( Supplementary Fig. S15 ). We note that we simulated  NB  counts, and so the methods  apeglm ,  DESeq2  and  edgeR  which assume the NB likelihood, are potentially at an advantage.  apeglm  was a top performer in the simulation for small and larger sample sizes, having consistently low error and also high concordance.
 Fig. 4. Simulation dataset (top row, 5 versus 5, and bottom row, 10 versus 10) modeled on estimated parameters from the  Pickrell  et al.  (2010)  dataset. Each point represents the average over 10 repeated simulations The shrinkage estimators  apeglm ,  DESeq2  and  ashr  tended to have low MAE across the range of counts ( Supplementary Fig. S16 ).  limma  and  edgeR  had high MAE for low counts, as expected. The MAE for  edgeR-PC5  when binning genes by counts was low for the sample size of 5 versus 5, but higher when the sample size was increased to 10 versus 10. Finally, we considered whether the methods which produce  s -values ( ashr  and  apeglm ) were able to achieve their FSR bounds. We also generated  s -values for  DESeq2  using the  DESeq2  posterior mode estimate and the associated uncertainty. We generated plots using the  iCOBRA  package ( Soneson and Robinson, 2016 ), showing the number of genes at various achieved FSR values ( Supplementary Fig. S17 ). This analysis indicated that  apeglm  and  ashr  with  DESeq2  input tended to hit the target of 1% and 5% FSR, while  DESeq2  and  ashr  with  limma  input were just slightly above their nominal FSR. The  iCOBRA  data objects for four iterations of the simulation can be accessed at  https://github.com/mikelove/apeglmPaper , and explored interactively using the  iCOBRA  Shiny app. 3.7 Evaluation of robustness, extensibility and consistency To examine the robustness of  apeglm  and other Bayesian shrinkage methods to violations of the unimodal assumption, we modified the simulation such that the true LFCs no longer are generated from a unimodal distribution, but instead a mixture of a zero-centered and a non-zero-centered distribution. We first assessed how the addition of a non-zero-centered component affected the estimation of the scale of the prior in  apeglm . We found that the estimated scale of the prior tracked with the variance of the mixture of distribution and not with the zero-centered component alone, as was expected ( Supplementary Fig. S18 ). To assess performance of  apeglm  relative to other methods in LFC estimation, we simulated a mixture of  N ( 0 , 0.5 )  and  N (3, 2) for the  Pickrell  dataset and a mixture of  N ( 0 , 0.125 )  and  N ( 0.75 , 0.5 )  for the  Bottomly  dataset, thus producing a bimodal distribution when the non-zero component was included at a high enough proportion. We considered the proportion of genes coming from the non-zero-centered component to be in the range {5, 10 and 20%}.  apeglm  was the top performing method, taking into account accuracy and concordance of ranking genes by effect size for the 5% case ( Supplementary Figs S19 and S20  for  Pickrell  and  Bottomly  datasets, respectively). The differences were more moderate for a 10% non-zero component for the  Pickrell  dataset ( Supplementary Fig. S21 ), where  apeglm  performed similarly to  limma  and  edgeR  which had decent concordance at ranking genes. However,  apeglm  outperformed those two methods in terms of ranking genes on the  Bottomly  dataset with 10% of genes from the non-zero component ( Supplementary Fig. S22 ). Finally, the differences were more moderate at the extreme of 20% of genes coming from a non-zero, large positive LFC component ( Supplementary Figs S23 and S24 ). Overall, we showed that  apeglm  still performed well with violation of the unimodal distribution assumption, with mean error close to  limma  and  edgeR  while having high concordance in ranking genes by effect size. 
 apeglm  was developed in a general manner such that it can be extended to generic likelihoods, in addition to the NB likelihood that has been used so far. We considered using  apeglm  with Zero Inflated NB (ZINB) generated data and likelihood. We used the  splatter  Bioconductor package ( Zappia  et al. , 2017 ) to simulate datasets with additional zeros beyond what would be expected with a NB distribution. We simulated 100 cells in the following partitions: (20, 20, 60), (30, 30, 40) or (50, 50). We focused the evaluation on the comparison of the first two groups which have the same sample size. We considered the proportions of genes that are differentially expressed across all three groups in the range 1, 5 or 10%. The estimation of the zero component was performed upstream of  apeglm  using the methods described in  Van den Berge  et al.  (2018)  and the  zinbwave  Bioconductor package ( Risso  et al. , 2018 ), and was either used to define zero weights or as input to a ZINB likelihood. The zero weights can be used to isolate the contribution to the counts from the NB component; and therefore, potentially remove bias due to ‘technical zeros’. We compared performance of the following approaches: the simple ratio of average scaled count after adding a pseudocount of 0.1, a ratio of weighted average scaled counts after adding a pseudocount of 0.1, the MLE from  DESeq2  taking into account the zero weights, usage of the Normal prior in  DESeq2  with a weighted NB likelihood,  apeglm  with weighted NB likelihood, and  apeglm  with a ZINB likelihood. We assessed the Pearson correlation of estimates to truth, the MAE for the top 30 genes as ranked by the method, and the MAE for all genes (Supplementary Figs S25–S33).  apeglm  with both weighted NB likelihood function and ZINB likelihood function consistently had the smallest MAE (whether total or for the top 30 genes ranked by the method) across all the combinations of differentially expressed proportions and sample sizes. The two variations were equivalent, while the weighted NB likelihood approach was much faster, taking advantage of optimized C++ code for the NB likelihood in  apeglm . The two  apeglm  variations outperformed the use of a Normal prior in  DESeq2  in terms of MAE of the top 30 genes, when the percent of DE genes was 1 or 5%. The improvement in MAE from using Bayesian shrinkage was greatest for sample size 20 versus 20, moderate for 30 versus 30, and  apeglm  was comparable to weighted pseudocount and MLE approaches at 50 versus 50. We finally assessed the consistency of the  apeglm  estimator, by considering bulk RNA-seq simulated datasets with large sample sizes (30 versus 30 and 50 versus 50). Here we expect that the relative advantage of Bayesian shrinkage for ranking genes will be reduced, as the posterior estimators converge to the MLE. We again produced simulated bulk RNA-seq data modeled on the  Pickrell  dataset ( Fig. 5 ) and the  Bottomly  dataset ( Supplementary Fig. S34 ). Across all methods, the MAE becomes much lower, and the concordance of gene ranks much higher. As seen previously,  apeglm  is one of the top performing methods, although the advantage over non-shrinkage based methods,  limma  and  edgeR  is reduced. This convergence is expected, and the large sample size analysis is mostly useful for assessment that the  apeglm  estimator is consistent—that it converges to the true, simulated value as the sample size increases.  edgeR-PC5  performs worst in these large sample size cases than previously, as the large prior count is no longer necessary to stabilize the non-shrinkage estimators.  Supplementary Figure S35  provides the MAE over the mean of counts for the large sample size simulations.
 Fig. 5. MAE plot over LFCs (left) and CAT plots (right) of simulation dataset (top row, 30 versus 30 and bottom row, 50 versus 50) modeled on estimated parameters from the  Pickrell  et al.  (2010)  dataset. Each point represents the average over 10 repeated simulations The simulations in this article allowed an exploration of performance of various LFC estimators across sample size and for the case of relatively large dispersion values ( Pickrell  dataset) and relatively small dispersion values ( Bottomly  dataset). Across all simulated datasets,  apeglm  was the top performing method in balancing low MAE with high concordance when ranking genes by their true, simulated effect size. By process of elimination,  DESeq2  and  ashr  in some cases had high error for medium to large effect sizes,  limma  and  edgeR  had low concordance in ranking genes, and  edgeR-PC5  had high error for the large sample size cases, while  apeglm  demonstrated reliable estimation of LFCs throughout. 3.8 Evaluation on cell line mixture experiment We additionally evaluated the relative performance of  apeglm  using a cell line mixture RNA-seq dataset designed for benchmarking ( Holik  et al. , 2017 ). In this study, the investigators chose two cell lines from the same type of lung cancer, and grew the cell lines (NCI-H1975 and HCC827) as three biological replicates, then mixed the RNA concentrations from each of these replicates at five pre-specified proportions (100:0, 75:25, 50:50, 25:75, 0:100%). Following the notation of their paper, we use 100, 075, 050, 025 and 000 to represent the proportions. We used for evaluation the 15 normally processed samples prepared with Illumina’s TruSeq poly-A mRNA kit. We compared two groups of mixtures, each with three independent replicates: 075 versus 025 and 050 versus 025. We found the 100 versus 000 and 000 versus 100 mixtures were highly influenced by the 100 and 000 samples, which would be used both for estimation and for evaluation. We computed the estimation error as in  Holik  et al.  (2017)  as the difference between the LFCs estimated by each method using two groups of samples and the LFCs predicted by a non-linear model fit to all 15 samples, using the fitmixture function in the  limma  package. The distribution of true LFCs for the 075 versus 025 and 050 versus 025 are bounded by  [   log   2 ( 1 / 3 ) ,   log   2 ( 3 ) ]  and  [   log   2 ( 2 / 3 ) ,   log   2 ( 2 ) ] , respectively, and so instead of considering the top ranked genes, we considered two plots to assess the accuracy of LFC estimation: once binning by true LFCs and once binning by estimated LFCs ( Fig. 6 ).  ashr  with  DESeq2  input and  apeglm  had higher MAE when binning by true LFCs, but had the lowest MAE when binning by estimated LFCs, which reveals that shrinkage did induce a bias, but protected against outputting large and unreliable LFCs.  limma  and  edgeR  had the opposite performance: low MAE when binning by true LFCs, but high MAE when binning by estimated LFCs.  DESeq2  and  ashr  with  limma  input had mixed performance. In this experiment,  edgeR-PC5  tended to have consistently low MAE, though we note that the sample size for the cell line mixture experiment was three per group, and we found that the relative bias of the prior count approach increased with sample size ( Fig. 5 ).
 Fig. 6. ( a ) The distribution of the true LFCs for comparison 050 versus 025, where the true LFCs is predicted with the fitted non-linear model. ( b ) Scatter plot of estimated LFCs from  apeglm  over true LFCs for comparison 050 versus 025. The vertical and horizontal lines indicate the two type of bins that were used for stratifying estimation error.  (c  and  d)  MAE plot binned by true LFCs and by estimated LFCs for comparison 075 versus 025 ( e  and  f ) MAE plot binned by true LFCs and by estimated LFCs for comparison 050 versus 025 4 Discussion Here we compared various shrinkage estimators for LFCs in DE analysis of RNA-seq counts. RNA-seq experiments often have limited number of biological replicates in each condition group, typically in the range of 3–5. It is particularly difficult to estimate LFCs for genes with low counts or high coefficient of variation with such a small number of replicates. We examined methods for mitigating this problem of LFC estimation, and find that common filtering rules may lead to loss of DE genes. On the other hand, we found that existing methods for shrinking LFC estimates, such as  DESeq2 , may overly shrink those genes with very large LFCs, although the ranking was not greatly impaired. To reduce the shrinkage of large effect sizes that occurred using a  Normal  prior, we substituted an adaptive  Cauchy  prior, which has sufficient probability density in the tails of the distribution to allow for very large effects. The resulting estimator both reduced the variance associated with LFC estimates across the range from low to high counts, and also preserved true large LFCs. We have shown the utility in an adaptive, heavy-tailed prior for high-throughput experiments in which an effect size is estimated over tens of thousands of features. The results presented here have focused on the task of estimating the LFCs in RNA-seq experiments, using an NB likelihood, but the software and methods are written in a general way, and in general, the use of the adaptive Cauchy prior may be adapted to other likelihoods and settings. The  apeglm  method accepts arbitrary likelihoods, as long as additional parameters are pre-specified, such as the dispersion.  apeglm  can therefore also be extended for use on other types of data, as long as it can be modeled by a GLM. For example, our method can be applied to allele-specific expression count data using a beta-binomial likelihood, as shown in the  apeglm  package vignette. Providing low variance posterior mode effect sizes and their posterior SD allows for various downstream uses, for example, plotting LFC estimates from two experiments against each other in a scatter plot, without having to make arbitrary filtering decisions that would have to apply to both datasets. In another context, the effect sizes of genetic variants across many different traits can be systematically correlated to one another to suggest potential relationships between the traits ( Pickrell  et al. , 2016 ). Such an analysis could benefit from shrunken estimates of effect size, to avoid hard filtering rules and to not have the correlations overly influenced by imprecise estimates. The computation of the approximate posterior provides useful aggregate statistics, such as the FSR and  s -value proposed by  Stephens (2017) , and the FSOS rate, which allows the user to define a range of effect sizes of biological significance. We note that, while the use of specific prior counts works well for providing point estimates of effect size for certain sample sizes and mean-variance relationships, it is difficult to choose a value that will work well for all datasets. For example, if one considers unique molecular identifiers ( Kivioja  et al. , 2012 ) and the counts produced following de-duplication in such an experiment, the information content of a low count can be much higher than in standard RNA-seq experiments without de-duplication, and so filtering rules and prior counts would need to be re-considered and manually adjusted for such a dataset. A Bayesian procedure for shrinkage of effect sizes, which takes statistical information into account, is desirable across different types of high-throughput datasets. 5 Availability 
 apeglm  is implemented as an R package and is available as part of the Bioconductor project ( Huber  et al. , 2015 ), at the following address:  http://bioconductor.org/packages/apeglm . A single function apeglm is used to estimate the LFCs in the package, which takes a data matrix, a design matrix and a user-defined likelihood function as input. The function will return a list of estimated LFCs and corresponding posterior SDs, interval estimates, and arbitrary tail areas of the posterior. The  apeglm  package comes with a detailed vignette that demonstrates the functions in the package on a real RNA-seq dataset. The  apeglm  shrinkage estimator for RNA-seq can also be easily accessed from the  DESeq2  package, using the lfcShrink function. The R code used in this paper for evaluating methods is available at the following repository:  https://github.com/mikelove/apeglmPaper 6 Software versions The following versions of software were used: REBayes 1.3, DESeq2 1.20.0, edgeR 3.22.3, limma 3.36.1, ashr 2.2-7 and apeglm 1.2.1. 7 Accession numbers The datasets analyzed during this study are available in the ENA, GEO, or SRA repositories:  Schurch  et al.  (2016) https://www.ebi.ac.uk/ena/data/view/PRJEB5348 ,  Holik  et al.  (2017) https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi? acc=GSE86337 ,  Pickrell  et al.  (2010) https://trace.ncbi.nlm.nih.gov/Traces/sra/? study=SRP001540 ,  Bottomly  et al.  (2011) https://trace.ncbi.nlm.nih.gov/Traces/sra/? study=SRP004777 . Supplementary Material bty895_Supplementary_Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Identifying differentially expressed transcripts from RNA-seq data with biological variation</Title>
    <Doi>10.1093/bioinformatics/bts260</Doi>
    <Authors>Glaus Peter, Honkela Antti, Rattray Magnus</Authors>
    <Abstract>Motivation: High-throughput sequencing enables expression analysis at the level of individual transcripts. The analysis of transcriptome expression levels and differential expression (DE) estimation requires a probabilistic approach to properly account for ambiguity caused by shared exons and finite read sampling as well as the intrinsic biological variance of transcript expression.</Abstract>
    <Body>1 INTRODUCTION High-throughput sequencing is an effective approach for transcriptome analysis. This methodology, also called RNA-seq, has been used to analyze unknown transcript sequences, estimate gene expression levels and study single nucleotide polymorphisms ( Wang  et al. , 2009 ). As shown by other researchers ( Mortazavi  et al. , 2008 ), RNA-seq provides many advantages over microarray technology, although effective analysis of RNA-seq data remains a challenge. A fundamental task in the analysis of RNA-seq data is the identification of a set of differentially expressed genes or transcripts. Results from a differential expression (DE) analysis of individual transcripts are essential in a diverse range of problems such as identifying differences between tissues ( Mortazavi  et al. , 2008 ), understanding developmental changes ( Graveley  et al. , 2011 ) and microRNA target prediction ( Xu  et al. , 2010 ). To perform an effective DE analysis, it is important to obtain accurate estimates of expression for each sample, but it is equally important to properly account for all sources of variation, technical and biological, to avoid spurious DE calls ( Robinson and Smyth, 2007 ;  Anders and Huber, 2010 ;  Oshlack  et al. , 2010 ). In this contribution, we address both of these problems by developing integrated probabilistic models of the read generation process and the biological replication process in an RNA-seq experiment. During the RNA-seq experimental procedure, a studied specimen of transcriptome is synthesized into cDNA, amplified, fragmented and then sequenced by a high-throughput sequencing device. This process results in a dataset consisting of up to hundreds of millions of short sequences, or reads, encoding observed nucleotide sequences. The length of the reads depends on the sequencing platform and currently typically ranges from 25 to 300 basepairs. Reads have to be either assembled into transcript sequences or aligned to a reference genome by an aligning tool, to determine the sequence they originate from. With proper sample preparation, the number of reads aligning to a certain gene is approximately proportional to the abundance of fragments of transcripts for that gene within the sample ( Mortazavi  et al. , 2008 ) allowing researchers to study gene expression ( Cloonan  et al. , 2008 ;  Marioni  et al. , 2008 ). However, during the process of transcription, most eukaryotic genes can be spliced into different transcripts which share parts of their sequence. As it is the transcripts of genes that are being sequenced during RNA-seq, it is possible to distinguish between individual transcripts of a gene. Several methods have been proposed to estimate transcript expression levels ( Katz  et al. , 2010 ;  Li  et al. , 2010 ;  Nicolae  et al. , 2010 ;  Turro  et al. , 2011 ). Furthermore, ( Wang  et al. , 2010 ) showed that estimating gene expression as a sum of transcript expression levels yields more precise results than inferring the gene expression by summing reads over all exons. As the transcript of origin is uncertain for reads aligning to shared subsequence, estimation of transcript expression levels has to be completed in a probabilistic manner. Initial studies of transcript expression used the expectation–maximization (EM) approach ( Li  et al. , 2010 ;  Nicolae  et al. , 2010 ). This is a maximum-likelihood procedure which only provides a point estimate of transcript abundance and does not measure the uncertainty in these estimates. To overcome this limitation,  Katz  et al.  (2010 ) used a Bayesian approach to capture the posterior distribution of the transcript expression levels using a Markov chain Monte Carlo (MCMC) algorithm.  Turro  et al.  (2011 ) have also proposed MCMC estimation for a model of read counts over regions that can correspond to exons or other suitable subparts of transcripts. In this contribution, we present BitSeq (Bayesian inference of transcripts from sequencing data), a new method for inferring transcript expression and analyzing expression changes between conditions. We use a probabilistic model of the read generation process similar to the model of  Li  et al.  (2010 ) and we develop an MCMC algorithm for Bayesian inference over the model.  Katz  et al.  (2010 ) developed an MCMC algorithm for a similar generative model, but our model differs from theirs because we allow for multialigned reads mapping to different genes. Furthermore, we infer the overall relative expression of transcripts across the transcriptome whereas  Katz  et al.  (2010 ) focus on relative expression of transcripts from the same gene. We have implemented MCMC using a collapsed Gibbs sampler to sample from the posterior distribution of model parameters. In many gene expression studies, expression levels are used to select genes with differences in expression in two conditions, a process referred to as a DE analysis. We propose a novel method for DE analysis that includes a model of biological variance while also allowing for the technical uncertainty of transcript expression which is represented by samples from the posterior probability distribution obtained from the probabilistic model of read generation. By retaining the full posterior distribution, rather than a point estimate summary, we can propagate uncertainty from the initial read summarization stage of analysis into the DE analysis. Similar strategies have been shown to be effective in the DE analysis of microarray data ( Liu  et al. , 2006 ;  Rattray  et al. , 2006 ) but given the inherent uncertainty of reads mapping to multiple transcripts, we expect the approach to bring even more advantages for transcript-level DE analyses. Furthermore, this method accounts for decreased technical reproducibility of RNA-seq for low-expressed transcripts recently reported by  Łabaj  et al.  (2011 ) and can decrease the number of transcripts falsely identified as differentially expressed. 2 METHODS The BitSeq analysis pipeline consists of two main stages: transcript expression estimation and DE assessment ( Fig. 1 ). For the transcript expression estimation, the input data are single-end or paired-end reads from a single sequencing run. The method produces samples from the inferred probability distribution over transcripts' expression levels. This distribution can be summarized by the sample mean in the case that only expression level estimates are required.
 Fig. 1. Diagram showing the BitSeq analysis pipeline divided into two separate stages. In Stage 1, transcript expression levels are estimated using reads from individual sequencing experiments. In Step 1, reads are aligned to the transcriptome. In Step 2, the probability of a read originating from a given transcript  P ( r n | I n ) is computed for each alignment based on Equation ( 1 ). These probabilities are used in Step 3 of the analysis, MCMC sampling from the posterior distribution in Equation ( 3 ). In Stage 2 of the analysis, the posterior distributions of transcript expression levels from multiple conditions and replicates are used to infer the probability that transcripts are differentially expressed. In Step 4, a suitable normalization for each experiment is estimated. The normalized expression samples are further used to infer expression-dependent variance hyperparameters in Step 5. Using these results, replicates are summarized by estimating the percondition mean expression for each transcript, Equation ( 4 ), in Step 6. Finally, in Step 7, samples representing the distribution of within-condition expression are used to estimate the probability of positive log ratio (PPLR) between conditions, which is used to rank transcripts based on DE belief The DE analysis uses posterior samples of expression levels from two or more conditions and all available replicates. The conditions are summarized by inferring the posterior distribution of condition mean expression. Samples from the posterior distributions are compared with score the transcripts based on the belief in change of expression level between conditions. 2.1 Stage 1: transcript expression estimation The initial interest when dealing with RNA-seq data is estimation of expression levels within a sample. In this work, we focus on the transcript expression levels, mainly represented by  θ =(θ 1 ,…, θ M ), the relative abundance of transcripts' fragments within the studied sample, where  M  is the total number of transcripts. This can be further transformed into relative expression of transcripts θ m (*) =θ m /( l m (∑ i =1 M  θ i / l i )), where  l m  is the length of the  m -th transcript. Alternatively, expression can be represented by  reads per kilobase per million mapped reads ,  RPKM m =θ m ×10 9 / l m , introduced by  Mortazavi  et al.  (2008 ). We use a generative model of the data, depicted in  Figure 2 , which models the RNA-seq data as independent observations of individual reads  r n ∈ R ={ r 1 ,…,  r N }, depending on the relative abundance of transcripts' fragments  θ  and a noise parameter θ act . The parameter θ act  determines the number of reads regarded as noise and enables the model to account for unmapped reads as well as for low-quality reads within a sample.
 Fig. 2. Graphical representation of the RNA-seq data probabilistic model. We can consider the observation of reads  R =( r 1 ,…,  r N ) as  N  conditionally independent events, with each observation of a read  r n  depending on the transcript (or isoform) it originated from  I n . The probability of sequencing a given transcript  I n  depends on the relative expression of fragments  θ  and the noise indicator  Z n act . The noise indicator variable  Z n act  depends on noise parameter θ act  and indicates that the transcript being sequenced is regarded as noise, which enables observation of low-quality and unmappable reads Based on the parameter θ act , indicator variable  Z n act ∼Bern(θ act ) determines whether read  r n  is considered as noise or a valid sequence. For a valid sequence, the process of sequencing is being modelled. Under the assumption of reads being uniformly sequenced from the molecule fragments, each read is assigned to a transcript of origin by the indicator variable  I n , which is given by categorical distribution  I n ∼Cat( θ ). For a transcript  m , we can express the probability of an observed alignment as the probability of choosing a specific position  p  and sequencing a sequence of given length with all its mismatches,  P ( r n | I n = m )= P ( p | m ) P ( r n |seq mp ). For paired-end reads, we compute the joint probability of the alignment of a whole pair, in which case, we also have to consider fragment length distribution  P ( l ),
 (1) 
Details of alignment probability computation including optional position and sequence-specific bias correction methods are presented in  Supplementary Material . For every aligned read, we also calculate the probability that the read is from neither of the aligned transcripts but is regarded as sequencing error or noise  P ( r n |noise). This value is calculated by taking the probability of the least probable valid alignment corrupted with two extra base mismatches. The joint probability distribution of the model can now be written as
 (2) 
where we use weak conjugate Dirichlet and Beta prior distributions for  θ  and θ act , respectively. The posterior distribution of the model's parameters given the data  R  can be simplified by integrating over all possible values of  Z act :
 (3) 
According to the model, any read can be a result of sequencing either strand of an arbitrary transcript at a random position. However, the probability of a read originating from a location where it does not align is negligible. Thus, the term  P ( r n | I n )Cat( I n | θ) θ act  has to be evaluated only for transcripts and positions to which the read does align. To accomplish this, we first align the reads to the transcript sequences using the  Bowtie  alignment tool ( Langmead  et al. , 2009 ), preserving possible multiple alignments to different transcripts. We then precompute  P ( r n | I n ) only for the valid alignments. (See Steps 1 and 2 in  Fig. 1 .) The closed form of the posterior distribution is not analytically tractable and an approximation has to be used. We can analytically marginalize  θ  and apply a collapsed Gibbs sampler to produce samples from the posterior probability distribution over  I n  ( Geman and Geman, 1993 ;  Griffiths and Steyvers, 2004 ). These are used to compute a posterior for  θ , which is the main variable of interest. Full update equations for the sampler are given in  Supplementary Material . In the MCMC approach, multiple chains are sampled at the same time and convergence is monitored using the   statistic ( Gelman  et al. , 2003 ). The   statistic is an estimate of a possible scale reduction of the marginal posterior variance and provides a measure of usefulness of producing more samples. We use the marginal posterior variance estimate and between chain variance to calculate the effective number of samples for each transcript as described by  Gelman  et al.  (2003 ), to determine the number of iterations needed for convergence. Posterior samples of  θ  provide an assessment of the abundance of individual transcripts. As well as providing an accurate point estimate of the expression levels through the mean of the posterior, the probability distribution provides a measure of confidence for the results, which can be used in further analyses. 2.2 Stage 2: combining data from multiple replicates and estimating DE To identify transcripts that are truly differentially expressed, it is necessary to account for biological variation by using replication for each experimental condition. Our method summarizes these replicates by estimating the biological variance and inferring percondition Mean expression levels for each transcript. During the DE analysis, we consider the logarithm of transcript expression levels  y m =logθ m . The model for data originating from multiple replicates is illustrated in  Figure 3 . We use a hierarchical log-normal model of within-condition expression. The prior over the biological variance is dependent on the mean expression level across conditions and the prior parameters (hyper-parameters) are learned from all of the data by fitting a nonparametric regression model. We fit a model for each gene using the expression estimates from Stage 1.
 Fig. 3. Graphical model of the biological variance in transcript expression experiment. For replicate  r , condition  c  and transcript  m , the observed log-expression level  y m ( cr )  is normally distributed around the normalized condition mean expression μ m ( c ) + n ( cr )  with biological variance 1/λ m ( c ) . The condition mean expression μ m ( c )  for each condition is normally distributed with overall mean expression μ m (0)  and scaled variance 1/(λ ( c ) m λ 0 ). The inverse variance, or precision λ ( c ) m , for a given transcript  m  follows a Gamma distribution with expression-dependent hyperparameters α G , β G , which are constant for a group of transcripts  G  with similar expression A novel aspect of our Stage 2 approach is that we fit models to posterior samples obtained from the MCMC simulation from Stage 1, which can be considered ‘pseudo-data’ representing expression corrupted by technical noise. A pseudo-data vector is constructed using a single MCMC sample for each replicate across all conditions. The posterior distribution over per-condition means is inferred for each pseudo-data vector using the model in  Figure 3  (described below). We then use Bayesian model averaging to combine the evidence from each pseudo-data vector and determine the probability of DE. This approach allows us to account for the intrinsic technical variance in the data; it is also computationally tractable because the model for a single pseudo-data vector is conjugate and therefore inference can be performed exactly. This effectively regularizes our variance estimate in the case that the number of replicates is low. As shown in  Section 3.5 , this provides improved control of error rates for weakly expressed transcripts where the technical variance is large. For a condition  c , we assume  R c  replicate datasets. The log expression from replicate  r ,  y m ( cr )  is assumed to be distributed according to a normal distribution with condition mean expression μ m ( c ) , normalized by replication-specific constant  n ( cr ) , and precision λ m ( c ) ,  y m ( cr ) ∼Norm(μ m ( c ) + n ( cr ) , 1/λ m ( c ) ). As our parameters represent the relative expression levels in the sample, BitSeq implicitly incorporates normalization by the total number of reads or the RPKM measure, as Was done when generating the results in this publication. Further more, normalization can be implemented using the normalization constant  n ( cr ) , which is constant for all transcripts of a given replicate and can be estimated prior to probabilistic modelling using, for example, a quantile-based method ( Robinson and Oshlack, 2010 ) or any other suitable technique. The condition mean expression is normally distributed μ m ( c )  ∼ Norm (μ m (0) , 1/(λ m ( c ) λ 0 )) with mean μ m (0) , which is empirically calculated from multiple samples and scaled precision λ m ( c ) λ 0 . The prior distribution over pertranscript, condition-specific precision λ m ( c )  is a Gamma distribution with hyperparameters α G , β G , which are fixed for a group of transcripts with similar expression level,  G . The hyperparameters α G , β G  determine the distribution over pertranscript precision parameter  λ m  which varies with the expression level of a transcript (see  Supplementary Figure 3 ). For this reason, we inferred these hyperparameters from the dataset for various levels of expression, prior to the estimation of precision  λ m  and mean expression  μ m . We used the same model as  Figure 3  applied jointly to multiple transcripts with similar empirical mean expression levels μ m (0) . We set a uniform prior for the hyperparameters, marginalized out condition means and precision, and used an MCMC algorithm to sample α G , β G . The samples of α G , β G  were smoothed by Lowess regression ( Cleveland, 1981 ) against empirical mean expression to produce a single pair of hyperparameters for each group of transcripts with similar expression level. This model is conjugate and thus leads to a closed-form posterior distribution. This allows us to directly sample  λ m  and  μ m  given each pseudo-data vector  y m  constructed from the Stage 1 MCMC samples:
 (4) Samples of μ m ( c 1 )  and μ m ( c 2 )  are used to compute the probability of expression level of transcript  m  in condition  c 1  being greater than the expression level in condition  c 2 . This is done by counting the fraction of samples in which the mean expression from the first condition is greater, that is  P (μ m ( c 1 ) &gt;μ m ( c 2 ) | R )=1/ N ∑ n =1 N  δ(μ m , n ( c 1 ) &gt;μ m , n ( c 2 ) ) which we refer to as the PPLR. Here,  n =1…  N  represents one sample from the above posterior distribution for each of  N  independent pseudo-data vectors. Subsequently, ordering transcripts based on PPLR produces a ranking of most probable upregulated and downregulated transcripts. This kind of one-sided Bayesian test has previously been used for the analysis of microarray data ( Liu  et al. , 2006 ). 3 RESULTS AND DISCUSSION 3.1 Datasets We performed experiments evaluating both gene expression estimation accuracy as well as DE analysis precision. For the evaluation of bias correction effects as well as comparison with other methods ( Table 1 ), we used paired-end RNA-seq data from the microarray quality control (MAQC) project ( Shi  et al. , 2006 ) (Short Read Archive accession number SRA012427), because it contains 907 transcripts which were also analyzed by TaqMan qRT-PCR, out of which 893 matched our reference annotation. The results from qRT-PCR probes are generally regarded as ground truth expression estimates for comparison of RNA-seq analysis methods ( Roberts  et al. , 2011 ). We used RefSeq refGene transcriptome annotation, assembly NCBI36/hg18 to keep results consistent with qRT-PCR data as well as previously published comparisons by  Roberts  et al.  (2011 ).
 Table 1. Comparison of expression estimation accuracy against TaqMan qRT-PCR data Read model BitSeq Cuff. 0.9.3 RSEM MMSEQ Uniform 0.7677 0.7503 0.7632 0.7614 Non-uniform 0.8011 0.8056 0.7633 0.7990 a The table shows the effect of non-uniform read distribution models using correlation coefficient  R 2  of average expression from three technical replicates with the 893 matching transcripts analysed by qRT-PCR, highest correlation is highlighted in bold. The sequencing data (SRA012427) are part of the MAQC project and was originally published by  Shi  et al.  (2006 ). a We were not able to use the default bias correction provided by MMSEQ ( Turro  et al. , 2011 ) due to an error in an external  R  package mseq used for the bias correction. Instead, we provided the MMSEQ package with effective lengths computed by BitSeq bias correction algorithm to produce results for this comparison. 
 The second dataset used in our evaluation was originally published by  Xu  et al.  (2010 ) in a study focused on identification of microRNA targets and provides technical as well as biological replicates for both studied conditions. We use this data to illustrate the importance of biological replicates for DE analysis ( Fig. 5 ;  Supplementary Fig. 3  for biological variance) and the advantages of using a Bayesian approach for both expression inference and DE analysis ( Fig. 4 ).
 Fig. 4. In plots ( a ) and ( b ), we show the posterior transcript expression density for pairs of transcripts from the same gene. This is a density map constructed using the MCMC expression samples for these three transcripts. In ( c ), we show the marginal posterior distribution of expression levels of the same transcripts as illustrated by histograms of MCMC samples. The sequencing data are from miRNA-155 study published by  Xu  et al.  (2010 ) For the purpose of evaluating and comparing BitSeq to existing DE analysis methods, we created artificial RNA-seq datasets with known expression levels and differentially expressed transcripts. We selected all transcripts of chromosome 1 from human genome assembly NCBI37/hg19 and simulated two biological replicates for each of the two conditions. We initially sample the expression for all replicates using the same mean relative expression and variation between replicates as were observed in the Xu  et al.  data estimates. Afterwards, we randomly choose one-third of the transcripts and shift one of the conditions up or down by a known fold change. Given the adjusted expression levels, we generated 300 k single-end reads uniformly distributed along the transcripts. The reads were reported in Fastq format with Phred scores randomly generated according to empirical distribution learned from the SRA012427 dataset. With the error probability given by a Phred score, we generated base mismatches along the reads. 3.2 Expression-level inference Figure 4  demonstrates the ambiguity that may be present in the process of expression estimation. In  Figure 4 a and  4 b, we show the density of samples from the posterior distribution of expression levels for two pairs of transcripts. The expression levels of transcripts uc010oho.1 and uc010ohp.1 ( Fig. 4 a) are negatively correlated. On the other hand, transcripts uc010oho.1 and uc001bwm.3 exhibit no visible correlation ( Fig. 4 b) in their expression-level estimates. Even though this kind of correlation does not have to imply biological significance, it does point to technical difficulties in the estimation process. These transcripts share a significant amount of sequence and the consequent read mapping ambiguity leads to greater uncertainty in expression estimates (see  Supplementary Fig. 1 d for transcript profile). Bayesian inference can be used to assess the uncertainty due to such confounding factors, unlike the maximum-likelihood point estimates provided by an EM algorithm. The marginal posterior probability of transcript expression for each transcript is shown in  Figure 4 c. In our analysis pipeline, the marginal posterior distributions are propagated into the DE estimation stage, thus the uncertainty from expression estimation is taken into account when assessing whether there is strong evidence that transcripts are differentially expressed. 3.3 Expression estimation accuracy and read distribution bias correction Initially, it was assumed that high-throughput sequencing produces reads uniformly distributed along transcripts. However, more recent studies show biases in the read distribution depending on the position and surrounding sequence ( Dohm  et al. , 2008 ;  Roberts  et al. , 2011 ;  Wu  et al. , 2011 ). Our generative model for transcript expression inference ( Fig. 2 ) includes a model of the underlying read distribution which is included in the  P ( r n | I n = m ) term that is calculated as a preprocessing step. The current BitSeq implementation contains the option of using a uniform read density model or using the model proposed by  Roberts  et al.  (2011 ) which can account for positional and sequence bias. The effect of correcting for read distribution was analyzed using the SRA012427 dataset and results are presented in  Table 1 . We also compare BitSeq with three other transcript expression estimation methods: Cufflinks v0.9.3 ( Roberts  et al. , 2011 ), MMSEQ v0.9.18 ( Turro  et al. , 2011 ) and RSEM v1.1.14 ( Li and Dewey, 2011 ). The dataset contains three technical replicates. These were analyzed separately and the resulting estimates for each method were averaged together. Subsequently, we calculated the squared Pearson correlation coefficient ( R 2 ) of the average expression estimate and the results of qRT-PCR analysis. All four methods used with the default uniform read distribution model provide similar level of accuracy with BitSeq performing slightly better than the other three methods. Both BitSeq and Cufflinks use the same method for read distribution bias correction and provide improvement over the uniform model similar to improvements previously reported by  Roberts  et al.  (2011 ). We used version 0.9.3 of Cufflinks (as used by Roberts  et al. ) since we found that the most recent stable version of Cufflinks (version 1.3.0) leads to much worse performance for both uniform and bias-corrected models (see Supplementary results  Section 2.2 ). The RSEM package uses its own method for bias correction based on the relative position of fragments, which in this case did not improve the expression estimation accuracy for the selected transcripts. In the case of BitSeq, the major improvement of accuracy originates from using the effective length normalization. To compare the results with qRT-PCR, the relative expression of fragments θ has to be converted into either relative expression of transcripts (θ * ) or RPKM units. Using the bias-corrected effective length for this conversion leads to the higher correlation with qRT-PCR ( Supplementary Table 1 ). This means that using an expression measure adjusted by the effective length, such as RPKM, is more suitable than normalized read counts for DE analysis. We also evaluated the accuracy of the four methods using three different expression measures on simulated data. First, we compared with transcripts' RPKM as an absolute expression measure. Second, we used relative within-gene expression in which transcript expression is the relative proportion within transcripts of the same gene. Finally, we used gene expression RPKM, the sum of transcript expression levels for each gene. The results are presented in  Table 2 . MMSEQ provides the best absolute expression accuracy with BitSeq and RSEM showing almost equally good results. For the relative within-gene expression levels, BitSeq is more accurate than the other methods. In spite of providing slightly better results in absolute measure, RSEM and MMSEQ show worse correlation in the relative within-gene measure as they tend to assign zero expression to some transcripts within one gene. This is most likely due to the use of maximum- likelihood parameter estimates as the starting point for the Gibbs sampling algorithm.
 Table 2. The  R 2  correlation coefficient of estimated expression levels and ground truth Expression Cutoff BitSeq Cuff. 0.9.3 RSEM MMSEQ Transcript 1 0.994 0.764 0.995 0.997 Relative 10 0.945 0.724 0.876 0.886 Relative 100 0.963 0.773 0.946 0.948 Gene 1 0.994 0.823 0.996 0.998 Three different expression measures were used: absolute transcript expression, relative within-gene transcript expression and gene expression. Comparison includes sites with at least 1 read per transcript for transcript expression, either 10 or 100 reads pre gene for within-gene transcript expression and at least 1 read per gene for gene expression. The highest correlation is in bold. 
 For more details and results comparing the transcript expression estimation accuracy, please refer to  Supplementary Material  Section 2.3. 3.4 DE analysis We use the Xu  et al.  dataset to demonstrate the DE analysis process of BitSeq. This dataset contains technical and biological replication for both studied conditions. We observed significant difference between biological and technical variance of expression estimates ( Supplementary Fig. 3 ). Furthermore, the prominence of biological variance increases with transcript expression level. We illustrate how BitSeq handles biological replicates to account for this variance in  Figure 5 , by showing the modelling process for one example transcript given only two biological replicates for each of two conditions.
 Fig. 5. Comparison of BitSeq to naive approach for combining replicates within a condition for transcript uc001avk.2 of the Xu  et al.  dataset. ( a ) Initial posterior distributions of transcript expression levels for two conditions (labelled C0, C1), with two biological replicates each (labelled R0, R1). ( b ) Mean expression level for each condition using the naive approach for combining replicates. The posterior distributions from replicates are joined into one dataset for each condition. ( c ) Inferred posterior distribution of mean expression level for each condition using the probabilistic model in  Figure 3 . ( d ) Distribution of differences between conditions from both approaches show that the naive approach leads to overconfident conclusion Figure 5 a shows histograms of expression-level samples produced in the first stage of our pipeline. BitSeq probabilistically infers condition mean expression levels using all replicates. For comparison, we used a naive way of combining two replicates by combining the posterior distributions of expression into a single distribution. The resulting posterior distributions for both approaches are depicted in  Figures 5 b and  5 c. The probability of DE for each transcript is assessed by computing the difference in posterior expression distributions of the two conditions. Resulting distributions of differences for both approaches are portrayed in  Figure 5 d with obvious difference in the level of confidence. The naive approach reports high confidence of upregulation in the second condition, with the PPLR being 0.995. When biological variance is being considered by inferring the condition mean expression, the significance of DE is decreased to PPLR 0.836. 3.5 Assessing DE performance with simulated data Using artificially simulated data with a predefined set of differentially expressed transcripts, we evaluated our approach and compared it with four other methods commonly used for DE analysis. DESeq v1.6.1 ( Anders and Huber, 2010 ), edgeR v2.4.3 ( Robinson  et al. , 2010 ) and baySeq v1.8.1 ( Hardcastle and Kelly, 2010 ) were designed to operate on the gene level and Cuffdiff v1.3.0 ( Trapnell  et al. , 2010 ) on the transcript level. Despite not being designed for this purpose, we consider the first three in this comparison as the use case is very similar and there are no other well-known alternatives besides Cuffdiff that would use replicates for transcript level DE analysis. All other methods beside Cuffdiff use BitSeq, Stage 1 transcript expression estimates converted to counts. Details regarding use of these methods are provided in the  Supplementary material , Section 2.5.  Figure 6  shows the overall results as well as split into three parts based on the expression of the transcripts. The receiver-operating characterization curves were generated by averaging over five runs with different transcripts being differentially expressed and the figures are focused on the most significant DE calls with false-positive rate below 0.2.
 Fig. 6. ROC evaluation of transcript level DE analysis using artificial dataset, comparing BitSeq with alternative approaches. DESeq, edgeR and baySeq use transcript expression estimates from BitSeq Stage 1 converted to counts. The curves are averaged over five runs with different set of transcripts being differentially expressed by fold change uniformly distributed in the interval (1.5,3.5). We discarded transcripts without any reads initially generated as these provide no signal. Panel ( a ) shows global average behaviour whereas in ( b ), ( c ) and ( d ) transcripts were divided into three equally sized groups based on the mean generative read count: [1,3), [3,19) and [19, ∞), respectively Overall ( Figure 6 a), BitSeq is the most accurate method, followed first by baySeq, then edgeR and DESeq with Cuffdiff further behind. This trend is especially clear for lower expression levels ( Fig. 6 b and  6 c). The overall performance here is fairly low because of high level of biological variance. For highest expressed transcripts ( Fig. 6 d), DESeq and edgeR show slightly higher true positive rate than BitSeq and baySeq, especially at larger false- positive rates. Furthermore details and more results from the DE analysis comparison can be found in  Supplementary material  Section 2.5. 3.6 Scalability and performance As BitSeq models individual read assignments, the running time complexity of the first stage of BitSeq increases with the number of aligned reads. Preprocessing the alignments and sampling a constant number of samples scales linearly with the number of reads. However, with more reads, the data become more complex and the Gibbs sampling algorithm needs more iterations to capture the whole posterior distribution. In  Table 3 , we present the running time for Stage 1, using simulated data generated from the UCSC NCBI37/hg19 knownGene reference. We ran the preprocessing of the reads with a uniform read distribution model on a single CPU and sampling with four parallel chains on four Intel Xeon 3.47 GHz CPUs. We set the sampler to run until it generates 1000 effective samples for at least 95% of transcripts. At the end, almost all transcripts converged according to the   statistic. The number of iterations necessary to produce the desired amount of effective samples seems to increase logarithmically with the number of reads.
 Table 3. Scalability and run-time complexity of BitSeq on different-sized datasets using simulated data with 9.9 up to 158.5 million paired-end reads Read pairs (M) 4.9 9.1 19.8 39.6 79.2 Alignments (M) 16 32 64 129 258 Preprocessing (m) 8 15 29 57 115 1000 samples (m) 7 14 32 56 71 Total time (h) 0:55 2:18 5:42 16:23 33:19 Convergence it. 5269 6900 8920 11970 15979 The table shows wall clock running times to preprocess the aligned reads, generate 1000 samples and full time for the sampling algorithm on four CPUs. The last row contains the estimated number of iterations needed to reach convergence for at least 95% of transcripts. 
 Running time of the DE analysis in Stage 2 does only depend on the number of reference transcripts, replicates and samples generated in Stage 1 for the analysis. Producing the result presented in  Section 3.4  took 97 min on the Intel Xeon 3.47 GHz CPU. 4 CONCLUSION We have presented methods for transcript expression level analysis and DE analysis that aim to model the uncertainty present in RNA-seq datasets. We used a Bayesian approach to provide a probabilistic model of transcriptome sequencing and to sample from the posterior distribution of the transcript expression levels. The model incorporates read and alignment quality, adjusts for non-uniform read distributions and accounts for an experiment-specific fragment length distribution in case of paired-end reads. The accuracy of inferred expression is comparable and in some cases, outperforms other competing methods. However, the major benefit of using BitSeq for transcript expression inference is the availability of the full posterior distribution which is useful for further analysis. The inferred distributions of transcript expression levels can be further analyzed by the second stage of BitSeq for DE analysis. Given biological replicates, BitSeq accounts for both intrinsic technical noise and biological variation to compute the posterior distribution of expression differences between conditions. It produces more reliable estimates of expression levels within each condition and associates these expression levels with a degree of credibility, thus providing fewer false DE calls. We want to highlight that to make accurate DE assessment, experimental designs must include biological replication and BitSeq is a method capable of combining information from biological replicates when comparing multiple conditions using RNA-Seq data. In our current work, we aim to reduce the computational complexity of BitSeq by replacing MCMC with a faster deterministic approximate inference algorithm and we are generalizing the model to include more complex experimental designs in the DE analysis stage. Funding : European ERASysBio+ initiative project SYNERGY by the Biotechnology and Biological Sciences Research Council [BB/I004769/2 to M.R.] and Academy of Finland [135311 to A.H.]; Academy of Finland [121179 to A.H.]; and IST Programme of the European Community, under the PASCAL2 Network of Excellence [IST-2007-216886]. This publication only reflects the authors' views. Conflict of Interest : none declared. 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>AssociationViewer: a scalable and integrated software tool for visualization of large-scale variation data in genomic context</Title>
    <Doi>10.1093/bioinformatics/btp017</Doi>
    <Authors>Martin Olivier, Valsesia Armand, Telenti Amalio, Xenarios Ioannis, Stevenson Brian J.</Authors>
    <Abstract>Summary: We present a tool designed for visualization of large-scale genetic and genomic data exemplified by results from genome-wide association studies. This software provides an integrated framework to facilitate the interpretation of SNP association studies in genomic context. Gene annotations can be retrieved from Ensembl, linkage disequilibrium data downloaded from HapMap and custom data imported in BED or WIG format. AssociationViewer integrates functionalities that enable the aggregation or intersection of data tracks. It implements an efficient cache system and allows the display of several, very large-scale genomic datasets.</Abstract>
    <Body>1 INTRODUCTION Advances in genotyping platforms have enabled the identification of millions of single nucleotide polymorphism (SNPs) in the human genome, which are intensively used to study the impact of genomic variation on phenotype. Dedicated software like WGAViewer (Ge  et al. ,  2008 ) was developed to facilitate the interpretation of results from early genome-wide association (GWA) studies. Recent dramatic increases in array resolution—the latest Affymetrix and Illumina arrays offer more than 1.8 M features—have created a novel and immediate need for efficient and scalable visualization tools. Scientists and clinicians strongly rely on such tools to interpret their results, while bioinformaticians need scalable applications to check the results from their high-throughput analyses. In this context, we have developed AssociationViewer, a software tool for visualization of GWA studies in genomic context. The program can efficiently handle large genomic datasets, is extensible to any genomic data represented in BED or WIG format and implements aggregation (union) or intersection of data tracks. 2 PROGRAM OVERVIEW 2.1 Cache and memory management With increasing data volumes, efficient resource management is essential. One approach is to store the data in a cache with fast indexing mechanisms to retrieve the data, and to keep in memory only the information that is visualized. We implemented such a system in AssociationViewer. For comparison, loading a single dataset with 500 K SNPs in WGAViewer needs about 224 MB of RAM, whereas loading 10 different datasets (a total of 10 M data points) and displaying all genes on chromosome 1 needs only 50 MB in AssociationViewer. 2.2 Data import and export A typical GWA dataset consists of a list of SNPs with  P -values derived from an association analysis. In AssociationViewer, such data can be imported from PLINK (Purcell  et al. ,  2007 ) output or other text files. Import of data in BED and WIG format is also possible ( Fig. 1 C). These formats are extensively used by the bioinformatics community and in the UCSC genome browser (Kent  et al. ,  2002 ) to describe genomic and transcriptomic data. BED describes gene features, whereas WIG allows representation of any single position associated with a score ( Fig. 1 A1). AssociationViewer allows export in WIG format ( Fig. 1 F). Window images can also be exported in many popular formats.
 Fig. 1. General view of AssociationViewer ( A  and  B ). Also displayed are the input files ( C ), annotation data downloaded ( D ), cross-references ( E ) and export format ( F ). 
 2.3 Annotation retrieval Gene and transcript data ( Fig. 1 A3) can be downloaded from Ensembl (Hubbard  et al. ,  2007 ) and Biomart (Kasprzyk  et al. ,  2004 ). Tag SNPs can be retrieved from the Hapmap website (The International HapMap Consortium,  2007 ) ( Fig. 1 D). The user can choose to connect to Ensembl or HapMap releases for NCBI Builds 35 or 36. 2.4 Genome navigation and data interaction Navigation in AssociationViewer is intuitive ( Fig. 1 A). The user selects a chromosome either by clicking on the appropriate ideogram or via genomic coordinates. Scrolling or zooming is done via a mouse or the appropriate icons. One can search for SNPs, either by providing IDs, a coordinate range, a score cut-off or a list of neighbouring genes. Genes are found using similar options except that there is no score-based filter. Retrieved data (position, function description) are displayed in a table ( Fig. 1 B) which includes cross-references to Ensembl (Hubbard  et al. ,  2007 ), IntAct (Kerien  et al. ,  2007 ), iHop (Fernández  et al. ,  2007 ), dbSNP ( http://www.ncbi.nlm.nih.gov/ ), STRING (Von Mering  et al. ,  2007 ) ( Fig. 1 E). The sequence surrounding a SNP and any associated SNPs can be downloaded and displayed in a table summary and in a linkage disequilibrium (LD) plot ( Fig. 1 A2). 2.5 GWA specialized functions To better understand the distribution of GWA  P -values, AssociationViewer can produce QQ plots to identify where a SNP's  P -value strongly deviates from random expectation. To compare SNP  P -values between different data tracks, it can generate a Manhattan plot. To rank SNPs with highly significant  P -values and obtain information for possible gene candidates, it can generate a ‘top hit’ report. 2.6 Track merging—aggregation and intersection When browsing multiple tracks, it can become tedious to visualize a region of interest. Merging two or more tracks can help this situation. In AssociationViewer WIG (score) tracks are aggregated in two steps: (i) within each track, set all values to 1 if they are greater than the mean score for that track, otherwise set them to 0; (ii) sum the discretized values at each position over all tracks. BED (gene) tracks are aggregated by merging features together and providing a colour code representing the overlap density. Intersection between WIG tracks is also possible, generating a tabulated report of common positions and scores. This is useful when comparing GWA results from different studies on the same phenotype. For example, intersecting SNPs with significant  P -values from different GWAs and deriving a top hit report will sort these SNPs by the number of times they were replicated in the different GWAs. This is a useful functionality to integrate different studies, to reduce the data complexity and to facilitate interpretation of the results. 3 CONCLUSION AND DISCUSSION AssociationViewer is a flexible software tool that permits visualization of GWA data. It implements essential features such as a ‘top hits’ report, SNP annotation retrieval, QQ and LD plots. Any genomic or transcriptomic data represented in BED or WIG format can be imported. Genomic annotation can be downloaded from Ensembl, BioMart and Hapmap. The ability to handle very large datasets is often limited in visualization software. We optimized resource management by using an efficient cache system and limiting the amount of information held in memory. As a result, our software performs remarkably well when simultaneously visualizing several large-scale GWA datasets. The aggregation and intersection of data tracks are useful functionalities to reduce data complexity. The intersection feature report offers the possibility to integrate and visualize results from different studies. As a proof of concept, simple aggregation methods were implemented in the current version of AssociationViewer, but more elaborate algorithms will be developed in future versions. Dedicated resources for SNP and copy number variant datasets are being set up [e.g. Ensembl Variation, European Genotype Archive ( http://www.ebi.ac.uk/ega/ ), Database of Genomic Variants (Iafrate  et al. ,  2004 )]. Once connection to these resources is possible, we plan to enable queries via the API to visualize results within AssociationViewer. 
 Supplementary Material 
 
 
 [Supplementary Data] 
 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Threshold-seq: a tool for determining the threshold in short RNA-seq datasets</Title>
    <Doi>10.1093/bioinformatics/btx073</Doi>
    <Authors>Magee Rogan, Loher Phillipe, Londin Eric, Rigoutsos Isidore, Hofacker Ivo</Authors>
    <Abstract/>
    <Body>1 Introduction ‘Short RNA-seq’ is widely employed to study categories of non-coding RNAs (ncRNAs), typically between 15 and 30 nucleotides (nts) in length ( Bartel, 2004 ;  Cloonan  et al. , 2011 ;  Londin  et al. , 2015 ). The benefits of short RNA-seq include its comprehensive nature, relatively low cost, and the relative ease with which it can be implemented and executed. Unlike microarray approaches where one is constrained to quantifying the abundance of only the ncRNAs represented by the microarray’s probes, short RNA-seq quantifies  any  ncRNA that is present in a sample. A number of factors establish the  effective  depth at which a given RNA sample is sequenced. These factors include RNA quality, library preparation, degree of multiplexing, DNA contamination, micro-organism contamination (e.g. by  mycoplasma ), etc. In the general case, the relative contributions of these factors cannot be quantified. Given these considerations, any two RNA-seq datasets will have different sequencing depths. The idea behind ‘ normalization ’ is to account for the uneven sequencing depths of datasets that are about to be compared, in order to ‘equalize’ the read counts of transcripts that are common to the datasets being compared. A popular  normalization  approach has been to express RNA transcript abundance in terms of ‘Reads Per Million Mapped’ reads (RPMM) values. RPMM can be applied to individual datasets. Several other normalization approaches have been suggested over the years; they are summarized and reviewed elsewhere ( Dillies  et al. , 2013 ;  Garmire and Subramaniam, 2012 ;  Tam  et al. , 2015 ). It is important to stress that normalization methods do not answer the question of how to establish the level of support below which one would likely be immersed in noise. The idea behind ‘ thresholding ’ is to separate molecules of putative biological relevance from those that likely result from degradation or aberrant transcription (‘background’ noise). A frequent and arbitrary threshold choice has been to use 1.0 RPMM. However, this choice will lead to complications if a dataset contains  outlier features  that receive a lot of support compared to the rest of the features that are present: such outliers can cause high variability in the number of features that are retained and analyzed (see  Supplementary Fig. S2 ). Here, we present Threshold-seq, a novel approach to thresholding short RNA-seq datasets. Threshold-seq adapts to sequencing depth variations and permits dynamic selection of dataset-specific thresholds that strike a balance between the competing requirements of  sensitivity  and  specificity . Threshold-seq alleviates the above-mentioned influence of outliers by calculating a threshold through analysis of  the number of distinct molecules  that can be identified in a dataset. In other words, Threshold-seq does not rely only on  the number of reads  in the dataset. 2 Materials and methods Threshold-seq proceeds as follows. After adapter removal and quality trimming, the sequenced reads are mapped uniquely to the genome of interest ( Londin  et al. , 2015 ).  No thresholding  is applied at this step: all sequences that are supported by at least one read are kept: let  K  be the number of unique sequences that are kept. Note that  K  is  not  user-defined; it is determined from the dataset being analyzed. Each of the  K  sequences that are kept is then paired with a count that reflects its support in terms of mapped reads: thus, we generate a collection  S  of  K  pairs of the type (sequence, count). We iterate over the following three steps for a total of  N  times (default  N  =   1000). During the  n -th iteration (1 ≤  n  ≤  N ):
 we randomly resample  K  (sequence, count) pairs with replacement. i.e. we draw  K  (sequence, count) pairs from the original pool of  K  unique sequence-count values, creating a collection  S′  that contains as many (sequence, count) pairs as the original collection  S . However, owing to our use of replacement during resampling, the latter being a key element of our approach,  S′  ≠  S. for the  resampled K  (sequence, count) pairs, we approximate numerically the cumulative distribution CDF as follows:  CDF  n x =   ∑ i = 1 x F i , where  F ( i ) is the fraction of the resampled sequences that are paired with a count of exactly  i  reads; for a user-defined interval [ MIN ,  MAX ] (default interval =  [0.90, 0.99]), we report the abscissas  x o ,  x 1 , x 2 , …, x max  at which  CDF n ( x 0 )=MIN,  CDF n ( x 1 ) =MIN  +  ε ,  CDF n ( x 2 )=  MIN  +   2 ε , …,  CDF  n ( x max )= MAX  (default  ε  = 0.5%). We then identify  CDF target  as the  smallest  CDF value at which multiple values of  x  were reported across  N  iterations:  CDF target  is the smallest value within [ MIN ,  MAX ] at which biologically relevant sequences begin to differentiate themselves from background. Finally, we report as the threshold of choice,  x thresh , the value  mode ( x n ) over all values  x n  that satisfy  CDF n ( x n )= CDF target . In other words,  x thresh  is the most frequent abscissa at which the CDF reaches  CDF target  (illustrated in  Supplementary Fig. S3 ) across  N  iterations. To compare Threshold-seq with arbitrary thresholds, we used two public collections of datasets for which technical replicates are available: (i) five samples from the GEUVADIS RNA sequencing project (1000 Genomes Project; see also  Lappalainen  et al. , 2013 ); and (ii) three samples that we published previously ( Londin  et al. , 2015 ). The GEUVADIS datasets were sequenced at seven different sequencing centers and have different sequencing depths even though they are technical replicates ( Supplementary Table S1 ). Our three samples were sequenced at two locations (our University and Applied Biosystems/Thermo Fisher Scientific—see also Supplement for more details). By working with these datasets, we can evaluate our method while removing biological variability. 3 Results For each of the 44 analyzed datasets (5 biological samples × 7 technical replicates plus 3 biological samples × 3 technical replicates), we compare the results of Threshold-seq to several fixed RPMM threshold choices ranging from 0.5 to 5.0 ( Figs 1 and 2 ). We also compare Threshold-seq to thresholds in terms of absolute read numbers ( Supplementary Tables S1 and S2 ,  Supplementary Figs S1 and S2 ). For each of the five GEUVADIS samples, we examined the corresponding seven technical replicates and identified: (i) collection A of all sequences that are supported by one or more reads and are  present in all seven replicates  of the sample; we refer to the set A as the set of  positives ; (ii) collection B of all sequences that are supported by one or more reads and are  absent from at least one of the seven replicates ; we refer to the set B as the set of  negatives ; and, (iii) collection C of all sequences that go  above threshold in at least one of the seven replicates . Thus, for a given choice of threshold, the true positives will be equal to C ∩ A; the false positives will be equal to C ∩ B; and, the true negatives will be equal to B\C. In  Figure 1 , we plot sensitivity ( X -axis) vs. specificity (Y-axis) for different choices of RPMM threshold, ranging from 0.5 to 5.0 (in steps of 0.5). For each of our three samples, we examined its three technical replicates and repeated the above analysis ( Fig. 2 ). Note how Threshold-seq adaptively approaches the RPMM value that achieves a balance between sensitivity and specificity and how that value differs across the eight samples shown in  Figures 1  and  2 . Also, importantly, Threshold-seq is reliable over a large range of values for  N  ( Supplementary Figs S3 and S4 ). Fig. 1. Comparison of Threshold-seq with arbitrary RPMM thresholds. The shown five samples were sequenced in seven technical replicates. In each case, we plot the obtained sensitivity ( X -axis) vs. the obtained specificity ( Y -axis) for different RPMM thresholds from 0.5 to 5 in increments of 0.5. Dark circles show the Threshold-seq equivalent metrics in each case Fig. 2. Comparison of Threshold-seq with arbitrary RPMM thresholds. The shown three samples were sequenced in three technical replicates. In each case, we plot the obtained sensitivity ( X -axis) vs. the obtained specificity ( Y -axis) for different RPMM thresholds from 0.5 to 5 in increments of 0.5. Dark circles show the Threshold-seq equivalent metrics in each case 4 Discussion We presented Threshold-seq, a method for automatically establishing read thresholds when analyzing short RNA-seq datasets. Threshold-seq works on any short RNA-seq dataset regardless of choice of mapping program and of parameters. Threshold-seq can work with individual datasets; i.e. it does  not  require the availability of technical or of biological replicates. In  Figure 1  and  Supplementary Figure S1  we show that when using low absolute thresholds (e.g. 5 reads) or low RPM values (e.g. 1.0 RPM) more distinct sequences go above threshold (=high sensitivity) at the expense of low specificity; on the other hand, using a higher absolute threshold (e.g. 15 reads) or higher RPM (e.g. 5.0 RPM) improves specificity, at the expense of lower sensitivity. By resampling the distinct sequences of the dataset at hand, Threshold-seq achieves a good balance between sensitivity and specificity. Threshold-seq will capture those sequences of a sample that can be confidently assumed to represent biologically relevant features in the tissue/cell of origin, while remaining immune to any outliers that could be present in the data (see also  Supplementary Fig. S2 ). Funding This work has been supported by a W. M. Keck Foundation grant (IR), NIH/NCI grant R21-CA195204 (IR), and by Institutional Funds. 
 Conflict of Interest : none declared. Supplementary Material Supplementary Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>PathCluster: a framework for gene set-based hierarchical clustering</Title>
    <Doi>10.1093/bioinformatics/btn357</Doi>
    <Authors>Kim Tae-Min, Yim Seon-Hee, Jeong Yong-Bok, Jung Yu-Chae, Chung Yeun-Jun</Authors>
    <Abstract>Motivation: Gene clustering and gene set-based functional analysis are widely used for the analysis of expression profiles. The development of a comprehensive method jointly combining the two methods would allow for greater biological insights.</Abstract>
    <Body>1 BACKGROUND The objective of gene clustering is to group genes with similar expression patterns or that are expressed in a coordinated manner (Eisen  et al. ,  1998 ). Subsequent functional enrichment analysis can provide clues as to which molecular functions or annotation categories are associated with individual gene clusters using biological knowledge. Despite its potential utility, the treatment of gene clusters as exclusive units may raise a number of practical concerns in subsequent functional analysis. For example, a large list of candidate functionalities is obtained as the number of clusters increases, thus making it difficult to compare the results between clusters or to establish appropriate significance thresholds considering multiple testing adjustments. Also, the performance of enrichment analysis is profoundly dependent on prior clustering result, which varies considerably according to the cluster methods and parameter settings. More importantly, the potential relationships between gene sets or clusters are difficult to identify in conventional settings. The integration of  a priori  knowledge of gene set information in clustering may be an appropriate solution to these problems (Rapaport  et al. ,  2007 ); however, there are currently no available user-friendly tools that implement this alternate algorithm. Thus, we developed a software package, PathCluster, which utilize an agglomerative hierarchical clustering algorithm for gene set-based clustering. In a given expression profile, the distance matrix is constructed between gene sets and illustrated as a dendrogram. The relationship between gene sets can be visually assessed in the results, thereby facilitating the construction of an association map between diverse annotation categories. The related algorithms are implemented in a freely available software package. Major functionalities of PathCluster are summarized as follows:
 Gene set-based hierarchical clustering and visualization of the results with user-friendly graphic interface, Identification of potential relationship between gene sets; putative interaction between molecular functions or synergism between regulatory motif sequences, Revealing previously unknown links between different annotation categories in terms of gene sets; function-versus-regulatory motif or drug-versus-function, Function-based class categorization of disease samples. 
 2 HIERARCHICAL CLUSTERING OF GENE SETS Two strategies can be employed to determine the expression similarities or distances between gene sets. First, individual gene sets can be scored for the mean expression of belonging genes or enrichment scores derived from non-parametric (GSEA) or parametric version (PAGE) of gene set enrichment algorithms (Cheadle  et al. ,  2007 ). The matrix of gene set scores with respect to the samples can be used to calculate the gene set distance and hierarchical clustering. Alternatively, the distance between two gene sets can be calculated directly as a mean correlation level of all possible gene pairs, each of which represents one possible gene-to-gene match between corresponding gene sets. When dealing with large gene sets and when the overlapping genes between gene sets have peculiar interests (especially the case of promoter gene sets), the mean correlation can also be calculated only for the gene pairs within overlapping genes between gene sets. Detailed descriptions of the metrics utilized and examples are available in the online manual at the PathCluster homepage ( http://www.systemsbiology.co.kr/PathCluster/Manual.pdf ).
 Fig. 1. Screenshots of PathCluster. ( A ) An example of analysis using publicly available expression profiles representing human erythroid differentiation (Keller  et al. ,  2006 ). The dendrogram shows a clustering of immune-related functional annotations as well as signal-related functionalities and relevant sequence. ( B ) The function-based classification of human lung cancer samples (Bhattacharjee  et al. ,  2001 ). Four histological subtypes of lung cancer samples (normal, NL; adenocarcinoma, AD; squamouse cell carcinoma, SQ; small cell carcinoma, SMCL) are distinguished at the gene set-based expression level. PathCluster provides default gene sets covering four kinds of gene annotation categories; molecular functions, the association with regulatory motifs corresponding to transcription factors or miRNA, as well as drug treatment-related expression changes. In addition, gene sets from public databases such as MSigDB or user-defined custom query sets can be readily included in the gene set reference, in order to ensure the versatility of the method. 3 BIOLOGICAL APPLICATION 3.1 Associated molecular functions or regulatory motif sequences in a biological process Using functional gene sets, PathCluster can identify the putative associations between molecular functions, thereby providing clues on coordinated action of specific functions in a given expression profile. Similarly, in the case of promoter gene sets, PathCluster can identify the putative motif synergy between  cis -regulatory motifs or corresponding transcription factors delineating the regulatory crosstalks in a transcriptional regulatory network. Moreover, using combined gene sets with different annotation categories, previously unknown, novel links can be revealed. In erythropoiesis-related expression profiles, a number of functionalities related with immunity and the major histocompatibility complex are observed in a cluster ( Fig. 1 A). Within the cluster, signal-related functionalities (Ras protein signal transduction and MAPKKK cascade) as well as sequence motifs corresponding transcription factors of GATA-1 and c-Rel (a component of NK-κ B ) were also observed indicative of their potential interactions during erythropoiesis. This strategy can be also applied to other combinations of gene sets to reveal novel links between different biological themes such as function versus drug and function versus miRNA. 3.2 Function-based sample classification Knowledge-driven or function-based class categorization has recently emerged as a highly challenging subject. This strategy has already been employed to identify the functional relationships in a large cancer-derived expression compendium or to elucidate drug-signature relationships for clinical benefits (Wong  et al. ,  2008 ). Adopting a user-friendly platform and extended reference of gene sets, PathCluster provides a platform for the classification or molecular diagnosis of clinical samples, also allowing for the interrogation of diverse biological knowledge in terms of gene sets. Figure 1 B shows that function-based classification can successfully distinguish between the three lung cancer subtypes, including normal tissues. In this cluster, eight cancer-related functions are specifically up-regulated in small cell lung cancer and squamous cell carcinoma of the lung. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>A Classifier-based approach to identify genetic similarities between diseases</Title>
    <Doi>10.1093/bioinformatics/btp226</Doi>
    <Authors>Schaub Marc A., Kaplow Irene M., Sirota Marina, Do Chuong B., Butte Atul J., Batzoglou Serafim</Authors>
    <Abstract>Motivation: Genome-wide association studies are commonly used to identify possible associations between genetic variations and diseases. These studies mainly focus on identifying individual single nucleotide polymorphisms (SNPs) potentially linked with one disease of interest. In this work, we introduce a novel methodology that identifies similarities between diseases using information from a large number of SNPs. We separate the diseases for which we have individual genotype data into one reference disease and several query diseases. We train a classifier that distinguishes between individuals that have the reference disease and a set of control individuals. This classifier is then used to classify the individuals that have the query diseases. We can then rank query diseases according to the average classification of the individuals in each disease set, and identify which of the query diseases are more similar to the reference disease. We repeat these classification and comparison steps so that each disease is used once as reference disease.</Abstract>
    <Body>1 INTRODUCTION Genome-wide association studies (GWAS) are an increasingly popular approach for identifying associations between genotype and phenotype. A large number of such studies have been performed recently to try to identify the genetic basis of a wide variety of diseases, and explore how this genetic basis differs depending on the geographic origin of the studied population. High-throughput genotyping chips are used to obtain the genotype of an individual at several hundreds of thousands of single nucleotide polymorphisms (SNPs). These sets of SNPs are able to represent most of the variability at the single locus level that was identified by the HapMap project (Frazer  et al. ,  2007 ). In a GWAS study, several thousands of disease individuals, and several thousands of healthy controls are genotyped. Statistical tests are used to identify SNPs that show a strong association with the disease. Strong association between a SNP and a disease can be evidence that the SNP is related to the disease, or that it is in linkage disequilibrium with SNPs that are related to the disease. In both cases significant associations provide promising leads for further experimental investigation into the genetic etiology of diseases. These studies have led to the identification of more than 150 risk loci in more than 60 diseases (Manolio and Collins,  2009 ). The Wellcome Trust Case-Control Consortium (WTCCC) genotype 500 000 SNPs in seven common diseases: type 1 diabetes (T1D), type 2 diabetes (T2D), coronary artery disease (CAD), Crohn's disease (CD), bipolar disease (BD), hypertension (HT) and rheumatoid arthritis (RA) (WTCCC,  2007 ). In this article we use the individual genotype data from this study. Computational methods have been used to identify disease similarities using a variety of data sources, including gene expression in cancer (Rhodes  et al. ,  2004 ) and known relationships between mutations and phenotypes (Goh  et al. ,  2007 ). However, while a large number of GWAS focusing on individual diseases have been recently published, the attempts to integrate the results of multiple studies have been limited. Most of these integration approaches focus on combining multiple studies of the same disease in order to increase the statistical power (Zeggini  et al. ,  2008 ), or use data from other high-throughput measurement modalities to improve the results of GWAS studies (Chen  et al. ,  2008 ). Comparison between the genetic components of diseases have been done using four different approaches. The first approach is based on the identification of the association between one SNP in two different diseases in two independent studies. The second approach selects a group of SNPs that have been previously associated with some disease and tests if they are also associated with a different disease. An example of this approach is the genotyping of a large number of individuals with T1D at 17 SNPs that have been associated with other autoimmune diseases, which leads to the identification of a locus previously associated with only RA as being significantly associated with T1D as well (Fung  et al. ,  2009 ). The third approach pools data from individuals with several diseases prior to the statistical analysis, and has been used in the original WTCCC study. Several similar diseases (autoimmune diseases, metabolic and cardiovascular diseases) are grouped in order to increase the statistical power for identifying SNPs that are significantly associated with all the diseases in the pool. The fourth approach compares the results of multiple GWAS, and has been previously applied to the WTCCC dataset (Torkamani  et al. ,  2008 ). They use the  P -values indicating the significance of the association between a SNP and a single disease, and compute the correlations between these  P -values in pairs of diseases, as well as the size of the intersection of the 1000 most significant SNPs in pairs of diseases. They identify strong similarities between T1D and RA, between CD and HT, and between BD and T2D. In this work, we introduce a novel approach to identify similarities in the genetic architecture of diseases. We train a classifier that distinguishes between a  reference disease  and the control set. We then use this classifier to classify all the individuals that have a  query disease . If there is a similarity at the genetic level between the query disease and the reference disease, we expect more individuals with the query disease to be classified as belonging to the disease class than if there is no similarity. We generalize our procedure to multiple disease comparison: given a set of multiple diseases, we use each in turn as the reference disease while treating all others as query diseases. There are two main differences between our new approach and existing analyses. First, previous approaches [such as Torkamani  et al.  ( 2008 )] compute a significance score for each SNP, and then use these scores for comparing diseases. In our approach, we first compute a classification for each individual, and then compare diseases using these classifications. Second, we train the classifier using information from all SNPs, and during this learning process select the SNPs that contribute to the classification based on the genotype data only. This genome-wide approach makes it possible to see the classifier as a statistical representation of the differences between the disease set and the control set. The use of classifiers in the context of GWAS has been limited so far. In particular, attempts at using them for predicting outcome based on genotype have been unsuccessful. For example, a recent prospective study in T2D (Meigs  et al. ,  2008 ) found that using 18 loci known to be associated with T2D in a logistic regression classifier together with known phenotypic risk factors does not significantly improve the risk classification, and leads to a reclassification in only 4% of the patients. A particular challenge in the context of outcome prediction is that the prevalence of most diseases is relatively low and that it is therefore necessary to achieve high precision in order for the classifier to be usable. Our goal is not predicting individual outcomes, and we only compare predictions made by a single classifier. We can therefore ignore disease prevalence. A second challenge in the use of a classification approach for finding disease similarities is that the classifier does not explicitly identify genetic features of the disease, but rather learns to distinguish the disease set from the control set. Differences between the two sets that are due to other factors might therefore lead to incorrect results. In most GWAS, a careful choice of matched controls limits this risk. However, when using a classifier trained on one GWAS to classify individuals from a different study, there is a risk that the background distribution of SNPs is very different between the populations in which the datasets have been collected, which could lead to errors, particularly when comparing diseases using datasets from different geographic origins. This risk can be limited by using disease data from a single source. In this work, we use genotype data provided by the WTCCC study, in which all individuals were living in Great Britain and individuals with non-Caucasian ancestry were excluded. In this article, we first provide a detailed description of the analysis approach. We then show that we are able to train classifiers that achieve a classification error that is clearly below the baseline error for T1D, T2D, BD, HT and CAD. We use these classifiers to identify strong similarities between T1D and RA, as well as between HT and BD, and weak similarities between T1D and both BD and HT. We also show that we are able to train a classifier that distinguishes between the two control sets in the WTCCC data. We use this classifier to identify similarities between some diseases and individual control sets. This finding matches observations made during the quality check phase of the original study. The implications of this finding on our approach are addressed in the  Section 5 . Finally, we discuss the implications of the similarities we find, and propose extensions of this approach. A detailed description of the dataset used in this work, the data pre-processing, the decision tree classifier and the comparison procedure are provided in  Sections 3  and  4 , respectively, at the end of the article. 2 APPROACH In this section, we define the general classifier-based approach to identify genetic similarities between diseases. The approach can be separated into four steps: data collection, preprocessing, classifier training and disease comparison.  Figure 1  provides an overview of the training and comparison steps.
 Fig. 1. Overview of the approach. This figure presents the  classification  and  comparison  steps of our analysis pipeline. These steps are repeated using a different  reference disease  each time. The classifier returns a real value between 0.0 and 1.0 which we call  disease-class probability . The histograms represent the distribution of the disease-class probability of the individuals with the reference disease (left) and of the controls (right). In the situation depicted in this figure, there is evidence that query disease C is more similar to the reference disease than the other query diseases. The data collection step consists of collecting samples from individuals with several diseases, as well as matched controls, and genotyping them. Alternatively, existing data can be reanalysed. In both cases, it is important to limit the differences between the disease sets and the control sets that are not related to the disease phenotype. Similarly, differences between the different disease sets should also be limited. In particular, it is recommended to use individuals with the same geographic origin, the same ancestry, and a single genotyping technology for the whole study. In this work, we use existing data from the WTCCC which satisfies these criteria. In the preprocessing step, the data are filtered and uncertain genotype measurements, as well as individuals and SNPs that do not fit quality requirements are discarded. It is important to develop preprocessing steps that ensure good data quality. Approaches that analyze each SNP individually can afford to have a more stringent, often manual post-processing step on the relatively few SNPs that show strong association. The SNPs that do not pass this quality inspection can be discarded without affecting the results obtained on other SNPs. In our approach however, classifier training is done using genome-wide information, and removing even a single SNP used by the classifier could potentially require retraining the entire classifier. It is therefore impractical to perform any kind of post-processing at the SNP level. The  Section 3  of this article describes the data used in this work, as well as the quality control measures we take. The classifier training and comparison steps are interleaved. We start with a list of diseases and a set of individual genotypes for each disease, as well as at least one set of control genotypes. We pick one disease as  reference disease , and refer to the remaining diseases as  query diseases . We train a classifier distinguishing the corresponding disease set from the control set. For any individual, this classifier could either return a binary classification (with values 0 and 1 indicating that the classifier believes the individual is part of, respectively, the controls class or the disease class) or a continuous value between 0 and 1. This continuous value can be seen as the probability of the individual to be part of the disease class, as predicted by the classifier. We refer to this value as  disease-class probability . For simplicity, we will only use the disease-class probability values for the rest of this section, but the comparison step can be performed similarly using binary classifications. During the comparison step, we classify individuals from the query disease sets using the classifier obtained in the training step, and for each query disease, compute the average disease-class probability. The training and comparison steps are then repeated so that each disease is used once as reference disease. We can compare the average disease-class probability of the different query diseases to identify similarities between them. Diseases that have a higher average disease-class probability are more likely to be similar to the reference disease than diseases with a lower average disease-class probability. Using cross-validation, we can obtain the average disease-class probability of the reference disease set and the control set used for training the classifier, and compare them with the values of the other diseases. One particular caveat that needs to be considered in this analysis is that while the classifier does distinguish the control set from the disease set, there is no guarantee that it will only identify genetic features of the disease set. It is also possible that it will identify and use characteristics of the training set, especially if there are data quality issues. This case can be identified during the comparison step if the average disease-class probability of most query diseases is close to the average disease-class probability of the reference disease, but very different from the average disease-class probability of the control set. It is therefore important to look at the distribution of the average disease-class probabilities of all query diseases before concluding that an individual disease is similar to the reference disease. It is important to note that the disease-class probability of a given individual does not correspond to the probability of this individual actually having the disease. The disease frequency is significantly higher in the datasets we use for training the classifier than in the real population. In a machine learning problem in which the test data are class-imbalanced, training is commonly done on class-balanced data, and class priors are then used to correct for the imbalance. Such priors would, however, scale all probabilities linearly, and would not affect the relationships we identify, nor their significance. Estimating the probability of an individual having the disease is not the goal of this project and we can therefore ignore class priors. A large variety of classifiers can be integrated into the analysis pipeline used in our approach. The  Section 4  provides a more formal description of the classification task. In this article, we use a common classifier, decision trees, to show that this approach allows us to identify similarities. The specific details about the decision tree classifier, and how its outputs are used in the analysis step are described in the  Section 4 . 3 RESULTS We evaluate the ability of our analysis approach to identify similarities between diseases using the set of seven diseases provided by the WTCCC. In this section, we first evaluate the performance of individual classifiers that distinguish one disease from the joint control set. We then show that these classifiers can identify similarities between diseases. Finally, we use our classifier to identify differences between the two control sets, and provide evidence indicating that these differences do not affect the disease similarities we identify. 3.1 Classifier performance We first train one classifier for each disease using both the  58C  and the  UKBS  sets as controls. The performance of each classifier is evaluated using cross-validation, and reported in  Table 1 . We compare our classifier to a baseline classifier that classifies all individuals into one class without using the SNP data at all. The best error such a classifier can achieve during cross-validation is the frequency of the smaller class in the training set. We refer to this value as the  baseline error .
 Table 1. Classifier performance (cross-validation) Disease Baseline (%) Error (%) Precision (%) Recall (%) Δ p Leaves T1D 40.05 22.93 71.65 70.71 0.383 9 RA 38.43 33.45 59.12 42.09 0.130 12 BD 38.24 33.59 62.60 30.18 0.087 11 HT 39.92 36.77 57.98 28.64 0.080 12 CAD 39.05 36.62 55.25 32.73 0.075 12 T2D 39.5 38.0 54.12 25.05 0.052 14 CD 36.63 36.28 29.83 18.43 0.046 11 Baseline  corresponds to the baseline error;  Error ,  Precision  and  Recall  to the cross-validation performance of the decision tree classifier; Δ p  to the difference between the average disease-class probability of the control set and the average disease-class probability of the disease set; and  Leaves  to the maximum number of leaves in the pruned classifiers for this disease. 
 The disease for which the classifier performs best is T1D, with a classification error of 22.93%, compared with a baseline error of 40.05%. The classification error obtained by the decision tree classifier is also below the baseline error for several other diseases, although by a substantially smaller margin. This is the case for RA (with an error of 33.45% versus 38.43%), BD (33.59% versus 38.24%), HT (36.77% versus 39.92%) and CAD (36.62% versus 39.05%). For two diseases, T2D and CD, the improvement compared with the baseline error is only minimal, and we choose not to use these classifiers in our analysis. While the classifiers that we keep only provide small improvements in terms of classification error (with the exception of T1D), they have a significantly better trade-off between precision (at least 55%) and recall (at least 28%) than the baseline classifier (which would classify all individuals as controls). We do not use these classifiers in a binary way, but rather use the disease-class probability, which is the conditional probability of an individual to be part of the disease-class given its genotype, under the model of the reference disease learned by the classifier (see  Section 7  for a precise definition for decision trees). It is therefore interesting to consider the distributions of the disease-class probability, as obtained during cross-validation.  Figure 2  illustrates that these distributions differ significantly for T1D. It can also be seen that there are individuals for which the disease-class probability is close to 50%, meaning that there are leaf nodes in the classifier that represent subsets of the data that cannot be distinguished well. Our approach takes this into account by using disease-class probabilities rather than binary classifications. In order to evaluate the ability of our classifiers to distinguish between the disease set and the control set using the disease-class probability metric, we use the difference Δ p  of the average disease-class probability between the two sets. The classifiers that we keep all have values of Δ p  above 0.075. This illustrates that while there are only small improvements in binary classification performance, the classifiers are able to distinguish between the disease set and the control set in the way we intend to use them.
 Fig. 2. Distribution of the disease-class probabilities for the T1D classifier. The two histograms show the distribution of the disease-class probability of the individuals, respectively, in the joint control set (top) and in the T1D set (bottom), as computed during cross-validation. The red lines represent the average disease-class probabilities, and the black line indicates the 0.5 probability cut-off used for binary classification. The plot in between the histograms shows the average disease-class probabilities of the six other diseases on the interval between the average disease-class probabilities of the control set and of the disease set. 3.2 Disease similarities For each of the five classifiers with sufficiently good performance, we compute the average disease-class probability of each of the six query diseases. In summary, we identify strong symmetrical similarities between T1D and RA, as well as between BD and HT. Furthermore, we find that T1D is closer to both BD and HT than other diseases, even though we did not find the symmetrical relation using the T1D classifier. This section provides a detailed presentation of these results. For T1D, the average disease-class probability for the control set and the disease set, as computed using cross-validation, are 0.259 and 0.642, respectively.  Figure 2  shows the distribution of the average disease-class probabilities for the query diseases. RA, another autoimmune disease, is clearly the closest to T1D (average disease-class probability of 0.337). This result is significant, with  P -value &lt;10 −5  (see the  Section 4  for details on how  P -values are obtained). All other diseases have an average disease-class probability that is close to that of the control set, which means that there is no evidence of similarity with T1D. For RA, the average disease-class probabilities are 0.303 for the control set and 0.433 for the disease set. The distribution of the average disease-class probabilities for the other diseases are shown on  Figure 3 a. We can observe that T1D (average disease-class probability of 0.397) is closest to RA ( P &lt; 10 −5 ), meaning that we find a symmetrical similarity between the two diseases. All other diseases have an average disease-class probability close to the one of the control set.
 Fig. 3. Disease-class probabilities comparisons. The plots represent the interval between the average disease-class probabilities of the control set and of the disease set for RA ( a ), BD ( b ), HT ( c ) and CAD ( d ), respectively. The average disease-class probabilities for all the query diseases are shown in blue on every plot. Note that while all plots on this figure use the same scale, different scales are used for the central plots of  figures 2  and  4 . For BD, the average disease-class probabilities are 0.297 for the control set and 0.384 for the disease set. The distribution of the average disease-class probabilities for the query diseases are shown in  Figure 3 b. We can observe that there is a wider spread in the average disease-class probabilities, and that there is no cluster of diseases close to the control set. We can also observe that HT (average disease-class probability of 0.359,  P  &lt; 10 −5 ) is closest to BD, followed by T1D (average disease-class probability of 0.354,  P -value of 0.001). For HT, the average disease-class probabilities are 0.315 for the control set and 0.395 for the disease set. The distribution of the average disease-class probabilities for the other diseases are shown in  Figure 3 c. We can observe that BD (average disease-class probability of 0.381,  P -value &lt; 10 −5 ) is clearly closest to HT. T1D (average disease-class probability of 0.368,  P  &lt; 10 −5 ) is also closer to HT than the remaining diseases. For CAD the average differences between the query diseases are smaller than for all the other classifiers ( Fig. 3 d). Furthermore, the classifier for CAD is the one with the worst performance amongst the ones we use in the comparison phase. Therefore, we believe that the results are not strong enough to report putative similarities identified using this classifier, even though some differences between diseases have significant  P -values. 3.3 Differences between control sets The original WTCCC study found several SNPs that are significantly associated with one of the two control sets. These SNPs are filtered out during preprocessing, both in the WTCCC study and in this work. However, the mere existence of differences between two control sets prompted the question whether a classifier could distinguish the two sets, and if so, what the implications of this finding would be on the validity of results obtained with these control sets. We perform several experiments using the two control sets separately, and report the results in  Table 2 . First, we train a  control–control classifier  that distinguishes the two control sets from each other. This classifier achieves an error of 41.15% compared with a baseline error of 49.62%, and a Δ p  of 0.093. This shows that we are able to distinguish to some extent between the two control sets.  Figure 4  shows the distribution of the  58C class probability  (which corresponds to the value called  disease-class probability  when the classifier distinguishes between one disease and the controls). In order to verify that this result is due to differences between the two specific control set, and not the ability of our classifier to distinguish between any two sets, we randomly split all control individuals into two sets,  R1  and  R2 . We train a classifier to distinguish between these two sets. We find that this classifier does only minimally improves the classification error (error of 49.45%, baseline error of 50.03%, Δ p  of −0.003).
 Fig. 4. Distribution of the class probabilities for the control–control classifier distinguishing the  UKBS  control set from the  58C  control set. The two histograms show the distribution of the  58C  class probability of the individuals, respectively, in the  UKBS  control set (top) and in the  58C  control set (bottom), as computed during cross-validation. The red lines represent the average class probabilities, and the black line indicates the 0.5 probability cut-off used for binary classification. The plot in between the histograms shows the average disease-class probabilities of all seven other diseases on the interval between the average class probabilities of the two control sets. 
 Table 2. Separate training set classifier performance Experiment Baseline (%) Error (%) Precision (%) Recall (%) Δ p Leaves UKBS/58C 49.62 41.15 58.33 64.05 0.093 11 R1/R2 50.03 49.45 50.59 46.42 −0.003 11 UKBS/T1D 42.62 23.15 79.53 80.34 0.402 8 58C/T1D 42.99 24.46 76.60 82.22 0.370 8 UKBS/RA 44.29 36.42 66.21 70.72 0.144 10 58C/RA 44.66 38.11 64.89 67.83 0.135 9 Baseline  corresponds to the baseline error;  Error ,  Precision  and  Recall  to the cross-validation performance of the decision tree classifier; Δ p  to the difference between the average disease-class probability of the control set, and the average disease-class probability of the disease set; and  Leaves  to the maximum number of leaves in the pruned classifiers for this experiment.  R1  and  R2  represent two random splits of the joint control set. 
 We apply the comparison step of our pipeline using the control–control classifier in order to identify possible similarities between the disease set and one of the control sets.  Figure 4  shows the distribution of the average  58C  class probabilities for each disease. The average disease-class probabilities obtained during cross-validation are 0.477 for the  UKBS  set and 0.561 for the  58C  set. Both HT (average  58C  class probability of 0.521,  P  &lt; 10 −5 ) and BD (average  58C  class probability of 0.514,  P -value of 0.0002) are closer to the  58C  control set, whereas both RA (average  58C  class probability of 0.487,  P  &lt; 10 −5 ) and CAD (average  58C  class probability of 0.489,  P -value of 0.0003) are closer to the  UKBS  control set. Given the differences between the control sets, and the unexpected similarities between control sets and diseases, we are interested in verifying that the performance of the disease classifiers used in the analysis is not an artifact caused by these differences. We therefore train two new classifiers for each disease, one using only  UKBS  as control set, and one using only  58C  as control set. The performance of these classifiers for T1D and RA is shown in  Table 2 , and is similar to the performance of the classifiers that use both control sets together. For the remaining diseases (including HT and BD), the classifiers using only one of the control sets do not achieve a classification error below the baseline error, most likely due to the smaller training set (i.e. overfitting). For each of the classifiers for T1D and RA, we compute the average disease-class probability for the other six diseases as well as the unused control set. The similarities between the two diseases are significant in all four classifiers. Furthermore, the average disease-class probability of the unused control set is similar to the average disease-class probability of the other five diseases, and not significantly closer to T1D or RA. Therefore, we can conclude that the results obtained using the T1D and RA classifiers are not due to differences between the control sets. Furthermore, the results using a single control set provide further evidence indicating that the classifiers do identify relevant features of T1D and RA, respectively, rather than relevant features of the control set. 4 DISCUSSION In this work, we introduce a novel approach for identifying genetic similarities between diseases using classifiers. We identify genetic similarities between several diseases. In this section, we first discuss the implications of these findings. We then consider challenges in the application of classifiers to GWAS data. Finally, we propose possible extensions of this approach. We identify a strong similarity between T1D and RA. Genetic factors that are common to these two autoimmune diseases were identified well before the advent of GWAS, and linked to the HLA genes (Torfs  et al. ,  1986 , Lin  et al. ,  1998 ). The original WTCCC study (WTCCC,  2007 ) identifies several genes that appear to be associated with both diseases. We look at the classifiers corresponding to these two diseases. The SNP with the highest information gain in T1D is rs9273363, which is located on chromosome 6, near MHC class II gene HLA-DQB1, and is also the SNP that is most strongly associated with T1D in the initial analysis of the WTCCC data, with a  P -value of 4.29 × 10 −298  (Nejentsev  et al. ,  2007 ). This is the strongest association reported for any disease in the WTCCC study, which explains to a large extent why the T1D classifier so clearly outperforms the classifiers for the other diseases. This SNP is also significantly associated with RA ( P -value of 6.74 × 10 −11 ). The SNP with the highest information gain in RA is rs9275418, which is also part of the MHC region, and is strongly associated with both RA ( P -value of 1.00 × 10 −48 ) and T1D ( P -value of 7.36 × 10 −126 ). This shows that our approach is able to recover a known result, and uses SNPs that have been found to be significantly associated with both diseases in an independent analysis of the same data. The similarity we identify between HT and BD is interesting, since there does not appear to be previous evidence of a link between the two diseases at the genetic level. However, a recent study identified an increased risk of HT in patients with BD compared with general population, as well as compared to patients with schizophrenia in the Dannish population (Johannessen  et al. ,  2006 ). The WTCCC study only identified SNPs with moderate association to HT (lowest  P -value of 7.85 × 10 −6 ) and a single SNP with strong association with BD ( P -value of 6.29 × 10 −8 ). The decision trees for both diseases use a large number of SNPs that have a very weak association with the respective disease. Both classifiers have a classification error that is clearly below the baseline error, and provide evidence of similarity between the two diseases. This indicates that our classifier-based approach is able to use the weak signals of a large number of SNPs to identify evidence for similarities that would be missed by comparing only SNPs that show moderate or strong association with the diseases. Further analyzes are necessary to identify the nature and implications of the similarity we find between HT and BD, as well as the weaker similarity we identified between these two diseases and T1D. We also show that we can train a classifier that can distinguish the two control sets, and we use it to identify diseases that are more similar to one of the control set than the other. This is not an unexpected finding, since SNPs that were strongly associated with a control set were identified and discarded in the WTCCC study. These SNPs were also removed in the preprocessing step of our study, and the results we obtain when trying to distinguish the two control sets therefore show that the decision tree classifier is able to achieve a classification error below the baseline error even though the SNPs with the strongest association could not be used by the classifier. The similarities between some diseases and one of the control sets can most likely be explained by some subtle data quality issue. During quality control, the authors of the WTCCC study found several hundreds of SNPs in which some datasets exhibited a particular probe intensity clustering [see the  Supplementary Material  of the original WTCCC study (WTCCC,  2007 ) for details]. This particular pattern was always observed in  58C ,  BD ,  CD ,  HT ,  T1D ,  T2D , but not in  UKBS ,  RA  and  CAD . This matches the result obtained using our classifier-based approach, in which  RA  and  CAD  were predicted to be most similar to  UKBS , and could therefore be a possible explanation of the similarities we find. While we do find several interesting similarities between diseases, we also observe that training a classifier that distinguishes between individuals with a disease and controls using SNP data poses numerous challenges. The first is that whether someone will develop a disease is strongly influenced by environmental factors. The genetic associations that can be identified using GWAS are only predispositions, and it is therefore likely that some fraction of the control set will have the predispositions, but will not develop the disease. Furthermore, depending on the level of screening, the disease might be undiagnosed in some control individuals, and individuals that are part of a disease set might have other diseases as well. This is especially true for high-prevalence diseases like HT. Obtaining good classifier performance by itself is not, however, the main goal of our approach. We show that we can find similarities even when the classifier performance only shows small improvements compared with the baseline error. In this work, we focus on the comparison approach, not on developing a classifier specially suited for the particular task of GWAS classification. We use decision trees because they are a simple, commonly used classification algorithm. This work shows that classifiers can be used to identify similarities between diseases. This novel approach can be expanded into several directions. First, classification performance can be potentially improved by using a different generic classifier, or by developing classifiers that do take into account the specific characteristics of SNP data. Second, further analysis methods need to be developed in order to analyze the trained classifiers, and identify precisely the SNPs that do lead to the similarities this approach detects. Such a methodology would be useful, for example, to further analyze the putative similarity between HT and BD. Third, building on the fact that our approach considers the whole genotype of an individual, it could be possible to identify subtypes of diseases, and cluster individuals according to their subtype. Finally, modifying the approach to allow the integration of studies performed in populations of different origins or using different genotyping platforms would allow the comparison of a larger number of diseases. Our approach identifies similarities between the genetic architecture of diseases. This is, however, only one of the many axes along which disease similarities could be described. In particular, both genetic and environmental factors interact in diseases, and the genetic architecture for two diseases could be similar, but the environmental triggers could be different, leading to low co-occurrence. There is therefore a need for methods that integrate similarities of different kinds that were identified using different measurement and analysis modalities. An example of such an approach is the computation of disease profiles that integrate both environmental ethiological factors and genetic factors (Liu  et al. ,  2009 ). 5 CONCLUSION GWAS have been used to identify candidate loci likely to be linked to a wide variety of diseases. In this article, we introduce a novel approach that allows identifying similarities between diseases using GWAS data. Our approach is based on training a classifier that distinguishes between a reference disease and a control set, and then using this classifier for comparing several query diseases to the reference disease. This approach is based on the classification of individuals using their full genotype, and is thus different from previous work in which the independent statistical significance of each SNP is used for comparing diseases. We apply this approach to the genotype data of seven common diseases provided by the WTCCC, and show that we are able to identify similarities between diseases. We replicate the known finding that there is a common genetic basis for T1D and RA, find strong evidence for genetic similarities between BD and HT, as well as evidence for genetic similarities between T1D and both BD and HT. We also find similarities between one of the control sets used in the WTCCC ( UKBS ) and two disease sets, RA and CAD. This similarity can possibly be a consequence of the subtle differences in genotyping quality that were observed during the initial quality control performed by the WTCCC. Our results demonstrate that it is possible to use a classifier-based approach to identify genetic similarities between diseases, and more generally between multiple phenotypes. We expect that this approach can be improved by using classifiers that are more specifically tailored for the analysis of GWAS data, and by the integration of a larger number of disease phenotypes. The ability to compare similarities between diseases at the whole-genome level will likely identify many more currently unknown similarities. Genetic similarities between diseases provide new hypotheses to pursue in the investigation of the underlying biology of the diseases, and have the potential to lead to improvements in how these diseases are treated in the clinical setting. 6 DATA We use the individual genotypes provided by the WTCCC. These genotypes come from a GWAS (WTCCC,  2007 ) of seven common diseases: T1D, T2D, CAD, CD, BD, HT, and RA. The data consist of a total of 2000 individuals per disease and 3000 shared controls, with 1500 control individuals from the 1958 British Birth Cohort (58C control set) and 1500 individuals from blood donors recruited specifically for the project (UKBS control set). The genotyping of 500 568 SNPs per individual was performed using the Affymetrix GeneChip 500 K Mapping Array Set. In the original analysis of this dataset by the WTCCC, a total of 809 individuals and 31 011 SNPs that did not pass quality control checks are excluded. In addition, SNPs that appear to have a strong association in the original study have been manually inspected for quality issues, and 578 additional SNPs were removed. In this work, we exclude all individuals and SNPs that were excluded in the WTCCC study, as well as an additional 9881 SNPs that do not appear in the WTCCC summary results. One concern with these quality control steps is the identification of SNPs for which the genotype calling is of poor quality. In the WTCCC study, this is done after the analysis, which makes it possible to visually inspect the small subset of SNPs that are potentially significant. In a classifier-based approach, it is impractical to perform any kind of visual inspection, and we must try to minimize the errors due to genotype calling prior to the analysis. The WTCCC study only uses genotype calls made by a custom algorithm, Chiamo (Marchini  et al. , in preparation), but the genotype calls made using the standard Affymetrix algorithm BRLMM are also available. While the study does show that Chiamo has, on average, a lower error rate than BRLMM, there are SNPs that are discarded during the quality control process that show errors in the genotype calls made by Chiamo. We use the two genotype sets to create a consensus dataset in which the genotype of a given individual at a given SNP is used only if there is agreement between the call made by Chiamo and the call made by BRLMM, and is considered to be unknown if the calls are different. This approach individually considers the call made for every individual at every SNP, and does not discard entire SNPs. The handling of SNPs that have a high proportion of unknown genotypes is left to the classification algorithm, and will be discussed in the corresponding section. While this approach does reduce the errors in genotype calling, this comes at the cost of discarding cases in which Chiamo is right but BRLMM is not. Overall, the frequency of unknown genotypes is 2% using the consensus approach, compared with 0.65% using Chiamo and 0.74% using BRLMM. Furthermore, BRLMM genotype calls are entirely missing for a total of 184 individuals, which are thus excluded from our study. After performing these preprocessing steps, the data set used in this study consists of 459 075 SNPs measured in 2938 control individuals (58C: 1480, UKBS: 1458), 1963 with T1D, 1916 individuals with T2D, 1882 individuals with CAD, 1698 individuals with CD, 1819 individuals with BD, 1952 individuals with HT and 1834 individuals with RA. 7 METHODS In this section, we first formally define the classification task that is central to our approach, then describe the specific classifier we use in this work and how we evaluate its performance, and finally describe how we use the classification results to infer relationships between diseases. 7.1 Classification Task The data consist of a list of individuals  i , a list of SNPs  s  ∈  S , and the measurement of the genotype  g ( s ,  i ) of individual  i  at SNP  s . We use  G i ={ g (1,  i ),…, g (| S |,  i )} to denote the genotype of individual  i  at all the SNPs in the study. The genotype measurement is a discrete variable which can take four values: homozygote for the major allele, homozygote for the minor allele, heterozygote and unknown:  g ( s ,  i )∈{ maj ,  min ,  het ,  unk }. Each individual belongs to one of several disease sets, or to the control set. For the WTCCC data used in this work, we have seven disease sets:  T1D ,  T2D ,  CAD ,  CD ,  BD ,  RA ,  HT , and we use the union of the  58C  and  UKBS  sets as control set. For each disease  d , we train a classifier that distinguishes between that disease set and the controls. The individuals that are not part of these sets are ignored during the training of this classifier. For each individual  i  used during training, a binary class variable  c i  indicates whether the individual belongs to the disease set ( c i == disease ) or to the control set ( c i == control ). The supervised classification task consists of predicting the class  c i  of an individual  i  given its genotype  G i . In this work, we use a decision tree classifier, but any algorithm able to solve this classification task can be easily integrated into our analysis pipeline. 7.2 Decision trees In this section, we describe the decision tree classifier (Breiman  et al. ,  1984 ). We use cross-validation in order to train the classifier, prune the trained decision tree and evaluate its performance on distinct sets of individuals. We train a decision tree  T  by recursively splitting the individuals in each node using maximum information gain for feature selection. We use binary categorical splits, meaning that we find the best rule of the form  g ( s ,  i )==γ, where γ∈{ maj ,  min ,  het }. Binary splits make it possible to handle cases in which only one of the three possible genotypes is associated with the disease without unnecessarily splitting individuals that have the two other genotypes. Unknown values are ignored when computing information gain. This is necessary since there is a correlation between the frequency of unknown values and the quality of the genotyping, which in turn is variable between the different datasets. Counting unknown values during training could therefore lead to classifiers separating the two sets of individuals based on data quality differences, rather than based on genetic differences. However, if a large number of measurements are unknown for a given SNP, the information gain for that SNP will be biased. This is particularly true if the fraction of unknowns is very different between the cases and the controls. In order to avoid this situation, we discard all SNPs that do have &gt;5% of unknown genotypes amongst the training individuals in the node we are splitting. In each leaf node  L , we compute the fraction  f L  of training individuals in that node of that are part of the disease class:  . In order to choose a pruning algorithm, we compare the cross-validation performance obtained using Cost-Complexity Pruning (Breiman  et al. ,  1984 ), Reduced Error Pruning (Quinlan,  1986 ), as well as a simple approach consisting of limiting the tree depth. We find that Reduced Error Pruning outperforms Cost-Complexity Pruning, and performs similarly well than limiting the tree depth, but results in smaller decision trees. We therefore use Reduced Error Pruning, which consists of recursively eliminating subtrees that do not improve the classification error on the pruning set (which only contains individuals that were not used during training). The classification of an individual  i  using a decision tree  T  is done by traversing the tree from the root towards a leaf node  L ( i ) according to the genotype of the individual which is classified. If  f L ( i )  &gt;0.5, then the individual is classified as  disease , else the individual is classified as  control . We can consider the decision tree  T  as a high-level statistical model of the difference between the disease and the control sets. Under this model, the fraction  f L ( i )  represents the conditional probability of individual  i  to be part of the disease class given its genotype:  P T ( c i == disease  |  G i )= f L ( i ) . This value is the  disease-class probability  of individual  i . In order to compute the fractions  f L  over sufficiently large numbers of individuals, we further prune our tree to only have leaf nodes containing at least 100 training individuals. The benefit of using this probability rather than the binary classification is that it allows to distinguish leaf nodes in which there are mainly training individuals from one class from those in which both classes are almost equally represented. In order to assess the performance of our classifier, we perform 5-fold cross-validation. We start by separating the data into five random sets containing 20% of the individuals each. A decision tree  T  is trained using four of these sets, while one set is reserved for pruning and testing. The unused set is split randomly into two equal sets. The first of these sets is used to obtain pruned tree  T ′ from tree  T , and the individuals in the second set are used to evaluate the performance of tree  T ′. The last step is then repeated using the second set for pruning, and the first for testing. Finally, we repeat the training and evaluation four more times, each time leaving out a different set for pruning and testing. This ensures that for every individual in our dataset, there is one pruned decision tree for which the individual was used neither for training nor for pruning. We can therefore evaluate the performance of the classifier on unseen data. We can also compute the average disease-class probability  p ( C ) of the control individuals, and the average disease-class probability  p ( d ) of the individuals with disease  d . The difference Δ p  between those two probabilities indicates how well the classifier is able to distinguish controls from diseases. We use the cross-validation results to compare the performance of the classifier against a baseline classifier which simply assigns the most frequent label amongst the training set to all individuals. Classifiers that do not outperform this baseline classifier, or for which the difference Δ p  is small, are not used to identify similarities between diseases. Given the cross-validation scheme used, we end up training not one, but several possibly distinct decision trees. Rather than arbitrarily choosing one, we use the set  T d  of all decision trees trained during cross-validation for a given disease  d . In order to classify a new individual  i , we first classify  i  using each classifier independently, and then return the average classification. Similarly, we average the results of individual classifiers to obtain the average disease-class probability:  . 7.3 Identifying similarities Once a classifier has been trained to distinguish the set of individuals with reference disease  d  from the control set, we can use it to identify diseases that are similar to disease  d . Using the classifier, we can compute the disease-class probability of an individual with a query disease  d ′. In order to be able to compare diseases, we are interested in computing the average disease-class probability of all individuals in  d ′:  . We expect this average probability to be in, or close to the interval between  p ( C ) and  p ( d ), which were the averages computed on, respectively, the control set and the disease set  d  during cross-validation. If  p ( d ′) is close to  p ( C ), then  d ′ is not very different from the control set, whereas a value  p ( d ′) that is close to  p ( d ) indicates similarity between the two diseases. Using this method, we can compare all query diseases to the reference disease  d , and identify if there are diseases that are more similar to  d  than others. If we find that a query disease  d ′ is closer to reference disease  d  than the other query diseases, then we need to assess the significance of this finding. In order to do so, we randomly sample a set  r  of individuals from all the disease sets except  d , such that  r  is of the same size as  d ′, and compute  p ( r ). We repeat this procedure 10 000 times. The fraction of random samples  r  for which  p ( r ) ≥ p ( d ′) indicates how often a random set of individuals would obtain a probability of being part of the disease-class at least as high as the set  d ′, and is therefore a  P -value indicating how significant the similarity between  d ′ and  d  is. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Metric learning for enzyme active-site search</Title>
    <Doi>10.1093/bioinformatics/btq519</Doi>
    <Authors>Kato Tsuyoshi, Nagano Nozomi</Authors>
    <Abstract>Motivation: Finding functionally analogous enzymes based on the local structures of active sites is an important problem. Conventional methods use templates of local structures to search for analogous sites, but their performance depends on the selection of atoms for inclusion in the templates.</Abstract>
    <Body>1 INTRODUCTION The influx of newly sequenced genomes has sparked the development of function-prediction methods that use global sequence/structure comparison for the annotation of genes and proteins (Loewenstein  et al. ,  2009 ). For enzyme proteins, many such methods attempt to predict functions from protein sequences and structures based on the Enzyme Commission (EC) classification scheme (Loewenstein  et al. ,  2009 ). The EC classification scheme, which has been used worldwide for many years, is based mainly on the whole chemical structures of substrates and products, and on the cofactors involved (Webb,  1992 ). However, because the EC classification scheme neglects protein sequence and structure information, it is sometimes difficult to detect a correlation between an enzyme sequence/structure and functions based on it. For instance, some homologous enzymes that are a result of divergent evolution from the same ancestral enzyme might catalyze different reactions, whereas some non-homologous enzymes from different superfamilies might catalyze the same reaction because of the convergent evolution. The enzyme pair trypsin and subtilisin, which shares the Ser-His-Asp catalytic triad, is a typical example of ‘analogous enzymes’ produced by convergent evolution (Wright,  1972 ). Nagano ( 2005 ) analyzed the catalytic mechanisms of 270 enzymes (mainly hydrolases and transferases) from 131 superfamilies, which are manually compiled in the enzyme reaction database, EzCatDB. Analysis of the enzyme reactions has revealed several analogous reactions that are observed in non-homologous enzymes (Nagano  et al. ,  2007 ). EzCatDB also provides a hierarchic classification of enzyme reactions, RLCP, which clusters the same reaction types together based on basic Reaction (R), Ligand group involved in catalysis (L), type of Catalytic mechanism (C), and residue/cofactors located on Proteins (P) (Nagano,  2005 ). Consequently, both the homologous reaction and the analogous reaction can be clustered together in the RLCP classification if they share the same catalytic mechanism and the same type of catalytic site (Nagano,  2005 ). Results of a recent study also suggest that such cases of active sites shared by analogous enzymes are not rare (Gherardini  et al. ,  2007 ). Consequently, for enzyme-function prediction, it is necessary to examine the specific local structures of the active sites that might reflect enzyme functions, rather than the global structures, such as the domain level or the chain level (Loewenstein  et al. ,  2009 ). Regarding local structure comparison methods to detect similar active sites, several ‘template-based’ methods have been reported (Barker and Thornton,  2003 ; Chou and Cai,  2004 ; Fetrow and Skolnick,  1998 ; Ivanisenko  et al. ,  2004 ; Kleywegt,  1999 ; Laskowski  et al. ,  2005 ; Stark and Russell,  2003 ; Torrance  et al. ,  2005 ; Wallace  et al. ,  1997 ). Those template-based methods search for the occurrence of a predefined template structure that consists of active-site residue atoms, within target protein structures. However, some questions and problems remain in relation to the template-based methods: (i) the prediction accuracy might be dependent on the number and types of atoms in the templates. Because it is sometimes very difficult to determine which atoms in the catalytic site should be included in the templates, even experts on enzyme structure and function might have to create the best template through trial and error. (ii) Some atoms in the catalytic site might be more important for the catalytic reaction than other atoms are. According to a previous report, the sidechain of catalytic residues is used (92%) much more frequently than the mainchain (only 8%) (Bartlett  et al. ,  2002 ). Moreover, charged and/or polar residues tend to be involved in catalysis (Bartlett  et al. ,  2002 ). Are such catalytically necessary atoms also important for the templates? (iii) These template-based methods also yield a huge number of mismatches along with site matches. Is it possible to reduce the number of mismatches? In this study, we developed a new metric learning algorithm to detect catalytic sites effectively in terms of search accuracy of RLCP classification (Nagano,  2005 ). One famous template-based method, TESS (Wallace  et al. ,  1997 ) uses geometric hashing to search for local structures. JESS (Barker and Thornton,  2003 ) uses kd-tree data structures. Ultimately, both methods compute the unweighted RMSD for the search results. To improve the accuracy of the template search, we use the metric learning algorithm by determining the weights of the atoms in the templates. That method also enables us to compare the importance of the atoms within the template, particularly between the atoms in the manually created template and those in the automatically refined template, based on their determined weight values. The Consurf algorithm (Ashkenazy  et al. ,  2010 ), which has been developed to detect conserved positions in proteins, is distinct from template-based methods, but related to our study. The algorithm computes evolutionary conservation scores from multiple sequence analysis, in order to project the scores onto 3D structures. It depends on sequences around the active site, although analogous sites acquired by convergent evolution have no conserved regions around the sites. In contrast, our algorithm emphasizes only active sites so that analogous sites can also be detected. This article is organized as follows. The next section presents a new algorithm of metric learning to search for functionally analogous enzymes. Numerical experiments in various conditions are conducted to confirm the effectiveness of our algorithm. Those conditions are described in  Section 3 . The results and discussion are presented in  Section 4 . The last section concludes this article with future work. Mathematical notations are given in  Supplementary Material . 2 PRINCIPLES 2.1 Problem setting Such template-based methods such as TESS (Wallace  et al. ,  1997 ) are local structure searching (LSS) algorithms, which search for the occurrence of a predefined template structure that comprises active-site residue atoms, from unknown protein structures. In the first place, the template structure must be created by carefully selecting the atoms in the active site of the query enzyme protein, for the LSS algorithm. Here, the set of selected atoms is called a  query template . The number of atoms is denoted by  n . The LSS algorithm searches for proteins having a local structure with  n  atoms that is similar to the query template, from a database of protein tertiary structures, such as the Protein Data Bank (PDB). The output of the LSS algorithm could be a set of sites such as that presented in  Table 1 , where ℓ represents the number of hits. The conventional usage of the LSS algorithm is to compute the mean square deviation from the query template to each of the hits, and then to sort the hits based on the deviation values to discriminate  site matches  from  mismatches , where site matches belong to the same functional class as the query template, and mismatches do not.
 Table 1. Variables of a dataset generated using the LSS algorithm Atom1 Atom2 … Atom n Class Site 1 x 1,1 x 1,2 ··· x 1, n y 1 Site 2 x 2,1 x 2,2 ··· x 2, n y 2 ⋮ ⋮ ⋮ ⋮ ⋮ Site ℓ x ℓ,1 x ℓ,2 ··· x ℓ, n y ℓ The ℓ sites are presumed to be hits by the LSS algorithm. Their functional classes are known. The vector  x i , j  ∈ ℝ 3  and the scalar  y i  ∈ {±1}, respectively, represent the coordinate of the  j -th atom and the binary class label of the  i -th site. 
 In this article, we propose weighting each atom to achieve better prediction. Conventional approaches use the unweighted mean square deviation to measure how similar a hit is to the query. To give mathematical deviations, we designate the query template and a hit by  X query  and  X ′, respectively. Query template  X query  has  n  atoms and the three-dimensional coordinates are stored in the matrix as  X query  = [ x query 1 ,…,  x query n ] ∈ ℝ 3× n  where  x query j  ∈ {ℝ 3  is the coordinate of the  j -th atom in the query template. Similarly, hit  X ′ is the ordered set of three-dimensional coordinates. It is expressed as  X ′ = [ x ′ 1 ,…,  x ′ n ] ∈ ℝ 3× n  where  x ′ j  is the  j -th atom in the hit. The unweighted mean square deviation is defined by the minimal value of the function
 
over rotation  R  ∈ 𝕆 3  and translation  v  ∈ ℝ 3 . We denote the optimal values of the rotation matrix and the translation by   and  , respectively. The unweighted root mean square deviation (Unweighted RMSD) is also used frequently (Kato  et al. ,  2004 ). The function  E unwei  takes the average of distances without weighting atoms. Our proposal is the use of the weighted version of the distance. Letting  w  ∈  Δ n  be the weight vector, we define the weighted mean square deviation as
 
In this study, rotation   and translation   are precomputed so that they are optimized in the sense of the unweighted mean square distribution. One might consider, instead of using rotation   and translation  , optimizing the two variables so that the weighted mean square deviation is minimized. Although it is possible to optimize the rotation and the translation as well as the weights simultaneously, such an approach makes the learning algorithm fairly complicated. Weighting is equivalent to adjusting the  metric  (Amari and Nagaoka,  2000 ) in the space of the coordinate set of  n  atoms. It will be revealed empirically in  Section 4  that the metric should be determined automatically to achieve good prediction of the hits produced by the LSS algorithm. Hits with a distance less than the threshold are predicted as site matches. To determine the values of the weight parameters of the metric,  w , hits whose functions are known are used for metric learning. Then, the data, an example of which is shown in  Table 1 , are obtainable. In the table, the number of known hits is ℓ. Vector  x i , j  ∈ ℝ 3  stores the three-dimensional coordinate of the  j -th atom in the protein for the  i -th site. Variable  y i  ∈ {±1} is the class label of the protein for the  i -th site where the value is +1 if the site is a site match; otherwise, it is −1. Two symbols, ℐ +  and ℐ − , are used to denote the index set of site matches and mismatches, respectively: ℐ +  ≡ { i  ∈ ℕ ℓ | y i  = +1} and ℐ −  ≡ { i  ∈ ℕ ℓ | y i  = −1}. The  i -th site is denoted by the matrix  X ( i )  ≡ [ x i 1 ,…,  x in ] ∈ ℝ 3× n , which corresponds to the  i -th row in  Table 1 . A new algorithm will be presented to perform automatic weighting, as described below. 2.2 Metric learning Ideal weighting should produce weighted distances that separate site matches from mismatches completely by a threshold. In the ideal case, the distances of all site matches are less than threshold θ ∈ ℝ + :
 (1) 
and the distances of all mismatches are greater than θ:
 (2) 
where  . Figure 1  presents an illustrative example describing the difference between unweighted RMSD and weighted RMSD. The figure has five site matches and five mismatches. The situation in which unweighted RMSD cannot separate site matches from mismatches as in  Figure 1 a often happens, but the data are separable completely by adjusting the weights, as shown in  Figure 1 b.
 Fig. 1. Example of metric learning. Computing RMSD is a typical means to search for site matches from numerous hits aligned with a query template. It involves taking the unweighted average of distances of each atom. This toy example shows a case in which each of the five site matches and five mismatches is aligned with a query template having 10 atoms. In this case, no threshold separates site matches from mismatches perfectly as long as the average of distances is unweighted, as shown in ( a ). Three mismatches and two site matches can be predicted incorrectly if the threshold depicted in (a) is used. Our metric learning algorithm finds a weight for each atom to generate a distance that separates site matches from mismatches. For this example, weighted RMSD supports a complete separation of site matches from mismatches, as shown in ( b ). In practice, however, a situation in which no weighting can separate site matches from mismatches completely can also happen.  Supplementary Figure 5  shows the results of using template 1jfh.  Supplementary Figure 5c  depicts the distribution of unweighted RMSD for site matches and mismatches. Even if the weighted RMSD is used, this dataset cannot be separated by any weights. Therefore, the above conditions of weights, given in (1) and (2), are too strict for practical use. To relax the condition, each site is allowed to violate the inequalities to some degree. Non-negative variable ξ i  is introduced to describe the quantity of the violation and to modify the inequalities to
 
which can be summarized to
 The total error is evaluated using the sum of the mean violation of positives, as   and the mean violation of negatives, as  . Our intention is to find the metric that can achieve the minimum total error. To avoid over-fitting, a constant upper bound  C  ∈ ℝ of the ℓ ∞ -norm of the weight vector ‖ w ‖ ∞  ≤  C  is used. The value of  C  is set to 2/ n  in our experiments. The upper bound has the effect of regularization (Hastie  et al. ,  2003 ). Then, the algorithm that is used to learn the metric is described as
 (3) Further analysis engenders the following theorem. T heorem  1. The problem in ( 3 ) can be reduced to a linear program (Hinrichs et al. ,  2009 ) . The proof is given in the  Supplementary Material . Linear programming has been studied well for many years as a class of convex programming (Boyd and Vandenberghe,  2004 ). There are several efficient solvers for linear programming problems (Dantzig,  2004 ). Supplementary Figure 5d  shows the resultant distribution of the weighted RMSD achieved by the metric learning algorithm using template 1jfh. Although no weighting can separate site matches from mismatches, the metric learning algorithm achieves almost complete separation of site matches from mismatches, with only a few exceptions. The procedures using the metric learning algorithm are summarized in  Figure 2 . Metric learning is performed in the learning stage. In the prediction stage, unknown local sites are superimposed onto templates with Euclidean metric and then weighted RMSDs are computed.
 Fig. 2. Flow of the respective algorithms. In the conventional algorithm, the sites found by LSS algorithms are predicted using unweighted RMSD. In our algorithm, the sites are predicted using weighted RMSD. The weights are obtained using metric learning from known active sites. 3 METHODS To illustrate the usefulness of the metric learning algorithm, experiments that search for active-site structures were conducted over PDB datasets. To create query templates for the active-site structures, 48 protein structures were selected. The LSS algorithm that uses those templates was applied to the PDB dataset. In the next section, the experimental results are shown for the 45 templates presented in  Supplementary Table 3 . A query template comprises a set of atoms in a protein structure. To generate a query template, we roughly selected amino acid residues that play catalytic roles in enzyme proteins. In EzCatDB (Nagano,  2005 ), each amino acid in the active site is classified into one of four types: catalytic-site residue, co-factor binding site residue, modified residue and mainchain catalytic residue. For catalytic-site residues and modified residues, atoms from the sidechains of residues are automatically included in the query template, whereas all atoms are included in the query template for co-factor binding site residues. For mainchain catalytic residues, only the mainchain atoms are included in the query template. The qualities of the query template created in this manner would not depend on the abilities or knowledge of the persons who created the template. In this study, the query template created in this manner is defined as a ‘ rough template ’. The conventional approach requires that we choose carefully those atoms which are involved in enzyme reactions, based on literature information, to create an appropriate query template. A query template produced in this manner can be designated as the ‘ precise template ’. As described in this article, it will be shown whether the rough templates combined with metric learning can discriminate more effectively than the manually created precise templates. To this end, the precise templates for the 45 proteins were created by selecting atoms carefully from the corresponding atoms in the rough templates. Here, the atoms in the precise template are designated as ‘ inner atoms ’, whereas the remaining atoms in the rough template are designated as ‘ outer atoms ’. In the first stage, an LSS algorithm reported by (Wallace  et al. ,  1997 ) was adopted to identify candidates for active sites in the PDB datasets. To investigate the performance of algorithms, 5692 PDB structures registered in EzCatDB were implemented for the PDB datasets. Among all the hit local sites, local sites whose PDB ids belong to the corresponding reaction type, in the RLCP classification (Nagano,  2005 ), and which include residues registered as active sites in EzCatDB, were extracted as site matches. In contrast, local sites whose PDB ids do not belong to the corresponding reaction type in the RLCP classification, or which include residues that are not registered as active sites in EzCatDB, were considered as mismatches. In our experiments, these obtained sites were used as the dataset for the evaluation of the algorithms. The number of site matches and the number of mismatches are shown in  Supplementary Table 3 . To evaluate the performance of the metric learning algorithm, half of the proteins in the datasets were used to learn the metric, and the remainder were used to evaluate the predictions based on the obtained metric. Evaluation criteria of two kinds were adopted: area under the curve (AUC) and sensitivity. AUC is the area under the receiver operating characteristic (ROC) curve, which plots the ratio of correctly predicted site matches against the ratio of wrongly predicted site matches over different possible thresholds. The sensitivity was computed with the threshold that is adjusted so that the specificity would be equal to 0.95. This procedure was repeated 100 times and the average of AUCs was considered to be the performance of each algorithm. Herein, note that ‘site matches’ and ‘mismatches’ in the LSS algorithm are treated as ‘positives’ and ‘negatives’ on computing these two criteria for performance evaluation, respectively. Because rough templates have less dependence on the qualities of the query template as compared with precise templates, the use of rough templates would be favorable to achieve good prediction. Here, the following terminologies are defined to distinguish the methods using rough templates or precise templates in discussing the experimental results for comparison. C ondition  1 [Euclidean Metric with Rough Templates (EMR)].  In the Euclidean method with rough templates ,  the rough template is adopted to perform prediction based on the unweighted RMSD . Consequently, EMR has a control condition that uses the rough template without metric learning. C ondition  2 [Metric Learning with Rough Templates (MLR)].  MLR is a method that adopts rough templates and performs prediction based on the weighted RMSD ,  where the weights are determined using the metric learning algorithm . Here, inner atoms in the rough templates were selected as the catalytic atoms. They are expected to play an important role in catalysis. Therefore, the inner atoms are considered more important than the outer atoms, to obtain better predictions. A condition that uses additional constraints, which would make the weights for inner atoms no smaller than those for outer atoms, has been prepared as follows: C ondition  3 [MLR with constraints favorable to inner atoms (MLR-CI)].  In the MLR-CI ,  the constraints favorable to the inner atoms are set ,  so that the weighted RMSD from the rough templates is adopted to perform predictions .  The metric learning algorithm to determine the weights can solve the optimization problem in ( 3 ) with additional constraints  (∀ j 1  ∈ 𝒥 inner , ∀ j 2  ∈ ℕ n ∖𝒥 inner ),  w j 1  ≥  w j 2   where  𝒥 inner  (⊆ℕ n )  is the index set of the inner atoms . By expanding the idea of MLR-CI, further constraints were applied to obtain a better metric by introducing a priori knowledge. The importance of inner atoms for the catalytic reaction should not be equal; some atoms might be more important than the others. One might infer that a priori information will engender further improvement. To confirm that notion, several atoms (at least three) that are more important for catalytic reaction than the other atoms were selected carefully for each precise template. These atoms are designated as the ‘ catalytically essential atoms ’. Consequently, the constraints that the weights for catalytically essential atoms should not be smaller than those of the other inner atoms were also introduced. C ondition  4 [MLR with constraints favorable to catalytically essential atoms (MLR-CE)].  MLR-CE adopts the weighted mean square deviation from the rough templates .  The metric learning algorithm solves the optimization problem in ( 3 ) with the following additional constraints:  (∀ j 1  ∈ 𝒥 inner , ∀ j 2  ∈ ℕ n ∖𝒥 inner ),  w j 1  ≥  w j 2   and  (∀ j 1  ∈ 𝒥 ce , ∀ j 2  ∈ ℕ n ∖𝒥 ce ),  w j 1  ≥  w j 2   where  𝒥 ce   is the index set of the catalytically essential atoms . The outer atoms might contain only irrelevant information for prediction. If the effects of irrelevant information are too large, then the metric learning algorithm would fail to get rid of the inappropriate effect. Better prediction is obtainable if all the outer atoms are removed. This would lead to use of the following method. C ondition  5 [Euclidean Metric with Precise Templates (EMP)].  EMP is a method that adopts the unweighted RMSD from the precise templates to perform predictions . Metric obtained by learning might engender improvement. Moreover, its variants, which employ constraints favorable to the catalytically essential atoms, might be obtained. Consequently, the following two methods were introduced. C ondition  6 [Metric Learning with Precise Templates (MLP)].  MLP is a method that adopts the weighted mean square deviation from the precise templates to make predictions .  The weights are determined using the metric learning algorithm given in ( 3 ) . C ondition  7 [MLP with the constraints favorable to the catalytically essential atoms (MLP-CE)].  MLP-CE is a method that adopts the weighted mean square deviation from the precise templates .  The method adds the following constraints to the optimization problem in ( 3 ):  (∀ j 1  ∈ 𝒥 ce , ∀ j 2  ∈ ℕ n ∖𝒥 ce ),  w j 1  ≥  w j 2 . 4 RESULTS 4.1 Effects of metric learning Figure 3 a presents the average prediction performance. When rough templates are adopted, metric learning significantly improves the prediction performance; MLR achieved the AUC of 0.984 on average, whereas EMR obtained the AUC of 0.947 on average. These differences are statistically significant according to results of the one-sample  t -test (Rosner,  2000 ) ( P -value of 7.28 × 10 −4 ).
 Fig. 3. Average AUCs and sensitivities. Forty-five templates are used for the experiments. The dataset was randomly split into a training set and a test set for each template 100 times, and the AUC and the sensitivity were computed for the 100 test sets. The bars show the average AUCs and the average sensitivities over the 100 trials and the 45 templates. EMR is the baseline method, whereas MLR is the main proposed method. The performances of the two methods are statistically significantly different in terms of  P -value (by one-sample  t -test) ( Section 4 ). The other methods are prepared to investigate various conditions ( Section 3 ). In  Supplementary Table 4 , the AUC values are shown for all the templates in this study. Values in bold and red indicate the best AUC, although those underlined in blue indicate values for which statistically significant differences were not found relative to the best AUC. Here, the one-sample  t -test was performed to detect statistically significant differences. The significance level was set to 1%. It was observed that plenty of empirical evidence supported the effectiveness of the metric learning algorithm. In 42 of the 45 templates, MLR (Condition 2) yielded the best performance or performance that did not differ from the best performance. The AUCs of MLR surpassed those of EMR in 30 templates, whereas the AUCs were equal for both MLR and EMR in 12 templates. Only in three templates—2ace, 2oke and 1dgy—was the AUC of EMR better than that of MLR in terms of statistical significance. However, the numbers of site matches in 2oke and 1dgy for performance evaluation were, respectively, five (= ⌊10/2⌋) and two (= ⌊5/2⌋). Those numbers are too small to obtain credible statistics for performance evaluation. Regarding the eight templates, 1bls, 1af0, 2oke, 2dhc, 1g42, 1isw, 1arg and 1cq7, the site matches were separated completely from the mismatches, even without metric learning. In six of the eight templates, MLR also separated site matches from mismatches completely. These data suggest that metric learning could rarely worsen the prediction performance of the good template datasets, which have already achieved favorable separation via the Euclidean metric. It is also noteworthy that MLR achieved complete prediction on four other templates (1eo4, 1f0o, 1ahg and 4tim), where EMR did not get complete prediction on the four templates. Sensitivity values at the specificity threshold of 0.95 were also calculated. The AUC values for the ROC curves are often adopted for the prediction performance evaluation. By changing the threshold, various specificity values were obtained. The AUC values are the average sensitivity values relative to all the specificity values, and are often used for the evaluation of prediction performance (Kato  et al. ,  2005 ). However, the evaluation with the AUC values has the following disadvantage. As shown in  Supplementary Table 3 , the LSS algorithm often yields many mismatches. In such cases, sensitivity at a low specificity tends to be pointless because it would be almost impossible to check hits in the lower order, when hits are checked in the order of highest to lowest. For this reason, the sensitivity at specificity 0.95 was adopted for the evaluation. The differences between EMR and MLR tended to be more remarkable for the sensitivity values at specificity 0.95 than for the AUC values. Average sensitivities are shown in  Figure 3 b. The average of the sensitivity was improved from 0.813 to 0.949. The change is statistically significant ( P -value of 1.27 × 10 −4 ). The individual sensitivities are shown in  Supplementary Table 5 . Except for three templates—2ace, 1ka1 and 2bvw—the sensitivity of EMR did not exceed that of MLR statistically significantly. Supplementary Figure 5  shows detailed results of the template made from the active site of 1jfh (α-amylase). This template comprises 13 atoms from three amino acid residues.  Supplementary Figure 5c  shows the frequency distribution of unweighted RMSD for the training dataset with equal weights for the 13 template atoms. The normalized distributions of 24 site matches and 6486 mismatches in the training dataset are shown in the figure, so that the sums of the frequencies for the site matches and mismatches could be 1.0 and 1.0, respectively. Here, site matches and mismatches could not be separated in the distribution of the unweighted RMSD data. The metric learning algorithm that used the 24 site matches and 6486 mismatches produced the weights for the atoms (see  Supplementary Fig. 5b  and h). The weighted RMSD, calculated using the weight values, is shown in  Supplementary Figure 5d , suggesting that the separation of site matches from mismatches could be improved. The distributions of the unweighted and weighted RMSD datasets for the evaluation, which were not used in the metric learning algorithm, are shown in  Supplementary Figure 5e  and f, respectively. Additionally, in the case of the evaluation data, site matches and mismatches were separated effectively. These data suggest that our metric learning algorithm can improve generalization capability without overfitting (Hastie  et al. ,  2003 ). Supplementary Figure 5g  portrays a box plot of the distribution of distances between the query template and each hit for each atom. Two atoms, ‘OD1 ASP A 197’ and ‘CB GLU A 233’, were particularly inseparable. The weight values for these two atoms turned out to be zero. Moreover, the remaining oxygen atoms gave small weights, probably as a result of the inseparable distribution between the site matches and the mismatches. Therefore, the metric learning algorithm can automatically select important atoms from the template atoms. 4.2 Effects of outer atoms We compared MLR-CI (Condition 3) with MLR (Condition 2), to investigate the effect of the constraint that the weights of the outer atoms be smaller than those of the inner atoms. However, we observed barely any improvement yielded by the additional constraints, as shown in  Figure 3  obtained from  Supplementary Tables 4 and 5 . Of the templates shown in blue italic in  Supplementary Tables 4 and 5 , 32 have outer atoms, and therefore might give different predictions. The information in the tables suggests that the constraint does not improve the prediction performance. The AUCs of MLR-CI were significantly worse than those of MLR for 19 templates, and the sensitivities of MLR-CI were worse for nine templates. Actually, MLR-CI achieves better AUCs in only three templates, and better sensitivities in only four templates. The rough template of 1map comprises 17 atoms from two residues. Thirteen atoms are inner atoms, whereas the other four atoms, N, CA, C and O of LYS 258, are the outer atoms, which are shown in gray in  Supplementary Figure 6h . In the distribution of distances for each atom ( Supplementary Fig. 6g ), despite the outer atoms, the separations of site matches and mismatches are good. For the inner atoms, CB, CG and CD1 of TYR 225, and CB of LYS 258, the separations are unsatisfactory, resulting in small weights that are nearly zero. In MLR-CE and MLP-CE experiments (conditions 4 and 7, respectively), the weights for the catalytically necessary atoms were set not to be smaller than those for any other atoms. This constraint could be a powerful prior knowledge for learning if it was true that the catalytically necessary atoms are useful for prediction. However, MLR-CE and MLP-CE did not show remarkable improvements. In fact, MLR-CE for 27 rough templates yielded significantly worse AUC than MLR did ( Supplementary Table 4 ). Furthermore, the sensitivities of MLR-CE were worse for 14 rough templates ( Supplementary Table 5 ). The MLR-CE improves AUCs for only six templates, and improves sensitivities for only five templates. Those results imply that, even if some atoms are catalytically necessary, they are not always important for prediction of active sites. Some concrete examples are presented below for illustration. In the case of the template 1arg (aspartate aminotransferase) ( Supplementary Fig. 4c ), the weight value of the OH atom of TYR 225, which is catalytically important, was large—0.075—compared with those of the atoms CD1, CD2, CE1 and CE2, which were nearly zero. This result suggests that the axis atoms of the phenyl group, CB, CG, CA and OH, might be fixed, whereas the atoms, CD1, CD2, CE1 and CE2, could be rotated along the axis atoms, or at least positioned differently, depending on the active sites of true positives. In contrast to the template 1arg, for the template 3daa ( d -alanine aminotransferase) ( Supplementary Fig. 4d ), the weight values for the atoms of TYR 31 do not vary. As for the weight values of acidic residues, the templates 1psa (pepsin) and 1qk2 (lysozyme) showed entirely different tendencies ( Supplementary Fig. 4a and b ). The weights for the oxygen atoms of ASP 32 and ASP 215 were nearly zero in the template 1psa, although the template 1qk2 gave larger weight values to the oxygen atoms of ASP 221 and ASP 401. In both cases, the oxygen atoms of the acidic residues are catalytically important and involved in enzymatic reactions. However, in the case of 1psa, the catalytical importance does not always affect the weight values in the prediction. These data suggest the following:
 The inner atoms, which are directly involved in catalytic reactions, are not always conserved from a structural viewpoint, although the structures of the outer atoms are more conserved than those of the inner atoms. The distribution of distances for each atom in the mismatches is important to separate site matches from mismatches. The atoms that can separate site matches from mismatches are as important in the prediction as the structurally conserved atoms. Although each template has different properties, metric learning automatically finds the effective combination of atoms that improves the prediction performance. 
 Torrance  et al.  ( 2005 ) also investigated whether functional atoms, which carry out catalytic function, can discriminate site matches from mismatches more effectively than mainchain atoms can. According to their analyses, templates based on protein mainchain positions are more discriminating than those based on functional atoms from sidechains because sidechain atoms are more flexible than the mainchain of a protein, especially in the presence of ligand (Torrance  et al. ,  2005 ). The inner atoms in our definition, which correspond to their functional atoms, can discriminate matches from mismatches less effectively than the outer atoms, which tend to be closer to the mainchain positions. Therefore, our results are apparently consistent with their results. Furthermore, other results that might support the importance of outer atoms for prediction were obtained. In the MLP method, only atoms that are involved in catalytic reactions directly are included in the templates. Therefore, no outer atoms are incorporated into the computation for prediction. Although the MLP method is not disrupted by irrelevant information from outer atoms, MLP does not always achieve superior sensitivity to MLR. The AUCs and the sensitivities of MLP were significantly worse than those of MLR for 19 templates and 10 templates, respectively. The AUCs and the sensitivities of MLP were better for only three templates—2ace, 1rpa and 1vcz—which implies that it is not necessary to remove outer atoms in advance for most cases, and that it would be a better approach to use the metric learning algorithm to remove irrelevant atoms automatically. 4.3 Effects on residue selection The results shown so far suggest that our algorithm selects predictive atoms in templates automatically. Even if users use rough templates, the residues that compose templates still must be selected manually. Our experiments adopt the TESS algorithm (Wallace  et al. ,  1997 ) as an LSS algorithm. The algorithm searches for local structures that have the same residue types as those of the template. Therefore, because no residue in the local structures, which should be hit as the site matches, can be matched with the unrelated residue in the template, the LSS algorithm can miss the site matches if an unrelated residue is added to the template. In contrast, if a necessary residue is removed, the algorithm will pick up many mismatches.  Supplementary Table 2  presents an example that shows how the selection of residues affects the prediction performance. The template 1acb contains four residues: HIS 57, ASP 102, GLY 193 and SER 195. A new template was created by removing the atoms in the residue GLY 193. Removal of the atom yields many mismatches: from 6300 to 19 066. Common hits to the previous four-residue template and the new three-residue template are used to investigate the effects of residue selection because it is necessary to analyze the same dataset for comparison of these performances. The MLR slightly reduced the sensitivity from 0.998 to 0.994. Actually, EMR is also degraded by removal of a residue, but the changes in sensitivity are much larger. Consequently, the performance depends on selection of residues, but the change in MLR is small compared with that in EMR. It should also be noted that selection of residues is much easier than selection of atoms. 4.4 Effects on re-superimposition In determining the metric, the parameters of the rigid-body transformation are fixed. One could also superimpose the template again and predict the sites with the obtained weights. The ‘re-superimposition’ approach was tested to be compared with the original ‘single superimposition’ approach. The re-superimposition approach slightly reduced the average AUC from 0.984 to 0.977, and the average sensitivity from 0.949 to 0.934. The  P -values of the differences are 0.004 and 0.021, respectively. The slight degradation of the performance may be because both the approaches optimize the metric for the first superimposition, not for the second superimposition, although the re-superimposition approach uses the metric for the second superimposition. 5 CONCLUSIONS This article presents a new algorithm that learns the metric to assess data obtained by LSS algorithms and discriminate site matches from mismatches. We design the parameterization for the metric so that the parameters can be interpreted directly as weights of atoms. An advantage of our algorithm is that redundant atoms are removed clearly by making those weights zero. This characteristic is obtained using the definition of the domain of the weight parameter. The domain is the probability simplex, which plays the role of ℓ 1 -regularization (Hastie  et al. ,  2003 ). Some literatures (Yu  et al. ,  2010 ) replace ℓ 1 -regularization with another regularization. In our algorithm, the ℓ ∞ -norm of the weight vector is forced to be bounded from above to improve the generalization performance. This is equivalent to combination of ℓ 1 -regularization with ℓ ∞ -regularization. As described in this article, we reported the results of family analyses that searches for active sites for one template. We are now developing an algorithm for library analyses that predicts the function of specified structures using a set of templates. 
          Supplementary Material 
          
             
                Supplementary Data 
             
             
             
          
       </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Database indexing for production MegaBLAST searches</Title>
    <Doi>10.1093/bioinformatics/btn322</Doi>
    <Authors>Morgulis Aleksandr, Coulouris George, Raytselis Yan, Madden Thomas L., Agarwala Richa, Schäffer Alejandro A.</Authors>
    <Abstract>Motivation: The BLAST software package for sequence comparison speeds up homology search by preprocessing a query sequence into a lookup table. Numerous research studies have suggested that preprocessing the database instead would give better performance. However, production usage of sequence comparison methods that preprocess the database has been limited to programs such as BLAT and SSAHA that are designed to find matches when query and database subsequences are highly similar.</Abstract>
    <Body>1 INTRODUCTION BLAST (Altschul  et al. ,  1997 ) is a package of programs to match a query biological sequence against a database and identify database sequences that have statistically significant local alignments with a part of the query. One of the principal heuristics that makes BLAST fast is preprocessing the query to build a lookup table that is subsequently used to find short high-scoring ungapped alignments, herein called ‘seeds’. Seeds can be extended to longer ungapped alignments and then to full gapped alignments in later phases of BLAST programs. The default length for seeds is 3 amino acids for proteins, 11 nucleotides for high-sensitivity nucleotide searching with the BLASTN module and 28 nucleotides for lower sensitivity searching with the MegaBLAST module (Zhang  et al. ,  2000 ). An important innovation introduced in BLAST version 2.0 was to require the existence of two nearby seeds to reduce the number of candidate matches that pass the seeding stage (Altschul  et al. ,  1997 ). For proteins, recent work shows that requiring a single contiguous seed of length 6 and more complex preprocessing gives better performance (Shiryev  et al. ,  2007 ). Instead of finding seeds by searching a data structure derived from the  query , one could instead find seeds by searching a data structure derived from the  database . Numerous research studies, recently reviewed in Jiang  et al.  ( 2007 ), suggest that preprocessing the database and searching a database-derived data structure can yield much faster search times. Two widely used software packages that do preprocess the database for biological sequence comparison are SSAHA (Ning  et al. ,  2001 ) and BLAT (Kent,  2002 ). However, both these packages are defined to find only near identical matches [e.g. for a comparison of BLAT and the TBLASTN module of BLAST see Gertz  et al.  ( 2006 )]. The only software package we could find that:
 produces results similar to or identical to some module of BLAST, preprocesses the database, rather than the query, to build a data structure for the seed search phase, uses comparable amounts of memory for the data structure and the database (to exclude suffix-tree solutions) and has executables or source code currently and freely available on the web 
is miBLAST (Kim  et al. ,  2005 ). miBLAST was designed for short queries of under 100 bases and we confirm that its relative performance does deteriorate for longer queries. Below, we refer to methods, such as miBLAST, that build a search structure from the database by the collective term ‘database indexing’. Since database indexing seems so promising, but no module of NCBI BLAST uses this paradigm, we set out to engineer a replacement for (at least) one module of NCBI BLAST and assess whether the claimed performance benefits could be achieved in production usage. The most commonly searched database at NCBI's BLAST Web service is  nr , which is a comprehensive ‘non-redundant’ collection of sequences. Indexing of the  nr  database poses specific challenges as compared with other databases because  nr  is so large and is updated daily. Other popular databases for web BLAST searches are the human and mouse genomes, and to a lesser extent other vertebrate genomes. Due in part to a redesign of NCBI's BLAST web page in 2007, single-genome databases for human and mouse have become increasingly selected by users who submit approximately 10 000 and 3000 queries per weekday for the human and mouse databases, respectively. The current default is that the genome-specific nucleotide databases are searched with the MegaBLAST (Zhang  et al. ,  2000 ) module, called from within the program  blastn  compiled from the NCBI C++toolkit. Therefore, we set out to develop a new version of MegaBLAST that could use a data structure derived from the genome-specific database to search for the initial seeds. We also needed to build a program, herein called  mkindexname , to construct the index structure. The basic goals for ‘indexed MegaBLAST’ were that it should find the same matching sequences and alignments as the version from which we started, and do so much faster for the majority of queries that arise in practice. Unlike miBLAST, performance of indexed MegaBLAST should degrade gracefully as the queries become longer or have an increasing number of matches. Early in the project, we realized that a basic impediment to good performance would be queries that align well to DNA sequences present with minor variations many times in the genome. To address this difficulty, we developed the software package WindowMasker (Morgulis  et al. ,  2006a ) to quickly mask frequently repeated sequences without using a library of repeats. We realized that most users rarely want to see such repetitive matches output from BLAST, so we modified MegaBLAST to be capable of searching a ‘soft-masked’ database. Soft-masked means that seeds cannot intersect a masked interval, but alignments can be extended into and through masked intervals. Soft masking a query has long been available in MegaBLAST. Sections 2  and  3  and Supplementary Material describe how we engineered and tested indexed MegaBLAST to document the potential performance improvement. The acid test is how users of NCBI's web BLAST service would respond. Indexed MegaBLAST was deployed in October 2007 for the BLAST nucleotide search variants of querying the human and mouse genomes (separately) and announced in regular release notes. By default, WindowMasked versions of the genomes are used, but users can turn off masking. There have been zero user complaints and only one general inquiry suggesting that indexed MegaBLAST works well in production. 2 METHODS In this section and Supplementary Material, we describe the data structures used to organize the database index and the ‘seed search algorithm’ used to find initial identical substrings in the query and the database. We also describe the testing strategy and how indexed MegaBLAST has been put into production usage. The seed search algorithm has been incorporated into a modified version of NCBI's MegaBLAST, as described subsequently, with few changes to the algorithmic code parts that do subsequent processing of the seeds. 2.1 Index structure The MegaBLAST database index contains compressed sequence data along with locations of  k -mers. Besides the length  k , two other important parameters are the minimum seed length  w (≥ k ) and the stride  s  used to move through the database. To guarantee that every exact match of length  w  between a query and a subject is found, we use the relationship  s = w − k +1. Information about the  k -mer ending at every  s -th position, satisfying conditions specified below which assure we can find all seeds, is stored in the index. The current implementation defaults to  w =16,  k =12 and  s =5. A database index is composed of several files called index volumes, each corresponding to a contiguous range of sequences from the underlying database. An index volume file contains three sections: header, sequence data and offset data, as illustrated in  Figure 1 . The number of nucleotides in the database is denoted by  n . In Supplementary Material, we derive an estimate for the number of bytes needed to store the index as:
 (1) 
bytes. For example, for an unmasked database of size 1 GB and our default values  k =12,  s =5, the estimated size of the index would be ∼1.175 GB. Fig. 1. Schematic of the data structure used for the database index. 2.1.1 Header The header section of an index volume primarily stores the range of sequences from the underlying database that are indexed by that volume. The header section also stores a few auxiliary values that are used for housekeeping purposes, such as index format version. The exact definition of the current version of the index format can be found in the external documentation accompanying the code. 2.1.2 Sequence data Sequence data are stored in compressed format with 2 bits per sequence base using NCBI2NA encoding. In that encoding, bases A, C, G and T are encoded as integers 0, 1, 2 and 3 correspondingly. Positions containing ambiguous characters are encoded as 0. The seed search algorithm, described in the next subsection, never touches ambiguous parts of the sequences. These parts serve merely as placeholders. The seed search algorithm works with  logical sequences  of more or less uniform size. This makes mapping from  k -mer locations to the corresponding sequences more efficient. Logical sequences may be formed by splitting long sequences or by combining several short sequences, as described subsequently. The mapping between the logical sequences and the actual database sequences is stored in the sequence data section of an index volume. All logical sequences participating in a given index volume are assigned a consecutive integer  sequence id . Both in indexed MegaBLAST and other variants of NCBI BLAST software, long database sequences are split into overlapping  chunks  to optimize processing. From now on, by  input sequence  we mean either a database sequence or a chunk produced by such splitting, which enables us to avoid cluttering the algorithm description with the details of handling long sequences. Mapping of input sequences to logical sequences.  Let ℓ be the chunk size defined in the BLAST code (current default is 5 Mbases), and let  B =⌈log 2 (ℓ)⌉. Because of the chunking, we may assume that all input sequences are of length at most ℓ. Consecutive input sequences are concatenated into logical sequences as long as their combined length does not exceed ℓ. Logical sequences are assigned consecutive integer  logical sequence ids  starting at 0. The sequence data section of an index volume contains a table that maps each logical sequence id to the input sequences from which it is composed. Each element of the table contains four integers: sequence id of the first input sequence in the logical sequence; sequence id of the last input sequence in the logical sequence; offset (in bytes) of the start of this logical sequence from the start of the sequence store and offset (in bytes) of the end of this logical sequence from the start of the sequence store. Sequence store.  The sequence store is the part of the sequence data section of an index volume that contains compressed sequence data. Logical sequences are stored there in order of increasing logical sequence id. Logical sequence data contains concatenated encoded constituent input sequences. The encoding of input sequences using 2 bits per base is padded with extra 0 bits at the end, if needed, to ensure that they end on a byte boundary. The index volume stores the true lengths of all input sequences, so the seed search algorithm will not treat the padding 0 bits as part of a nucleotide encoding. 2.1.3 Offset data The offset data section of an index volume consists of two parts: the lookup table and the offset lists. The lookup table contains 4 k  entries. The  i -th entry contains the pointer to the start of the list of positions in the database called the  offset list , where the  k -mer with value  i  occurs. Offset lists come after the lookup table in the data structure. Individual lists are terminated by a sentinel 4-byte word encoding 0. Our method of offset encoding ensures that 0 is not a valid value for a list entry. Positions of  k -mers can be added to the offset lists in the order of their appearance in the underlying database, but in practice, the order is permuted for faster searching, as described in Supplementary Material. To identify which  k -mer positions to store, the set of  seed-eligible intervals  is computed for each input sequence. A  seed-eligible interval  is a subsequence of an input sequence that does not contain ambiguities or lower case (masked) letters. A  k -mer is stored in the index if the following three conditions hold: the  k -mer is fully contained within a seed-eligible interval; the seed-eligible interval is at least  w  nucleotides in length and the offset of the last letter of the  k -mer relative to the start of the corresponding logical sequence is divisible by the stride,  s . Let  s 2 =2 ⌈log 2 s  ⌉ ; i.e. the smallest power of 2 greater than or equal to  s . For a  k -mer the actual value  o  stored in the offset list is computed in the following way:
 
where  h  is the logical sequence id, ⌈2 B / s ⌉ is the number of bits used to encode the position of a  k -mer within its logical sequence at stride  s  and  p  is the offset of the last letter of the  k -mer relative to the start of the logical sequence.  o  is stored as a 32-bit wide unsigned integer. Special offsets and prefixes.  For some  k -mer instances, extra information is stored along with its encoded position. Let  d l  ( d r ) be the distance from the first (last) base of the  k -mer to the left (right) end of the corresponding valid interval. We call a  k -mer instance  special  if either  d l  &lt; s , or  d r  &lt; s , or both. We define  u l  ( u r ) to be equal to  d l  ( d r ) if  d l  &lt; s  ( d r  &lt; s ) and 0 otherwise. Special prefixes are used in the offset lists by the seed search algorithm to ensure that reported seeds never cross the boundary of a seed-eligible interval. For special  k -mer instances, an extra 32-bit prefix value  o s  = s 2 u l + u r  is stored in the offset list immediately prior to their encoded position  o . It is always the case that  o s  &lt; s 2 2  and  o  ≥ s 2 2 , so the prefix is easy to distinguish from the actual encoded position. An example for offset entries.  Consider the following subsequence from the first contig NT_019273.18 of the masked human genome starting at offset 8 500 000 (with the first base at offset 0): gagaggACAACACTTAAAGGTTCAACTAGCAATA With (default) values of  k =12,  s =5, chunk size 5 Mb, and chunk overlap of 100 bp, base at offset 8 500 000 in the input is in the second logical sequence and has offset 3 500 100 in its logical chunk. The 12-mers at offset 3 500 100 (gagaggACAACA) and 3 500 105 (gACAACACTTAA) are partially masked. Therefore, they are not part of any seed-eligible interval and are not added to the lookup table offset lists in the index structure. The next 12-mer considered for addition to the lookup table is at offset 3 500 110 (CACTTAAAGGTT), and that is in a seed-eligible interval. This is also a special offset as the distance to its seed-eligible interval boundary to the left is only four. The 12-mer at offset 3 500 115 (AAAGGTTCAACT) is also in a seed-eligible interval, but it is not a special offset. Computations for CACTTAAAGGTT in the subsequence above resulting in the entries in the index structure for masked human genome are: (i) lookup table entry index of 4 702 383 using the bit encoding for the four bases, (ii) special offset value of 32 using the formula 2 ⌈ log 2 5⌉ · 4 + 0, and (iii) offset value of 2 377 873 using the formula (2 ⌈log 2 5 ⌉) 2  + 3 500 110/5 + 1 · ((2 ⌈  log 2  5 ⌉) 2  + ⌈ 2 ⌈ log 2 (5 000 000)⌉/5 ⌉ + 1). The special offset value of 32 is followed by the offset value of 2 377 873 in the list of offsets for lookup table entry index 4 702 383. 2.2 Approach to testing 2.2.1 General setup Database indexing functionality is implemented in the NCBI C++toolkit via the executables  blastn  and  mkindexname . For testing, we used statically linked versions of  blastn  and  mkindexname  built from the toolkit sources of September 5, 2007. The executables were built for the 32-bit Intel x86 architecture under Linux OS kernel version 2.6.5 using GCC v4.0.1 compiler. The executables and some documentation are available in the directory:
 ftp://ftp.ncbi.nlm.nih.gov/pub/agarwala/indexed_megablast All tests were performed on a Dual Intel Xeon 5160 machine running at 3 GHz (two dual-core CPUs) with 8 GB of RAM. All runs were performed in single-thread mode. When measuring the running time of a program, three runs were performed and the median time was recorded, except for the large-scale test. The standard UNIX  time  utility was used to measure total running time. To measure the time used to search for seeds,  blastn  was instrumented with checkpoints that measured the time of seed searching procedures using Linux  gettimeofday  system call. Instantiations of the following command line, including some variables, were used to run  blastn  for different tests. - db   &lt;database&gt;  specifies the name for BLAST database created using  formatdbname  utility.  formatdbname  is a part of NCBI BLAST software distribution. - task megablast  selects MegaBLAST module. - outfmt 6  is used to choose the tabular output format. - use_index  allows to choose whether to use database indexing functionality. - index_name   &lt;index&gt;  is optional and only needed when using database indexing functionality.  index  in this case is the base name of the database index volumes created via the  mkindexname  utility that sets up the data structures described in Supplementary Material. For example, if the index volumes are named  dbi.00.idx ,  dbi.01.idx , …, then the value of  index  should be  dbi . - query   &lt;query&gt;  specifies the name of the FASTA formatted file containing the query sequence. - filtering_db   &lt;osr_db&gt;  is optional and is used to enable masking the query for organism specific repeats. In this case  osr_db  is the name of the database containing the repeats. Runs in which - use_index  is set to  false  are referred to as either ‘baseline’ or ‘non-indexed’. Databases.  The tests were run against human build 36 and mouse build 36 contig genome databases. The indices were created from unmasked databases and databases masked with WindowMasker (Morgulis  et al. ,  2006a ), including low-complexity filtering by DUST (Morgulis  et al. ,  2006b ). Each database was split into several volumes, so that each volume index was ∼1 GB in size, or less for the last database volume. The runs were done for each volume individually, and the sum over all volumes was recorded as the running time. Queries.  Queries were selected with a focus on human and mouse because a central goal was to use indexed MegaBLAST in the production NCBI web service to search the nucleotide sequences of these two genomes. For each organism, four query sets, each containing 100 queries, were used to test the performance of MegaBLAST. For production testing, we used 100 queries from human only. We call these query sets qsmall, qmedium, qlarge, quser and qprod. To form qsmall, qmedium and qlarge, sample queries for three approximate sizes were randomly selected from within bacterial artificial chromosome (BAC) sequences from the genome being queried. The sizes are: small (∼500 bases, range: 501–506), medium (∼10 Kbases, range: 10000–10446), and large (∼100 Kbases, range: 100001–102087); 100 queries of each size were selected for each of human and mouse. The query sets Qsmall, Qmedium, Qlarge are available in subdirectories of  ftp://ftp.ncbi.nlm.nih.gov/pub/agarwala/indexed_megablast/queries The fourth set of queries, quser, was created by first collecting real user submissions over a period of one week to the NCBI BLAST web service, with the restriction that the user clicked the link for querying the human genome or the link for querying the mouse genome. For each organism, 100 queries were selected at random from among these real user queries. When preparing quser, we did not take note of any information about the user submitting the query, only the query sequence and parameter settings. Unlike the other query sets we used, Quser cannot be made public, so as to protect the confidentiality of users and usage of the NCBI Web BLAST service. For production tests, we randomly selected 100 human sequences from the nt database ( ftp://ftp.ncbi.nih.gov/blast/db/ ) as our query set, and we call this set qprod. The 100 query sequences ranged in size from 29 bases to 181 166 bases. The 50th percentile query length was 1044 bases and the 75th percentile query length was 2120 bases, roughly reflecting the length distribution of DNA queries submitted to the NCBI web pages. 2.2.2 Comparison of indexed MegaBLAST with miBLAST We compared the performance of miBLAST (Kim  et al. ,  2005 ) and NCBI MegaBLAST with indexed database support by running qsmall and qmedium against the unmasked database comprising contigs on human chromosomes 1 through 5. These chromosomes were selected because the size of the database is ∼1 GB. For all runs, the queries were masked using DUST algorithm (Morgulis  et al. ,  2006b ), but not masked for organism-specific repeats. The comparison to miBLAST is imperfect because of differences in the intended usage and engineering of miBLAST and indexed MegaBLAST. Some comparison on usage where functionality is similar provides more useful information than showing no comparison at all. When comparing against miBLAST, we used qsmall and qmedium but not qlarge, because miBLAST is intended only for short queries. Conversely, indexed MegaBLAST includes some compromises in data structure design that damage its performance on Qsmall slightly, so that its performance on longer queries is competitive with or better than baseline MegaBLAST. miBLAST uses the BLASTN module of BLAST to compute alignments, which means that it takes longer and finds more imperfect local alignments than MegaBLAST. To quantify this tradeoff, we ran miBLAST and indexed MegaBLAST on a large-scale test of 10 000 50mer queries extracted from human chromosomes 1-5. Because the queries are from the sequences in the database, one might expect each query to yield at least one perfect, full-length alignment, but no perfect match is reported for some queries because of the use of DUST filtering. 2.2.3 Comparison of stand-alone indexed and non-indexed MegaBLAST In all cases, each command line call to  blastn  involved one query. In other words, the query concatenation feature of  blastn  application was never used, so that query length can be used as a parameter in evaluating performance. Recent versions of  blastn  also support splitting of long queries. All queries used in the tests were short enough so that no query splitting happened. To compare how the query length and the number of results affected the performance of the indexed database search compared to non-indexed case, the query sets qsmall, qmedium and qlarge were run against unmasked indexed databases. No additional query masking was applied for these tests. 2.2.4 Comparison of indexed and non-indexed MegaBLAST in a production environment The NCBI BLAST web service utilizes a custom queueing system ( ftp://ftp.ncbi.nih.gov/blast/documents/blast-sc2004.pdf ) to process search requests and present results to the user. Depending on the database size, a database may or may not be split into smaller chunks that can be searched in parallel on worker nodes. Similarly, long queries can be split and short queries, even submitted by different users, can be combined into a single task. The system encourages cache reuse by steering tasks to worker nodes that have recently performed similar searches. While traditional MegaBLAST searches are parallelized and run in this manner, indexed MegaBLAST searches are run in serial on dedicated machines. A script runs periodically on these dedicated machines to ensure that the entire index resides in memory. We used a script ( http://www.ncbi.nlm.nih.gov/blast/docs/web\blast.pl ) to submit one indexed and one non-indexed search for each query in qprod against the human genome, allowing 2 s between submissions. Reflecting the defaults of the NCBI BLAST web pages, the non-indexed queries were masked for low complexity regions and human repeats. The indexed queries were not masked, since the index incorporates masking information. Times recorded were the start-to-finish wall-clock time for each run, where each non-indexed search was spread over 10–20 worker nodes and the time reported does not combine wall-clock times for individual nodes. 3 RESULTS To make BLAST databasse indexing feasible in production, a first requirement is that the amount of memory used by the index data structures should be reasonable.  Table 1  shows the sizes of the index volumes in relation to the actual database length. The data are provided for the databases used for timing tests described in  Sub sections 2.2.3  and  2.2.4  as well as for a shorter  Drosophila  genome. The indices were created using  s =5 and  k =12. The last column of the table shows the upper bound provided by formula 1. The bound is pretty tight for unmasked databases. It is less tight in the masked case, because it does not take into account the actual number of unmasked bases.
 Table 1. Database index size for different source databases Source database Database Index size Bound size (Mbp) (Mbytes) (Mbytes) Drosophila , unmasked 116.78 229.60 250.62 Drosophila , masked 116.78 204.74 250.62 Human chr. 1-5, unmasked 1025.20 1197.79 1204.46 Human chr. 6-13, unmasked 1077.86 1254.13 1259.75 Human chr. 14-Y, unmasked 767.77 926.56 934.16 Human chr. 1-8, masked 1493.03 1229.81 1695.68 Human chr. 9-Y, masked 1377.79 1137.62 1574.68 Mouse chr. 1-7, unmasked 1140.61 1297.20 1325.64 Mouse chr. 8-16, unmasked 1074.64 1222.98 1256.37 Mouse chr. 17-Y, unmasked 428.82 538.65 578.26 Mouse chr. 1-10, masked 1526.66 1258.19 1730.99 Mouse chr. 11-Y, masked 1117.42 932.09 1301.29 Windowmasker software (Morgulis  et al. ,  2006a ) was used to generate masked databases. Windowmasker was run with default parameters and low complexity masking enabled. The bound column shows the upper bound calculated from formula 1. A second requirement is that the amount of preprocessing time needed to create the index data structures should be reasonable. We measured the time it takes to create human unmasked, human masked, mouse unmasked and mouse masked indices as 1319.11 s, 1000.67 s, 1176.56 s, and 911.27 s, respectively. For each database, the time in the previous sentence is the sum of index volume creation times taken over all volumes of the database. Having shown that the data structures for database indexing can be created in a reasonable amount of memory and time, we proceed to evaluate the time for searching, especially the time for finding seeds. The performance advantage or disadvantage of indexed seed search, by which we mean the seed-finding phase of BLAST with an indexed database, depends on the size of the seed candidate lists (see Supplementary Material). When the database (or the query originating from the corresponding genome) is masked for repeats and low complexity regions, then the seed candidate lists are likely to be short. However, for an unmasked database and query, the indexed search may actually have worse performance compared to the non-indexed case, if the query contains highly repetitive regions. 3.1 Comparison of indexed MegaBLAST and miBLAST Table 2  summarizes a performance comparison of miBLAST and indexed MegaBLAST. miBLAST performs best on very short queries; the query length of 500 bp used in qsmall query set is on the upper end of the useful query length range for miBLAST. Since the performance of indexed MegaBLAST depends on the total number of matches,  Table 2  contains separate rows for queries producing less than 5000 results and for the ones producing at least 5000 results. The performance of indexed MegaBLAST is better than that of miBLAST by at least 2.5 times in all cases except for the queries from qsmall producing over 5000 results. In the latter case, the performance advantage of miBLAST is under 12%. The rightmost columns show that indexed MegaBLAST is faster than miBLAST on the vast majority of queries considered.
 Table 2. Performance comparison of miBLAST and indexed MegaBLAST Set No. of results Time (s) Faster MI MB MI MB Tied Qsmall &lt;5000 170.20 23.21 0 89 0 ≥ 5000 161.11 182.72 10 1 0 Qmedium &lt;5000 129.16 39.26 0 17 0 ≥ 5000 15340.13 4237.12 6 75 2 The time represents the sum of median results per query. In this table and in  Table 3 , the number of results refers to the number of alignments reported, not the number of database sequences with at least one reported alignment. The queries are grouped based on the number of results. The three rightmost columns count the number of queries for which either software package was faster or the running times were considered a tie. The two running times for a query were considered a tie, if the time difference is &lt;0.1s. On a large-scale test of 10 000 50-mer queries, miBLAST required 76 times more time (20 h and 25 min versus 16 min) to find 22 times more local alignments (122.7 million versus 5.5 million). However, indexed MegaBLAST found a perfect length 50 match for 9648 queries, while miBLAST found a perfect match for 9269 queries. The superior performance of indexed MegaBLAST in finding perfect matches is unexpected, and the description of miBLAST in Kim  et al.  ( 2005 ) does not suggest to us why a perfect match should be missed by miBLAST on ∼4% of queries, where it is not blocked by DUST filtering. The index structure used by miBLAST differs fundamentally from that described here in that for each  k -mer, the miBLAST index stores only the sequence identifiers containing that  k -mer but not the offsets. As demonstrated by Kim  et al.  ( 2005 ), the resulting space savings can be advantageous when queries are short. However, as query length grows, the fraction of database sequences that contain a  k -mer of the query grows closer to one. Therefore, the fraction of database sequences that have to be considered for alignment to the query, which Kim  et al.  call the ‘filtration ratio’ grows closer to one and the relative performance of miBLAST deteriorates. 3.2 Comparison of stand-alone indexed and non-indexed MegaBLAST Table 3  compares the performance of baseline and indexed searches. Queries are categorized depending on whether they yield a small number (&lt;5000) of result alignments or a large number. In the former case, indexed seed search is 5–9 times faster than non-indexed search. In the latter case, however, indexed seed search is slower due to more cache misses. When the number of seeds is very large, the proportion of time spent in seed search is small; almost all time is spent in seed extension procedures. As a consequence, the relative decrease in total running time is rather small—between 0% and 8%.
 Table 3. Running time in seconds for baseline and indexed versions of MegaBLAST in the case of the unmasked human genome database Set &lt;5000 results ≥5000 results Count Baseline Indexed Count Baseline Indexed Qlarge 1 5.08 3.49 61 44173.96 43888.14 1.46 0.29 961.24 1396.93 Qmedium 13 82.17 71.95 87 13009.04 13325.62 24.06 6.29 305.10 745.59 Qsmall 88 118.05 56.51 12 528.85 570.84 68.19 7.24 13.70 65.86 For each query set the top row contains the number of queries in the corresponding group and the total search time. The second row shows the time taken by the seed search phase only. For 38 queries in Qlarge, at least one version of MegaBLAST ran out of memory due to the large number of results. For masked databases, indexed MegaBLAST is consistently faster than baseline across all query sets, as shown by  Table 4 . In these runs, the baseline search is not performed with the query unmasked because baseline MegaBLAST cannot utilize database masking information. Masking of the query results in a small number of seeds in both the indexed and baseline runs, making the amount of time spent in later processing of seeds and alignments comparable. The performance advantage is achieved partly because in the indexed run, a substantial fraction of the masked database is not scanned for potential seeds. The decrease in total running time is also more substantial in the masked case because the extension procedures have much fewer seeds to process.
 Table 4. Running time in seconds for baseline and indexed versions of MegaBLAST in the case of masked genomes and query masked (indicated by yes) or unmasked (indicated by no) Human masked Mouse masked Set Baseline Indexed Baseline Indexed Yes Yes No Yes Yes No Qlarge 1097.11 511.76 418.31 1702.35 736.74 613.17 447.42 108.32 127.60 447.50 98.71 114.07 Qmedium 266.43 90.80 95.36 437.45 146.63 140.94 176.21 23.97 28.64 162.15 22.82 25.63 Qsmall 104.79 27.24 26.44 148.98 64.93 67.76 76.83 5.27 5.33 70.10 5.32 5.11 Quser 144.06 46.12 NA 164.21 82.82 NA 93.49 8.27 NA 83.18 7.01 NA For each query set the top row is the total search time, and the second row is the time taken for the seed search phase only. NA—not applicable. For masked databases, indexed MegaBLAST searches with the query also masked for repeats were done to evaluate if masking the query makes sense when the database is masked already. The results ( Table 4 ) show that although the seed search phase is somewhat faster in the case of masked queries, the total running time may be worse. This anomaly occurs because masking the query involves calling a separate BLAST search against a repeats database and the overhead of doing that actually offsets any performance benefit due to reduced number of seeds. 3.3 Comparison of indexed and non-indexed MegaBLAST in a production environment Figure 2  presents run times for 100 indexed searches and 100 non-indexed searches in a production setting, with the exception that the compute cluster used for this test was not being used for other tasks. Time reported is for the second of the two searches done for each query and each method. The average wall-clock time was 0.5 s for an indexed search and 0.8 s for a non-indexed search. The 75th percentile times offer a better measure of the user experience; this was 0.38 s for an indexed search and 1.37 s for a non-indexed search. Times are plotted against logarithm of query size to show that indexed search is generally faster for shorter queries and non-indexed search is faster for longer queries.
 Fig. 2. Wall-clock times for 100 indexed and non-indexed searches in a production setting, as a function of logarithm of query length. Considering times to be tied if they are within 0.01 s, indexed search is faster 75 times, non-indexed search is faster 19 times, and they tie on 6 queries. Indexed search is faster on shorter queries and slower on the longest queries. Since the non-indexed search uses 10–20 nodes while the indexed search uses one node, non-indexed search may use a lot more total CPU time than indexed search even when the non-indexed wall-clock time is less. So long as the dedicated machines for indexed search can keep up with incoming MegaBLAST queries to the human and mouse genomes, users perceive a much faster response time for the genome-specific queries because of the combined effects of faster runs and no competition from queries to other databases. Quantifying the benefits to throughput for all types of queries is difficult, since the workload for the dedicated and shared machines is variable over time. 4 DISCUSSION We presented a new implementation of the seed search phase of MegaBLAST (Zhang  et al. ,  2000 ) in which seeds are found by searching an index structure of  k -mers derived from preprocessing the database. We showed that this ‘indexed MegaBLAST’ is faster than the ‘baseline MegaBLAST’, which preprocesses the query, in most cases and especially for masked databases. When indexed MegaBLAST is slower because there are too many seeds, performance degradation is limited enough that the code can be used in production. Indexed MegaBLAST satisfies the requirement that the outputs are identical or nearly identical to baseline MegaBLAST, so long as all options are set consistently. In addition, usage of the preprocessor program  mkindexname  is being tested at NCBI for other projects such as alignment of short sequences generated by ‘next-generation sequencing technologies’ to a reference genome. Undertaking this project was positively influenced by several research papers suggesting that preprocessing the database for initial lookup could give faster search times than preprocessing the query. We decided to put our effort into a relatively simple data structure design, the seed search phase of the MegaBLAST module, and support for masked databases, so as to get some use of database indexing into production BLAST usage more quickly. Now that this overarching goal has been achieved, there are many possibilities to extend and improve our implementation. First, our implementation could be extended to the BLASTN module by changing the strides and  k -mer lengths. To get good performance, some alternative data structure for long lists may be necessary, as discussed subsequently. Extending our work to protein searching would be much more challenging due to the increased alphabet size. Williams and Zobel (2002) implemented a research prototype for nucleotide and protein searching, called CAFE, in which they preprocessed the database into an inverted index. They demonstrated quite impressive performance for CAFE, but to get that performance, they substantially changed the searching methods and scoring used in phases of the homology search beyond the initial word lookup. Thus, to make a production version that replaces some BLAST module would be more difficult than the seed search phase replacement we carried out. There are some possible algorithmic improvements to our implementation that could be tried without a major redesign. We summarize two of these possible changes. Instead of using a uniform stride (by default  s =5) when selecting  k -mer occurrences to put in the index, one could stride by a non-uniform increment in such a way that every potential seed (default length 28) is adequately represented by one or more  k -mer substrings. The stride at each position could be chosen so that the  k -mers whose occurrences are in the index are as infrequent as possible. This would make the  k -mer lists longer but speed up the seed search phase. A preliminary assessment of the human genome suggested that using non-uniform strides could simultaneously increase the length of the  k -mer lists by 9% and shorten the weighted average length by 25%; ‘weighted average’ means that each  k -mer is weighted by its number of occurrences in the genome. Using non-uniform strides would make the preprocessing take longer, but we do not consider database preprocessing time as a primary measure of performance, since preprocessing is done outside MegaBLAST. Another possible improvement is that long singly linked lists of  k -mers could be replaced by a more sophisticated structure such as a hash table or some sort of tree (Giladi  et al. ,  2002 ). Possible improvements suggested by research implementations of database indexing, which require a major redesign, include the following. Using ‘ q -gram filtering’, which considers the number of  k -mer matches within a region of dynamic programming alignment matrix, can provide a more stringent filter for candidate matches than single seeds (Rasmussen  et al. ,  2006 ). The database sequences could be transformed into data feature vectors as proposed in Lee  et al. ( 2007 ). In the same spirit as the  k -mer reordering we described under optimizations already implemented (Supplementary Material), some subset of ‘pier’  k -mers could be stored in a smaller, more rapidly accessible data structure as proposed in Cao  et al.  ( 2004 ). Finally, the sequence representation implemented in MICA (Stokes and Glick,  2006 ) could allow for a space efficient way to store database indices for smaller values of  k  [Stokes and Glick ( 2006 ) state  k =7 as the effective upper limit for  k ]. Such indices could be used for more sensitive searches currently provided by BLASTN. In sum, one part of NCBI's web BLAST service has been using indexing of the database rather than indexing of the query in the seed search phase since October 2007. Production usage of indexed MegaBLAST and the tests herein validate the performance improvements that have been claimed for database indexing. Further improvements to our implementation, possibly requiring a major redesign, may be achievable. 
          Supplementary Material 
          
             
                [Supplementary Data] 
             
             
             
          
       </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>CorGAT: a tool for the functional annotation of SARS-CoV-2 genomes</Title>
    <Doi>10.1093/bioinformatics/btaa1047</Doi>
    <Authors>Chiara Matteo, Zambelli Federico, Tangaro Marco Antonio, Mandreoli Pietro, Horner David S, Pesole Graziano, Valencia Alfonso</Authors>
    <Abstract/>
    <Body>1 Introduction The recent outbreak of COVID-19 has underlined the importance of rapid and effective sharing of molecular data for combating the spread of human pathogens and tracing possible routes of infection. At present, more than 200 000 genomic SARS-CoV-2 sequences have been deposited in dedicated repositories ( Shu and McCauley, 2017 ) along with associated metadata. Harnessing this wealth of information to identify functionally relevant genomic changes and/or recognizing the emergence of novel viral strains is of pivotal importance in the fight against COVID-19. Currently tools for functional annotation of genomic sequences have not been specifically devised for the analysis of SARS-CoV-2, e.g. do not take into account the unusual mechanisms of transcription and post-translational processing of coronaviruses gene products ( Sawicki  et al. , 2007 ). Additionally, while a wealth of resources and datasets for the fine-grained annotation of functional genomic elements are currently available, including: detailed studies of transcriptional mechanisms ( Kim et al., 2020 ), conserved regulatory sequences ( Sawicki  et al. , 2007 ), sites under evolutionary selection ( http://hyphy.org/covid/ ), predicted epitopes ( Kiyotani et al., 2020 ) and non-coding secondary structure elements, are available, these are not normally incorporated in the functional annotation of SARS-CoV-2 genomic variants. To overcome these limitations, we propose a novel, highly effective and user friendly approach for the functional annotation of SARS-CoV-2 genomes: CorGAT - the Coronavirus Genome Analysis Tool. By integrating a curated selection of datasets and resources, CorGAT provides a richer and more detailed annotation of SARS-CoV-2 variants when compared with other state of the art methods. To illustrate its advantages, we apply CorGAT to the complete collection of 57 558 currently available SARS-CoV-2 genomic sequences, and derive relevant insights concerning the evolution of this novel pathogen. 2 Implementation CorGAT has been made available as a collection of Perl script and annotation files at  https://github.com/matteo14c/CorGAT/tree/Revision_V1 . A user friendly version of the software is available in the form of a standalone Galaxy ( Afgan  et al. , 2018 ) implementation, based on the Laniakea@ReCaS Galaxy on-demand service ( Tangaro et al., 2020 ) at  http://corgat.cloud.ba.infn.it/galaxy . A Docker container image can be obtained from  https://hub.docker.com/r/laniakeacloud/galaxy_corgat . A complete account of the resources used for the annotation of the SARS-CoV-2 genomes, and of their integration in CorGAT is presented in the  Supplementary Materials . A detailed user manual is available at  https://corgat.readthedocs.io/ . Functional annotation files incorporated in CorGAT are updated on a monthly basis, to cope with the constant increase in publicly available data and genomic sequences of SARS-CoV-2. CorGAT has a modular architecture (see  Supplementary Materials ), allowing the rapid inclusion of novel or even custom types of annotations, simply by editing plain text files. 3 Results To demonstrate the application of CorGAT, we compared the functional annotation of the complete collection of 20 045 genetic variants derived from 57 558 genomic sequences of SARS-CoV-2 (see  Supplementary Materials ) by CorGAT, with the annotations by SNPeff ( Cingolani  et al. , 2012 ) and by the Variant Annotation Integrator ( Hinrichs  et al. , 2016 ). Simple statistics concerning the number and types of variants are reported in Supplementary  Table S1 . As outlined in Supplementary  Table S2 , all the tools herein considered provided highly consistent annotations of functional effects of variants associated with protein coding genes, thus confirming that CorGAT attains the same level of sensitivity as the other methods. However, as illustrated in Supplementary  Table S3 , CorGAT provides additional layers of annotation that are not provided by other methods, for a total of 14753 single distinct annotations. These include 33 variants associated with regulatory elements (transcription regulatory sequences, TRS), 69 variants associated to consensus cleavage sites ( Kiemer  et al. , 2004 ) in the ORF1a and ORF1ab polyproteins, 1164 variants associated with sites under selection according to Hyphy ( Kosakovsky Pond  et al. , 2020 ) and 161 variants in conserved secondary structure elements (Supplementary  Table S3 ). According to our analyses, a highly significant reduction of missense substitutions is observed at sites predicted to be under negative selection (Fisher  P -value &lt; 2.2e-16), compared to the background of all the substitutions in protein coding genes. Nevertheless, 229 missense substitutions alter highly conserved amino acid residues that are predicted to be under negative selection. Furthermore, analysis of genetic variants associated with functional non-coding elements in the genome of SARS-CoV-2 highlight some potentially interesting patterns. While the 5′ and 3′ UTRs are the most variable regions of the genome, TRS and secondary structure elements in general are considerably less variable, and show levels of conservation comparable to protein coding genes (Supplementary  Table S4 ). This is well exemplified by the TRS-L element, which is the single most conserved region in the 5′ UTR (Supplementary  Fig.    S1 ). Strikingly, the s2m element in the 3′ UTR ( Tengs et al., 2013 ) exhibits more of variability and recurrent indels than other annotated functional elements (Supplementary  Tables S4 and S5 ). Interestingly, our functional annotation (see  Supplementary Materials ), indicates that several observed substitutions might result in substantial changes to s2m structure consistent with change or loss of s2m function in SARS-CoV-2 ( Chiara  et al. , 2020 ). We conclude, that CorGAT constitutes a useful addition to the collection of tools for the functional characterization of SARS-CoV-2 genomes. Supplementary Material btaa1047_Supplementary_Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>NPS: scoring and evaluating the statistical significance of peptidic natural product–spectrum matches</Title>
    <Doi>10.1093/bioinformatics/btz374</Doi>
    <Authors>Tagirdzhanov Azat M, Shlemov Alexander, Gurevich Alexey</Authors>
    <Abstract/>
    <Body>1 Introduction Antimicrobial resistance is a global concern as admitted by WHO in its recent review ( World Health Organization, 2014 ). Coordinated action from governments, physicians and scientists is required to minimize the emergence and spread of antimicrobial resistance. An important component of such complex solution is the boost in the discovery of new antibiotics and other drugs from natural sources. This kind of speed up is now possible with the latest breakthroughs in experimental and computational technologies, as exemplified by the discovery of teixobactin ( Ling  et al. , 2015 ), the first class of antibiotics with a novel mode of action reported in three decades. Modern mass spectrometry methods enable high-throughput screening of huge volumes of natural products that potentially lead to the discovery of many more bioactive compounds. However, the interpretation of these large amounts of data remains a bottleneck. For instance, recently launched Global Natural Products Social (GNPS) molecular network ( Wang  et al. , 2016 ) contains more than a billion of natural product mass spectra but only a tiny fraction of them is annotated to date. Thus, mature computational methods are needed to turn antibiotic discovery into a high-throughput technology and to realize the promise of GNPS and other massive metabolomics projects ( da Silva  et al. , 2015 ). Natural product researchers tend to maximize the discovery of new compounds while minimizing the reevaluation of known ones. Identification of known molecules (so-called  dereplication ) performed early in a workflow significantly reduces time and cost required for the discovery of novel compounds ( Gerwick and Moore, 2012 ). One of the state-of-the-art dereplication strategies is the search of tandem mass spectra (MS/MS) against databases of known chemical structures performed computationally. Given a spectrum and a peptide database, a dereplication algorithm should find a peptide in the database that generated the given spectrum or report that the database does not contain peptides with statistically significant similarity to this spectrum ( Kertész-Farkas  et al. , 2012 ). This problem is nearly solved for regular peptides and many proteomics software for database search of MS/MS spectra are freely available nowadays ( Craig and Beavis, 2004 ;  Eng  et al. , 1994 ;  Frank and Pevzner, 2005 ;  Kim and Pevzner, 2014 ). However, identification of spectra derived from natural products is usually much more difficult than traditional peptide identification in proteomics. This study focuses on algorithms for identification of peptidic natural products (PNPs), an important class of natural products with many pharmacological applications. PNPs consist of non-ribosomal peptides (NRPs) ( Marahiel  et al. , 1997 ) and ribosomally synthesized and post-translationally modified peptides (RiPPs) ( Arnison  et al. , 2013 ). Despite the fact that PNPs are much more similar to regular peptides than all other classes of natural products, they have several important structural differences preventing the use of conventional proteomics tools for PNP identification. In contrast to regular peptides, PNPs are often short and have non-linear structures, such as cyclic or branch-cyclic compounds. Moreover, while regular peptides can be represented as strings over an alphabet of 20 letters (proteinogenic amino acids), PNPs contain non-standard amino acids and complex modifications making the number of different building blocks larger than a hundred. Finally, most PNPs form families of related peptides and only the most abundant representatives of the families are commonly present in the current PNP databases. Thus, PNP identification requires blind-search algorithms for finding of an unknown PNP from its known modified or mutated variants available in the database (so-called  variable dereplication ). Variable PNP identification is difficult because the set of possible modifications/mutations is not known in advance which makes the computational space of this problem several orders of magnitude larger than for standard PNP identification. To date, there are just a few computational tools for dereplication of natural products and all of them have some important limitations. NRP-Dereplication ( Ng  et al. , 2009 ) and iSNAP ( Ibrahim  et al. , 2014 ) algorithms are among the first  in silico  dereplication tools designed to work with cyclic peptides. NRP-Dereplication was initially developed with the ability to perform variable identification and iSNAP was extended with such functionality in later versions ( Yang  et al. , 2015 ). However, both tools are focused on NRPs only and have critical shortcomings in their software implementations preventing their use in high-throughput analysis pipelines. NRP-Dereplication development was discontinued a while ago and there is no possibility to download the tool now. iSNAP software is available only as a web-service application for analyzing a single spectrum per run. MS-FINDER ( Lai  et al. , 2018 ;  Tsugawa  et al. , 2016 ) and SIRIUS+CSI: FingerID ( Böcker  et al. , 2009 ;  Böcker and Dührkop, 2016 ;  Dührkop  et al. , 2015 ) are popular programs for metabolomics mass spectra annotation. The both instruments try to predict chemical formulas based on spectra isotope patterns and further analyze the formula fragmentation to annotate MS/MS peaks. The formula deduction problem gets significantly more complex with the increase of molecular weight of putative compounds, so these methods work prohibitively slow for molecules larger than 500 Da (the majority of known PNPs). Technically, these tools can still be used for PNP dereplication since the number of allowed molecular formulas is limited by the chemical database and is relatively small. However, a specialized PNP search tool will produce much better results. Beyond that, MS-FINDER is currently available for Windows platform only which complicates its usage for analysis of large collections of MS/MS spectra typically stored and processed on Linux-based servers. Dereplicator ( Mohimani  et al. , 2017a ) and its extension for variable identification, VarQuest ( Gurevich  et al. , 2018 ), are the first tools that enabled high-throughput PNP identification via database search of mass spectra. They were incorporated into the GNPS platform and tested on more than one hundred million MS/MS spectra. The benchmarking revealed an order of magnitude more PNPs (and their new variants) than any previous dereplication effort on this data. Despite the success of these methods, their core module uses a pretty naïve function for computing similarity score between given experimental spectrum and PNP chemical structure. This issue is partially solved by using two-stage scoring ( Kim  et al. , 2008 ) that involves estimation of the statistical significance of the primitive first-level scores ( Mohimani  et al. , 2013 ). Nonetheless, strict thresholds on false discovery rate (FDR) induce these tools to filter out many true identifications which can be safely revealed with a more thoughtful scoring method. In this work, we present NPS—a two-stage approach for scoring PNP-spectrum matches ( NPScore ) and estimating the statistical significance of such scores ( NPSignificance ). The method takes into account intensities of MS/MS peaks and occurrence of various additional ions during the fragmentation process in mass spectrometers. Both the set of considered ion types and the weights for scoring annotated and missed peaks of various intensity are computationally learned from real data. To create an appropriately sized training dataset, we processed over one hundred million natural product mass spectra with Dereplicator and curated the most reliable PNP annotations. Until recently, such high-quality training dataset was nearly impossible to obtain in the case of PNPs, so NPS to our knowledge is the first high-throughput PNP identification method that uses statistically learned scoring model. The created dataset is freely available at our website and can be used by other researchers in their future studies. We incorporated NPS into Dereplicator and VarQuest pipelines and benchmarked it against the current baseline algorithm. The test on millions of GNPS mass spectra showed a more than 20% increase in the number of identified PNPs comparing to the baseline at a conservative FDR level of 1%. As a sanity check, we also tested our method on a well-studied regular peptides dataset ( Kim  et al. , 2014 ) and demonstrated that NPS accuracy is comparable with the accuracy of one of the leading proteomics tools, MS-GF+ ( Kim and Pevzner, 2014 ). 2 Materials and Methods 2.1 Baseline scoring model Similarly to other database search algorithms, Dereplicator compares each spectrum in the spectral dataset against each PNP in the chemical database. A PNP-spectrum match (PSM) is formed if the precursor mass of the spectrum matches the molecular mass of the PNP (up to a predefined maximum error, typically  0.02   Da  for high-resolution spectra). VarQuest can form a PSM even if the masses of a spectrum and a PNP do not match but in this case it considers the mass difference as a modification/mutation and applies it to the PNP structure ( Gurevich  et al. , 2018 ). In any case, the scoring of a PSM requires comparison of an experimental spectrum and a chemical structure. For doing this, the tools construct a  theoretical spectrum  of the compound which models its ionization and fragmentation in a mass spectrometer. The theoretical and experimental spectra are further examined and a measure of their similarity is reported as the PSM score. 2.1.1 Theoretical spectrum construction The simplest fragmentation model assumes that a mass spectrometer cleaves peptide bonds in a charged molecule passing through the instrument and breaks the compound into two parts which mass-to-charge ratios are measured. For a linear peptide, this assumption results in the theoretical spectrum consisting of masses of all prefixes ( b -ions) and all suffixes ( y -ions) of the peptide sequence. For a non-linear peptide—which most of PNPs are—a single bond cleavage may not result in the molecule breakage. To model the fragmentation of a such compound, a  PNP graph  is constructed with amino acids as nodes and generalized peptide bonds as edges ( Mohimani  et al. , 2017a ). The PNP graph could be fragmented into subgraphs by removing of a single edge ( bridge ) or a pair of edges ( 2-cut ). The theoretical spectrum is then a set of masses of all such subgraphs. 2.1.2 SPC scoring Dereplicator and VarQuest measure similarity between a spectrum  S  and a PNP  P  simply as the shared peak count (SPC)—the number of peaks shared between  S  and the theoretical spectrum of  P . Two peaks are shared if their masses are within a certain threshold  ε  ( 0.02   Da  for high-resolution spectra by default). Therefore, the SPC scoring does not take into account peak intensities but it is very easy to compute and this model does not require training of any parameters. Early versions of Dereplicator also used more advanced MS-GF+ score ( Kim and Pevzner, 2014 ) for evaluating linear PNPs consisting of proteinogenic amino acids ( Mohimani  et al. , 2017a ). However, such compounds represent a small fraction of the broad chemical diversity of PNPs. For the sake of simplicity and consistency, MS-GF+ scoring was removed from Dereplicator starting from v.2.0 ( Gurevich  et al. , 2018 ) and SPC is used for all compounds as the baseline scoring method. 2.2 Proposed scoring model 2.2.1 Peak intensities There are several common strategies for considering peak intensities in proteomics software. Some of them, such as PepNovo ( Frank and Pevzner, 2005 ), deal with normalized intensity values while others, such as MS-GF+ ( Kim and Pevzner, 2014 ), utilize the ranking approach. In this approach, all the peaks are ranked according to their intensities such that  i th highest intensity peak gets rank  i . In either case, intensity-aware scoring models normally have different weights for various intensity levels. To minimize the number of model parameters, normalized intensity values are usually discretized into a small number of bins and only a few first  K  ranks (the highest intensity peaks) are distinguished while the rest are considered to have the same rank ( K  +   1). In this work, we try both the strategies to find an optimal approach to PNP data interpretation. In case of normalized intensity values, we discretize intensities into PepNovo-like bins [each peak’s intensity is divided by the grass level, four intensity levels are distinguished ( Frank and Pevzner, 2005 )] or simple logarithmic bins (10 levels). In case of the ranking approach, we distinguish the first 100 ranks for well-fragmented spectra of regular linear peptides and 50 ranks for experimental spectra derived from PNPs. We further refer to the described approaches as  NP S PN - bins ,   NP S   log - bins , and  NPS ranks , respectively. Since ranks can be considered as intensity bins, the experimental spectrum in either case may be represented as a set  S = ( m z 1 , b 1 ) , … ( m z n , b n ) , where  mz j  characterizes mass-to-charge ratio of the  j th peak ( X -axis position) and  b j  characterizes its intensity bin ( Y -axis position). 2.2.2 Ion types We further refer to the set of masses obtained by the aforementioned theoretical spectrum construction procedure as  TheorMasses . This set provides a good estimate of masses of basic ionized fragments passing through a mass spectrometer. However,  TheorMasses  does not describe many additional types of ions occurring during the fragmentation and present in the most experimental spectra. These types of ions include doubly and triply charged ions (which have different  X -axis position in experimental spectra since mass spectrometers measure mass-to-charge ratios rather than the real fragment masses), neutral loss fragment ions, isotopic shifts, etc. To take this into account, we transform short  TheorMasses  into an expanded set of mass-to-charge ratios using the following procedure. An ion type is characterized by a pair  ( charge ,   offset ) , where  offset  represents a mass shift (in Da) and  charge  represents charge of the fragment. For example, the most abundant basic ionized fragments ( b -ions and  y -ions) correspond to the ion type (1, 0) while peaks occurring due to the neutral loss of water (H 2 O) correspond to the ion type (1, 18.011). For a given set  TheorMasses  and an ion type  ion = ( charge , offset ) , we define a corresponding set of mass-to-charge ratios  T ion  as
 T ion = { mass − offset + H · charge charge | mass ∈ TheorMasses } , 
where H corresponds to the mass of proton (1.007 Da). The resulting NPS theoretical spectrum  T  is defined then as
 T = ∪ ion ∈ I { ( m z , ion ) | mz ∈ T ion } , 
where  I  is a set of the considered ion types. The set  I  can be derived for a particular dataset using the offset frequency function (OFF) approach ( Dančik  et al. , 1999 ). Given a set of reliable PSMs, OFF constructs an empirical distribution of the offsets characteristic for the spectral data. This is done by matching all the experimental peaks that are located at distance  offset  from the basic peaks of the related peptide/PNP, that is the peaks corresponding to the ion type (1, 0). The resulting counts are averaged over the dataset. The ion type is selected if the value of OFF at the corresponding offset is above a certain threshold. The selected  I  sets for regular linear peptides and PNPs are in  Supplementary Table S1 . 2.2.3 NPScore The intuition behind our scoring procedure is to estimate probability  Prob ( S | T )  of observing an experimental spectrum  S  given a theoretical spectrum  T . To compute it, we assume that all the experimental peaks are generated independently. An experimental peak can be generated by a matching theoretical peak or may have nothing to do with the theoretical spectrum but occur due to noise, contamination or some rare ion types. Furthermore, an experimental peak corresponding to a certain theoretical peak can be missing in the actual spectrum. Considering all these possibilities, define  Match ( S , T )  as a union of the three following sets:
 Shared ( S , T ) = { ( b i , ion j ) | i ∈ S , j ∈ T     s . t .     match ( i , j ) = 1 } ,     Noise ( S , T ) = { ( b i , ∅ ) | i ∈ S     s . t .     match ( i , j ) = 0   ∀ j ∈ T } , Missing ( S , T ) = { ( 0 , ion j ) | j ∈ T     s . t .     match ( i , j ) = 0   ∀ i ∈ S } , 
where  ion = ∅  stands for a dummy ion type which represents a noise peak (that is absent in the theoretical spectrum), and analogously intensity bin  b  =   0 represents a missing experimental peak. Here
 match ( i , j ) = { 1 , | m z i − m z j | &lt; ε , 0 , otherwise . The probability of  S  given  T  is then defined as
 Prob ( S | T ) = ∏ ( b , ion ) ∈ Match ( S , T ) Prob ( b | ion ) , 
where  Prob ( b | ion )  is the probability of an experimental peak from the intensity bin  b  to be generated by a theoretical peak of the ion type ion. We also formulate a null hypothesis that spectrum  S  is generated by an empty theoretical spectrum denoted by  ∅ . Evidently, in this case  Match ( S , ∅ ) = Noise ( S , ∅ ) , and
 Prob ( S | ∅ ) = ∏ ( b , ion ) ∈ Match ( S , ∅ ) Prob ( b | ∅ ) . Define scoring function as a log odds ratio for these hypotheses,
 NPScore   ( S , T ) = log   Prob ( S | T ) Prob ( S | ∅ ) . Substituting the expressions for  Prob ( S | T )  and  Prob ( S | ∅ )  into the expression above, we finally come up with
 (1) NPScore   ( S , T ) = ∑ ( b , ion ) ∈ Shared ( S , T )   log   Prob ( b | ion ) Prob ( b | ∅ ) + ∑ ( b , ion ) ∈ Missing ( S , T )   log   Prob ( 0 | ion ) Prob ( 0 | ∅ ) . The first summand in (1) defines gains for the theoretical peaks that explain some experimental peaks and the second summand penalizes the rest peaks in the theoretical spectrum. Note that computation of log odds ratios is a common approach employed in scoring models of many proteomics tools ( Cannon  et al. , 2005 ;  Dančik  et al. , 1999 ;  Frank and Pevzner, 2005 ;  Havilio  et al. , 2003 ;  Kim  et al. , 2009 ;  Kim and Pevzner, 2014 ;  Tanner  et al. , 2005 ), so its use for PNP data analysis is a natural direction to proceed in. 2.2.4 Learning NPScore parameters Given a set of ion types  I  and a number of different intensity levels  K , NPScore function relies on  K · | I |  and  | I |  parameters for defining the first and second summands in (1), respectively. We statistically learn them using training datasets of highly reliable PSMs. The probabilities in (1) are estimated as frequencies of the corresponding events in the dataset. For example, in case of the ranking approach,  Prob ( b = 1 | ion = ( 1 , 0 ) )  is computed as the number of PSMs in which the most intense experimental peak is explained by a theoretical peak of type (1, 0) divided by the total number of PSMs in the training set. For the sake of regularization, a smoothing function was applied for learning the ranking approach parameters. For each ion type, the value at any rank was smoothed with moving average with the window length of 11 ranks. The parameters for the normalized intensity methods were not smoothed. 2.3 Proposed significance estimation procedure Since PSM scores are often biased toward spectra with many peaks or PNPs with different molecule structures and sizes ( Mohimani  et al. , 2017a ), it is critically important to estimate the statistical significance of the scores ( Gupta  et al. , 2011 ). Proteomics software normally estimate score  P -values in the space of  20 k  possible linear peptides of a given length  k . At the same time, PNP identification tools have to deal with several orders of magnitude of larger space of potential compounds and require completely different algorithms. To estimate  P -value of a PSM score, Dereplicator uses MS-DPR ( Mohimani  et al. , 2013 ), one of a few algorithms specialized in working with PNPs. Given a spectrum  S  and a peptide  P  forming the PSM, MS-DPR approximates a tail of the score distribution of  S  against a population of millions of compounds similar to  P  (having the same molecular weight and structure but different amino acid compositions). The approximation technique is based on constructing a Markov chain on a state space of all possible scores of peptides from the population. By design, the number of states has to be finite or in other words the scoring function has to be discrete. While this requirement is obviously satisfied for the SPC scoring, the state space resulting from NPScore model is, in practice, continuous, and requires a certain discretization. Our  P -value estimation procedure, NPSignificance, is based on MS-DPR and discretizes NPScore output in the following manner. First, we make an initial guess of what the state space look like with a naïve Monte Carlo approach. Then we take an interval of scores obtained from the previous step and divide it into  N  bins of equal size. We performed a series of experiments with various values of  N  and showed that our approach to the state space discretization produces rather stable results (see  Supplementary Fig. S1 ). We take  N  =   10 as it provides a conservative estimation of  P -value in all experiments. 2.4 Estimation of FDR The target-decoy approach ( Elias and Gygi, 2007 ) is the most popular strategy to estimate FDR in proteomics and metabolomics database search experiments. The method is based on generating a database of decoy peptides similar to the target peptide database and searching spectra against the both databases at once. FDR is then estimated as the number of identifications in the decoy database divided by the number of identifications in the target database at a given  P -value or score threshold. There are several approaches to generating decoy databases. For benchmarking on regular peptide datasets, we use a traditional proteomics approach, in which the decoy database is obtained from the reversed protein sequences. For the experiments on PNP datasets, we utilize the approach described in  Gurevich  et al.  (2018) , in which decoys are generated from target PNPs by shuffling of amino acids together with structure modification. PNP benchmarks using several alternative decoy generation strategies are available in the  Supplementary Material . 3 Results 3.1 Benchmarking on proteomics data To prove applicability of the  NPS    model, we first benchmark it within the Dereplicator pipeline (referred to as  Dereplicator NPS ) on a spectral dataset of linear peptides and compare its performance with the baseline method (the SPC scoring, referred to as  Dereplicator SPC ) and the state-of-the-art proteomics tool, MS-GF+ ( Kim and Pevzner, 2014 ). We run our model in the three different intensity-aware modes ( ranks ,  PN-bins  and  log-bins ) increasing the total number of compared approaches to five. 3.1.1 Spectral data As a test dataset, we used a subset of the human proteome map project ( Kim  et al. , 2014 ). The full dataset is freely accessible on GNPS under MassIVE accession number MSV000079514 and contains approximately 25 million high-resolution MS/MS spectra. These spectra were obtained on LTQ-Orbitrap Velos and LTQ-Orbitrap Elite mass spectrometers from proteins of 30 organ tissues. For our benchmarking, we randomly selected two adult tissues and collected all spectra related to them. We further refer to these spectral datasets as  Spectra Heart  (heart, 426 086 spectra; used for training) and  Spectra Kidney  (kidney, 439 253 spectra; used for testing). 3.1.2 Peptide database The target peptide database  HUMANdb  was obtained from the Human RefSeq proteins ( Pruitt  et al. , 2005 ). The protein sequences were digested with the Trypsin/P method allowing no missed cleavages using the Pyteomics framework ( Goloborodko  et al. , 2013 ). Carbamidomethylation of cysteine was set as a fixed modification. Only peptides with sequence length from 8 to 20 amino acids were kept. The decoy database was prepared from the reversed protein sequences in the same manner. The resulting combined database contains 47 284 target peptides and 47 239 decoy peptides. 3.1.3 NPS parameters training To learn NPS parameters, we form a training dataset  TrainSet Linear  from MS-GF+ identifications on  Spectra Heart  against  HUMANdb  at FDR level 0%. We retained only PSMs with charge + 1 and + 2. For each peptide, we kept only the best PSM according to  E -value reported by MS-GF+. The resulting dataset contains 17 794 PSMs, mostly of charge + 2 ( &gt; 99 % ). Using  TrainSet Linear , we derived the most frequent ion types ( Supplementary Fig. S2a ,  Table S1 ) and trained a set of  NPScore    model parameters for them ( Supplementary Fig. S3 ). 3.1.4 Number of identifications We compared the number of identified PSMs and unique peptides from  HUMANdb  in  Spectra Kidney  at various FDR levels ( Fig. 1  and  Supplementary Table S2 ). Beside the five competing methods,  Figure 1  also shows four extra Dereplicator curves corresponding to the runs without the significance estimation, that is the runs in which PSMs were ranked based solely on the raw scores.
 Fig. 1. MS-GF+,  Dereplicator NPS  and  Dereplicator SPC  results on  Spectra Kidney . The curves display the number of identified PSMs at different FDR levels. The dashed curves correspond to Dereplicator runs without  P -value estimation The raw score-based results highlight the importance of the two-stage scoring.  Dereplicator SPC  with the  P -value computation identified approximately 50% more PSMs at FDR 1% than its raw score version (94 663 versus 61 250). While all three the raw score  Dereplicator NPS  methods significantly improved over the SPC analog, they still performed worse than the default  P -value-based  Dereplicator SPC . The comparison of various NPS approaches to considering peak intensities demonstrates that the ranks and log-bins methods are superior to the PN-bins approach on the test dataset. However, the differences in their results are mostly insignificant when using the two-stage scoring procedure. MS-GF+ obviously outperformed all other approaches at all FDR levels. However, the beating of one of the leading proteomics tools on its own ground is clearly out of scope of this paper. Note that impressive MS-GF+ results are partially based on some extra peptide-specific techniques, such as comparing distances between experimental peaks with the known exact masses of 20 proteinogenic amino acids ( Kim and Pevzner, 2014 ). Since NPS is designed for much more chemically diverse PNP structures, it cannot rely on such assumptions. Moreover, despite the set of NPS estimated parameters is large, it is still an order of magnitude smaller than the number of MS-GF+ parameters. Thus, our approach will normally lose to MS-GF+ and other leading proteomics tools on any regular peptide dataset. The more important observation is that all three  Dereplicator NPS  approaches outperformed  Dereplicator SPC  at the most important 0–1% FDR levels. Albeit the increase is mostly fractional with just 6% more PSMs and 4% more peptides at FDR 1% for the best, rank-based NPS method (100 444 versus 94 663 PSMs and 16 570 versus 15 968 peptides,  Supplementary Table S2 ). The small improvement on this dataset may be due to relative simplicity of the peptide identification from high-quality data. Note that the numbers of  Dereplicator NPS  identifications are also close to almost gold-standard MS-GF+ results at strict FDR 1% level (5% less PSMs and 3% less peptides for the rank-based NPS method,  Supplementary Table S2 ). The main aim of the NPS approach is to improve identification of mediocre PSMs which are often present in PNP identification experiments (see the benchmarking below). 3.1.5 Validation of the results We validated  Dereplicator NPS  output by comparing its identifications to the results of  Dereplicator SPC  and MS-GF+ at FDR 1%.  Supplementary Figure S4  shows that all methods are in fairly good agreement. Over 96% of PSMs reported by rank-based  Dereplicator NPS  were also reported by MS-GF+ which output may be roughly considered as the ground truth annotations. 3.2 Benchmarking on PNP data 3.2.1 Spectral data We created the main natural product test dataset by combining 13 high-resolution GNPS spectral datasets ( Supplementary Table S3 ). The resulting dataset consists of ∼16 million spectra and we further refer to it as  Spectra GNPS . Three out of these 13 GNPS datasets were extensively studied before, so we used them separately for more rigorous validation of the results and for benchmarking NPS within the VarQuest pipeline which is considered as less robust than Dereplicator. These three datasets are:  Spectra PSEUD  [∼400 000 spectra from  Pseudomonas  isolates ( Gurevich  et al. , 2018 ;  Nguyen  et al. , 2016 )],  Spectr a STRE P 1  [∼200 000 spectra from  Streptomyces  ( Gurevich  et al. , 2018 ;  Mohimani  et al. , 2014 a)],  Spectr a STRE P 2  [∼500 000 spectra from  Streptomyces  ( Duncan  et al. , 2015 ;  Gurevich  et al. , 2018 ;  Mohimani  et al. , 2017a )]. 3.2.2 PNP database As a target chemical database we used the PNP database from  Gurevich  et al.  (2018)  (further referred to as  PNPdb ).  PNPdb  consists of 5021 compounds (1582 PNP families) from AntiMarin ( Blunt  et al. , 2007 ), DNP ( Gozalbes and Pineda-Lucena, 2011 ), MIBiG ( Medema  et al. , 2015 ) and StreptomeDB ( Lucas  et al. , 2013 ) databases. The decoy database of the same size was generated using Dereplicator API. 3.2.3 NPS parameters To learn NPS parameters for PNP identification, we form a training dataset  TrainSet PNP  from  Dereplicator SPC  identifications on virtually the entire GNPS. To obtain the initial set of annotations, Dereplicator v.2.0 was run with the default parameters on 120 high-resolution publicly available GNPS datasets (∼130 million spectra) against  PNPdb . The run resulted in 14 757 PSMs corresponding to 420 unique PNPs. To get the training set of a reasonable size and quality, we further considered all identifications of charge + 1 and + 2 at FDR level 5% and keep up to five best PSMs per compound. The resulting dataset contains 2 213 PSMs ( Supplementary Fig. S5 ).  Supplementary Figure S2b  shows the offset frequency functions (OFFs) computed for  TrainSet PNP , which results in the ion types presented in  Supplementary Table S1 . Using the same dataset for both training NPS parameters and evaluating the tool performance raises concern about overfitting. To prove that this is not the case, we come up with the following procedure. From  TrainSet PNP , we randomly formed subsets  Train 1 ,  Train 2  and  Test  so that they do not contain PSMs obtained from the same spectral datasets. Since some of the spectral datasets share the producing organisms, we also required that  Train 1  and  Train 2  do not contain PSMs related to the same PNP families (see  Fig. 2 ). The resulting datasets  Train 1  and  Train 2  contain 346 and 493 PSMs, respectively. Using these datasets we trained two sets of  NPS    parameters. We compared performance of these two models on spectra from  Test  dataset. The comparison shows difference of less than 5% at FDR level 1% ( Supplementary Fig. S6 ).
 Fig. 2. Split of  TrainSet PNP  into training and testing datasets The final set of  NPS    model parameters was trained on the full  TrainSet PNP  dataset. The resulting weights are shown in  Supplementary Figure S7 . 3.2.4 Standard identification We matched  Spectra GNPS  against  PNPdb  with  Dereplicator NPS  (the three different intensity-aware models) and compared the results with the baseline ( Dereplicator SPC ). To retain only the most reliable hits, all PSMs with  P -values above  10 − 10  were removed beforehand and the FDR was conservatively computed for the remaining PSMs (see Section 2). 
 Figure 3  depicts a significant boost in the number of PSMs identified by NPS comparing to  Dereplicator SPC  at all FDR levels using all three models for considering peak intensities. The ranking approach ( NPS ranks ) demonstrates the best results at all FDR levels, although its advantage over the normalized intensity methods ( NP S PN - bins  and  NP S   log - bins ) is insignificant despite the higher number of estimated parameters (50 ranks versus 4 and 10 intensity levels for each ion type, respectively). Nevertheless, the running time for all three methods is the same since all of them require  O (1) lookup in the weight table. Thus, we decided to simply use currently the best strategy ( NP S ranks ) as the only method in further experiments and refer to it simply as  NPS . Note that this intensity-aware approach also shows the best results among the three alternative methods in the proteomics benchmarking experiment (see above).
 Fig. 3. 
 Dereplicator SPC  and  Dereplicator NPS  results on  Spectra GNPS . The curves display the number of identified PSMs at different FDR levels 
 Table 1  shows a more than 20% increase in the number of PNPs and a more than 45% increase in the number of PSMs identified by NPS at FDR 1% under the default decoy generation strategy. Experiments with two alternative strategies also demonstrate a superiority of the suggested scoring method over the baseline ( Supplementary Fig. S8 ). Table 1. Summary on  Dereplicator NPS ,   Dereplicator SPC ,   VarQuest NPS  and  VarQuest SPC  results on natural products spectral datasets Method Number of PSMs Number of PNPs Number of PNP families 
 P − 10 
 
 FDR 
 0 
 
 FDR 
 1 
 
 P − 10 
 
 FDR 
 0 
 
 FDR 
 1 
 
 P − 10 
 
 FDR 
 0 
 
 FDR 
 1 
 
 Spectra GNPS 
 
 Dereplicator SPC 
 8544 4538 6972 351 231 304 108 80 92 
 Dereplicator NPS 
 10 504 8811 10 287 395 290 378 132 101 129 
 Spectr a STRE P 1 
 
 VarQuest SPC 
 1978 177 177 429 13 13 196 8 8 
 VarQuest NPS 
 2650 412 783 497 33 82 218 14 37 
 Dereplicator SPC 
 233 230 233 21 20 21 10 10 10 
 Dereplicator NPS 
 250 250 250 24 24 24 12 12 12 
 Spectr a STRE P 2 
 
 VarQuest SPC 
 220 15 15 78 2 2 52 2 2 
 VarQuest NPS 
 402 174 186 138 30 33 66 12 14 
 Dereplicator SPC 
 83 83 83 13 13 13 6 6 6 
 Dereplicator NPS 
 179 179 179 29 29 29 10 10 10 
 Spectra PSEUD 
 
 VarQuest SPC 
 5311 280 949 290 32 68 145 15 30 
 VarQuest NPS 
 6562 2495 5405 256 143 226 121 58 102 
 Dereplicator SPC 
 1380 814 1380 36 29 36 12 11 12 
 Dereplicator NPS 
 1881 1881 1881 42 42 42 11 11 11 
 Note :  P − 10 ,  FDR 0  and  FDR 1  stand for the number of identified PSMs, unique PNPs or unique PNP families with  P -value below  10 − 10 , at FDR 0% and 1%, respectively. Here  NPS  stands for the rank-based NPS model, the finally best overall intensity-aware approach. 3.2.5 Variable identification We benchmarked NPS within the VarQuest pipeline (referred to as  VarQuest NPS ) on  Spectra PSEUD ,  Spectr a STRE P 1 , and  Spectr a STRE P 2  and compared its results to the VarQuest baseline ( VarQuest SPC ) and the standard dereplication of these datasets ( Dereplicator SPC  and  Dereplicator NPS ). 
 Figure 4  shows that  VarQuest NPS  significantly increased the number of identified PSMs and PNP families comparing to all other considered methods at all FDR levels and on all spectral datasets.  Table 1  demonstrates that while  VarQuest SPC  showed the less accurate results than the Dereplicator-based methods, NPS-powered version of VarQuest outperformed all the competitors even at the strictest FDR 0% level in all categories.
 Fig. 4. 
 VarQuest NPS ,   VarQuest SPC ,   Dereplicator NPS  and  Dereplicator SPC  results on ( a )  Spectr a STRE P 1 , ( b )  Spectr a STRE P 2 , ( c )  Spectra PSEUD  and ( d ) the combination of the three. The curves display the number of identified PSMs (a–c) and PNP families (d) at different FDR levels 3.2.6 Validation of the results A comparison of  Dereplicator NPS  and  Dereplicator SPC  identifications on  Spectra GNPS  shows that over 96% PSMs (91% PNPs) found by  Dereplicator SPC  were also reported by  Dereplicator NPS  ( Supplementary Fig. S9 ). On the other hand, only 64% PSMs (70% PNPs) found by  Dereplicator NPS  were reported by  Dereplicator SPC . To check that these additional identifications represent likely true positives, we limited ourselves to  Spectr a STRE P 1 ,  Spectr a STRE P 2  and  Spectra PSEUD  datasets. We considered all PNPs found by  Dereplicator NPS  at 0% FDR and compared them to  Dereplicator SPC  identifications at FDR 0% and 5%. The results are listed in  Supplementary Tables S4–S6 . The origin of PNPs was determined based on literature search. Our analysis shows that in the  Streptomyces  datasets 72% for  Spectr a STRE P 1  and 43% for  Spectr a STRE P 2  of PNPs were found both by  Dereplicator SPC  and  Dereplicator NPS . The rest of found PNPs can be attributed to  Streptomyces  spp. or contaminants such as  Bacillus  spp. which are common to these datasets ( Gurevich  et al. , 2018 ). One of the PNPs found in  Spectr a STRE P 2  is of unknown origin. We manually curated the corresponding PSM and it suggests that this identification represents contamination rather than a false positive (see  Supplementary Fig. S10c ). In the  Pseudomonas  dataset 72% of found PNPs were reported by both of the methods. Over 96% of found PNPs have  Pseudomonas  or  Bacillus  origin. We further examined identifications visualizing the PSMs related to PNPs which were reported only by  Dereplicator NPS . Some of the visualizations are presented in  Supplementary Figure S10 .  Supplementary Figure S10b –d shows matches attributed to putative contaminants. 
 Supplementary Figure S11  shows that over 98% PSMs (87% PNP families) found by  VarQuest SPC  in  Spectr a STRE P 1  dataset at FDR 1% were also reported by  VarQuest NPS . For  Spectr a STRE P 2  and  Spectra PSEUD  datasets, these figures rise to 100% PSMs (100% PNP families) and 97% PSMs (100% PNP families), respectively. VarQuest is designed to identify PNP families rather than PNPs, that is why we compared the PNP families identified by  VarQuest NPS  at FDR 0% to  VarQuest SPC  identifications at FDR 0% and 5% ( Supplementary Tables S7–S9 ). We further visualized some of the  VarQuest NPS  identifications in  Supplementary Figure S12  to show matches with PNPs related to the datasets origins and attributed to likely contaminants. 4 Discussion Although there are many highly reliable computational tools for analysis of MS/MS spectra in proteomics, there is still a lack of them in the field of natural products discovery. Here, we demonstrate how some of the state-of-the-art computational ideas from proteomics could be adapted to the specifics of natural products data. The developed approach enabled a significant boost in the results of the leading PNP database search instruments. Moreover, the proposed model can be further improved if more high-quality PNP training data become available in the future. In particular, it is known that peak intensities strongly depend not only on the ion type but also on the fragmentation site. While proteomics tools successfully utilize such information ( Frank and Pevzner, 2005 ), there is currently not enough training data to learn these patterns in the case of PNPs. Other possible directions include structure-specific and/or instrument-specific weights, e.g. having different parameters for linear, cyclic or branch-cyclic compounds obtained on maXis, micrOTOF or LTQ-Orbitrap mass spectrometers. The created method has a much wider range of applications than solely PNP dereplication. In particular, a proper procedure for scoring of PSMs and estimation of their statistical significance is desperately needed for discovery of novel PNPs. Recently developed NRPquest ( Mohimani  et al. , 2014 b) and MetaRiPPquest ( Mohimani  et al. , 2017b ) use genome mining technique for creating databases of putative compounds and further match them against MS/MS data to find the correct predictions. Since the PNP databases in such cases are huge and error-prone, it is critically important to minimize the number of false positive identifications. Implementation of NPS inside easy-to-use Dereplicator and VarQuest pipelines makes it useful for natural product researchers with various computational background. In particular, the scientists without Unix command-line experience can use convenient web interfaces of the aforementioned tools at the GNPS platform. We believe that our method will be helpful for the natural products community and will be used for identifying numerous PNP spectra that evaded all attempts to interpret them. Supplementary Material btz374_Supplementary_Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Functional annotation of genomic variants in studies of late-onset Alzheimer’s disease</Title>
    <Doi>10.1093/bioinformatics/bty177</Doi>
    <Authors>Butkiewicz Mariusz, Blue Elizabeth E, Leung Yuk Yee, Jian Xueqiu, Marcora Edoardo, Renton Alan E, Kuzma Amanda, Wang Li-San, Koboldt Daniel C, Haines Jonathan L, Bush William S, Birol Inanc</Authors>
    <Abstract/>
    <Body>1 Introduction Multiple large-scale whole-exome (WES) and whole-genome sequencing (WGS) campaigns are currently underway to identify genetic variants that associate to a variety of traits, including studies of schizophrenia ( Genovese  et al. , 2016 ), type 2 diabetes ( Fuchsberger  et al. , 2016 ), height ( Marouli  et al. , 2017 ), myocardial infarction ( Do  et al. , 2015 ) and others. While the designs and scope of sequencing for these studies vary, previous similar studies have routinely identified millions of genetic variants that were novel (at the time of publication). The targeted sequencing of 82 pharmacogenes using the PGRN-seq platform identified 48.5% novel variants ( Bush  et al. , 2016 ), and a prior sequencing study of known pharmacogenes reported 90% novel variants ( Nelson  et al. , 2012 ). The Exome Aggregation Consortium (ExAC), which compiled WES data from multiple studies reported 72% novel variants ( Lek  et al. , 2016 ), and the UK10K study used WES and reported 57% novel variants ( UK10K Consortium  et al. , 2015 ). As such, any sequencing-based study will likely report several million variants that are not previously described in the scientific literature. With no published information about these variants available, the genomics community has increasingly relied on computational approaches for predicting variant function ( Cingolani  et al. , 2012 ;  De Baets  et al. , 2012 ;  Wang  et al. , 2010 ;  Yourshaw  et al. , 2015 ). A major goal of sequencing studies is to statistically examine the impact of low-frequency genetic variation on a trait of interest. Methods for the assessment of this rare-variant hypothesis now increasingly rely on biological information to group or bin variants together, improving the statistical power to detect an effect. Gene-based burden, collapsing and kernel-based tests are the most commonly applied ( Bansal  et al. , 2010 ;  Lee  et al. , 2012 ;  Li and Leal, 2008 ;  Liu and Leal, 2010 ;  Madsen and Browning, 2009 ;  Moutsianas  et al. , 2015 ;  Neale  et al. , 2011 ;  Price  et al. , 2010 ;  Sun  et al. , 2013 ;  Wu  et al. , 2011 ). In addition to grouping variants by gene, multiple modifications to this strategy have been proposed that incorporate the predicted impact or consequence of genetic variants on the molecular function of the gene being tested ( Ng  et al. , 2009 ,  2010 ;  Wu  et al. , 2011 ). Because of these technological and analytical advancements, bioinformatic annotation of variant function plays an increasingly critical role in the analysis of whole-genome and whole-exome sequence data. Prior large-scale sequencing efforts have employed a variety of annotation strategies. The UK10K project ( UK10K Consortium  et al. , 2015 ) from the Wellcome Trust Sanger Institute is annotated with RSIDs from dbSNP138 ( Smigielski, 2000 ) and functional annotations through the Ensembl Variant Effect Predictor (VEP; version 75) ( Yourshaw  et al. , 2015 ). The ExAC similarly used VEP (version 81), along with the Loss-of-Function Transcript Effect Estimator (LOFTEE) plugin to evaluate protein-truncating variants. They further annotated variant impact using Polymorphism Phenotyping version 2 (PolyPhen2), Scale-Invariant Feature Transform (SIFT) and Combined Annotation Dependent Depletion (CADD) scores ( Adzhubei  et al. , 2010 ;  Kircher  et al. , 2014 ;  Kumar  et al. , 2009 ). Notably, these studies employed exome sequencing in their design, shifting the annotation focus onto coding variation. In this work, we describe variant annotation efforts motivated by the Alzheimer’s Disease Sequencing Project (ADSP), a collaborative effort between the National Institutes on Aging and the National Human Genome Research Institute, along with members of the Alzheimer’s disease genetics research community, designed to study the genetics of late-onset Alzheimer’s disease (LOAD). The ADSP discovery-phase datasets consist of 578 individuals from 111 densely affected LOAD families selected for WGS as well as 5778 LOAD cases and 5136 controls selected for WES from the Alzheimer’s Disease Genetics Consortium (ADGC) and the neurology phenotype working group of the Cohorts for Heart and Aging Research in Genomic Epidemiology (CHARGE) consortium (Beecham  et al. , 2017). Analytical efforts within the ADSP are divided into multiple working groups focused on data flow, quality control ( Malamon  et al. , 2016 ), structural variant calling, family-based analyses ( Ahmad  et al. , 2016 ;  Beecham  et al. , 2016 ;  Jaworski  et al. , 2016 ), case/control analyses and protective variant analyses. The annotation working group is tasked with providing broad, consistent variant annotation resources for all workgroups, across all datasets and genomic builds, and with specific emphasis on functional effects of variants within AD-related tissues. To date, most sequencing studies have generated  ad hoc , minimal annotation sets, which provide insight into only basic biological function and largely ignores the biological complexity of tissue-specific effects variants may have. Furthermore, on a project-level, there has been no described attempt to generate and maintain a consistent set of comprehensive annotations as a resource for all analyses. In this work, we describe an  in silico  annotation approach, offering guidance and resources to other ADSP investigators, and providing a common base for testing functional hypotheses in their analyses. We provide a summary of annotated variant consequences, illustrate the impact of using a tissue-specific transcript reference set, examine the annotation impact of including regulatory elements and provide an overview of issues arising from an anticipated transition from build 37 to build 38. 2 Materials and methods 2.1 The Alzheimer’s Disease Sequencing Project and the annotation pipeline Details of the sample selection criteria and AD phenotyping are described in detail elsewhere ( Beecham  et al. , 2016 , 2017). In brief, ∼1400 LOAD families were reviewed for potential inclusion in the project, excluding families with known Mendelian mutations for AD. Families were prioritized based on the number of affected individuals, the number of generations affected, age of onset and absence of apolipoprotein E (APOE) ε4 risk alleles. Cases met the National Institute of Neurological and Communicative Disorders and Stroke—the Alzheimer’s NINCDS-ADRDA (Alzheimer's Disease and Related Disorders Association) criteria for possible, probable or definite AD ( McKhann  et al. , 1984 ). Case/control samples were selected from a pool of over 30 000 samples under a balanced risk/protection design; cases were selected with low  a priori  risk (younger and lacking APOE ε4 alleles), and control samples selected based on low rates of expected conversion to AD. Using these two sample sets, the ADSP pursued two complementary sequencing efforts—WGS on multiplex families, and WES on a large dataset of unrelated cases and controls. Details of the sequencing, capture design and quality control procedures for the ADSP data are described elsewhere ( Malamon  et al. , 2016 ) and here described briefly. Extracted genomic DNA was sent to one of three Large-Scale Sequencing and Analysis Centers for sequencing using Illumina WGS technology, the Broad Institute Genomics Service (Broad), the Baylor College of Medicine Human Genome Sequencing Center (Baylor) or the McDonnell Genome Institute at the Washington University in St. Louis (WashU). Resulting sequence reads were aligned to GRCh37 1000 Genomes reference (human_g1k_v37.fasta.gz within GATK resources) and genotype calling of both bi-allelic single nucleotide variants (SNVs) and insertion–deletion variants (indels) was performed centrally on the entire sample set using GATK-HaplotypeCaller and Atlas V2. The ADSP QC working group then applied pipeline-specific variant-level and sample-level quality control criteria, and a consensus call set was generated by including high-certainty genotype calls that were identical across the two calling pipelines. Multiple algorithms are also being applied to call structural variants from WGS data. As the ADSP progresses to the replication phase, sequencing centers are shifting their sequencing and alignment protocols to use GRCh38. A visual overview of the ADSP annotation pipeline is shown in  Figure 1 . Variants (SNVs) are first annotated to all Ensembl transcripts, producing multiple possible functional consequence predictions per variant. Variant consequences are then ranked by a custom ranking process based on Ensembl’s severity ranking, but that up-weights consequences of protein-coding transcripts (to produce an overall most-damaging consequence per gene for each variant) relative to non-coding or nonsense mediated decay transcripts. We also produce rankings using tissue-specific transcripts based on RNA-sequencing isoform data (to produce a tissue-specific most-damaging consequence per gene for each variant). These variant annotations are released as flat-files allowing one variant to have multiple annotated consequences. Attributes that are specific to a single variant agnostic of transcript information, such as external allele frequency data, variant scores etc. are generated and stored as a single flat annotation (with one entry per variant). While the primary unit of analysis for ADSPs is currently SNVs, structural variants of various sizes are also being called for analysis and are processed by our annotation pipeline with some limitations [SPIDEX™ and Contextual Analysis of TF Occupancy (CATO) scores are not available for structural variants].
 Fig. 1. Overview of ADSP annotation pipeline. The process begins with VCF input (left). Solid square items represent pipeline workflow processes, cylinder items are external data sources, and open items are intermediate files or outputs (right) 2.2 Comparisons to other variant sets For the purposes of both validating variant calling, and to annotate known variants with additional frequency information, we accessed multiple sources of allele frequency information. We report minor allele frequencies (MAF) for variants called within two major genotyping projects, the 1000 Genomes Project ( Abecasis  et al. , 2012 ) [including AFR (African), AMR (American), ASN (Asian) and EUR (European) populations] and the NHLBI-ESP [AA (African-American) and EA (European-American) populations]. Together, these two sources reflect exome variation and low coverage sequencing of the remainder of the genome. We also included variant information from the ExAC representing exome variants called from ∼60 000 samples. We accessed additional variant allele frequencies from the Known Variants (Kaviar) database ( Glusman  et al. , 2011 ), comprised of SNVs, small insertions and deletions, and complex variants discovered in humans. It contains 162 million SNV sites, of which 25 million were not reported in dbSNP, and data from 35 projects spanning 77 781 individuals (13.2 K whole genome, 64.6 K exome). The database also includes 50 million short indels and substitutions from a subset of the data sources. Kaviar excludes cancer genomes but includes some data from cell lines and individuals affected by disease. Given the importance of age on AD phenotypes, we also accessed WGS data from the Wellderly study ( Erikson  et al. , 2016 ). The Wellderly study investigates a cohort of elderly individuals with an age range of 80–105 years and who have no diagnosed chronic medical diseases. Using genomic sequencing, the whole genomes of 600 participants were analyzed and correlated with genetic data collected by Inova Translational Medicine Institute (ITMI) from 1507 adults, representing the general population. 2.3 Variant consequence prediction and tissue-specific consequences The starting point of our annotation strategy, VEP ( Yourshaw  et al. , 2015 ), overlays variant positions with extensive resources from the Ensembl database, and algorithmically assigns a predicted consequence to a variant based on where it falls with genes, transcripts and protein sequence, and further assigns a Sequence Ontology (SO) term ( Mungall  et al. , 2002 ) for every variant consequence. Variant consequences are reported with respect to each transcript and protein within the Ensembl database, meaning that a single variant can have multiple consequences, relative to the multiple distinct transcripts it affects. Additional identifiers are reported from various data sources, including from dbSNP ( Smigielski, 2000 ), consensus coding sequence (CCDS) ( Pruitt  et al. , 2009 ), Uniprot ( Magrane and Consortium, 2011 ) and HGVS ( den Dunnen  et al. , 2016 ). Because multiple variant consequences are reported for each transcript/protein affected by a variant, Ensembl VEP provides an option (‘–per_gene’) to generate a ‘most damaging consequence’ for each gene–variant relationship. Using this option, when a variant overlaps multiple possible transcripts for a gene, the consequences are ranked according to multiple criteria within the VEP logic, including the canonical status of the transcript, an Ensembl estimate of transcript support level, the biological type of transcript (preferring protein-coding transcripts) and translated transcript length. For the specific purposes of ADSP analyses, we constructed a custom ranking table ( Supplementary Table S1 ) that down-weights transcripts undergoing nonsense mediated decay and RNA-based transcripts. Using this approach, we generate both full variant annotation and a ‘most damaging consequence’ annotation. To provide a tissue-specific set of variant annotations, we accessed RNA-seq based transcript expression data from a study of 276 samples of temporal cortex and 275 samples of cerebellum ( Allen  et al. , 2016 ), generated as part of the Accelerating Medicines Partnership for Alzheimer’s Disease (AMP-AD). Normalized transcripts were accessed from the AMP-AD Portal ( AMPAD Knowledge Portal, 2016 ;  Hodes and Buckholtz, 2016 ). Based on prior assessments of RNA-seq data ( Hebenstreit  et al. , 2011 ), transcripts were considered ‘highly expressed’ within a tissue if the average normalized transcript count &gt;1 (corresponding to log 2  &gt; 0). 2.4 Additional annotations for coding variants VEP also provides SIFT and PolyPhen predictions for each variant. These scores assess the impact of amino acid substitution based on sequence homology and the physical properties of amino acids ( Kumar  et al. , 2009 ) and impact on structure and function of a human protein ( Adzhubei  et al. , 2010 ). We also employ external scoring metrics including CADD ( Kircher  et al. , 2014 ) and SPIDEX ( Xiong  et al. , 2015 ). These metrics gauge how damaging a variant might be, and thus whether the variant would be a potential candidate for subsequent investigation. The CADD score ( Kircher  et al. , 2014 ) evaluates the deleteriousness of SNVs as well as indels in the human genome. CADD is based on allelic diversity, pathogenicity of both coding and non-coding variants, experimentally measured regulatory effects, and quantitatively prioritizes variants across a wide range of functional categories, effect sizes and genetic architectures. While CADD was designed to provide uniform variant pathogenicity scoring across both coding and non-coding regions, some comparisons indicate that CADD may be best suited for scoring coding variants ( Shihab  et al. , 2015 ). CADD also relies on evolutionary conservation as a predictive feature, and it is unclear the extent to which risk alleles for a late-onset condition like AD will be conserved. The CADD score is presented on a PHRED-based scale, with scores from 0 to 20 generally considered as non-deleterious and scores &gt;20 generally considered as deleterious, with an accuracy of above 99.0% (the variant is amongst the top 1% of deleterious variants). SPIDEX ( Xiong  et al. , 2015 ) assesses whether a SNV causes a dysregulation of a splicing event. The score covers all synonymous, missense and nonsense exonic SNVs, as well as intronic SNVs that are in proximity of splice junctions. It evaluates exons on RefSeq transcripts for predefined features and uses a prediction model to infer whether the exon was spliced into the transcript given the reference genome and the given variant with respect to 16 human tissues. The SPIDEX scores exhibit substantial predictive performance, especially for deleterious mutations, such as intronic mutations that are far from splice sites. SPIDEX scores are available as transformed  z -scores, with values &gt;3 indicating a high likelihood of splicing. 2.5 Additional annotations for non-coding variants Unlike coding variants, VEP provides only minimal annotation of non-coding variants, indicating only intron-exon boundaries or overlap with a known regulatory feature, often a sequence motif or epigenetic state. We expanded this set of annotations to include expressed enhancers and predictions of transcription factor occupancy genome-wide. Using capped analysis of gene expression (CAGE) data, we identified expressed enhancer elements from the FANTOM5 project ( Andersson  et al. , 2014 ). Elements were used from all available cellular contexts, and were matched (by the most relevant tissue) to expression quantitative trait loci (eQTL) associations from the genotype tissue expression (GTEx) project ( Mele  et al. , 2015 ) (analysis by tissue v6) to identify enhancer–gene relationships. Because eQTL data were only used to establish a linkage between known enhancer elements and the genes they regulate, we used a nominal significance level ( P  &lt; 0.05). After compiling data, we identified WGS-based variants that were in or within +/−500 bases from 20 220 enhancer elements and tied these elements to the genes they putatively impact. We also included CATO scores ( Maurano  et al. , 2015 ) to provide a recalibrated probability of affecting the binding of any transcription factor. CATO also provides a quantitatively ranked list of transcription factor families where binding might be altered. CATO scores can only be applied to variants in regulatory regions and therefore are only applicable to a specific subset of the ADSP data. CATO scores above 0.1 are generally considered indicative of variants that significantly alter transcription factor binding. 2.6 LiftOver-based annotation Because of the multi-stage nature of the ADSP, the project data will migrate to build 38 between the discovery and replication phases. This migration will raise many issues, most imminently, how to compare statistical results from the discovery phase (build 37) to those of the replication phase (build 38). Variants mapped within build 37 were transitioned to build 38 using LiftOver, part of the UCSC Genome Browser toolset ( Karolchik  et al. , 2007 ). For known variants (those with RSIDs), the resulting LiftOver build 38 coordinates were compared to the build 38 coordinates from the Ensembl Variant database (version 80), which were mapped to build 38 using the Ensembl pipeline. Variants called by the ADSP QC working group were assigned unique variant identifiers consisting of chromosome, position, reference allele, alternate allele and genomic build. Additional variant identifiers (such as reference sequence RS numbers) are assigned as part of our annotation process from external data sources, including Ensembl (version 80) and dbSNP (build 144). To facilitate the comparison of statistical results and to support annotation-based replication analyses, we have also constructed a single unique variant index to identify variants in a build-agnostic manner. 3 Results 3.1 Allelic discovery in the ADSP At the completion of the Discovery Phase of the ADSP, a total of 578 individuals from 111 families were whole-genome sequenced, and 10 913 unrelated cases and controls were whole-exome sequenced. An overview of variant annotations and frequencies is shown in  Table 1 . Following QC, WGS generated 27 896 774 called variants, with just over 5 million novel variants (18.16%) not previously reported by dbSNP (b144) or ExAC (v0.3). The majority of variants (84%) are annotated as intronic or intergenic. Based on Ensembl VEP, variants were annotated to an average of 3.5 features, including transcripts and regulatory motifs. Crude allele frequency estimates (not accounting for family structure) show the majority of identified variants are rare (MAF &lt; 0.01) with ∼25% observed on only one or two chromosomes. Compared to other published studies of exome-sequence variants, we observe a larger number (22.4%) of common variants (MAF &gt; 0.05).
 Table 1. Overview of ADSP variant annotations Whole exome (case/control) Whole genome (family-based) Variants called 1 586 703 — 27 896 774 — Variants annotated 1 586 703 — 27 674 996 — Variants unannotated 0 — 221 778 — Variants in ExAC v0.3 933 318 58.82% 361 205 1.29% Variants in dbSNP 936 417 59.02% 22 837 563 81.86% Variants in ClinVar 17 860 1.13% 10 960 0.04% Variants in Wellderly 163 733 10.31% 10 304 395 36.93% Novel variants 608 092 38.32% 5 065 664 18.16% Average transcripts per variant 7.75 — 3.494 — AF&gt;0.05 35 377 2.23% 6 247 716 22.40% 0.01&lt;AF&lt; 0.05 20 566 1.30% 4 795 467 17.19% Two observations&lt;AF&lt;0.01 1 000 280 63.04% 9 187 863 32.94% Two observations 152 770 9.63% 3 554 970 12.74% One observation 377 555 23.79% 4 110 758 14.74% In contrast, WES revealed 1 586 703 called variants, of which 38.32% were previously unreported. Over all variants, an average of 7.75 transcripts were impacted per variant, illustrating the enrichment of coding variants. The categories of predicted variant consequence are shown relative to the allele frequency spectrum of WES variants in  Figure 2 . The frequency spectrum of captured variants is heavily shifted toward low-frequency alleles (MAF &lt; 0.01), with only 3.5% of variants having a larger frequency. We observe an expected enrichment for missense and synonymous variants relative to other consequence types. CADD scores are highest on average for stop-gained variants, followed by splicing-associated variants. There is also a notable relative lack of overlap with variants identified in the Wellderly cohort in the WES versus WGS, suggesting a relative lack of coding alleles in this successfully aged cohort.
 Fig. 2. Allele frequency spectrum by variant annotation (whole-exome sequencing). Total variant counts from the ADSP WES case/control dataset are shown by VEP predicted consequence and dataset minor allele frequency (inset legend). CADD score averages (center point) ±1 SD are shown as embedded lines 3.2 Non-coding variant annotation ∼3% of WGS-identified variants are annotated to a regulatory region. Regulatory annotations are shown by variant frequency in  Figure 3 . VEP reported regulatory consequences for many variants, including 438 461 variants within a CTCF binding site, 295 603 variants within an open chromatin region, 707 302 within predicted enhancer regions, 437 642 within predicted promoter regions, 1 719 607 within predicted promoter flanking regions and 247 701 within transcription factor binding sites. Of these, variants within predicted promoter regions have the highest CADD scores (avg. 7.7, SD 4.98). In contrast, CATO score predictions of transcription factor occupancy are highest in transcription factor binding sites and CTCF binding sites, as expected (avg. 0.05, SD 0.06).
 Fig. 3. Allele frequency spectrum by regulatory annotation (whole-genome sequencing). Total variant counts from the ADSP WGS family-based dataset are shown by either VEP predicted regulatory consequence or FANTOM5 enhancer annotation, and crude minor allele frequency estimates (inset legend) from the dataset. CADD score averages (center point)  ±1 SD are shown as embedded lines We also expanded our annotation set by including regulatory enhancers identified from FANTOM5 and matched to nearby genes using eQTL from the GTEx) project ( Mele  et al. , 2015 ). Mapping distant regulatory elements to the genes they regulate provides a biology-based means to include additional variants into a gene unit-based association test. Including FANTOM5-based enhancers, we annotate an additional 86 789 variants from the WGS dataset to 31 351 genes. Variants are an average of 380 KB (SD 298 KB) from the transcription start or end site when they fall outside gene bounds, and add an average of 35 variants to each gene-unit. Variants also have an average CADD score of 6.2 (SD 4.5), and an average CATO score of 0.042 (SD 0.048), showing similar properties to the VEP predicted promoter regions. Of these variants, 1576 have CADD scores &gt;20 (indicating a predicted damaging impact), which likely influence 10 528 genes. In total, 1111 regulatory variants potentially impacting 8375 genes have CATO scores &gt;0.2, indicating a high probability of transcription factor occupancy. 3.3 Tissue-specific transcript reference changes annotations Given the relevance of brain tissues for AD risk, we examined the impact of selecting brain-specific transcript references on variant annotation. Using both the full transcript reference and a cerebellum-specific transcript reference, we generated a ‘most damaging consequence per gene’ for each variant. With the full Ensembl transcript set, 1 586 703 variants had 2 812 045 predicted consequences ultimately affecting 30 121 genes. In this context, the Ensembl definition of ‘gene’ includes immunoglobulin, RNA-based and pseudo genes. Restricting annotations to only genes highly expressed in the cerebellum based on RNA-sequencing, we annotate 1 230 598 variants with 1 574 165 predicted consequences affecting 14 312 genes. The RNA-seq dataset notably reduced annotations to mostly (90.8%) protein-coding transcripts. Of variants annotated using both transcript references, 95% of variant annotations were identical, with 63 010 annotations (4%) having a different predicted consequence relative to cerebellum-specific transcripts.  Figure 4  illustrates the change in variant consequence relative to total variant annotations. These differences were largely due to alternative splicing, with 27% annotated as intron variants. Most other changes are due to alternate transcription start sites, annotated as upstream gene variants (10.8%) and 5′-UTR variants (4%), and alternate transcription end sites, annotated as downstream gene variants (4.46%) and 3′-UTR variants (8.58%).
 Fig. 4. Transitions between WES annotation consequences when using full versus cerebellum-expressed transcript references. The total number of variant consequences from the ADSP WES case/control dataset is shown in the inner ring. Transitions between variant consequences when shifting from the full transcript set to cerebellum-expressed transcripts are shown via internal lines, with proportions shown in the outer ring. For example, the most common transition was from missense to intron due to differential splicing in the cerebellum Because a likely application of variant annotations is to weight or prioritize variants for gene-based burden tests, we also examined the impact that transcript references have to the collection of consequences by gene. While only 4% of variant annotations are altered when using a cerebellum-based transcript reference, those annotations impact 4490 genes. Of the genes affected, 3973 (88.4%) contained at least one annotation with a different predicted impact on the gene function, indicating that for gene-based tests that use variant impact (such as loss-of-function analyses) may be significantly altered by selecting a tissue-specific transcript reference. 3.4 LiftOver accuracy assessments We examined the impact of using LiftOver by first quantifying variants that are unmapped by the LiftOver process (only 148 from WES and 3831 from WGS), and by comparing the resulting GRCh38 coordinates for known variants (identified by RSIDs) to those reported in the Ensembl Build 38 database. From the WES data, 15 751 variants (1% of all mapped variants) showed discrepancy between the LiftOver coordinates and the Ensembl coordinates, with an average of 55 808 bases away from the Ensembl position. From the WGS data, this proportion slightly increased with 360 459 variants (1.29%) showing discrepancy with an average of 831 512 bases away from the Ensembl position. As expected from WES data, the majority of these coordinate mismatches affect missense (53.3%) and synonymous (33.58%) variants. Within the WGS data, most variants are intronic (46.63%) and intergenic (29.39%), with a small number of downstream (6.625) and upstream variants (8.72%). 4 Discussion A growing list of annotation resources are now available for the prioritization and filtering genomic variants. Due to the density of available data, a key hurdle of sequencing projects is to precisely define  a priori  what annotation resources will be used to test various functional hypotheses within the data. In this paper, we describe a variant annotation pipeline developed for the ADSP, and the resulting annotations for over 30 million distinct variants, over 5 million of which are novel. As expected, the vast majority of variants identified are low-frequency events, with WES of AD cases and controls identifying mostly missense and synonymous variants, and WGS of AD-affected family members identifying intergenic, intronic and regulatory variants. While the general practice in genomic analysis is to assume a single, most-damaging biological consequence per variant, it is important to note that variants have a variety of effects across biological contexts—94% of all annotated WES variants impact two or more transcripts. Due to tissue-specific splicing and expression, this implies that no variant has a singular effect. As others have noted ( Frankish  et al. , 2015 ), the choice of transcript reference can have a significant impact on predicted variant consequences, however this work explores changes to the global transcript reference set (Ensembl versus RefSeq). Given the specific relevance of the central nervous system for AD pathogenesis, we explored how tissue-specific splicing changes variant annotation. Restricting annotations to transcripts within tissues of interest (cerebellum and temporal cortex) showed only a modest change to individual variant annotations; however, these changes can have a large impact on the downstream gene-based analyses that use them. With nearly one-fourth of protein-coding genes harboring variants that change their consequence and impact, statistical analyses using tissue-specific annotation could result in dramatically different association test results. Generating a definitive conclusion on what is ‘expressed’ in any given tissue however is difficult, and introduces new algorithm thresholds that may influence the resulting annotation. Applying regulatory annotations from the FANTOM5 project in conjunction with eQTL data from the GTEx project provides a tissue-specific set of long-range enhancers tied to the genes they potentially regulate. Including these annotations for variants identified in WGS of families extends the traditional concept of gene-based unit tests to include variants within their regulatory elements, adding on average 35 variants to each gene-unit. Average CADD scores for FANTOM5 enhancer elements are increased relative to other regulatory annotations, and incorporating these along with promoter elements should greatly improve the interpretation of regulatory rare-variant hypothesis tests. Ideally, all annotation resources would be regenerated for GRCh38, however this would require extensive effort to reconstitute the enormous training data resources for prediction algorithms such as CADD. Anticipating the need to use LiftOver for GRCh37-mapped annotation resources, we explored how well LiftOver mapped our own variants into GRCh38 by comparing the coordinates of known variants to the GRCh38 version of Ensembl. While only a fraction of a percentage of variants were unmapped by the LiftOver process, roughly 1% of known variants had discrepant positions between LiftOver and Ensembl. The majority of these were localized to chromosome 6, likely owing to the complexity of the major histocompatibility complex region, but also to chromosomes 15 and 17. These mapping errors were not insignificant, with LiftOver coordinates an average of 50 KB from the reported Ensembl position. This result indicates that while LiftOver is an important stop-gap approach for mapping annotation resources into GRCh38,  post-hoc  checks for annotation accuracy will be needed to ensure that the biological implications of GRCh38 variants are properly portrayed, and whenever possible, resources constructed based on GRCh38 should be used instead. All together, we show that annotation is more than ever a critical component of genetic data analysis. Regardless of the annotation strategy employed, it is critical to be aware that variant annotation injects biological assumptions into the statistical analysis of genomic data. Providing a uniform set of annotation resources for all ADSPs will ease comparisons between analyses and informs the interpretation of results. Funding A detailed acknowledgment of all funding sources supporting this work is available in the  Supplementary Information . 
 Conflict of Interest : none declared. Supplementary Material Supplementary Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>TEAM: efficient two-locus epistasis tests in human genome-wide association study</Title>
    <Doi>10.1093/bioinformatics/btq186</Doi>
    <Authors>Zhang Xiang, Huang Shunping, Zou Fei, Wang Wei</Authors>
    <Abstract>As a promising tool for identifying genetic markers underlying phenotypic differences, genome-wide association study (GWAS) has been extensively investigated in recent years. In GWAS, detecting epistasis (or gene–gene interaction) is preferable over single locus study since many diseases are known to be complex traits. A brute force search is infeasible for epistasis detection in the genome-wide scale because of the intensive computational burden. Existing epistasis detection algorithms are designed for dataset consisting of homozygous markers and small sample size. In human study, however, the genotype may be heterozygous, and number of individuals can be up to thousands. Thus, existing methods are not readily applicable to human datasets. In this article, we propose an efficient algorithm, TEAM, which significantly speeds up epistasis detection for human GWAS. Our algorithm is exhaustive, i.e. it does not ignore any epistatic interaction. Utilizing the minimum spanning tree structure, the algorithm incrementally updates the contingency tables for epistatic tests without scanning all individuals. Our algorithm has broader applicability and is more efficient than existing methods for large sample study. It supports any statistical test that is based on contingency tables, and enables both family-wise error rate and false discovery rate controlling. Extensive experiments show that our algorithm only needs to examine a small portion of the individuals to update the contingency tables, and it achieves at least an order of magnitude speed up over the brute force approach.</Abstract>
    <Body>1 INTRODUCTION Genetic association analysis examines the statistical correlation between an organism's genotype with its phenotype. With the development of high-throughput genotyping technologies, genetic variation of human and other model organisms has been measured at genome-wide scale. As the most abundant source of genetic variation, the number of single nucleotide polymorphism (SNPs) in public databases (dbGaP, JAX) is up to millions. Genome-wide association study (GWAS) has been shown to be a promising tool to locate the genetic factors that cause phenotypic differences (Saxena  et al. ,  2007 ; Scuteri  et al. ,  2007 ; WTCCC,  2007 ; Weedon  et al. ,  2007 ). Epistasis, or gene–gene interaction detection, has received increasing attention in complex trait analysis. Different from single-locus approach, the goal of two-locus epistasis detection is to identify interacting SNP pairs that have strong association with the phenotype. Please refer to Balding ( 2006 ), Hirschhorn and Daly ( 2005 ), Hoh and Ott ( 2003 ) and Musani  et al.  ( 2007 ) for reviews of current progress and challenges in epistasis detection in GWAS. There are two grand challenges in epistasis detection. The first is to develop statistical tests that can effectively capture the interaction between SNPs. Various tests have been proposed for two-locus association study, such as the chi-square test, likelihood ratio test and entropy-based test (Balding,  2006 ). Another crucial challenge in two-locus association study is the intensive computational burden imposed by the enormous search space. Suppose that there are  N  SNPs for  M  individuals. The overall search space of pairwise interactions is  MN ( N  − 1)/2. The large number of tests also causes the multiple testing problem (Miller,  1981 ). Controlling the family-wise error rate (FWER) and false discovery rate (FDR) are standard ways to control the error rate (Dudoit and Laan,  2008 ; Westfall and Young,  1993 ). In the FWER and FDR controlling, permutation test is preferred over simple Bonferroni correction since many SNPs are correlated (Churchill and Doerge,  1994 ). The correlation structure among genotype profiles is preserved across permutations and is incorporated into permutation  P -value estimation. The idea of permutation test is to randomly shuffle the phenotype values among the individuals and recalculate the test statistics. The distribution of these test values are used to estimate the null distribution. Permutation test dramatically increases the search space. With  K  permutations, the entire search space of two-locus association mapping is  KMN ( N  − 1)/2. Consider a moderate GWAS setting, in which  M  = 1000,  N  = 100 000 and  K  = 1000. The size of the search space is about 5 × 10 15 . Apparently, a brute force enumeration of the search space is infeasible and thus efficient algorithms are in demand. Although the computational challenge of epistasis detection has been well recognized, the algorithmic development is still very limited. For a small number of SNPs, e.g. from tens to a few hundreds, exhaustive algorithms that explicitly enumerate all possible SNP combinations have been developed (Nelson  et al. ,  2001 ; Ritchie  et al. ,  2001 ). These methods are not scalable for genome-wide computing. Genetic algorithm (Carlborg  et al. ,  2000 ) has been proposed. This approach is heuristic, which does not guarantee to find the optimal solution. To avoid explicitly exploring the entire search space, a common heuristic used in epistasis detection is a two-step approach (Evans  et al. ,  2006 ; Hoh  et al. ,  2000 ; Yang  et al. ,  2009 ). First, a subset of SNPs are selected according to certain criteria. Then the selected SNPs are used for subsequent epistatic analysis. However, the SNP screening process suffers from the same problem as the single-locus approach. SNPs with strong epistasis but low marginal effects are likely to be filtered out (Zhang  et al. ,  2009a ). Recently, the approach based on search space pruning has been shown to be able to dramatically speed up the process of epistasis detection without compromising the optimality of the results. FastANOVA (Zhang  et al. ,  2008 ) and FastChi (Zhang  et al. ,  2009b ) are specifically designed for ANOVA test and chi-square test, respectively. The COE algorithm (Zhang  et al. ,  2009a ) is a more general approach that is applicable to all convex tests. Utilizing an upper bound derived for the test being used, these algorithms only need to examine a small number of promising SNP pairs and prune the SNP pairs that are proven to have no strong association with the phenotype. Unlike heuristic approaches, these algorithms are guaranteed to find the optimal solution. Although these methods provide promising alternatives for GWAS, there are two major drawbacks that limit their applicability. First, they are designed for relatively small sample size and only consider homozygous markers (i.e. each SNP can be represented as a {0, 1} binary variable). In human study, however, the sample size is usually large and most SNPs contain heterozygous genotypes and are coded using {0, 1, 2}. These make existing methods intractable. Second, although the FWER and the FDR are both widely used for error controlling, existing methods are designed only to control the FWER. From a computational point of view, the difference in the FWER and the FDR controlling is that, to estimate FWER, for each permutation, only the maximum two-locus test value is needed. To estimate the FDR, on the other hand, for each permutation, all two-locus test values must be computed. Further details of the FWER and the FDR controlling are described in  Section 2 . In this article, we propose an exhaustive algorithm, TEAM (Tree-based Epistasis Association Mapping), for efficient epistasis detection in human GWAS. TEAM has several advantages over previous methods.
 It supports to both homozygous and heterozygous data. By exhaustively computing all two-locus test values in permutation test, it enables both FWER and FDR controlling. It is applicable to all statistics based on the contingency table. Previous methods are either designed for specific tests or require the test statistics to satisfy certain property. Experimental results demonstrate that TEAM is more efficient than existing methods for large sample study. 
 TEAM incorporates permutation test for proper error controlling. The key idea is to incrementally update the contingency tables of two-locus tests. We show that only four of the 18 observed frequencies in the contingency table need to be updated to compute the test value. In the algorithm, we build a minimum spanning tree (Cormen  et al. ,  2001 ) on the SNPs. The nodes of the tree are SNPs. Each edge represents the genotype difference between the two connected SNPs. This tree structure can be utilized to speed up updating process for the contingency tables. A majority of the individuals are pruned and only a small portion are scanned to update the contingency tables. This is advantageous in human study, which usually involves thousands of individuals. Extensive experimental results demonstrate the efficiency of the TEAM algorithm. 2 THE PROBLEM OF TWO-LOCUS EPISTASIS DETECTION IN HUMAN GWAS Suppose that the genotype dataset consists of  N  SNPs { X 1 ,…,  X N } for  M  individuals { S 1 ,…,  S M }. We adopt the convention of using 0 and 2 to represent the homozygous majority and homozygous minority genotypes, respectively, and 1 to represent the heterozygous case. Let  Y 0  ∈ {0, 1} be the phenotype of interest (0 for controls and 1 for cases). Let  Y ′ = { Y 1 ,…,  Y K } be the set of  K  permutations of  Y 0 . In each permutation  Y k , the phenotype labels are randomly reassigned to individuals with no replacement. Table 1  shows an example dataset of SNPs and phenotype permutations. The genotype dataset consists of six SNPs { X 1 ,…,  X 6 } for 24 individuals { S 1 ,…,  S 24 }. Individuals { S 1 ,…,  S 12 } are cases and { S 13 ,…,  S 24 } are controls. The phenotype is permuted five times, i.e.  Y ′ = { Y 1 ,…,  Y 5 }.
 Table 1. An example dataset consisting of six SNPs { X 1 ,…,  X 6 }, the original phenotype  Y 0  and five phenotype permutations { Y 1 ,…,  Y 5 } for 24 individuals { S 1 ,…,  S 24 } S 1 S 2 S 3 S 4 S 5 S 6 S 7 S 8 S 9 S 10 S 11 S 12 S 13 S 14 S 15 S 16 S 17 S 18 S 19 S 20 S 21 S 22 S 23 S 24 X 1 0 0 0 1 2 0 2 0 2 0 0 2 0 0 0 2 0 2 1 0 0 2 2 0 X 2 2 2 0 2 0 2 0 2 2 2 2 0 1 0 0 2 0 2 1 0 2 2 2 2 X 3 2 0 0 2 0 2 0 1 2 1 2 2 1 0 2 2 0 2 1 2 2 2 2 2 X 4 0 2 2 0 0 0 2 1 0 2 2 0 0 0 0 0 0 0 1 0 1 2 0 0 X 5 0 2 2 0 0 0 1 1 2 1 2 0 0 0 0 0 0 2 1 0 2 2 0 2 X 6 0 2 2 0 0 0 2 1 0 1 2 0 0 0 0 2 0 2 1 0 2 2 0 0 Y 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 Y 1 0 1 0 0 1 1 1 0 0 0 1 1 1 1 1 0 1 0 0 1 0 1 0 0 Y 2 0 0 1 0 1 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 1 0 0 Y 3 1 0 1 1 1 1 0 1 1 1 0 0 0 1 0 1 0 0 1 0 0 0 1 0 Y 4 0 1 1 1 1 1 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 1 1 0 Y 5 1 0 1 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 0 0 0 1 1 1 
 Let ℑ denote the statistical test to be used. Specifically, we represent the test value of SNP  X i  and phenotype  Y k  (0≤ k ≤ K ) as ℑ( X i ,  Y k ), and represent the test value of SNP pair ( X i X j ) and  Y k  as ℑ( X i X j ,  Y k ). A contingency table that records the observed values of certain events, is the basis of many statistical tests.  Tables 2–4  show contingency tables for the single-locus tests ℑ( X i ,  Y k ) and ℑ( X j ,  Y k ), genotype relationship between SNPs  X i  and  X j  and two-locus test ℑ( X i X j ,  Y k ), respectively.
 Table 2. Contingency tables for single-locus tests ℑ( X i ,  Y k ) and ℑ( X j ,  Y k ) Contingency table for ℑ( X i ,  Y k ) Contingency table for ℑ( X j ,  Y k ) X i =0 X i =1 X i =2 Total X j =0 X j =1 X j =2 Total Y k =0 Event  A Event  B Event  E Y k =0 Event  G Event  H Event  I Y k =1 Event  C Event  D Event  F Y k =1 Event  J Event  L Event  O Total M Total M 
 Table 3. Contingency table for genotype relation between two SNPs  X i  and  X j X i =0 X i =1 X i =2 Total X j =0 Event  S Event  T Event  R X j =1 Event  P Event  Q Event  U X j =2 Event  V Event  W Event  Z Total M 
 Table 4. Contingency table for two-locus test ℑ( X i X j ,  Y k ) X i =0 X i =1 X i =2 Total X j =0 X j =1 X j =2 X j =0 X j =1 X j =2 X j =0 X j =1 X j =2 Y k =0 Event  a 1 Event  a 2 Event  a 3 Event  b 1 Event  b 2 Event  b 3 Event  e 1 Event  e 2 Event  e 3 Y k =1 Event  c 1 Event  c 2 Event  c 3 Event  d 1 Event  d 2 Event  d 3 Event  f 1 Event  f 2 Event  f 3 Total M 
 Due to the large number of hypotheses being tested, multiple testing problem has received considerable attention in GWAS. Controlling the FWER and FDR are two widely used approaches to control the error rate. The FWER is the probability of having at least one false positive. The FDR is the expected proportion of false positives among rejected hypotheses. Permutation test is the standard way to estimate the null distribution in both approaches. Next, we briefly describe the typical procedures of the FWER and FDR control. For statistical background of these approaches, refer to Dudoit and Laan ( 2008 ) and Westfall and Young ( 1993 ) for details. The FWER controlling procedure : for each permutation  Y k  ∈  Y ′, let ℑ Y k  represent the maximum test value among all SNP pairs, i.e. ℑ Y k  = max {ℑ( X i X j ,  Y k )|1 ≤  i  &lt;  j  ≤  N }. The distribution of {ℑ Y k | Y k  ∈  Y ′} is used as the null distribution. Given an error rate threshold α, the  critical value  ℑ α  is the α K -th largest value in {ℑ Y k | Y k  ∈  Y ′}. A SNP pair ( X i X j ) is considered significant if its test value with the original phenotype  Y 0  exceeds the critical value, i.e. ℑ ( X i X j ,  Y 0 )≥ℑ α . The FDR controlling procedure : let  PV  represent the set of the pooled test values of all permutation tests, i.e.  PV ={ℑ( X i X j ,  Y k )|1≤ i &lt; j ≤ N , 1≤ k ≤ K }. The  P −value of test ℑ( X i X j ,  Y 0 ) can be calculated as  p (ℑ( X i X j ,  Y 0 ))=|{ t ≥ℑ( X i X j ,  Y 0 )| t ∈ PV }|/| PV |, i.e. the proportion of the values in  PV  that are no less than ℑ( X i X j ,  Y 0 ). Let  p (1) ≤ p (2) …≤ p ( N ( N −1)/2)  be the ordered  P -values of the original tests. Let  . The classic Benjamini–Hochberg method rejects all hypotheses for which the corresponding  P -values are in the set { p (1) ,  p (2) ,…,  p ( v ) }. In the FWER controlling, we only need the maximum test value of each permutation. To control the FDR, all test values need to be computed to estimate the  P -value of the original tests. The existing algorithms, such as FastChi (Zhang  et al. ,  2009b ) and COE (Zhang  et al. ,  2009a ), prune the SNP pairs having weak associations. Thus they cannot be used to control the FDR. Our algorithm, TEAM, exhaustively computes the test values of all SNP pairs for every permutation. It can be used for both the FWER and FDR controlling. In this article, we mainly focus on the problem of permutation test, since it is the most computationally intensive procedure. Testing SNP pairs using original phenotype can be treated as a special case of permutation test. 3 FREE VARIABLES IN THE CONTINGENCY TABLE OF TWO-LOCUS TEST Let  E event  and  O event  denote the expected frequency and observed frequency of an event in  Tables 2–4 . Note that each event represents a subset of individuals. For example, event  D  is a subset of individuals satisfying ( X i  = 1 ∧  Y k  = 1), and  O D  represents its observed frequency, i.e.  O D  = | D |. Using the dataset in  Table 1 , consider  X 3  and  Y 4  (i.e.  i  = 3 and  k  = 4), we have  D  = { S 10 ,  S 13 ,  S 19 }, and  O D  = 3. Many statistics, such as chi-square test and likelihood ratio test are defined as functions of the observed frequencies in contingency tables. For any test ℑ based on the contingency table, to calculate the two-locus test value ℑ( X i X j ,  Y k ), one needs all 18 observed frequencies for the events in the two-locus contingency table shown in  Table 4 . The following theorem shows that we only need four of the 18 values to calculate the two-locus test value given the three contingency tables in  Tables 2  and  3 . T heorem  3.1. For SNPs X i ,  X j   and permutation Y k ,  given the observed frequencies in   Tables 2   and   3 ,  specifically, the values of  { O D ,  O F ,  O J ,  O L ,  O O ,  O S ,  O P ,  O V ,  O T ,  O Q ,  O W ,  O R ,  O U ,  O Z },  all of the observed frequencies in   Table 4   can be determined if the values of  { O d 2 ,  O d 3 ,  O f 2 ,  O f 3 }  are known . P roof . See Appendix. Suppose that we have all the single-locus contingency tables, i.e.  Table 2 . Given a SNP pair ( X i ,  X j ),  Table 3  is fixed. Thus, from Theorem 3.1, for permutation  Y k , once we have the values of { O d 2 ,  O d 3 ,  O f 2 ,  O f 3 }, ℑ( X i X j ,  Y k ) can be calculated accordingly. In the following, we show that these values can be computed incrementally utilizing a minimum spanning tree built on SNPs. We focus on the incremental process for  O d 2 . The same process can be applied to update  O d 3 ,  O f 2  and  O f 3 . We first discuss how to update  O d 2  for a specific permutation. Then we show that the procedure can also handle all the permutations in a batch mode. 4 BUILDING THE MINIMUM SPANNING TREE ON THE SNPS To build a minimum spanning tree (Cormen  et al. ,  2001 ) on the SNPs, let the SNPs { X 1 ,  X 2 ,…,  X N } be the nodes and SNP pairs ( X i X j ) ( i ≠ j ) be the (undirected) edges. For each edge ( X i X j ), we denote its weight (the number of individuals having different genotypes in the two SNPs) as  w ( X i X j ). A  spanning tree  𝒯 is a tree that spans (connects) all SNPs. Let  V (𝒯) be its node set and  E (𝒯) be its edge set. A  minimum spanning tree  is a spanning tree whose weight  W 𝒯 =∑ w ( X i X j ), where ( X i X j ) ∈  E (𝒯), is no greater than any other spanning tree.  Figure 1  shows the minimum spanning tree built using the example dataset in  Table 1 . The number on each edge represents its weight. For example, in  X 3  and  X 2 , there are six individuals, { S 2 ,  S 8 ,  S 10 ,  S 12 ,  S 15 ,  S 20 }, having different genotypes.
 Fig. 1. The minimum spanning tree built on the SNPs in the example dataset shown in  Table 1 . For any individual, the genotype difference from  X i  to  X j  can be any one of the six combinations, i.e. 0→ 1 (indicating that the genotype in  X i  is 0, and the genotype in  X j  is 1), 1→ 0, 0→ 2, 2→ 0, 1→ 2 and 2→ 1. Using the example dataset in  Table 1 ,  Table 5  shows the genotype differences between the connected two SNPs in the minimum spanning tree in  Figure 1 . We use ( X i X j ) { u → v } ( u ,  v  ∈ {0, 1, 2}) to represent the set of individuals whose genotype in  X i  is  u  and genotype in  X j  is  v . For example, ( X 3 X 2 ) {1→2} ={ S 8 ,  S 10 } and ( X 3 X 2 ) {1→2}∪{0→2}  = { S 2 ,  S 8 ,  S 10 }.
 Table 5. Genotype difference between the connected SNPs in the minimum spanning tree shown in  Figure 1 0→1 1→0 0→2 2→0 1→2 2→1 ( X 3 X 2 ) ∅ ∅ { S 2 } { S 12 ,  S 15 ,  S 20 } { S 8 ,  S 10 } ∅ ( X 2 X 5 ) { S 7 } { S 13 } { S 3 } { S 1 ,  S 4 ,  S 6 ,  S 16 ,  S 23 } ∅ { S 8 ,  S 10 } ( X 5 X 6 ) ∅ ∅ { S 16 } { S 9 ,  S 24 } { S 7 } ∅ ( X 6 X 1 ) { S 4 } { S 8 ,  S 10 } { S 5 ,  S 9 ,  S 12 ,  S 23 } { S 2 ,  S 3 ,  S 11 ,  S 21 } ∅ ∅ ( X 6 X 4 ) ∅ ∅ ∅ { S 16 ,  S 18 } { S 10 } { S 21 } 
 5 INCREMENTALLY UPDATING OBSERVED FREQUENCY  O d 2 In this section, we discuss how to update  O d 2  by utilizing the minimum spanning tree. For clarity, from now on, we use  d 2 ( X i X j ,  Y k ) to denote the specific event  d 2  for the SNP pair ( X i X j ) and permutation  Y k , i.e. the subsets of individuals satisfying ( X i  = 1 ∧  X j  = 1 ∧  Y k =1 ). We use  O d 2 ( X i X j ,  Y k ) to represent its observed frequency, i.e.  O d 2 ( X i X j ,  Y k )=| d 2 ( X i X j ,  Y k )|. This notation also applies to other events in the contingency tables shown in  Tables 2–4 . For example,  D ( X i ,  Y k ) represents the subset of individuals satisfying ( X i  = 1 ∧  Y k =1), and  O D ( X i ,  Y k ) = | D ( X i ,  Y k )|. Next, we show that for any SNP pair ( X i X j ) and an edge ( X j X j ′) ∈  E (𝒯), given  O d 2 ( X i X j ,  Y k ), how to update the value for  O d 2 ( X i X j ′,  Y k ). From the contingency tables in  Table 2–4 , it is easy to see that
 
and
 
The following theorem shows that, given  O d 2 ( X i X j ,  Y k ) and  D ( X i ,  Y k ), using the genotype difference associated with edge ( X j X j ′), we can get the value of  O d 2 ( X i X j ′,  Y k ). T heorem  5.1. For any SNP pair  ( X i X j )  and an edge  ( X j X j ′) ∈  E (𝒯),  we have O d 2 ( X i X j ′,  Y k ) =  O d 2 ( X i X j ,  Y k )+| D ( X i ,  Y k )∩( X j X j ′) {0→1}∪{2→1} |− | D ( X i ,  Y k )∩( X j X j ′) {1→0}∪{1→2} |. P roof . See Appendix. E xample  5.2. Using the example dataset in   Table 1 ,  let i =3,  j =2,  j ′=5,  and k =4,  i.e. we consider SNP pair  ( X 3 X 2 ),  permutation Y 4   and the edge  ( X 2 X 5 )  in   Figure 1 .  Suppose that we already know that O d 2 ( X 3 X 2 ,  Y 4 )=2,  and event D ( X 3 ,  Y 4 )={ S 10 ,  S 13 ,  S 19 }.  From   Table 5 ,  we have  ( X 2 X 5 ) {0→1}∪{2→1} ={ S 7 ,  S 8 ,  S 10 },  and  ( X 2 X 5 ) {1→0}∪{1→2} ={ S 13 }.  Thus according to Theorem 5.1 ,  we have O d 2 ( X 3 X 5 ,  Y 4 )= O d 2 ( X 3 X 2 ,  Y 4 )+|{ S 10 }|−|{ S 13 }|=2.  Note that by this way, we get the value of O d 2 ( X 3 X 5 ,  Y 4 )  from O d 2 ( X 3 X 2 ,  Y 4 ) without scanning all individuals . So far, we have discussed the procedure to update the value of  O d 2 ( X i X j ′,  Y k ) from  O d 2 ( X i X j ,  Y k ) for a specific phenotype permutation  Y k . This procedure can be easily extended to handle all the permutations. From Theorem 5.1, for any permutation  Y k , to update the value of  O d 2 ( X i X j ′,  Y k ) from  O d 2 ( X i X j ,  Y k ), we need the value of  D ( X i ,  Y k ) and the genotype difference associated with edge ( X j X j ′). Note that the genotype difference is fixed once the minimum spanning tree is built. Next, we discuss how to compute  D ( X i ,  Y k ) for all permutations { Y 1 ,  Y 2 ,…,  Y K } in a batch mode in detail. Let  D K ( X i ) be a list of  M  entries, with each entry corresponding to an individual. For each individual  S m , we record in  D K ( X i )[ m ] the set of phenotypes satisfying ( X i  = 1 ∧  Y k =1 ). For example, consider the dataset in  Table 1 , we have that  D K ( X 3 )[8]={ Y 2 ,  Y 3 }.  Table 4  shows the entries of  D K ( X 3 ). Only non-empty entries, i.e.  D K ( X i )[ m ]≠∅, are shown in the table. It is easy to see that, for any  X i  and  Y k , we can get  D ( X i ,  Y k ) from  D K ( X i ) as follows:  D ( X i ,  Y k ) is the set of individuals whose corresponding entries in  D K ( X i ) contain  Y k  as an element, i.e.
 (1) 
For example, using the example dataset in  Table 1 , from  Table 6 , we know that  D ( X 3 ,  Y 4 )={ S 10 ,  S 13 ,  S 19 }.
 Table 6. Entries of  D K ( X 3 ) with empty entries omitted for all permutations in a batch mode Individual id. Phenotype permutations S 8 { Y 2 ,  Y 3 } S 10 { Y 2 ,  Y 3 ,  Y 4 ,  Y 5 } S 13 { Y 1 ,  Y 2 ,  Y 4 ,  Y 5 } S 19 { Y 3 ,  Y 4 } 
 For SNP pair ( X i X j ), let  O d 2 ( X i X j )=[ O d 2 ( X i X j ,  Y 1 ),  O d 2 ( X i X j ,  Y 2 ),…,  O d 2 ( X i X j ,  Y K )]. From Theorem 5.1 and Equation ( 1 ), for any SNP pair ( X i X j ) and an edge ( X j X j ′)∈ E (𝒯), we can get  O d 2 ( X i X j ′) from  O d 2 ( X i X j ) using  D K ( X i ) and the genotype difference information associated with edge ( X j X j ′). First, initialize  O d 2 ( X i X j ′)= O d 2 ( X i X j ). Next, for every  m  (1≤ m ≤ M ), if  Y k  ∈  D K ( X i )[ m ], we update  O d 2 ( X i X j ′) as follows:
 E xample  5.3. Following Example 5.2 ,  we consider the two SNP pairs  ( X 3 X 2 )  and  ( X 3 X 5 ),  with  ( X 2 X 5 )  being an edge of the tree in   Figure 1 .  Assume that D K ( X 3 )  is as shown in   Table 6 ,  and O d 2 ( X 3 X 2 )=[1, 1, 1, 2, 1].  From   Table 5 ,  the genotype difference on edge  ( X 2 X 5 )  is  ( X 2 X 5 ) {0→1}∪{2→1} ={ S 7 ,  S 8 ,  S 10 },  and  ( X 2 X 5 ) {1→0}∪{1→2} ={ S 13 }.  For individual S m  ∈ { S 7 ,  S 8 ,  S 10 } ( S m  ∈ { S 13 }),  we need to increase (decrease) the corresponding values in O d 2 ( X 3 X 2 )  according to D K ( X 3 ).  Table 7   shows the updating process for O d 2 ( X 3 X 5 ).  Initially ,  O d 2 ( X 3 X 5 )= O d 2 ( X 3 X 2 ).  For individual S 7 ,  since its corresponding entry in D K ( X 3),  D K ( X 3)[7]=∅,  O d 2 ( X 3 X 5 )  remains unchanged. For individual S 8 ,  D K ( X 3)[8]={ Y 2 ,  Y 3 },  we increase the values of O d 2 ( X 3 X 5 ,  Y 2 )  and O d 2 ( X 3 X 5 ,  Y 3 )  by 1. Similarly, we increase and decrease the values in O d 2 ( X 3 X 5 )  according to D K ( X 3)  for S 10   and S 13 .  For individual S 19 ,  we do not have any update because S 19  ∉ { S 7 ,  S 8 ,  S 10 }  and S 19  ∉ { S 13 }.  The final result is O d 2 ( X 3 X 5 )=[0, 2, 3, 2, 1].
 Table 7. Updating  O d 2 ( X 3 X 5 ) from  O d 2 ( X 3 X 2 ) for all permutations in a batch mode Y 1 Y 2 Y 3 Y 4 Y 5 O d 2 ( X 3 X 5 ) after initializing 1 1 1 2 1 O d 2 ( X 3 X 5 ) after updating for  S 7 1 1 1 2 1 O d 2 ( X 3 X 5 ) after updating for  S 8 1 2 2 2 1 O d 2 ( X 3 X 5 ) after updating for  S 10 1 3 3 3 2 O d 2 ( X 3 X 5 ) after updating for  S 13 0 2 3 2 1 
 Note that to get the value of  O d 2 ( X i X j ), using a brute force approach, we need to scan a (2+ K ) ×  M  matrix consisting of the genotype of ( X i X j ) and permutations { Y 1 ,  Y 2 ,…,  Y K } for the  M  individuals. In the previous example, to compute the value of  O d 2 ( X 3 X 5 ), the cost of the brute force approach is (3+5) × 24=192. Using our approach, the total number of updates is | D K ( X 3)[8]|+| D K ( X 3)[10]|+| D K ( X 3)[13]|=10, which is significantly less than the cost of the brute force approach. More formally, given  D K ( X i ), the time complexity of updating  O d 2 ( X i X j ′) from  O d 2 ( X i X j ) is  O ( w ( X j X j ′) K ). The procedure of updating  O d 2 ( X i X j ′) from  O d 2 ( X i X j ) can also be applied to update the remaining free variables  O d 3 ( X i X j ),  O f 2 ( X i X j ) and  O f 3 ( X i X j ). Note that, to update  O f 2 ( X i X j ),  O f 3 ( X i X j ), we will need  F K ( X i ), which can be defined in a similar way to that of  D K ( X i ): for each individual  S m , we record in  F K ( X i )[ m ] the set of phenotypes satisfying ( X i  = 2 ∧  Y k =1). 6 THE TEAM ALGORITHM TEAM examines SNP pairs through a double loop, where the outer loop visits a leaf node at a time, and the inner loop traverse the rest of the tree, starting from the parent node of the leaf. Let  O d 2 d 3 f 2 f 3 ( X i X j )=[ O d 2 ( X i X j ),  O d 3 ( X i X j ),  O f 2 ( X i X j ),  O f 3 ( X i X j )]. Let  L (𝒯) ∈  V (𝒯) be the set of leaf nodes of the minimum spanning tree 𝒯. For any  leaf  node  X i  ∈  L (𝒯), let  AP ( X i ) = {( X i X j )| i ≠ j ,  X j  ∈  V (𝒯)}. Let  X a  be the parent node of  X i . Since all SNPs are connected in 𝒯, once we have  O d 2 d 3 f 2 f 3 ( X i X a ), we can update all  O d 2 ( X i X j ) ∈  AP ( X i ) by enumerating the edges in  E (𝒯) in a breath-first traversal starting from  X a . E xample  6.1. Consider the tree in   Figure 1 .  Let X i  =  X 3   and X a  =  X 2 .  We have AP ( X 3 ) = {( X 3 X 2 ), ( X 3 X 5 ), ( X 3 X 6 ), ( X 3 X 1 ), ( X 3 X 4 )}.  Starting from X 3 ,  a breadth-first search will enumerate edges  {( X 2 X 5 ), ( X 5 X 6 ), ( X 6 X 1 ), ( X 6 X 4 )},  which can be utilized to update O d 2 d 3 f 2 f 3 ( X i X j )  for the SNP pairs in AP ( X 3 ). Once the SNP pairs in  AP ( X i ) have been processed, we delete  X i  from  L (𝒯), and repeat the same process for another leaf node. The overall algorithm is summarized in Algorithm 1. Given the SNPs  X ′ = { X 1 ,  X 2 ,…,  X N }, phenotype permutations  Y ′ = { Y 1 ,  Y 2 ,…,  Y K }, we first enumerate and store all single-locus contingency tables. We then build the minimum spanning tree 𝒯, with genotype difference associated with each edge. For leaf node  X i , we compute  D K ( X i ),  F K ( X i ) and  O d 2 d 3 f 2 f 3 ( X i X a ). This information is then used to incrementally update  O d 2 d 3 f 2 f 3  ( X i X j ′) for all SNP pairs in  AP ( X i ). After processing  AP ( X i ), we delete  X i  from 𝒯 and repeat the procedure for the remaining leaf nodes. Time complexity: the time complexity on generating all single-locus contingency tables and building the minimum spanning tree is  O ( MNK ) and  O ( MN 2 ), respectively. The time complexity to compute  D K ( X i ) and  F K ( X i ) for all SNPs is  O ( MNK ). The total updating cost for all  AP ( X i ) is  O ( W 𝒯 NK ). Thus, the overall time complexity of TEAM is  O ( MNK + MN 2 + W 𝒯 NK ). Note that the complexity of the brute force approach is  O ( MN 2 K ). The number of SNPs  N  is the dominant factor. Space complexity: the dataset size is  O ( M ( N + K )). The space needed to store all single-locus contingency tables is  O ( NK ). The size of tree 𝒯 is  O ( W 𝒯 ). The size of  D K ( X i ) and  F K ( X i ) is  O ( MK ). Thus, the total space complexity of TEAM is  O ( M ( N  +  K )+ K ( N + M )+ W 𝒯 ). Note that we can do incremental computation using any exploration order. TEAM utilizes minimum spanning tree to update the contingency tables. The reason is that the cost of such update depends on the difference between the SNPs. The more similar they are, the lower the cost. Since minimum spanning tree has the minimum weight  W 𝒯  over all spanning trees, using it to guide the computation leads to optimal efficiency. It is not absolutely necessary to use a minimum spanning tree. As long as the tree is close to a minimum spanning tree, we should expect good performance. An implementation issue in building the minimum spanning tree is that we need  O ( N 2 ) space to store all pairwise differences between the SNPs. In practise, we divide the SNPs into sub groups of equal size. A minimum spanning tree is built for each group. Then the sub trees are merged to a larger tree by randomly connecting leave nodes. The tree built in this way is an approximate minimum spanning tree. Our focus in this article is not to build an optimal minimum spanning tree, but to use the tree structure for efficient updating. Refer to Eisner ( 1997 ) and Graham and Hell ( 1985 ) for surveys on minimum spanning tree construction. In the experiments, we show the performance evaluation using different spanning trees. 7 EXPERIMENTAL RESULTS In this section, we present extensive experimental results on the performance of the TEAM algorithm. TEAM is implemented in C++. We first evaluate the efficiency of TEAM. Then, we present the findings of epistasis detection in simulated human genome-wide study. 7.1 Efficiency evaluation We use both simulated human and real mouse for the efficiency evaluation experiments. The experiments are performed on a 2.6 GHz PC with 8G memory running Linux system. 7.1.1 Human data The human datasets are generated by the simulator Hapsample (Wright  et al. ,  2007 ), which is publicly accessible from the web site  http://www.hapsample.org . We evaluate the performance of TEAM by comparing it with the brute force approach since there is no previous algorithm readily applicable to human datasets. Note that the brute force approach is very time consuming, we use a moderate number of SNPs and permutations in the experiments so that the brute force approach can finish in a reasonable amount of time. Unless otherwise specified, the default experimental setting is the following: #individuals = 400, #SNPs = 10 000, #permutations = 100, and the case/control ratio is 1. These experimental settings are chosen to demonstrate the efficiency gain offered by TEAM over the brute force implementation. TEAM can handle much larger datasets. The performance of TEAM is expected to follow the same trends presented in this section. TEAM contains three major components: building the minimum spanning tree, updating the contingency tables, and calculating the actual test values. Note that TEAM can be applied to any statistics defined on the contingency table. With different statistics, the only difference in runtime would be caused by the last component calculating the statistics. In the experiments, we choose chi-square test as our statistic.  Figure 2  shows the running time comparison of TEAM and the brute force approach using different experimental settings. The  y -axis is in logarithm scale. In these figures, we also show the detailed runtime of these three components.
 Fig. 2. Comparison between TEAM and the brute force approach on human datasets under various experimental settings: varying the number of SNPs ( a ), individuals ( b ), permutations ( c ) and varying the case/control ratio ( d ). Table 8  shows the percentage of individuals pruned by TEAM under different experimental settings. Since in theory we can update the contingency tables in any exploration order, in the table, we also show the pruning effect of using a random spanning tree and a linear spanning tree to guide the updating process. The random spanning tree is generated by starting from a randomly picked SNP and growing edges that connect the remaining SNPs in a random order. The linear tree is a single path connecting all SNPs sequentially. From the table, we can see that TEAM prunes more effectively than the other two updating methods. In the table, we also show the ratio of the tree weights and the size of the SNP dataset, i.e.  W 𝒯 /( M  ×  N ), which is a determining factor of the pruning ratio. Note that varying the number of permutations and the case/control ratio does not effect the tree being built.
 Table 8. The tree weight and the proportion of the individuals pruned by TEAM on the human datasets Settings TEAM Updating by Random Tree Updating by Linear Tree Tree weight (%) Pruning ratio (%) Tree weight (%) Pruning ratio (%) Tree weight (%) Pruning ratio (%) No. of SNPs 10 K 17.721 94.104 53.326 88.722 53.158 89.210 20 K 18.692 93.981 52.881 88.895 52.851 89.390 30 K 19.314 93.802 53.011 88.823 52.946 89.380 No. of Individuals 200 16.641 94.376 53.358 88.749 53.179 89.205 300 17.342 94.209 53.343 88.730 53.142 89.213 400 17.721 94.104 53.326 88.722 53.158 89.210 No. of Permutations 100 17.721 94.104 53.326 88.722 53.158 89.210 300 17.721 94.105 53.326 88.724 53.158 89.212 500 17.721 94.104 53.326 88.724 53.158 89.212 Case/control ratio 100/300 17.721 97.049 53.326 94.355 53.158 94.599 200/200 17.721 94.104 53.326 88.722 53.158 89.210 300/100 17.721 97.049 53.326 94.355 53.158 94.599 
 Figures 2 a depicts the runtime comparison when varying the number of SNPs. TEAM is more than an order of magnitude faster than the brute force approach. Among the three components of TEAM, the procedures on building the minimum spanning tree and calculating test values only take a small portion of the total runtime of TEAM. The runtime of TEAM is dominated by the cost of updating the contingency tables. As will be shown later, TEAM prunes most of the individuals when updating the contingency tables. In  Figures 2 b–d, we can also observe a similar one to two orders of magnitude speed up of TEAM over the brute force approach when varying the number of individuals, the number of permutations and the case/control ratio. 7.1.2 Mouse data The mouse datasets is extracted from a set of combined SNPs from the 10 K GNF ( http://www.gnf.org/ ) mouse dataset and the 140 K Broad/MIT mouse dataset (Wade and Daly,  2005 ). This merged dataset has 1 56 525 SNPs for 71 mouse strains. The missing values in the dataset are imputed using NPUTE (Roberts  et al. ,  2007 ). We compare TEAM and the recently proposed COE algorithm (Zhang  et al. ,  2009a ), which is specifically designed for association study in mouse datasets. The default experimental setting is as follows: #individuals = 70, #SNPs = 10 000, #permutations = 100, and the case/control ratio is 1. Figure 3  shows the comparison results. In the figure, we also plot the runtime of the brute force approach.  Figure 3 a shows the runtime of the three approaches when varying the number of SNPs. It is clear that both TEAM and COE are orders of magnitude faster than the brute force approach. TEAM is about twice faster than COE.  Figure 3 b shows the runtime comparison when varying the number of individuals. From the figure, COE is more suitable for datasets having small number of individual. As the number of individuals increases, the TEAM algorithm becomes more efficient than COE. Note that in human study, the number of individuals usually ranges up to thousands, much larger than that in typical mouse datasets.
 Fig. 3. Comparison between TEAM, COE and the brute force approach on mouse datasets under various experimental settings: ( a ) varying the number of SNPs and ( b ) varying the number of individuals. 7.2 Epistasis detection in simulated human GWAS In this section, we report the results of epistasis detection using simulated human GWAS data generated by Hapsample. In total, we generate four datasets, each of which has 112 036 SNPs for 250 cases and 250 controls. In each dataset, a disease causal interacting SNP pair is embedded. The embedded SNP pairs are: (rs768529, rs3804940) in dataset 1, (rs10495728, rs521882) in dataset 2, (rs1016836, rs2783130) in dataset 3 and (rs648519, rs1012273) in dataset 4. We use standard chi-square test with 500 permutations. Similar results can be found by using likelihood ratio test. With an overall FDR threshold of 0.005,  Table 9  shows the identified significant SNP pairs using TEAM. TEAM successfully identified the embedded SNP pairs in all simulated datasets. The embedded SNP pairs are labelled with stars ‘*’. The table shows the SNP loci on the genome. For example, in dataset 1, we embed SNP pair rs768529 and rs3804940, which are located on chromosome 1 at position 51 946 762 bp and chromosome 3 at 7 520 545 bp, respectively. The FWER for each reported SNP pair is also shown. Note that, for a SNP pair, an FDR (or FWER) value of 0 indicates that permutation tests do not generate any test value larger than value of the reported SNP pair. In dataset 1, except for the embedded SNP pair (rs768529, rs3804940), five other SNP pairs are also reported. One of the embedded SNP, rs768529, is involved in all the five pairs. A closer look at the other SNPs in the reported SNP pairs shows that they are all adjacent to the embedded SNP rs3804940. The normalized linkage disequilibrium (Lewontin and Kojima,  1960 ) between rs3804940 and the other five SNPs are  D ′(rs3804940, rs756084)=1,  D ′(rs3804940, rs779742)= 0.477,  D ′(rs3804940, rs1872393)= 0.442,  D ′(rs3804940, rs779744)= 0.442 and  D ′(rs3804940, rs6764561)= 0.454, indicating there is strong linkage disequilibrium between them.
 Table 9. Identified significant SNP pairs in the simulated human GWAS datasets Dataset Significant SNP-pair Chromosome and location FDR FWER 1 (rs768529, rs3804940) * (chr1: 51946762, chr3: 7520545) 0.00067 0 (rs768529, rs756084) (chr1: 51946762, chr3: 7536149) 0.00067 0 (rs768529, rs779742) (chr1: 51946762, chr3: 7558058) 0.00067 0 (rs768529, rs1872393) (chr1: 51946762, chr3: 7546236) 0.00067 0.004 (rs768529, rs779744) (chr1: 51946762, chr3: 7555121) 0.00067 0.004 (rs768529, rs6764561) (chr1: 51946762, chr3: 7514592) 0.00067 0.004 2 (rs10495728, rs521882) * (chr2: 22811773, chr8: 16688797) 0.004 0.004 3 (rs1016836, rs2783130) * (chr10: 31935845, chr13: 79068161) 0 0 4 (rs648519, rs1012273) * (chr11: 98972936, chr16: 58525067) 0.002 0.002 
 8 CONCLUSION AND FUTURE WORK The large number of SNPs genotyped in the genome-wide scale poses great computational challenges in two-locus epistasis detection. The permutation test used for proper error rate controlling makes the problem computationally even more intensive. In this article, we propose an efficient algorithm, TEAM, for epistasis detection human GWAS. TEAM has the same strength as the recently developed epistasis detection methods, i.e. it guarantees to find the optimal solution. Compared with existing methods, TEAM is more efficient in large sample study, and offers broader applicability. Existing methods designed for homozygous SNPs cannot be used for human data where most SNPs are heterozygous. TEAM, on the other hand, can handle both homozygous and heterozygous SNPs. Since it exhaustively enumerate all SNP pairs, TEAM can be used to control the FWER and FDR, both of which are widely used in controlling error in GWAS; while previous methods only control the FWER. Existing methods need to exam the formulation of the statistic. TEAM is focused on efficiently updating contingency tables rather than any specific statistic. It can, therefore, be used for any statistical test based on contingency table regardless of its formulation. In this artcile, we focus on the disease phenotypes that can be represented as binary variables. Many association studies involve phenotypes measured as continuous variables. We will investigate how to apply the idea of the current algorithm to quantitative phenotypes in the future study. Funding : National Science Foundation (awards IIS0448392, IIS0812464, partially). Conflict of Interest : none declared. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Profile Comparer: a program for scoring and aligning profile hidden Markov models</Title>
    <Doi>10.1093/bioinformatics/btn504</Doi>
    <Authors>Madera Martin</Authors>
    <Abstract>Summary: Profile Comparer (PRC) is a stand-alone program for scoring and aligning profile hidden Markov models (HMMs) of protein families. PRC can read models produced by SAM and HMMER, two popular profile HMM packages, as well as PSI-BLAST checkpoint files. This application note provides a brief description of the profile–profile algorithm used by PRC.</Abstract>
    <Body>1 INTRODUCTION Profile Comparer (PRC) is a program for scoring and aligning a profile hidden Markov model (HMM) of a protein family against other profile HMMs. Profiles are tables that give a score for a particular amino acid to be found at a particular position in an alignment of a protein family. The best known profile method is probably PSI-BLAST (Altschul  et al. ,  1997 ). Profile HMMs are similar to profiles, but replace scores with probabilities, and introduce additional probabilities for insertions and deletions at each position in the profile (Durbin  et al. ,  1998 ; Eddy,  1998 ). All probabilities are placed within a single statistical framework, an HMM. In this note, we shall count profile HMMs among profile methods. It is now well established that profile–profile methods detect more distant homologies than profile–sequence methods, which in turn are more powerful than sequence–sequence methods (see e.g. Sadreyev and Grishin,  2008 ; Soding,  2005 ). Profile–profile methods also generate the most accurate alignments; in fact, profile–profile methods were first used in progressive multiple sequence alignment and only later for homology recognition. Out of profile–sequence methods, the SAM and HMMER profile HMM programs (Eddy,  1998 ; Hughey and Krogh,  1996 ) are believed to be the best ( Fig. 1 ). In addition to insertion and deletion probabilities that vary along the profile, the improvement over, e.g. PSI-BLAST comes from a number of other innovations, including use of the forward algorithm instead of Viterbi (Durbin  et al. ,  1998 ) and a better algorithm for estimating a profile from a given alignment.
 Fig. 1. A SCOP domain benchmark (Madera and Gough,  2002 ) of PRC, illustrating the improvement over standard methods. The SCOP seed sequences were filtered to &lt;25% sequence identity. PRC and SAM (Hughey and Krogh,  1996 ) used SUPERFAMILY profile HMMs (Gough  et al. ,  2001 ). PSI-BLAST (Altschul  et al. ,  1997 ) checkpoint files used in the benchmark were derived from SUPERFAMILY profile HMMs and use identical probabilities for the profile part. For a comparison of PRC to competing profile–profile methods, the reader is referred to Soding ( 2006 ) and Sadreyev and Grishin ( 2008 ). The goal of PRC is to apply lessons learned from development of SAM and HMMER to the profile–profile case. PRC was first publicly released in 2002 and has been used by Pfam since 2005. Recently PRC has performed well in benchmarks (Sadreyev and Grishin,  2008 ; Soding,  2006 ) carried out by the authors of the two main alternative profile–profile methods, COMPASS (Sadreyev and Grishin,  2008 ) and HHsearch (Soding,  2005 ). Here, we provide an overview of the PRC algorithm (version 1.5.5) and explain how to use the program. 2 THE PRC ALGORITHM When scoring a profile HMM against a library of profile HMMs, PRC reports  E -values, which give an estimate of how significant the matches are. In order to calculate  E -values, PRC first calculates three other scores: co-emission, simple and reverse. Each score builds upon the previous one, until finally reverse scores are converted into  E -values. The co-emission score  S co - em  is a generalization of the log-odds score  S log - odds  calculated by SAM and HMMER,
 (1) 
to the HMM–HMM case:
 (2) 
The sum is over all possible amino acid sequences σ, and the probability  P (σ|HMM) that the profile HMM emits a sequence σ is calculated using the forward algorithm (Durbin  et al. ,  1998 ). When one of the HMMs is extremely ‘narrow’, e.g. it only emits a single sequence τ with a non-zero probability ( P (σ|HMM) = 1 if σ = τ, 0 otherwise), the co-emission score tends to the profile HMM log-odds score for τ. The null model emits random sequences with background amino acid frequencies and a geometric distribution of lengths. The simple score  S simple  is the same as the co-emission score  S co - em , but both profile HMMs are restricted to regions of significant similarity. The regions are found by an iterative procedure that picks a new end point as the maximum of the forward score in the dynamic programming matrix, and a start point as the maximum of the backward score. The reverse score  S rev  for two profile HMMs 1 and 2 is defined as
 (3) 
where the reverse HMM is defined as follows:
 (4) 
Here,  rev  is a reverse operator that maps residue or model segment  i  (1 ≤ i ≤ L ) onto residue  L − i +1. This is a generalization of the reverse sequence null model used by SAM (Karplus  et al. ,  2005 ). Finally, for library runs the reverse score  S rev  is turned into an  E -value by fitting the following function to the observed distribution of reverse scores:
 (5) 
The  E -value  E  is the expected number of random matches with a reverse score better than  x , and  n unrel  is the number of profile HMMs in the library that are unrelated to the query. The formula is a slight generalization of the function used by SAM (Karplus  et al. ,  2005 ). Optimal values of the two parameters λ and κ for each run are found using a censored Maximum Likelihood fitting procedure. HMM–HMM alignments are computed by finding the Viterbi path that maximizes the sum of forward–backward odds scores (Durbin  et al. ,  1998 ). 3 USING PRC PRC can read SAM3 (ASCII and binary) and HMMER2 model files, and PSI-BLAST checkpoint files. The same internal profile HMM is used for scoring all three. For PSI-BLAST checkpoint files, the profile part is taken from the checkpoint file and the insertion and deletion probabilities are set to default values, constant throughout the model. For best performance, users should build a full profile HMM using the SAM w0.5 script. For accurate  E -values, the library should contain at least 1000 profile HMMs. For libraries of sufficient size,  E  &lt;0.003 can be taken as indicative of homology and  E  &lt; 10 −5  as a strong match. When a large library is not available, Equation ( 5 ) with λ = 0.8, κ = 0 can be used as a conservative guide. Starting with version 1.5.5, the PRC source code also includes a simple Perl script, merge_aligns.pl. Given two HMM–sequence alignments in the SAM a2m format, and a PRC alignment between the two HMMs, the script will output a pairwise alignment between the two sequences. Users who would like to visualize their HMM–HMM alignments are referred to the pairwise HMM logos server (Schuster-Bockler and Bateman,  2005 ). Funding M.M.'s Internal Graduate Studentship from Trinity College, Cambridge, UK; the UK Medical Research Council and the Laboratory of Molecular Biology, Cambridge, UK (Cyrus Chothia's group); the European Bioinformatics Institute (Nick Goldman's group); Kevin Karplus's  National Institutes of Health  grant  R01 GM068570 ; Julian Gough's European Union Framework Programme 7 IMPACT grant. Conflict of Interest : none declared. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>BAMixChecker: an automated checkup tool for matched sample pairs in NGS cohort</Title>
    <Doi>10.1093/bioinformatics/btz479</Doi>
    <Authors>Chun Hein, Kim Sangwoo, Birol Inanc</Authors>
    <Abstract/>
    <Body>1 Introduction Increasing use of Next Generation Sequencing (NGS) in clinical practice requires a large number of samples to be processed in a limited time. While improvements in algorithms have provided more accurate means of detecting genomic variants, human errors in sample handling remain a constant concern. Sample mismatch, in particular, is a frequent occurrence detrimental to sequencing analyses ( Westra  et al. , 2011 ). In the last few years, several tools have been developed to detect mismatching of samples in NGS datasets. Conpair ( Bergmann  et al. , 2016 ) detects a mismatched pair of BAM files based on 7387 known polymorphic loci. BAM-matcher ( Wang  et al. , 2016 ) uses a similar approach, but allows for faster testing as it only uses 1500 exotic single nucleotide polymorphism (SNP) sites. The recently developed NGSCheckMate ( Lee  et al. , 2017 ) accepts FASTQ, BAM or VCF files as input and provides a list or a tree graph of genotype correlations among the samples. In general, the reported accuracies of these tools are all over 95%. Despite the good accuracy of these tools, we have found areas for improvement in two major features that would allow for more active use in cohort-level checkup. First, the number and the composition of SNP sites for individual matches need to be optimized. These SNP sites should be applicable to various targeted sequencing panels in order to cope with large-scale clinical genomic tests. Second, the tool should be fast and automated to minimize intervention from users, even with a large number of samples. Accordingly, we developed BAMixChecker, which facilitates fast and accurate assessment of mismatches in sample-pair assignment from combinations of WGS/WES/RNA-Seq and targeted sequencing panels in NGS cohort. BAMixChecker uses 853 highly informative human polymorphic sites that are optimized for WGS/WES and RNA-Seq data. For targeted sequencing data, BAMixChecker instantly constructs an optimal SNP list specific to the targeted genomic regions; the use of smaller SNP set enabled a reduced running time while maintaining accuracy, even in a small panel. Although the tool was mainly developed for the analysis of human data BAMixChecker provides specific functions for the identification of sets of highly informative polymorphic positions which allow the application of the tool also to non-human species. BAMixChecker categorizes orphan and swapped samples using rules based on genetic distances and file names edit distances. The pipeline is fully automated, allowing users to quickly check abnormal events without the need for further intervention to interpret the result. 2 Materials and methods BAMixChecker only takes pairs of BAM/CRAM files as inputs with optional genomic region information (BED file) for targeted sequencing and reports mismatched samples and their types ( Fig. 1A ). The overall workflow consists of the four major steps described below. Detailed procedures are described in  Supplementary Data .
 Fig. 1. 
 (A)  Overall workflow of BAMixChecker.  (B)  Score distribution of BAMixChecker in five datasets. Each dot reflects a comparison result between two samples. Red dots indicate unmatched pairs; blue dots are matched pairs.  (C)  Accuracies of the four tools in five cohorts. NGSCheckMate contains two different modes (BAM and FASTQ input). WES/RNA-Seq represents a WES-RNA-Seq pair.  (D)  Accuracy of the four tools in downsampled cohorts.  (E)  Running times of the four tools. The running times of BAMixChecker and NGSCheckMate were measured in two different modes (p1: single-thread, p4: multi-thread with four processors).*: default 1.  SNP site selection : To select only highly informative SNP loci and to reduce ambiguous calls, we considered two criteria: (i) mappability and (ii) population allele-frequency. From gnomAD v2.0.2 ( Lek  et al. , 2016 ), we collected 57 582 candidate exonic SNPs that passed filters, including variant quality, mapping quality and genomic mappability depending on position like not in a low complex region, segment duplicated region and sample repeat region. Out of the 57 582 candidates, 853 SNPs with a global minor allele frequency (gMAF) between 0.45 and 0.55, and also population-specific MAF between 0.35 and 0.65 for eight populations ( Supplementary Methods ) were selected to build a fixed list for WGS/WES and RNA-seq data. For targeted sequencing, BAMixChecker automatically adjusts MAF condition of SNPs from higher global MAF and MAF in each population by downing the values, ranging 0.45–0.1, until at least 200 SNPs overlap given targeted genomic region ( Supplementary Methods ). 2.  Genotype-based pairing : For selected SNP sites, BAMixChecker calls genotypes of samples using GATK HaplotypeCaller with further filtering ( Supplementary Methods ). Genotype concordance scores are then calculated between all pairs in the cohort. Sample pairs with a concordance score of &gt;0.7 are considered matched. The use of the fixed cut-off value is supported by a large margin in the observed concordance scores between matched and unmatched samples from large-scale databases ( Fig. 1B ). Although a perfect concordance (1.0) is expected between matched samples in general, we assumed that many confounding factors including contamination, copy number variation, allele-specific expression and poor sample quality allowed the lenient cut-off. Unpaired samples in this step are considered  orphans . 3.  Name-based paring : Assuming that file names are rule-based within a cohort, sample relationships can be inferred from the names, just as a human would do. BAMixChecker emulates this using  entropy-based file matching  ( Supplementary Methods  and  Supplementary Fig. S1 ). Briefly, the uncertainty of values in the same position of a delimited file name is measured. Positions with high uncertainty tend to represent sample- or individual-specific information (e.g. sample id), while low uncertainty reflects global information (e.g. cohort id). File-name similarity is calculated by adding or subtracting positional entropy for each matched or mismatched value: file names of matched samples only differ in low entropy positions (e.g. T versus N) and gain a high score in high entropy positions (e.g. sample id), thereby being considered as the best match in the cohort. We have confirmed that this approach perfectly identifies true matches for 463 sample pairs in four different cohorts ( Supplementary Table S2 ). The file-name based matching algorithm searches matched paired sample with the best similarity score. Otherwise, a user can directly offer matched samples information with a list if the matched samples are not a pair ( Supplementary Methods ). 
 4.   Decision and report : After genotype-based and file-name-based pairing, BAMixChecker categorizes all samples into three classes:  matched  (match for genotype and file-name pairing),  swapped  (genotype match that is not file-name matched, or vice versa) and  orphan  (no genotype match found). BAMixChecker outputs the final judgment in an HTML file, with an additional Heatmap that describes the overall sample concordance ( Supplementary Fig. S2 ). 3 Results We evaluated the accuracy of BAMixChecker in comparison to previously reported tools (NGSCheckMate, BAM-matcher and Conpair) in five real NGS cohorts with tumor-normal pairs: (1) TCGA WES pair cohort ( n  =   202), (2) TCGA RNA-Seq pair cohort ( n  =   130), (3) TCGA WES/RNA-Seq pair cohort ( n  =   168), (4) Korean Cancer Study Group (KCSG) panel sequencing pair cohort ( n  =   192) ( Lim  et al. , 2019 ) and (5) Korean Lung Cancer Consortium (KLCC) panel sequencing pair cohort ( n  =   402) ( Supplementary Table S1 ). For TCGA WES and RNA-Seq, all tools exhibited good accuracy, except for a few miscalls by BAM-matcher and Conpair ( Fig. 1C  and  Supplementary Table S3 ). However, there was a noticeable drop in accuracy for the targeted sequencing cohorts (KCSG and KLCC in  Fig. 1C ) with Conpair. For all cohorts, only BAMixChecker showed perfect accuracy. For evaluation of smaller panels, TCGA WES data were downsampled to gene lists of four popular commercial panels: Ion AmpliSeq Comprehensive Cancer Panel (Ion-CCP, 409 genes), Foundation One (FONE, 315 genes), xGen Pan-Cancer Panel (xGen-PCP, 127 genes) and Comprehensive Common Cancer Panel (CCCP, 46 genes). We found BAMixChecker showed almost perfect accuracy in all panels (&gt;99.8%), while the other tools showed lower accuracy in smaller panels ( Fig. 1D ). BAMixChecker showed robust performance even with a family dataset ( Supplementary Methods  and  Supplementary Fig. S3 ). For additional validation, we generated artificial mismatches by intentionally changing file names ( Supplementary Methods ). For each NGS cohort, 10% of files were randomly selected and simulated to be swapped (by switching file names) or orphan (by assigning a file name to a wrong sample), which were repeated 100 times with different randomization. Testing by BAMixChecker confirmed that all mismatches were perfectly reported (100% accuracy), regardless of the mismatch type or used NGS cohort ( Supplementary Methods  and  Supplementary Fig. S4 ). Finally, running times were assessed for all tools ( Fig. 1E  and  Supplementary Methods ). BAMixChecker and NGSCheckMate can be run in a multiprocessing mode and were tested with two different CPU numbers (single and 4-CPUs). BAMixChecker exhibited comparable or faster speed than BAM-matcher and Conpair, and was remarkably faster (∼18×) than NGSCheckMate. Considering the reduced need for intervention from users, we expect that the practical hands-on time would be much shorter with BAMixChecker. Supplementary Material btz479_Supplementary_Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Tabhu: tools for antibody humanization</Title>
    <Doi>10.1093/bioinformatics/btu667</Doi>
    <Authors>Olimpieri Pier Paolo, Marcatili Paolo, Tramontano Anna</Authors>
    <Abstract>Summary: Antibodies are rapidly becoming essential tools in the clinical practice, given their ability to recognize their cognate antigens with high specificity and affinity, and a high yield at reasonable costs in model animals. Unfortunately, when administered to human patients, xenogeneic antibodies can elicit unwanted and dangerous immunogenic responses. Antibody humanization methods are designed to produce molecules with a better safety profile still maintaining their ability to bind the antigen. This can be accomplished by grafting the non-human regions determining the antigen specificity into a suitable human template. Unfortunately, this procedure may results in a partial or complete loss of affinity of the grafted molecule that can be restored by back-mutating some of the residues of human origin to the corresponding murine ones. This trial-and-error procedure is hard and involves expensive and time-consuming experiments. Here we present tools for antibody humanization (Tabhu) a web server for antibody humanization. Tabhu includes tools for human template selection, grafting, back-mutation evaluation, antibody modelling and structural analysis, helping the user in all the critical steps of the humanization experiment protocol.</Abstract>
    <Body>1 INTRODUCTION Monoclonal antibodies (mAbs) are an important class of therapeutic molecules. The high specificity and affinity towards their respective antigens, their modular structure that facilitates their engineering and the relative low costs for their production in model animals makes them excellent drug candidates against several diseases ( Chames  et al. , 2009 ;  Reichert, 2012 ). However, together with all these desirable characteristics, xenogeneic mAbs have drawbacks that limit their therapeutic benefits and can ultimately endanger the patients’ health ( Hansel  et al. , 2010 ;  Hwang and Foote, 2005 ). To overcome these hurdles, different methods have been developed for increasing the mAbs ‘degree of humanness’ ( Abhinandan and Martin, 2007 ) by replacing parts of the original non-human antibody with the corresponding human counterparts. This process is generally referred as ‘humanization’ and takes advantage of the particular architecture of the antibody molecule ( Almagro and Fransson, 2008 ;  Padlan, 1994 ). The molecules generated by such humanization procedures may partially or completely lose affinity for their intended antigen; this can be usually restored by re-introducing specific and case-dependent native residues in the humanized molecule through an experimental trial-and-error procedure going under the name of ‘back-mutation’ phase. Taking advantage of our experience in antibody sequence and structure analysis ( Chailyan  et al. , 2011 ;  Ghiotto  et al. , 2011 ;  Marcatili  et al. , 2013 ), we developed Tools for AntiBody Humanization (Tabhu), a comprehensive platform meant to help antibody humanization experiments. Tabhu integrates different methods to guide researchers through several steps of the humanization cycle, from the selection of a suitable human acceptor molecule to the evaluation of the back-mutations effect. 2 DESCRIPTION The initial input page of Tabhu requires the sequence of the light and heavy chain variable domains (VL and VH, respectively;  Padlan, 1994 ) of the xenogeneic antibody to be humanized (native Ab) and the antigen volume since the latter can be used to improve the prediction of the residues involved in antigen recognition ( Olimpieri  et al. , 2013 ). Tabhu uses two alternative sources of human sequences to choose the framework donor with the highest sequence similarity to the xenogeneic V region: a database consisting of both light and heavy chain sequences retrieved from the Digit database ( Chailyan  et al. , 2012 ) or human germline gene sequences compiled by IMGT ( Giudicelli  et al. , 2005 ) from which the user can select the Variable and Joining genes, that are eventually assembled together with the mouse complementarity determining regions (CDRs) to form the initial acceptor molecule. Tabhu lists the possible templates and shows relevant information for each of them. Once a receiving framework has been selected, the server starts an antibody humanization procedure that resembles what is usually done experimentally and involves four steps: (i) loop grafting, (ii) estimate of the binding mode similarity between the native and human antibody, (iii) back-mutations and (iv) re-evaluation of the binding mode similarity between input and humanized antibody ( Supplementary Material ,  Supplementary Figure S1 ). The first step consists of grafting the xenogeneic CDRs into the human framework. The evaluation of the expected similarity of the binding mode is based on the proABC method that we have previously developed ( Olimpieri  et al. , 2013 ), that predicts the probability that every single antibody residue is involved in antigen recognition taking into account the entire sequence of the variable domains. If the pattern of interaction is very different between the input and humanized sequence, it can be expected that the resulting binding mode, and most likely the affinity, will be different. More details on the formula used to evaluate individual back-mutation importance are reported as  Supplementary Material . Once the user selects which residues to back-mutate and submits them to the system, a new variant is generated and the process can be repeated. However, the introduction of mutations in the human antibody can lead to structural problems, such as the appearance of clashes or cavities in the modelled humanized antibody. Taking advantage of our antibody structure prediction tools ( Chailyan  et al. , 2011 ;  Marcatili  et al. , 2008 ), upon user request, Tabhu builds the three-dimensional models of the mouse and humanized antibodies, runs the procheck and EDTSurf tools ( Laskowski  et al. , 1996 ;  Xu and Zhang, 2009 ) and alerts the user if the introduction of a back-mutation generates clashes or cavities, that the user can ignore or use as a guide to remove or introduce additional back-mutations. When the desired binding mode similarity between the xenogeneic and humanized antibody has been achieved the user can finalize the model and retrieve the three-dimensional model of the parental antibody, the amino acid sequence of the selected human template, the contact probabilities of the humanized antibody, the amino acid sequence of the final redesigned antibody and a back-translated nucleotide sequence optimized for being expressed in a number of organisms.  Supplementary Material ,  Supplementary File S1  reports an example of antibody humanization with Tabhu. 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>BioBlend.objects: metacomputing with Galaxy</Title>
    <Doi>10.1093/bioinformatics/btu386</Doi>
    <Authors>Leo Simone, Pireddu Luca, Cuccuru Gianmauro, Lianas Luca, Soranzo Nicola, Afgan Enis, Zanetti Gianluigi</Authors>
    <Abstract>Summary: BioBlend.objects is a new component of the BioBlend package, adding an object-oriented interface for the Galaxy REST-based application programming interface. It improves support for metacomputing on Galaxy entities by providing higher-level functionality and allowing users to more easily create programs to explore, query and create Galaxy datasets and workflows.</Abstract>
    <Body>1 INTRODUCTION In recent times, the massive increase in the amount of data produced by genomic sequencers and other data-intensive acquisition devices used in the life sciences has led to a continuous intensification of the effort required for biological data analysis. Huge and numerous datasets must be processed by complex analysis workflows, articulated in a large number of steps, most of which are highly dependent on many configuration parameters. Data processing frameworks can help mitigate the complexity by simplifying the pipeline execution. An example of such a framework is Galaxy ( Goecks  et al. , 2010 ), an extremely popular Web application for bioinformatics analysis. It provides a simple way to encapsulate computational tools and datasets in a graphical user interface (GUI), together with a mechanism to keep track of the execution history in a reproducible manner. However convenient and user-friendly, though, GUIs are ill-suited to automated analysis and bulk processing. For instance, consider a situation that happens regularly with each release of a new reference genome for resequencing, or with the update of sequence alignment software: to ensure that analysis results stay relevant, such events require that the full set of experimental results (e.g. single nucleotide polymorphism discovery) be reevaluated from scratch using the new model data or software. This laborious task requires better support from the computational framework being used, in the form of reliable ways to automate operations, process datasets in bulk and document the analysis performed on any of them. More generally, studies tend to handle a growing numbers of samples; they also tend to last longer than the relatively frequent update cycles for model data and software. Both these conditions pose requirements for such automated bulk data operations that are currently not handled well by GUIs. To facilitate this sort of processing, Galaxy includes a RESTful ( Richardson and Ruby, 2007 ) application programming interface (API) that allows other programs to control it automatically. However, this API is fairly low level, as it requires users to construct and issue HTTP requests, explicitly handle the standard error cases that occur in such distributed scenarios and take care of data serialization and deserialization in exchanges between the client and the server. This gap in functionality motivated the development of BioBlend ( Sloggett  et al. , 2013 ), a Python package that hides HTTP communication, error handling and JSON (de)serialization from the user, providing a dictionary-based API that greatly simplifies interaction with the Galaxy server. However, despite its significant enhancements over the raw low-level interface, BioBlend still leaves room for improvement. For instance, most of the BioBlend API still offers a one-to-one mapping of generic Python dictionaries to the Galaxy REST resources, with no explicit modeling of Galaxy entities and their relationships. Also, the interface fails to isolate client code from changes in the Galaxy API, as it passes to the caller the same dictionary structures that the server sends. Finally, BioBlend does not provide much in the way of ‘rich’ functionality to perform higher-level, sophisticated yet generic tasks, despite being positioned in a prime location in the software stack where it is potentially shared by all the user’s client applications. In this work we present BioBlend.objects, a Galaxy interface implemented as a new layer above BioBlend. The new API addresses the aforementioned issues with two main features: an object-oriented (OO) programming model, which simplifies development and isolates client code from changes in the Galaxy API and a high-level component that simplifies complex operations and supports  metacomputing  on the information describing the various Galaxy entities. With BioBlend.objects, running a Galaxy workflow requires just a few lines of simple code:
 from bioblend.galaxy.objects import GalaxyInstance gi = GalaxyInstance ( “ URL", 
 “ API_KEY ” ) wf 
 = 
 gi.workflows.list()[0] hist 
 = 
 gi.histories.list()[0] inputs 
 = 
 hist.get_datasets()[:2] input_map 
 = 
 dict(zip(wf.input_labels, 
 inputs)) params 
 = 
 {"Paste1": 
 {"delimiter": 
 “ U"}} wf.run(input_map, 
 “ wf_output", 
 params=params) 
 The new API is described in more detail in Section 2. 2 METHODS BioBlend.objects has been developed as a submodule of the original BioBlend library. Hierarchically, the code is currently located at the same level as BioBlend’s Galaxy submodules ( Fig. 1 ); in the future, the new API will be moved up to replace the current one. The library consists of two main components: the  wrappers  module, which defines the object structure that mirrors Galaxy’s entities, and the  client  module, a high-level code layer built upon the original API to expose a simpler, more concise interface based on the object hierarchy defined in  wrappers . The  client  module consists of three main classes that encapsulate interactions with Galaxy’s most important entities: histories, workflows and libraries. The  galaxy_instance  module contains the  GalaxyInstance  class, which unifies the three clients, acting as a common entry point for all interactions with the Galaxy server.
 Fig. 1. BioBlend.objects location within BioBlend's logical structure. New modules are displayed in white background OO  interface:  BioBlend.objects provides a compact OO interface for controlling operations performed with Galaxy. The new interface provides objects for the entities that are handled within Galaxy, explicitly modeling the underlying logical structure, and thus is arguably more intuitive than the older one. The OO interface also facilitates development by enabling programmer-friendly features such as code completion in modern development tools and in the IPython shell ( Perez and Granger, 2007 ). Moreover, the new API defines specific objects as its method return values, thus effectively isolating client code from changes in the server-side Galaxy interface. This improvement should result in less painful upgrades of the Galaxy server since, at worst, client compatibility would require updating BioBlend.objects to the latest version. Metacomputing library:  The second principal contribution in BioBlend.objects consists of a set of high-level functions that simplify complex interactions with the Galaxy back end. These functions encapsulate sequences of common operations and implement functionality to support computing on the information describing the various Galaxy entities––i.e.  metacomputing . Supported features range from running workflows to downloading Galaxy histories and querying for datasets with particular characteristics. This library is a key component of the automation mechanisms used at CRS4 to run its sequencing pipeline and acquire the details of the operations applied to generate each dataset so that they may be stored into OMERO.biobank, a ‘computable biobank’ that extends OMERO ( Allan  et al. , 2012 ) to handle data types produced in sequencing and microarray experiments ( http://www.openmicroscopy.org/site/support/partner/omero.biobank ). Consider the example given in the introduction, where a workflow is retrieved and run on a set of input datasets, setting a tool parameter at run time. With the original BioBlend API, the same task requires writing the following code:
 from bioblend.galaxy.objects import GalaxyInstance gi = GalaxyInstance ( " URL", 
 " API_KEY " ) summaries 
 = 
 gi.workflows.get_workflows() wf_id 
 = 
 summaries[0] [ " id " ] wf_info 
 = 
 gi.workflows.show_workflow(wf_id) hist_infos 
 = 
 gi.histories.get_histories() hist_id 
 = 
 hist_infos[0] [ " id " ] hist_dict 
 = 
 gi.histories.show_history(hist_id) content_info 
 = 
 gi.histories.show_history(hist_id,  contents=True) datasets 
 = 
 [gi.histories.show_dataset (hist_id, 
 _ [ " id " ] ) 
 for 
 _ 
 in 
 content_info] inputs 
 = 
 datasets[:2] input_slots 
 = 
 wf_info [ " inputs " ] .keys() input_map 
 = 
 {   input_slots[0]: 
 {"id": 
 inputs[0] [ " id " ] , 
 " src": 
 " hda"},   input_slots[1]: 
 {"id": 
 inputs[1] [ " id " ] , 
 " src": 
 " hda"} } params 
 = 
 {"Paste1": 
 {"delimiter": 
 “ U"}} gi.workflows.run_workflow(wf_id, 
 input_map,   history_name="wf_output", 
 params=params) 
 A comparison of the two versions shows how the higher-level interface allows for much more compact code that is easier to read and write. To keep the example as simple as possible, in the above code fragments we used a ‘toy’ workflow that merges columns from two input tabular files. However, the git repository (see ‘Availability and implementation’) includes three examples of interaction with real-world microbiology workflows ( Cuccuru  et al. , 2014 ) hosted by CRS4’s Orione platform ( http://orione.crs4.it ): bacterial resequencing, bacterial  de novo  assembly and metagenomics. The examples are available under  docs/examples/objects  and can be run on Orione after registering and obtaining an API key (details are included in the examples directory itself). 3 DISCUSSION BioBlend.objects is designed to model the relations between Galaxy entities. For instance, a History object can be used to retrieve its datasets through an instance method: this makes the API similar to an object-relational mapping library for Galaxy. The BioBlend.objects module has received the support of the original BioBlend team members, who are involved in its development. As such, it is expected to supplant, in the future, the original programming interface. Funding : CRS4 work was partially supported by a  Wellcome Trust Strategic Award  [ 095931/Z/11/Z ]. S.L. and L.P. have performed their activity within the context of the PhD program in  Biomedical Engineering  at the  University of Cagliari, Italy . Conflict of Interest : none declared. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>A neural network model for constructing endophenotypes of common complex diseases: an application to male young-onset hypertension microarray data</Title>
    <Doi>10.1093/bioinformatics/btp106</Doi>
    <Authors>Lynn Ke-Shiuan, Li Li-Lan, Lin Yen-Ju, Wang Chiuen-Huei, Sheng Shu-Hui, Lin Ju-Hwa, Liao Wayne, Hsu Wen-Lian, Pan Wen-Harn</Authors>
    <Abstract>Motivation: Identification of disease-related genes using high-throughput microarray data is more difficult for complex diseases as compared with monogenic ones. We hypothesized that an endophenotype derived from transcriptional data is associated with a set of genes corresponding to a pathway cluster. We assumed that a complex disease is associated with multiple endophenotypes and can be induced by their up/downregulated gene expression patterns. Thus, a neural network model was adopted to simulate the gene–endophenotype–disease relationship in which endophenotypes were represented by hidden nodes.</Abstract>
    <Body>1 INTRODUCTION To effectively map disease genes of complex diseases is one of the ultimate goals of genomic research. However, etiology of complex diseases involves multiple pathways and their dynamic interactions with environmental factors (Sing  et al. ,  2003 ; Zerba  et al. ,  1996 ). Gottesman and Gould ( 2003 ) advocated reducing genetic heterogeneity and introduced the concept of endophenotypes with characteristics that are closer to disease genes than the disease itself. Pan  et al.  ( 2006 ) suggested taking advantage of the vast amount of transcriptomic and proteomic data from patients and, via data mining, generating endophenotypes that conceptually resemble ‘pathway clusters’ which can then be utilized as binary phenotypes or quantitative trait loci (QTLs) for gene mapping. It has been demonstrated repeatedly that microarray data from gene expression can be reduced to the gene signature of specific cancer types and used for diagnosis/prognosis (Macgregor,  2003 ; Perez-Diez  et al. ,  2007 ; Quackenbush,  2006 ). Little effort has been made to apply genome-wide expression data to reveal a complex disease's etiology or to facilitate gene mapping. Data-mining technology has been widely used in the analysis of microarray data (Quackenbush,  2002 ; Valafar,  2002 ; Verducci  et al. ,  2006 ). To date, most studies have concentrated on identifying differentially expressed genes between case and control subjects, clustering the identified genes using certain correlation measures and then linking the gene clusters to known functional pathways. However, this scheme, which relies heavily on correlation measures between genes, is more suitable for mapping rare diseases than common complex diseases, which often involve multiple common variants (Collins  et al. ,  1997 ) and complex mechanisms (Gu  et al. ,  2002 ). In this article, we assume that multiple endophenotypes (pathway clusters) for a complex disease exist and that such a disease can be induced by various up/downregulated patterns of these endophenotypes. To simulate the gene–endophenotype–disease relationship that fulfills the above assumptions, we propose using a one-hidden-layer, feed-forward neural network model with endophenotypes represented by hidden nodes. We describe below the procedure to construct the neural network model used to obtain the gene–endophenotype–disease relationship. We used microarray data obtained from a case–control study in Taiwan to construct a gene-endophenotype-disease model for male young-onset hypertension and tested the effectiveness and consistency of the constructed model and endophenotypes. 2 METHODS 2.1 A neural network-based gene-endophenotype-disease model Based on current research, we made the following two assumptions regarding the etiology of young-onset hypertension.
 There exist multiple endophenotypes, each of which represents a pathway or a cluster of pathways (Pan  et al. ,  2006 ), comprised of a set of genes. In addition, each endophenotype can be in either an up- or a downregulated mode predicted from the expression profiles of the associated genes. The development of hypertension requires specific up/downregulated patterns of endophenotypes, and there exist distinct up/downregulated patterns of endophenotypes for hypertensive cases and normotensive controls. 
We constructed a hypothetical gene-endophenotype-disease model that involves  n  genes and  m  endophenotypes. The model can be represented by a one-hidden-layer neural network with  n  input nodes and  m  hidden nodes as follows: (i) the  n  input nodes correspond to  n  potential genes for hypertension; (ii) the  m  hidden nodes correspond to  m  endophenotypes; (iii) the connection weight,  w i , j 1 , between input node  i  and hidden node  j  represents the influence of gene  i  on endophenotype  j  (1 ≤ i ≤ n , 1 ≤ j ≤ m , a zero weight denotes that the gene is not connected to, or has virtually no effect on, the endophenotype); (iv) the connection weight,  w j 2 , between hidden node  j  and the output node indicates the influence of endophenotype  j  on the disease; (v) all of the hidden nodes and the output node use sigmoid functions to determine the status (active/inactive) of the nodes where θ 1L j  is a threshold value for hidden node  j  and θ 2  is a threshold value for the output node. Next, we developed a training algorithm to determine the network parameters, including the number of hidden nodes and all the connection weights. 2.2 Determination of model's parameters Conventional network training algorithms, which focus on minimizing the training error, require a data size that is usually much larger than modern microarray studies can provide. However, training with insufficient data can lead to poor generalization of the resultant network (Marinov and Weeks,  2001 ). To generalize solutions for our gene-endophenotype-disease model, we imposed the following five objectives for the network training: (i) maximize the classification accuracy ( E ); (ii) maximize the proportion ( P uniq ) of unique genes (genes that are not involved in multiple endophenotypes) in the endophenotypes; (iii) maximize the absolute correlations ( R int ) among gene expression levels in an endophenotype; (iv) maximize the correlation ( R BP ) between average model outputs and average blood pressure measurements of patient subgroups determined by the patterns of binarized hidden node output; and (v) maximize the proportion of the unused genes ( P unused ). For the first objective, the proportion of data that are correctly classified as hypertensive or as normotensive is used to represent the classification accuracy. The second objective is designed to involve a gene in as few endophenotypes as possible by setting some of the connection weights,  w i , j 1L , to zero (&lt;10 −3  in our model). The third objective aims to construct endophenotypes by assembling genes with similar (both highly correlated and highly anti-correlated) expression profiles across patients. The fourth objective tends to link the model outputs with blood pressure measurements, and the fifth objective is employed to remove redundant or irrelevant genes. Let the network parameters (a combination of  w i , j 1L , θ j 1L ,  w j 2L  and θ 2 ) be denoted by a vector  w , and  m  represents the number of hidden nodes in the network (and the number of endophenotypes in the proposed model). We propose to solve the following multiple objective (MO) problems to construct our neural network model:
 (1) 
subject to
 
 This is a mixed-integer, non-linear, MO problem. The task of simultaneously determining the optimal values of  w  and  m  is NP-hard. We propose to solve this MO problem with the following procedure:
 Step 1: Set  m = m init ; Step 2: Repeat steps 2.1–2.2 until  m = m final ; Step 2.1: Solve the unconstrained MO problem ( 1 ); Step 2.2: Record the computed objective values and the corresponding  w , and then set  m = m +1. Step 3: Set the minimal network complexity  m opt  to  m ′, where the average of the sum of the recorded objective values reaches the maximum. Step 4: Among those  w 's that satisfy ( 1 ) as  m = m opt , choose the one that meets the prespecified requirement for model construction. 
 We note that, in an MO problem, a point that simultaneously reaches the global optimal solutions of all the objective functions is usually non-exist. Instead, there are infinite so-called ‘global non-inferior solutions’ that are of interest in an MO problem (Coello,  1999 ; Fonseca and Fleming,  1995 ). We employed the Strength Pareto Evolutionary Algorithm (SPEA) (Zitzler and Thiele,  1999 ) in our study because of its effectiveness in finding multiple  near  global non-inferior solutions (see Appendix A in  Supplementary Material  for details). The SPEA statistical search method is less efficient in convergence speed compared with gradient-based search methods. To overcome such a drawback, we initially employed SPEA to reach near-global non-inferior solutions, chose the suitable ones with high  E ,  R int  and  R BP , and then used a greedy search method to obtain global non-inferior solutions. In this study, the greedy search was performed by solving a series of constrained, single-objective, optimization problems. That is, the five objectives were solved one at a time, leaving the other four constrained by their most updated values. We also adopted a 5-fold cross-validation technique to improve the generalization of computed network parameters and to test the consistency of the constructed endophenotypes. In each of the five validation iterations, we first trained the network by solving ( 1 ) using SPEA for 300 generations to obtain near-global non-inferior solutions. Then, the solution with the highest sum of  E ,  R int  and  R BP  was used as the starting point and a greedy search was employed to obtain a global non-inferior solution. During the training process, early stopping was used to avoid overtraining (Amari  et al. ,  1996 ). The final network parameters were selected from the resultant five solutions that had the best average performance on the five datasets. 2.3 Identification of significant hypertension genes and mechanisms To probe the finer structure and to identify key elements in each of the computed endophenotypes, we constructed a tree of gene clusters for each endophenotype based on correlations between gene profiles. First, the correlations between all pairs of genes in an endophenotype were computed. And then the average linkage method, Unweighted Pair Group Method with Arithmetic mean (UPGMA), was adopted for the tree construction. In each endophenotype, the hierarchical structure of gene clusters determined successively in their mergence order may reveal certain sequential relationships, e.g. from upstream to downstream. To relate a gene cluster to its corresponding endophenotype, we first computed the weighted sum of genes in a cluster for connection weights, and then correlated it with the endophenotype output. Clusters with a high correlation coefficient were considered influential to their corresponding endophenotype. Significance of a gene in an endophenotype was evaluated by two indices: (i) the connection weight between a gene and its corresponding endophenotype and (ii) the sum of the correlations between a gene and all other genes in the endophenotype. The first index characterizes the influence of a gene on its corresponding endophenotype and thus genes with a high connection weight may be somewhat downstream in the disease pathogenesis. On the other hand, the second index indicates the degree of association with other genes in the endophenotype, therefore, genes with a high correlation sum may play a significant role upstream in the pathway of disease pathogenesis. In selecting potential hypertension genes in each endophenotype, we targeted those genes that both had the top 15 index values for either one of the two indices and also were within influential gene clusters. 2.4 Data collection and preprocessing The study was approved by the Institution Review Board of Academia Sinica and all participants provided written informed consent. Subjects were recruited from the clients of MJ Life Enterprise Co. Ltd. (Taiwan), a healthcare facility, from September 8 to December 31, 2004. The inclusion criteria were: (i) 20–50 years of age; (ii) BMI &lt; 35; (iii) fasting for at least 8 h; (iv) fasting blood sugar &lt; 126 mg/dl; (v) not taking hypertension medication; (vi) no history of cancer or other major illnesses of the liver, kidneys, heart or lungs; and (vii) no acute hypertension-related symptoms in the previous 2 weeks. Blood pressure was measured three times for each participant according to the established protocol (Pan  et al. ,  2001 ) and the average of the last two was used for hypertension diagnosis. A participant was classified as a hypertension case if the systolic pressure was &gt;140 mmHg, or the diastolic pressure was &gt;90 mmHg; otherwise, the participant was classified as a normotensive control. Secondary hypertension patients were excluded. A total of 77 newly diagnosed non-medicated young-onset male hypertensive cases (age 37.6 ± 7.2) and 82 male normotensive controls (age 36.9 ± 6.6) were included in this study. Unlike in the field of cancer genetics, it is difficult to acquire affected tissue from hypertensive patients. Fortunately, using blood instead of affected tissue to obtain gene expression profiles has been proposed as a possible alternative in several proof-of-concept studies (Bull  et al. ,  2004 ; Chon  et al. ,  2004 ; Matsunaga  et al. ,  2002 ). In this study, fasting blood (10 ml) was obtained from each participant, then stabilized and frozen immediately at –7 ○ C. Then, total RNA was extracted from the whole blood. Finally, four replicates of microarray data were generated for each participant, using Human OneArray (Phalanx Biotech Group, Taiwan), a one-channel array. Each microarray chip contained 39 200 polynucleotide data, of which 22 184 were mapped to the latest draft of the human genome. Micoarray arrays data were subjected to quality control using parameters: percentage of present calls, coefficient of variations (CV) and Pearson correlation (R), and array quality filter (see Appendix B in  Supplementary Material  for details). For each subject, the qualified (392 out of 636) replicates were merged using a weighted (1/SD) average. A logarithm and  Z -score global normalization were then applied to all the averaged values. A total of 103 out of 22 184 genes that were differentially expressed ( P  &lt; 0.01) between hypertensive and normotensive groups were used in the model construction. We divided our dataset into a training set and a test set: the former containing 61 hypertensive cases (age 38.0 ± 7.3) and 61 age-matched normotensive controls (age 37.1 ± 6.9), whereas the latter comprised of the remaining 16 hypertensive cases (age 36.4 ± 6.8) and 21 normotensive controls (age 36.4 ± 5.9). The training set was further divided into five subsets (of sizes 12, 12, 12, 12 and 13) for model construction and selection via 5-fold cross-validation. According to the sample size table suggested by Tsai  et al. ( 2005 ), our training dataset (sample size 61 and gene number 22184) was more than capable of achieving an accuracy of 0.99 or a sensitivity of 0.95 at the family-wise power of 90%, at the expected number of false positive of 1, and at mean difference (standardized effect size) of 2. In addition, our technical replications are likely to further improve the above figures. 3 RESULTS 3.1 Model's architecture and performance Following the construction process described in  Section 2.2 , we first set  m init =2, and set  m final =8 to ensure that the computed endophenotypes contained a sufficient number of genes. During each iteration, the MO problem solver, SPEA (3000, 500, 0.9, 0.9, 0.9, 300) (see Appendix A in  Supplementary Material  for details), was employed to solve the associated MO problem. After computation of nearly all the near-global non-inferior solutions, we observed that the maximal mean value of the objective sums occurred at  m =3. In addition, the standard deviation of the objective sum was also small when  m =3 suggesting that the model was rather stable at this value. Therefore, we adopted a three-endophenotype model for the remainder of the model construction process. We then evaluated the performances of the five neural network models constructed via the 5-fold validation datasets.  Table 1  provides simple performance statistics of the five models. We selected the third model (constructed via the third dataset) because it had the best average performance ( E =0.967,  P uniq =0.772,  R int =0.499,  R BP =0.734 and  P unused =0.019, with training and validation datasets combined) and minimum performance variations in most of the objectives. Accordingly, the constructed model was capable of achieving an accuracy of 96.72% (i.e. 118 out of 122 subjects) in distinguishing hypertensive cases from normotensive controls in the dataset. The three constructed endophenotypes contained 62, 33 and 38 genes. Among these genes, 78 (41, 14 and 28 accordingly in the three endophenotypes) were unique (contained in only one endophenotype) and 23 were shared (contained in more than one endophenotypes) resulting in 101 genes that were actually used in the model. As a result, the proportion of unique genes,  P uniq , was 0.772 (78/101) and the proportion of unused genes,  P unused , was 0.019 (2/103). Furthermore, the minimal averaged gene correlation,  R int , of the three endophenotypes was 0.499, while the correlation between the average network outputs and the average blood pressure measurements in each subgroup,  R BP , was 0.734. The three endophenotypes are illustrated in  Figure 1  and detailed information about the selected genes is listed in Appendix C in  Supplementary Material .
 Fig. 1. The three constructed endophenotypes indicated in red, green and blue. From left to right the figure shows: (1) cluster tree of genes in each endophenotype (the dark and light shades are used to distinguish subclusters), (2) correlation coefficient between the gene cluster and its corresponding endophenotype, (3) normalized correlation (0–1) of the gene with others in the endophenotype, (4)  P -value of the endophenotype to hypertension, (5)  P -value (0–0.01) of a gene to hypertension, (6) absolute weight (0–0.7) of a gene to its corresponding endophenotype, (7) sequence number of the gene in the endophenotype, (8) gene symbol (highly consistent and influential genes are highlighted in purple, while genes with hypertension-related functions are highlighted in yellow), (9) cytogenetic information, (10) Unigene ID from Unigene build #163 and (11) major functions of selected genes. 
 Table 1. Performances of the five neural network models constructed via the 5-fold cross-validation E P uniq R int R BP P unused Model 1     Training sets 0.959 ± 0.012 0.612 0.461 ± 0.005 0.626 ± 0.158 0     Validation sets 0.960 ± 0.047 0.612 0.708 ± 0.013 0.289 ± 0.139 0     Test set 0.865 0.612 0.654 0.775 0 Model 2     Training sets 0.959 ± 0.010 0.709 0.417 ± 0.021 0.564 ± 0.084 0     Validation sets 0.960 ± 0.040 0.709 0.664 ± 0.021 0.207 ± 0.250 0     Test set 0.811 0.709 0.609 0.711 0 Model 3 a     Training sets 0.967 ± 0.009 0.772 0.452 ± 0.006 0.815 ± 0.073 0.019     Validation sets 0.967 ± 0.035 0.772 0.689 ± 0.028 0.412 ± 0.292 0.019     Test set 0.946 0.772 0.648 0.895 0.019 Model 4     Training sets 0.951 ± 0.014 0.738 0.451 ± 0.011 0.741 ± 0.058 0     Validation sets 0.950 ± 0.054 0.738 0.709 ± 0.012 0.466 ± 0.490 0     Test set 0.811 0.738 0.668 0.725 0 Model 5     Training sets 0.976 ± 0.009 0.738 0.432 ± 0.007 0.825 ± 0.109 0     Validation sets 0.976 ± 0.035 0.738 0.696 ± 0.014 0.479 ± 0.292 0     Test set 0.865 0.738 0.654 0.781 0 a Model 3 is adopted as the final model which is specified by values in bold face. 
 Similar performances were achieved on the test data (E=0.946, P uniq =0.772, R int = 0.648, R BP =0.895 and P unused =0.019). The test accuracy of 94.59% (35 out of 37) may seem high for a machine learning algorithm. However, the test accuracies of the other four models were 86.49% (32 out of 37), 81.08% (30 out of 37), 81.08% and 86.49%, respectively, making the average test accuracy of all five models at 85.95%. The high accuracy of 94.59%, although there are only five cases differing from the lowest accuracy of 81.08%, may be due to high correlation between the test dataset and the third training dataset. 3.2 Characteristics and effectiveness of the constructed endophenotypes To visualize different endophenotypic patterns in the hypertensive cases and in the normotensive controls, using a threshold of 0.5, we binarized the values of the endophenotypes to either 1 ( Fig. 2g  and  h ; red color indicates upregulation) or 0 ( Fig. 2g  and  h ; blue color indicates downregulation). For example, the pattern {Blue, Blue, Blue} of the first patient group (first column,  Fig. 2g ) was interpreted as genes having little rather than no effect for the three endophenotypes on hypertension. For comparison purposes, the original endophenotype values were also shown in  Figure 2i  and  j . Similar patterns were observed on the test dataset (Figure in Appendix D in  Supplementary Material ). We observed four phenomena from the two figures.
 Fig. 2. Average blood pressure values [diastolic blood pressure (DBP), systolic blood pressure (SBP)] and average network outputs across various endophenotypic patterns using training data: a blue circle or cross indicates a data point for an individual; error bars indicate the means and standard errors (SEs) of subject subgroups. ( a  and  b ) Individual DBP, their mean values and SEs. ( c  and  d ) Individual SBP, their mean values and SEs. ( e  and  f ) The network outputs, their mean values and SEs. ( g  and  h ) Binarized endophenotype values (using a threshold of 0.5) showing endophenotypic patterns. ( i  and  j ) Original endophenotype values represented in a gradation of red and blue colors (refer to the color bar at the right-hand side for actual magnitude); the vertical blocks between the black dashed lines denote subject subgroups defined by different endophenotype patterns (refer to g and h); horizontal blocks denote endophenotypes. 
 Observation 1—unique hypertensive patterns: two major and unique (not seen in normotensive controls) patterns {Blue, Blue, Red} (second column,  Fig. 2g ) and {Blue, Red, Red} (fourth column,  Fig. 2g ) were observed in the male hypertensive patients. In combining the training and test results, 28 cases were associated with the former pattern and 29 cases with the latter, which comprised a total of 74.03% (57/77) of the hypertensive population. Observation 2—unique normotensive patterns: two major and unique (not seen in hypertensive cases) patterns {Red, Blue, Blue} (second column,  Fig. 2h ) and {Red, Red, Blue} (fourth column,  Fig. 2h ) were observed in the male normotensive subjects. With the training and test results combined, 21 controls were associated with the former pattern and 30 controls with the latter, which included a total of 62.20% (51/82) of the normotensive population. Observation 3—overlapping patterns: five patterns {Blue, Blue, Blue}, {Blue, Red, Blue}, {Red, Blue, Red}, {Red, Red, Red} and {Blue, Red, Red} were observed in both the case and control groups. Subjects associated with these patterns seemed to have borderline blood pressure measurements (systolic/diastolic blood pressure=140/90 mmHg) than those with the unique patterns. Observation 4—protective/risk endophenotypes for hypertension: endophenotype 3 was upregulated in most of the hypertensive cases (65/77) and downregulated in most of the normotensive controls (61/82) ( Fig. 2g  and  h ). This finding suggests that endophenotype 3 has a strong effect on raising blood pressure. In contrast, a reverse pattern in endophenotype 1 suggests its strong effect on reducing blood pressure. The effect of endophenotype 2 is rather ambiguous. It was upregulated in more than half of the controls (47/82) and in less than half of the hypertensive cases (34/77), suggesting that endophenotype 2 is weakly associated with reducing blood pressure. Similar conclusions can be drawn using the magnitude and sign of the connection weights (2.1797, −1.5534 and −2.0513 for endophenotypes 1, 2 and 3, respectively) between the endophenotypes and the decision nodes of the model. In comparison with individual genes, the constructed endophenotypes also improved the efficacy of distinguishing hypertensive cases from normotensive controls. For a single gene, MIST was the most differentially expressed gene between cases and controls in our dataset (unadjusted  P =2.11 × 10 −5 ). However, the three endophenotypes were differentially expressed [ P =(5.78 × 10 −27 , 0.0384 and 3.87 × 10 −13 ), respectively] between cases and controls. 3.3 Gene cluster structure and major genes Gene clusters that were determined successively in their mergence order are presented by colored blocks in the first column of  Figure 1 . Clusters with a high correlation coefficient between cluster outputs and the outputs of their corresponding endophenotypes were considered influential to their corresponding endophenotype ( Fig. 1 , second column, clusters containing genes highlighted in purple). We evaluated the significance of each gene in an endophenotype using the two indices described in  Section 2.3  ( Fig. 2 , bar height in the sixth column represents index 1 and that in the third column shows index 2). Among the 15 genes with the highest index 1 values in each endophenotype, many were present within the influential gene clusters, including: the 48th (FLJ31393), 50th (BNIP1), 52nd (SLC4A5) and 57th (LOC283116) genes in endophenotype 1, the 1st (APOBEC3F), 5th (FLJ12221), 9th (SLC5A10), 11th, 12th, 18th and 19th genes in endophenotype 2, and the 6th, 9th (LDOC1), 20th (ATP1A4), 21st (STAT2), 22nd, 31st, 34th and 35th genes in endophenotype 3. Among these genes, SLC4A5 and STAT2 were previously identified as candidate genes for hypertension (Hunt  et al. ,  2006 ; Pan  et al. ,  1997 ); ATP1A4 was correlated with hypertension in an animal study (Tian  et al. ,  2001 ). Furthermore, according to the Gene Ontology Annotation (GOA) database (Camon  et al. ,  2004 ), SLC5A10, BNIP1 and LDOC1 are related to sodium ion transport, induction of apoptosis and negative regulation of cell-proliferation, respectively. With regards to the second index, the top 15 genes in each endophenotype included within the most influential clusters were the 52nd (SLC4A5) in endophenotype 1, the 1st, 2nd (APOBEC3F), 4th (SEC61A2), 6th (C6orf206), 7th, 8th (CHST8), 9th (SLC5A10), 11–13th, 16th (SDOS), 17th and 19th genes in endophenotype 2, and the 1st, 2nd, 3rd (SLC5A10), 4th, 5th (ECM1), 6th, 7th (LOC338864), 8th (MUC1), 9th (LDOC1), 10th, 15th and 19th genes in endophenotype 3. Among these genes, SEC61A2 is known to interact with ApoB, the main apolipoprotein of chylomicrons and low-density lipoprotein (LDL) (Chen  et al. ,  1998 ) and according to GOA, CHST8 is associated with central nervous system development. 4 DISCUSSION 4.1 Generalization of the constructed model An artificial neural network is a powerful classification tool capable of generating complex boundaries between different classes of data. However, with limited and noisy data, some training algorithms may construct a network that picks up noise in the data and thus loses its generalization capability. The 85.95% (average) accuracy evaluated on the test dataset suggested that our procedure is capable of constructing a generalized neural network model to simulate the gene–endophenotype–hypertension relationship. The overall 26 misclassifications by the five models in the test dataset were due to 11 normotensive controls and 2 hypertensive cases. Most of the misclassified subjects had borderline blood pressure (SBP/DBP=120/80 mmHg). Of the 11 misclassified controls, 10 were prehypertension (SBP &gt; 120 or DBP &gt; 80) and 1 was hypotension (SBP/DBP=101/56.5). Of the 10 prehypertensive subjects, 7 were high-normal (SBP &gt; 130 or DBP &gt; 85 based on JNC VI). On the other hand, both misclassified hypertensive cases (SBP/DBP=137.5/93 and 143.5/77.5) were in stage-1 hypertension (SBP=140–159 or DBP=90–99 based on JNC VI) with relatively lower blood pressure values. Recall that our model and endophenotypes were computed via the third dataset prepared for 5-fold validation. We evaluated the consistency of the endophenotypes and that of the identified significant genes by comparing them with those computed via the other four datasets. For the consistency of the endophenotypes, we found that 48 (out of 62), 23 (out of 33) and 29 (out of 38) genes in the three endophenotypes were also appeared in the corresponding endophenotype computed via at least two other datasets (reproducibility=75.19%). For the consistency of the significant genes identified by index 1, we found that 4, 4 and 4 of the top 15 genes in the three endophenotypes were also among the top 15 genes of the corresponding endophenotype computed via at least two other datasets. On the other hand, when using index 2, there were 13, 4 and 10 of the top 15 genes in the three endophenotypes that also appeared in the top 15 genes of the corresponding endophenotype computed via at least two other datasets. The significant genes selected by index 1 were less consistent than those by index 2, most likely because index 1 was computed using a single connection weight, which is very sensitive to the quality of data. Four of the 12 consistent significant genes identified by index 1 were among the aforementioned influential gene clusters, including BNIP1 in endophenotype 1, APOBEC3F in endophenotype 2, the 34th gene and LDOC1 in endophenotype 3. On the other hand, 12 of the 27 consistent significant genes identified by index 2 were among the influential gene clusters, including SLC4A5 in endophenotype 1; SLC5A10, SDOS, the 13th and 17th genes in endophenotype 2; and SLC5A10, ECM1, LOC338864, LDOC1, the 2nd, 4th and 6th genes in endophenotype 3. 4.2 Potential mechanisms in the endophenotypes We proposed that genes with high correlations to other genes in an endophenotype may be closer to the genetic origin of hypertension. In our model, SLC4A5, SLC5A10 and LDOC1 belong to such a type suggesting that sodium/bicarbonate transport, sodium/glucose transport and cell-proliferation regulation may play important upstream roles in pathogenesis of hypertension. On the other hand, genes with large connection weights to an endophenotype may be closer to the development of hypertension. BNIP1, APOBEC3F and LDOC1 are such genes consistently identified in our models, suggesting that induction of apoptosis, innate immune response and cell-proliferation regulation may play important downstream roles in development of hypertension. Although not as consistent as the aforementioned genes, ATP1A4 and STAT2 with high index 1 values suggesting that magnesium/potassium ion transport and calcium ion binding may lead to the development of hypertension. In addition, CHST8 and SEC61A2 with high index 2 values suggesting that central nervous system and LDL may also contribute to the hypertension onset. The roles of several genes with either high index 1 or 2 remain unknown and require further investigation. 4.3 Identification of causal pies and patient groups Rothman's ( 1976 ) concept of sufficient causes provides a theoretical framework of how multiple causal pies or phenocopies of a disease dilute the effect of target genes, resulting from either genetic or environmental causes. The following example demonstrates how the significance of a gene can vary in different types of patients. We compared the expression levels of  GCGR , a candidate gene for hypertension, obtained from two major case subgroups (denoted as S1 and S2 in  Fig. 3 ) with those obtained from a major control subgroup (denoted as S3 in Fig. 3 ). The  GCGR  was not significantly differentiable ( P  = 0.16) between S1 and S3, however, it was significantly differentiable ( P  = 0.03) between S2 and S3. Therefore, conventional approaches for modeling a particular disease may fail to identify potential genes when only examining gene expression profiles.
 Fig. 3. Gene expression plot of the three endophenotypes: each vertical strip represents a subject; each horizontal strip represents a gene; the colors used to indicate expression level is illustrated at the color bar in the right-hand side. The vertical blocks between black lines denote subject subgroups defined by different endophenotypic patterns (refer to  Fig. 2g  and  h ); horizontal blocks denote endophenotypes. The ‘S1’, ‘S2’ and ‘S3’ are magnified views of GCGR expression level of the three major subject groups (indicated in the three yellow boxes). Although the gene expression profile in  Figure 3  is not as clear as those of many gene mapping studies in the field of cancer genomics, it still exhibits certain patterns that differentiate hypertensive cases from controls. For example, in the top gene cluster (endophenotype 1 in our model), the predominantly {Red, Blue, Red, Blue} pattern (the dominant color in the block between the green dashed lines) is evident in the majority of male hypertensive cases. In contrast, the predominantly {Blue, Red, Blue, Red} pattern is evident in the majority of male controls. Patterns containing multiple color blocks were also observed in other gene clusters of cases and controls. Such patterns show how multiple mechanisms may work together to trigger disease onset. 4.4 Potential improvement To ensure that the constructed endophenotypes are biomedically significant, we linked the model's outputs to blood pressure measurements. Participants' blood pressures were obtained at different time points throughout the day; thus, these values may not truly reflect the condition of the participants. We linked the average model outputs to the average blood pressure measurements in each subgroup to reduce fluctuation between individuals. A model with more meaningful outputs could be constructed if multiple blood pressure readings (e.g. over a 24 h period) were available. The model performance could also be improved if more genes are used for model construction. Due to multiple causal pathways and genetic heterogeneity, certain disease genes may not express differentially between diseased and normal groups on the surface. In the absence of a priori knowledge regarding disease pathogenesis, this problem can be resolved by introducing more genes into the model. Because our training procedure adopts a SPEA approach instead of a gradient-based approach, the memory and computation time requirement remain manageable for large datasets. Regarding cardiovascular disease, Sing  et al.  ( 2003 ) pointed out that a biological model of genome–phenotype relationships should incorporates interactions between possible genetic and environmental factors. Although gene–gene interactions (these are partly included in our endophenotypes) and environmental factors were not considered in this study, they can be input into the model if properly encoded. However, the sample size must also be increased with the number of variables to ensure that the computed results are statistically meaningful. 5 CONCLUSIONS We have proposed a neural network-based model that simulates the gene–endophenotype–disease relationships for complex diseases where genetic heterogeneity is involved. In a real application, we successfully constructed a three-endophenotype model for Taiwanese hypertensive males. The model achieved high identification accuracy and was generalized to an independent set of data. The three computed endophenotypes, one strong protective, another weakly protective and the third highly risk, can be applied to predict young-onset male hypertension and to determine patient subgroups. Moreover, the three endophenotypes were consistent among datasets. Among the 101 genes involved in our model, we identified SLC4A5, SLC5A10 and LDOC1 as key upstream genes in each of the three endophenotypes, whereas BNIP1, APOBEC3F and LDOC1 were identified as key downstream genes in hypertension pathogenesis. In addition, four genes (ATP1A4, GCGR, SLC4A5, STAT2) were hypertension candidate genes and eight genes were associated with hypertension-related functions. These findings may help researchers to better understand the causes of hypertension. Several novel genes residing in multiple chromosomes were also found to be highly influential to the three endophenotypes. However, future studies are needed to examine whether and how these genes are involved in the pathogenesis of hypertension. We have also developed a procedure for constructing the proposed model. The procedure is capable of computing multiple models in a single iteration so that researchers can choose the most suitable one for their research needs. The developed procedure is applicable to other microarray platforms as well as to other genetic markers, such as single nucleotide polymorphisms (SNPs) and short tandem repeat polymorphic (STRP) markers. 
 Supplementary Material 
 
 
 [Supplementary Data] 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Using chemical organization theory for model checking</Title>
    <Doi>10.1093/bioinformatics/btp332</Doi>
    <Authors>Kaleta Christoph, Richter Stephan, Dittrich Peter</Authors>
    <Abstract>Motivation: The increasing number and complexity of biomodels makes automatic procedures for checking the models' properties and quality necessary. Approaches like elementary mode analysis, flux balance analysis, deficiency analysis and chemical organization theory (OT) require only the stoichiometric structure of the reaction network for derivation of valuable information. In formalisms like Systems Biology Markup Language (SBML), however, information about the stoichiometric coefficients required for an analysis of chemical organizations can be hidden in kinetic laws.</Abstract>
    <Body>1 INTRODUCTION Reaction networks are widely used to model biological systems at various levels, including the molecular level (Le Novére  et al. ,  2006 ), the cellular level (Wodarz and Nowak,  1999 ), the ecological scale (Hofbauer and Sigmund,  1998 ) and the level of social interactions (Dittrich  et al. ,  2003 ). Because of the growing number of models, methods for their verification are needed. Approaches that can be used for this purpose are flux balance analysis (FBA; Varma and Palsson,  1994 ), elementary mode analysis (Schuster  et al. ,  1999 ), extreme pathway analysis (Schilling  et al. ,  2000 ) and chemical organization theory (OT; Dittrich and Speroni di Fenizio,  2007 ). These methods have in common that they allow deriving constraints to dynamic behavior from the stoichiometric structure of a network. This stoichiometric structure is defined by the number of molecules of educts as well as the products each reaction consumes and produces. Thus, stoichiometry-based methods do not require a precise knowledge of the underlying reaction kinetics, which are often partly or totally unknown. Here, we will use OT. An important property of chemical organizations is that every steady state and growth state 1  of a network corresponds to a chemical organization (Dittrich and Speroni di Fenizio,  2007 , and  Supplementary Material ). These states we call the limit behavior of a model. However, this property is fulfilled only if a reaction network meets a condition formulated by Feinberg and Horn ( 1974 ): each reaction has a non-zero flux if and only if all of its educts have a positive concentration. Using this property, OT has already been applied to the prediction of growth phenotypes (Centler  et al. ,  2007 ) and the outcome of knockout experiments (Kaleta  et al. ,  2008 ), as well as in the design of chemical programs to solve NP-complete problems (Matsumaru  et al. ,  2007 ). In a recent work, we used OT to assess the quality of a genome-scale reaction network of  Escherichia coli  by identifying species and reactions that could not be present in the limit behavior of the model during simulation (Centler  et al. ,  2008 ). We concluded that these species and reactions hint at missing knowledge as they were mostly part of pathways starting from or ending in dead-end species. Here, we want to extend this approach in two directions. First, we present a method for more accurately predicting the limit behavior of a reaction network if information on reactions kinetics is available. If modeled in Systems Biology Markup Language (SBML, Hucka  et al. ,  2003 ), the velocity of a reaction depends on the concentration of its educts, products and modifiers. A modifier is a species whose concentration affects the reaction velocity, but whose concentration itself is not changed by this reaction. Some modifiers, as for example catalysts or activators, are required to be present for a non-zero reaction velocity. However, if we want a reaction to fulfill the Feinberg condition, such modifiers need to be added on its educt and product sides. Hence, since information necessary for the analysis using OT can be hidden in the kinetic laws, we present an algorithm for extracting this information. Second, using this approach, we demonstrate how knowledge of the organizational structure of a reaction network and thus of its limit behavior can help to uncover modeling inconsistencies. These inconsistencies are represented by species as well as by reactions that belong to no organization, indicating either incomplete knowledge, compounds missing from the specified growth media or modeling errors. This work is structured as follows. In  Section 2 , we give a short outline of OT and present an algorithm that modifies the stoichiometric structure in a reaction network such that the Feinberg condition is fulfilled. We use this algorithm in  Section 3  to demonstrate how these modifications affect the organizational structure of a model of the extracellular signal related-kinase (ERK)/Wnt-signaling pathway. In  Section 4 , we use our approach to find inconsistencies in a large-scale analysis of the models of the BioModels Database (Le Novére  et al. ,  2006 ) and compare our results with those obtained by other stoichiometric analysis techniques. Finally, we conclude in  Section 5 . 2 METHODS 2.1 Chemical OT We define a reaction network 〈ℳ, ℛ〉 by a set of molecular species ℳ and a set of reaction rules ℛ. A reaction rule ρ∈ℛ is defined by the stoichiometric coefficients  l i ,ρ  and  r i ,ρ  denoting the left- and right-hand sides of a reaction rule, respectively. Given a reaction rule ρ∈ℛ, we denote the set of reactant species and set of product species by LHS(ρ)≔{ i ∈ℳ| l i ,ρ &gt;0} and RHS(ρ)≔{ i ∈ℳ| r i ,ρ &gt;0}, respectively. With  N =( n i ,ρ )=( r i ,ρ − l i ,ρ ), we denote the stoichiometric matrix of 〈ℳ, ℛ〉. W.l.o.g. we assume  v ρ ≥0; hence a reversible reaction has two entries in  v . Given a set  A ⊆ℳ, its set of reaction rules ℛ A ={ρ∈ℛ|LHS(ρ)⊆ A }, and the corresponding stoichiometric matrix  N A , we say that  A  is closed if for all reaction rules ρ∈ℛ A , RHS(ρ)∈ A . Thus, we call  A  closed if there is no reaction with educts from  A  producing a species not in  A .  A  is self-maintaining if there exists a strictly positive flux vector  v′ ∈ℝ &gt;0 |ℛ A |  such that all species in  A  are produced at a non-negative rate, that is,  N A v′ ≥0 (Dittrich and Speroni di Fenizio,  2007 ). A set  A  that is closed and self-maintaining is called an organization (Fontana and Buss,  1994 ). An organization is called reactive if each of its species participates in at least one reaction of that organization. Elementary organizations are reactive organization that cannot be generated as union of other reactive organizations (Centler  et al. ,  2008 ). Because organizations may share the same species, the set of organizations together with the set inclusion ⊆ form a partially ordered set that can be visualized in a Hasse diagram, providing a hierarchical view of the network under consideration: organizations are vertically arranged by size, with small organizations at the bottom. Two organizations are connected by a line, if the upper contains the lower organization and no other organization exists between them. For simplicity, only species appearing for the first time, i.e. which are not element of a lower organization, are displayed. 2.2 Analyzing reaction networks with modifiers In this section, we introduce an algorithm that allows application of OT to reaction network models containing modifiers. As an example, we use a phosphorylation cycle, a typical motive found in signaling networks ( Fig. 1 A ). The network consists of seven molecular species ℳ={ A ,  B ,  M 1,… M 5} and three reactions ℛ={ R 1,  R 2,  R 3}.
 Fig. 1. ( A ) Example network (phosphorylation cycle) with seven species and three reaction rules. ( B  and  C ) Hasse diagrams of elementary organizations of the unprocessed and processed networks, respectively. Only species appearing for the first time in each organization are displayed. For the reactions ℛ = {  R 1:∅ →  A  ,  R 2: B  →  A ,  R 3: A  →  B }, we assume the following kinetic laws (omitting rate constants and units):
 
This model can be formulated in SBML, with  M 1,  M 2 being modifiers of reaction  R 2 and  M 3,  M 4,  M 5 being modifiers of reaction  R 3, while not appearing as reactants. Our algorithm consists of two steps: first, we examine the kinetic law of each reaction to detect minimal sets of modifiers that are necessary for that reaction to have a positive flux. Then, we use this information to adapt a reaction's set of reactants in order to more faithfully reflect the algebraic structure of the network used for computation of chemical organizations. 2.2.1 Step 1: identifying sets of essential modifiers In this first step, we identify all minimal supporting modifier sets of each reaction. Given a reaction ρ∈ℛ, a  minimal supporting modifier set  ( supporting set , for short) is defined as a minimal set of modifiers that need positive concentrations (while all others are absent) to allow reaction ρ to have a positive flux. If at least one of these modifiers is additionally set to a zero concentration, the flux of the reaction is constrained to zero. There might be several possibly overlapping supporting sets. With respect to a certain reaction, a modifier is called  essential  if it is contained in all supporting sets of the reaction. Determination of supporting sets: to decide whether a set of modifiers is a supporting set for a particular reaction, we follow a straightforward approach. If a set of modifiers is a supporting set, a positive concentration of only these modifiers allows a non-zero flux, while a positive concentration of only a proper subset of these modifiers constrains the flux to zero. Following this idea, we implemented FormulaChecker, which tries to compute the velocity of each reaction in terms of modifier concentrations. All variables in the kinetic law that represent undefined parameters or educt or product species are not further resolved; i.e. they are treated as symbols. The modifiers we want to test to determine whether they belong to a supporting set are also treated as symbols. The remaining modifiers are set to zero concentration. Function calls are resolved by application of their respective parameters, if necessary. Applying FormulaChecker can lead to two different results for the reaction velocity:
 The result is zero. In this case the tested modifier set is not a supporting set. Let { M 3}, for example, be the set to be checked in  R 3. Setting the concentrations of the remaining modifiers to zero results in  v R 3 =0. Thus, { M 3} is not a supporting set of R3. This also applies to the sets { M 4} and { M 5}. The result is non-zero. Thus, it might be a constant only depending on parameters, or a formula, dependent on variables. Checking { M 3,  M 4} in  R 3 yields the kinetic law  v R 3 =[ A ]([ M 3][ M 4]). Since we know that { M 3}, { M 4} and { M 5} do not represent supporting sets, { M 3, M 4} has to be a supporting set. In contrast, if we check the empty set in  R 2 by setting  M 1 and  M 2 to zero values in the kinetic law, we obtain  v R 2 =[ B ]. In consequence, neither { M 1} nor { M 2} represent supporting sets of R2; the supporting set is the empty set, and no further tests are required. 
 Finding all supporting sets: in order to find all supporting sets of a reaction, the algorithm analyzes the power set of the reaction's set of modifiers to ensure that all supporting sets are found. The sets are checked in increasing size order, trying to avoid testing the whole power set of modifiers. If we find that a set of modifiers is a minimal supporting set, we do not need to test any of its supersets. Looking at  R 3 in the example, after the empty set, all single-modifier sets are checked. We find that neither  M 3 nor  M 4 nor  M 5 allow a positive flux if standing alone. In the next step all two−element sets are tested. Since all these sets allow a positive flux of  R 3, but none of the smaller ones, we conclude that { M 3,  M 4}, { M 3,  M 5} and { M 4,  M 5} are the supporting sets. In consequence, we do not have to test the superset { M 3,  M 4,  M 5}. 2.2.2 Step 2: adapting the reactions In the second step, each reaction possessing at least one supporting set is processed. For each supporting set the reaction is duplicated and the modifiers of the supporting set are added as catalysts to the duplicate reaction. Finally, the original reaction is removed from the model. In order to preserve the dynamics of the original model in the processed model, the kinetic law of each of the duplicate reactions is divided by the number of derived reactions, i.e. the number of supporting sets. The duplicate reactions get new names of the form  [old_reaction_name] variant [number] . For our example, we obtain the following set of reaction rules ℛ={ R 1,  R 2,  R 3 variant1 ,  R 3 variant2 ,  R 3 variant3 } with
 For a more detailed outline of the processing of the kinetic laws, see the  Supplementary Material . 2.2.3 Example application Applying the algorithm to our example, we can see several effects of the processing of the kinetic laws (see  Figs 1 B and C for the Hasse diagrams of elementary organizations). Two trends are superimposed. First, some organizations vanish, including the organization solely containing  A  and  B  in the unprocessed network. In the processed network, a reaction still converts  B  to  A . In order to replenish  B , one pair of the modifiers  M3 ,  M4  and  M5  is necessary. Thus, { A ,  B } does not fulfill the self-maintenance condition in the processed network. Second, some organizations appear for the first time, as in the case of the organization containing  A  in the processed network. In the original network, the set { A } was not closed since  R 3 unconditionally produced  B  from  A . 3 ORGANIZATIONAL STRUCTURE OF THE ERK/WNT-SIGNALING PATHWAY In order to demonstrate the utility of the incorporation of kinetic laws into the analysis with OT, we analyze the model  BIOMD149 2  from the BioModels database (Le Novére  et al. ,  2006 ) containing an integrated ERK and Wnt/β-catenin signaling pathway ( Figure 2 ). This model is based on the work of Kim  et al.  ( 2007 ), who described a positive feedback loop between these two pathways important in the development of some cancer. The positive feedback loop works through a yet unknown mechanism modeled by a species called ‘molecule X’. The transcription of this molecule is modeled to be upregulated by a complex of β-catenin and T-cell factor (TCF). The availability of β-catenin is regulated by active glycogen synthase kinase 3β (GSK-3β), which in turn is inactivated by phosphorylated ERK. According to the model, X upregulates the signaling through the ERK-pathway. The rates of phosphorylation of the different levels of the ERK-pathway are modeled with kinetic laws. Thus, a high concentration of phosphorylated Raf increases the rate of phosphorylation of MEK, which in turn increases the rate of phosporylation of ERK.
 Fig. 2. Simplified representation of the reaction network from  BIOMD149  (Kim  et al. ,  2007 ) combining the ERK- and Wnt-signaling pathways. The Wnt signal, serving as input to both pathways, is not shown. Lines with circles represent essential modifiers identified with the presented approach. Lines ending in orthogonal bars indicate inhibition. Without the processing of the kinetic laws the network contains 384 reactive organizations generated from the union of 11 elementary organizations. After processing, the network contains 150 reactive organizations generated from the union of 18 elementary organizations. Thus, the number of reactive organizations declines, while the number of elementary organizations increases.  Figures 3  and  4  depict the Hasse diagram of elementary organizations of both networks. The Hasse diagram of the unprocessed network ( Fig. 3 ) displays a very simple structure. The smallest organization already contains X. From the kinetic law of the production reaction of X, it can be determined that a positive concentration of the complex β-catenin/TCF is required for a non-zero flux of this reaction. But this is not taken into account since this constraint is modeled through the modifiers of the reaction and not on the level of substrates and educts as required by the Feinberg condition. Consequently, the different levels of the ERK-signaling pathway are also present independent of each other. This can be observed by the presence of the corresponding phosphorylated and dephosphorylated proteins directly above the smallest organization in the Hasse diagram.
 Fig. 3. Hasse diagram of elementary organizations of  BIOMD149  without processing of the kinetic laws. Only species appearing for the first time in each organization are shown. For example organization 9 contains the species displayed in the nodes corresponding to organization 0, 7 and 9. Not all species in organization 10 are displayed. A list of abbreviations can be found in the  Supplementary Material . Phosphorylated forms of a protein are denoted by the suffix ‘_ast’. Active/Inactive forms by the suffix ‘a’/‘i’. 
 Fig. 4. Hasse diagram of elementary organizations of  BIOMD149  after the processing of the kinetic laws. Only species appearing for the first time in each organization are shown. Not all species in organization 12 are displayed. Naming follows the same conventions as in  Figure 3 . The different pathways for upregulation of the ERK-signaling pathway are indicated. In comparison to  Figure 3 , we find, for example, the node corresponding to organization 6 above the node corresponding to organization 2 (corresponding to the nodes labeled 7, respectively, 3 in  Fig. 3 ). This corresponds to the conclusion that a positive concentration of Rasa and Rasi is required for the presence of Raf1 and Raf1* in the limit behavior. Comparison with  Figure 3  shows that this conclusion can be drawn only if the kinetic laws are processed. From a simulation perspective, the reactive organizations of the original network would indicate a state of the network where, for example, MEK and MEK* as well as the input species could be constantly present ( Fig. 3 , organization 4). However, by examining the kinetic laws of the phosphorylation from MEK to MEK*, we find that this reaction has a flux of zero if the species Raf1* is not present. Thus, only the dephosphorylation of MEK would have a positive flux, finally using up all MEK*. After processing of the corresponding kinetic law, Raf1* is identified as an essential modifier and added as a catalyst to the reaction, as seen in the Hasse diagram of the processed network ( Fig. 4 ). The organization containing the species MEK* and MEK ( Fig. 4 , organization 8) is situated above the organization containing Raf1* ( Fig. 4 , organization 6). From this perspective, the processing of the kinetic laws can be seen as adding mechanistic detail to the reactions. Thus, when we find Raf1* necessary for the phosphorylation of MEK to MEK*, the addition of the modifier Raf1* as catalyst corresponds to the complex formation between Raf1* with MEK prior to phosphorylation. The approach to consider kinetic laws in OT can be seen as refinement of the reactions of a model making use of the additional information present in kinetic laws. Even though OT does not explicitly require the kinetic laws of a reaction network, knowledge about them can be used to better predict the limit behavior of a reaction network. Conversely, in the sense of the Feinberg condition, the underlying mechanisms are modeled more accurately on the stoichiometric level of the network if this approach is used. In agreement with the results of Kim  et al.  ( 2007 ), we find an alternative route for the activation of the ERK-pathway, indicated by the organizations 3, 7, 9 and 11 in  Figure 4 . Through the action of the complex β-catenin/TCF, the transcription of X is upregulated and, thus, bypasses the activation of Raf by Ras. A constant activation of β-catenin/TCF, for example through a mutation, can result in a decoupling from any signal and consequently lead to a constant upregulation of the ERK-signaling pathway, as is often found in cancer (Kim  et al. ,  2007 ). In the unprocessed network, we do not obtain these results. 4 LARGE-SCALE ANALYSIS OF BIOMODELS In order to demonstrate the utility of our approach, we analyze the models of the 11th release 3  of the BioModels database (Le Novére  et al. ,  2006 ). This database contains 185 manually curated models of biological networks in SBML format. SBML allows species to be defined as external. Thus, their concentration is assumed constant. For the computation of chemical organizations, we add an inflow and outflow reaction of the form ∅→ s  and  s →∅ for each external species  s . For all except 3 models, we were able to compute the reactive organizations using the deterministic algorithms for organization computation [see Centler  et al.  ( 2008 ) for algorithmic details]. For the remaining three models ( BIOMD014 ,  BIOMD019  and  BIOMD049 ), a heuristic based on a random walk strategy to determine organizations (Centler  et al. ,  2008 ) needed to be applied. Since we wanted to identify species appearing in no organization and each of these models did contain an organization encompassing the entire species set, computation of the complete set of organization was not necessary for these models. A total of 172 models contained a non-empty organization. In the remaining 13 models only the empty organization was found, since they contained neither reactions nor species. An overview of the number of reactive organizations is given in Table 2 in the  Supplementary Material . While 77 models contained only a single reactive organization, the highest number of organizations was found in  BIOMD175 , with 319 248 reactive organizations. An overview of the distribution of the number of organizations can be found in  Figure 5 .
 Fig. 5. Histogram of the number of reactive organizations in the models of the BioModels Database. Please note that this number includes six models with more than 1000 organizations (listed below &gt;10 organizations). Species participating in none of the reactions can drastically increase the number of organizations in a network. Thus, we computed only the reactive organizations in each network and omitted species participating in no reaction (in 24 models) from the analysis. In 31 models some species did not appear in any reactive organization. A first analysis showed that this set contains many models where such behavior was intended. Thus, in several models the concentration of some species was set to a non-zero value at a given time point (e.g.  t =0). To take into account this short-time behavior, we added an inflow reaction for each such species. Doing this, we found that only five models with species absent from any reactive organization remained:  BIOMD044 ,  BIOMD093 ,  BIOMD094 ,  BIOMD143  and  BIOMD151  ( Table 1 ). By analyzing the reactions in which the missing species participated and comparing the SBML models to their description in the corresponding publications, we found the potential inconsistencies. We identified all these inconsistencies as actual modeling errors.
 Table 1. Selected results from the large-scale analysis See  Supplementary Material  for the entire table. The five models in which inconsistencies have been identified are shaded in light gray. The first 4 columns give general details about the models. Numbers in brackets indicate the number of reactions of the original network that can increase through processing of the kinetic laws. The number of species remains constant. The fourth column gives the number of reactive organizations in the modified and (in brackets) the original network. In the fifth and sixth columns species and reactions that can be present in the limit behavior of the processed network are given. OT denotes the predictions by OT, and FBM the predictions by flux-based methods. In some cases, FBM identifies more reactions to be present in the limit behavior than OT. These cases are shaded in dark gray. The seventh and eighth columns give the same numbers when inflow reactions for species with an event setting their concentration to a positive value at a certain time point are added. In cases where the original network already contained all species, those numbers are omitted. 
 4.1 Resolving network inconsistencies In three of the five models,  BIOMD093 ,  BIOMD094  and  BIOMD143 , we identified reactions that were set to irreversible despite their kinetic laws producing negative fluxes in the course of the simulation, as described in the corresponding publications. Thus, they were indeed reversible and we modified them accordingly. Repeating the analysis, we found all species present in the reactive organizations of  BIOMD093 . In  BIOMD094 , missing species remained. However, this was an intended behavior since a gene knockout was modeled (Yamada  et al. ,  2003 ). In  BIOMD143 , we still found some species absent after we had changed reactions with negative fluxes in the simulation to be reversible. This model describes the oscillatory metabolism of activated neutrophils (Olsen  et al. ,  2003 ). A simplified and decompartmentalized version of the relevant reactions is depicted in  Figure 6 . The species absent from the reactive organizations are hydrogen from cytoplasm and phagosome. The model contains only reactions consuming these two species. During simulation even negative concentrations of both species appear. The reason for the consumption of these species is inconsistent modeling of the stoichiometry of the reactions and an inconsistent kinetic law. Cytoplasmatic and phagosomal hydrogen are consumed together with superoxide ( O 2 − ) to produce hydrogen peroxide ( H 2 O 2 ). In the course of the disposal of  H 2 O 2  by ferric peroxidase in the phagosome, additional four protons from melatonin (MLTH) are consumed to produce the initial form of ferric peroxidase. With the exception of ferric peroxidase and free radicals of melatonin (MLT), all species are consumed without producing equivalent products. Thus, the disposal of  H 2 O 2  by ferric peroxidase consumes oxygen and protons. The model contains an inflow for NADPH and O 2 . Oxidation of NADPH by oxygen or free radicals of melatonin can produce superoxide and melatonin, respectively. Thus, there is a constant inflow of NADPH and oxygen that can replenish the consumed species. However, the kinetic law of the production of superoxide from  O 2 −  and hydrogen does not depend on the concentration of hydrogen in the model. Together with a zero initial concentration of hydrogen, the simulation of the model leads to a negative concentration of this species. Making the rate law dependent on the concentration of hydrogen resolves the problem of negative concentration of hydrogen. Additionally, either removing the inconsistencies in the stoichiometry or adding an inflow for hydrogen allows positive concentrations of this species during simulation.
 Fig. 6. Simplified representation of the reactions of  BIOMD143 . As a result of inconsistent stoichiometries hydrogen and oxygen are consumed in the course of detoxification of hydrogen peroxide. There is only an inflow of oxygen, and the consumption of hydrogen does not depend upon its concentration. Consequently, a simulation leads to a negative concentration of hydrogen. The reasons for the missing species in  BIOMD044  are very similar. Here, a species is modeled to serve as a pseudo-substrate to a reaction that could have been modeled without substrate. The kinetic law governing the reaction does not depend upon the concentration of this substrate. Since it is not produced by any other reaction, negative concentrations appear in the course of the simulation. Replacing the respective reaction by an inflow reaction resolves the problem. In  BIOMD151  almost all species are absent from reactive organizations. This network represents an integrated model of the JAK/STAT and ERK-signaling pathways regulated by IL-6 in hepatocytes (Singh  et al. ,  2006 ). A detailed analysis of the model and the set of ordinary differential equations presented in Singh  et al.  ( 2006 ) showed that a complex formation step was missing, such that the signal from IL-6 could not be transmitted to the subsequent signaling pathways. Only the complex dissociation reaction was present. During simulation it had a negative flux, mimicking the complex formation reaction. Adding the missing step produced a model in which all species appeared in a reactive organization. 4.2 Comparison with flux-based methods Next, we will compare our results with those obtained with flux-based methods, including FBA (Varma and Palsson,  1994 ), elementary mode analysis (Schuster  et al. ,  1999 ) and extreme pathway analysis (Schilling  et al. ,  2000 ). These methods can be used to check whether a certain reaction can be present in a steady-state flux obeying the irreversibility constraint. Thus, they can predict whether a reaction can be present in the limit behavior of a reaction network. In FBA this can be done directly, while elementary mode analysis and extreme pathway analysis return a set of vectors spanning the solution space of the steady-state condition. However, since OT also takes into account growth states, in which some species accumulate, the steady-state condition is adapted accordingly (details can be found in the  Supplementary Material ). Furthermore, since we only want to know whether a reaction can appear in any steady state or growth state, we do not need to apply these methods directly, but can use a linear programming approach similar to FBA, outlined in the  Supplementary Material . We compared the predictions of flux-based methods to those of OT for the models of the BioModels Database. With OT, we identified 31 models where some reactions did not appear in any reactive organization. The same 31 models are identified using flux-based methods. However, when analyzing the predicted set of available reactions in detail, we found differences in 25 of the 31 models. Due to the definition of self-maintenance, the set of available reactions is a subset of those predicted by flux-based methods. Thus, in all 25 cases, flux-based methods found reactions present in the limit behavior that indeed could not maintain a positive flux in a long-term simulation. The reason for this difference closely follows a concept presented in Kaleta  et al.  ( 2006 ): a steady-state flux in a network uses some species that cannot be produced at a positive rate. In this flux, these species might be interconverted into each other or act as catalysts. Further assume that there is a reaction steadily draining some of the unproducible species. Thus, they will finally vanish. In consequence, this steady-state flux cannot be part of any steady state of the complete network. If a particular reaction is present only in such steady-state fluxes, it is predicted to be present in the limit behavior of a reaction network by flux-based methods, while OT correctly identifies it as absent since it correctly takes into account the drain of the unproducible species. We will outline this concept in more detail using  BIOMD037 , a model of the sporulation control network in  Physarum polycephalum  by Marwan ( 2003 ) ( Fig. 7 ). While OT predicts 8 of the 12 reactions to be absent from the limit behavior ( Fig. 7 A), flux-based methods identify only four such reactions ( Fig. 7 B). The differentially predicted reactions account for the interconversion of Pfr to Pr and Xi to Xa. Flux-based methods find a flux where the conversion of Pfr to Pr and vice versa is in equilibrium. However, this does not take into account that there is also a reaction irreversibly converting Pr to Pi. Thus, a non-zero concentration of Pr will be depleted by the conversion into Pi. In consequence, there is no reactive organization containing Pfr and Pi.
 Fig. 7. Reaction network from  BIOMD037  modeling the sporulation control in  P.polycephalum  from Marwan ( 2003 ). Lines ending in circles indicate essential modifiers for a reaction. Light gray reactions cannot have a positive flux in the limit behavior, according to  A  OT and  B  flux-based methods. Abbreviations: Pr, active photoreceptor; Pi, inactive photoreceptor; (pre/prepre)S, sporulation signal (and precursors); Ya/i, active/inactive glucose receptor; Gluc, glucose; Xa/i, active/inactive signal transducer. Additionally, we find an interesting case in the interconversion of Xa to Xi and vice versa. The conversion of Xi to Xa requires the presence of Pr. Flux-based methods identify an equal flux of both reactions as a feasible flux, since Pr acts only as a catalyst. However, the analysis using OT shows that such a flux also requires the presence of Pr. Thus, both species cannot persist in the limit behavior since Pr, required for the reaction of Xi to Xa, will vanish over time. Since Xa is steadily converted to Xi, only this species would finally remain. This demonstrates how our approach takes the kinetic laws into account which is not possible using flux-based methods. In two of the models in which we identified inconsistencies,  BIOMD094  and  BIOMD151 , predictions for the presence of reactions in the limit behavior between OT and flux-based methods differ. In  BIOMD151 , OT predicts nine reactions to be present, while flux-based methods identify 112 of the 114 overall reactions. As outlined above, flux-based methods can predict only the same or a larger set of reactions to be present in the limit behavior. Thus, the search for inconsistencies is simplified by reducing the size of the system to analyze if OT is used. This is also corroborated by three models in the uncurated branch of the BioModels Database containing inconsistencies. In all three models, flux-based methods predict more reactions to be present in the limit behavior than OT (see  Supplementary Material  for further details). 5 CONCLUSIONS In this work, we demonstrated that information hidden in kinetic laws affects the results obtained from chemical organization theory (OT). We presented an approach that is able to uncover this information. This approach enabled us to refine the chemical organizations in 41 of the 185 models (22%) of the BioModels Database. The Hasse diagram of organizations of the processed model of a combined ERK/Wnt-signaling pathway took into account the different levels of phosphorylation in the signaling cascade, while the set of organizations of the unprocessed network did not. Furthermore, the Hasse diagram of organizations demonstrated several possible pathways for constant upregulation of this pathway, an important event in carcinogenesis consistent with the results of Kim  et al.  ( 2007 ). Analyzing the 185 models of the BioModels Database, we checked the behavior of the models during long-term simulation (limit behavior). Thus, we found 31 models where several species could not persist in a long-term simulation. Furthermore, we identified five models in which some species could not be present at all during simulation. This was due to inconsistent reversibility constraints in two models, negative concentrations of some species during simulation in other two models and a missing reaction in the fifth model. In the non-curated branch of the BioModels Database, we identified the models with modeling errors. Comparing the set of species present and the reactions having a non-zero flux in the limit behavior, we found OT able to predict those sets more accurately in 25 models (14%) compared with flux-based methods like FBA, elementary mode analysis and extreme pathway analysis. These models account for 81% of the models in which the set of species and reactions present in the limit behavior of the model did not encompass the entire set of species and reactions. In five of the 8 models of both branches of the BioModels Database in which we detected modeling errors, OT made more accurate predictions in comparison to flux-based methods. These results demonstrate that OT is a valuable tool in three important aspects of network design and analysis. First, when this approach is used to extract additional information from the kinetic laws of the reactions, the set of organizations corresponds to the potential steady state and growth states of a reaction network. Thus, important information about the dynamic structure of a reaction network can be uncovered. Second, OT can be used in an iterative fashion to assist in model building by identifying inconsistencies that need to be resolved. Third, OT more faithfully identifies parts of a network whose maintenance is not yet explained than flux-based methods. Thus, it is of particular interest for identifying gaps due to missing knowledge in large-scale metabolic networks as documented in Centler  et al.  ( 2008 ). In consequence, it can be beneficial for methods aiming to remove such inconsistencies (Kumar  et al. ,  2007 ; Reed  et al. ,  2006 ). In the other direction, our approach could be extended by these methods to automatically propose changes in order to remove inconsistencies. However, computational constraints currently prohibit the application of our deterministic algorithms to very large networks (e.g. more than 500 reactions). An approximation can be used for networks of this size, but the results require manual checking. A more efficient algorithm that will enable the application of OT to genome-scale networks is in development. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>TagDust—a program to eliminate artifacts from next generation sequencing data</Title>
    <Doi>10.1093/bioinformatics/btp527</Doi>
    <Authors>Lassmann Timo, Hayashizaki Yoshihide, Daub Carsten O.</Authors>
    <Abstract>Motivation: Next-generation parallel sequencing technologies produce large quantities of short sequence reads. Due to experimental procedures various types of artifacts are commonly sequenced alongside the targeted RNA or DNA sequences. Identification of such artifacts is important during the development of novel sequencing assays and for the downstream analysis of the sequenced libraries.</Abstract>
    <Body>1 INTRODUCTION Next-generation sequencing is applied to address a whole range of biological questions (Mardis,  2008 ; von Bubnoff,  2008 ). A widely recognizable challenge lies in the computational treatment of the huge volumes of data being generated. An initial step is to verify whether a sequencing run was successful. A low mapping rate to a reference genome is commonly a good indicator of the run quality, however, it fails to explain the source of the unmapped sequences. From experience we know that large fractions of the unmapped sequences often correspond to artifacts arising from linker and adaptor sequences used in the library construction. Such artifacts are comparable with vector sequences found in traditional Sanger sequencing (White  et al. ,  2008 ). The identification of these artifacts is important during the development of novel sequencing assays. More importantly, a fraction of artifacts commonly maps to reference genomes and can thus influence the biological interpretation of the libraries. The situation is particularly problematic when comparing two RNA samples sequenced at different biological states. If the total number of sequences from states A and B is the same but the fraction of artifacts is increased in state B, it may appear that non-artifactual sequences are downregulated compared with state A. Identification of known library sequences in sequenced reads should be trivial. However, sequencing errors, PCR errors, short read lengths, combinations of several fragmented sequences and their reverse complements complicate this task dramatically. To resolve this basic issue we developed TagDust, a program employing a fast, fuzzy string-matching algorithm to identify partial matches to library sequences in the reads. A read is annotated as an artifact if a large fraction of its residues can be explained by matches to library sequences. 2 METHODS We previously employed the Muth–Manber algorithm (Muth and Manber,  1996 ) in the context of multiple alignments to quickly assess sequence similarity (Lassmann  et al. ,  2008 ). It allows for multiple string matching with up to one error (mismatch, insertion or deletion). The latter is achieved by creating libraries of  k -mers from both query and target strings. The library is then extended to patterns of length  k −1 by deleting each character in all the original  k -mers in turn. For example, the 4mer  ACGT  will be converted into  CGT ,  AGT ,  ACT  and  CGT . We will refer to these extended patterns as  lk -mers. A comparison of these libraries via fast exact string matching reveals all matches with up to one error. For example, a mismatch in the original sequences is detected with an exact match of the  lk -mers lacking the mismatched residues. As a default, TagDust used a  k -mer length of 12. For detecting artifacts, we are not really interested in the individual matches to a read but instead whether a large proportion of a read can be labeled as matching library sequences. Hence, we altered the default Muth–Manber algorithm to return the percentage of nucleotides involved in matches to library sequences and to run efficiently on very large datasets. Briefly, we record all  lk -mers derived from the library sequences in a bitfield, scan all reads and identify matches with quick bit-lookups. Since the sequenced reads are currently short, between 30–50 nt in length, spurious hits often occur. Discarding reads based on these matches is obviously undesirable. Therefore, it is crucial to select a suitable cutoff on the percentage of residues covered by library sequences. We approach this problem in a manner analogous to recent work by Zhang  et al.  ( 2008 ) relating to the interpretation of ChIP-sequencing data. Initially, we simulate a sequencing dataset with the same length distribution and nucleotide composition as the input dataset. Secondly, we apply the modified Muth–Manber algorithm to the simulated reads to derive a distribution of the number of reads labeled as 5%, 10%,…, 100% library sequences. The distribution reflects how often we expect reads to be labeled as  X % library sequence by chance. Finally, we obtain  P -values from this null distribution and adjust them using the Benjamini–Hochberg method to reflect the controlled false discovery rate (FDR; Benjamini and Hochberg,  1995 ). The lowest sequence coverage that gives the requested FDR is then used as the cutoff value. For efficiency, TagDust is implemented in the C programming language. TagDust uses &lt;5 MB of memory since only single reads are read into memory at a time for processing. Hence, it is applicable to current datasets and the large volume of data expected with future next-generation sequencing instruments. A computational bottleneck is the calculation of the adjusted  P -values since this step, in principle, requires sorting of millions of  P -values. However, since sequence lengths are natural numbers, only a selection of coverage cutoffs and associated  P -values is possible. For example, a 20-nt sequence can be 95% or 100% labeled as library sequences but not by 97%. We take advantage of this and use a bit-sort-like algorithm to perform this step in linear memory and time. TagDust is freely available from the OMICS software repository or by request from the author. 3 RESULTS AND DISCUSSION Obtaining suitable datasets for benchmarking our method is not trivial since partially failed sequencing runs are commonly not deposited in public databases. Nevertheless, we obtained five datasets sequenced by the Illumina Genome Analyzer from the NCBI short read archive. We used the standard Illumina adaptors and primers used in the different sequencing assays as target sequences to be filtered out from the reads. As expected, only a relatively small percentage of the deposited reads can be explained by library sequences ( Table 1 ). To determine whether the same sequences could be filtered out by simply mapping to the reference genome, we mapped all artifactual sequences with up to two mismatches to the human genome (hg18 assembly) using nexalign (T.Lassmann, manuscript in preparation). Evidently, a varying percentage of the artifactual sequences map to the genome. In the absence of replicates it is difficult to determine whether such tags are actual artifacts and hence we recommend users to merely flag such reads and their mapping positions.
 Table 1. Percentages of reads identified as artifacts in five sequencing runs at varying FDR thresholds Description Accession Sequences FDR 0.05 (%) FDR 0.01 (%) FDR 0.001 (%) CPU sec. Genomic PE (18 nt) ERR000017 6 381 596 1.4 (98.79) 0.4 (98.91) 0.1 (98.61) 28 Genomic PE (36 nt) ERR000130 10 209 914 3.2 (84.05) 0.8 (52.72) 0.4 (11.44) 84 Genomic (25 nt) SRR000723 7 230 975 1.7 (57.64) 0.5 (54.26) 0.1 (36.44) 45 Chip-Seq (25 nt) SRR000731 6 011 079 3.7 (29.15) 2.5 (12.81) 2.0 (1.73) 37 RNA-Seq (33 nt) SRR002052 12 099 833 1.8 (23.32) 0.6 (22.30) 0.1 (20.38) 103 The mapping rates of the artifactual sequences to the human genome are indicated in brackets. The last column lists the runtime of TagDust in CPU seconds for the 0.05 FDR cutoff. 
 TagDust processes even the largest dataset here in &lt;2 min on a standard desktop PC while using &lt;5 MB of memory. Conceivably, the time it takes to map libraries can be reduced by using TagDust to filter out artifacts before the mapping. The two main applications for TagDust are to troubleshoot failed large-scaled sequencing runs and to filter out artifactual sequences from successful ones. The latter may affect the biological interpretation of the produced data since some artifactual sequences map to the respective reference genomes. Funding : Research Grant for the RIKEN Omics Science Center from the Ministry of Education, Culture, Sports, Science and Technology of the Japanese Government (MEXT to Y.H.); a grant of the Genome Network Project from the Ministry of Education, Culture, Sports, Science and Technology, Japan; the Strategic Programs for R&amp;D of RIKEN Grant for the RIKEN Frontier Research System, Functional RNA research program. Conflict of Interest : none declared. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Towards next-generation diagnostics for tuberculosis: identification of novel molecular targets by large-scale comparative genomics</Title>
    <Doi>10.1093/bioinformatics/btz729</Doi>
    <Authors>Goig Galo A, Torres-Puente Manuela, Mariner-Llicer Carla, Villamayor Luis M, Chiner-Oms Álvaro, Gil-Brusola Ana, Borrás Rafael, Comas Espadas Iñaki, Birol Inanc</Authors>
    <Abstract/>
    <Body>1 Introduction Tuberculosis (TB) is the most lethal infectious disease caused by a single agent, namely bacteria belonging to the  Mycobacterium tuberculosis  complex (MTBC) ( World Health Organization, 2017 ). Whereas isolating the bacteria from clinical specimens is a time-consuming process, rapid molecular tests have the potential to identify the pathogen DNA in a few hours ( Eddabra and Benhassou, 2018 ;  Machado  et al. , 2018 ). Over the years, many different molecular assays have been developed for the specific detection of MTBC and its differentiation from non-tuberculous mycobacteria (NTM) ( Chin  et al. , 2018 ). Most of these assays are based on the PCR amplification of genomic targets that are thought to be specific to the MTBC, like the insertion sequence IS6110, or rely on the design of specific primers that amplify conserved bacterial regions such as the  rpoB  or  rrs  genes. Most of these markers were identified in the nineties and have not been evaluated in the light of current genomic databases. In addition, several shortcomings are known for the different assays targeting current MTBC markers, being of special concern the lack of specificity and sensitivity ( Chin  et al. , 2018 ). The development of new molecular tools for TB diagnosis is an active area of research, with many companies involved, looking for the endorsement of the World Health Organization (WHO) ( Pai  et al. , 2016 ). The most successful example has been the Xpert MTB/RIF test ( Cirillo  et al. , 2017 ), which was endorsed by the WHO back in 2010 for TB diagnosis and recommended as the first-line diagnostic in 2017 ( WHO, 2017 ). The Xpert assay amplifies a conserved region within the  rpoB  gene to detect both MTBC DNA and drug resistance mutations to rifampicin. With the aim of improving its sensitivity, the new Xpert MTB/RIF Ultra assay also amplifies the MTBC insertion sequence IS6110 and IS1018. However, these insertion sequences were described as MTBC-specific decades ago ( Collins and Stephens, 1991 ;  Thierry  et al. , 1990 ) and several studies have shown them to be present in non-MTBC organisms while some MTBC strains are known to lack any copy ( Liébana  et al. , 1996 ;  Müller  et al. , 2015 ;  Pérez-Osorio  et al. , 2012 ;  Roychowdhury  et al. , 2015 ). The fact that the novel assays developed are still based on the amplification of loci that are not specific to the MTBC, highlights the need for the discovery of novel and specific MTBC targets. Analyzing omic data has been proven to be an effective strategy for the identification of species-specific markers in several organisms ( Buchanan  et al. , 2017 ;  Carmona  et al. , 2012 ;  Carrera  et al. , 2017 ;  Koul and Kumar, 2015 ;  Wang  et al. , 2017 ;  Zozaya-Valdés  et al. , 2017 ). In the field of TB, large-scale omic studies have been conducted to identify new biomarkers that are present in patient samples as response to TB infection and genetic markers that are associated with drug-resistance ( Cui  et al. , 2017 ;  Drouin  et al. , 2016 ;  Ezewudo  et al. , 2018 ;  Groote  et al. , 2017 ;  Walzl  et al. , 2018 ). In contrast, comparative genomics studies identifying MTBC-specific loci have been scarce and based either on limited Mycobacteria genome databases or on selection criteria that does not assure specificity ( Kakhki  et al. , 2019 ;  Zhao  et al. , 2014 ). Furthermore, none of the approaches have analyzed the genetic diversity of the markers using a representative global collection of MTBC strains, a key feature for a universal diagnostic target. Here, we perform a large-scale comparative genomic analysis to provide a catalog of MTBC-specific loci that will be of great utility for the scientific community working on the development of new research and clinical tools for TB. We assess the global diversity of each MTBC-specific gene among a comprehensive dataset of more than 4700 MTBC strains, spanning all known lineages in which MTBC is divided, showing the value of using the genomic data to identify the best targets for diagnostic assays. We found that the main MTBC markers used up to date are also present in other organisms, mainly NTM. As a proof of concept, we develop a qPCR assay capable of quantifying MTBC DNA with 100% specificity. 2 Materials and methods 2.1  In silico  identification of MTBC-specific diagnostic gene markers To identify MTBC-specific loci, we used blastn ( Altschul  et al. , 1990 ) to look for all the genes of the  M.tuberculosis  reference strain H37Rv (NC_000962.3) in the NCBI nucleotide non-redundant database (accessed October 2018) and a custom database comprising 4277 NTM assemblies ( Supplementary Methods S1 ). We filtered the results with a set of stringent parameters that allowed us to provide a diverse catalog of MTBC-specific loci. We focused on the identification of loci having large fractions with no homology outside the MTBC or alignments with low identities, thus enabling the development of highly specific molecular assays minimizing the risk of cross-reaction. We analyzed loci instead of genomic fragments as they are functional units that tend to be more conserved across MTBC strains. We kept loci that aligned with query coverages below 60% and identities lower than 80%. In addition, we also kept genes with query coverages below 25% disregarding the identity of the alignment. In this way, we retained loci containing conserved domains across species but with no global match with genes of other species. Finally, we only considered loci that were present in all the species of the MTBC species genomes in the database. Once we identified the MTBC-specific loci, we assessed their genetic diversity in circulating MTBC strains. To do this, we analyzed the polymorphisms [single nucleotide polymorphisms (SNPs) and indels] observed at each position across a dataset comprising 4766 genomes of MTBC strains ( Chiner-Oms  et al. , 2019 ). In the case of indels, we only considered positions showing an indel in at least 10 strains (0.2% of the database) to avoid the noise introduced by single-strain indels spanning large genic regions. Finally, we looked for available information of these genes in the bibliography, what allowed us to discard some candidates based on their genomic context and provide extended information about their physiology. We gathered transcriptomic and proteomic data derived from different published studies: transcriptomic data in response to overexpression of 206 transcription factors ( Turkarslan  et al. , 2015 ), different genotoxic stresses ( Namouchi  et al. , 2016 ) and response to nitric oxide stress at different time-points ( Cortes  et al. , 2017 ), as well as proteomic data in response to nutrient starvation ( Albrethsen  et al. , 2013 ). 2.2 Set-up of a MTBC-specific qPCR assay for DNA detection and quantification We selected one of the MTBC-specific targets to set up a qPCR assay for the detection and quantification of MTBC DNA. To select the target for the assay, we took into consideration the number of polymorphisms per base, the absence of high-prevalent polymorphisms, the gene length and its genomic context. We designed the primers and probes for the assay using the web tool Primer-BLAST ( Ye  et al. , 2012 ). The qPCR assay consisted on the amplification of a 65 bp region within the Rv2341 gene using the following primers: Forward-GCCGCTCATGCTCCTTGGAT, Reverse-AGGTCGGTTCGCTGGTCTTG, Probe-TGAGTGCCTGCGGCCGCAGCGC. To test the specificity of the assay, we performed qPCR experiments with DNA from different MTBC lineages selected from the MTBC reference dataset described in ( Borrell  et al. , 2019 ), human DNA, a mock sample with mixed DNA from 20 different bacterial species and 15 different species of NTM ( Supplementary Methods S2 and S3 ). All assays were done per triplicate and with an initial DNA concentration of 0.5 ng/ul. In addition, we evaluated the performance of the assay detecting MTBC DNA in a small test set of clinical samples. We used DNA extracted from 12 homogenized sputum samples from confirmed TB patients, two of them with negative smear microscopy (see ethics statement in  Supplementary Material ). The reaction efficiency was calculated with decreasing concentrations of H37Ra DNA. 3 Results 3.1 A catalog of MTBC-specific markers We identified 40 genes to be uniquely present in members of the MTBC according to our filtering parameters ( Fig. 1 ). The median number of SNPs per base (across 4766 MTBC strains) was 0.07, with some of these genes showing either higher or lower diversities (up to 0.10 and 0.04 SNPs/base, respectively). Importantly, although most of the polymorphisms analyzed were strain-specific, we observed high prevalent polymorphisms as well ( Fig. 1 ,  Supplementary File S1 ). For instance, Rv0610c showed a SNP present in 4182 strains and Rv2823c showed an insertion in 4345 strains. Analysis of the phylogenetic distribution of these polymorphisms confirmed that they mapped to deep branches in the phylogeny. For example, the SNP in Rv0610c affected all modern lineages (L2, L3, L4).
 Fig. 1. The 40 MTBC-specific loci. Gene names in red indicate the 10 loci that were discarded as diagnostic markers for being within RD (Rv2274c and Rv2816c-2820c), associated to CRISPR (Rv2816c-2823c) or duplicated in the genome (Rv3424c). Concentric circles represent different genetic diversity metrics. Outer heatmap: number of SNPs per base. Blue circle: prevalence of each SNP of each gene across the database of MTBC strains. Inner, red circle: prevalence of each indel of each gene across the database of MTBC strains. Note that both inner circles have two scales, one from 0 to 300 strains and other from 300 to 4800 strains. The region of the Rv2341 gene amplified in our qPCR assay is indicated in light yellow. RD 182 and 207 are clearly detected in our analysis, indicated as contiguous deleted regions in a high number of strains. Among the initial 40 MTBC-specific genes, 9 were discarded as potential diagnostic markers since they were included in large genomic deletions known as regions of difference (RD) 182 (Rv2274c) and RD 207 (Rv2816c-Rv2820c) ( Gagneux  et al. , 2006 ) or were in variable genomic regions associated with CRISPR elements (Rv2816c-2823c) ( Freidlin  et al. , 2017 ). Another gene, Rv3424c was also discarded as we found it to be duplicated in a labile genomic region, between the transposase of the insertion sequence IS1532 and PPE59. Therefore, the curated list of MTBC-specific diagnostic markers finally consisted of 30 genes ( Fig. 1 ). When looking at published transcriptomic and proteomic data, we found that Rv2003c, Rv2142c and Rv3472 proteins are produced in greater levels (6.19, 3.6 and 100-fold, respectively) when the bacteria is subjected to starvation. Interestingly, Rv2003c is also observed to be overexpressed upon treatment with nitric oxide ( Supplementary File S2 ). 3.2 A specific qPCR assay for MTBC DNA quantification Based on our genomic analysis, we set up a qPCR assay targeting the Rv2341 gene. This gene, described as ‘probable conserved lipoprotein lppQ’ in the Mycobrowser database ( Kapopoulou  et al. , 2011 ), is situated in a stable genomic region, between the asparagine tRNA and the gene of the DNA primase. As shown in  Figure 1 , we were able to design an optimized set of primers avoiding prevalent polymorphisms. The specificity of the assay was 100% since no cross-reaction was observed with non-MTBC samples. Fluorescence was occasionally detected for some non-MTBC samples in cycles beyond Cq 35 ( Fig. 2a ). This only happened for 1/9 replicates of the mock bacterial DNA and 16/135 replicates of NTM. Importantly, no NTM sample amplified consistently between replicates, indicating that fluorescence in late cycles are not due to non-specific amplifications but likely to cross-contamination or qPCR artifacts. As shown, the sensitivity of the assay in our small test set was of 100%, since we were able to detect MTBC DNA in all TB patient sputa, including two confirmed TB cases with a negative smear microscopy ( Supplementary File S3 ). The standard curve using purified H37Ra DNA showed an efficiency of the reaction of 100% (2.01) with a limit of detection of 10fg (at Cq 34.43), hypothetically corresponding to two genome equivalents ( Fig. 2b ). Based on these observations, any result beyond Cq 35 should be considered negative and, therefore, the final setup of our qPCR assay consists in 35 amplification cycles.
 Fig. 2. ( a ) Cq values for MTBC samples of different lineages, sputum samples from confirmed TB patients, and non-MTBC samples. ( b ) Standard curve for the assay with decreasing concentrations of pure H37Ra DNA (from 10 ng/ul to to 1e–5 ng/ul). The reaction efficiency was of 100% (2.01). In both panels (a) and (b) points represent the mean Cq value of the three sample replicates in each of the three replicated experiments 4 Discussion Identification of MTBC markers has been an active area of research over the last decades. It is striking that, for such a relevant disease, for which tons of genomic data are already available, the identification of MTBC-specific genes had been relegated to the background. So far, efforts have been focused on the design of MTBC-specific primers and the optimization of assay conditions based on targets identified even decades ago. Recently, Lei Zhou  et al.  analyzed one loci widely distributed across mycobacteria, the  ku  gene, against a database of more than 7000 genomes and assessed its suitability to identify several mycobacteria species including MTBC ( Zhou  et al. , 2019 ). Other works have developed molecular assays based on targets that are claimed to be MTBC-specific, with some of them also trying to identify MTBC-specific loci by using different strategies, including comparative genomics ( Kakhki  et al. , 2019 ;  Zhao  et al. , 2014 ). However, the targets described until now are far from specific ( Supplementary File S6 ), most likely due to the limited datasets analyzed and permissive selection criteria. Our analysis addresses these limitations by analyzing large genomic datasets and using stringent selection criteria to provide a diverse catalog of highly specific MTBC targets to be used by developers of novel molecular assays for TB. Importantly, compared with previous efforts, we also analyzed the genomic diversity of these targets across thousands of strains from all known MTBC lineages, since the conservation of the targets is a key feature to ensure the reproducibility of the diagnostic tests across clinical settings. Additionally, we found that some of the markers that we identify could be targeted to determine the physiological status of MTBC bacteria under certain conditions. For example, Rv2003c, overexpressed during starvation and upon treatment with nitric oxide ( Albrethsen  et al. , 2013 ;  Cortes  et al. , 2017 ), is also up-regulated during dormancy ( Hegde  et al. , 2012 ). Similarly, Rv1374c contains a small RNA that is highly expressed during exponential growth ( Arnvig  et al. , 2011 ), and hence might be used to evaluate the replicative state of the bacilli. Strikingly, none of the markers considered to be MTBC-specific up to date and included in most diagnostic tests are in our list of unique MTBC genes. For instance, when examining in which species the IS6110 can be found, we observed several non-MTBC organisms, including 14 NTMs, carrying at least 1 copy. The same is true for IS1081 and mpt64, present in 38 and 6 NTM, respectively (Supplementary Files S4 and S6). To illustrate the translational potential of our work, we developed a proof-of-concept qPCR assay capable of quantifying MTBC DNA with 100% specificity and a sensitivity up to 2 genome copies. However, its use as a diagnostic tool would require additional evaluations and optimizations with larger sets of samples to ensure the robustness of the assay and to maximize its sensitivity. It is important to note that the aim of our work was not to compare our assay with current diagnostic tests but to provide novel targets for future ones. Here, we use the genomic data available for a rational, evolutionary-guided, design of a targeted assay for TB. However, this strategy could be used for many other organisms and applications. When possible, the development of next-generation molecular assays should be guided by comprehensive analyses that integrate the data available for each organism. Funding This work was supported by projects of the European Research Council (ERC) (638553-TB-ACCELERATE), Ministerio de Economía y Competitividad and Ministerio de Ciencia, Innovación y Universidades (Spanish Government), SAF2013-43521-R, SAF2016-77346-R and SAF2017–92345–EXP (to I.C.), BES-2014-071066 (to G.A.G.), FPU 13/00913 (to A.C.O.). 
 Conflict of Interest : none declared. Supplementary Material btz729_Supplementary_Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Modelling cancer progression using Mutual Hazard Networks</Title>
    <Doi>10.1093/bioinformatics/btz513</Doi>
    <Authors>Schill Rudolf, Solbrig Stefan, Wettig Tilo, Spang Rainer, Schwartz Russell</Authors>
    <Abstract/>
    <Body>1 Introduction Tumours turn malignant in a Darwinian evolutionary process by accumulating genetic mutations, copy number alterations, changes in DNA methylation, gene expression and protein concentration. Such progression events arise in individual tumour cells, but their effect on the reproductive fitness of this cell depends on earlier events ( Nowell, 1976 ), which makes some chronological sequences of alterations more likely than others. These sequences and their driving dependencies are a priori unknown and inferring them from data is the goal of cancer progression models. These roughly fall into three classes: phylogenetic models, models of population dynamics and cross-sectional models ( Beerenwinkel  et al. , 2015 ;  Schwartz and Schäffer, 2017 ). We focus on the latter. While progression is a dynamic process, available genotype data are cross-sectional and combine static snapshots from different tumours at different stages of development. Nevertheless, assuming that the tumour genomes are observations from the same stochastic process, cancer progression models can infer dependencies between events from their co-occurrence patterns. The dependencies are then reported as a directed graph, where each node stands for an event whose probability depends in some way on the events connected to it by incoming edges ( Fig. 1 ).
 Fig. 1. Overview of several types of cancer progression models. For models with deterministic dependencies (a-e)  A → B  denotes that A is necessary for B, and  A ⊣ B  denotes that A prevents B. For models with stochastic dependencies (f,g)  A → B  denotes that A makes B more likely, and  A ⊣ B  denotes that A makes B less likely. In (d-e) the arrows between groups of events denote that at least one of the events in the parent group is necessary for the events in the child group For example, one family of models [reviewed by  Hainke  et al.  (2012) ] approximate tumour progression by deterministic dependencies: An event has a non-zero probability if and only if all its parent events have occurred. These models were inspired by  Fearon and Vogelstein (1990) , who inferred that colorectal cancers progress along a chain of mutations in the genes  APC → K-RAS → TP53 .  Desper  et al.  (1999)  formalized and extended this concept to Oncogenetic Trees, where a single event may be necessary for multiple successor events in parallel.  Beerenwinkel  et al.  (2007)  further generalized these to Conjunctive Bayesian Networks (CBN), where events may require multiple precursors, thus replacing trees by directed acyclic graphs. In this paper, we relax three assumptions of this model family:
 Dependencies need not be deterministic. An event A can make an event B more likely without being absolutely necessary for it. In particular, events can occur with non-zero probability at all times and all event patterns are possible. A dependency graph need not be acyclic. Why should it? Clearly an event A cannot be necessary for B if B is also necessary for A. But it is certainly possible that A makes B more likely when it occurs first, and vice versa. Besides enabling dependencies, there are also inhibiting dependencies. Although there is, to the best our knowledge, no method that addresses all three issues, there are methods that address one or two of them. Stochastic dependencies (1) have been previously proposed in  Farahani and Lagergren (2013 ),  Misra  et al.  (2014 ) and  Ramazzotti  et al.  (2015)  for acyclic models. Moreover, stochasticity at the point of observation has been addressed by  Gerstung  et al.  (2011)  who allow for mislabeled events, and in  Beerenwinkel  et al.  (2005)  and  Montazeri  et al.  (2016)  who treat tumour data as a mixture from multiple stochastic processes. Network Aberration Models (NAM) by  Hjelm  et al.  (2006)  have stochastic dependencies (1) and allow cycles in their dependency graph (2). Inhibition (3) is at the centre of mutual exclusivity, which is a frequently observed phenomenon in cancer ( Yeang  et al. , 2008 ). Two events are considered mutually exclusive if they co-occur less frequently than expected by chance. There are at least two mechanisms that can cause this data pattern: (i) Synthetic lethality, where cells carrying two mutations A and B are no longer vital. (ii) The events disrupt the same molecular pathway such that whichever event occurs first conveys most of the selective advantage and decreases selective pressure for the others. Both mechanisms can be described by a double edge A  ⊢ ⊣  B (A inhibits B, and B inhibits A) 
 Gerstung  et al.  (2011)  proposed to model pathways within a progression model from biological prior knowledge. They extracted predefined sets of events from databases reflecting biological pathways which are considered affected if at least one of its constituent events has occurred. Tumour progression is then modelled on the level of pathways rather than events. Alternatively pathways can be derived from data by detecting patterns of mutual exclusivity (Constantinescu  et al. , 2015;  Leiserson  et al. , 2013 ;  Miller  et al. , 2011 ;  Szczurek and Beerenwinkel, 2014 ) or by a combination of knowledge and data ( Ciriello  et al. , 2012 ;  Kim  et al. , 2015 ), see ( Vandin, 2017 ) for a review.  Raphael and Vandin (2015)  pointed out that inferring pathways separately from their dependencies can lead to inconsistencies in the presence of noise. They presented the first algorithm that simultaneously groups events into pathways and arranges the pathways in a linear chain. PathTiMEx ( Cristea  et al. , 2017 ) generalizes this from linear chains to acyclic progression networks (CBN). Building on both CBNs and NAMs, we propose Mutual Hazard Networks (MHN). MHNs do not group events into pathways but directly model the mechanisms behind mutual exclusivity. MHNs have cyclic dependency networks, in particular allowing for bidirectional and inhibiting edges. MHN characterize events by a baseline rate and by multiplicative effects they exert on the rates of successive events. These effects can be greater or less than one, i.e. promoting or inhibiting. We provide formulas for the log-likelihood of MHN and its gradient, and an implementation that is computationally tractable for systems with up to 25 events on a standard workstation and for larger systems on an HPC infrastructure. 2 Materials and methods 2.1 Mutual Hazard Networks We model tumour progression as a continuous time Markov process  { X ( t ) , t ≥ 0 }  on all  2 n  combinations of a predefined set of  n  events. Its state space is  S = { 0 , 1 } n , where  X ( t ) i = 1  means that event  i  has occurred in the tumour by age  t , while  X ( t ) i = 0  means that it has not. We assume that every progression trajectory starts at a normal genome  X ( 0 ) = ( 0 , … , 0 ) T , accumulates irreversible events one at a time, and ends at a fully aberrant genome  X ( ∞ ) = ( 1 , … , 1 ) T . Observed tumour genomes correspond to states at unknown intermediate ages  0 &lt; t &lt; ∞  and typically hold both 0 and 1 entries. Let  Q ∈ R 2 n × 2 n  be the transition rate matrix of this process with respect to a basis of  S  in lexicographic order ( Fig. 2 , left). An entry
 (1) Q y , x = lim Δ t → 0 Pr ( X ( t + Δ t ) = y | X ( t ) = x ) Δ t ,   y ≠ x is the rate from state  x ∈ S  to state  y ∈ S , and diagonal elements are defined as  Q x , x = − ∑ y ≠ x Q y , x  so that columns sum to zero.  Q  is lower triangular and has non-zero entries only for transitions between pairs of states  x = ( … , x i − 1 , 0 , x i + 1 , … ) T  and  y = x + i : = ( … , x i − 1 , 1 , x i + 1 , … ) T  that differ in a single entry  i .
 Fig. 2. (Left) Transition rate matrix  Q  for the Markov process  X  with  n  =   4, where  ·  is a zero entry and  •  is a non-zero entry. The states are depicted as squares with four compartments as shown below. A white compartment denotes 0 and a black compartment denotes 1. The matrix is lower triangular because events are irreversible, and sparse because events accumulate one at a time. (Right) Parameterization  Q Θ  of the Markov process by a Mutual Hazard Network Our aim is to learn for each event  i  how its rate  Q x + i , x  depends on already present events in  x  as a function  f i : { 0 , 1 } n → R . A common choice in time-to-event analysis is the proportional hazards model ( Cox, 1972 ) which assumes that binary predictors have independent and multiplicative effects on the rate of the event. We therefore specify the Markov process by a system of  n  functions
 (2) Q x + i , x = f i ( x ) = exp   ( θ ii + ∑ j = 1 n θ ij x j ) = Θ ii ∏ x j = 1 Θ ij   and collect their parameters in a matrix  ( Θ ij ) : = ( e θ ij ) ∈ R n × n . We call Θ a  Mutual Hazard Network (MHN) , where the baseline hazard Θ ii  is the rate of the event  i  before any other events are present and the hazard ratio Θ ij  is the multiplicative effect of event  j  on the rate of event  i  ( Fig. 2 , right). Note that while the baseline hazard in  Cox (1972)  is generally a function of time, here it must be constant so that our model constitutes a Markov process. 2.2 Parameter estimation A dataset  D  of tumours defines an empirical probability distribution on  S . It can be represented by a vector  p D  of size  2 n , where an entry  ( p D ) x  is the relative frequency of observed tumours with state  x  in  D . At  t  =   0 tumours are free of any events, so the Markov process  X  starts with the initial distribution  p ∅ : = ( 100 % , 0 % , … , 0 % ) T , which then evolves according to the parameterized rate matrix  Q Θ . If all tumours had been observed at a common age  t ,  p D  could be modelled as a sample from the transient distribution
 (3) e t Q Θ p ∅ . Since the tumour age is usually unknown, we follow  Gerstung  et al.  (2009)  and consider  t  to be an exponential random variable with mean 1. Marginalizing over  t  yields
 (4) p Θ = ∫ 0 ∞ d t e − t e t Q Θ p ∅ = [ I − Q Θ ] ︸ = : R Θ − 1 p ∅ , and the marginal log-likelihood score of Θ given  D  is
 (5) S D ( Θ ) = p D T   log   p Θ = p D T   log ( R Θ − 1 p ∅ ) , where the logarithm of a vector is taken component-wise. When optimizing  S D  with respect to Θ we further add an L1 penalty in order to control for model complexity and to avoid overfitting. This promotes sparsity of the networks, such that many events do not interact and off-diagonal entries Θ ij  are exactly 1:
 (6) S D ( Θ ) − λ ∑ i ≠ j |   log   Θ ij | , where  λ  is a tuning parameter. We will optimize this expression using the Orthant-Wise Limited-Memory Quasi-Newton algorithm ( Andrew and Gao, 2007 ). This general-purpose optimizer handles the non-differentiable regularization term by approximating the objective at each iteration with a quadratic function that is valid within an orthant of the current set of (logarithmic) parameters. It requires only a closed form for the derivatives  ∂ S D / ∂ Θ ij  with respect to each parameter. From the chain rule of matrix calculus we have
 (7) ∂ S D ∂ Θ ij = ∂ S D ∂ R Θ − 1 · ∂ R Θ − 1 ∂ Θ ij = p D p Θ p ∅ T · ( − R Θ − 1 ∂ R Θ ∂ Θ ij R Θ − 1 ) = − ( p D p Θ ) T R Θ − 1 ∂ R Θ ∂ Θ ij R Θ − 1 p ∅ , where  ·  is the Frobenius product and the ratio  p D / p Θ  is computed component-wise. In general this optimization converges to one local optimum out of possibly several depending on the starting point. In this paper we always report the optimum reached from an independence model, where the baseline hazard Θ ii  of each event was set to its empirical odds and the hazard ratios were set to exactly 1. 2.3 Efficient implementation To compute the score in  equation (5)  and its gradient in  equation (7)  we must solve the exponentially sized linear systems  [ I − Q Θ ] − 1 p ∅  and  ( p D / p Θ ) T [ I − Q Θ ] − 1 . To this end, we employ the (left) Kronecker product which is defined for matrices  A ∈ R k × l  and  B ∈ R p × q  as the block matrix
 (8) A ⊗ B = [ b 11 A ⋯ b 1 l A ⋮ ⋱ ⋮ b k 1 A ⋯ b kl A ] ∈ R kp × lq . We follow the literature on structured analysis of large Markov chains ( Amoia  et al. , 1981 ;  Buchholz, 1999 ) and write the transition rate matrix  Q Θ  as a sum of  n  such Kronecker products,
 (9) Q Θ = ∑ i = 1 n [ ⊗ j &lt; i ( 1 0 0 Θ ij ) ⊗ ( − Θ ii 0   Θ ii 0 ) ⊗ ⊗ j &gt; i ( 1 0 0 Θ ij ) ] . Here, the  i th term in the sum is a sparse  2 n × 2 n  matrix consisting of all transitions that introduce event  i  to the genome. It corresponds to a single subdiagonal of  Q Θ , together with a negative copy on the diagonal to ensure that columns sum to zero (see Supplementary Section S2). The benefit of this compact representation is that matrix-vector products can be computed in  O ( n 2 n − 1 )  rather than  O ( 2 2 n )  without holding the matrix explicitly in memory ( Buis and Dyksen, 1996 ). We split  R Θ = I − Q Θ  into a diagonal and strictly lower triangular part,
 (10) R Θ = D + L = D ( I + D − 1 L ) , and use the nilpotency of  D − 1 L  to compute
 (11) R Θ − 1 p ∅ = ( I + D − 1 L ) − 1 D − 1 p ∅ = ( ∑ k = 0 n − 1 ( − D − 1 L ) k ) D − 1 p ∅ . 3 Results 3.1 Simulations We tested in simulation experiments how well an MHN of a given size can learn a probability distribution on  S  when trained on a given amount of data. We ran 50 simulations for each of several sample sizes  | D | ∈ { 100 , 250 , 500 , 1000 }  and number of events  n ∈ { 10 , 15 , 20 } . In each simulation run, we chose a ground truth model Θ with  n  possible events. A random half of its off-diagonal entries were set to 1 (no dependency) and the remaining off-diagonal entries were drawn from a standard log-normal distribution. For the diagonal entries we chose the largest  n  out of 20 rates drawn from a log-normal distribution with mean  μ = − 2  and variance  σ 2 = 1 . This was done to mimick the event frequencies observed in the biological datasets in Section 3.2 and yielded on average 2.7, 3.5 and 3.9 realized events in tumours with  n ∈ { 10 , 15 , 20 }  possible events. We then generated a dataset of size  | D |  from each model and trained on it another model  Θ ^  by optimizing expression (6). We chose a common regularization parameter for all 50 simulation runs, which we found to be roughly  λ = 1 / | D |  through validation on separate datasets of each sample size. We then assessed the reconstructed model  Θ ^  by the Kullback-Leibler (KL) divergence from its probability distribution to the distribution of the true model Θ,
 (12) D KL ( p Θ ‖ p Θ ^ ) = p Θ T   log   p Θ − p Θ T   log   p Θ ^ . The median KL divergence, as well as its variance over the 50 simulation runs, improved with larger training datasets and reached almost zero ( Fig. 3 ).
 Fig. 3. KL divergence from estimated MHNs to ground truth MHNs over 50 simulations for the shown sample sizes and number of events Next, we simulated datasets of size  | D | = 500  from random MHNs and CBNs as ground truth models with  n  =   8 events. We added noise by flipping each event independently with probability  ϵ , trained MHNs and CBNs on both datasets and evaluated how well the estimated models fit the distribution of the ground truth models. ( Fig. 4 ) shows the average KL divergence over 5 simulation runs for each noise level  ϵ ∈ { 1 % , 5 % , 10 % , 15 % , 20 % } . For CBNs as ground truth we found that CBNs outperformed MHNs when noise was below 10%, while MHNs performed better than CBNs at higher levels of noise. For MHNs as ground truth we found that MHNs performed better than CBNs at all levels of noise.
 Fig. 4. Average KL divergence from estimated MHNs/CBNs to ground truth MHNs/CBNs over 5 simulations for each of the shown noise levels Lastly, we tested the performance of our implementation. The runtime of a single gradient step for random and dense Θ was about 1 min for  n  =   20 on a standard workstation and scaled exponentially with  n  as expected (see  Supplementary Section S2  for details). 3.2 Application to cancer progression data 
 3.2.1  Comparison to conjunctive Bayesian Networks We tested our method and first compared it to Conjunctive Bayesian Networks (CBN) on three cancer datasets that were previously used by  Gerstung  et al.  (2009) . They were obtained from the Progenetix molecular-cytogenetic database ( Baudis and Cleary, 2001 ) and consist of 817 breast cancers, 570 colorectal cancers, and 251 renal cell carcinomas. The cancers are characterized by 10, 11 and 12 recurrent copy number alterations, respectively, which were detected by comparative genomic hybridization (CGH). On average 3.3, 3.5 and 2.6 of these possible events were observed in individual tumours within each dataset. We trained MHNs on all three datasets (see  Supplementary Section S3 ) and compared them to the CBNs given in  Gerstung  et al.  (2009)  which provide log-likelihood scores in-sample. The in-sample scores of MHNs are not directly comparable because MHNs have more degrees of freedom than CBNs. Therefore we additionally provide the average log-likelihood scores of MHNs in 5-fold cross-validation and the Akaike Information Criterion ( Akaike, 1974 ) (AIC) for both models. MHN compared favourably on all three datasets ( Table 1 ).
 Table 1. MHNs compare favourably to CBNs on three datasets in terms of the log-likelihood scores per tumour, averaged over 5 folds in cross-validation Cross-validated In-sample AIC Dataset MHN CBN MHN CBN MHN Breast cancer −5.63 −5.73 −5.54 
 ≥ 9373 9152 Colorectal cancer −5.64 −5.79 −5.41 
 ≥ 6612 6288 Renal cell carcinoma −5.02 −5.13 −4.81 
 ≥ 2587 2559 
 Note : They also compare favourably in terms of the AIC which penalizes the number of parameters in a model and is weighted by the sample size. While MHNs have  n 2  continuous parameters, CBNs have  n  continuous parameters and a discrete graph structure that is hard to quantify in terms of degrees of freedom, hence we ignore the latter and bound the AIC of CBNs from below. 
 3.2.2  Comparison to pathTiMEx Next, we compared MHN to pathTiMEx on a glioblastoma dataset from The Cancer Genome Atlas ( Cerami  et al. , 2012 ) which was previously used in  Cristea  et al.  (2017)  (see  Fig. 5 ). The data consist of  | D | = 261  tumours characterized by 486 point mutations (M), amplifications (A), or deletions (D). We focus on  n  =   20 of these events which were pre-selected by pathTiMEx using the TiMEx algorithm ( Constantinescu  et al. , 2016 ). On average 3.3 of these possible events were observed in individual tumours.
 Fig. 5. ( A ) Glioblastoma dataset from TCGA, where rows show events sorted by frequency and columns show tumours sorted lexicographically. The purple stripes highlight tumours which have IDH1(M) but lack TP53(M). ( B ) PathTiMEx model inferred in  Cristea  et al.  (2017) . It simultaneously divides the dataset into pathways, i.e. into mutually exclusive groups of events and learns a CBN of these pathways. The CBN considers a pathway altered if at least one of its constituent events has occurred. A pathway alteration fixates at the rate given in the upper right-hand corner once all its parent pathways in the CBN have been altered. ( C ) Mutual Hazard Network, where nodes show the base rates Θ ii  and edges show the multiplicative interactions Θ ij . Similarities to pathTiMEx are highlighted in colour and roughly correspond to the signaling pathways Rb, p53 and PI(3)K (red, blue and green). ( D ) Highlighted data interpreted differently by the models. While both models explain the anti-correlation between  CDKN2A(D)  and  CDK4(A)  by mutual inhibition, pathTiMEx treats these as a group and infers a positive effect of any of these events on  MDM2(A)  from the correlation between  CDK4(A)  and  MDM2(A) . MHN reports only a positive effect of  MDM2(A)  on  CDK4(A)  and in fact infers inhibition between  CDKN2A(D)  and  MDM2(A)  from their anti-correlation in the data We trained MHN as above for 100 iterations, which achieved a log-likelihood score of -7.70 in-sample and a score of -7.97 in 5-fold cross-validation. While pathTiMEx does not yield a directly comparable log-likelihood score, it quantifies discrepancies between model and data by considering the data to be corrupted by noise, each event in a tumour being independently flipped with probability  ε . PathTiMEx estimated this noise parameter as  ε ^ = 20 % , from which we gauge an upper bound on its log-likelihood score as follows: even a hypothetical model that learns the data distribution  p D  perfectly but assumes a level of noise
 (13) p ε ^ = ⊗ i = 1 n ( 1 − ε ^ ε ^ ε ^ 1 − ε ^ ) p D achieves only a score of  p D T   log   p ε ^ = − 8.50  in-sample, which is less than the cross-validated score of MHN. Nevertheless MHN largely agreed with pathTiMEx on the inhibitions implied by the three most mutually exclusive groups of events, which broadly correspond to the signaling pathways Rb, p53 and PI(3)K (red, blue and green in  Fig. 5 ) and are well known to be affected in glioblastoma ( McLendon  et al. , 2008 ). The RB1 signaling pathway (red) regulates cell cycle progression and involves the genes  CDKN2A ,  CDK4  and  RB1 .  CDKN2A  codes for the tumour suppressor protein p16 INK4a  which binds to CDK4 and prevents it from phosphorylizing RB1, thereby blocking cell cycle transition from G1 to S-phase. This function can be disrupted by deletion of  CDKN2A  or  RB1 , or by amplification of  CDK4 . MHN and pathTiMEx both report a corresponding inhibition between the events  CDKN2A(D)  and  CDK4(A) , while MHN additionally reports inhibition between  CDKN2A(D)  and  RB1(D) . The p53 signaling pathway (blue) induces apoptosis in response to stress signals and involves the genes  TP53 ,  MDM2 ,  MDM4  and  CDKN2A .  TP53  codes for the tumour suppressor protein p53 which is antagonized by MDM2 and MDM4 in a non-redundant manner ( Toledo and Wahl, 2007 ). PathTiMEx identifies the events  TP53(M) ,  MDM2(A) ,  MDM4(A)  as mutually exclusive ways to evade apoptosis, while MHN reports inhibition only between  TP53(M)  and each of  MDM2(A)  and  MDM4(A)  separately. This may reflect non-diminishing returns due to their complementary roles in the pathway. The gene  CDKN2A  is, in addition to its role in the RB1 pathway, also involved in the p53 pathway by coding for the protein p14 ARF  in an alternate reading frame. p14 ARF  physiologically inhibits MDM2, which suggests that a deletion of  CDKN2A  may be functionally similar to an amplification of  MDM2 . While MHN reports a corresponding inhibition between  CDKN2A(D)  and  MDM2(A) , pathTiMEx cannot because this would lead to an overlap of pathways. To the contrary, pathTiMEx implies that  CDKN2A(D)  promotes  MDM2(A)  despite their anti-correlation in the data ( Fig. 6D ). We argue that this is an artifact driven by the assumption that all events in a group are interchangeable, and by the need to group  CDKN2A(D)  with  CDK4(A)  which is in turn highly correlated with  MDM2(A) .
 Fig. 6. Most likely chronological order of events for all 261 tumours in the glioblastoma dataset. Each of the 193 distinct tumour states corresponds to a terminal node in the tree or an internal node with a black outline and contains all events indicated by the symbols along the trajectory from the starting state (white circle in the centre). The order of events and the unobserved intermediate tumour states (internal nodes without a black outline) were imputed from the estimated transition rate matrix  Q Θ ^ . To this end we used the uniformization method ( Grassmann, 1977 ) to construct the time-discretized transition probability matrix  ( I + Q Θ ^ / γ ) , where  γ  is the greatest absolute diagonal entry of  Q Θ ^ . The most likely trajectories from the starting state to each observed tumour state were then computed using a single-source shortest path algorithm, where each transition was weighted by the negated logarithm of its probability The PI(3)K pathway (green) regulates cell proliferation and involves the genes  PTEN ,  PIK3CA ,  EGFR ,  PDGFRA . While  IDH1  is not a canonical member of the PI(3)K pathway, MHN reports an inhibition between  IDH1(M)  and  PTEN(M)  and pathTiMEx groups  IDH1(M)  together with  PTEN(M) ,  PTEN(D)  and  PIK3CA(M) . Notably, MHN inferred that the rare event  IDH1(M)  promotes the more common event  TP53(M) . This is further illustrated in  Figure 6  which shows the most likely chronological order of events for all 261 tumours. Each of their 193 distinct states is represented by a path that starts at the root node and terminates at either a leaf node or an internal node with a black outline. As can be seen in the lower left, all tumours that contain  IDH1(M)  are located on a common branch and thus share an early mutation history initiated by  IDH1(M) . This interpretation is in line with the fact that  IDH1(M)  is considered a defining attribute of the Proneural subtype of gliobastoma which is clinically distinct and also associated with  TP53(M)  ( Verhaak  et al. , 2010 ). It is further supported by independent data from consecutive biopsies of gliomas where  IDH1(M)  in fact preceded  TP53(M)  ( Watanabe  et al. , 2009 ). 4 Discussion We presented Mutual Hazard Networks, a new framework for modelling tumour progression from cross-sectional observations. MHN are an extension of Conjunctive Bayesian Networks ( Beerenwinkel  et al. , 2007 ): The multiplicative dependencies between rates [ Equation (2) ] approximate the conjunctive dependencies of CBNs in the limit of a vanishing baseline hazard (see  Supplementary Section S1 ). MHN are also an extension of Network Aberration Models ( Hjelm  et al. , 2006 ): NAM also use multiplicative dependencies of rates but restrict hazard ratios to be greater than one, which cannot generate patterns of mutual exclusivity. Moreover, MHN further develop the idea of  Raphael and Vandin (2015)  and  Cristea  et al.  (2017)  that grouping events into ‘pathways’ of mutual exclusive events cannot be done independently from dependencies between pathways. In fact, MHN give up the concept of grouping events entirely. It is not needed anymore, because patterns of mutually exclusive events can be naturally formed by pairwise bidirectional inhibitions or longer inhibiting cycles [such as between  TP53(M) ,  MDM2(A)  and  CDKN2A(D) ], both allowing for overlapping pathways. Such overlap exists. The screening approach of  Leiserson  et al.  (2015)  finds overlapping gene sets forming patterns of mutual exclusivity, nicely reflecting the fact that several cancer genes participate in multiple pathways ( McLendon  et al. , 2008 ). However, this approach does not yet embed overlapping pathways into progression models while MHNs do just this. Still, MHN shares some limitations with earlier models.  Kuipers  et al.  (2017)  have shown that back mutations, which MHNs cannot account for, do occur in tumour progression, although not frequently. Our proposed implementation of the MHN learning algorithm has a space and time complexity that is exponential in the number of events  n , which compares similarly to  Hjelm  et al.  (2006)  and trails ( Montazeri  et al. , 2016 ). Therefore events have to be pre-selected by recurrency or more sophisticated approaches ( Hainke  et al. , 2017 ). In practice, we saw limits at  n  =   25 on a standard workstation. Modern cancer datasets report hundreds of recurrent mutations, and the question arises whether MHN can deal with them. In fact we believe that MHN is competitive with other algorithms also for these large datasets, because interactions between low-frequency events cannot be resolved reliably at all. For example, in the glioblastoma dataset, the rare events  OBSCN(M), CNTNAP2(M), LRP2(M), TP53(D)  and  PAOX(M)  remained unconnected to the rest of the network. In other words, the evidence for possible interactions was so low that it could not compensate for the L1-costs of an additional edge. These are limitations in the data itself and not in computation times. An interesting feature of MHN are the spontaneous occurrence/fixation rates Θ ii . The event pair  IDH1(M)  and  TP53(M)  was instructive for understanding their role.  IDH1  mutations were infrequent in the glioblastomas compared to  TP53  mutations. Moreover, 10 out of 14  IDH1(M)  positive glioblastoma also showed a  TP53  mutation. We see at least two alternative explanations for this noisy subset pattern: (1)  TP53  mutations are needed for  IDH1  mutations to occur. (2)  TP53(M)  has a much higher spontaneous rate than  IDH1(M)  explaining that it is more frequent, and moreover, an  IDH1  mutation strongly increases the rate of a  TP53  mutation, explaining why so many  IDH1(M)  positive glioblastoma were also positive for  TP53(M) . While both scenarios explain the noisy subset pattern, they disagree with respect to the chronological order of events. In (1) the  TP53  mutation precedes the  IDH1  mutation, while in (2) the events occur in reverse order. MHN decided for explanation (2) and is endorsed by independent data from consecutive biopsies ( Watanabe  et al. , 2009 ). Where in the training data was the evidence in favour of (2) over (1)? If  TP53(M)  were necessary for  IDH1(M) , we would expect  IDH1(M)  to have a very small spontaneous rate in the absence of  TP53(M)  and hence to occur later than other events, if at all. Yet, of the four cases that were  IDH1(M)  positive and  TP53(M)  negative, all of them had at most one mutation in addition to  IDH1(M)  ( Fig. 5A , purple), which is in line with (2) but not with (1). In summary, we introduced a new, very flexible framework for tumour progression modelling that naturally accounts for cyclic interactions between events. Supplementary Material btz513_Supplementary_Material Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>
Dot2dot: accurate whole-genome tandem repeats discovery</Title>
    <Doi>10.1093/bioinformatics/bty747</Doi>
    <Authors>Genovese Loredana M, Mosca Marco M, Pellegrini Marco, Geraci Filippo, Hancock John</Authors>
    <Abstract/>
    <Body>1 Introduction A tandem repeat (TR) consists in a certain number of copies of a (typically small) motif sequence that occur adjacent to each other. More realistic definitions admit a certain degree of heterogeneity among copies of the motif sequence as well as tiny insertions or deletions. The abundance of these structures in eukaryotic DNAs has been observed since the first sequencing data became available in the early 90s ( Bacolla  et al. , 2008 ). Although TRs functional role is not completely understood yet, their distribution in eukaryotic genomes suggests involvement in several cellular processes including gene expression ( Tóth  et al. , 2000 ). Besides confirming this thesis, the steady growth of the number of genetic disorders related to the expansion of TRs has kindled the hope of associating TRs polymorphism with the etiology of those genetic diseases that are still unexplained. This trend led the bioinformatics community to focus on research projects aimed at a large-scale analysis of repetitions. Unfortunately, validating a new relevant TR can be as difficult as finding a needle in the haystack and the success of these projects heavily depends on the sensitivity of the searching algorithms. The difficulty of capturing the variability of satellites and microsatellites into a single comprehensive computational model has encouraged researchers to design new methods for large-scale TR discovery. Nevertheless, an agreed computational model for non-naive biologically relevant TRs is still far away. According to different searching strategies and definitions of TRs, several algorithm families have been proposed. Dictionary-based methods ( Castelo  et al. , 2002 ;  Delgrange and Rivals, 2004 ;  Karaca  et al. , 2005 ;  Parisi  et al. , 2003 ;  Pokrzywa and Polanski, 2010 ) leverage on a pre-determined set of seeds that is searched along the input sequence and subsequently expanded. This class of algorithms is particularly efficient in those situations where the user is interested in a relatively small class of repeats. Exhaustive search of TRs, instead, is unaffordable because of the exponential growth of the dictionary size as a function of the allowed motif length. The dual approach is that of  ab-initio  methods ( Benson, 1999 ;  Boeva  et al. , 2006 ;  Girgis and Sheetlin, 2013 ;  Kofler  et al. , 2007 ;  Kolpakov  et al. , 2003 ;  Krishnan and Tang, 2004 ;  Kurtz  et al. , 2001 ;  Pellegrini  et al. , 2010 ;  Sokol  et al. , 2007 ;  Wexler  et al. , 2005 ;  Wirawan  et al. , 2010 ;  Zhou  et al. , 2009 ) that do not require any pre-existing knowledge about the input sequence. Although more complex, this class of algorithms is the most studied and used in practice. The presence of variations (single nucleotide polymorphisms, insertions and deletions) in the genomes has directed many researchers to develop algorithms in which the distance between units of a motif is based on the Needleman–Wunsch sequence alignment algorithm described in  Needleman and Wunsch (1970) . The high quadratic cost of this procedure, however, has convinced researchers to shift to algorithms that work in Hamming distance. Hybrid approaches use Hamming distance instead of sequence alignment but allow insertions between consecutive copies of the motif. Output filtering is a desirable but not mandatory feature of TR searching methods. Its main advantage is the elimination/reduction of redundancy often caused by software artifacts. Aggressive filtering, however, can cause the removal of relevant results and, as a consequence, the reduction of the algorithm accuracy. Two distinguishing features have recently gained importance: the ability of managing multi-sequence files coming from NGS sequencing and the possibility of scanning entire assembled genomes. Both these features can involve a potentially huge amount of data and thus require fast algorithms that avoid computationally expensive operations without sacrificing output quality. In this paper, we present  Dot2dot , a novel algorithm for TR discovery. Our method borrows some ideas from a widely used tool to visually display local alignments between pairs of sequences, namely  dot-plots . In particular, we observed that aligning a sequence with itself, TRs form a regular pattern. Our algorithm mimics such visual search for these patterns to accomplish TR discovery. One of the main novelties of our approach is a compact representation of the dot-plot matrices that: (i) allows us to scale at genome-wide analysis, and (ii) can find application to other problems where dot-plots are used. Our algorithm belongs to the class of  ab-initio  methods, it allows both a tunable degree of divergence from the consensus sequence and a small insertion between two consecutive motifs. Both fasta and fastq are accepted as input allowing the analysis of NGS sequences.  Dot2dot  implements an optional customizable filter able to: remove biologically irrelevant results, and control the degree of overlap among TRs in the output list. Under the sensible assumption that the longest TR in the input sequence is much shorter (by orders of magnitude) than the entire sequence, our algorithm runs in linear time, thus enabling the analysis at whole-genome scale. Besides designing a new searching algorithm, we built five testing datasets covering diverse applicative areas. In particular: we collected from several public sources a set of 45 validated pathology-linked TRs; we compiled the list of coordinates of the CODIS loci including the seven loci that have been added since January 2017; we mapped on the hg38 reference genome a set of 620 manually annotated TRs reported in the Marshfield panel of variable loci; and we computed a catalog of 15 326 TRs located in upstream regulatory regions. 2 Materials and methods Our algorithm leverages on a data structure at the base of dot-plots. Dot-plots are used to gain a visual insight of local alignments between two different sequences or even a sequence against itself. Matches between two elements are represented as (typically black) spots. A natural extension of this visual representation of alignments allows the use of color graduation to represent degree of similarity between pairs of elements ( Sonnhammer and Durbin, 1995 ). The underlying data structure (called dot matrix) is a matrix where the element  M [ i ,  j ] stores the degree of similarity between the character in position  i  of the first sequence and the element in position  j  of the other string. When a sequence  s  is aligned with itself,  M [ i ,  j ] stores the degree of similarity between the element in position  i  and that in position  j  of  s . We observed that TRs form a distinctive pattern on self-sequence alignment dot-plots and, in turn, this pattern reflects on the underlying dot matrix. Consider e.g. a pure TR, since each instance of the motif perfectly aligns with the first instance, it will form a diagonal on the dot-plot. All these diagonals will lie stacked over the main diagonal. Counting the number of stacked diagonals we compute the number of copies of the consensus sequence, while from the length of the diagonals we derive the motif length. Fuzziness of TRs can easily be captured within this model. In fact, mismatches correspond to gaps in the diagonals, deletions cause interrupted diagonals and insertions cause the shift of the remaining part of the TR (see examples in the  Supplementary Material ). We noticed that the presence in the dot matrix of the above described pattern is a necessary and sufficient condition for the existence of a tandemly repeated sequence in the input, thus an algorithm that locates all and only these patterns ensures a computationally correct and complete tool for TRs discovery. The naive quadratic cost of building, storing and searching the dot matrix is inadequate for whole-genome analysis. In order to lower the memory consumption and speed up the computation we propose an alternative representation of this data structure that can be built and stored in linear time/space. We also propose a fast searching heuristic algorithm to enumerate all the instances of the TR pattern in the dot matrix. 2.1 Data structure In this section we provide details on how to infer the dot matrix  M  without explicitly building it. Let  S = s 1 s 2 … s n  be a sequence of length  n  over a finite alphabet Σ (where  Σ = { A , C , G , T }  in our case). Given a character  x ∈ Σ  we define  P ( x ) as the set of positions of  S  where  s i  =  x . The comparison of the character  x  and the sequence  S  induces a row  M [ x ] of  M  whose content does not depend on the position of  x  in the sequence but only on the content of  S . As a consequence, comparing the sequence  S  with itself, the resulting matrix  M  has  | P ( x ) |  rows identical to  M [ x ]. Following the above observation, building the whole  M  it suffices to compute only the vector  M [ x ] for each  x ∈ Σ . In order to obtain a direct positional access to the induced dot matrix we build an auxiliary vector  V  of length  n  where in position  i  we store a reference to  M [ s i ] .  Figure 1  shows an example data structure.
 Fig. 1. Sample data structure for the matrix associated to the sequence TTACGACGTACGATGACGACGT Since  | Σ |  is a constant much smaller than  n , building the data structure inducing  M  has time/space cost linearly proportional to  n . In fact, both  V  and the  M [ x ] s  are vectors of size  n  that can be filled with a single scan of the sequence  S . 2.2 TR searching procedure Searching is a relatively simple procedure that scrolls a window of fixed length along the sequence and, for each position, checks for a stacked diagonal in the matrix  M . Let  D M ( i , j , l ) = { V [ i ] [ j ] , V [ i + 1 ] [ j + 1 ] , … , V [ i + l − 1 ] [ j + l − 1 ] }  be a  l -long sequence of adjacent cells along  M  starting from position ( i ,  j ) such that two consecutive elements have coordinates that differ by 1 in terms of both rows and columns and let  W M ( i , j , l ) = ∑ k = 0 l − 1 V [ i + k ] [ j + k ]  be the summation of the scores of  M  over the positions  D M ( i , j , l ) .  D M ( i , j , l )  is a diagonal if and only if  W M ( i , j , l ) &gt; l − δ  where  δ ∈ [ 0 , l ]  is a user-defined threshold. Given a certain position  i  on the sequence  S , the event  W M ( i , i + l , l ) = l  corresponds to a pure TR of motif length  l  and copy number at least 2 starting from position  i . Again  W M ( i , i + l , l ) = W M ( i , i + 2 l , l ) = l  corresponds to a pure TR with copy number at least =3 and so on. The parameter  δ  is used to control the degree of fuzziness of TRs. In fact, the event  W M ( i , i + l , l ) = l − δ  means that the second copy of the repeat contains  δ  mismatches. In its simplest form, the core of our searching procedure (depicted in Algorithm 1) scans the sequence  S  (see line 1) and checks for different values of  l  (see line 2) if the event  W M ( i , i + l , l ) ≥ l − δ  is verified. When this happens, the algorithm iteratively attempts to locate further diagonals in positions  i + 2 l , i + 3 l  and so on (see lines 7–10). The procedure stops when an integer  c  such that  W M ( i , i + c l , l ) &lt; l − δ  is found. The sequence  S [ i , i + c l ]  is reported as a TR with motif length  l  and copy number  c . We further refined our procedure to deal with insertions and deletions. We restrict to the most significant class of these variants. In particular, insertions can occur only between two copies of the motif sequence and their length is limited to be lower than the motif length  l  (see line 12). In addition, the length of insertions is fixed within the same TR. This means that, if a TR contains two or more insertions, they must have the same length to be correctly detected. Deletions are modeled as the insertion of a spurious sequence between two copies of the motif string. Although this model can appear limited, it has practical advantages and it is consistent with the replication slippage process described in  Viguera  et al.  (2001) . In fact, without a limit on the length of an insertion nearly every genomic sequence can be confused with a TR. The constraint on the equality of the insertion lengths within the same TR helps to predict the correct motif length and copy number when dealing with impure TRs. From the biological point of view, according to the model described in  Viguera  et al.  (2001) , the polymerase is arrested after replicating a unit of the repeat. Then, the realignment between the new strand and the template causes the insertion/deletion to happen between two copies of the TR motif sequence. In presence of an insertion between two copies of the consensus motif the condition  W M ( i , i + c l , l ) &gt; = l − δ  becomes false for a certain  c . In this case (see lines 12–16), we seek for a gap checking for a possible diagonal in at most the next  l − 1  positions (i.e.  i + c l + 1 ,   i + c l + 2 , etc.). In case of success we take note of the gap and its length and continue the standard searching procedure checking the next diagonal at distance  l . When the condition on  W ( )  becomes false again we do not test all the possible sizes of the gap but only the previously annotated length. In order to enable  Dot2dot  to identify the longest possible TR, the searching procedure still has to check whether the sequence immediately downstream a TR is a prefix of the consensus sequence, even though it does not satisfy the condition  W M ( ) &lt; l − δ . In this case, however, we seek only for perfect matching so as not to reduce TR’s purity. We perform this test iteratively verifying (see lines 22–25) that the  i -th character in the downstream sequence matches the  i -th character of the motif (i.e. the first instance of the TR). If a suitable prefix is found, it is included in the output TR. 
 Algorithm 1 Tandem repeat searching procedure 
 Require:  sequence  S , dot matrix  M , parameter  δ 
 Ensure:  list  R  of tandem repeats 1:  for all i ∈ [ 1 , | S | ] do 2:   for all l ∈ r a n g e _ o f _ m o t i f _ l e n g t h s do 3:    c n ← 1 ; 4:    g a p _ o f f s e t ← g a p ← 0 ; 5:    repeat 6:    c a n _ e x t e n d ← false ; 7:    while W M ( i ; i + g a p _ o f f s e t + ( c n * l ) ; l ) &lt; δ − l do 8:     c n ← c n + 1 ; 9:     c a n _ e x t e n d ← true ; 10:    end while 11:    if gap  = 0  then  try to guess gap length 12:     for k ← 1 ; g a p = = 0 and k &lt; l ; k ← k + 1 do 13:      if W M ( i ; i + g a p _ o f f s e t + k + ( c n * l ) ; l ) &lt; δ − l then 14:      g a p ← k ; 15:      end if 16:     end for 17:   end if 18:   g a p _ o f f s e t ← g a p _ o f f s e t + g a p ; 19:  until c a n _ e x t e n d  is  true ; 20:   if c n ≥ 2 then  Found a valid TR 21:    g a p _ o f f s e t ← g a p _ o f f s e t − g a p ; 22:    l a s t ← 0 ; 23:    while last  &lt;  l and S [ i + l a s t ] = S [ i + g a p _ o f f s e t + ( c n * l ) + l a s t ] do 24:    l a s t ← l a s t + 1 ; 25:   end while 26: R.append ( S [ i , i + g a p _ o f f s e t + ( c n * l ) + l a s t ] ,copy_number =  cn , motif_length =  l ); 27:  end if 28:  end for 29: end for 
 In terms of asymptotic analysis, the overall computational cost of  Dot2dot  is proportional to the number of times the condition  W ( ) &gt; l − δ  is tested. A single computation of  W ( )  takes  O ( l ) time since it costs  l  accesses to the matrix  M . Given a certain position  p ∈ [ 1 , | S | − 2 l ]  of  S , there are two cases: either there exists in  S  a TR with  k p  copies starting in position  p  or not. In the first case  W ( )  is computed  k p + l  times while in the latter case  W ( )  is computed only once. In general, however,  l  is constant, but  k p  cannot be bounded and, thus, it can hold  k = m a x p ∈ [ 1 , | S | − 2 l ] ( k p ) = | S | / l  in the worst case. Moreover, a similar condition can hold for every position  p . As a result, the overall running time of the searching procedure would be  | S | l k = O ( | S | k ) = O ( | S | 2 )  in the worst case. In terms of average-case analysis, we have to estimate the value  k ^ ≤ k  that balances the high cost paid every time a new TR is found and the low cost paid otherwise. Dealing with real genomic sequences we observed that the probability for a given position to be the starting point of a TR is fairly low. Moreover, the longest TR is order of magnitude shorter than the input sequence. These assumptions would lead to an expected linear running time of our algorithm. To confirm our hypotheses we estimated the value of  k ^  over the entire hg38 reference genome. According to our experiments we measured  k  =   300 and  lk  = 2495. Moreover, the probability of a random position  p  to be the starting coordinate of a TR =0.0051. Consequently, the expected value of  k ^  is 1.53. 2.3 Filtering Exhaustive approaches to TR discovery suffer from the fact that many reported results may be artifacts rather than proper TRs. For example, finding a TR of copy number  c , this class of algorithms returns also all the sub-instances of copy number  c  – 1,  c  – 2, etc. Since this behavior has also a strongly negative impact on running time, it would be better avoiding computing these artifacts instead of filtering them a posteriori. In order to solve this problem, we maintain a bit vector to mask those positions that will certainly produce such an undesired result. Once our searching procedure identifies a new TR, it sets the bit corresponding to the first position of each copy of the motif sequence. Marked positions are ignored during the subsequent searching. We notice that this filter applies not only to pure TRs, but its benefits partially extend to fuzzy TRs. Suppose e.g. that we want to find TRs within the toy sequence ACG AGG ACG ACG ACG AGT AGT and let suppose  δ  = 1. In this case,  Dot2dot  would return the TR ACG AGG ACG ACG ACG which is not pure because of the substitution C/G in the second copy of the motif. However, our filtering would avoid computing two uninteresting TRs consisting in: the last three instances of the motif (ACG ACG ACG), and the last two instances (ACG ACG). A tandem beginning with the motif AGG, instead, needs to be searched since it could be the starting point of a second TR not entirely included in the first result (in the case of the example AGG ACG ACG ACG AGT AGT). A second class of artifacts is that of pairs (or groups) of overlapping results consisting in the same TR (with the same motif length and copy number) shifted one position forward along the sequence (namely starting and ending positions differ by only 1 bp). This case arises only when the initial characters of the TR match the corresponding characters immediately after the TR. Dealing with this class of artifacts, the searching procedure attempts to expand a retrieved TR allowing it to have a final (fractional) pure motif (see lines 22–25 of Algorithm 1). The resulting extended TR is conceptually equivalent to merging together a sequence of shifted TRs into a longer result. The above equivalence suggests a sufficient condition to determine whether searching from a given position would lead to find an artifact. In fact, since a TR with a  0 &lt; l ^ &lt; l  long final motif is equivalent to a sequence of  l ^  TRs in which motifs are shifted by 1 bp, all the positions at distance at most  l ^  from a marked element in the bit vector must be the starting coordinates of a sub-TR of the expanded one. As a result, these positions can be ignored. A last class of artifacts derives from the fact that the same TR can admit several distinct combinations of period length and copy number. Since  Dot2dot  increasingly tests several possible motif lengths (see line 2 of Algorithm 1), the absence of a specific mechanism to filter (or to avoid the computation of) all the possible combinations of the same result would lead to a potentially huge redundancy in the output. Dealing with impure TRs the only available option is that of computing all the combinations and then evaluating which one better fits a pre-determined criterion (in our case we prioritize the purest one). Pure TRs, instead, do not require computing all the possible combination. In fact, testing for multiples of the motif length can only produce either TRs of the same size or shorter. Consequently, once we find a pure TR we do not need to test multiples of its motif length. 3 Results We experimentally tested our software to assess whether it is able to find non-naive biologically relevant TRs. Moreover, we thoroughly scanned the literature to find the widest possible pool of alternative algorithms to compare with. As a testing dataset we used five collections of TRs: 45 disease-related TRs, the 20 extended CODIS repeats, a set of Y-STR loci, 620 markers from the Marshfield panel and a wide list of TRs located in the regulatory regions. Although it could be considered inconsequential for our purposes, we choose to map all TRs and genes onto the most recent main release of the human genome (namely hg38). We manually mapped genes querying the UCSC genome browser ( https://genome-euro.ucsc.edu ) while we used the batch  LiftOver  interface ( https://genome.ucsc.edu/cgi-bin/hgLiftOver ) to convert the coordinates of TRs. Results show that our method is able to find biologically relevant repeats not reported from the other methods. 3.1 Tandem repeats discovery tools selection Despite the vast literature on TRs discovery tools [see  Lim  et al.  (2013)  for a recent survey], only few algorithms are usable in practice. In fact, most tools seem to be no longer available or no longer supported ( Abajian, 1994 ;  Krishnan and Tang, 2004 ;  Kurtz  et al. , 2001 ;  Parisi  et al. , 2003 ;  Pokrzywa and Polanski, 2010 ;  Sokol  et al. , 2007 ;  Taneda, 2004 ;  Wirawan  et al. , 2010 ;  Zhou  et al. , 2009 ); some other is still available but no longer maintained [this is the case e.g.  Wexler  et al.  (2005) ] that is distributed only in binary form and requires an obsolete version of the operating system). Some algorithms are subjected to limitations that make comparing with them unfair. In particular: STAR ( Delgrange and Rivals, 2004 ) uses a dictionary-based approach that makes its computational cost unaffordable even using a small dictionary; MsDetector ( Girgis and Sheetlin, 2013 ) and IMEx ( Mudunuri and Nagarajaram, 2007 ) are designed only for microsatellites with motif length ≤6; the approach in  Thiel  et al.  (2003)  leverages on a species-specific database; E-TRA ( Karaca  et al. , 2005 ) can only find perfect TRs; and  Pop (2015)  provides only a visual representation of the distribution of TRs over the input sequence. At the end of our investigation we identified only seven algorithms that can be realistically employed in daily TR discovery tasks: tandem repeats finder (TRF) ( Benson, 1999 ), mreps ( Kolpakov  et al. , 2003 ), tandemSWAN ( Boeva  et al. , 2006 ), TRStalker ( Pellegrini  et al. , 2010 ), SciRoKo ( Kofler  et al. , 2007 ), TROLL ( Castelo  et al. , 2002 ) and RepeatMasker ( Smit  et al. , 2017 ) ( http://www.repeatmasker.org ). Since all these methods are provided with a default set of parameters that perform well in most cases, after verifying that they match the characteristics of our datasets, we always used them (see details in the  Supplementary Material ). 3.2 Disease-associated tandem repeats Due to the proven relationship between repeat expansion and a consistent number of neurological and neuromuscular disorders ( Mirkin, 2007 ), we tested our algorithm with the aim of evaluating its ability to locate significant TRs associated with diseases. To obtain a convincing list of pathology-related TRs we built our dataset collecting information from several sources. In  Castel  et al.  (2010)  a list of 44 well-known polymorphic TRs (36 of which confirmed to be associated with a pathology) is given. We removed from this list two entries: SCA31 because the associated repeat is not present in the reference genome since the Spinocerebellar Ataxia type 31 is caused by the insertion of the entire repeat ( Sato  et al. , 2009 ), and Facioscapulohumeral muscular dystrophy because the 3.3 k D4Z4 macrosatellite repeat is too long for all the tested algorithms. Due to the lack of the chromosomal coordinates in the above list, we extracted this information from other sources. In a master thesis ( Mador-House, 2014 ) of the same research group of the article in  Castel  et al.  (2010) , the authors extend the list including four new TRs (two of which linked with a pathology: C9ORF72 and SCA36) and removing two [one linked with the fragile X tremor/ataxia syndrome and one (FRA16A) not directly linked with a specific pathology] providing the coordinates of the TRs in the hg19 reference genome. From the list in  Mador-House (2014)  we removed 12 elements corresponding to TRs with not confirmed association to a pathology (at the time of this publication) because (as the author mentioned in the thesis) they have been located using the Tandem Repeat Finder software. Since some of the TRs in  Castel  et al.  (2010)  have now been confirmed as associated with a pathology, and thus their sequence and genomic position is known, we could manually extend our list exploiting  blastn  ( http://blast.ncbi.nlm.nih.gov ) to locate them. In particular: according to  Todd  et al.  (2013)  the sequence of the TR causing the fragile X tremor is an almost pure CGG sequence; in  Wieben  et al.  (2012)  the authors provide the sequence of a repeat expansion in the transcription factor 4 (TCF4) causing the Fuchs corneal dystrophy; in  DeJesus-Hernandez  et al.  (2011)  the authors report the motif of a pure TR whose expansion in the non-coding region of C9ORF72 is associated with FTD and ALS; and in  Grube  et al.  (2011)  the trinucleotide expansion in KCNN3 reported in  Chandy  et al.  (1998)  appears to be associated with schizophrenia. We further extended our list including other notable TRs reported in the literature. In  Pellegrini  et al.  (2012)  the authors list 29 TRs two of which (one in PHOX2B and one in SOX3) were not included in our list. In  Winnepenninckx  et al.  (2007)  an expansion of a pure CGG-repeat in the 5' UTR of the DIP2B gene is associated with the FRA12A disease. In  de Pontual  et al.  (2003)  a mutation of the proneural HASH-1 gene is associated with CCHS. Finally, a CAG repeat in POLG1 has been associated with male infertility in  Aknin-Seifer  et al.  (2005)  and recently with breast cancer risk in  Azrak  et al.  (2012) . Our final dataset consists in 45 TRs with: a disease-associated polymorphism, period ranging from 3 to 24 bp, and size ranging from 15 to 405 bp (see  Supplementary Material  for the complete list and coordinates). 3.3 DNA profiling tandem repeats As a consequence of their contribution to DNA profiling in forensic sciences, a large database of short TRs [STRbase  Ruitberg  et al.  (2001) ] has been made publicly available on the web ( http://www.cstl.nist.gov/strbase/ ). Although STRbase does not report the exact genomic location of the censused STRs, it provides useful information that we exploited to pinpoint the coordinates of the two most commonly used collections of TR listed in it: CODIS and Y-STR. 3.3.1 FBI CODIS The Combined DNA Index System (CODIS) database consists of 13 tetra-nucleotide TRs spread in 12 chromosomes. As of January 1, 2017 this dataset has been extended with 7 new tetra-nucleotide TRs: D1S1656, D2S441, D2S1338, D10S1248, D12S391, D19S433 and D22S1045. We collected the coordinates in hg19 of the 13 CODIS core STR loci from the lobSTR ( Gymrek  et al. , 2012 ) website, while we manually retrieved the coordinates of the new TRs. 3.3.2 Y-STR Y-STR is a collection of short TRs in the  Y  chromosome with period lower than 6 and size ranging from 24 to 166 bp. These repeats are often used for paternity or genealogical tests as well as for forensic purposes. An almost complete list of these loci has been published in  Gymrek  et al.  (2013)  and the genomic coordinates in hg19 are available in the lobSTR website. The two markers DYS448 and DYS449 are not TRs in a strict sense and consist of two pure STRs interrupted by a small spurious sequence. According to  Gymrek  et al.  (2013) , for these markers we included both: the entire STR and the two parts. We completed our list including two extra TRs not mentioned in  Gymrek  et al.  (2013)  but present in the lobSTR website: DYS640 and DYS464. The forensic value of DYS464 is studied in  Butler and Schoske (2004) . The final collection of Y-STR consists of 86 loci. 3.4 Marshfield linkage panel The Marshfield linkage panel ( Rosenberg  et al. , 2005 ) consists of more than 600 loci distributed across the autosomes each of which containing a short and highly polymorphic TR. The main purpose of this panel is that of exploiting the great degree of polymorphism of TRs to conduct genome-wide population analyses. The original work in  Rosenberg  et al.  (2005)  neither describes in details the structure of the TRs nor reports their coordinates. Thus, some authors used TRF to pinpoint the repetitive structures. This choice is sensible for certain problems [e.g. in  Willems  et al.  (2017)  where the Marshfield panel is used as a benchmark for genotypization], but it is inappropriate for our purposes. In the  Supplementary Material  of  Pemberton  et al.  (2009) , the authors provide the PCR primers, the RefSeq sequences, as well as a manually curated description of the TR structures, for 627 of the Marshfield loci (see the file Pemberton_AdditionalFile1_11242009.txt in the Rosenberg’s website  https://web.stanford.edu/group/rosenberglab/data/pembertonEtAl2009/ ). Using the RefSeq sequences as input for  blastn  ( http://blast.ncbi.nlm.nih.gov ) we located 598 loci. We further obtained the hg38 coordinates of another 22 loci by means of the UCSC’s  in silico  PCR tool ( https://genome.ucsc.edu/cgi-bin/hgPcr ). Finally, we pinpointed the exact coordinates of each TR by a local alignment of the flanking regions of the TR in refseq and the locus in hg38. The resulting dataset consists in 620 microsatellites with size ranging from 8 to 270 bp. 3.5 Tandem repeats in the regulatory regions Despite being ubiquitously distributed in eukaryotic genomes, TRs have been reported to be more abundantly present in regulatory regions and in particular in promoters ( Sawaya  et al. , 2013 ;  Vinces  et al. , 2009 ). Their great variability as well as intrinsic instability suggests that mutations of this class of genotypic variation in the promoter regions can influence the observed phenotype ( Gemayel  et al. , 2010 ). Notwithstanding their importance, a manually curated reference database of variable TRs in promoter regions is not available yet. Partial lists of pure TRs are reported in  Heidari  et al.  (2012)  and  Ohadi  et al.  (2012)  while in  Bolton  et al.  (2013)  the authors describe a resource where a partial list is obtained by means of the only TRF software. Besides the database resource, the authors of  Bolton  et al.  (2013)  provide a description of a sensible methodology that we used to recompute the list of TRs in the promoter regions using all the algorithms available to us. Following the procedure in  Bolton  et al.  (2013)  we downloaded the coordinates of the human genes from the UCSC table browser ( Karolchik  et al. , 2004 ), then we removed non-coding and putative genes as well as genes not in haplotypic regions. After removing duplications we obtained a list of 36 096 elements including coding genes and isoforms. In order to ensure that the promoter regions have been entirely covered, as an input sequences we used a 3 kb interval from −2 kb to +1 kb from the transcription start sites. We run all the algorithms on these sequences limiting the TR period up to 9 bps, discarding results with purity lower than 90% and removing TRs shorter than 25 bp. Given the reasonably limited number of results, we could refine the list of TRs by: re-estimating the correct period length, and trimming spurious endpoints. Estimating the period length, we used a brute force procedure that tested all the possible combinations and decided for the shortest value maximizing purity. Then we compared the first and the last instance of each TR motif with the corresponding consensus sequence and trimmed those not matching. Finally, we get rid of overlapping TRs by means of an iterative procedure that, at each step, removed the less pure overlapping TR or, in case of tie, the overlapping TR with higher motif length or the shortest one. At the end of this procedure we obtained a final list of 15 326 TRs. 3.6 Evaluation metrics Assessing quality of TR discovery algorithms via comparing results with a gold standard requires a few caveats. A first issue is the definition of matching. In the most restrictive setting, one could be interested in the exact match of the starting and ending coordinates, while in general a limited degree of divergence is acceptable. A perfect match is very hard to achieve and it could not be significant because of the subjectivity of the identification procedure. In fact, the coordinates of (more often impure) TRs can slightly differ according to the interpretation given by the curator during the manual annotation phase. In addition, when a TR is adjacent to a region similar enough to the repeat itself, many alternatives are equally possible. In this case, establishing which is the ‘correct’ repeat, results in an arbitrary choice. Another relevant issue is that of redundancy (namely, the presence in the output of overlapping TRs). Despite often due to software artifacts, a moderately redundant output may not be problematic when the subsequent analysis is automated, while it could defeat the purpose of an algorithm when only a restricted number of results can be analyzed. An evaluation based only on the score of the best hit can penalize those algorithms that employ filtering to reduce the number of biologically irrelevant results. Finally, the possibility of defining the concepts of true/false positives/negatives, that are at the base of measures like sensitivity and specificity, should be critically examined. As observed in  Saha  et al.  (2008) , evaluating an  ab-initio  method through a gold standard dataset (either the output of another algorithm or a collection of TRs) the true positives are easily defined as the algorithm results that are also listed in the gold standard, while the false negatives are sequences of the testing dataset not reported in the list of results. The problem arises with the other two classes. In particular, it is questionable whether a result reported by the tool but not present in the reference dataset is a false positive or it is a new legal TR that was not previously known. The difficulty of defining the concepts of false positives and true negatives has the effect of hindering the estimation of specificity. Sensitivity, instead, can be computed through the standard formula. However, since sensitivity and specificity are dual evaluation functions that need to be considered as a whole, we decided to use alternative measures. In particular, we used precision and recall. In order to address the issue of matching algorithm results with the gold standard, we used the Jaccard coefficient. This score has originally been employed to measure the degree of similarity between sets. Subsequently, it has been extended to measure the overlap between intervals. In short, the Jaccard coefficient is defined as the ratio of the length of the intersection of two intervals divided by the size of their union. Its value is bounded in the range  [ 0 , 1 ]  and to a higher value corresponds a higher degree of matching. Entirely covering a TR is necessary but not sufficient to get the highest score, in fact, the Jaccard coefficient has value 1 only when the comparing intervals have exactly the same coordinates. Dealing with redundancy, we used three measures: the average number of results covering an element of the gold standard, the average precision and the average recall. Let  T = { t 1 , … , t n }  be a dataset of TRs,  R  be the set of results of a given algorithm, and  R ( t i )  be the subset of  R  overlapping with  t i  by at least 1 bp. We further denote  j a c ( )  as the Jaccard coefficient between two genomic intervals. The average precision and average recall are defined as follows:
 σ P ( T , R ) = 1 n ( ∑ i = 1 n ∑ x ∈ R ( t i ) j a c ( x , t i ) | R ( t i ) | ) σ R ( T , R ) = 1 n ( ∑ i = 1 n max x ∈ R ( t i ) j a c ( x , t i ) ) . The above measures have several advantages. Firstly, they are independent from the arbitrary choice of a threshold value. Secondly, results over different datasets can be merged into a single score or can be compared directly. Lastly, it is possible to compare algorithms that apply filtering with methods that do not use it. In fact, even in a case where the filtering phase removes the most overlapping repeat, causing the Jaccard score of the second best hit to drop under threshold, the removal of a promising repeat could be balanced by the removal of a certain number of irrelevant repeats with low Jaccard score. 4 Discussion In this section we discuss the outcome of our comparative evaluation. We run  Dot2dot  using two sets of parameters: one with the default values but without any filtering, and one with the same values but enabling the most stringent filtering (see  Supplementary Material  for details). TRStalker was run in multiprocessor mode using 64 threads. We also set a time limit of four days to accomplish a single run over a sequence. Because of the severe restrictions on the input length due to the computational cost of TRStalker, we run this software also on trimmed subsequences of length at most 10 kbp centered on a TR (notice that this had no effect on the TRs in the regulatory regions since the input is always shorter). Although a direct comparison of TRStalker on the diseases and CODIS datasets with or without shortened input reveals a remarkable advantage in favor of the former (henceforth indicated asTRStalker*), using trimmed input was the only way to estimate TRStalker performances on all the datasets. 
 Table 1  reports the number of TR identified setting Jaccard score to 0.5 and 0.7 (see results for other thresholds in the  Supplementary Material ), the average number of results per locus (RPL), the average precision and the average recall of each algorithm for the five test collections. Accepting  Jaccard  =0.5  Dot2dot  and TRStalker* rank alternatively first and second in terms of absolute number of located TRs with a negligible gap. TRStalker on the diseases loses only two TRs, while the other algorithm in general misses a remarkable number of elements. Increasing Jaccard to 0.7 causes  Dot2dot  to became less accurate than TRStalker and TRStalker* but still more accurate than the others. This higher accuracy, however, is the effect of a very large redundancy due to the exhaustive enumeration of all the possible alternative TRs covering the same locus. In fact, TRStalker and TRStalker cover a locus with an average of over 110 different results.
 Table 1. Average precision  σ P , average recall  σ R , average number of reported results covering a target locus (RPL) and total number of TRs intersected (with Jaccard=0.5 and Jaccard=0.7) of the compared algorithms and datasets Dataset Measure Dot2dot-filter Dot2dot TRF MREPS TRStalker TRStalker* SWAN Troll SciRoKo Repeatmasker Diseases σ P 0.830 0.678 0.722 0.630 0.464 0.491 0.372 0.508 0.724 0.452 σ R 0.835 0.899 0.763 0.686 0.901 0.960 0.575 0.571 0.752 0.475 RPL 1.0 6.0 1.4 1.5 155.9 157.5 2.0 4.1 1.1 1.2 #TR j=0.5 44 45 37 37 43 45 27 29 38 24 #TR j=0.7 36 42 33 23 43 45 24 18 33 20 CODIS σ P 0.860 0.721 0.836 0.659 0.485 0.565 0 0.682 0.819 0.812 σ R 0.860 0.899 0.850 0.818 0.839 0.988 0 0.797 0.867 0.822 RPL 1.0 6.3 1.1 2.1 188.2 172.2 0.0 3.2 1.2 1.0 #TR j=0.5 19 19 20 20 18 20 0 19 20 19 #TR j=0.7 18 18 15 15 18 20 0 16 16 15 Y-STR σ P 0.877 0.706 0.770 0.617 — 0.535 0.021 0.682 0.827 0.721 σ R 0.879 0.916 0.786 0.712 — 0.979 0.029 0.777 0.841 0.729 RPL 1.0 6.1 1.1 2.1 — 205.5 2.5 2.8 1.1 1.0 #TR j=0.5 84 86 70 74 — 86 3 79 80 67 #TR j=0.7 74 81 65 59 — 86 1 75 69 62 Marshfield σ P 0.856 0.637 0.784 0.662 — 0.559 0 0.683 0.810 0.767 σ R 0.860 0.905 0.794 0.755 — 0.975 0 0.792 0.830 0.775 RPL 1.0 7.1 1.1 2.0 — 154.3 0.0 3.2 1.1 1.0 #TR j=0.5 589 609 559 577 — 619 0 583 587 554 #TR j=0.7 503 557 447 405 — 619 0 490 471 437 Promoters σ P 0.825 0.667 0.663 0.575 0.422 0.422 0.663 0.517 0.808 0.743 σ R 0.803 0.934 0.548 0.687 0.929 0.929 0.447 0.627 0.709 0.432 RPL 1.0 4.8 1.4 1.9 116.0 116.0 2.2 2.1 1.0 0.6 #TR j=0.5 13 783 15 192 8864 12 490 15 264 15 264 5156 10 939 12 125 7128 #TR j=0.7 12 329 14 792 7905 7880 15 098 15 098 3233 5723 10 492 6286 Note : TRStalker run on the trimmed sequences is reported as TRStalker *. (The best value is highlighted in bold). Although the ability of identifying the repeats of the gold standard with a reasonably high Jaccard score is a desirable property, achieving a high recall at the cost of a large output redundancy can greatly reduce the usefulness of an algorithm in practice. In particular, a good filtering strategy should increase the average precision at the cost of a negligible reduction of accuracy. 
 Table 1  shows that an aggressive filtering typically has a positive impact on the average precision as well as on controlling the redundancy of the output. In fact, the algorithms implementing the most aggressive filtering strategy ( Dot2dot-Filter , TRF and SciRoKo) achieve the highest values of average precision. As a notable exception, the (embedded) purity-based filtering of TandemSWAN caused the algorithm to filter out most of the TRs on the profiling datasets because they were considered uninteresting. A high level of redundancy is not necessarily a guarantee of a high average recall. This is the case, e.g. of Troll that has the third highest number of RPL but achieves one of the lowest average recall.  Dot2dot , TRStalker and TRStalker*, which have the most verbose output, achieve the highest average recall. However, in spite of having comparable average recall (almost the same narrowing to TRStalker),  Dot2dot  returns a number of RPL of the same order of magnitude of the other methods, while TRStalker produces two orders of magnitude more RPL. Comparing  Dot2dot  run with and without filtering, we observed how our filtering strategy has a remarkably positive impact in terms of a drastic reduction of overlapping results, passing from about 5 RPL to ∼1. Filtering causes an increase of average precision of about 0.15 allowing  Dot2dot  to attain the highest score over all the tested datasets at the cost of a limited reduction of average recall (−0.06). However, this lower recall does not change the relative ranking of our algorithm in comparison with the other methods. 4.1 Running time Although running time is a secondary feature for TRs discovery algorithms, it can still be important to evaluate the feasibility of large-scale analyses of whole-genomes and NGS data. Assessing the selected algorithms on whole-genomes, we run them on seven assembled references with sizes ranging between 1.3 and 27 Mbp, available on the NCBI website. NGS data, instead, have been tested on the high-coverage long reads of the NA12878 individual sequenced with Oxford Nanopore Technology ( Jain  et al. , 2018 ). However, since  Dot2dot  and TRF are the sole two methods that can directly handle large multifasta files (Mreps, Tandem SWAN can handle only one sequence at time, Troll and SciRoKo, instead, require one file for each sequence to be specified in the command line). We narrowed comparison on NGS data only to them. We used a MacOS-based workstation endowed with a processor Intel Xeon 3.1Ghz and run the software in single thread mode. For tandemSWAN and Troll, which run only under Linux, we had to use a different hardware. We computed the performance difference between the two machines by comparing the speed of  Dot2dot  over chromosome 1 of hg38. We found that the Linux server is 1.4556 times faster. As a result, we used this constant to adjust tandemSWAN and Troll’s running time. We report in  Table 2  the running time of the tested algorithms. We excluded TRStalker because it cannot run on large sequences and Repeat Masker because its running time is dominated by the identification of other classes of repetitions different from TRs. Since we endorse filtering, we run  Dot2dot  only with it. We notice that this choice does not give  Dot2dot  any advantage in the comparison, in fact, without filtering our software would run slightly faster.
 Table 2. Running time of the compared algorithms over seven common reference genomes Organism name Size Mb Dot-to-dot filter TRF Mreps Tandem SWAN SciRoKo Troll Pinus lambertiana 27602.70 12 h 15 min 18 s 16 h 15 min 14 s N/A N/A 2 h 13 min 51 s N/A Triticum aestivum 13427.40 5 h 57 min 14 s 11 h 13 min 43 s N/A N/A 41 min 59 s N/A Locusta migratoria 5759.80 2 h 37 min 14 s 2 h 25 min 12 s 11 h 20 min 14 s N/A 18 min 39 s N/A Homo sapiens 3241.95 1h 24min 15s 2 h 40 min 51 s 2 h 36 min 00 s 12 h 31 min 48 s 10 min 43 s 35 min 14 s Rattus norvegicus 2870.18 1 h 18 min 52 s 2 h 04 min 44 s N/A N/A 9 min 40 s N/A Mus musculus 2807.72 1 h 16 min 29 s 1 h 55 min 37 s N/A N/A 9 min 41 s N/A Danio rerio 1371.72 36 min 29 s 2 h 33 min 36 s 2 h 07 min 54 s 13 h 59 min 40 s 5 min 7 s 17 min 16 s NA12878 82705.67 1 d 12 h 15 min 18 s 1 d 16 h 15 min 14 s — — — — Note : The genome dimension is measured as the size in Mb of the corresponding fasta file. As  Table 2  shows, three algorithms are unable to run on the majority of the genomes. This is mostly due to intrinsic thresholds hardwired in the software. In particular, Mreps has a constraint on the length of the longest consecutive stretch of Ns in a sequence, while Troll, which would be the second fastest software, has a limit on the overall length of the input that cannot exceed 2 31  bp (about 2.1 Gbp). Because of these limitations Mreps could not run on relatively small genomes like that of Mus musculus, while to run Troll on the human genome we had to search each chromosome independently and merge results together. Being based only on the computation of the Hamming distance with a seed, SciRoKo consistently achieve the highest speed regardless the sequence length. However, the absence of any corrections to this simple mechanism causes SciRoKo to have one of the less rich output in terms of number of results per kilobase (as few as 0.61 TRs per kbp) with a large predominance of pure (or almost pure) TRs. Despite the reachest output in terms of results per kBps (see details in the  Supplementary Material ),  Dot2dot  runs consistently faster than TRF. The gap between the two algorithms is quite negligible for small and medium-sized genomes, while it tends to became rather large for the two biggest assembled genomes. Comparisons on the NGS data, instead, show a substantial gap that can have a crucial weight in large-scale analyses. Moreover, the absence of a multithread release of the TRF software, and the consequent difficulty of using parallelism of modern CPUs, has the effect of sharpen this gap even more. In fact, running 24 parallel instances of TRF (one per chromosome), it still requires more than 22 h to finish while  Dot2dot  with 24 threads completes in 5 h. Data on  Table 2  confirm also the average-case cost analysis of our algorithm. In fact, the  Dot2dot  running time grows proportionally with the genome’s length with a small stable constant factor. 5 Conclusion In this paper we presented  Dot2dot : a new algorithm for TR identification in a target genome. Our model of repeat has shown to be general enough to capture well various classes of TRs with different characteristics: pathology-linked, forensic, for population analysis, genealogic-oriented and repeats in the regulatory regions. Experiments have shown that  Dot2dot  is fast and effective since it was able to identify almost all the TRs of our test collections with an accuracy of at least 0.7 in terms of Jaccard score. Even applying a severely stringent filtering where overlap among the returned repeats is not allowed, our algorithm is still more accurate than the alternative tested tools. Tests over the entire human genome have confirmed the hypothesis that the longest TR (with a length of 2495 bp) is several order of magnitude shorter than any of the input sequences enabling  Dot2dot  to run in linear time with the input length. 
 Dot2dot  is freely available and can be used without restrictions. To make it useful in the daily laboratory practice we enabled it to read standard formats for both assembled genomes (fasta) and NGS data (fastq) as well as return its output also in a standard bed format. Another contribution of this paper is that of proposing a rigorous assessment methodology of TRs discovery algorithms as well as providing five reference collections of TRs (four of which manually curated). We hope that the proposed methodology and datasets can help to facilitate future research in this field. Funding This work was supported by the project RepeatALS FGBR 17/2013 funded by Arisla (Italian Society for Research on Amyotrophic Lateral Sclerosis) and the project PRIN 201534HNXC. The role of tandem repeats in neurodegenerative diseases: a genomic and proteomic approach funded by the Italian Ministry of Education and University (MIUR). 
 Conflict of Interest : none declared. Supplementary Material Supplementary Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>ntHash: recursive nucleotide hashing</Title>
    <Doi>10.1093/bioinformatics/btw397</Doi>
    <Authors>Mohamadi Hamid, Chu Justin, Vandervalk Benjamin P., Birol Inanc</Authors>
    <Abstract>Motivation: Hashing has been widely used for indexing, querying and rapid similarity search in many bioinformatics applications, including sequence alignment, genome and transcriptome assembly, k-mer counting and error correction. Hence, expediting hashing operations would have a substantial impact in the field, making bioinformatics applications faster and more efficient.</Abstract>
    <Body>1 Introduction Hashing is a common function across many informatics applications, and refers to mapping an input key value of arbitrary size to an allocated memory of predetermined size. Among other uses, it is an enabling concept for rapid search operations, and forms the backbone of Internet search engines. In bioinformatics, it has many applications including sequence alignment, genome and transcriptome assembly, RNA-seq expression quantification,  k -mer counting and error correction. Large-scale sequence analysis often relies on cataloguing or counting consecutive  k -mers in DNA/RNA sequences for indexing, querying and similarity searching. An efficient way of implementing such operations is through the use of hash based data structures, such as hash tables or Bloom filters. Therefore, improving the performance of hashing algorithms would have a great impact in a wide range of bioinformatics tools. Here, we present ntHash, a fast function for recursively computing hash values for consecutive  k -mers. 2 Methods We propose an algorithm to hash consecutive  k -mers in a sequence,  r  of length  l  &gt;  k , using a recursive function,  f , in which the hash value of the current  k -mer  H  is derived from the hash value of the previous  k -mer: 
 (1) H ( k − mer i ) = f ( H ( k − mer i − 1 ) , r [ i + k − 1 ] , r [ i − 1 ] ) 
 Such a recursive function, also called rolling hash function, offers huge improvements in performance when hashing consecutive  k -mers. This has been previously described and investigated for  n -gram hashing for string matching, text indexing and information retrieval ( Cohen, 1997 ;  Gonnet and Baezayates, 1990 ;  Karp and Rabin, 1987 ;  Lemire and Kaser, 2010 ). In this paper, we have customized the concept for hashing all  k -mers of a DNA sequence, and implemented an adapted version of the cyclic polynomial hash function, ntHash, to compute normal or canonical hash values for  k -mers in DNA sequences efficiently. In hashing by cyclic polynomial, ntHash uses barrel shifts instead of multiplications to make the process faster. To compute hash values for all  k -mers of the sequence  r  of length  l , we first hash the initial  k -mer,  k -mer 0 , as follows: 
 (2) H ( k − mer 0 ) = r o l k − 1 h ( r [ 0 ] ) ⊕ r o l k − 2 h ( r [ 1 ] ) ⊕ … ⊕ h ( r [ k − 1 ] ) 
where  rol  is a cyclic binary left rotation, ⊕ is the bit-wise EXCLUSIVE OR (XOR) operator, and  h ( . ) is a seed table, where the letters of the DNA alphabet, Σ = {A, C, G, T}, are assigned different random 64-bit integers. The hash values for all consequent  k -mers,  k -mer 1 , …,  k -mer l-k , are then computed recursively as follows:
 (3)   H ( k − mer i ) = r o l 1 H ( k − mer i − 1 ) ⊕ r o l k h ( r [ i − 1 ] ) ⊕ h ( r [ i + k − 1 ] ) 
 We note that the time complexity of ntHash for sequence  r  is O( k  +  l ) compared to O( kl ) complexity of regular hash functions. In some bioinformatics applications, one might be interested in computing the hash value of forward and reverse-complement sequences of a  k -mer. To do so, we add in the seed table integers that correspond to the complement bases, such that table indices of base-complement base pairs are separated by a fixed offset. Using this table, we can easily compute the hash value for the reverse-complement (as well as the canonical form) of a sequence efficiently, without actually reverse-complementing the input sequence, as follows:
 (4) H ( k − mer ´ 0 ) = h ( r [ 0 ] + d ) ⊕ r o l 1 h ( r [ 1 ] + d ) ⊕ … ⊕ r o l k − 1 h ( r [ k − 1 ] + d ) H ( k − mer' ´ i : 1.. l − k ) = r o r 1 H ( k − mer ´ i − 1 ) ⊕ r o r 1 h ( r [ i − 1 ] + d ) ⊕ r o l k − 1 h ( r [ i + k − 1 ] + d ) 
where  r o r  is a cyclic binary right rotation, and  d  is the offset of complement base in the seed table  h ( . ). Further, ntHash provides a fast way to compute multiple hash values for a given  k -mer, without repeating the whole procedure for each value. To do so, a single hash value is computed from a given  k -mer, and then each extra hash value is computed by few more multiplication, shifting and XOR operations on the initial hash value ( Supplementary Data ). This would be very useful for certain bioinformatics applications, such as those that utilize the Bloom filter data structure. Experimental results demonstrate a substantial speed improvement over conventional approaches, while retaining a near-ideal hash value distribution ( Fig. 1 ,  Supplementary Tables S1–S8 ).
 Fig. 1. Performance of ntHash. ( a ) A good hash function should have its values uniformly and independently distributed across the target domain. One way of measuring that is through correlation coefficients between the bits of hashed values. The plot shows natural statistical fluctuations for smaller sample sets (100 data points, the area above diagonal). The correlations dissipate rapidly for large sample sets (100 000 data points, the area below diagonal). ( b ) Runtime for hashing 250 bp DNA sequences with different  k -mer lengths from 50 to 250. ntHash outperforms all other hash methods when hashing more than two subsequent  k -mers, i.e.  k  &lt; 249. ( c ) Comparing multi-hashing runtime of ntHash with the leading hash functions for one billion 50-mers. ntHash performs over 20× faster than the closest competitor, cityhash. Grey, orange and blue bars refer to calculation of one, three and five hash functions, respectively 
 In the Results section, we have used sequencing data on the human individual NA19238 from the Illumina Platinum Genomes project, as well as simulated random DNA sequences, as detailed in Supp. Data. 3 Results A good hash function should generate hash values that are independently and uniformly distributed across the target domain, resulting in fewer collisions. To evaluate the independence of ntHash values, one way is to use the correlation coefficient between the bits of 64-bit hash values. That is, if each bit  x i ,  i  = 1..64, in a 64-bit hash value vector  X  = { x 1 ,  x 2 , …,  x 64 } is an independent random variable, then there should be no correlation between them. To test this, we first generated sets of hash values for a given input. We next computed the (Pearson) correlation coefficient matrix for each sample set.  Figure 1a  shows the correlation coefficients of two sample sets of size 100 (above diagonal) and 100 000 (below diagonal). The plot shows natural statistical fluctuations for smaller sample sets. The correlations dissipate rapidly for large sample sets ( Supplementary Figs S1–S5 ). Comparing the computed correlation coefficients with a confidence interval around the theoretical zero correlation shows that for all hash functions tested, the number of observations outside the 99.7% confidence is around 0.3%, in agreement with theoretical expectations. We have also evaluated the uniformity of different hash methods by utilizing a Bloom filter data structure. Here, we first load a Bloom filter with a number of unique  k -mers, and then query the Bloom filter with another set of unique  k -mers. The results show the false positive rates of ntHash and other hash functions comply with the theoretical false positive rate for Bloom filters, indicating the uniform distribution of hash values generated by the tested hash methods ( Supplementary Tables S1–S8 ). We have compared the runtime performance of ntHash algorithm with three most widely used hash methods in bioinformatics: cityhash, murmur and xxhash (unpublished tools; references to websites provided in the  Supplementary Materials ). ntBase is the hash function for the base equation of ntHash ( Eq. (2) ).  Figure 1b  shows the runtimes for hashing different length  k -mers in 5 000 000 DNA sequences of length 250 bp. In the inset, we see ntHash outperforms other algorithms when hashing more than two  k -mers in a DNA sequence.  Figure 1c  illustrates a typical use case of computing multiple hash values for 50-mers in DNA sequences of length 250 bp, and shows that ntHash is over 20× faster than the closest competitor ( Supplementary Figs S6, S7 ). Funding This work has been funded by BC Cancer Foundation, Genome BC, Genome Canada, UBC and  NIH  (under award number  R01HG007182 ). Conflict of Interest : The authors have a provisional patent on the technology with USPTO # 62288334. 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 
 
 Supplementary Data 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Somatic selection distinguishes oncogenes and tumor suppressor genes</Title>
    <Doi>10.1093/bioinformatics/btz851</Doi>
    <Authors>Chandrashekar Pramod, Ahmadinejad Navid, Wang Junwen, Sekulic Aleksandar, Egan Jan B, Asmann Yan W, Kumar Sudhir, Maley Carlo, Liu Li, Schwartz Russell</Authors>
    <Abstract/>
    <Body>1 Introduction In tumor development, oncogenes (OGs) and tumor-suppressor genes (TSGs) work complementarily to promote and maintain abnormal cell growth ( Morris and Chan, 2015 ;  Weinberg, 1994 ). OGs cause cancers through gain-of-function variants, whereas TSGs operate by loss of function. While there are a few well-known OGs (e.g.  RAS ) and TSGs (e.g.  TP53 ), it is fast becoming clear that the tumor-enabling activities of a gene is not the same for all types of cancers. Activities of driver genes depend strongly on their cellular contexts because of tissue-specific organizations of cancer pathways ( Schaefer and Serrano, 2016 ;  Schneider  et al. , 2017 ;  Visvader, 2011 ). Prediction of functional status of genes in different cancer types and cellular contexts is critical for not only understanding tumor biology, but also informing targeted therapies and drug-repurposing ( Morris and Chan, 2015 ;  Schneider  et al. , 2017 ;  Sleire  et al. , 2017 ). Interestingly, only one computational method (20/20+) are available to predict OGs and TSGs ( Tokheim  et al. , 2016 ). The 20/20+ is an extension of the 20/20 rule in which OGs have &gt;20% mutations causing missense changes at recurrent positions and TSGs have &gt;20% mutations causing inactivating changes ( Vogelstein  et al. , 2013 ). However, recurrent missense mutations are not a deterministic feature of OGs because these events can cluster at functionally neutral positions due to high mutational rates ( Schaub  et al. , 2018 ), and many TSGs harbor hotspots of inactivating missense mutations ( Iacobuzio-Donahue  et al. , 2004 ;  Miller  et al. , 2015 ). Meanwhile, random mutational processes may introduce protein-truncating mutations (i.e. nonsense and frame-shifting mutations) into OGs, which increase in frequency via genetic drift with no significant impact on tumor fitness and mislead annotations ( Lipinski  et al. , 2016 ;  Mort  et al. , 2008 ;  Schaub  et al. , 2018 ). Therefore, conventional ratiometric measures are inadequate to distinguish these two groups of genes. Because tumor development is an evolutionary process, cells carrying somatic mutations are under natural selection within tumors. The positive selection promotes advantageous genotypes that confer higher fitness to a tumor. The negative selection eliminates genotypes with adverse effects. Neutral evolution lets insignificant genotypes to drift up or down in frequency. In OGs, gain-of-functions may be achieved via missense mutations, which are expected to be positively selected. In contrast, protein-truncating mutations (e.g. nonsense mutations and frame-shifting mutations) often inactivate an OG and are detrimental to tumor fitness, resulting in negative selection. In TSGs, both protein-truncating mutations and missense mutations can be positively selected when they result in the loss of functions. Otherwise, they may drift neutrally or be even under negative selection if they disrupt essential biological functions. For passenger genes (PGs) that do not have significant impact on tumor fitness, we expect that all mutations are under neutral selection ( Sun  et al. , 2017 ;  Williams  et al. , 2016 ). In this study, we tested whether the difference in evolutionary dynamics of missense and truncating mutations has sufficient signal and power to improve the detection of OGs and TSGs beyond that of ratiometric measures. Such contrast is essential to distinguish TSGs deactivated by missense mutations from OGs activated by missense mutations, which is a challenging task for conventional ratiometric measures because hotspots of missense mutations are present in both cases. Furthermore, when activities of a gene vary across cancer types, the direction and magnitude of somatic selection will change accordingly, enabling contextual classification of driver genes. Our analysis of 10 172 tumor exomes from The Cancer Genome Atlas (TCGA) ( Cancer Genome Atlas Research Network  et al. , 2013 ) project revealed significant differences in selective patterns of OGs, TSGs and PGs. Based on these patterns, we developed a computational method, named genes under selection in tumors (GUST) that integrates somatic selection of genes in tumor development, molecular conservation during species evolution and conventional ratiometric measures to classify cancer genes in different tissues and organs. 2 Materials and methods 
 Curation of cancer-type specific functions of driver genes : to test our hypothesis and to train a random forest model, we needed cancer-type specific functional annotations of cancer genes. Because these annotations are not currently available, we conducted manual curations using two lists of genes with complementary information. The first list consisted of 36 OGs, 48 TSGs and 21 genes with dual OG/TSG roles annotated in the cancer gene consensus (CGC, version 87) ( Sondka  et al. , 2018 ). The tumor-activating or -suppressing roles of these genes have been confirmed with cancer hallmarks in experimental assays and are attributable to coding substitutions or indels ( Hanahan and Weinberg, 2000 ). The second list consisted of 235 computationally predicted driver genes assigned to specific cancer types ( Bailey  et al. , 2018 ). These predictions were based on a meta-analysis of the TCGA samples with multiple computational programs. These two lists shared 70 genes. We then retrieved somatic mutations of these 70 genes from the TCGA project ( Cancer Genome Atlas Research Network  et al. , 2013 ). For a gene to qualify as an OG in a specific cancer type, it needs to be annotated as an OG or a dual-role gene in the CGC, predicted as a driver in the meta-analysis of the matching cancer type, and display mutational hotspots in the corresponding TCGA tumor samples. For a gene to qualify as a TSG in a specific cancer type, it needs to be annotated as a TSG or a dual-role gene in the CGC, predicted as a driver in the meta-analysis, and have an overabundance of truncating mutations or missense mutations in the corresponding TCGA tumor samples. For a gene to qualify as a PG in a specific cancer type, it needs to be predicted as a PG in the meta-analysis and shows no mutational hotspots or overabundance of truncating mutations in corresponding TCGA tumor samples. Genes that did not meet these requirements were removed. The final collection consisted of 55 OG annotations, 174 TSG annotations and 304 PG annotations that involved a total of 50 known driver genes and 33 cancer types ( Supplementary Table S1 ). 
 Somatic selection features : given a gene with somatic mutations reported in a collection of tumor samples, we denote the selection coefficient of missense mutations as ω, and the selection coefficient of protein-truncating (nonsense and frame-shifting) mutations as φ. To account for differences in mutational rates, we consider seven substitution types (1: A→C or T→G, 2: A→G or T→C, 3: A→T or T→A, 4: C→A or G→T, 5: C→G or G→C, 6: C→T or G→A at non-CpG sites, and 7: C→T or G→A at CpG sites), one insertion type and one deletion type. Based on the statistical framework proposed by  Greenman  et al.  (2006) , the probability of observing these mutations is a product of multinomial distributions
 (1) L ( { s k , m k , n k , i k , f k } k ) = ∏ k t k ! s k ! m k ! n k ! i k ! f k ! ( S k ) s k ( ω M k ) m k ( φ N k ) n k ( I k ) ( φ F k ) f k ( S k + ω M k + φ N k + I k + φ F k ) t k where  s k , m k , n k , i k  and  f k  are the observed numbers of synonymous, missense, nonsense, in-frame indel and frame-shifting indel mutations in the  k th rate category, respectively;  S k , M k , N k , I k  and  F k  are the corresponding expected numbers of changes computed by saturated mutations, in which we introduced each possible single nucleotide mutation one at a time; and  t k   = s k   + m k   + n k   + i k   + fk  is the total number of observed mutations. The values of log(ω) and log(φ) are determined by maximizing the log likelihood  L  and constrained within the range of [−5, 5]. The sign and absolute value of log(ω) and log(φ) indicate the direction and magnitude of somatic selection. Values around 0 indicate neutral somatic evolution. Details of parameter tuning are available in  Supplementary Methods  and  Supplementary Figure S1 . 
 GUST algorithm : GUST is a random forest model that predicts the class label (OG, TSG or PG) of a gene based on 10 features ( Supplementary Table S2  and  Fig. S2 ). In addition to the log(ω) and log(φ) values, we also compute ratiometric measures to detect mutational hotspots and conservational measures to estimate substitutional rate across species. Specifically, given a gene and a set of somatic missense mutations detected in tumor samples, we applied density estimates with a rectangular kernel and a bandwidth of five protein positions to aggregate closely-spaced mutations into peaks and denoted the highest peak as the summit. To estimate evolutionary conservation of a gene, we downloaded multiple sequence alignments of 100 vertebrate species from the UCSC Genome Browser ( Kent  et al. , 2002 ), and computed the substitution rate of each protein position ( Kumar  et al. , 2012 ;  Liu and Kumar, 2013 ). The average substitution rate over all positions measures the gene-level conservation. The average substitution rate over positions in a summit measures the conservation of a mutational hotspot. For a given gene/cancer-type pair in the curated annotations, we retrieved somatic mutations from the corresponding TCGA tumor samples and computed values of the 10 features. Using these training data, we constructed a random forest classifier with 200 trees. For each gene, this model produces three probability scores of it being an OG, a TSG or a PG, respectively. It assigns the class label based on the highest probability score. For all predictions, GUST reports random forests probability score, sensitivity and specificity. For OG or TSG predictions, GUST also reports false discovery rate. Detailed information of data processing, feature selection and false discovery rate calculation is available in the  Supplementary Materials . 3 Results 3.1 Different selection patterns of cancer genes For each gene/cancer-type pair in our manual annotations, we retrieved somatic mutations in the matching tumor samples from the TCGA project, and computed the somatic selection coefficients. We found that missense mutations in OGs were under stronger positive selection than in TSGs, as the mean log(ω) was 4.18 and 1.68, respectively ( P  &lt;   10 −10 ,  Fig. 1A ). In contrast, protein-truncating mutations showed positive selection in TSGs [mean log(φ) = 4.08)], but negative selection in OGs [mean log(φ) = −3.25, respectively)]. The effect size of the differences observed is very large, and the  P -values highly significant ( P  &lt;   10 −8 ). The selection measures observed on PGs were close to zero [mean log(ω) = 0.60 for missense and mean log(φ) = −0.28 for nonsense mutations]. Therefore, TSGs, OGs and PGs show significant evolutionary differences. Fig. 1. The distribution of selection coefficients of the curated genes. ( A ) Split violin plot showing densities of log(ω) and log(φ) values for PGs, TSGs and OGs. ( B ) Positional distribution of somatic mutations of the  BCOR  gene in stomach cancer and in melanoma. Vertical lines represent frequencies of various types of mutations at a given position. Synonymous, missense and truncating mutations are represented by green, blue and red lines, respectively. Gray lines are density curves. ( C ) Scatter plot of log(ω) and log(φ) values. Shades of hexagon bins represent the number of observations. ( D ) Positional distribution of somatic mutations of the  FBXW7  gene in uterine carcinosarcoma. (Color version of this figure is available at  Bioinformatics  online.) The distribution of log(φ) values of PGs had two peaks. The largest peak located close to 0, consistent with the expected neutral selection of PGs. The second peak located close to –5, indicating that loss of function of these PGs is detrimental to tumor growth. Interestingly, many genes in the second peak are established TSGs in other cancer types where loss of their functions is beneficial to tumors. For example, the  BCOR  gene regulates apoptosis in stomach cancer and had overabundant truncating mutations ( Cancer Genome Atlas Research Network, 2014 ). However, this gene was depleted of truncating mutations in melanoma ( Fig. 1B ). Such contrast suggested that although disabled TSGs promote tumor growth in certain cellular contexts, maintaining their activities may be essential for tumor development in other contexts. We then examined the joint distributions of log(ω) and log(φ) values and found that somatic selection patterns reflected the contextual activities of a gene ( Fig. 1C ). For example, the  PIK3CA  gene had high log(ω) values and low log(φ) values in bladder cancers, breast cancers and colorectal cancers, consistent with its well-known OG role. The log(ω) and log(φ) values of this gene were close to zero in melanoma, indicating lack of a role resulting in neutral patterns. Recently, the passenger role of  PIK3CA  in melanoma has been proposed in a study that shows  PIK3CA -mutated melanoma cells rely on cooperative signaling to promote cell proliferation and PI3K inhibitors do not repress tumor growth in the absence of other activating driver genes in melanoma ( Silva  et al. , 2017 ). For TSGs, such as  TP53 , their high log(φ) values occupied spaces distant from OGs in the distribution plot ( Fig. 1C ). As discussed earlier, TSGs with hotspots of missense mutations, such as the  FBXW7  gene in uterine carcinosarcoma ( Fig. 1D ) are challenging to distinguish from OGs using ratiometric methods. Based selection measures [log(ω)=5.0, log(φ) =3.9], this gene is unambiguously separated from OGs [log(φ)≪0], consistent with our expectations. 3.2 Performance of the GUST method We trained a random forest classifier (GUST) using the 10 features of the curated genes. Via 10-fold gene-holdout cross-validations, the testing accuracy of GUST was 0.92. As a comparison, the accuracy of 20/20+ on the entire training dataset was 0.86. To calculate traditional performance metrics, we converted three-class predictions to binary predictions by contrasting one class with the other two classes combined, i.e. one-vs-rest predictions. In all categories, GUST showed better or comparable performance than 20/20+. The largest improvements were on the precision of identifying OGs and TSGs, which increased from 0.78–0.82 in 20/20+ to 0.85–0.92 in GUST ( Table 1 ). The receiver operating characteristic (ROC) curves reconfirmed the superior performance of GUST ( Fig. 2A ). Compared to 20/20+, GUST had a significantly higher area under the curve (AUC) value of the PG-vs-rest ROC curve (0.97 versus 0.94, DeLang’s test  P  =   0.0008), and a significantly higher AUC value of the TSG-vs-rest ROC curve (0.97 versus 0.93,  P  =   0.001). However, the AUC values of the OG-vs-rest ROC curves were not significantly different between these two methods (0.99 versus 0.97,  P  =   0.21). Fig. 2. The GUST method. ( A ) ROC curves of one-vs-rest predictions for GUST and for 20/20+. ( B ) Variable importance of each feature in the random forest model. ( C ) Positional distribution of somatic mutations of the  MB21D2  gene. Mutations were combined from tumor samples of bladder cancer, cervical cancer, head and neck cancer, lung adenocarcinoma and lung squamous cell carcinoma. A mutation hotspot is located at coding position 931 that corresponds to protein position 311. ( D ) Selection coefficients estimated for the  MB21D2  gene in individual cancer types (dots) and for combined samples (cross). Broken lines are the mean selection coefficient of all genes analyzed using all TCGA samples. Shaded areas are the 95% confidence intervals of the mean selection coefficients Table 1. Performance of GUST and 20/20+ Binary classes Three classes Positive OG, TSG OG TSG Negative PG PG, TSG PG, OG GUST TPR 0.93 0.84 0.93 — TNR 0.94 0.98 0.95 — PPV 0.92 0.85 0.9 — NPV 0.95 0.98 0.96 — ACC 0.94 0.97 0.94 0.92 AUC 0.97 0.99 0.97 0.98 a 20/20+ TPR 0.90 0.95 0.86 — TNR 0.85 0.97 0.90 — PPV 0.82 0.78 0.81 — NPV 0.92 0.99 0.93 — ACC 0.88 0.97 0.89 0.86 AUC 0.94 0.97 0.93 0.95 a a Macro-AUC values were calculated by averaging three one-vs-rest ROC curves. Linear interpolation was used between points of ROC ( Wei and Wang, 2018 ). TPR, true positive rate, sensitivity; TNR, true negative rate, specificity; PPV, positive predictive value, precision; NPV, negative predictive value; ACC, accuracy; AUC, area under the ROC curve. To evaluate the concordance of GUST classifications with other methods that predict cancer drivers but do not distinguish OGs and TSGs, we computed a driver score by adding the OG and TSG scores of each gene. The TCGA PancanAtlas consortium reported a collection of putative driver genes based on consensus predictions from 12 computational methods ( Bailey  et al. , 2018 ). We first examined the 510 gene/cancer-type pairs (204 unique genes) predicted as drivers by ≥2 methods. In this permissive list, GUST predicted 373 pairs (73.1%, 145 unique genes) as drivers. We then examined the 283 gene/cancer-type pairs (109 unique genes) predicted as drivers by ≥3 methods. In this more stringent list, GUST predicted 254 pairs (89.8%, 96 unique genes) as drivers. These results showed that drivers predicted by GUST had a high agreement with existing methods while providing additional OG/TSG classifications. To measure the importance of each predictor in the random forest model, we computed the mean decreased Gini index by permuting out-of-bag samples ( Louppe  et al. , 2013 ). The most informative predictors are the selection coefficients and fraction of truncating mutations, followed by the selection coefficient and fraction of missense mutations ( Fig. 2B ). Interestingly, evolutionary conservation was not very informative, which may be because a vast majority of drivers are known to occur at highly conserved positions ( Dudley  et al. , 2012 ), providing a limited power to discriminate OGs and TSG. Although recurrence among patients has been taken as a surrogate of mutations under functional selection, recent investigations have shown that passenger hotspot mutations are common ( Buisson  et al. , 2019 ;  Hess  et al. , 2019 ). For example, multiple samples of various cancer types harbored a C-&gt;T or C-&gt;G mutation at position 931 of the  MB21D2  gene ( Fig. 2C ,  Supplementary Fig. S3 ). Buisson  et al.  discovered that this mutational hotspot is due to its location in a hairpin loop susceptible to mutagenesis and functions as a passenger ( Buisson  et al. , 2019 ). GUST analysis confirmed that the selection pattern of this gene was consistent with neutral evolution in individual cancer types and in the combined samples ( Fig. 2D ). Thus, GUST predicted the  MB21D2  gene as a PG correctly. This demonstrated the effectiveness of quantifying the contribution of genetic alterations to tumor fitness in cancer gene classifications. 3.3 Application to TCGA data We retrieved somatic mutations from whole-exome sequencing data of 10 172 TCGA tumor samples spanning 33 cancer types. We then removed low-quality mutations, hyper-mutated or hypo-mutated samples, genes with fewer than four protein-altering mutations and genes mutated in &lt;2% of tumors ( Supplementary Materials ). We applied GUST to the remaining 9663 samples. We predicted 161 OGs of which 98 were unique genes in 29 cancer types. We also predicted 331 TSGs of which 179 were unique genes in 33 cancer types ( Fig. 3A ,  Supplementary Tables S3 and S4 ). Fig. 3. GUST analysis of the TCGA samples. ( A ) Number of common and rare OGs and TSGs found in each cancer type. Abbreviations of cancer types are listed in  Supplementary Table S3 . ( B – E ) Positional distributions of somatic mutations in novel OGs and TSGs. Evolutionary conservation of each position, measured as number of substitutions per billion years is displayed above each plot. ( F ) Distribution of driver genes with different spectrum of tissue specificity. ( G ) Positional distribution of mutations in the  EGFR  gene in lung adenocarcinoma and glioma (low-grade glioma and glioblastoma combined). ( H ) Two-way clustering of driver genes and cancer types. Driver genes found in more than one cancer type are used (OGs in red and TSGs in blue). (Color version of this figure is available at  Bioinformatics  online.) 3.3.1 Novel driver genes The GUST-predicted drivers consisted of 55 putative OGs and 97 putative TSGs that were classified as PGs in the CGC database ( Sondka  et al. , 2018 ). Most (81.7%) of these new putative drivers were annotated in only one cancer type and had low probability scores. To estimate the confidence of each prediction, we computed the sensitivity and specificity of each one-vs-rest prediction based on the ROC curves. We then derived a list of high-confidence drivers consisting of 22 OGs with OG-vs-rest specificity ≥0.99 and 74 TSGs with TSG-vs-rest specificities ≥0.99, all of which had a PG-vs-rest sensitivity ≥0.99. This short list of high-confidence drivers included two novel OGs and 28 novel TSGs not annotated in the CGC. The two novel OGs ( CNOT9  in melanoma and  GTF2I  in thymoma) had single mutational hotspots disrupting highly conserved protein positions ( Fig. 3B and C ). The  GTF2I  mutant stimulates cell proliferation  in vitro  and has been associated with favorable prognosis of thymoma ( Roy, 2017 ). All of the novel TSGs had an overabundance of truncating mutations ( Supplementary Fig. S4 ). For example, frame-shifting mutations in  SOX9  were observed in 40 colon cancers ( Fig. 3D ). As an atypical tumor suppressor,  SOX9  has been shown to interact with nuclear β-catenin. Inactivation of  SOX9  causes loss of inhibition of the oncogenic Wnt/β-catenin signaling pathway and is associated with patient survivals ( Prevostel  et al. , 2016 ). Some novel TSGs harbor mutational hotspots. For instance, the N583fs frame-shifting mutation in  BMPR2  introduced premature stops of protein synthesis and was observed in nine stomach adenocarcinomas ( Fig. 3E ). We searched the literature and found supporting evidence of the tumor suppressing functions of 22 (78.6%) novel TSGs ( Supplementary Table S5 ). Many of these novel TSGs were also annotated as putative drivers by other computational methods ( Bailey  et al. , 2018 ). As an independent assessment of the validity of these predicted drivers, we examined how many of their mutations were in major clones and compared with PGs. The rationale is that genes frequently mutated in sub-clones may not suggest a selective advantage, but rather other mechanisms, such as increased background mutational rates. Specifically, we used SciClone ( Miller  et al. , 2014 ) to cluster mutations in each tumor based on variant allele frequencies. We considered the cluster with the highest variant allele frequencies as the major clone and the remaining clusters as sub-clones. For the 30 novel drivers, 93.2% of protein-altering mutations were in major clones, which was similar to the percentage (93.7%) for the 96 known drivers (Fisher’s exact test  P  =   0.54). For the 40 most frequently mutated PGs, a significantly lower percentage (89.9%) of protein-altering mutations were in major clones (Fisher’s exact test  P  =   10 −4 ). Therefore, these predicted drivers highly likely promote tumorigenesis. 3.3.2 Spectrum of tissue specificity Even after removing low-confidence predictions, most of the drivers annotated by GUST were engaged in only one cancer type, showing high tissue-specificities. Only 13 (59.1%) OGs and 25 (33.8%) TSGs in this high-confidence set are broad-spectrum drivers, promoting tumorigenesis in two or more cancer types ( Fig. 3F ). The most prevalent OG was the  PIK3CA  gene found in 15 cancer types with high confidence, followed by the  KRAS / NRAS / HRAS  genes found in 13 cancer types. The most prevalent TSG was the  TP53  gene found in 18 cancer types, followed by the  ARID1A  gene found in 10 cancer types. Furthermore, 11 out of the 13 broad-spectrum OGs possessed multiple hotspots (one-sided proportional test  P  &lt;   0.05 after Bonferroni corrections,  Supplementary Fig. S5  and  Supplementary Methods ). For each significant hotspot, we examined the affected functional domains as annotated in the NCBI Gene database. A representative example is the  EGFR  gene. In lung adenocarcinoma, 48% of missense mutations clustered at a single mutational hotspot affecting the tyrosine kinase activation loop ( Fig. 3G ). In glioma, only one mutation hit this loop (chi-square test  P  &lt;   10 −18 ), and 69.3% of all missense mutations clustered at two hotspots affecting the extracellular domains independent of kinase activities. The contextual selection of mutations averting the kinase catalytic domain in glioma suggests an alternate path of activating  EGFR  signaling. In fact, several studies have reported the associations of these hotspot mutations with different levels of  EGFR  activities ( Kamburov  et al. , 2015 ;  Niu  et al. , 2016 ;  Porta-Pardo  et al. , 2017 ). For cancer management, although tyrosine kinase inhibitors blocking  EGFR  are common in the therapeutic armamentarium of lung cancer ( Grigoriu  et al. , 2015 ;  Takeda and Nakagawa, 2019 ), these agents have not been successful in treating glioma even with improved drug delivery techniques to penetrate the blood–brain barrier ( Bethune  et al. , 2010 ;  Vivanco  et al. , 2012 ;  Westphal  et al. , 2017 ). These findings suggest a potential direction to investigate and enhance current treatment regimen. Interestingly, each of the 33 cancer types engaged at least one broad-spectrum driver and multiple tissue-specific drivers, implicating the synchrony of convergent and divergent disease pathways. Clustering of cancers based on broad-spectrum driver genes grouped cancer types largely matching their tissue and cellular origins ( Fig. 3H ). 4 Discussions Distinguishing OGs and TSGs in individual cancer types is critical to understanding cancer etiology and pinpointing clinically actionable targets. In this study, we proved that protein-coding mutations in OGs and TSGs are under different somatic selection, and subsequently developed the GUST method to discover cancer-type specific functions of cancer driver genes. We compared GUST with the 20/20+ method that is the only available method to classify OGs and TSGs. Both GUST and 20/20+ employ a random forest model to integrate features extracted from tumor exomes. Despite that GUST uses only 10 features compared to 24 features in 20/20+, the accuracy of GUST is consistently higher. In the GUST model, selection measures contribute the most information content. In 20/20+, the  P -value of enrichment of inactivating mutations is the most informative feature. Interestingly, this feature is also related to selection, although it is not a strict evolutionary measure ( Kryazhimskiy and Plotkin, 2008 ;  Temko  et al. , 2018 ). These results suggest that using a small number of features engineered on evolutionary mechanisms is more powerful than feeding a large number of raw features to machine learning models. Furthermore, given the scarcity of known drivers for specific cancer types, reducing the number of features in predictive models helps mitigate overfitting problems. We acknowledge that a driverMAPS ( Zhao  et al. , 2019 ) method has been recently developed that estimates selection coefficients of a gene under three competing models (i.e. a PG, an OG and a TSG model). However, this method later combines the OG model and the TSG model into a driver model and contrasts it with the PG model to predict driver genes. Consequently, the reported posterior likelihood and false discovery rate are for the purpose of distinguishing drivers and passenger, but not OGs versus TSGs. Via personal communications with the authors of driverMAPS, we confirmed that this method does not provide statistical significance of OG and TSG classifications. Therefore, we did not compare GUST with driverMAPS. While we discovered many known and novel cancer driver genes, none of them showed dual OG/TSG roles with high confidence in our analysis. A straightforward explanation is that GUST makes predictions based on protein-altering substitutions and indels, thus it is unable to capture genes acting through other mechanisms, such as noncoding regulatory variants, copy number variants, translocations, fusions, differential expressions, post-translational modifications and epigenetic regulations. Further investigations will shed light on key switches that divert paths of dual-role drivers. We also note that genes with only a small number of mutations may cause non-convergence problems during maximum likelihood estimations of selection coefficients, which limits the application of GUST to discovering rare drivers. For practical use, we have built an online database ( https://liliulab.shinyapps.io/gust ) with pre-computed results of analyzing TCGA samples. Users can query the database and visually inspect somatic selection patterns and conservational patterns of selected genes. Combined with information showing if a gene has been annotated by CGC as a driver or a drug target, users can make informed decisions on prioritizing candidate genes for further investigations. The R implementation of the GUST algorithm is available on Github ( https://github.com/liliulab/gust ). 5 Conclusions Somatic selection is a quantitative measure of the impact of mutated genes on tumor fitness. The GUST method estimates these features directly from whole-exome sequencing or targeted sequencing data and pinpoints to genes and functional domains driving tumorigenesis in different cellular contexts. As gene-centered treatment and drug-repurposing attracts increasing interest, we expect this new method and the online database will facilitate discoveries of clinically actionable targets. Supplementary Material btz851_Supplementary_Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>A weighted sampling algorithm for the design of RNA sequences with targeted secondary structure and nucleotide distribution</Title>
    <Doi>10.1093/bioinformatics/btt217</Doi>
    <Authors>Reinharz Vladimir, Ponty Yann, Waldispühl Jérôme</Authors>
    <Abstract>Motivations: The design of RNA sequences folding into predefined secondary structures is a milestone for many synthetic biology and gene therapy studies. Most of the current software uses similar local search strategies (i.e. a random seed is progressively adapted to acquire the desired folding properties) and more importantly do not allow the user to control explicitly the nucleotide distribution such as the GC-content in their sequences. However, the latter is an important criterion for large-scale applications as it could presumably be used to design sequences with better transcription rates and/or structural plasticity.</Abstract>
    <Body>1 INTRODUCTION At the core of the emerging field of synthetic biology resides, our capacity to design and reengineer molecules with target functions. RNA molecules are well tailored for such applications. The ease to synthesize them (they are directly transcribed from DNA) and the broad diversity of catalytic and regulation functions they can perform enable to integrate  de novo  logic circuits within living cells ( Rodrigo  et al. , 2012 ) or reprogram existing regulation mechanisms ( Chang  et al. , 2012 ). Future advances and applications of these techniques in gene-therapy studies will strongly rely on efficient computational methods to design and reengineer RNA molecules. Most of RNA functions are, at least partially, encoded by the 3D molecular structures, which are themselves primarily determined by the secondary structures. The development of efficient algorithms for designing RNA sequences with predefined secondary structures is thus a milestone to enter the synthetic biology era.  RNAinverse  pioneered RNA secondary structure design algorithms. It has been developed and distributed with the Vienna RNA package ( Hofacker  et al. , 1994 ). However, only posterior experimental studies revealed the potential and practical impact of these techniques. Thereby, during the past 6 years, many improvements and variants of  RNAinverse  have been proposed. Conceptually, almost all of existing algorithms follow the same approach. First a seed sequence is selected, then a local search strategy is used to mutate the seed and find, in its vicinity, a sequence with desired folding properties. Using this strategy,  INFO-RNA  ( Busch and Backofen, 2006 ),  RNA-SSD  ( Aguirre-Hernández  et al. , 2007 ) and  NUPACK:Design  ( Zadeh  et al. , 2011 ) significantly improved the performance of RNA secondary structure design algorithms. More recent research studies aimed to include more constraints in the selection criteria.  RNAexinv  focused on the design of sequences with enhanced thermodynamical and mutational robustness ( Avihoo  et al. , 2011 ), while  Frnakenstein  enables to design RNA with multiple target structures ( Lyngsø  et al. , 2012 ). We recently introduced with  RNA-ensign  a novel paradigm for the search strategy of RNA secondary structure design algorithm ( Levin  et al. , 2012 ). Instead of a local search approach, we proposed a global sampling strategy of the mutational landscape based on the  RNAmutants  algorithm ( Waldispühl  et al. , 2008 ). This methodology offered promising performances, but suffered from prohibitive runtime and memory consumption. Following our work,  Garcia-Martin  et al.  (2013)  proposed  RNAiFOLD , an alternate methodology that uses constraint programming techniques to prune the mutational landscape. While also suffering from prohibitive running times, it is worth noting that this latter algorithm also proposes a seedless approach to the RNA secondary structure design problem. In this article, we introduce  IncaRNAtion , an RNA secondary structure design algorithm that benefits from our recent algorithmic advances ( Reinharz  et al. , 2013 ) to expand our original  RNA-ensign  algorithm ( Levin  et al. , 2012 ).  IncaRNAtion  addresses previous limitations of  RNA-ensign  and offers new functionalities. First, while our previous program had a running time complexity of  ,  IncaRNAtion  now runs in linear-time and space complexity, allowing it to demonstrate similar speeds as any local search algorithm. Next,  IncaRNAtion  is  seedless . Unlike  RNA-ensign,  it does not require a seed sequence to initiate its search. Finally,  IncaRNAtion  implements a novel algorithm based on weighted sampling techniques ( Bodini and Ponty, 2010 ) that enables us to control, for the first time,  explicitly  the GC-content of the solution. This functionality is essential because wild-type sequences within living organisms often present medium or low GC-content, presumably to offer better transcription rates and/or structural plasticity. Previous programs do not allow to control this parameter and tend to output sequences having high GC-contents ( Lyngsø  et al. , 2012 ). We demonstrate the performance of our algorithms on a set of real RNA structures extracted from the RNA STRAND database ( Andronescu  et al. , 2008 ). To complete this study, we develop a hybrid method combining our global sampling approach with local search strategies such as the one implemented in  RNAinverse.  Remarkably, our glocal methodology overcomes both local and global approaches for sampling sequences with a specific GC-content and target structure. 2 METHODS We introduce a probabilistic model for the design of RNA sequences with a specific GC-content and folding into a predefined secondary structure. For the sake of simplicity, we choose to base this proof-of-concept implementation on a simplified free-energy function  , which only considers the contributions of stacked canonical base pairs. We show how a modification of the dynamic programming scheme used in  RNAmutants  allows for the sampling of good and diverse design candidates, in linear time and space complexities. 2.1 Definitions A targeted secondary structure   of length  n  is given as a non-crossing arc-annotated sequence, where   stands for the base-pairing position of position  i  in   if any (and, reciprocally,  ), or −1 otherwise. In addition, let us denote by   the number of occurrences of G and C in an RNA sequence  s . 2.1.1 Simplified energy model We use a simplified free-energy model, which only includes additive contributions from stacking base pairs. Using individual values from the Turner 2004 model [retrieved from the NNDB ( Turner and Mathews, 2010 )]. Given a candidate sequence  s  for a secondary structure  , the free energy of any sequence  s  of length   is given by
 
where   is set to 0 if   (no base pair to stack onto), the tabulated free energy of stacking pairs   in the Turner model if available, or   for non–Watson-Crick/Wobble pairs (i.e. not in  ). This latter parameter allows one to choose whether to simply penalize invalid base pairs ( ), or forbid them altogether ( ). Position-specific sequence constraints can also be enforced at this level (details omitted for the sake of clarity) by assigning to   a   penalty (leading to a null probability) in the presence of a base incompatible with a user-specified constraint mask. 2.1.2 GC-weighted Boltzmann ensemble and distribution To counterbalance the documented tendency of sampling methods to generate GC-rich sequences ( Levin  et al. , 2012 ), we introduce a parameter  , whose value will influence the GC-content of generated sequences. For any secondary structure  , the GC-weighted Boltzmann factor of a sequence  s  is   such that
 (1) 
where  R  is the Boltzmann constant and  T  the temperature in Kelvin. Summing the GC-weighted Boltzmann factor over all possible sequences of a given length  , one obtains the GC-weighted partition function  , from which one defines the GC-weighted Boltzmann probability   of each sequence  s , respectively, such that
 (2) 
 2.2 Linear-time stochastic sampling algorithm for the GC-weighted Boltzmann ensemble Let us now describe a linear-time algorithm to sample sequences at random in the GC-weighted Boltzmann distribution. This algorithm follows the general principles of the recursive approach to random generation ( Wilf, 1977 ), pioneered in the context of RNA by the  SFold  algorithm ( Ding and Lawrence, 2003 ). The algorithm starts by precomputing the partition function restricted to each substructure occurring in the target structure, and then performs a series of recursive stochastic backtracks, using precomputed values to decide on the probability of each alternative. 2.2.1 Precomputing the GC-weighted partition function Firstly, a dynamic programming algorithm computes   the GC-weighted partition function (the dependency in  x  is omitted here for the sake of clarity) for a structure  , assuming its (previously chosen) flanking nucleotides are  a  and  b , respectively, either forming a closing base pair ( ) or not ( ). Remark that the empty structure only supports the empty sequence, having energy 0, so one has
 (3) 
 The general recursion scheme consists in three different terms, depending on the first position in  :
 Case 1 First position is unpaired  ( ):
 (4) 
 
 Case 2 First position is paired with last position  [ ], stacking onto a preexisting exterior pair ( ):
 (5) 
 
 Case 3 First position is involved in a base pair [ ], which is not stacking onto an exterior base pair  (  or  ):
 (6) 
 Remark that the number of combinations of  a ,  b  and   remains bounded by a constant, thus the complexity of computing   mainly depends on the values taken by   on subsequent recursive calls. Such values are entirely determined by   at any given step of the recursion, and their dependency can be summarized in a tree having  . Therefore, the computation of   requires   time and space using dynamic programming. 2.2.2 Stochastic backtrack Once the GC-weighted partition functions have been computed and memorized, a stochastic backtrack starts from the target structure   with any exterior bases   and no nesting base pair, corresponding to a call  SB x 
  to Algorithm 1. At each step, a suitable assignment for one or several positions is chosen, using probabilities derived from the precomputation, as illustrated by  Figure 1 . One or several recursive calls over the appropriate substructures are then performed. On each recursive call, the algorithm assigns at least 1 nt to a previously unassigned position. Moreover, the number of executions of each loop is bounded by a constant. Consequently, the complexity of Algorithm 1 is in   time and space.
 Fig. 1. Stochastic backtrack procedure for a given substructure  S . Either the first position is left unpaired (top), a base pair is formed between the two extremities, stacking onto an exterior base pair (middle) or paired without creating a stacking, defining two regions on which subsequent recursive calls are needed (bottom). For the empty structure (omitted here), the empty sequence is returned. Positions indicated in red are assigned at the current stage of the backtrack 2.2.3 Self-adaptive sampling strategy Let us remind that our goal is to produce a set of sequences whose GC-content matches a prescribed value  gc . An absolute tolerance κ may be allowed, so that the GC-content of any valid sequence must fall in  . Because sequences of arbitrary GC-content may be generated by Algorithm 1, we use a rejection-based approach ( Bodini and Ponty, 2010 ), previously adapted by the authors in a similar context ( Waldispühl and Ponty, 2011 ). This gives an algorithm that generates  k  valid sequences in expected time   when   [or   when κ is a positive constant] and memory in  . A complete analysis of the rejection process can be found in an earlier contribution ( Waldispühl and Ponty, 2011 ), but let us briefly outline the approach, and the main arguments used to establish its complexity. As summarized by  Figure 2 , our adaptive sampling approach simply generates sets of sequences by repeatedly running the stochastic backtrack algorithm. The average GC-content induced by the current value of the  x  parameter can then be adequately estimated from the sample, or computed exactly using recent algorithmic advances ( Ponty and Saule, 2011 ). The set of sequences is filtered to only retain valid sequences. The value of the parameter  x  is then adapted to match the average GC-content (induced by the value of  x ) with the targeted one. It can be shown that the expected GC-content is a continuous and strictly increasing monotonic function of  x , whose limits are 0 when  x  = 0 and  n  when  . Consequently, for any targeted GC-content  , there exists a unique value  x gc  such that generated sequences feature, on the average, the right GC-content. In practice, a simple binary search ( Waldispühl and Ponty, 2011 ) is used in our implementation, and typically converges after few iterations. An optimal value for  x  can also be derived analytically using interpolation after   evaluations of   for different candidate values of  x , as previously noted ( Waldispühl and Ponty, 2011 ), and could be implemented using the Fast-Fourier Transform ( Senter  et al. , 2012 ).
 Fig. 2. General workflow of our adaptive sampling algorithm ( Waldispühl and Ponty, 2011 ) 2.2.4 Overall complexity It was previously established ( Waldispühl and Ponty, 2011 ) that, for each value of  x , there exists constants   and   such that the distribution of GC-content asymptotically converges toward a normal law having expectation in   and standard deviation in  . Furthermore, the distribution of GC-content is highly concentrated, as asserted by its limited standard deviation; therefore, the expected number of attempts required to generate a valid sequence when   [respectively  ] grows like   [respectively  , i.e. a constant], leading to the announced complexities. Formally, because a suitable weight  x  must be recomputed for each targeted structure and GC-content, then the number  M  of iterations required for the converge can be accounted for explicitly, leading to time complexities in   (if  , i.e. without any tolerance) and   (if  ). 2.3 Postprocessing unpaired regions: a local/global (glocal) hybrid approach Owing to our simplified energy model, unpaired regions are not subject to design constraints other than the GC-content, leading to modest probabilities for refolded design candidates to match the targeted structure. To improve these performances and test the complementarity of our global sampling approach with previous contributions based on local search, we used the  RNAinverse  software to redesign unpaired regions. We specified a constraint mask to prevent stacking base pairs from being modified and, whenever necessary, reestablished their content  a posteriori , as  RNAinverse  has been witnessed to take some liberties with constraint masks. As shown in the  Supplementary Material , this postprocessing does not drastically alter the GC-content, so the glocal approach reasonably addresses the constrained GC-content design problem. 3 RESULTS 3.1 Implementation Our software,  IncaRNAtion,  was implemented in  Python 2.7 . We used  RNAinverse  from the  Vienna Package 2.0  ( Hofacker  et al. , 1994 ). All-time benchmarks were run on a single AMD Opteron(tm) 6278 Processor at 2.4 GHz with cache of 512 kb. The penalty β, associated with invalid base pairs, was set to 15. Figure 3  presents the average times spent running  IncaRNAtion  +  RNAinverse  to generate one sequence with the required GC-content. As expected, the time grows linearly in function of the length of the structures for  IncaRNAtion .
 Fig. 3. Average time in seconds to generate one sequence for  IncaRNAtion  and  RNAinverse 3.2 Dataset To evaluate the quality of our method, we used secondary structures from the RNA STRAND database ( Andronescu  et al. , 2008 ). Those are known secondary structures from a variety of organisms. We considered a subset of 50 structures selected by  Levin  et al.  (2012) , whose length ranges between 20 and 100 nt. To ease the visualization of results, we clustered together structures having similar length, stacks density and proportion of free nucleotides in loops, leading to distributions of structures shown in  Figure 4 .
 Fig. 4. Number of secondary structures per bin, according to our three clustering criteria 3.3 Design We ran our method as follows. First, we sampled approximately 100 sequences per structure. Then, we use these sequences as seed in  RNAinverse.  Finally, we computed the Minimal Free-Energy (MFE) with the  RNAfold  program from the  Vienna Package 2.0  ( Hofacker  et al. , 1994 ). Before starting our benchmark, we asses the need for our methods and performed an analysis of the GC-content drift achieved with state-of-the-art software. Using our dataset of 50 structures, we generated 100 samples per structure with classical softwares that do not control the GC-content. Namely,  RNAinverse, 
 INFO-RNA, 
 NUPACK:Design  and  Frnakenstein.  We show the distribution of the GC-content of the sequences produced with these softwares in  Figure 5 .
 Fig. 5. Overall GC-content distribution for sequences designed using  RNAinverse ,  INFO-RNA ,  NUPACK:Design  and  Frnakenstein  folding in the desired structure As anticipated, we observe a clear bias toward high GC-contents and a complete absence of sequence with &lt;30% of GC. This striking result motivates a need for methods that enable to explicitly control the GC-content and more precisely that enable to design sequences with low GC-content (i.e. ≤30%). To provide a complete overview of the performance of  IncaRNAtion , we provide additional statistics for these softwares in the  Supplementary Material . 3.4 Success rate We started by estimating the success rate of our methodology and computed the percentage of sequences with a MFE structure identical to the target secondary structure.  Figure 6  shows our results. We clearly see that before the postprocessing step (i.e.  RNAinverse ) the sequences sampled by  IncaRNAtion  have a low success rate (first row). As mentioned earlier, this could be explained by the fact that no selection criterion has been at this stage applied to unpaired nucleotides. Remarkably, after the local search optimization (with  RNAinverse ) of nucleotides in unpaired regions (second row), we observe a dramatic improvement of our success rate. As expected, we observed that length is, in general, not a good predictor for the hardness of designing a structure. Instead, a high number of free nucleotides in the structure seems to be a good measure of the hardness of its design. Similarly, these data also show that designing sequences with low GC-content is challenging for all types of targets.
 Fig. 6. Success rate  IncaRNAtion  before and after  RNAinverse  postprocessing. The first row shows the percentage of sampled sequences folding into the target when using only  IncaRNAtion . The second shows after processing previous results with  RNAinverse We investigated further the quality of the sequences generated by  IncaRNAtion.  In particular, we estimated the capacity of our methods to generate ‘good’ sequences with desired folding capabilities regardless of the property to fold exactly into the target structure. In  Figure 7 , we show the ratio of well-predicted base pairs in the MFE structure of our sampled sequences. As above, we can observe that, in all cases, the sequences that are the hardest to design are those with an extremely low GC-content. Indeed, the energetic contribution of the base pairs to the stability of the structure is weaker. Interestingly, we also notice that the most accurate sequences yield a GC-content of  . Overall, we observe that all our samples have good folding properties, and that there is a correlation between the ‘precision’ of the samples and the hardness of the design.
 Fig. 7. Structural sensitivity (i.e. number of well predicted base pairs/number of base pairs in target) of the sampled sequences MFE We noticed a highly decreased structural sensitivity for the sequences with 15% free nucleotides in the loops. However, one must remain careful interpreting this observation, as the structures within this class all originate from the PDB, and are relatively small (for the complete STRAND DB, the average length is   nt, compared with   nt around 15% unpaired bases). 3.5 Properties of designed sequences In this section, we further analyze the generated sequences with a MFE structure that folds into the target structure. A desirable feature in sequence design is to produce samples with a high sequence diversity and stable secondary structure. Therefore, in the following, we will use two useful measures, which are the sequence identity of the samples, and the Boltzmann probability of the target structure in the low-energy ensemble. The sequence identity is defined over a set   of aligned sequences (in our case, all sequences have the same length and can be trivially aligned) as follows:
 (7) 
where  s i  is the nucleotide at position  i  in sequence  s . Intuitively, this measure captures the diversity of sequences generated by a given method. Next, the Boltzmann frequency is defined for a structure   and a sequence  s  as follows:
 (8) 
where   is the partition function of sequence  s . This measure tells us how dominant is a structure   in the Boltzmann ensemble of structures over a sequence  s . A high value implies a stable structure. We compute this frequency with  RNAfold  from the  Vienna Package 2.0  ( Hofacker  et al. , 1994 ). Figure 8  shows the number of solutions generated (i.e. sequences with a MFE structure identical to the target structure). Here, we note that low GC-contents have a strong (negative) influence on the number of sequences generated, and in parallel also affect negatively the sequence diversity. This observation emphasizes the difficulty to design sequences with low GC-content. Once again, large percentages of free nucleotides increase the difficulty of the task.
 Fig. 8. Number of solutions generated with  IncaRNAtion  +  RNAinverse  on the first row and their average sequence identity on the second The thermodynamical stability of the target structure on the designed sequence is another important property when estimating the performance of RNA design algorithms. We estimate the quality of our solutions in  Figure 9 . First, we observe a slow decline of the structure stability (i.e. the frequency) when the target structure increases in size. Yet, for an average GC-content, the frequency stays &gt;10% even at size of 100 nt. Next, we note that for the most difficult target structures (i.e. the longer ones or those with high percentages of unpaired nucleotides in loops) the GC-content has a limited (almost null) influence on the stability of the target structure on the designed sequence. By contrast, this is less true for easiest and small structures with only few free nucleotides in internal loops.
 Fig. 9. Thermodynamical stability of the target structure. The curves report the average Boltzmann probability of the target structure (which is also the MFE structure) at various GC-contents with respect to the length of the target (left), density of stacked base pairs (centre) and number of unpaired nucleotides in loops (right) 3.6 Global sampling versus Local search versus Glocal approach To conclude this study, we estimate the impact of the design methodology on the performances. More precisely, we aim to determine the merits of a global sampling approach  (IncaRNAtion),  compared with a glocal procedure  (IncaRNAtion + RNAinverse)  and a local search methodology ( RNA-SSD ). To the best of our knowledge,  RNA-SSD,  beside  IncaRNAtion , is the only software that implements an explicit control of the GC-content. Here, we compare the running time and the sequence diversity of the solutions produced by each software. In addition, we focus on the design of sequences with low GC-contents (≤30%) as they are almost impossible to design with classical software ( Figure 5 ). Figure 3  shows the running time of each software. These data demonstrate the efficiency and scalability of our techniques. In particular, this figure suggests that our strategy has the potential to be applied efficiently for designing sequences on long (and difficult) target secondary structures at low GC-content—a task that could have not been achieved before due time requirements. Next, we show in  Figure 10  the average sequence identity achieved by the various methods. Our results show that at extremely low GC-contents (i.e. 10%),  IncaRNAtion  slightly outperforms  RNA-SSD,  while this advantage becomes less evident when the GC-content increases. Our experiments on higher GC-contents (i.e. ≥50%) showed that our glocal strategy and the local search approach perform similarly. Similarly, we did not find any clear evidence that a global, local or glocal approach outperforms others when we compare at the thermodynamical stability of the target structure (data not shown).
 Fig. 10. Sequence identity of  IncaRNAtion  and  RNAinverse  for 10 and 30% of GC 4 CONCLUSION In this article, we described a novel algorithm,  IncaRNAtion , for the RNA secondary structure design problem, i.e. the design of an RNA sequence adopting a predefined secondary structure as its minimal free-energy fold. Implementing a global sampling approach, it optimizes affinity toward the target secondary structure, while granting the user full control over the GC-content of the resulting sequences. This extended control does not necessarily induce additional computational demands, and we showed the linear complexity of both the preprocessing stage and the generation of candidate sequences for the design, allowing for the design of larger and more complex secondary structures in a matter of minutes on a single processor (e.g. ∼28 min for 100 candidate sequences for a ∼1500 nt 16S rRNA). We evaluated the method on a benchmark composed of target secondary structures extracted from the RNA STRAND database. We observed good overall success rate, with the notable exception of very low targeted GC-content (10%), and a good to excellent entropy within designed candidates. Finally, we implemented a hybrid approach, using the  RNAinverse  software as a postprocessing step for unpaired regions. This approach greatly increased the success rate of the method, allowing for the design of highly diverse candidates for almost all of the structures in our benchmark, while largely preserving the targeted GC-content. In the future, we would like to complement this study by further investigating the potential of hybrid local/global—or  glocal —approaches. A global sampling approach would capture the positive aspects of design, optimizing affinity toward a given structure while allowing the specification of expressive systems of constraints. Designed sequences would serve as a seed for a restricted local approach which, by breaking unwanted symmetries, would perform the negative part of the design, while ideally maintaining obedience to the constraints. Another perspective of this work is the incorporation of the full Turner energy model, which should in principle yield better designs for unpaired regions. 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>varDB: a pathogen-specific sequence database of protein families involved in antigenic variation</Title>
    <Doi>10.1093/bioinformatics/btn477</Doi>
    <Authors>Hayes C. Nelson, Diez Diego, Joannin Nicolas, Honda Wataru, Kanehisa Minoru, Wahlgren Mats, Wheelock Craig E., Goto Susumu</Authors>
    <Abstract>Summary: Infectious diseases are a major threat to global public health and prosperity. The causative agents consist of a suite of pathogens, ranging from bacteria to viruses, including fungi, helminthes and protozoa. Although these organisms are extremely varied in their biological structure and interactions with the host, they share similar methods of evading the host immune system. Antigenic variation and drift are mechanisms by which pathogens change their exposed epitopes while maintaining protein function. Accordingly, these traits enable pathogens to establish chronic infections in the host. The varDB database was developed to serve as a central repository of protein and nucleotide sequences as well as associated features (e.g. field isolate data, clinical parameters, etc.) involved in antigenic variation. The data currently contained in varDB were mined from GenBank as well as multiple specialized data repositories (e.g. PlasmoDB, GiardiaDB). Family members and ortholog groups were identified using a hierarchical search strategy, including literature/author-based searches and HMM profiles. Included in the current release are&gt;29 000 sequences from 39 gene families from 25 different pathogens. This resource will enable researchers to compare antigenic variation within and across taxa with the goal of identifying common mechanisms of pathogenicity to assist in the fight against a range of devastating diseases.</Abstract>
    <Body>1 INTRODUCTION Infectious diseases including AIDS, tuberculosis and malaria are collectively responsible for significant mortality and morbidity on a global scale. While the individual pathogens come from a range of divergent taxa, they share the common mechanisms of antigenic variation and drift to avoid clearance by the host immune system. These mechanisms are ruled by two evolutionary pressures: maintaining protein function and varying the antigens present within the proteins. For example, influenza undergoes antigenic drift and therefore requires continual adjustment to the vaccine formulation (Oxford  et al. ,  2003 ), whereas  Plasmodium  uses surface antigen switching to maintain a pool of surface receptors that vary sufficiently to avoid the development of cross-reactive antibodies (Kaviratne  et al. ,  2003 ). The large size of these protein families, combined with the rapid evolution of the variant genes within strains, leads to an immense collection of unique antigens. varDB was developed to serve as a centralized database of gene families involved in antigenic variation. While there are a number of pathogen- or disease-specific sequence databases, e.g. PlasmoDB (Bahl  et al. ,  2003 ) and the Los Alamos HIV Database (Hraber  et al. ,  2007 ), to our knowledge there is no database specifically dedicated to antigenic variation that encompasses multiple taxonomic groups. However, common strategies for generating phenotypic diversity in response to immune defenses have been noted among diverse pathogens (Deitsch  et al. ,  1997 ). Consequently, there are a number of potential advantages to having a common repository of antigenic variable sequences including: (i) the ability to compare common mechanisms across diverse taxa, (ii) centralized access to an expanding repertoire of isolate and genome project data and (iii) the development of specialized tools for the analysis of hyper-variable nucleic acid and protein sequences. 2 DATA COLLECTION Pathogens of medical and veterinary importance demonstrating antigenic variation were selected for inclusion in varDB.  Table 1  shows several representative taxa included in the database, and additional pathogens will be added with continued development. Table 1. Selected organisms and antigenic variation gene families in varDB Taxonomic group Species Disease Gene families N o  sequences Bacteria Anaplasma spp. Anaplasmosis msp2 141 Ehrlichia spp. Canine ehrlichiosis msp4 94 Fungi Pneumocystis carinii Pneumonia msg 73 Helminthes Echinococcus granulosus Cystic echinococcosis antigen B 153 Protozoa Giardia lambia Giardiasis vsp 384 Plasmodium spp. Malaria var, rifin/stevor, pir 23,719 Trypanosoma brucei African trypanosomiasis vsg 221 Viruses HIV-1 HIV/AIDS env, gag, nef 3466 The identification of antigenic variant gene families was based on published reports in the scientific literature. Sequences and annotations were downloaded from DDBJ/EMBL/GenBank based on taxon-specific identifiers and separated either as isolate-specific or genome project sequences. Isolate sequences were collected based on sequence similarity profiles, and keyword searches based on author, publication and gene/protein annotations. Genome sequences were searched for similarity to the target gene families using HMM models. GenBank records for some genome sequencing projects may be out of date due to progressive re-annotation following initial submission. In this case, current data were retrieved directly from project or species-specific centers (e.g. the Broad Institute Microbial Sequencing Center). A detailed description of the process is available in Figure S1. varDB will be updated on a regular basis as new genome sequences and field isolates become available. 3 DATA ANALYSIS The main functionalities of varDB include querying and retrieving antigenic variation sequence data for comparative analysis. To this end, a keyword search and local BLAST database permit querying, sorting and filtering of sequences using a variety of criteria, e.g. species, strain, chromosome, Pfam domain, etc. (Figure S2). Filtering by collection date or geographic region is also possible and may provide insight into the degree of sequence variation within and among populations as well as over time. Using the integrated shopping cart tools, sequences can be selected and organized into subsets and aligned using MAFFT (Katoh  et al. ,  2005 ). Precomputed protein and DNA alignments can be downloaded or viewed online using either Jalview (Clamp  et al. ,  2004 ) or a browser-based alignment viewer. Sequences can be downloaded in several standard formats, and sequence annotations can be downloaded in a tab-delimited property file. 4 SYSTEM IMPLEMENTATION Sequences and annotation data are stored in a PostgreSQL ( http://www.postgresql.org/ ) database. varDB is deployed on a JBoss application server ( http://www.jboss.org ) running under Linux. The Ajax-driven web interface is based on open source tools including BioJava ( http://biojava.org/ ), the Spring Framework ( http://www.springframework.org/ ) and the Ext JS library ( http://extjs.com/ ). 5 PERSPECTIVES Infectious diseases and their pathogens are often studied in isolation, but observations made in one species may yield insights in other taxa. The ability to perform comparative analyses over diverse taxa should assist in elucidating biological mechanisms of pathogenicity and increase our ability to identify new biological pathways, drug targets and therapeutic interventions. This version of varDB provides a framework for retrieving and storing sequences from different gene families from diverse species and multiple data sources. Building upon this foundation, varDB will expand the breadth and depth of the sequence coverage as well as specific tools for the analysis of antigenic variation. This new resource should assist in efforts to understand the fundamental biological mechanisms behind the observed pathogenicities as well as increase our ability to combat these devastating diseases. 
 Supplementary Material 
 
 
 [Supplementary Data] 
 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>GlycanFormatConverter: a conversion tool for translating the complexities of glycans</Title>
    <Doi>10.1093/bioinformatics/bty990</Doi>
    <Authors>Tsuchiya Shinichiro, Yamada Issaku, Aoki-Kinoshita Kiyoko F, Valencia Alfonso</Authors>
    <Abstract/>
    <Body>1 Introduction Glycans are biomolecules that consist of a small number (2–20) of monosaccharide residues connected by glycosidic linkages. The term polysaccharide is typically used to denote any linear or branched polymer consisting of monosaccharide residues, such as cellulose. Thus, the relationship of monosaccharides to oligosaccharides or polysaccharides is analogous to that of amino acids to proteins, or nucleotides to nucleic acids ( Varki  et al. , 2009 ). Diverse structures can be created by simply linking different monosaccharides through glycosidic linkages, to make oligosaccharides or polysaccharides ( Fig. 1 ). The common classes of glycans are primarily defined according to the nature of the linkage to the aglycone (protein or lipid). A glycoprotein is a glycoconjugate in which a protein carries one or more glycans covalently attached to the polypeptide backbone, usually via  N - or  O -linkages in mammalian organisms. An  N -glycan makes a glycosidic linkage with the side-chain nitrogen of an asparagine residue that is a part of a consensus peptide sequence NX(S/T). An  O -glycan makes a glycosidic linkage with the terminal oxygen of a serine or threonine residue ( Packer  et al. , 2017 ).
 Fig. 1. Illustration of various diverse glycans. These glycans are represented based on the symbolic rules of Structure Nomenclature for Glycans (SNFG). ( A ) The basic  N -Glycan core structure. ( B ) Glycan fragments. ( C ) Cross-linked substituent. ( D ) Repeating structure. ( E ) Monosaccharide compositions. ( F ) Cyclic structure Many bacterial glycoconjugates are associated with the cell envelope and cell surfaces, and some are essential for viability. Surface glycoconjugates drive a variety of important interactions with elements of the host innate and adaptive immune defenses. They show a wide diversity in structures and often feature sugars not found elsewhere in nature. Antigens of many pathogenic microorganisms have carbohydrate origin, and recognition of bacteria by the host immune system is determined by the structure of these compounds. The bacterial saccharide sequences have a greater diversity of monosaccharides, with certain monosaccharides being specific to certain groups of bacteria. Bacterial glycan structures form a very complicated structure that is unlike in any other organisms. Therefore, it is difficult to visualize and encode these structures. Due to the development of glycan databases, several formats for representing glycan structures have been developed. Many of these formats represent glycans using a connection table, or adjacency matrix. The features of the glycan sequence formats utilized in this study are shown in  Table 1 . ‘Style’ indicates the format of each representation, which is either linear text or a connection table. Usually, a connection table format divides monosaccharides and linkages, whereas linear text is a single string representing the glycan structure. ‘Repeating units’ indicates whether it is possible to handle a repeating glycan structure ( Fig. 1D ). ‘Cyclic units’ indicates whether it is possible to handle cyclic glycan structures ( Fig. 1F ). ‘Structural ambiguity’ indicates whether it is possible to handle ambiguous structures such as glycan fragments ( Fig. 1B ). ‘Support for non-monosaccharides’ indicates whether it is possible to handle aglycones such as amino acids. Modifications such as N-acetylation of monosaccharides does not apply to this. ‘Rare monosaccharides’ indicates whether it is possible to accurately represent non-mammalian monosaccharides such as ‘Lgro-L3, 9dmanNon-2-ulop5N7NFormyl-onic’. ‘Human readability’ indicates whether the text is in general human readable. This usually applies to linear text formats. Table 1. Comparison with text formats Format Style Repeating units Cyclic units Structural ambiguity Support for non- monosaccharides Rare monosaccharides Human readability IUPAC linear text 
 ✓ 
 
 ✓ 
 KCF connection table 
 ✓ 
 
 ✓ 
 
 ✓ 
 
 ✓ 
 LinearCode linear text 
 ✓ 
 
 ✓ 
 
 ✓ 
 
 ✓ 
 
 ✓ 
 GlycoCT{Condensed} connection table 
 ✓ 
 
 ✓ 
 
 ✓ 
 
 ✓ 
 WURCS linear text 
 ✓ 
 
 ✓ 
 
 ✓ 
 
 ✓ 
 
 ✓ 
 The International Union of Pure Applied Chemistry—International Union of Biochemistry and Molecular Biology (IUPAC—IUBMB) has specified the ‘Nomenclature of Carbohydrates’ to describe complex oligosaccharides based on a three-letter code to represent monosaccharides ( McNaught, 1997 ;  Sharon, 1986 ). Each monosaccharide code is preceded by the anomeric descriptor and the configuration symbol. The ring size is indicated by an italic  f  for furanose or  p  for pyranose. The carbon numbers that link the two monosaccharide units are given in parentheses between the symbols separated by an arrow. The KEGG Chemical Function (KCF) format for representing glycan structures was originally used to represent chemical structures in Kyoto Encyclopedia of Genes and Genomes (KEGG) ( Hashimoto  et al. , 2006 ;  Hattori  et al. , 2003 ). It represents the first published sequence format for saccharides which uses a connection table approach. KCF uses the graph notation, where nodes are monosaccharides and edges are glycosidic linkages. The NODE section of the format describes the building blocks (monosaccharides) sequentially. The list is numbered and its members constitute the nodes of a graph. The EDGE section describes the linkages (glycosidic linkages) between the entries defined in the NODE section. The KCF description includes X, Y coordinates for each node, used for drawing purposes. An extension of the format allows repeating structures to be encoded, using an additional BRACKET section. LinearCode ®  is a new syntax for representing glycoconjugates and their associated molecules in a simple linear fashion. This format is used to represent glycans in the Consortium Functional Glycomics (CFG) ( Raman  et al. , 2006 ). It uses a single letter code to represent each monosaccharide and includes a condensed description of the connections between monosaccharides ( Banin  et al. , 2002 ). Modifications to the common structure are indicated by specific symbols. For example, D-Galp is the common form of galactose, thus its code A is used alone. However, if it is in a furanose form, it would be written as A’. Linkage information is represented using the symbols a and b for  α  and  β , respectively. This is followed by the carbon number of the parent to which the residue is attached. GlycoCT was developed as a part of the EuroCarbDB project ( Herget  et al. , 2008 ;  Lieth  et al. , 2010 ;  Ranzinger  et al. , 2008 ). This format is used in UniCarbKB ( Campbell  et al. , 2014 ) and ExPASy ( Artimo  et al. , 2012 ). GlycoCT uses a similar graph concept to the KCF format and consists of two varieties: a condensed format and an XML format. The monosaccharide namespace consists of five components and basically follows those defined by IUPAC: the base type, anomeric configuration, the monosaccharide name with configurational prefix, chain length indicator, ring forming positions and further modification designators. Trivial names such as fucose or rhamnose are not permitted in GlycoCT. Because GlycoCT is based on very complicated nomenclatures, it is difficult for humans to describe. Thus, EUROCarb has published MolecularFramework. MolecularFramework is an open source software that converts glycan texts such as KCF and IUPAC to GlycoCT. Web3 Unique Representation of Carbohydrate Structures (WURCS) was proposed as a new linear notation for representing carbohydrates for the Semantic Web during the GlyTouCan project ( Aoki-Kinoshita  et al. , 2016 ). This format supports the representation of nonstandard monosaccharide units as a part of the glycan structure, as well as compositions, repeating units and ambiguous structures where linkages/linkage positions are undefined ( Tanaka  et al. , 2014 ). Moreover, WURCS was updated to additionally handle ambiguous monosaccharide structures ( Matsubara  et al. , 2017 ). WURCS represents monosaccharides by using a ResidueCode, which is similar to the Extended Stereocode used for representing monosaccharides in MonosaccharideDB ( http://www.monosaccharidedb.org/ ). The aim of GlyTouCan is to simplify the identification of glycan structures and to help link related research with the many life sciences databases available worldwide. A unique accession number is generated to every unique glycan structure, which can be used for reference in any glycan-related research or publications. Thereby, WURCS is designed to represent a wide variety of glycan structures in linear notation, but, it has poor human-readability. It was considered that there was a need to provide a human-readable format translated from WURCS on GlyTouCan. Since IUPAC is the most widely used human-readable format, there was a need for a converter for glycans to/from WURCS. In this paper, we report the development of GlycanFormatConverter. This tool implements two types of conversion functions between IUPAC and WURCS. This conversion functionality can be executed with IUPAC-Extended, KCF, LinearCode ®  and WURCS. To verify the conversion accuracy, we did the following:
 The glycans registered in KEGG and CFG were converted to WURCS. All glycans registered in GlyTouCan were converted to IUPAC and reconverted to WURCS. We compared whether the reconverted WURCS matches the WURCS registered in GlyTouCan. As a result, we were able to show that the conversion between WURCS and IUPAC implemented in GlycanFormatConverter succeeded to encode glycan sequences with a wide variety of glycan structures and indicated high conversion accuracy. GlycanFormatConverter has been developed as an open source tool, and the source code has been released at  https://github.com/glycoinfo/GlycanFormatConverter.git . 2 Materials and methods 2.1 Development environment and resources The GlycanFormatConverter was developed utilizing Java version 7, and it has been tested on Mac (OS X version 10.12.6). We used the WURCSFramework library to implement WURCS input/output functionalities. WURCSFramework is an open source library developed in Java and generates three types of data structures from a WURCS string ( https://github.com/glycoinfo/wurcsframework.git ). 2.2 Development of the GlycanFormatConverter First, we designed the data structure to handle glycan structures. We named this data structure  GlyContainer  and show the architecture in  Figure 2 .  GlyContainer  defines components of carbohydrates using basically two types of objects.  Node  is the most basic object, and it is a superclass of  Monosaccharide  and  Substituent . On the other hand,  GlycanUndefinedUnit  is an object for handling ambiguous information for any building blocks.
 Fig. 2. The architecture of GlyContainer, the main object used in GlycanFormatConverter. Each box indicates a class in GlyContainer. The top field lists the name of the class. The bottom area lists the member(s) defined in the class. Italic members are primitive data types. Non-italic members are other classes. Arrows indicate a dependency between objects. Dotted arrows indicate an inheritance relationship. For example, Node is inherited by both Monosaccharide and Substituent 
 Monosaccharides  have the following members:  AnomericStateDescriptor ,  anomeric position ,  Stereo ,  SuperClass ,  Modifications ,  RingStart  and  RingEnd .  AnomericStateDescriptor  enumerates the anomeric states as  α ,  β  or unknown.  SuperClass  enumerates the size of monosaccharide, which should be between a value between 3 and 10 indicating the number of carbons forming the monosaccharide backbone.  Stereo  stores the most basic monosaccharide three letter code ( Supplementary Table S2.1 ).  GlycoModification  is an object for defining the molecular state of each backbone carbon, such as deoxy. This object contains a binding position and a  ModificationTemplate  listing molecular information about the modification. Repeating units and substituent information such as N-acetyl groups are handled by  Substituent . Substituent notation is stored in two types of dictionaries in  SubstituentTemplate  and  CrossLinkedTemplate , and these dictionaries are integrated into  SubstituentInterface .  SubstituentTemplate  enumerates single linkage substituents. This object can support 42 types of substituents ( Supplementary Table S2.2 ).  CrossLinkedTemplate  enumerates multiple-linkage substituents. This object can support 15 types of substituents ( Supplementary Table S2.3 ).  Edge  stores nodes and linkages for both donor and acceptor.  Linkage  has an acceptor side position, probability annotation and LinkageType.  LinkageType  enumerates the molecular state of each linkage. 2.3 Implementation of text conversion functionality We implemented four types of sequence parsers including IUPAC-Extended, KCF, LinearCode ®  and WURCS in the GlycanFormatConverter. In each sequence parser, each format was broken down into node or edge, and these notations were utilized to generate the GlyContainer. This object was utilized to output WURCS, IUPAC-Short, IUPAC-Condensed and IUPAC-Extended. In the implementation of the WURCS parser and export functionalities, we utilized objects defined in the WURCSFramework library. WURCSFramework provides three types of objects including WURCSArray, WURCSSequence2 and WURCSGraph, which can all be constructed from a WURCS string. WURCSGraph supports many detailed molecular states of monosaccharides, so it was most suitable to implement the IUPAC conversion functionality. Thus, we attempted to build an object converter between GlyContainer and WURCSGraph. Also, WURCS conversion functionality utilized methods in WURCSFramework. 2.4 Extension of the IUPAC nomenclature In this study, we encoded IUPAC based on the ‘Nomenclature of Carbohydrates’. However, IUPAC cannot represent all the glycan structures registered in GlyTouCan, and it was difficult to encode ambiguous glycan structures such as glycan fragments. Therefore, we needed to devise an extension to IUPAC to handle these unique glycan structures. For cyclic structures, glycan fragments and cross-linked substituents, the nomenclature utilized in CFG and the Complex Carbohydrate Structure database (CCSD) ( Doubet  et al. , 1989 ) was adapted to IUPAC. We show examples of expanded IUPAC notation in  Table 2 . Monosaccharide compositions ( Fig. 1E ) are represented by surrounding each building block by parentheses, followed by their cardinality. Cyclic structures ( Fig. 1F ) use parentheses to represent the cyclic position. Cross-linked substituents ( Fig. 1C ) are indicated by inserting modification information within the glycosidic linkage. Glycan fragments ( Fig. 1B ) are shown by assigning an ID to each fragment. Each ID is also assigned to the monosaccharide to which the fragment can bind. If more than one bond exists between two monosaccharides, the linkage information is separated by a colon. Probability annotation is indicated by inserting probability information within the glycosidic linkage. When the substituent is probabilistic, the probability is indicated in the linkage position of the substituent. More detailed rules for describing these glycan structures with IUPAC are listed in  Supplementary Section S1 . Table 2. Examples of our proposed extension of IUPAC sequences for glycans that cannot be represented by the original IUPAC recommendation format Type of glycan Example sequences Compositions {?-?-HexNAc-(? →}5,{?-?-Hexp??-(1 →}2,{?-?-Hexp-(1 →}4 Cyclic 6)- α -D-Glcp-(1 →6)- α -D-Glcp-(1 →6)- α -D-Glcp-(1 →6)- α -D-Glcp-(1 → Cross-linked substituent 
 β -D-Glcp-(1-S →4)- β -D-Glcp(1 → Fragments ?-D-Neup5Gc-(2 →3)=1$, 1$?-D-Galp-(1 →3)[1$?-D-Galp-(1 →4)-1$?-D-GlcpNAc-(1 →6)]-1$?-D-GalpNAc-(1 → Multiple linkages 
 α -D-Neup5Ac-(2 →8: 1 →9)- α -D-Neup5Ac-(2 → Probabilistic components 
 α -D-ManpNAc-(1 →4)[ β -D-GlcpNAc-(1 →30%3)-[4)- β -D-ManpA2NAc-(1 →4)- β -D-GlcpNAc-(1 →6)]- α -D-GlcpNAc-(1 →]n 2.5 Validation of conversion accuracy We attempted to verify the accuracy of the conversion function between IUPAC and WURCS. First, 98 925 text strings of WURCS were collected from GlyTouCan, converted to IUPAC-Extended and the converted IUPAC was reconverted to WURCS. The reconverted WURCS was compared with the WURCS before conversion to see if they completely matched. To verify the conversion accuracy to/from KCF, we searched the glycan structures linked with KEGG GLYCAN in GlyTouCan, and as a result, the KCF formats of 10 370 glycan structures were obtained. We converted the KCF collected from KEGG into WURCS and compared it with the corresponding WURCS registered in GlyTouCan. To verify the conversion accuracy to/from LinearCode, 1217 glycan structures were collected from GlycomeAtlas ( Konishi and Aoki-Kinoshita, 2012 ). In GlycomeAtlas, GlyTouCan accession numbers and their LinearCode are assigned to each registered glycan structure. We converted the LinearCode to WURCS and compared it with the WURCS registered in GlyTouCan. 3 Results 3.1 Validation of the conversion accuracy of KCF to WURCS We succeeded in converting 10 321 glycans in KCF format into WURCS ( Fig. 4A ). Of these, the majority, 9195, completely matched with the corresponding WURCS registered in GlyTouCan. On the other hand, 1126 WURCS sequences did not match. We found that among these, 343 actually did not have a link to GlyTouCan, so they could not be compared with the converted WURCS. For the remaining 783 that were linked to GlyTouCan, we analyzed the causes of this discrepancy ( Fig. 3 ). Examples of the mismatched structures are listed in  Supplementary Section S3.1 .
 Fig. 3. The causes of mismatches between the 783 out of 10 370 WURCS converted from KCF. This validation found six types of problems from 783 glycans. ‘Ignored’ refers to a lack of representation for building blocks such as monosaccharides or substituents in the process of conversion. ‘Mismatched’ refers to a difference of notation between the original WURCS and the converted WURCS There were basically six causes of mismatches, and each cause could be classified as ‘Ignored’ or ‘Mismatched’. ‘Ignored’ indicates that some structural information of WURCS converted from KCF was lost, and these were broken down into ignored modifications, substituents and reducing end. ‘Ignored modification’ indicates that the WURCS had monosaccharides that had lost the specific state of molecules, such as deoxy. ‘Ignored substituent’ indicates that monosaccharides had lost substituents such as sulfate groups. ‘Replaced reducing end’ indicates that there were different numbers of monosaccharides between KEGG and GlyTouCan, usually due to different representations of the reducing end. ‘Mismatched’ indicates that the structural information of WURCS converted from KCF was different from the WURCS in GlyTouCan. These were broken down into mismatched anomer, configuration and ring size. ‘Mismatched anomer’ indicates that the anomeric state of one or more monosaccharides were different. ‘Mismatched configuration’ indicates that the stereoisomer of one of more monosaccharides were different. ‘Mismatched ring size’ indicates that the ring size of one or more monosaccharides were different. Of the 10 370 KCF text, 49 could not be converted to WURCS. Among these, non-monosaccharides such as amino acids were represented in places other than the root. GlycanFormatConverter could not handle such non-monosaccharides if they were not substituents. 3.2 Validation of the conversion accuracy of IUPAC to WURCS The results of validation of WURCS to IUPAC to WURCS conversion are shown in  Figure 4B . We succeeded in reconverting 83 300 glycans to WURCS, among which 80 760 WURCS matched perfectly with the original WURCS from GlyTouCan. On the other hand, 15 524 WURCS strings failed in the conversion to IUPAC, and five types of IUPAC strings failed in the conversion back to WURCS. Examples of these mismatched glycans are shown in  Supplementary Section S3.2 . In this section we categorize the types of mismatches that occurred in the conversion of glycan structures and describe the details of their causes. We show details of the causes of the errors in the conversion between ‘IUPAC to WURCS’ and ‘WURCS to IUPAC’ in  Table 3 . For each cause of error we categorized them into either ‘Conversion issue’ or ‘Format limitation’. Conversion issue essentially stemmed from an issue in the WURCSFramework library, whereas Format limitation indicates that information difficult to handle in the IUPAC format was included in the glycan structure. The majority of conversion errors in WURCS to IUPAC were due to unsupported substituents: ‘Unsupported substituent’, ‘Unsupported substituent with double anchor’, ‘Unsupported substituent with multiple anchor’ and ‘Unsupported cross-linked substituent’. We identified 3478 types of substituents from these errors. Also, some errors occurred in the conversion from ResidueCode to its trivial monosaccharide name. ‘Invalid superclass’ occurred in reading 321 glycans containing monosaccharides composed of 10 or more backbone carbons such as a 2112x22122h. Conversely, ‘Unsupported residue’ occurred in reading 623 glycans containing monosaccharides composed of less than 10 backbone carbons such as AOCm. ‘Opposite linkage’ occurred in four glycans, where the relationship between donor and acceptor was reversed in WURCS. The reversal of the order of monosaccharides was often seen with phosphate cross-linked with gulose. This is a problem with the WURCSFramework library not able to handle this case, about which we have notified the developers. The following two types of glycan structures failed during the import into WURCSFramework. ‘Unsupported substituent’ occurred in glycans with carboxyl ethyl groups. More details about these errors and possible fixes are provided in the  Supplementary Section S3.3 . Fig. 4. Conversion result of GlycanFormatConverter. ( A ) 9195 out of 10 321 glycans were successfully converted between KCF and WURCS. Among the rest, 783 did not match, 343 could not be verified and 49 resulted in errors. ( B ) 83 300 out of 98 829 glycans were successfully converted between IUPAC to WURCS. Among the rest, 80 760 matched, 2540 did not match and 15 529 resulted in errors Table 3. A list of conversion errors that arose for ‘WURCS to IUPAC’ and ‘IUPAC to WURCS’ Conversion Type of error Error No of Glycans Error in WURCSFramework 2 Conversion issue Invalid superclass 321 Unsupported residue 623 Opposite linkage 4 WURCS to IUPAC Unsupported double cross-linkage substituent 489 Format limitation Unsupported substituent 13 952 Unsupported substituent with double anchor 3 Unsupported substituent with multiple anchors 132 IUPAC to WURCS Conversion issue Unsupported substituent 3 Total number of glycans utilized for conversion 98 829 In  Figure 5 , we show the causes of mismatches found in the 2540 glycan structures that did not match. ‘Mismatched double cross-linkage position’ indicates a difference in linkage position that occurred in cross-linked substituents containing a pyruvic acid. ‘Mismatched saturation’ indicates that the saturation of some monosaccharide was different. For example, the monosaccharide represented as AEe22h was reconverted to AFf22h because IUPAC does not make such distinctions. ‘Mismatched monosaccharide’ indicates that the composition of the ResidueCode was different in some monosaccharide. As an example, consider the ambiguous monosaccharide represented as a26h-1b_1-4. This monosaccharide is translated to erythrose (Ery) in IUPAC, which was reconverted to the less ambiguous form a22h-1b_1-4 in WURCS, resulting in the mismatch. This mismatched also occurred in many glycans containing methylated monosaccharides. ‘Mismatched substituent’ indicates that the notation of the substituent was different in the reconverted WURCS. ‘Conversion problem’ indicates that two or more of the above issues arose during the conversion process.
 Fig. 5. The causes of mismatches between the 2540 out of 98 829 WURCS converted from IUPAC and the original WURCS from GlyTouCan. There were six types of mismatches: Mismatched linkage position, Mismatched saturation, Mismatched monosaccharide, Mismatched substituent and some other Conversion problem. There were no mismatches due to missing structure information as seen in the conversion from KCF to WURCS In addition, out of the 2540 glycan structures that were mismatched, the majority, 1973, were originally monosaccharide compositions with linkage information, which contain glycosidic linkages on the acceptor side of each monosaccharide. Compared to monosaccharide compositions without linkage information, this form of composition indicates whether or not the glycan has the possibility of being cyclic or contains multiple glycosidic linkages between the same two monosaccharides. It also enables the accurate calculation of its mass. Unfortunately, IUPAC is unable to represent such monosaccharide compositions, as it automatically assumes that the glycan is a ‘tree’ structure. Therefore, these glycans will never match. 3.3 Validation of the conversion accuracy of LinearCode ®  to WURCS All the LinearCode ®  strings obtained from GlycomeAtlas were converted to WURCS. However, most of glycans registered in GlycomeAtlas were not assigned accession numbers in GlyTouCan. Accession numbers were assigned to 259 out of the total 1217 glycans, and as a result, the WURCS sequences obtained from these glycans all completely matched with WURCS in GlyTouCan. 4 Discussion In this study, we developed GlycanFormatConverter which can conversely convert between WURCS and IUPAC. The functionality to output IUPAC from WURCS contributes to providing human-readable text formats of glycans in GlyTouCan. We showed that this tool can also be executed using IUPAC-Extended, KCF and LinearCode ® . In other words, we made it possible to utilize GlyTouCan not only with GlycoCT and WURCS, but also with other major glycan representation formats. In the verification process of conversion via KCF and IUPAC, there were some glycans that could not be converted accurately. In analyzing the KCF data, we thought that there was a problem with the data that GlyTouCan referenced from other databases, as GlyTouCan integrates glycan information of various databases and in the integration process converts every data to WURCS format. For example, the glycan information from KEGG is represented in KCF, which is converted first to GlycoCT through the MolecularFramework library. In analyzing the functionality of importing KCF and outputting GlycoCT using MolecularFramework, structural information of such monosaccharides as arabinose and ribose were not completely reflected in GlycoCT. Furthermore, aglycone information such as amino acids are omitted, so if an amino acid is found between two monosaccharides in the middle of a glycan, it would be removed in GlycoCT, resulting in an incomplete and different glycan structure. Because GlyTouCan has used GlycomeDB as its original source for glycan structures, GlyTouCan also contains the incomplete version of such glycans. Due to our validation procedure, we were able to gleen out such inconsistencies within the GlyTouCan data, which has been reported. The issue above is caused by the fact that modifications in GlycoCT are based on a library of known modifications. However, a glycan may consist of two glycan parts that are bridged by a peptide sequence. This would require GlycoCT to store a library of all possible peptide sequences, which is not realistic. Therefore, such glycans are currently beyond the scope of GlycoCT. Regarding IUPAC, we found that GlycanFormatConverter has many unsupported monosaccharides and substituents. GlycanFormatConverter handles 49 kinds of substituent residues in a dictionary format. However, as a result of investigating all the glycans registered in GlyTouCan, 3478 substituents were unsupported. We assumed that unsupported substituents will increase with the registration of new glycan structures to GlyTouCan. As a countermeasure against this, it is necessary to implement a function to convert the substituents without using a dictionary. WURCSFramework has some initial implementation of such functionality, which will be investigated in future work. WURCS format is capable of describing all structural information of glycans. Examples of structural information include molecular states between monosaccharides or between monosaccharide and substituent groups. In contrast, the IUPAC nomenclature was not designed to handle such detailed structural information. Therefore, many errors arose from the inability of IUPAC to capture the details in WURCS, which were subsequently lost or resulted in errors when trying to reconvert them back to WURCS. The majority of monosaccharides that did not match in the conversion from WURCS to IUPAC was due to ambiguous representations. It is difficult to perfectly represent the ambiguity of monosaccharides with IUPAC’s simple three letter code. Because IUPAC does not provide rules for representing ambiguous forms of monosaccharides, IUPAC strings end up with the default representation of monosaccharides, which are translated to less ambiguous forms in WURCS. Additionally, IUPAC is unable to handle ambiguity in glycan structures as a whole. Thus, we proposed additions to the IUPAC nomenclature for handling such structures ( Supplementary Material S1 ). Before approaching the IUPAC commission regarding the consideration of adopting our recommendations, we will first approach the glycoinformatics community, such as GLIC (GlycoInformatics Consortium;  http://glic.glycoinfo.org ), to try to get agreement from them. Such an agreement will make it easier to propose a well-defined and well-supported recommendation for IUPAC. In the future, we will improve the import process so that GlycanFormatConverter can handle a larger variety of glycan text formats. In particular, since GlycoCT is utilized by most researchers in the field of glycoinformatics, conversion from/to GlycoCT is indispensable. At present, there currently exists a translator for generating WURCS from GlycoCT ( https://github.com/glycoinfo/glycocttowurcs.git ) however it would be most useful to incorporate this functionality into GlycanFormatConverter. Moreover, in the current version of GlycoCT (2.0), rules for representing all glycan structures are insufficient. Therefore, we need to discuss with users/developers of GlycoCT to define all necessary rules in order to completely implement GlycoCT conversion functionality. 5 Conclusion The GlycanFormatConverter was able to encode WURCS from IUPAC-Extended, KCF and LinearCode ®  for the great majority of glycans registered in GlyTouCan. Those that could not be matched were mainly caused by limitation of the intermediate format. Thus this tool provides bioinformaticians with a new tool to aid in obtaining WURCS sequences. Furthermore, it allows users to register structures in GlyTouCan using representations other than the currently supported WURCS and GlycoCT. By making it possible to convert WURCS to IUPAC-Extended, IUPAC-Condensed and IUPAC-Short formats, it has become possible to represent glycans in a human-readable format. This tool has been released under a GNU GPL license, and it can be downloaded from  https://github.com/glycoinfo/GlycanFormatConverter.git . Supplementary Material bty990_Supplementary_Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Small sets of interacting proteins suggest functional linkage mechanisms via Bayesian analogical reasoning</Title>
    <Doi>10.1093/bioinformatics/btr236</Doi>
    <Authors>Airoldi Edoardo M., Heller Katherine A., Silva Ricardo</Authors>
    <Abstract>Motivation: Proteins and protein complexes coordinate their activity to execute cellular functions. In a number of experimental settings, including synthetic genetic arrays, genetic perturbations and RNAi screens, scientists identify a small set of protein interactions of interest. A working hypothesis is often that these interactions are the observable phenotypes of some functional process, which is not directly observable. Confirmatory analysis requires finding other pairs of proteins whose interaction may be additional phenotypical evidence about the same functional process. Extant methods for finding additional protein interactions rely heavily on the information in the newly identified set of interactions. For instance, these methods leverage the attributes of the individual proteins directly, in a supervised setting, in order to find relevant protein pairs. A small set of protein interactions provides a small sample to train parameters of prediction methods, thus leading to low confidence.</Abstract>
    <Body>1 INTRODUCTION Functional mechanisms in the cell involve cascades of interactions among gene products, mostly proteins and small molecules. In recent years, a number of large-scale efforts have collected and organized data produced by the community in publicly available, online databases. Human curators summarize the current state of our understanding of the cell's functional landscape, for instance, by categorizing genes' and proteins' functional roles, distinguishing those that have been experimentally verified from those that are most probable using well established computational methods ( Ashburner  et al. , 2000 ;  Finn  et al. , 2008 ;  Kanehisa and Goto, 2000 ;  Letunic  et al. , 2006 ;  Mewes  et al. , 2004 ;  Mulder  et al. , 2007 ). However, the current understanding of signaling and regulation dynamics that instantiate functions in the cell is far from complete. Functional discovery is key. The main challenges are the scale of the problem and the costs of the necessary experimental validation ( Evanko, 2009 ). The space of possible interactions and their multiple functional roles is very large. Exploring it at random is inefficient—expensive and time consuming. A sensible approach is that of prioritizing the experiments, testing the most probable functions and interactions first ( Schwartz  et al. , 2008 ). Methods routinely used by biologists today to assist biological discovery boil down to assigning priority scores to protein–function pairs and to protein–protein–function triples, individually or in small sets. Here we consider a slightly different scenario, where we want to assign priority scores to protein–protein pairs in the absence of a clear functional implication. We propose a methodology to rank protein–protein pairs that are most similar to a given  small set  of protein–protein pairs of interest. We show that our approach, based on analogical reasoning, is able to identify additional protein–protein pairs that share functional implications with the set of pairs of interest. These results suggest that our method may be useful in establishing new functional implications, which are not necessarily well annotated in existing databases. Bayesian analogical reasoning, the ability to learn and generalize relations between objects, seems particularly appropriate for biological discovery because it does not estimate feature weights and relations (during training) that are then fixed in stone to produce predictions. Rather, it calibrates a prior distribution on feature weights that takes into account how features relate to one another, but then updates these weights depending on the query, on the fly. To illustrate the practical benefits of the proposed methodology, consider the following scenario. A certain cellular process is poorly understood and the corresponding gene ontology function is poorly annotated, say, protein kinases. Consider extant protein interaction prediction methods; they typically estimate the correlation structure among features of all known proteins that is useful to predict existing interactions, e.g. with a Bayesian network ( Myers  et al. , 2005 ), store it on a web-server, and use it to rank interactions every time users submit a set of protein interactions. However, if we were to discover a few interactions with protein kinases and submit them as a query, extant systems would not be able to rank other interactions between kinases well, mainly for two reasons: (i) we are submitting a small training set, and (ii) the correlations that were estimated during the training phase did not include kinase interactions, which were poorly annotated, and the corresponding ranking mechanism (feature weights in a classifier or conditional probability tables in a Bayesian network) will have to rely on poor estimates of the correlations. The proposed methodology on the other hand, addresses the issue of small training sets (i) by regularizing the estimates through the use of a prior distribution on the weights in a Bayesian estimation setting. It gets around the issue of poor annotations and poor parameters estimates during training (ii) by updating the feature weights on the fly using information in the query set of interactions. In this example, our method would be able to attribute more importance to features such as specific protein domains, other features of the protein amino-acid sequences and cellular localization in order to retrieve other kinase interactions with high probability. The proposed method is also computationally scalable to large databases. In this article, we develop a statistical approach that leverages relational learning: given a set of protein pairs  S ={ A (1) : B (1) ,  A (2) : B (2) ,…, A ( N ) : B ( N ) }, it measures how well other pairs  A : B  fit in with the set  S . Our work addresses the question of whether the relation between proteins  A  and  B  is analogous to those relations found in  S . Such questions are particularly relevant in exploratory data analysis, where an investigator might want to search for protein pairs that are analogous to pairs in the query set of interest. Our approach combines a similarity measure on function spaces with Bayesian analysis to produce a ranking of pairs. It requires data containing attributes of the proteins of interest and a link matrix specifying existing relationships; no further attributes of such relationships are necessary. We illustrate the potential of our method on a collection of protein binding events and on metabolic pathways. We show that our approach can work in practice even if a small set of protein pairs is provided, when evaluated on two functional categorization systems: GO and MIPS. Furthermore, we develop a variational inference algorithm that scales to very-large databases. Related problems and approaches : similarity-based methods compute the score of a given protein pair as a function of observable attributes (e.g. corresponding genes' expression levels) of the two proteins under examination. Possibly multiple functions are then assigned to each protein  P i  using a guilt-by-association principle, e.g. by looking up in a curated database the most frequent functions of the proteins  P j 's that interact with  P i  with high scores ( Butte and Kohane, 2006 ;  Clare and King, 2003 ;  Margolin  et al. , 2006 ;  Markowetz  et al. , 2007 ). The main obstacle to the success of these methods is that the similarity among proteins' attributes is informative about a shared function only to a minor extent ( Margoln and Califano, 2007 ). Clustering-based methods compute the score of a given protein pair as a function of topological properties of the interactions. The interactome is divided into clusters, or modules, and the inferred memberships of proteins-to-modules are used as attributes to predict protein functions in a number of ways ( Adamcsek  et al. , 2006 ;  Altaf-Ul-Amin  et al. , 2006 ;  Bader and Hougue, 2003 ;  Enright  et al. , 2002 ;  Sharan  et al. , 2005 ). Recent results, however, suggest that a simple non-clustering method that relies on the guilt-by-association principle is more accurate in predicting proteins' functional roles ( Song and Singh, 2009 ). The curation of the interactome is noisy and its degree of inaccuracy is higher than expected ( Cusick  et al. , 2009 ). Proteins participate in the execution of multiple functional processes ( Airoldi  et al. , 2008 ). These are perhaps two of the factors that can explain this surprising result. Data integration methods compute the score of a given protein pair as a function that combines observed attributes, both of individual proteins and of protein pairs, from multiple data sources. These integrative methods have been the most successful to date in predicting protein function ( Fraser and Marcotte, 2004 ;  Guan  et al. , 2008 ;  Llewellyn and Eisenberg, 2008 ) and networks of functional relationships between proteins ( Hess  et al. , 2009 ;  Huttenhower  et al. , 2009 ;  Ideker  et al. , 2002 ;  Jansen  et al. , 2003 ;  Jensen  et al. , 2009 ;  Lee  et al. , 2004 ;  Myers  et al. , 2005 ;  Troyanskaya  et al. , 2003 ;  von Mering  et al. , 2005 ) from large collections of data. The main drawback of these approaches is that they often involve a hodgepodge of different scores that only resemble  P -values ( Schervish, 1996 ;  Sterne and Smith, 2001 ). Multiple scores are usually combined using ad hoc considerations that are specific to the data under examination. The computational burden is substantial. Our contribution : we introduce a new approach that determines similarity between protein pairs by essentially computing similarity between predictive functions that relate proteins pairs to functions. Our methodology is based on the idea of analogical reasoning. The proposed methodology departs from the existing approaches in a few aspects. First, we explicitly address the ranking problem of which pairs of proteins are most similar to an input set of protein pairs. This is different from the typical protein pair prediction problem addressed in the literature. While we rely on similarity to rank protein pairs, we compute similarity between  predictive functions  that map pairs of proteins to functional links, rather than between  attributes  of functionally related proteins. This will require a description of the space where such functions live. Second, our methodology focuses on the case where little evidence is available, i.e. a small set of input pairs. This will require using prior functional knowledge to calibrate a prior on the space of functions that places mass on a most likely subspace of functions. Third, our methodology is rooted in Bayesian statistical methodology. It captures and updates prior knowledge about proteins' functions stored in online databases within a hierarchical Bayesian model. It can easily be integrated in a computational pipeline for general use. Fourth, our variational inference algorithm scales to very-large databases. We consider two case studies where the underlying functional implications of the interactions we consider correspond to physical protein binding events and to signaling events within a metabolic pathway. We quantify the extent to which small sets of interacting proteins are suggestive of functional linkage mechanisms on the collection of pathways in KEGG and of functions in MIPS. 2 METHODS To define an analogy is to define a measure of similarity between structures of related objects. In our setting, we need to measure the similarity between pairs of objects. The key aspect that distinguishes our approach from others is that we focus on the similarity between  predictive functions  that map pairs to links, rather than focusing on the similarity between the  attributes  of objects in a candidate pair and the features of objects in the query pairs. As an illustration, consider an analogical reasoning question from a SAT-like exam where for a given pair (say,  water : river ) we have to choose, out of 5 pairs, the one that best matches the type of relation implicit in such a ‘query’. In this case, it is reasonable to say  car : highway  would be a better match than (the somewhat nonsensical)  soda:ocean , since cars flow on a highway, and so does water in a river. Notice that if we were to measure the similarity between  objects  instead of  relations ,  soda : ocean  would be a much closer pair, since  soda  is similar to  water , and  ocean  is similar to  river . It is reasonable to infer relational similarity from individual object features ( Gentner and Medina, 1998 ). What is needed is a mechanism by which object features should be weighted in a particular relational similarity problem. We postulate that, in analogical reasoning, similarity between features of objects is only meaningful to the extent to which such features are useful to predict the existence of the relationships. 2.1 An illustrative example As an intuitive illustration, consider university admission exams, like the American Scholastic Assessment Test (SAT) and Graduate Record Exam (GRE). These exams used to include a section on analogical reasoning. A prototypical analogical reasoning question is shown in  Figure 1 . The examinee has to answer which of the 5 pairs best matches the relation implicit in  DOCTOR:HOSPITAL . Although all candidate pairs interact in some way, pair  professor:college  seems to best capture the notion of ( object ,  place of work ) implicit in the relation between doctor and hospital.
 Fig. 1. Example with words. Performing this type of analogical reasoning in an automated way may be useful in less mundane domains, where it is hard to reason about relations that have not yet been discovered. Consider the question shown in  Figure 2 , composed solely of pairs of interacting proteins according to the MIPS classification system ( Mewes  et al. , 2004 ). In the given question,  YDL061C  is a protein of type  cytoplasm  (category 40.03), while  YLR167W  is of type  cytoplasmic and nuclear degradation  (category 6.13.01). Such is also the case of B)  YBL092W : YKR094C . Other pairs contain members which are both in the 40.03 category but none in 6.13.01, and are in this sense not as close to the question pair as option B. In a realistic situation where the categorization system at hand contains a few annotation errors and a large number of omissions, automated strategies for analogical reasoning that leverage the current state of our knowledge about the proteins would be useful.
 Fig. 2. Example with proteins. Dividing the population of protein interactions into  subpopulations with similar mechanisms of linkage  is therefore a way of categorizing proteins and their functional roles. The population of interacting pairs of proteins is not uniform. The biological mechanism by which protein pair P1:P2 is linked might not be the same mechanism behind the linkage of P3:P4, as illustrated above. The result of this effort is that taxonomies such as the Gene Ontology ( Ashburner  et al. , 2000 ), or the Munich Institute for Protein Sequencing (MIPS) database ( Mewes  et al. , 2004 ) can be enriched by suggesting analogical similarities of protein interactions. 2.2 Statistical background on analogical reasoning Probabilities can be exploited as a measure of similarity. Let  R  be a binary random variable representing whether an arbitrary data point  X  is ‘relevant’ for a given query set  S  ( R =1) or not ( R =0). Let  P (· | ·) be a generic probability mass function or density function, with its meaning given by the context. Points are ranked in decreasing order by the following criterion
 
which is equivalent to ranking points by the expression
 (1) The challenge is to define what form  P ( X  |  R = r ,  S ) should assume. It is not practical to collect labeled data in advance which, for every possible class of queries, will give an estimate for  P ( R =1 |  X ,  S ): in general, one cannot anticipate which classes of queries will exist. Methods based on Bayesian networks with known labels (e.g.  Myers  et al. , 2005 ), for instance, would suffer in terms of recall. Instead, a variety of approaches has been developed in the literature in order to define a suitable instantiation of ( 1 ). These include a method that builds a classifier on-the-fly using  S  as elements of the positive class  R =1, and a random subset of data points as the negative class  R =0 (e.g.  Turney, 2008 ). In this setup, the event ‘ R =1’ is equated with the event that  X  and the elements of  S  are i.i.d points generated by the same model. The event ‘ R =0’ is the event by which  X  and  S  are generated by two independent models: one for  X  and another for  S . The parameters of all models are random variables that have been integrated out, with fixed (and common) hyperparameters. The result is the instantiation of ( 1 ) as
 (2) 
the Bayesian  score function  by which we rank points  X  given a query  S . The right-hand side was rearranged to provide a more intuitive graphical model. From this graphical model interpretation we can see that the score function is a Bayes factor comparing two models ( Kass and Raftery, 1995 ). Next, we describe how the Bayesian sets method can be adapted to define analogical similarity in biological networks settings. 2.3 Bayesian analogical similarity for protein pairs Let 𝒜 and ℬ represent object spaces. To say that an interaction  A : B  is analogous to  S ={ A (1) : B (1) , A (2) : B (2) ,…, A ( N ) : B ( N ) } amounts to implicitly defining a measure of similarity between the pair  A : B  and the set of pairs  S , where each query item  A ( k ) : B ( k )  corresponds to some pair  A i : B j . However, this similarity is not directly derived from the similarity of the information contained in the distribution of objects themselves, { A i }⊂𝒜, { B i }⊂ℬ. Rather, the similarity between  A : B  and the set  S  is defined in terms of the similarity of the  functions  mapping the pairs as being linked. Each possible function captures a different possible relationship between the objects in the pair.
 Bayesian analogical reasoning : Consider a space of latent functions in 𝒜×ℬ→{0, 1}. Assume that  A  and  B  are two objects classified as linked by some unknown function  f ( A ,  B ), i.e.  f ( A ,  B )=1. We want to quantify how similar the function  f ( A ,  B ) is to the function  g (·, ·), which classifies all pairs ( A i ,  B j )∈ S  as being linked, i.e. where  g ( A i ,  B j )=1. The similarity depends on the observations { S ,  A ,  B } and our prior distribution over  f (·,·) and  g (·,·). 
 Functions  f (·) and  g (·) are unobserved, hence the need for a prior that will be used to integrate over the function space. Our similarity metric will be defined using Bayes factors, as explained next. 2.3.1 Scoring analogy of linkage functions using logistic regression For simplicity, we will consider a family of latent functions that is parameterized by a finite-dimensional vector: the logistic regression function with multivariate Gaussian priors for its parameters. For a particular pair ( A i ∈𝒜,  B j ∈ℬ), let  X ij =[Φ 1 ( A i ,  B j ) Φ 2 ( A i ,  B j )… Φ K ( A i ,  B j )] T  be a point on a feature space defined by the mapping Φ:𝒜×ℬ→ℜ K . This feature space mapping computes a  K -dimensional vector of attributes of the pair that may potentially be relevant to predicting the relation between the objects in the pair. Let  L ij ∈{0, 1} be an indicator of the existence of a link or relation between  A i  and  B j  in the database. Let Θ=[θ 1 ,…,θ K ] T  be the parameter vector for our logistic regression model such that
 (3) 
and logistic( x )=(1+ e − x ) −1  is the standard mapping from ℝ→[0, 1]. We now apply the same score function underlying the Bayesian methodology explained in  Section 2.2 . However, instead of comparing objects by marginalizing over the parameters of their feature distributions, we compare  functions  for link indicators by marginalizing over the parameters of the functions. Let  L S  be the vector of link indicators for  S : in fact each  L ∈ L S  has the value  L =1, indicating that every pair of objects in  S  is linked. Consider the following Bayes factor:
 (4) This is an adaptation of Equation ( 2 ) where relevance is defined now by whether  L ij  and  L S  were generated by the same model, for fixed { X ij ,  S }. In one sense, this is a discriminative Bayesian sets model, where we predict links instead of modeling joint object features. Since we are integrating out Θ, a prior for this parameter vector is needed. Thus, each pair ( A i ,  B j ) is evaluated with respect to a query set  S  by the score function given in ( 4 ), rewritten after taking a logarithm and dropping constants as:
 (5) The exact details of our procedure are as follows. We are given a relational database (𝒟 A , 𝒟 B , ℒ AB ). Dataset 𝒟 A  (𝒟 B ) is a sample of objects of type 𝒜(ℬ). Relationship table ℒ AB  is a binary matrix modeled as generated from a logistic regression model of link existence. A query proceeds according to the following steps:
 the user selects a set of pairs  S  that are linked in the database, where the pairs in  S  are assumed to have some relation of interest; perform Bayesian inference to obtain the corresponding posterior distribution for Θ,  P (Θ| S ,  L S ), given a Gaussian prior  P (Θ); and iterate through all linked pairs, computing the following for each pair:
 
where  P ( L ij =1| X ij ) is similarly computed by integrating over  P (Θ). All pairs are presented in decreasing order according to the score in Equation ( 5 ). The integral presented above does not have a closed formula. Because computing the integrals by a Monte Carlo method for a large number of pairs would be unreasonable, we use a variational approximation ( Airoldi, 2007 ;  Jordan  et al. , 1999 ).  Figure 3  presents a summary of the approach. The suggested setup scales as  O ( K 3 ) with the feature space dimension, due to the matrix inversions necessary for (variational) Bayesian logistic regression ( Jaakkola and Jordan, 2000 ). A less precise approximation to  P (Θ| S ,  L S ) can be imposed if the dimensionality of Θ is too high. However, it is important to point out that once the initial integral  P (Θ |  S ,  L S ) is approximated, each score function can be computed at a cost of  O ( K 2 ). Our analogical reasoning formulation is a relational model in that it models the presence and absence of interactions between objects. By conditioning on the link indicators, the similarity score between  A : B  and  C : D  is always a function of pairs ( A ,  B ) and ( C ,  D ) that is not in general decomposable as similarities between  A  and  C , and  B  and  D . 2.3.2 Empirical priors and calibration using biological databases The choice of prior is based on the observed data, in a way that is equivalent to the choice of priors used in the original formulation of Bayesian sets ( Ghahramani and Heller, 2005 ). Let   be the maximum-likelihood estimator (MLE) of Θ using the relational database (𝒟 A , 𝒟 B , ℒ AB ). Since the number of possible pairs grows at a quadratic rate with the number of objects, we do not use the whole database for MLE. Instead, to get   we use all linked pairs as members of the ‘positive’ class ( L =1), and subsample unlinked pairs as members of the ‘negative’ class ( L =0). We subsample by sampling each object uniformly at random from the respective datasets 𝒟 A  and 𝒟 B  to get a new pair. Since link matrices ℒ AB  are usually very sparse, in practice this will almost always provide an unlinked pair.  Section 3  provides more details. We use the prior  ), where 𝒩( m ,  V ) is a normal of mean  m  and variance  V . Matrix   is the empirical second moments matrix of the linked object features, although a different choice might be adequate for different applications. Constant  c  is a smoothing parameter set by the user. In all of our experiments, we set  c  to be equal to the number of positive pairs. A good choice of  c  might be important to obtain maximum performance, but we leave this issue as future work. ( Wang  et al. , 2009 ) present some sensitivity analysis results. Empirical priors are a sensible choice, since this is a retrieval, not a predictive, task. Basically, the entire dataset is the population, from which prior information is obtained on possible query sets. A data-dependent prior based on the population is important for an approach such as Bayesian sets, since deviances from the ‘average’ behavior in the data are useful to discriminate between subpopulations.
 Fig. 3. General framework of the procedure: first, a ‘prior’ over parameters Θ for a link classifier is defined empirically using linked and unlinked pairs of points (the dashed edges indicate that creating a prior empirically is optional, but in practice we rely on this method). Given a query set  S  of linked pairs of interest, the system computes the predictive likelihood of each linked pair 𝒟 ( i ) ∈𝒟 +  and compares it to the conditional predictive likelihood, given the query. This defines a measure of similarity with respect to  S  by which all pairs in 𝒟 +  are sorted. 2.3.3 Extensions to continuous/multivariate relations Although here we focused on measuring similarity of qualitative relationships, the same idea could be extended to  continuous  (or ordinal) measures of relationship, or relationships where each  L ij  is a vector. Several similarity metrics can be defined on this vector of continuous relationships. Given data on protein attributes, one can easily modify our approach by substituting the logistic regression component with some multiple regression model. 3 RESULTS The budding yeast is a unicellular organism that has become a de-facto model organism for the study of molecular and cellular biology ( Botstein  et al. , 1997 ). There are about 6000 proteins in the budding yeast, which interact in a number of ways ( Cherry  et al. , 1997 ). For instance, proteins bind together to form protein complexes, the physical units that carry out most functions in the cell ( Krogan  et al. , 2006 ). In recent years, significant resources have been directed to collect experimental evidence of physical proteins binding, in an effort to infer and catalogue protein complexes and their multifaceted functional roles ( Fields and Song, 1989 ;  Ho  et al. , 2002 ;  Ito  et al. , 2000 ;  Uetz  et al. , 2000 ). Currently, there are four main sources of interactions between pairs of proteins that target proteins localized in different cellular compartments with variable degrees of success: (i) literature curated interactions ( Reguly  et al. , 2006 ); (ii) yeast two-hybrid (Y2H) interaction assays ( Yu  et al. , 2008 ); (iii) protein fragment complementation (PCA) interaction assays ( Tarassov  et al. , 2008 ); and (iv) tandem affinity purification (TAP) interaction assays ( Gavin  et al. , 2006 ;  Krogan  et al. , 2006 ). These collections include a total of about 12 292 protein interactions ( Jensen and Bork, 2008 ), although the number of such interactions is estimated to be between 18 000 ( Yu  et al. , 2008 ) and 30 000 ( von Mering  et al. , 2002 ). 3.1 Design of experiments We consider multiple functional categorization systems for the proteins in budding yeast. For evaluation purposes, we use individual proteins' functional annotations curated by the Munich Institute for Protein Sequencing (MIPS) ( Mewes  et al. , 2004 ), those by the Kyoto Encyclopedia of Genes and Genomes (KEGG) ( Kanehisa and Goto, 2000 ), and those by the Gene Ontology consortium (GO) ( Ashburner  et al. , 2000 ). We consider multiple collections of physical protein interactions that encode alternative semantics. Physical protein interactions in the MIPS curated collection measure physical binding events observed experimentally in Y2H and TAP experiments, whereas physical protein-to-protein interactions in the KEGG curated collection measure a number of different modes of interactions, including phosporelation, methylation and physical binding, all taking place in the context of a specific signaling pathway. So we have three possible functional annotation databases (MIPS, KEGG and GO) and two possible link matrices (MIPS and KEGG), which can be combined. Our experimental pipeline is as follows. (i) Pick a database of functional annotations, say MIPS, and a collection of interactions, say MIPS (again). (ii) Pick a pair of categories,  M 1  and  M 2 . For instance, take  M 1  to be  cytoplasm  (MIPS 40.03) and  M 2  to be  cytoplasmic and nuclear degradation  (MIPS 06.13.01). (iii) Sample, uniformly at random and without replacement, a set  S  of 15 interactions in the chosen collection. (iv) Rank other interacting pairs 1  according to the score in Equation ( 5 ) and, for comparison purposes, according to three other approaches to be described in  Section 3.1.4 . (v) The process is repeated for a large number of pairs  M 1 × M 2 , and 5 different query sets  S  are generated for each pair of categories. (vi) Calculate an evaluation metric for each query and each of the four scores. Report a comparative summary of results. 3.1.1 Protein specific features The protein specific features were generated using the datasets summarized in  Table 1  and an additional dataset ( Qi  et al. , 2006 ), from which 20 gene expression attributes were obtained. Each gene expression attribute for a protein pair  P i : P j  corresponds the correlation coefficient between the expression levels of corresponding genes. The 20 different attributes are obtained from 20 different experimental conditions as measured by microarrays. We did not use pairs of proteins from Qi  et al.  (2006) for which we did not have data in the datasets listed in  Table 1 . This resulted in approximately 6000 positively linked data points for the MIPS network and 39 000 for KEGG. We generated another 25 protein–protein gene expression features from the data in  Table 1  using the same procedure based on correlation coefficients. This gives a total of 45 attributes, corresponding to the main dataset used in our relational Bayesian sets runs. Another dataset was generated using the remaining (i.e. non-microarray) features of  Table 1 . Such features are binary and highly sparse, with most entries being 0 for the majority of linked pairs. We removed attributes for which we had fewer than 20 linked pairs with positive values according to the MIPS network. The total number of extra binary attributes was 16. Several measurements were missing. We imputed missing values for each variable in a particular datapoint by using its empirical average among the observed values.
 Table 1. Collection of datasets used to generate protein-specific features Type of data Data sources for our study Gene expression Brem  et al.  (2005 );  Gasch  et al.  (2000 )  Primig  et al.  (2000 ;  Yvert  et al.  (2003 ) Synthetic genetic int. Breitkreutz  et al.  (2003 ); SGD Cellular localization Huh  et al.  (2003 ) TF binding sites Harbison  et al.  (2004 ); TRANSFAC Sequence data Altschul  et al.  (1990 );  Zhu and Zhang (1999 ) Given the 45 or 61 attributes of a given pair { P i ,  P j }, we applied a non-linear transformation where we normalize the vector by its Euclidean norm in order to obtain our feature table  X . 3.1.2 Calibrating the prior for Θ We fit a logistic regression classifier using a MLE and our data, obtaining the estimate  . Our choice of covariance matrix   for Θ is defined to be a rescaling of a squared norm of the data:
 (6) 
where  X POS  is the matrix containing the protein–protein features only of the linked pairs used in the MLE computation. 3.1.3 Evaluation metrics We propose an objective measure of evaluation that is used to compare different algorithms. Consider a query set  S , and a ranked response list  R ={ R 1 ,  R 2 ,  R 3 ,…,  R N } of protein–protein pairs. Every element of  S  is a pair of proteins  P i : P j  such that  P i  is of class  M i  and  P j  is of class  M j , where  M i  and  M j  are classes from either MIPS, KEGG or Gene Ontology. In general, proteins belong to multiple classes. The retrieval algorithm that generates  R  does not receive any information concerning the MIPS, KEGG or GO taxonomy.  R  starts with the linked protein pair that is judged most similar to  S , followed by the other protein pairs in the population, in decreasing order of similarity. Each algorithm has its own measure of similarity. The evaluation criterion for each algorithm is as follows: as before, we generate a precision-recall curve and calculate the area under the curve (AUC). We also calculate the proportion (TOP10), among the top 10 elements in each ranking, of pairs that match the original { M 1 ,  M 2 } selection (i.e. a ‘correct’  P i : P j  is one where  P i  is of class  M 1  and  P j  of class  M 2 , or vice-versa. Notice that each protein belongs to multiple classes, so both conditions might be satisfied.) Since a researcher is only likely to look at the top ranked pairs, it makes sense to define a measure that uses only a subset of the ranking. AUC and TOP10 are our two evaluation measures. The original classes { M 1 ,  M 2 } are known to the experimenter but not known to the algorithms. Our criterion is rather stringent, in the sense it requires a perfect match of each  R I  with the MIPS, KEGG or GO categorization. There are several ways by which a pair  R I  might be analogous to the relation implicit in  S , and they do not need to agree with MIPS, GO or KEGG. Still, if we are willing to believe that these standard categorization systems capture functional organization of proteins at some level, this must lead to association between categories given to  S  and relevant sub-populations of protein–protein interactions similar to  S . Therefore, the corresponding AUC and TOP10 are useful tools for comparing different algorithms even if the actual measures are likely to be pessimistic for a fixed algorithm. 3.1.4 Competing algorithms We compare our method against a variant of it and two similarity metrics widely used for information retrieval: (i) the cosine score ( Manning  et al. , 2008 ), denoted by  cos ; (ii) the nearest neighbor score, denoted by  nns ; (iii) the relational maximum-likelihood sets score, denoted by  mls . The nearest neighbor score measures the minimum Euclidean distance between  R I  and any individual point in  S , for a given query set  S  and a given candidate point  R I . The relational MLS is a variation of RBS ets  where we initially sample a subset of the unlinked pairs (10 000 points in our setup) and, for each query  S , we fit a logistic regression model to obtain the parameter estimate Θ S MLE . We also use a logistic regression model fit to the  whole  dataset (the same one used to generate the prior for RBS ets ), giving the estimate Θ MLE . A new score, analogous to ( (5) ), is given by log  P ( L ij =1| X ij , Θ S MLE )−log  P ( L ij =1| X ij , Θ MLE ), i.e. we do not integrate out the parameters or use a prior, but instead the models are fixed at their respective estimates. Neither  cos  or  nns  can be interpreted as measures of analogical similarity, in the sense that they do not take into account how the protein pair features  X  contribute to their interaction 2 . 3.2 Analysis of physical interactions (MIPS) For this batch of experiments, we use the MIPS network of protein–protein interactions to define the relationships. In the initial experiment, we selected queries from all combinations of MIPS classes for which there were at least 50 linked pairs  P i : P j  in the network that satisfied the choice of classes. Each query set contained 15 pairs. After removing the MIPS-categorized proteins for which we had no feature data, we ended up with a total of 6125 proteins and 7788 positive interactions. We set the prior for RBS ets  using a sample of 225 842 pairs labeled as having no interaction, as selected by ( Qi  et al. , 2006 ). For each tentative query set  S  of categories { M 1 ,  M 2 }, we scored and ranked pairs  P i ′ : P j ′  such that both  P i ′  and  P j ′  were connected to some protein appearing in  S  by a path of no more than two steps, according to the MIPS network. The reasons for the filtering are 2-fold: to increase the computational performance of the ranking since fewer pairs are scored; and to minimize the chance that undesirable pairs would appear in the top 10 ranked pairs. Tentative queries would not be performed if after filtering we obtained fewer than 50 possible correct matches. Trivial queries, where filtering resulted only in pairs in the same class as the query, were also discarded. The resulting number of unique pairs of categories { M 1 ,  M 2 } was 931 classes of interactions. For each pair of categories, we sampled our query set  S  5 times, generating a total of 4655 rankings per algorithm. We run two types of experiments. In one version, we give to RBS ets  the data containing only the 45 (continuous) microarray measurements. In the second variation, we provide to RBS ets  all 61 variables, including the 16 sparse binary indicators. However, we noticed that the addition of the 16 binary variables hurts RBS ets  considerably. We conjecture that one reason might be the degradation of the variational approximation. Including the binary variables hardly changed the other three methods, so we choose to use the 61 variable dataset for the other methods. Table 2  summarizes the results of this experiment. We show the number of times each method wins according to both the AUC and TOP10 criteria. The number of wins is presented as divided by 5, the number of random sets generated for each query type { M 1 ,  M 2 } (notice these numbers do not need to add up to 931, since ties are possible). Moreover, we also presented ‘smoothed’ versions of this statistic, where we count a method as the winner for any given { M 1 ,  M 2 } category if, for the group of 5 queries, the method obtains the best result in at least 3 of the sets. The motivation is to smooth out the extra variability added by the particular set of 15 protein pairs for a fixed { M 1 ,  M 2 }. The proposed relational Bayesian sets method is the clear winner according to all measures when we select only the continuous variables. For this reason, for the rest of this section all analysis and experiments will consider only this case.  Table 3  displays a pairwise comparison of the methods. In this Table, we show how often the row method performs better than the column method, among those trials where there was no tie. Again, RBS ets  dominates.
 Table 2. Number of times each method wins when querying pairs of MIPS classes using the MIPS protein–protein interaction network Method #AUC #TOP10 #AUC.S #TOP10.S COS 240 294 219 277 NNS 42 122 28 75 MLS 105 270 52 198 RBS ets 542 556 578 587 Method #AUC #TOP10 #AUC.S #TOP10.S COS 314 356 306 340 NNS 75 146 62 111 MLS 273 329 246 272 RBS ets 267 402 245 387 The first two columns,  #AUC  and  #TOP10 , count the number of times the respective method obtains the best score according to the AUC and TOP10 measures, respectively, among the 4 approaches. This is divided by the number of replications of each query type (5). The last two columns,  #AUC.S  and  #TOP10.S  are ‘smoothed’ versions of this statistic: a method is declared the winner of a round of 5 replications if it obtains the best score in at least 3 out of the 5 replications. The top table shows the results when only the continuous variables are used by RBS ets , and in the bottom table when the discrete variables are also given to RBS ets . 
 Table 3. Pairwise comparison of methods according to the AUC and TOP10 criterion COS NNS MLS RBS ets AUC  COS – 0.67 0.43 0.30  NNS 0.32 – 0.18 0.06  MLS 0.56 0.81 – 0.25  RBS ets 0.69 0.93 0.74 – TOP10  COS – 0.70 0.46 0.30  NNS 0.29 – 0.25 0.11  MLS 0.53 0.74 – 0.28  RBS ets 0.69 0.88 0.71 – Each cell shows the proportion of the trials where the method in the respective row wins over the method in the column, according to both criteria. In each cell, the proportion is calculated with respect to the 4655 rankings where no tie happened. Another useful summary is the distribution of correct hits in the top 10 ranked elements across queries. This provides a measure of the difficulty of the problem, besides the relative performance of each algorithm. In  Table 4 , we show the proportion of correct hits among the top 10 for each algorithm for our queries using MIPS categorization and also GO categorization, as explained in the next section. About 14% of the time, all pairs in the top 10 pairs ranked by RBS ets  were of the intended type, compared to 8% of the second best approach.
 Table 4. Distribution across all queries of the number hits in the top 10 pairs, as ranked by each algorithm 0 1 2 3 4 5 6 7 8 9 10 Proportion of top hits using MIPS categories and links specified by the MIPS database COS 0.12 0.15 0.12 0.10 0.08 0.07 0.06 0.05 0.04 0.07 0.08 NNS 0.29 0.16 0.14 0.10 0.06 0.05 0.03 0.03 0.03 0.03 0.02 MLS 0.12 0.12 0.12 0.10 0.09 0.08 0.07 0.06 0.07 0.06 0.07 RBS ets 0.04 0.08 0.09 0.09 0.09 0.08 0.09 0.07 0.09 0.08 0.14 Proportion of top hits using GO categories and links specified by the MIPS database COS 0.12 0.13 0.11 0.10 0.11 0.09 0.06 0.06 0.04 0.06 0.06 NNS 0.53 0.23 0.07 0.02 0.02 0.02 0.04 0.01 0.00 0.00 0.01 MLS 0.16 0.11 0.12 0.10 0.08 0.08 0.08 0.06 0.05 0.06 0.05 RBS ets 0.09 0.09 0.10 0.10 0.08 0.08 0.06 0.08 0.08 0.07 0.12 The more skewed to the right, the better. Notice that using GO categories doubles the number of zero hits for RBS ets . 3.2.1 Changing the categorization system A variation of this experiment was performed where the protein categorizations do  not  come from the same family as the link network, i.e. where we used the MIPS network but not the MIPS categorization. Instead we performed queries according to Gene Ontology categories. Starting from 150 pre-selected GO categories ( Myers  et al. , 2006 ), we once again generated unordered category pairs { M 1 ,  M 2 }. A total of 179 queries, with 5 replications each (a total of 895 rankings), were generated and the results summarized in  Table 5 .
 Table 5. Number of times each method wins when querying pairs of GO classes using the MIPS protein–protein interaction network Method #AUC #TOP10 #AUC.S #TOP10.S COS 58 73 58 72 NNS 1 10 0 4 MLS 26 55 13 38 RBS ets 93 105 101 110 Columns #AUC, #TOP10, #AUC.S and #TOP10.S are defined as in  Table 2 . This is a more challenging scenario for our approach, which is optimized with respect to MIPS. Still, we are able to outperform other approaches. Differences are smaller, but consistent. In the pairwise comparison of RBS ets  against the second best method,  cos , our method wins 62% of the time by the TOP10 criterion. 3.2.2 The role of filtering In both experiments with the MIPS network, we filtered candidates by examining only a subset of the proteins linked to the elements in the query set by a path of no more than two proteins. It is relevant to evaluate how much coverage of each category pair { M 1 ,  M 2 } we obtain by this neighborhood selection. For each query  S , we calculate the proportion of pairs  P i : P j  of the same categorization { M 1 ,  M 2 } such that both  P i  and  P j  are included in the neighborhood. For the MIPS categorization, 93% of the queries resulted in a coverage of at least 75% (with 24% of the queries resulting in perfect coverage). Although filtering implies that some valid pairs will never be ranked, the gain obtained by reducing false positives in the top 10 ranked pairs is considerable (results not shown) across all methods, and the computational gain of reducing the search space is particularly relevant in exploratory data analysis. 3.3 Analysis of signaling pathways (KEGG) We repeated the same experimental setup, now using the KEGG network to define the protein interactions. We selected proteins from the KEGG categorization system for which we had data available. A total of 6125 proteins was selected. The KEGG network is much more dense than MIPS. A total of 38 961 positive pairs and 226 188 negative links were used to generate our empirical prior. Since the KEGG network is much more dense than MIPS, we filtered our candidate pairs by allowing only proteins that are directly linked to the proteins in the query set  S . Even under this restriction, we are able to obtain high coverage: the neighborhood of 90% of the queries included all valid pairs of the same category, and essentially all queries included at least 75% of the pairs falling in the same category as the query set. A total of 1523 possible category pairs (7615 queries, considering 5 replications) were generated. Results are summarized in  Table 6 . Again, it is evident that RBS ets  dominates other methods. In the pairwise comparison against  cos , RBS ets  wins 76% of the times according to the TOP10 criterion. However, the ranking problem in the KEGG network was much harder than in the MIPS network (according to our automated non-analogical criterion). We believe that the reason is that, in KEGG, the simple filtering scheme has much less influence, as reflected by the high coverage. The distribution of the number of hits in the top 10 ranked items is shown in  Table 7 . Despite the success of RBS ets  relative to the other algorithms, there is room for improvement.
 Table 6. Number of times each method wins when querying pairs of KEGG classes using the KEGG protein–protein interaction network Method #AUC #TOP10 #AUC.S #TOP10.S COS 159 575 134 507 NNS 30 305 17 227 MLS 290 506 199 431 RBS ets 1042 1091 1107 1212 Columns #AUC, #TOP10, #AUC.S and #TOP10.S are defined as in  Table 2 . 
 Table 7. Distribution across all queries of the number hits in the top 10 pairs, as ranked by each algorithm 0 1 2 3 4 5 6 7 8 9 10 Proportion of top hits using KEGG categories and links specified by the KEGG database COS 0.56 0.21 0.08 0.03 0.02 0.01 0.01 0.01 0.01 0.01 0.01 NNS 0.89 0.03 0.04 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 MLS 0.57 0.21 0.08 0.04 0.02 0.01 0.01 0.00 0.00 0.00 0.00 RBS ets 0.29 0.24 0.16 0.09 0.06 0.03 0.02 0.01 0.03 0.02 0.01 The more skewed to the right, the better. 4 CONCLUDING REMARKS We presented a novel measure of similarity between biological structures based on the principle of analogical comparison. It provides a way of clustering biological data that is considerably different from other methods, due to its focus on analysing the space of functions that map object features to their relations, instead of the feature space itself. For small size queries, our method finds analogies that are functionally relevant among the top matches. We evaluated our approach with thousands of experiments. This work can be expanded in many ways, including but not limited to: allowing for extra dependencies between interactions that are not due to input features  X , scaling up the algorithm to allow for higher-dimensional data, and applying it to other domains such as evaluating analogies between cells from different species. Filtering could also be improved by using criteria other than path lengths in a interaction network. It is also of interest to apply substantive background knowledge in evaluating rankings that take into account the actual analogical similarity of different pairs. We believe several useful variations of our approach can be designed in the future. Funding :  National Science Foundation  (under grants no.  DMS-0907009  and no.  IIS-1017967 );  National Institute of Health  (under grant no.  R01 GM096193 );  Army Research Office Multidisciplinary University Research Initiative  (under grant no.  58153-MA-MUR ) all to  Harvard University ; additional funding was provided by the  Harvard Medical School's Milton Fund . Conflict of Interest : none declared. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>A soft kinetic data structure for lesion border detection</Title>
    <Doi>10.1093/bioinformatics/btq178</Doi>
    <Authors>Kockara Sinan, Mete Mutlu, Yip Vincent, Lee Brendan, Aydin Kemal</Authors>
    <Abstract>Motivation: The medical imaging and image processing techniques, ranging from microscopic to macroscopic, has become one of the main components of diagnostic procedures to assist dermatologists in their medical decision-making processes. Computer-aided segmentation and border detection on dermoscopic images is one of the core components of diagnostic procedures and therapeutic interventions for skin cancer. Automated assessment tools for dermoscopic images have become an important research field mainly because of inter- and intra-observer variations in human interpretations. In this study, a novel approach—graph spanner—for automatic border detection in dermoscopic images is proposed. In this approach, a proximity graph representation of dermoscopic images in order to detect regions and borders in skin lesion is presented.</Abstract>
    <Body>1 INTRODUCTION Melanoma is the fifth most common malignancy in the US. Invasive and in-situ melanoma has rapidly become one of the leading cancers in the world. Malignant melanoma, the most deadly form of skin cancer, is one of the most rapidly increasing cancers. 62 480 incidences and 8420 deaths are the estimated numbers in the USA in 2008 (Jemal  et al. ,  2008 ). In malignant melanoma, early diagnosis is particularly important since melanoma can be cured with a simple excision operation. Dermoscopy, also known as dermatoscopy, is one of the major non-invasive skin imaging techniques that is extensively used in the diagnosis of melanoma and other skin lesions. This imaging technique offers more visible image subsurface structures when compared to conventional clinical images (Argenziano  et al. ,  2002 ; Fleming  et al. ,  1998 ). Dermoscopy also helps identifying various morphological features; for instance, blotches, streaks, blue-white areas, dots/ globules and pigment networks (Menzies  et al. ,  2003 ). Because of these unique features, screening errors can be reduced at the inspection. In addition, greater differentiation between difficult lesions, such as pigmented Spitz nevi, clinically equivocal lesions can be provided (Steiner  et al. ,  1993 ). Dermoscopic assessment remains one of the most critical steps in the diagnosis and subsequent treatment of malignant melanoma. Recent improvements in imaging techniques have led to the automated discovery of lesions. Traditionally, assessment of tumor margins is done manually by a dermatologist. The recognition of cancerous regions is a time consuming and error prone process, and it is innate in the nature of the human inspection. Unfortunately, for inexperienced dermatologists, dermoscopy may actually lower the diagnostic accuracy (Binder  et al. ,  1995 ). The use of a fast and reliable computerized system could markedly increase the number of examined images for the existence of cancer regions. Moreover, the computerized image analysis is able to minimize the effect of inter- and intra-observer variability. Inter-observer variability is defined in terms of the decisions assigned between different observers on the same subject. However, intra-observer variability is defined in terms of the decisions assigned within the observer; for instance, the same dermatologist judges differently on the same image at different times. Therefore, unlike inexperienced dermatologists, when it comes to trying to minimize the chance of diagnostic errors, it is important to develop computerized image analysis techniques. These techniques alleviate the difficulty and subjectivity of visual interpretations which are the major contributors to the diagnostic errors (Celebi  et al. ,  2009 ). In the investigation of melanoma, delineation of region-of-interest is the first and key step in the automated analysis of skin lesion images for many reasons. First and foremost, the border structure provides important information for accurate diagnosis. Asymmetry, border irregularity and abrupt border cutoff are of many clinical features calculated based on the border lesion. Furthermore, the extraction of other important clinical indicators such as atypical pigment networks, globules and blue–white areas critically depends on the border detection (Schaefer  et al. ,  2009a ,  b ). At the first stage for analysis of dermoscopy images, automated border detection is usually being applied (Celebi  et al. ,  2007a ,  b ). There are many factors that make automated border detection complex, e.g. low contrast between the surrounding skin and the lesion, fuzzy and irregular lesion border, intrinsic artifacts such as cutaneous features (air bubbles, blood vessels, hairs and black frames) to name a few (Celebi  et al. ,  2009 ). According to Celebi  et al.  ( 2009 ) automated border detection can be divided into four sections: pre-processing, segmentation, post-processing and evaluation. Pre-processing step involves color space transformation (Pratt,  2007 ), contrast enhancement (Delgado  et al. ,  2008 ) and artifacts removal (Celebi,  2008 ; Geusebroek,  2003 ; Lee  1997 ; Perreault and Hébert,  2007 ; Schmid,  1999 ; Wighton  2008 ; Zhou  2008 ). Segmentation step involves partitioning of an image into disjoint regions (Sonka  et al. ,  2007 ). Post-processing step is used to obtain the lesion border (Celebi,  2007a ,  b ; Melli,  2006 ; Iyatomi,  2006 ). Evaluation step involves dermatologists' evaluations on the border detection results. In this study, a proximity graph representation approach to detect regions and borders in skin lesions for dermoscopic images is introduced. This approach is based on a soft kinetic data structure (SKDS): graph spanners. 2 A SKDS Today the need for processing continuously moving points in a wide range of application areas such as geographic information systems, networking, traffic control, weather forecasting and nearest neighbor search is more pervasive than ever. The ideal solution for maintaining and processing continuously moving points has not been thoroughly presented yet. However, there are two recent approaches for processing moving points; they are dynamic data structures (DDS) and kinetic data structures (KDS). DDS assume that the data points change their positions only at explicitly known time steps; thus, they are not adequate solution for maintaining and processing continuously moving points. The alternative approach to DDS is given in the context of computational geometry with KDS which are recently introduced by Basch  et al.  ( 1999 ). In KDS, it is assumed that points' motions are unknown but not arbitrary since equations of motions are assumed to be algebraic functions of time (typically linear or polynomially bounded) (Guibas  et al. ,  1998 ). In other words, it is assumed that there are no sudden position leaps that cannot be defined by an algebraic function of time. KDSs consist of hierarchies of points that keep hierarchy updated for moving points. Superior form of KDS for dynamic points under unpredictable motions is called SKDS (Czumaj and Sohler,  2001 ). SKDSs are approximate data structures that are used to answer proximity queries. In the light of abovementioned definition of SKDS, to our knowledge, deformable spanner (DS) (Gao  et al. ,  2006 ) is the first SKDS that maintains its structure under formerly unknown motion models in 3D. DS supports all the criteria of uniformity, controllability, locality, being discrete and proximity based that a good KDS would provide (Alexandron  et al. ,  2007 ; Russel and Guibas,  2005 ). These criteria facilitate spanner's usage in different application areas. Locality of edges is affected by a small subset of the total point set. This means that changes in one part of the graph do not generally affect the spanner edges in the other parts. There is only one uniform combinatorial element in the spanner that is called edge. Spanner is controllable to allow us to produce controllers (e.g. expansion ratio) capable of capturing the shape of the object with various degrees of tightness or looseness. Spanners are discrete; thus, their description does not include any geometric coordinates. Proximities in the spanner determine the local interactions and in turn local interactions determine the behavior of the moving points. We consider the dermoscopy image's pixel colors in the context of computational geometry with SKDS. Our hypothesis is that: important characteristics (e.g. color combinations) of a skin lesion image can be obtained in a proximity graph representation by examining colors and their color-space (e.g. RGB) closeness relationships. Thus, we represent images as graphs to obtain color patterns. In order to represent an image as a graph, we take a cue from Gao  et al.  ( 2006 ) and produce graph spanner approach for image segmentation. High-level patterns from properties of unique colors represented in an image are exposed by a hierarchical graph spanner. SKDS approach we use—hierarchical graph spanner—representation (in short: balls hierarchy, BH) is explained in  Section 2.2 . Even though BH is capable of handling dynamic changes in images (e.g. video stream) in our case of detecting lesion borders in dermoscopy images; image pixels are static data points in 2D. There exists numerous innovative graph-based image segmentation approaches in the literature. Shi  et al.  ( 1998 ) treated segmentation as a graph partitioning problem, and proposed a novel unbiased measure for segregating subgroups of a graph, known as the normalized cut (NC) criterion. More recently, Felzenszwalb and Huttenlocher ( 2004 ) developed a segmentation technique by defining a predicate for the existence of boundaries between regions, utilizing graph-based representations of images. However, our graph spanner method approaches image segmentation problem as an approximate shortest path finding problem with a given expansion ratio. 2.1 Notations and preliminaries A  B i -level hierarchical representation is called clusters of balls ( B ) or BH. A pair (χ,  d ) is called metric space where χ is a point set and  d  is a distance function.  d : χ × χ → [0, ∞) satisfying the  d ( u ,  v ) = 0 if and only if points  u  and  v  are equal, and symmetry and triangle inequality hold for distance function  d ( u ,  v )= d ( v ,  u ),  d ( u ,  v )+ d ( v ,  z )≥ d ( u ,  z ), respectively, where  u ,  v  and  z  are nodes in the hierarchy.  B  is in the metric (χ,  d ) and is a sequence of levels  B 0 ,  B 1 ,…,  B h  where  .   indicates upper bound,  h  represents number of hierarchical levels,  R  represents radius at the highest level and ξ is a constant and called expansion ratio. This value determines how much a cluster ball will expand from one level to the upper level. For instance,  v  at level  i +1 is a parent of  u  at level  i  is represented as  P i +1 ( u i )= v  or  P ( u i )= v i +1 , where  u ∈ B i ,  v ∈ B i +1 .  P j ( u i )= v ,  j &gt; i  indicates that  u  at level  i  has  v  as a parent at level  j  where  j ≥ i +1.  C i ( v i +1 )= u  or  C ( v i +1 )= u i  indicates that  u  at level  i  is a child of  v  at level  i  + 1 or  C i ( v j ) = u,  j &gt; i ,  v 's  i -th level child is  u . One node may have multiple children at the same level and has single parent. However, one node can be covered by multiple cluster balls from the upper levels. Neighbors of  v  where  v  ∈  B i  is represented as  N ( v i ),  N ( v i ) = { u i  ∈  B i , | u i v i |≤η r ξ i −1 } where η is neighbor coefficient,  r  is minimum radius, and  r ξ i is a radius at the level  i . Ball  v 's neighbor at level  i  is represented as  N ( v i ) =  u i . 2.2 BH Hierarchical representation of graph spanners is approximating the metric space with a hierarchy of nodes based upon a sampling of the data subsets. The construction of these sets should be guaranteed with the following properties (Gao  et al. ,  2006 ):
 B h —the highest level cluster ball—covers entire set of points (χ) in the graph  G . Vertex set  V  initially exists from points where each point is a center of a cluster ball.  V i  represents vertices at level  i  where these vertices are cluster balls' centers at level  i . V =   and  V i  ⊆  V i −1 . No cluster ball at the same level covers other ball's center. Therefore,  d | c ( B i k ) c ( B i m )|&gt; r ξ i , where  i  is level,  c ( B i k ) ∈  V  represents center of a ball  k  at level  i .  B i k  ∈  B i ,  r ξ i  or  r i  is radius at the level, and  k  ≠  m . There is a geometrically decreasing order among the clusters;  B h ,  B h −1 ,…, B 0 , where  B 0  is the original point set itself and only contains a single vertex cluster. The cluster  B i -where  I  = 0, 1, 2,…,  h -is a ball with radius  r i  at level  i . Ball clusters are hierarchical.  B i  is refined representation of one lower level cluster  B i −1 . There is no lower level cluster which is not contained or covered by an upper level cluster. That means hierarchical representation of clusters is laminar family of sets. Each edge in hierarchy connects some neighbor clusters at the same level in  B i  (neighbor edge). Some children clusters of  B i  in  B i −1  are connected by edges (child edge). Each  B i −1  cluster is covered by cluster in  B i ( B i −1 ⊆ B i ). Therefore, | B i −1 |≥| B i | where | B i −1 | represents number of clusters at level  i −1. Equality holds only in the situation that each and every cluster becomes parent cluster of itself. Maximum number of distance scales (number of levels) is   where  t  is maximum distance between any two nodes. According to coverage property ∀ u ∈ B i −1  with  i  ≤  h , ∃ v ∈ B i . Every cluster ball's center is covered by one upper level's cluster ball's center. A cluster ball can be covered by another concentric ball ( u  =  v ) at one upper level. According to the separation property,  u ,  v  ∈  B i +1  where  u  ≠  v ,  d G ( u ,  v )≥ r ξ i , and  r  is minimum radius (radius at the first level where  i  = 0). Cousin property implies that two close points that belong to different balls at  B i  are children of the same ball at  B k  where  i  &lt;  k  ≤  h  ( h , number of levels in the hierarchy). This implies that cousins will have common ancestor. There exists a parent chain from a point  u  at level  i  to its parent at  j , where  j &gt; i . This chain can be followed by:  P j (…( P i +3 ( P i +2 ( P i +1 ( u i ))))…) =  v ,  u  =  v  when  u  expands up to the level  j  (parent of itself). There is no unique hierarchy even for the same point set. Different insertion order of points would result in different but equally good hierarchy construction. If  C i ( v i +1 ) =  u  or  C ( v i +1 ) =  u i  and  u  ≠  v , then  N ( v i ) =  u i . If a ball  u i  is a child of another ball  v i +1  at upper level, then  u i  must be neighbor of  v i . This implies that a ball cannot become a child before becoming a neighbor. This is because no position leap is assumed. 
Next section clarifies how these properties are satisfied and applied to construct BH. 2.3 Hierarchy construction In this section, survivors, non-survivors and balls represent points. Ball is used in order to represent point with its radius-covering.  Figure 1  illustrates BH construction steps for 10 points by using properties given in  Section 2.2 . In this illustration, hierarchy is consisting of four levels (property 9). Each level of the hierarchy is represented in different colors; black, red, blue and green colors, respectively. Dashed circles represent radius-covering at the level.
 Fig. 1. Hierarchy construction steps. In the first level original data points exist with minimum radius  r . Since minimum distance between closest color pairs (assuming in 3D RGB color space) in an image is 1,  r  is assigned as 1. As seen from the first level, since no other ball center is covered by any other ball, all survive (exist in the level). In the second level, only five points survive, since other points are covered by survivors with radius in the second level,  R 2. Non-survivor points become children of survivors. This relation is represented by child edges (see property 7 above) which are illustrated as steady black lines.  R 2 is expanded from minimum radius by expansion ratio (ξ); thus,  R 2 =  r ξ 1 . Superscript 1 represents level difference between level 2 and 1. Steady red lines in the second level represent neighbor edges (see property 6 above) which represent neighbor relations between any two survivor points ( N ( v i ) = { u i  ∈  B i , | u i v i | ≤ η r ξ i −1 }). Two survivors are neighbors if and only if distance between them is smaller than or equal to the neighbor coefficient (η) times radius at the level ( R 2). In the third level, there exist only three balls (blue) with radius  R 3=ξ 2 r , and a single non-survivor which becomes a child. Now, there are three neighbors (blue lines) in the third level. In level 4, there is only a single survivor (green) with two children. This survivor is called root and covers all existing points. Notice that in order a non-survivor point to become a child of a survivor point, it needs to be a neighbor of the survivor point in the previous level. Once hierarchy is constructed, BH keeps hierarchical representation of approximate shortest paths among all the existing points. Hierarchy construction time is  O ( n  log  n ). 3 THE HIERARCHY REPRESENTATION FOR DERMOSCOPY IMAGES Data points for an image are pixel's RGB colors. Once all pixels are inserted into the BH, color pixels are segmented as illustrated in  Figure 2 . Note that, due to the nature of RGB space similar colors are close to each other. After the construction, hierarchy is ready for a proximity query e.g. nearest neighbors. A proximity query will return any pixel with the specified color and its geometric neighbors in color space (RGB or any color space). Firing a query is corresponding to tree traversal.
 Fig. 2. A sample hierarchy representation of a dermoscopy image. As seen from  Figure 2 , the suspected cancer regions, borders and the background are available right after the indexing process. In  Figure 2  each branch of the tree is corresponding to one of the segments; the background (branch 1), right outside the border (branch 2), right inside the border including border (branch 3) and the lesion (inside the border, branch 4), respectively. Once hierarchy is constructed, the segments can be easily extracted by a simple hierarchy traversal. Depending on applications, segmentation also contributes to image regions or border identification. In dermoscopy images, some edges of the lesions are clearly defined while some have poorly defined edges such as the basal cell carcioma. Different significant parts of these images are successfully identified by performing three unique queries to recognize: (i) cancer region, (ii) cancer region border and (iii) the entire background. It is observed after the experiments that smallest number of children branch under the root node always indicates border. Since dermatologists focus on borders for diagnosis, we also return borders (branch 3 in  Fig. 2 ) as a result of border query. Note that this hierarchy indexing method does not require any initial seed point which means that BH is a global approach. Most often networks and points in nature are dynamic. Points are moving towards or against applied stimuli. Therefore, dynamic update operations such as insert and expand in the BH have profound impact on adapting hierarchy for motions. The following section will introduce insert and expand operations in the pseudo codes. 4 IMPLEMENTATION The most important operation in hierarchy construction is insert operation. To do that, first, expand operation is given as below. A range of precision and computation time trade-offs have been implicated for using two different insertion schemes; optimum insertion and best optimum insertion respectively. Since precision is more important for us, we implemented best optimum insertion scheme as seen in Algorithm 2 below. This insertion scheme inserts new point into the best optimum place instead of the first optimum place found. In the best optimum insertion, the closest point among all other points is found by simply traversing the tree. To do that, in Algorithm 2 node Q's all children must be traversed. When the closest node is found among all the children, this closest node must be traversed recursively until no covering node (see property 3 in  Section 2.2 ) is found. The last found node is called the best optimum place for insertion. Best optimum insertion scheme as a result succinctly provides higher precision and in turn leads to a better range query e.g. nearest neighbors. 5 RESULTS BH method is tested on a set of 100 dermoscopy images obtained from the EDRA Interactive Atlas of Dermoscopy (Argenziano  et al. ,  2002 ). These are 24-bit RGB color images with dimensions ranging from 577 × 397 to 1921 × 1285 pixels. The benign lesions include nevocellular nevi and dysplastic nevi. Two unique queries are used to extract the desired region of the images. The first query is performed by looking for an entire branch of a directly related child node (DRCN) of the root node (RN). According to  Figure 2 , node 1, 33, 572 and 916 are DRCNs. Node zero is the RN. Each DRCN along with all their children nodes form one branch of the hierarchy. The hierarchy segments an image based on distance between colors and stores the pixel values in separated branches. Therefore, by querying each branch of the hierarchy, different color segment of an image can be obtained separately. For instance, the border of an indexed image can be archived by querying the entire branch of 33 in  Figure 2 . The second query is very similar to the first query except that the neighbors of children of all DRCN are included. This query yields an additional region of the image: the background. According to  Figure 2 , children of one are neighbors of 916's children. The BH based border detection errors are objectively quantified using dermatologist-determined borders as the ground truth. The BH detected border images overlaid on top of the dermatologist-determined border images. Quantative error metrics such as true/false positive/negative ratios found according to the overlay images.  Figure 4  shows sample original images [which are selected among 100 images with the highest, lowest, and on average border error (BE) ratios]. The BH detected borders and interiors (e.g. combination of branches 3 and 4 of  Fig. 2 ), overlaid images, and dermatologist-determined region are all illustrated, respectively, in  Figure 4 . Second, accuracy of our method is quantified by digitally comparing results with manually determined borders from a dermatologist. We evaluated border detection error of BH. Manual borders were obtained by selecting a number of points on the lesion border, connecting these points by a second-order B-spline and finally filling the resulting closed curve (Celebi,  2008 ). Using the dermatologist-determined borders, the automatic borders obtained from the BH are compared using three quantitative error metrics: BE, precision and recall. BE is developed by Gao  et al.  ( 1998 ) and Schaefer  et al.  ( 2009a ,  b ), and given by
 where ⊕ is exclusive OR operator, essentially underlines disagreement between target (ManualBorder, MB) and predicted (AutomaticBorder, AB) regions. Referring to information retrieval terminology, nominator of the BE means summation of false positive (FP) and false negative (FN). Denominator is obtained by adding true positive (TP) to false negatives (FN). An illustrative example is given in  Figure 3 . In the figure, assume that red and blue borders are drawn by a dermatologist and a non-expert, respectively. TP indicates correct lesion region found automatically. Similarly, TN shows healthy region (background) for both manual and computer assessment agreed on. FN and FP are labels for missed lesion and erroneous positive regions, respectively. Addition to BE, we also reported precision (positive predictive value) and recall (sensitivity) for each experimental image in  Table 2 . Note that all definitions run over number of pixels in the particular region.
 
 Fig. 3. Accuracy and error quantification. Figure 4  demonstrates how our method is compared quantitatively against dermatologist drawn image. In the figure left is original image, middle is dermatologist drawn image, and right (red area) is automatically found border image which is overlaid on top of the manually drawn image. In the experiments for 100 dermoscopy images dataset, the expansion ratio constant is determined by randomly chosen three sample images as shown in  Table 1 . As a result, the expansion ratio is chosen as 1.5 since this value was maximizing both precisions and recalls for three sample images as seen in  Table 1 . In  Table 2  results for 100 images are displayed. For 100 images, BH's BE's mean is 2.28% (and σ = 3.01), precision's mean is ∼1.0 (and σ = 0.001) and recall's mean is 0.97 (σ = 0.003). The results show that accuracy of assessment averages out at 97.72%. A rough comparison of our findings with Schaefer  et al.  ( 2009a ,  b ) and Celebi  et al.  ( 2008 ) showed that the mean errors of our method are obviously less than their results.
 Fig. 4. Overlay images. 
 Table 1. Benchmark results for different expansion ratios for three sample images Expansion ratio 1.2 1.5 2.0 Precision Image 1 0.927 0.975 0.976 Image 2 0.881 0.882 0.878 Image 3 0.977 0.939 0.930 Recall Image 1 1 0.995 0.995 Image 2 0.999 1 0.999 Image 3 1 1 0.999 
 Table 2. Border errors, precision and recall of BH Img. ID Border error (%) Precision Recall Img. ID Border error (%) Precision Recall 1 1.30 1 0.98 51 3.09 1 0.96 2 1.46 1 0.99 52 1.86 1 0.99 3 0.90 1 0.999 53 2.60 1 0.96 4 2.10 1 0.98 54 0.00 1 1 5 1.10 1 0.98 55 0.02 1 1 6 2.30 1 0.99 56 4.81 1 0.96 7 0.50 1 1 57 2.60 1 0.98 8 0.75 1 1 58 2.03 1 0.98 9 0.44 1 1 59 1.67 1 0.99 10 0.00 1 1 60 0.00 1 1 11 0.00 1 1 61 1.00 1 0.99 12 1.70 1 0.97 62 2.43 1 0.98 13 3.60 1 0.93 63 1.93 1 0.99 14 0.00 1 1 64 0.17 1 1 15 2.30 1 0.96 65 0.05 1 1 16 1.99 1 0.98 66 3.10 1 0.96 17 2.98 1 0.95 67 3.10 1 0.96 18 2.00 1 0.99 68 1.13 1 0.99 19 2.67 1 0.98 69 0.27 1 1 20 2.40 1 1 70 2.68 1 0.96 21 1.98 1 0.99 71 4.80 1 0.89 22 2.76 1 0.98 72 2.68 1 0.94 23 2.85 1 0.98 73 0.56 1 0.99 24 4.95 1 0.91 74 3.10 1 0.94 25 2.00 1 0.96 75 3.00 1 0.92 26 5.30 1 0.92 76 5.30 1 0.9 27 3.20 1 0.98 77 1.00 1 0.98 28 3.30 1 0.96 78 7.45 1 0.89 29 2.30 1 0.98 79 6.36 1 0.9 30 2.78 1 0.97 80 4.43 1 0.9 31 2.98 1 0.99 81 0.43 1 0.99 32 3.05 1 0.96 82 3.50 1 0.92 33 0.90 1 1 83 4.78 1 0.91 34 0.89 1 1 84 1.00 1 0.99 35 0.00 1 1 85 2.94 1 0.98 36 1.23 1 0.98 86 4.10 1 0.9 37 1.00 1 0.99 87 2.56 1 0.95 38 2.20 1 0.98 88 2.99 1 0.94 39 0.05 1 1 89 1.34 0.99 0.98 40 1.23 1 0.98 90 4.20 1 0.91 41 3.56 1 0.99 91 2.72 1 0.93 42 0.78 1 1 92 2.23 1 0.97 43 2.80 1 0.99 93 0.55 1 0.99 44 5.59 1 0.93 94 8.24 1 0.9 45 1.89 1 0.97 95 1.42 1 0.99 46 0.10 1 1 96 2.86 1 0.93 47 1.58 1 0.99 97 4.00 1 0.93 48 0.00 1 1 98 0.84 1 0.97 49 2.60 1 0.98 99 6.17 1 0.89 50 3.10 1 0.97 100 0.62 1 0.99 
 6 COMPARISON AND DISCUSSION In this section, existing graph based image segmentation methods NCs method (Shi and Malik,  1997 ), the efficient graph based segmentation method (EGS) (Felzenszwalb and Huttenlocher,  2004 ), and the BH are compared with respect to efficiency on lesion border detection and computation complexities on dermoscopy images. C++ source codes for NC and EGS obtained from authors' websites. In  Figure 5 , results for a single lesion image generated from NC, EGS and BH overlaid, respectively, on the physician drawn ground truth image. As shown in  Figure 5 , much better results are obtained from BH.
 Fig. 5. Segmentation results from NC, EGS and BH from left to right. In NC, minimization of the normalized cost function is NP hard. This method down-samples the original image then conducts initial segmentation in down-sampled image and finally initial segmentation is expanded to the original image. Its computation complexity is  O ( n 3 ). Therefore, it only works on small sized images (e.g. 100 × 90). NC is a supervised method which requires expected number of clusters as a parameter. It is efficient on finding global segments; however, its efficiency reduces greatly if there are noises or tiny blemishes in the image. As seen in  Figure 5 , that degrades efficiency of the method for border detection in lesions since border regions have similar features with noise. Overall average precision of NC for 100 dermoscopy image dataset is 100%; however, average recall is 67% which makes NC inefficient for border detection in dermoscopy images. However, EGS is able to preserve details in low-variability image regions (e.g. shadow regions) while ignoring details in high variability regions. Therefore, as seen from illustration in  Figure 5 , high variability regions in the interior of border (holes) are segmented as different regions. In this approach, results will be considerably affected by outliers since even two segments with low weight edge between them are combined as a single segment. Even though the computational complexity is  O ( n  log  n ), in order to make the method more robust to outliers; definition of the difference between two regions needs to be changed. However, finding correct definition of difference is NP hard. The threshold function is the key element to determine the size of the segments. Variability on threshold changes results drastically. In this approach, when threshold is low, accuracy becomes high and computation speed reduces significantly. Overall average precision of EGS for 100 dermoscopy images is 100%; however, average recall is 55% which makes EGS also inefficient for border detection in dermoscopy images. The BH is capable of accurately locating segments and has following features. First, it locates all local, non-local and global segments in a single hierarchical structure. Global segments are represented at higher levels in the hierarchy, non-local segments are represented in middle levels of the hierarchy, and local segments are represented in lower levels of the hierarchy. Second, it is a seedless and non-supervised method which does not require prior knowledge about expected number of clusters. Third, it is dynamic. Since it is a SKDS, it is capable of updating hierarchy according to dynamic changes. This dynamic nature may lead to analyzing dynamic behavior of changes in segments by time. Fourth, inherently hierarchical classes of special structures are hold in the hierarchy. There is inherent hierarchical structuring in nature. For instance, in a scene object parts exist in objects and lower-level features exist in object-parts in a hierarchical fashion. Although these spatial patterns (objects, object-parts, etc.) are quite different, they are manifested in different levels of the BH. Finally, unlike EGS and NC, BH has single parameter which is called expansion ratio. As seen from  Table 1 , change in expansion ratio does not affect results significantly unlike in EGS. The complexity of the BH is  O ( n  log  n ); however, it runs in near linear time with respect to number of graph edges. In addition, the complexity of dynamic update operation is  O (log  n ). Detail precision, recall, BE rates are listed for BH in  Table 2 . 7 CONCLUSION In this study, a novel approach for automatic detection of skin lesions: BH is presented. The BH is a SKDS that keeps proximity graph representation in hierarchical form. It maintains geometric closeness relations among the point sets (colors). This data structure builds a hierarchical decomposition of a connected graph with certain properties (see  Section 2.2  for properties). Our approach is examined on a set of 100 dermoscopy images. Error rates: false positives and false negatives along with true positives and true negatives are quantified by digitally comparing results with manually determined borders from a dermatologist as the ground truth. The assessments obtained from our method are quantitatively analyzed with respect to BEs, precisions and recalls. Moreover, visual outcome showed that our method effectively delineated targeted lesions. Results proved that accuracy of automated assessments with the BH averages 97.72% which is higher than previously proposed methods. Also, BH is compared against well-known graph based segmentation methods on dermoscopy images and BH outperformed these methods. As a result, our approach finds both the lesions and the lesion borders with high precision rates. Conflict of Interest : none declared. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Predicting site-specific human selective pressure using evolutionary signatures</Title>
    <Doi>10.1093/bioinformatics/btr241</Doi>
    <Authors>Sadri Javad, Diallo Abdoulaye Banire, Blanchette Mathieu</Authors>
    <Abstract>Motivation: The identification of non-coding functional regions of the human genome remains one of the main challenges of genomics. By observing how a given region evolved over time, one can detect signs of negative or positive selection hinting that the region may be functional. With the quickly increasing number of vertebrate genomes to compare with our own, this type of approach is set to become extremely powerful, provided the right analytical tools are available.</Abstract>
    <Body>1 INTRODUCTION One of the central goals of comparative genomics is to use the comparison of genomes from different species to delineate functional regions in those genomes. Since most functional regions are under various degrees of negative selection, they tend to exhibit higher interspecies sequence conservation than their flanking neutral regions. Although mutations occur randomly in the genome (though at rates that may vary according to sequence context), the rate at which they become fixed in a population depends, among other things, on the fitness of the mutated individuals ( Moran and Pierce, 1962 ). Whereas non-functional regions of the genome evolve mostly due to neutral drift ( Kimura, 1983 ), functional regions are, for the most part, under negative selection, i.e. most mutations are deleterious. A consequence of this is that, over time, more mutations become fixed in non-functional regions than in functional regions. This principle is the foundation of phylogenetic footprinting, whereby one can hope to distinguish functional from non-functional regions of a given genome based on the observed number of mutations they have undergone during the evolution of a set of species. This approach has been used with success to identify all kinds of functional regions of the human and drosophila genomes (among others), including protein-coding genes ( Dewey  et al. , 2004 ;  Gross and Brent, 2006 ), non-coding RNA genes ( Dowell and Eddy, 2006 ;  Pedersen  et al. , 2006 ) and transcription factor binding sites ( Loots and Ovcharenko, 2004 ;  Moses  et al. , 2004 ). A number of generic approaches have been developed to identify sites that appear to be under negative selection based on comparative genomics. After identifying and aligning orthologous regions from two or more related species [e.g. using Multiz ( Blanchette  et al. , 2004a ) or MLAGAN ( Brudno  et al. , 2003 )], one can scan the alignment to identify regions where the sequence conservation is higher than expected. Early approaches dealt with a small number of genomes and evaluated conservation based on a sliding window strategy ( Boffelli  et al. , 2003 ;  Frazer  et al. , 2004 ;  Margulies  et al. , 2003 ). More recent approaches such as the very popular PhastCons approach ( Siepel  et al. , 2005 ) use a tree hidden-Markov model (HMM) to assign sites to one of several rate categories. A strength of this type of approach is that it can take advantage of the fact that most functional regions involve several consecutive sites, while avoiding the drawback of using a fixed-size window. This family of approaches has been used to identify likely functional regions in all kinds of species, including vertebrates ( Margulies  et al. , 2003 ,  2007 ;  Thomas  et al. , 2003 ), yeast ( Kellis  et al. , 2003 ) and drosophila ( Stark  et al. , 2007 ), among others, although most of these regions remain functionally uncharacterized to date. With the number of genomes being sequenced quickly increasing, the prospect of accurately measuring evolutionary rates and selective pressure at  individual  sites became an achievable goal. Several approaches were developed for this purpose, including SCONE ( Asthana  et al. , 2007 ) and GERP ( Cooper  et al. , 2005 ), which both attempt to evaluate selective pressure on a site-by-site basis. What these approaches lose in specificity because of the fact that they do not combine signals from neighboring sites, they gain in sensitivity, since they are able to detect very small regions under selection. A significant drawback of all these approaches is their assumption that the mutation rate in a given region or at a given site has not changed during the evolution of the set of species considered. Although negative selection in species ancestral to human and in sister species is clearly informative about selection in human, it is only an indirect predictor of it. Past results do not guarantee future performance in the stock market—neither does past selection necessarily implies current selection in human. There is considerable evidence that certain types of short functional regions such as transcription factor binding sites indeed turn-over quickly, causing a given locus to evolve neutrally in some species and under selection in another ( Moses  et al. , 2006 ). Recently, Siepel  et al.  introduced PhyloP ( Pollard  et al. , 2010 ), an impressive package that allows the detection of sites under negative or positive selection, while allowing changes in evolutionary rates over the branches of the phylogenetic tree ( Siepel  et al. , 2006 ). To our knowledge, this represents the best approach available to date for identifying individual sites under selection. In this article, we propose a very different approach to the identification of functional sites in a given reference genome. Instead of  measuring  past selection at a site, we use the evolutionary history at a site (and at neighboring positions) to  predict  current (or very recent) selective pressure. More specifically, using whole-genome multiple alignments for a collection of 44 vertebrates ( Margulies  et al. , 2007 ;  Miller  et al. , 2007 ), we first reconstruct ancestral sequences ( Blanchette  et al. , 2004b ;  Diallo  et al. , 2010 ) and identify evolutionary events at each site and along each branch of the tree. We then train machine learning classifiers to use the evolutionary history of a region to estimate the likelihood that it will have undergone a substitution in recent human history (since human–chimp divergence). Sites that are strongly predicted to remain conserved are likely functional sites in human. This approach has a number of key advantages. First, it does not require modeling or making assumptions about the evolution of functional regions of the genome process. Second, it is free to use whichever features of evolutionary history, and to weigh each feature as it wishes, in order to maximize the accuracy of the selective pressure prediction in human. A predictor may for example weigh more heavily conservation along primate lineages than along more distantly related lineages. It may also weigh more or less heavily conservation at neighboring sites. This article is organized as follows. We first study how individual evolutionary events along specific branches of the phylogenetic tree relate to the probability of substitution in human. We then propose a set of machine learning classifiers, including a Naive Bayes classifier, a  k -nearest neighbor classifier and a support-vector machine (SVM) classifier and show that some of them outperform recently proposed measures of sequence conservation. Finally, we show that the sites predicted by our best predictor (SVM) show better evidence of selection and function than those of existing conservation measures. 2 METHODS 2.1 Alignments and ancestral reconstructions The genomes of 44 vertebrate species have been completely or partially sequenced to date. These include 7 primates, 25 other mammals, 2 marsupials, 3 birds and reptiles, 1 frog and 5 fish. Roughly half of these genomes are completely sequenced, while the other half is sequenced to at least 2× coverage ( Margulies  et al. , 2005 ). A whole genome 44-way multiple alignment was produced by Miller  et al.  (unpublished, based on the methology described in  Miller  et al. , 2007 ) using the MultiZ multiple alignment program ( Blanchette  et al. , 2004a ) and is available from the UCSC Genome Browser ( Kuhn  et al. , 2006 ). MultiZ produces a set of alignment blocks, where each block contains (presumably) orthologous regions from a subset or all the 44 species. When no reliable alignment can be found in a given species, that species is not included in the alignment block. We applied a maximum likelihood ancestral sequence reconstruction approach ( Diallo  et al. , 2007 ,  2010 ) to infer, for each multiple alignment block, the ancestral sequence at each of the 43 internal nodes of the vertebrate phylogenetic tree. The program uses a tree-HMM approach to infer insertions and deletions over the branches of the tree, and then infers substitutions using a context-dependent substitution model.  Blanchette  et al.  (2004b ) and  Diallo  et al.  (2007 ) have previously shown, using simulations, that the expected accuracy of this reconstruction can be as high as 99% base-by-base accuracy for early eutherian ancestors such as the Boreoeutheria ancestor, and that it is above 90% for almost all other ancestral nodes of the eutherian phylogeny. 2.2 Selection of unambiguously human conserved and mutated sites A subset of human non-protein-coding genomic sites that could be unambiguously labeled as having undergone a substitution along the human branch since the human–chimp ancestry were identified. Several filters were applied to ensure that this set of sites is as enriched as possible for bona-fide mutated sites, and not the result of alignment errors. For site  i  to be considered as eligible, the following rules were applied:
 Site  i  must be conserved along the branches leading to the orangutan, gorilla and chimp. Sites  i −1 or  i +1 must be perfectly conserved between the human–chimp ancestor and both human and chimp. If site  i  is not conserved from the human–chimp ancestor to human, then it must be a transition (purine-to-purine or pyrimide-to-pyrimidine). Transversions are not considered, as they occur at different rates than transitions, which may introduce biases. If the human–chimp ancestral nucleotide is C, then it must not be followed by a G. This avoids CpG → TpG substitutions, whose elevated rate ( Siepel and Haussler, 2004 ) may bias our analyses. 
Finally, sites that satisfy all the above requirements are either called conserved or mutated, based on the event that took place between the human–chimp ancestor and human. When applied to human chromosome 22, these filters resulted in the identification of ~41 600 mutated non-coding sites, and roughly one hundred times more non-coding conserved sites. A subset of 41 600 conserved sites was then selected randomly from all conserved sites, to form a balanced set of 83 200 examples, which was divided into a 50 000-example training set and a 33 200-example test set. 2.3 Feature set definition and extraction Various approaches were considered to encode the history matrix  H  into a set of features that can be used to train classifiers. Those that resulted in the best results were the following. Feature Set  1: This contains the number of conservations and substitutions at each position within the window:  F 1 C ( p )=∑ b   1 H ( b , p )= C ,  F 1 S ( p )=∑ b   1 H ( b , p )= S . This feature set thus contains 2(2 w +1) integer features. Insertions and deletions are not explicitly accounted for in this feature set, but the presence of a large number of these events reduces the counts of conservations and substitutions and thus impacts the values of the features. An alternate feature set where all five types of events were counted separately was also evaluated but produced slightly worse results, probably because the increase in the size of the feature set is not counterbalanced by the informativeness of the new features. Feature Set 2 : This takes an orthogonal approach and counts events for each branch of the tree rather than for each site. Specifically,  F 2 C ( b )=∑ p ∈ W ( i , w )   1 H ( b , p )= C ,  F 2 S ( b )=∑ p ∈ W ( i , w )   1 H ( b , p )= S . This feature set thus includes 85×2=170 integer features, irrespective of window size. A number of alternate feature sets have been considered including using the matrix  H  itself (or part of it corresponding to a smaller window around the site). However, no predictor was able to take advantage of the richness of the provided information, probably in part because of the huge feature space involved. 2.3.1 Other measures of sequence conservation The PhyloP package ( Pollard  et al. , 2010 ) includes implementations of PhastCons ( Siepel  et al. , 2005 ), GERP ( Cooper  et al. , 2005 ), evolutionary rate likelihood-ratio test (LRT), as well as the PhyloP-SCORE. Each was run on the MultiZ 43-species multiple alignment obtained by removing the human sequence from the alignment, so that human conservation/mutations do not affect the scores produced. Default parameters were used for each algorithm. Each program outputs a score for each site in the dataset. 2.4 Classifiers Our Naive Bayes approach was implemented in the straightforward manner, using uniform pseudocounts to estimate posterior probability features of the given class label. This classifier can be trained in a few seconds and make predictions equally quickly. The  k -nearest neighbor approach used the Euclidian distance between feature vectors to identify neighbors. We obtained best results on Feature Set 2, using  w =0. Fairly large values of  k  (number of neighbors) produced better results;  k =400 was used for the results reported here. This approach is substantially slower as the running time required to classify a single test example is proportional to the number of training examples. Nonetheless, it runs in a few minutes on a standard desktop computer. We used the SVMlight package ( Joachims, 1999 ) from a Matlab interface to train classifiers for Feature sets 1 and 2. While our efforts to reduce overfitting failed for Feature set 2 (which contains a large number of features), we obtained our best results on Feature Set 1, with window size  w =0 and  w =1. We obtained our best results using a radial basis function (RBF) with γ=1.5 (kernel parameter),  C =100 (trade−off between training error and margin) and  J =0.7 (relative cost of errors on positive examples). This is type of classification problem where the examples are fairly poorly separable results in a very large number of support vectors (more than half the training examples are retained as support vectors), which impacts running time and generalization. However, for Feature Set 1, the training and testing errors were essentially equal. Training on our 50 000 example took ~1 h, and predicting on the 33 200 test examples took &lt; 10 min. 3 RESULTS Our goal is to develop a machine learning predictor that will estimate the probability that a given site of the human–chimp ancestor will undergo a substitution along the branch leading to human, given the complete set of evolutionary events that took place in that region during vertebrate evolution (but excluding recent human evolution). Because the only way to predict positions where substitutions will become fixed is to predict their effect on fitness, sites that are predicted to remain conserved with high probability are likely to be functional ones. It may seem counter–intuitive that in order to predict the selective pressure on a site in human, we purposefully ignore evolutionary events having taken place along that branch. In fact, although events along the human branch are excluded from our feature set, they play a very important role in our training set, as they form the label of each training example. We also underscore the fact that, although a substitution along the human branch is a very strong indicator of the absence of selective pressure at that site, such events are also extremely rare (~0.5% of sites). Thus, events along the human branch are much more productively used as labeled of a (artificially balanced) training set than as features. 3.1 Training data Our study is based on a balanced dataset consisting of all 41 600 non-coding sites from human chromosome 22 with unambiguous substitution along the human branch since the human–chimp ancestry and equally many non-coding sites with no substitutions, randomly sampled from the same chromosome. Coding regions were excluded from consideration because they can be accurately detected using a variety of approaches [see ( Siepel, 2007 ) and references therein] and they obey fairly different rules than non-coding sites. A set of rules were applied to ensure that apparent human substitutions are not simply due to alignment errors or to increased substitution rate caused by sequence context effects (see  Section 2 ). We then inferred the evolutionary history of each site, together with a 501 bp window centered on it, using a multiple sequence alignment of the genomes of 44 vertebrate species ( Miller  et al. , 2007 ) ( Fig. 1 ) and a maximum likelihood ancestral sequence inference approach for both substitutions and indels ( Diallo  et al. , 2007 ,  2010 ) (see  Section 2 ). The full history of a site  i  was then encoded as a matrix  H i  with 85 columns (corresponding to the 85 branches of the phylogenetic tree, excluding that leading to human) and 501 rows (corresponding to the 501 human sites in the window surrounding site  i ), where the entry  H i (δ, b ) for branch  b  at relative position δ∈{−250,…,−1,0,1,…,250} corresponds to the evolutionary event inferred on that branch at position  i +δ: C(onservation), S(ubstitution), I(nsertion), D(eletion) or G(ap). The ‘Gap’ event denotes the presence of a gap in both the ancestor and descendant of branch  b . Our goal is to assess the extent to which the fate of site  i  along the human branch can be predicted from the matrix  H i .
 Fig. 1. Phylogenetic tree of the 44 vertebrate species considered to predict regions under selective pressure in human. 3.2 Individual feature informativeness We first measured how informative are individual events along each branch of the tree. This information can be measured by several means. First, we consider the question of whether the presence of orthologous bases in a given species (extant or ancestral) affects the likelihood of a conservation event along the human branch. A human site may have no detectable ortholog in a given species  s  for several reasons: (i) Site  i  was inserted after the last common ancestor of  s  and human [denoted LCA( s ,human)]; (ii) Site  i  was deleted since the LCA( s ,human) along the lineage leading to  s ; (iii) Site  i  actually has an ortholog in  s , but that and the surrounding sequence have diverged to the point where orthology cannot be detected (or, in the case of ancestral sequences, none of its descendant has a detectable ortholog).  Figure 2 a plots the likelihood ratio of human conservation in the presence or absence of an orthologous base on branch  b  at site  i +δ:  . As expected, one observes that detectable orthology is relatively uninformative for primate species, as the vast majority of both functional and non-functional human sites have orthologs in these species. However, the value of orthology increases as we consider more divergent species, especially fast evolving ones such as rodents. This is because for highly diverged species, an increasing fraction of non-functional regions are either deleted or mutated beyond recognition, thus concentrating human functional sites in the fraction of sites with detectable orthologs. For example, sites with orthologs in other primate species are only ~7% more likely to be conserved than those without primate orthologs, but this number increases to 16% for other eutherians, 23% for marsupials and 33% for birds and reptiles. The trend presumably continues for more distant species such as fish, but we have insufficient data to observe it. It is interesting to consider how the events occurring at neighboring sites at position  i +δ are also quite informative on the fate of site  i , even for large δ. It appears that the presence of bases with a human ortholog even located 250 bp away from the current site is only marginally less informative than considering orthology at the site itself. This is due to the fact that functional regions and detectable orthology blocks are generally quite large.
 Fig. 2. Individual feature informativeness. ( a ) Log-likelihood ratio of human conservation in the presence or absence of orthologous bases in the given species and at the given offset from the considered position. ( b ) Log-likelihood ratio of human conservation in the presence of a conservation versus a substitution along the given branch and at the given offset. For both (a) and (b), ratios for non-mammalian species are too noisy and are not shown. ( c ) Mutual information between the presence of human conservation and the event along each branch at each offset. Next, we ask whether the actual event (conservation or substitution) taking place at site  i +δ along branch  b  brings any information on the fate of site  i  in human:  . Note that the probabilities at both the numerator and denominator assume that site  i +δ has a detectable orthology between human and the species considered; they differ only in the fate of the site on branch  b .  Figure 2 b reveals a trend that is quite different from that seen previously. Here, events taking place along branches close from human (e.g. primates) are the most informative. In fact, it appears that the main determinant of this likelihood ratio is how far away from the human lineage (defined as the set of branches leading from the root of the tree down to human) is the branch being considered. All branches along the human lineage have high log-ratios. This ratio decreases as we consider branches that are more distant from that lineage. It remains fairly large for ancestral branches close to it (e.g. those leading to lemurids and artiodactyls, for example), but decreases as we reach more distant branches (e.g. those leading to most extant non-primate species). This observation is new and quite significant. It suggests that as species diverge away from the human lineage, the function of some sites changes, making events along branches far from the human lineage less informative when it comes to studying the human genome. Figure 2 a and b fail to reveal an important feature of our data: detectably orthologous sites become increasingly rare as more and more distant species are considered, in such a way that &lt;2% of human sites have orthologs in chicken, and this number drops to &lt;0.1% in fish. A useful measure combining both the frequency of orthology and the informativeness of the evolutionary events taking place at or around the site is the mutual information between the event taking place along branch  b  at site  i +δ and the presence or absence of conservation along the human branch:
 
Strikingly, the amount of mutual information is quite uniform over all the eutherian subtree. The single major exception are branches belonging to the old world monkeys subtree (the most closely related to human), which, despite their relative short length, bear significant amount of information over the fate of sites along the human branch, probably because their functionality is very similar to that in human. This would indicate that marks of selective pressure in old world monkeys is a very strong indicator of human selection, as previously suggested ( Boffelli  et al. , 2003 ). We also note that branches outside the eutherian subtree provide relatively little information on the fate of human sites, because the vast majority of human sites are aligned to gaps in those species. 3.3 Learning evolutionary signatures We now turn to the problem of training a classifier to predict the fate of a human site based on the evolutionary history of its surroundings. Although one could in principle use the matrix  H i  as the set of features from which to train a classifier, its size (85×501=42585 features) is too large to hope to obtain good results with the size of the training set we are using (but see  Section 4 ). Consequently, we considered two types of summary feature sets, parameterized by a window size parameter  w ≤250, that reduce that number of features. Let  W ( i , w )={ i − w ,…, i ,…, i + w } be the set of 2 w +1 sites surrounding  i .
 Feature Set 1 contains 2·(2 w +1) features: for each position  j ∈ W ( i , w ), we record the total number of ‘C’ and ‘S’ observed at site  j  (summed over all branches). Feature Set 2 contains 2×85 features: for each branch  b , we record the total number of ‘C’ and ‘S’ observed along branch  b , summed over all sites in  W ( i , w ). 
 The decision to exclude counts of ‘I’, ‘D’ and ‘G’ events was made on the basis that these events are relatively rare in mammals and contribute little to the prediction problem while greatly increasing the number of features. Note, however, that although the counts of ‘I’, ‘D’ and ‘G’ events are not individually present as features, their presence is nonetheless reflected in the feature set through the counts of ‘C’ and ‘S’. For example, in Feature Set 1, the total number of ‘I’, ‘D’ and ‘G’ events at a site is given by 85− n ( C )− n ( S ). 3.4 The difficulty of the classification problem We start by illustrating the difficulty of the classification task at hand, in order to calibrate our expectations. The fraction of human sites under selection has been estimated (based on human-mouse alignments) to be at least 5% ( The International Mouse Genome Sequencing Consortium, 2002 ); more recent estimates place it between 4% and 7% ( Margulies  et al. , 2007 ). If we assume that the substitution rate in regions under selection is on average half the rate in neutral regions and that the probability of a neutral site mutating between the human–chimp ancestor and human is 0.5%, we obtain that   ~2.56% of mutated sites are under selection, while this fraction jumps to   ~5.01% among conserved sites. Thus, both mutated and conserved sites are rich in non-functional sites, but conserved sites are almost two times richer in selected sites. Consider, for the sake of example, a balanced training set of 1000 human-mutated sites and 1000 human-conserved sites. We expect the mutated sites to contain ~975 neutral sites and 25 selected sites, while the conserved sites should contain ~950 neutral sites and 50 selected sites. This has significant implications on the accuracy that our human-conservation predictor can be expected to obtain. For example, consider a classifier Ω that is able to distinguish selected from non-selected sites with 100% accuracy and that bases its prediction of human conservation prediction on this. The classifies Ω would predict 25+50=75 sites as conserved, of which 50 would be correct (67% positive predictive value). It would predict 975+950=1925 sites as mutated, of which 975 would be correct (50.64% negative predictive value). This is the best classification accuracy we can expect (under the assumption that the substitution rate at selected sites is half that at neutral sites). Remember, however, that our true goal is not to train a predictor to predict mutated versus conserved sites, but to predict non-functional versus functional sites. As we just saw, a relatively low accuracy at the mutated/conserved prediction task does not mean an equally low accuracy at the non-functional/functional prediction task. 3.5 Accuracy of predictors of human selection Our set of 83 200 examples was divided into a training set of 50 000 examples and a test set of 33 200 examples, both with exactly the same number of human-conserved and human-mutated sites. The results reported in this section describe the performance on the test set, which was not used during the training phase. Several types of classifiers were trained on each type of feature sets (see  Section 2  for details on training and parameter tuning):
 Naive Bayes:  classifiers calculate the posterior probability of the example label conditional on the set of observed features, under the (often unrealistic) assumption that features are independent. Despite their relative simplicity, Naive Bayes classifiers have proved remarkably useful in several settings ( Rish, 2001 ). k-nearest neighbor predictors (KNN)  ( Shakhnarovish and Indyk, 2005 ): This simply obtain the probability of an example being positive based on a voting procedure using the  k -nearest (most similar) training examples. This type of classifier can be very accurate for very large datasets, if an appropriate measure of similarity is available. SVM  ( Vapnik, 1995 ): This construct the optimal separating hyperplane between the two input classes to maximize the margin between the examples of the two classes. In the case of non-linearly separable classification problems, SVM uses kernel functions in order to implicitly map the feature vectors to a very high-dimensional feature space to obtain non-linear boundaries. After training SVM, support vectors are identified as those training examples that best define the boundary between the two classes. In the past few years, SVM have shown very good performance in a number of machine learning and pattern recognition problems ( Cristianini and Shawe-Taylor, 2000 ). 
 All three types of predictors produced poor results when large values of  w  were used. The Naive Bayes and KNN classifiers performed best on feature set 2 (with  w =0), whereas the SVM-based approaches were unable to handle the large number of features this set contains. On the other hand, the SVM approach was very effective at using the smaller number of features from Feature Set 1 and produced good results for both  w =0 and 1.  Figure 3  shows the positive predictive values (PPV, defined as the ratio of the number of true positive predictions to the number of positive predictions) obtained for each of these classifiers. Because we expect the fraction of functional sites in our balanced training and testing sets to be relatively small (probably around 5 to 10%), we only plot PPVs for prediction thresholds resulting in up to 20% of the test examples being predicted positive. The two SVM predictors (Feature Set 1,  w =0 or 1) clearly outperforms all other approaches over much of the range of prediction threshold. The Naive Bayes and KNN predictors perform relatively poorly for high-confidence predictions, although they become competitive with the two SVM predictors at lower confidence calls.
 Fig. 3. Performance of various previously published measures of sequence conservation (PhastCons, PhyloP-SCORE, GERP, PhyloP-LRT), compared with predictors developed in this article.  X -axis: fraction of test examples predicted as positive;  Y -axis: positive predictive value (fraction of human-conservation predictions that are indeed human conserved). In addition to these three types of classifiers trained on our two feature sets, we considered four previously proposed measures of sequence conservation, all implemented in the PhyloP package ( Pollard  et al. , 2010 ), that aim at detecting sites under selection (although not specifically along the human lineage):
 PhastCons ( Siepel  et al. , 2005 ): this approach identifies genomic regions with reduced mutation rate using a tree-HMM approach that assumes that neighboring sites are highly likely to have similar rates. It has been shown to perform very well at identifying various types of functional sites, in particular larger ones such as exons, enhancers and RNA genes. Genomic Evolutionary Rate Profiling (GERP) ( Cooper  et al. , 2005 ): this approach assigns a conservation score to each site in an alignment, independently of neighboring sites. It measures the ‘number of rejected substitutions’, defined as the expected number of substitution per site minus the observed number. Likelihood-ratio test (LRT) ( Margulies  et al. , 2007 ;  Pollard  et al. , 2010 ): this approach assigns a  P -value to the difference in the likelihoods of an observed alignment column under a null model of neutral evolution versus a model with an additional rate parameter that is estimated from the column. PhyloP-SCORE ( Pollard  et al. , 2010 ): similarly to the LRT, the SCORE test compares the hypotheses of neutrality to that of reduced or accelerated rate, but without the need to fit the rate parameter of the alternate hypothesis. 
 Each of these methods assigns a conservation score to each site  i , based either on the evolutionary history of that site alone (in the case of GERP, LRT and SCORE) or based on the evolutionary history of site  i  and its surrounding sites (in the case of PhastCons). These conservation scores were calculated after excluding the human sequence from the alignment, to ensure that our class labels (event along human branch) do not taint our feature set. The accuracy of the predictions made by these four measures of conservation are shown in  Figure 3 . PhastCons, the only measure that integrates signals from several consecutive positions, performs best for highly confident predictions (top 5% predicted as positive) but its accuracy decreases and quickly becomes worse than the other approaches at slightly more lenient thresholds. This is likely due to its excellent ability to detect relatively large regions under selection, but its inability to detect smaller ones or to identify weakly conserved sites within highly conserved regions. At more lenient thresholds (3–20% of test examples being called positive), PhyloP-SCORE outperforms the other three approaches. Note, however, that the two SVM-based predictors clearly outperform both PhastCons and PhyloP-SCORE over the full range of prediction threshold, often by fairly substantial margins. 3.6 Comparison of predictions Figure 4  shows an example of the predictions made by our SVM predictor (FeatureSet 2,  w =1), compared with PhyloP-SCORE and PhastCons. The most obvious difference is the increased resolution of the SVM and PhyloP approaches, which evaluate each base independently (though in the context of the two flanking bases in the case of SVM), compared with the HMM-based approach of PhastCons, which produces much smoother estimates. Whereas the PhastCons scores are extremely useful to define broad regions of conservation such as exons or enhancers, they are essentially useless when it comes to determining which bases within those regions are actually functional. For example, not all bases within an enhancer are equally constrained, because some are bound by a transcription factors and others not, and because not all bases within a TF binding site are equally important. Thus, PhyloP and SVM predictions provide an extremely valuable finer-grain estimation of selective pressure.
 Fig. 4. Conservation scores obtained by three predictors: PhyloP-Score, PhastCons and our SVM predictor (featureSet 2,  w =1) on human region chr22:31835800-31838550. While it is difficult to compare the merits of each predictor based on a single example,  Figure 5  provides a comprehensive comparison of the three sets of predictions, based on the ENCODE regions of human chromosome 22 (1.69 Mb in total) ( ENCODE-Project-Consortium  et al. , 2007 ). The top 10% of the sites predicted by each of the three methods was compared (this fraction is deliberately set slightly higher than the current estimates of the fraction of the genome under selection, to assess the ability of the methods to detect weak selection or selection on isolated sites). Clearly, the PhyloP and SVM approaches yield similar predictions (59% of the sites overlap), and quite different from the PhastCons sites (30–35% overlap only).
 Fig. 5. Overlap of the top 10% most confident predictions made by three predictors on 1.69 Mb of human chromosome 22. Black numbers indicate the number of sites. Red numbers indicate the percentage of sites that are polymorphic in the human population, based on HapMap 3. Green numbers indicate the percentage of sites that overlap DNAse I hypersensitive regions in at least one ENCODE cell line. Purple numbers indicate the percentage of sites that overlap regions with H3K4me1 histone marks in one of eight ENCODE cell lines. The predictions made by each method can be compared based on various external evidences of selection or function that were not part of their training. First, we compared the rate of human polymorphisms [HapMap project, phase III ( International HapMap Consortium, 2007 )] at sites predicted by each method or combination of methods. As expected, this rate is lowest for sites predicted by all three methods, suggesting that those are under the strongest selective pressure within the human population. Surprisingly though, the sites identified by PhastCons alone are equally depleted of polymorphisms, more so than those predicted by PhyloP-SCORE and SVM. The reason for this depletion, which contradicts the relatively poor positive predictive value of Phastcons compared with the other two predictors ( Fig. 3 ), may lie in PhastCons's ability to identify fairly large regions under weak selection, such as non-coding RNA genes. We also note that the polymorphism rate is slightly lower in sites predicted by the SVM approach only, compared with those predicted by PhyloP, suggesting stronger selection on the former than the latter. Second, we evaluated the evidence for a regulatory function of the predicted sites. Two types of experimental evidence were considered. Regions of the genome where the chromatin is open makes it possible for transcription factors to bind DNA. These regions can be identified by DNAseI digestion followed by hybridization or sequencing ( Sabo  et al. , 2006 ). DNAseI hypersensitive regions have been mapped in eight human cell lines as part of the ENCODE project ( ENCODE-Project-Consortium  et al. , 2007 ). As seen in  Figure 5 , 27% of the sites identified by all three predictors overlap one of these regions in at least one cell line. The numbers are lower for sites predicted by a single of the three methods, but still 17% SVM-only sites overlap DNAseI hypersensitive regions, a larger percentage than for PhyloP-SCORE-only sites or PhastCons-only sites. Another type of evidence of regulatory function is the presence of specific histone modifications, with H3K4me1 being strongly associated to enhancers ( Heintzman  et al. , 2009 ). This modification has also been mapped as part of the ENCODE project. Considering the union of H3K4me1 regions identified in eight cell lines as a set of likely enhancers, we observe again that sites predicted by all three methods overlap the most often with H3K4me1 regions, but that those predicted only by our SVM also overlap H3K4me1 marks significantly more often than those predicted only by one of the other two methods. Combined, these three types of evidence strongly suggest that the SVM predictor is better able to identify non-coding regulatory regions that are under selection in the human population. 4 DISCUSSION AND CONCLUSION With the explosion of the number of vertebrates being sequenced comes the opportunity to detect marks of selection in subtler manners. Although a number of approaches have been proposed to identify regions that have evolved at a lower rate than the surrounding DNA, with some even allowing rates to change over the branches of the tree, we argue that it can be useful to consider the problem from a machine-learning perspective. Here, we show that it is possible to train simple classifiers to predict whether a human base is likely to mutate or not. The classification problem at hand is a challenging one, for several reasons: (i) the mutational process is a completely random one; only the rate of fixation varies between functional and non-functional sites. (ii) The fraction of functional sites is expected to be small. Because of these two properties, a predictor's ability to predict the label of a site (human mutated versus conserved) can only be achieved through the detection of function. Our conservation predictor is thus a predictor of functional sites. Although we have restricted our analysis to sites from human chromosome 22, the next challenge will consist of using sites from the whole genome (~100 times larger than our current dataset). A training set of that size (close to 10 million examples), combined with the fact that example labels are in large part random, poses significant challenges to most types of classifiers. Additional training examples could be obtained by considering substitutions along other branches of the mammalian tree (e.g. the branch leading to chimp), at the cost of losing some of the human specificity of our predictor. If these challenges are met, however, one can expect the development of highly accurate evolutionary signatures that may encompass not only the type of features used in this article, but also features such as the presence/absence of predicted transcription factor binding sites. It is difficult to imagine a more challenging yet fascinating classification problem! Evolution has been conducting site-specific functionality assays for hundreds of millions of years. The ability to decipher the results of these experiments has and will continue to provide us with a wealth of information about our genome and the impact of mutations therein. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>L-GRAAL: Lagrangian graphlet-based network aligner</Title>
    <Doi>10.1093/bioinformatics/btv130</Doi>
    <Authors>Malod-Dognin Noël, Pržulj Nataša</Authors>
    <Abstract>Motivation: Discovering and understanding patterns in networks of protein–protein interactions (PPIs) is a central problem in systems biology. Alignments between these networks aid functional understanding as they uncover important information, such as evolutionary conserved pathways, protein complexes and functional orthologs. A few methods have been proposed for global PPI network alignments, but because of NP-completeness of underlying sub-graph isomorphism problem, producing topologically and biologically accurate alignments remains a challenge.</Abstract>
    <Body>1 Introduction Understanding the patterns in molecular interactions is of foremost importance in systems biology, as it is instrumental to understanding the functioning of the cell ( Ryan  et al. , 2013 ). Because molecular interactions are often modeled by networks, a large number of studies focused on understanding the topology of these networks ( Nepusz and Paccanaro, 2014 ;  Pržulj, 2011 ). In the case of protein–protein interaction (PPI) networks, where nodes represent proteins and edges connect proteins that interact, comparative studies based on network alignments were particularly successful. Given two networks, aligning them means finding a node-to-node mapping (also called an  alignment ) between the networks that optimizes two objectives: (i) maximizing the number of mapped proteins (nodes) that are evolutionarily or functionally related and (ii) maximizing the number of common interactions (edges) between the networks. Network alignment uncovers valuable information, such as evolutionarily conserved pathways and protein complexes ( Kelley  et al. , 2003 ;  Kuchaiev  et al. , 2010 ) or functional orthologs ( Bandyopadhyay  et al. , 2006 ). Finding these allows transfer of information across species, such as performing Herpes viral experiments in yeast or fly and then applying the insights toward understanding the mechanisms of human diseases ( Uetz  et al. , 2006 ). Network alignment problem is computationally intractable due to NP-completeness of the underlying sub-graph isomorphism problem ( Cook, 1971 ). Hence, several network alignment heuristics (i.e. approximate aligners) have been proposed. Earlier methods, called  local network aligners , search for small but highly conserved sub-networks called motifs ( Flannick  et al ., 2006 ;  Kelley  et al ., 2004 ;  Koyutürk  et al ., 2006 ). As motifs can be duplicated, local network aligners often produce one-to-many or many-to-many mappings, in which a node from a given network can be mapped to several nodes of the other network. Although these multiple mappings can indicate gene duplications, they are often biologically implausible ( Singh  et al. , 2007 ). Hence,  global network aligners , which perform an overall comparison of the input networks and produce one-to-one mappings between the nodes of the two networks have been introduced. Several heuristics have been proposed for solving the global alignment problem. The first one is ISORANK ( Singh  et al. , 2007 ), which rephrases aligning networks as an eigenvalue problem, mimicking Google's Pagerank algorithm. HOPEMAP ( Tian and Samatova, 2009 ) iteratively constructs the alignment between two networks by searching for a common maximally connected component. PATH and GA algorithms ( Zaslavskiy  et al. , 2009 ) optimize the same objective function, which balances between mapping similar proteins and increasing the number of mapped interactions. The GRAAL family ( Kuchaiev and Pržulj, 2011 ;  Kuchaiev  et al ., 2010 ;  Memišević and Pržulj, 2012 ;  Milenković  et al ., 2010 ) is a set of network aligners, which are based on the idea that mapping together nodes that are involved in similar local wiring patterns (as measured by the statistics of small induced sub-graph called graphlets) will result in a large number of shared interactions. As newer GRAAL aligners use improved alignment heuristic strategies, using C-GRAAL or MI-GRAAL is recommended. Also, these two methods allow using additional node scores, such as sequence similarity. NATALIE ( El-Kebir  et al. , 2011 ) is a combinatorial optimization method based on Lagrangian relaxation, which searches for top scoring network alignments over the biologically plausible node mappings obtained by sequence alignment. GHOST ( Patro and Kingsford, 2012 ) is a spectral approach, where nodes are mapped according to the similarity of their spectral signatures. NETAL ( Neyshabur  et al. , 2013 ) is a fast greedy heuristic that constructs an alignment by iteratively inserting the node mapping with the highest probability to induce common edges, where the probabilities are recomputed at each iteration. SPINAL ( Aladağ and Erten, 2013 ) is a two-step approach, which first computes coarse-grained node similarity scores, and then based on these scores, iteratively grows a seed solution. PISWAP ( Chindelevitch  et al. , 2013 ) optimizes global alignments using a derivative of the local 3-opt heuristic, which is originally used for solving the traveling salesman problem. MAGNA ( Saraph and Milenković, 2014 ) uses a genetic algorithm to maximize the edge conservation between the aligned networks. HUBALIGN ( Hashemifar and Xu, 2014 ) uses a minimum-degree heuristic to align ‘important’ proteins first and then gradually extends the alignment to the whole networks. Although all the above methods align networks to derive additional biological knowledge (e.g. orthology group and functional annotations), DUALALIGNER ( Seah  et al. , 2014 ) does the opposite and uses biological knowledge to produce network alignments. The number of known molecular interactions has increased tremendously during the last decade due to the technological advances in high-throughput interaction detection techniques such as yeast two-hybrid ( Fields and Song, 1989 ) and affinity purification coupled to mass spectrometry ( Ho  et al. , 2002 ). Because of the increasing amount of available interaction data, coupled with the computational hardness of the network alignment problem, producing topologically and biologically accurate alignments is still challenging. In this article, we introduce a novel global network alignment tool that we call Lagrangian GRAphlet-based ALigner (L-GRAAL). Unlike previous aligners, which either do not take into account the mapped interactions (e.g. the previous GRAAL aligners and ISORANK) or use naive interaction mapping scoring schemes (e.g. NATALIE), L-GRAAL optimizes a novel objective function that takes into account both sequence-based protein conservation and graphlet-based interaction conservation, by using a novel alignment search heuristic based on integer programming and Lagrangian relaxation. We compare L-GRAAL with the state-of-the-art network aligners on the largest available PPI networks from BioGRID and observe that L-GRAAL uncovers the largest overlaps between the networks, as measured with edge-correctness (EC) and symmetric sub-structure scores. These largest overlaps are key for transferring annotations between networks. Using semantic similarity, we observe that L-GRAAL's protein mappings and interaction mappings are in better agreement with Gene Ontology (GO) ( Ashburner  et al. , 2000 ) than any other network aligners. By aligning the PPI networks of baker's yeast and human, we additionally show that the results of L-GRAAL can be used to predict new PPIs. Finally, using our novel semantic similarity measure of the interaction mappings and the ability of L-GRAAL to produce alignments by using both topological and sequence similarity, we observe for the first time that topological similarity plays a more important role than sequence similarity in uncovering functionally conserved interactions, a result that escaped all previous approaches. 2 Materials and methods 2.1 Definitions and notations 2.1.1 PPI network The PPIs of a given organism are represented by a PPI network,  N = ( V , E ) , where nodes in  V  represent proteins and two nodes  u  and  v  are connected by an edge ( u ,  v ) in  E  if the corresponding proteins are known to interact. 2.1.2 Global network alignment Given two PPI networks,  N 1 = ( V 1 , E 1 )  and  N 2 = ( V 2 , E 2 ) , for which  | V 1 | ≤ | V 2 | , a  global alignment ,  f :  V 1 → V 2 , is a 1-to-1 mapping of the nodes in  V 1  to the nodes in  V 2 . Formally, global alignment is assigned a real-valued score  S :
 (1) S ( f ) = ∑ u ∈ V 1 n ( u , f ( u ) ) + ∑ ( u , v ) ∈ E 1 e ( u , f ( u ) , v , f ( v ) ) , 
where  n : V 1 × V 2 → R +  is the score of mapping a node of  V 1  to a node in  V 2 , and  e : E 1 × E 2 → R +  is the score of mapping an edge of  E 1  to an edge of  E 2 . The  Global Network Alignment problem  aims to find a global alignment that maximizes  S . 2.1.3 Graphlets and orbits Graphlets  are small, connected, non-isomorphic, induced sub-graphs of a larger graph (denoted by  G 0 , … , G 29  in  Fig. 1 ) ( Pržulj  et al. , 2004 ). Within each graphlet, some nodes are topologically identical with others: such identical nodes are said to belong to the same  automorphism orbit  (denoted by  0 , … , 72  in  Fig. 1 ) ( Pržulj, 2007 ). Graphlets generalize the notion of node degree: the  graphlet degree  of node  v , denoted by  d v i , is the number of times node  v  touches a graphlet at orbit  i  ( Pržulj, 2007 ). Graphlet degrees are successfully used for measuring the distance between two networks ( Pržulj, 2007 ), as well as measuring the topological similarities among nodes in networks ( Milenković and Pržulj, 2008 ), which are further applied for guiding the network alignment process in the GRAAL family of network aligners, and for comparing protein structures ( Malod-Dognin and Pržulj, 2014 ).
 Fig. 1. The 2- to 5-node graphlets and their automorphism orbits ( Pržulj, 2007 ) 
 2.2 L-GRAAL method 2.2.1 Similarity scores and objective function In L-GRAAL, we measure the evolutionary relationship between two mapped proteins  u  and  f ( u ) according to their BLAST sequence alignment:
 n ( u , f ( u ) ) = seqsim ( u , f ( u ) ) max ⁡ i , j seqsim ( i , j ) , 
where seqsim can be any sequence-based similarity score (in this article, we use both log of BLAST's  e -values and BLAST's bit-scores). We measure the topological similarity between two mapped proteins  u  and  f ( u ) using their 2- to 4-node graphlet degree similarity  t :
 t ( u , f ( u ) ) = 1 15 ∑ i = 0 14 min ( d u i , d f ( u ) i ) max ( d u i , d f ( u ) i ) . 
 We measure the topological similarity between two mapped interactions (edges), ( u ,  v ) and  ( f ( u ) , f ( v ) ) , according to the graphlet degree similarity of their mapped end nodes:
 e ( u , f ( u ) , v , f ( v ) ) = 1 2 ( t ( u , f ( u ) ) + t ( v , f ( v ) ) ) . 
This score is in [0, 1] and it rewards mapping edges that are involved in similar local wiring patterns. Note that we also use all 2- to 5-node graphlet degrees, but it only resulted in larger running times, without improving the quality of the alignments. L-GRAAL's objective function,  S , either favors the evolutionary relationships between the mapped proteins or the topological similarity between the mapped interactions, according to a balancing parameter  α ∈ [ 0 , 1 ] :
 (2) S ( f ) = α × ∑ u n ( u , f ( u ) ) + ( 1 − α ) × ∑ ( u , v ) e ( u , f ( u ) , v , f ( v ) ) 
 2.2.2 Two-step alignment search strategy Because of the large sizes of PPI networks, solving the network alignment problem when considering all possible node mappings is computationally intractable. In a first step, we use sequence and graphlet degree similarities to select a subset of the node mappings on which L-GRAAL will optimize seed alignments; namely, we only consider the node mappings  u ↔ v , such that  α n ( u , v ) + ( 1 − α ) t ( u , v ) ≥ 0.5 . We term the mapping that satisfy this criteria  selected node mappings . In a second step, a greedy heuristic extends the seed alignments using all possible node mappings, i.e. without being restricted to selected node mappings anymore. Because both L-GRAAL's and NATALIE's alignment search algorithms are based on integer programming and Lagrangian relaxation,  Supplementary Section 1.4  presents the differences between the two approaches. 2.2.3 Generating seed alignments using integer programming First, to each selected node mapping,  i ↔ k ,   i ∈ V 1 , k ∈ V 2 , we associate a binary variable  x ik ,  such that  x ik  = 1 if the node mapping is in the alignment and 0 otherwise. Similarly, we associate to each edge mapping between selected node mappings,  ( i , j ) ↔ ( k , l ) , ( i , j ) ∈ E 1 ,   ( k , l ) ∈ E 2 , a binary variable  y ijkl ,  such that  y ijkl  = 1 if the edge mapping is in the alignment and 0 otherwise. For brevity, henceforth, we ensure that each edge mapping is only considered once by enforcing  k  &lt;  l . This allows us to differentiate the two end-node mappings that result from an edge mapping  ( i , j ) ↔ ( k , l ) : we term  i ↔ k  a  tail-node mapping  and  j ↔ l  a  head-node mapping . The network alignment problem can now be expressed with the following integer program (IP):
 (3) IP = max x , y ( α ∑ n ( i , k ) × x i k + ( 1 − α ) ∑ e ( i , j , k , l ) × y i j k l ) , 
subject to:
 (4) ∑ k ∈ V 2 x i k ≤ 1 ,       ∀ i ∈ V 1 , 
 (5) ∑ i ∈ V 1 x i k ≤ 1 ,       ∀ k ∈ V 2 , 
 (6) x j l − y i j k l ≥ 0 ,       ∀ ( i , j ) ∈ E 1 , ∀ ( k , l ) ∈ E 2 , 
 (7) x i k − y i j k l ≥ 0 ,       ∀ ( i , j ) ∈ E 1 , ∀ ( k , l ) ∈ E 2 , 
where constraints (4, 5) enforce that a node from  V 1  is mapped to at most one node from  V 2  and vice versa and constraints (6, 7) enforce that the selected edge mappings  ( i , j ) ↔ ( k , l )  must have their end-nodes mapped as:  i ↔ k  and  j ↔ l . Because of the 1-to-1 mapping constraints (4,5), the relations between the edge mappings and their head-node mappings can be rewritten in a compact form. Given a node mapping  j ↔ l  and any node  i ∈ N 1 , such that edge  ( i , j ) ∈ E 1 , then at most one edge mapping  ( i , j ) ↔ ( k , l )  can be selected by choosing a node mapping  i ↔ k . Constraint (6) can then be replaced by the following two set of constraints:
 (8) x j l − ∑ k y i j k l ≥ 0 ,       ∀ ( i , j ) ∈ E 1 , ∀ l ∈ V 2 
 (9) x j l − ∑ i y i j k l ≥ 0 ,       ∀ ( k , l ) ∈ E 2 , ∀ j ∈ V 1 
In IP, when all the constraints between the node mappings and the edge mappings are considered, the head-node mappings also respect the 1-to-1 matching constraints. To keep this property in our relaxed model, we add the following constraints into LR ( λ ) :
 (10) ∑ l y i j k l ≤ 1 ,       ∀ ( i , j ) ∈ E 1 , ∀ k ∈ V 2 
 (11) ∑ j y i j k l ≤ 1 ,       ∀ ( k , l ) ∈ E 2 , ∀ i ∈ V 1 . 
 2.2.4 Lagrangian relaxation To solve IP, we apply Lagrangian relaxation ( Held and Karp, 1970 ), by relaxing constraints (8, 9), i.e. disconnecting the edge mappings from their head nodes. Relaxed constraints are added as penalties into the objective function, associated with Lagrangian multipliers ( λ E 1 i j l  for each constraint 8 and  λ E 2 k j l  for each constraint 9). Because relaxed constraints are inequalities, the Lagrangian multipliers must be non-negative real numbers (i.e.  λ ∈ R + , 0 ). This gives us the relaxed problem (LR ( λ ) ):
 (12) L R ( λ ) = max x , y ∑ n λ ( i , k ) × x i k + ∑ e λ ( i , j , k , l ) × y i j k l 
subject to (4, 5, 7 and 10, 11), where  e λ ( i , j , k , l ) = ( 1 − α ) × e ( i , j , k , l ) − λ E 1 i j l − λ E 2 k j l  and  n λ ( i , k ) = α × n ( i , k ) + ∑ j λ E 1 j i k + ∑ l λ E 2 l i k  are the new node and edge scores after adding the penalties from the relaxed constraints. We developed a double bipartite matching algorithm, detailed in the  Supplementary Material, Section 1.1 , for solving LR ( λ )  in  O ( | V | 3 + | V | 2 d 3 )  time, where  | V |  is the number of nodes in the networks and  d  is the largest degree of a node. Solving LR ( λ )  generates a relaxed solution  ( x → , y → ) . This relaxed solution is an upper bound on IP, as its score is greater than or equal to the one of IP, but it is often infeasible, as chosen edge mappings (the components of  y →  that are set to 1) may not coincide with chosen node mappings (the components of  x →  that are set to 1). However, any relaxed solution  ( x → , y → )  can be repaired into a feasible solution  ( x → , y ′ → )  of IP by selecting the edge mappings  y ′ →  corresponding to the selected node mappings. Such feasible solution is a lower bound on IP, as its score is smaller than or equal to the one of IP. To solve IP, we solve its Lagrangian dual problem, which is a minimization of LR ( λ )  over λ. Many methods have been proposed so far for solving Lagrangian dual problem ( Guignard, 2003 ). Here, we choose the sub-gradient descent method ( Held  et al. , 1974 ) because of our large number of relaxed constraints. The sub-gradient descent is an iterative method that generates a sequence of Lagrangian multipliers  λ ( 0 ) , λ ( 1 ) , λ ( 2 ) , … , starting from  λ ( 0 ) = 0 , where  λ ( i + 1 )  aims to fix the broken relaxed constraints in the solution of LR( λ ( i ) ), by making a step along its sub-gradient vector. Details on our implementation are given in the  Supplementary Material, Section 1.2 . Unfortunately, the Lagrangian dual problem is also NP-complete, and thus one could not expect to solve it in a reasonable time. In practice, the process of solving the Lagrangian dual is used for generating a sequence of seed solutions  ( x → 0 , y ′ → 0 ) ,   ( x → 1 , y ′ → 1 ) , … , until a given time limit or an iteration number limit is reached (we use 1 h and 1000 iterations as default). 2.2.5 Heuristically extending seed alignments At each iteration of the sub-gradient descent, the seed alignment  ( x → , y ′ → )  is extended to include all node mappings with a three-step greedy heuristic (see Algorithm 1 in  Supplementary Material, Section 1.3 ). All node mappings that do not positively contribute to the score of the alignment are removed. The alignment is then maximally extended by sequentially visiting the yet unaligned nodes in  V 1  and mapping them to the yet unaligned nodes in  V 2 ,  so that the score of the alignment is maximized. Then, a greedy local search sequentially visits  V 1  and tries inserting or exchanging node mapping  i ↔ k  to improve the score of the alignment. Note that the extended alignments are not returned to the dual solver, since they are not computed on the same search space (seed alignments are restricted to selected node mappings, whereas the extended alignments are not), so they would invalidate the sub-gradient descent scheme if included. When these computations end, L-GRAAL returns the extended alignment with the best score. 2.3 Datasets From the manually curated BioGRID database (v3.2.101, June 2013) ( Chatr-Aryamontri  et al. , 2013 ), we obtained PPI networks of eight organisms that have the largest number of known physical interactions:  Homo sapiens  (HS, 13 276 nodes and 110 528 edges),  Saccharomyces cerevisiae  (SC, 5831 nodes and 77 149 edges),  Drosophila melanogaster  (DM, 7937 nodes and 34 753 edges),  Arabidopsis thaliana  (AT, 5897 nodes and 13 381 edges),  Mus musculus  (MM, 4370 nodes and 9116 edges),  Caenorhabditis elegans  (CE, 3134 nodes and 5428 edges),  Schizosaccharomyces pombe  (SP, 1911 nodes and 4711 edges) and  Rattus norvegicus  (RN, 1657 nodes and 2330 edges). Note that physical interactions in BioGRID include both direct (e.g. from yeast-two-hybrid) and indirect (e.g. from affinity capture) interactions, so edges in our PPI networks connect proteins that either directly interact or that co-exist in stable complexes. We retrieved the corresponding protein sequences and GO annotations from NCBI's Entrez Gene database ( Maglott  et al. , 2005 ). Note that we only retrieved experimentally validated GO annotations, from which we further removed the annotations inferred from PPIs (code IPI). L-GRAAL is one of the few methods that can align even the largest of the networks presented above. As already reported by  Clark and Kalita (2014) , many of the other aligners have memory issues when handling the two largest networks of yeast and human. Thus, the comparisons presented in sections 3.1 and 3.2 are based on the  ( 6 2 ) = 15  pairs of networks that involve DM, AT, MM, CE, SP and RN, which can be solved by all methods. L-GRAAL's alignments of yeast and human PPI networks are presented in Section 3.3. In the  Supplementary Material , we also assess the robustness of our results by comparing the performance of network aligners on two more datasets. First, we create the binary PPI networks by restricting our BioGRID networks to the yeast-two-hybrid captured interactions only. Second, we use the synthetic random networks from the NAPA benchmark ( Sahraeian and Yoon, 2012 ). 2.4 Evaluation We compare the alignments of L-GRAAL to those of HUBALIGN, MAGNA, PISWAP, SPINAL, NETAL, GHOST, NATALIE, MI-GRAAL and ISORANK. We set MI-GRAAL to use graphlet degree vector similarity (GDS) alone, as well as to use GDS coupled with sequence similarity (GDS+SEQ); since it is a randomized algorithm, we repeat each alignment process 15 times for GDS and 15 times for GDS+SEQ, to find alignments of the best topological and biological quality. We set SPINAL to use mode II, as recommended in the corresponding paper. For all aligners that can produce alignments using pure topology or pure sequence information by balancing parameters varying in [0,1] (e.g. parameter α for L-GRAAL), we sample the balancing parameters from 0 to 1 in increments of 0.1. We set the time limits of both L-GRAAL and NATALIE to 1 h per alignment. We set MAGNA to optimize S3 score, on a population size of 2000, over 15 000 generations, setting that is recommended in the corresponding paper. For all network aligners, we leave other parameters at their default values. All computations are done on a desktop computer with an Intel Core I7–2600 CPU at 3.40 GHz with 64 GB of memory. For all these aligners, we report the results of their best alignments, according to the following measures. 2.4.1 Topological quality Network aligners are first compared by their ability to map proteins that are similarly connected in both PPI networks. First, the size of the alignment is measured by EC, which is the percentage of interactions from the smaller network that are mapped to interactions from the other network ( Kuchaiev  et al. , 2010 ). Because large EC can be achieved by mapping sparse regions of the smaller network to densely connected regions of the larger one, we also measure how topologically similar are the mapped regions using the  symmetric sub-structure score  (S3), which is the percentage of the conserved edges between the smaller network and the sub-network from the larger network that is induced by the alignment ( Saraph and Milenković, 2014 ). Finally, we use the size of the  largest connected component  (LCC) to ensure that the alignments correspond to large common connected sub-structure, instead of several small disconnected ones ( Kuchaiev  et al. , 2010 ). 2.4.2 Biological quality It is not known which proteins from one PPI network should be mapped to which ones in the other PPI network. Biological similarity of two mapped proteins can be measured by the semantic similarity of their GO term annotations. We compute the semantic similarity using Resnik semantic similarity ( Resnik, 1995 ) with best-match average mixing strategy. Then, we measure the biological quality of the entire alignment by the sum of the semantic similarities of the mapped proteins, divided by the smaller number of annotated proteins in the two networks. 3 Results and discussion Here, we present the results achieved by network aligners on the real PPI networks from BioGRID. 3.1 Topological analysis First, L-GRAAL, HUBALIGN and GHOST produce the largest alignments, with EC of 52.2% for L-GRAAL, 52.1% for HUBALIGN and 42.7% for GHOST (see the left panel of  Fig. 2 ). These large alignments are key, as they allow transferring more information across networks. We also measure the statistical significance of the obtained EC scores using the standard model of sampling without replacement, as proposed by  Kuchaiev  et al.  (2010)  (the formula is presented in the  Supplementary Material ). All are statistically significant, as the probability of obtaining similar or higher values by chance is always smaller than 0.05. We test whether L-GRAAL achieves larger EC by mapping the smaller network to the densest regions of the larger network (the dense regions corresponding to, e.g. large complexes captured by affinity capture-based methods). This is not the case, since L-GRAAL, NETAL and MAGNA best map sparse regions with sparse regions and dense regions with dense regions, with symmetric sub-structures score = 31.1% for L-GRAAL, 29.3% for NETAL and 26.4% for MAGNA (see the middle panel of  Fig. 2 ). In other word, L-GRAAL is less biased toward cliquish structures than other aligners. On the opposite, while HUBALIGN achieves EC that is comparable to the one of L-GRAAL, it achieves smaller S3 score. This is not surprising as HUBALIGN favors mapping densely connected proteins. Finally, HUBALIGN, L-GRAAL and MI-GRAAL produce the least fragmented network alignments, with LCC = 74.6% for HUBALIGN, 71.5% for L-GRAAL and 67.7% for MI-GRAAL (see the right panel of  Fig. 2 ).
 Fig. 2. Topological comparisons of aligners. Methods ( x  axis) are compared according to the minimum, average and maximum of the best topological scores (the error bars on  y  axis) that they obtain when aligning PPI networks. Left: Methods are compared according to EC. Middle: Methods are compared according to symmetric sub-structure score (S3). Right: Methods are compared according to the size of the LCC in their alignments 
 Overall, L-GRAAL and HUBALIGN outperform all other aligners in terms of the topological quality of their alignments on the real networks from BioGRID (we also observe similar results when aligning binary PPIs only, see  Supplementary Fig. S4 ). However, although L-GRAAL also achieves good performances when aligning the synthetic networks from the NAPA benchmark, HUBALIGN does not, which shows the higher robustness of L-GRAAL (see  Supplementary Fig. S5 ). 3.2 Biological analysis As presented in the left panel of  Figure 3 , L-GRAAL, HUBALIGN and GHOST map proteins that are involved in similar GO biological processes (GO-BPs) best, with average semantic similarity of the protein mappings of 1.09 for L-GRAAL, 1.08 for HUBALIGN and 1.04 for GHOST. Similar holds for GO molecular functions (GO-MFs) and GO cellular component annotations (GO-CC), as presented in the middle and right panels of  Fig. 3 ).
 Fig. 3. Biological comparison of network aligners. Methods ( x  axis) are compared according to the minimum, average and maximum semantic similarity of their aligned proteins (the error bars on  y  axis), when semantic similarity is measured using GO-BP (left), GO-MF (middle) or GO-CC (right) 
 Large semantic similarities of the protein mappings indicate that the alignments map functionally conserved proteins, but it does not mean that these functions are performed through conserved interaction patterns between the two PPI networks. Although functionally conserved interactions may highlight fundamental mechanisms (e.g. key binary interactions or complexes that must be conserved), network aligners are never compared in this respect. To measure the  functional conservation of the mapped interactions , we define the semantic similarity of two mapped interactions as the average of the semantic similarities of the corresponding pairs of mapped proteins. Then, we measure the biological quality of the whole interaction mapping as the sum of all the interaction semantic similarities divided by the smaller number of interactions between annotated proteins in the two networks. To the best of our knowledge, this is the first time that the biological quality of the interaction mapping is considered. As presented in  Supplementary Figure S1 , HUBALIGN, L-GRAAL and SPINAL are the best in mapping interactions that are involved in similar BPs, in similar MFs, and that are localized in similar cellular regions. Overall, L-GRAAL and HUBALIGN outperform all other aligners in terms of the biological quality of their alignments when aligning real networks from BioGRID (we also observe similar results when aligning binary PPIs only, see  Supplementary Fig. S4 ) and again L-GRAAL shows higher robustness than HUBALIGN when aligning synthetic networks from the NAPA benchmark (see  Supplementary Fig. S5 ). 3.3 Predicting protein interactions Although a good network alignment should map together functionally related proteins that interact in similar ways, alignments are also composed of  edge-mismatches , where interacting proteins are mapped to non-interacting proteins. To illustrate this phenomenon, we first investigate the largest shared pathway between  Saccharomyces cerevisiae  and Homo sapiens PPI networks that is found in L-GRAAL's alignment, which is the ribosome pathway (KEGG Id 3010) that contains 105 proteins and 862 interactions. This alignment, illustrated in the left panel of  Figure 4 , correctly aligns the dense sub-network from yeast to the dense sub-network of human. However, it also aligns interacting proteins to non-interacting ones. For example, it aligns yeast's MRPS5 and MRP17, which interact according to BioGRID's data, with human's MRPS5 and MRPS6, which are not reported to interact in BioGRID (see the inset of  Fig. 4 ). Further investigation shows that these two protein mappings are biologically relevant. First, the proteins are evolutionarily related: human's and yeast's MRPS5 share 33.6% of sequence identity, and human's MRPS6 and yeast's MRP17 share 29.3% of sequence identity. Second, human's MRPS5 and MRPS6 are known to interact, as captured by anti tag coimmunoprecipitation assay ( Richter  et al. , 2010 ). Therefore, L-GRAAL's alignment of yeast edge (MRPS5, MRP17) predicted the missing interaction in human data from BioGRID.
 Fig. 4. Predicting new protein interactions. Left: Part of L-GRAAL's alignment that aligns human and yeast ribosome pathways. The PPI sub-network of yeast (white nodes and gray edges) is mapped to the PPI sub-network of human (red nodes and orange edges) as indicated by the blue edges. The inset highlights a predicted interaction: Proteins MRPS5 and MRP17, which are interacting in the yeast PPI network, are aligned to proteins MRPS5 and MRPS6, which are not interacting in the human PPI network. Right: Using the whole L-GRAAL's alignment between yeast and human PPI networks, we plot in black the number of predicted interactions ( y  axis) as a function of the minimum sequence identity between the aligned yeast-human proteins ( x  axis). We add in red the number of these predicted interactions that are also predicted in I2D database 
 Building upon this insight, we measure how many potential interactions can be predicted by L-GRAAL's alignment, by counting the number of edge-mismatches whose node mappings involve proteins with high sequence identity. In this way, we show that L-GRAAL's alignment can predict 200 potential interactions for which the sequence identity between the mapped proteins is ≥70%, threshold for which the mapped proteins are expected to share the same functions ( Rost, 2002 ), see the right panel of  Figure 4 .  Supplementary Figure S3  presents the number of predictions that are obtained when using less stringent sequence identity thresholds (the list of all predicted interactions is available in the  Supplementary Excel Table ). We find that 34% of these predicted interactions are also predicted in the Interologous Interaction Database (I2D ver. 2.3) ( Brown and Jurisica, 2007 ), which is statistically significant since the probability to obtain better or equal overlaps by chance is less than  10 − 99  (using sampling without replacement, as detailed in the  Supplementary Material, Section 1.5 ). This result suggests that network aligners such as L-GRAAL can be used as alternative protein interaction predictors. 3.4 Balancing sequence and topological information L-GRAAL can produce alignments from topology and sequence information when the balancing parameter, α, varies from 0 to 1. In the previous experiments, we report the best scores (EC, S3, semantic similarities of protein and interaction mappings) that are obtained when α varies from 0 to 1 using a step size of 0.1. Here, we report the effect of α on each of these scores. The corresponding plots are presented in  Supplementary Figure S2 . First, all topological scores reach their maximum values when using topological information only (α = 0), with EC = 51.5%, S3 = 30.9% and LCC = 68.1% on average. It is also important to notice that using sequence information only (α = 1) results in alignment having almost no common interactions (EC = 2.0%, on average). Second, the semantic similarities of the aligned  proteins  either reach their maximum when using both topological and sequence information or when using sequence similarity only,  α \hskip-13\simeq\hskip-6 0.9  for BP and cellular component and α = 1 for MF. In contrast, the semantic similarities of the aligned  interactions  reach their maximum when using topological information only (α = 0). These results show again the complementarity of the two sources of information. Also, the comparison between the interactions’ semantic similarities that are obtained when using topological information only with the ones that are obtained when using sequence information only suggests that topology plays a more important role than sequence for uncovering functionally conserved interactions. To the best of our knowledge, this is the first time that this is observed. The importance of topology may be due to the concept of function itself, which implies interactions with some part of the cell or the environment ( Hartwell  et al. , 1999 ) and these interactions are captured by the topology of the PPI networks. Also, sequence similarity may fail at identifying the correct one-to-one relationships between genes when their homology relationships are not straightforward. Such difficult cases where topology is required include finding the relationships between a set of paralogous genes in a given species and its set of co-ortholog genes in another species. 4 Concluding remarks First, we propose a global network alignment method called L-GRAAL, which combines a novel objective function where the topological similarity of the mapped interaction is based on graphlet degree, with an efficient network alignment search algorithm based on integer programming and Lagrangian relaxation. Using the largest PPI networks from BioGRID, we show that L-GRAAL's alignments outperform other network alignments: they uncover the largest common sub-networks between aligned networks, as measured by EC and symmetric sub-structure scores. Second, as measured by the average semantic similarity of the mapped proteins, we observe that L-GRAAL best uncovers functionally conserved proteins. Because the objective of network aligners is not only to uncover functionally conserved proteins but also functionally conserved interactions among these proteins, we propose a novel way of measuring the semantic similarity of the mapped interactions and observe that L-GRAAL is among the best aligners for uncovering functionally conserved interactions. Third, on a case study of aligning human and yeast PPI networks, we show that L-GRAAL can be used to predict new interactions. Designing a whole benchmarking and validation strategy needed for finding which network aligners best predict protein interactions and for precisely comparing such predictions with the ones of traditional predictors are out of scope of this study. Fourth, using the ability of L-GRAAL to produce alignments using topological and sequence similarity, we observe that topological similarity plays a more important role than sequence similarity for uncovering functionally conserved interactions. To the best of our knowledge, this is the first time that this has been observed. Finally, L-GRAAL's computations can be easily speed up by using parallel programming. In each iteration of the Lagrangian dual solver, i.e. when solving LR ( λ )  for a given λ, each local bipartite matching for finding the best set of outgoing edges from a given node is an independent task. In addition, each bipartite matching problem, local and global, can be solved with parallel versions of the successive shortest paths algorithm ( Storøy and Sørevik, 1997 ). This high level of parallelism for speeding up L-GRAAL's computations is very promising as it allows it to scale with the future growth of the interaction data. 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>The Microbiome Modeling Toolbox: from microbial interactions to personalized microbial communities</Title>
    <Doi>10.1093/bioinformatics/bty941</Doi>
    <Authors>Baldini Federico, Heinken Almut, Heirendt Laurent, Magnusdottir Stefania, Fleming Ronan M T, Thiele Ines, Wren Jonathan</Authors>
    <Abstract/>
    <Body>1 Introduction Microbial community sequencing data are increasingly available for numerous environmental niches ( Mitchell  et al. , 2018 ). The analysis of this data often relies on investigating which microbes are present in a given sample. However, to further our understanding of the functional contribution of individual microbes in a community as well as the overall functional differences between communities, advanced analysis approaches, such as computational modeling, are required.  One possible approach is the constraint-based reconstruction and analysis (COBRA) approach, which builds genome-scale reconstructions of an organism and enables the prediction of, e.g. phenotypic properties ( Palsson, 2006 ). Through the application of condition-specific constraints, an organism’s metabolic reconstruction can be converted into many condition-specific models, which can be analyzed using available toolboxes, such as the Matlab (Mathworks, Inc.)-based COBRA Toolbox ( Heirendt  et al. , 2017a ). Metabolic reconstructions have been assembled for many organisms, including hundreds of gut microbes ( Magnúsdóttir  et al. , 2017 ) and human ( Brunk  et al. , 2018 ). Although the COBRA Toolbox encapsulates many tools developed by the community for biotechnological and biomedical applications, it is currently focused on modeling single organisms or cells. Here, we present the Microbiome Modeling Toolbox, which enables the generation, simulation and interpretation of (i) pairwise microbe-microbe and host-microbe interactions, and (ii) sample-specific microbial community models. By integrating sample-specific metagenomic data, the Microbiome Modeling Toolbox facilitates its analysis in the context of microbial reconstructions. 2 Features 2.1 Pairwise interactions The pairwise interaction analysis determines metabolic exchange between two metabolic reconstructions. A joint matrix of two individual genome-scale reconstructions is generated, which enables them to freely exchange metabolites ( Fig. 1A ). Defined nutrient input, e.g. a particular medium formulation, can be applied via the shared compartment using the corresponding exchange reactions. The pairwise microbial models can be investigated for six possible interaction types (i.e. competition, parasitism, amensalism, neutralism, commensalism and mutualism) and Pareto optimality frontiers can be calculated. The tutorials  MicrobeMicrobeInteractions  and  HostMicrobeInteractions  illustrate the implemented functionalities.
 Fig. 1. Overview of the Microbiome Modeling Toolbox.  (A)  Pairwise modeling of microbe–microbe and host–microbe interactions. ( B)  Microbial community modeling  2.2 Microbial community modeling Metagenomic data can be analyzed using  mgPipe  ( Fig. 1B ), which requires microbe identification and relative abundance data for each sample, obtained with bioinformatic tools, such as QiIME 2 ( Caporaso  et al. , 2010 ) and MetaPhlAn ( Segata  et al. , 2012 ).  mgPipe  is divided into three parts: (i) the analysis of individuals: specific microbes abundances, including metabolic diversity and classical multidimensional scaling of the reactions in the identified microbes. (ii) Construction of a personalized microbial community model using the identified microbes and their relative abundance data. For each personalized (or sample-specific) model, the corresponding microbial reconstructions are joined by adding reactions to each microbial reconstruction transporting metabolites from the extracellular space to the common lumen compartment. Metabolites present in the lumen compartment are connected to a diet and fecal compartment, enabling the uptake and secretion from/to the environment, respectively. Hundreds of reconstructions can be combined and modeled with using static parallelization. In each microbial community model, the community biomass reaction is personalized using the relative abundance data. Finally, coupling constraints ( Heinken  et al. , 2013 ) are applied to couple the flux through each microbial reaction to its corresponding biomass reaction flux. (iii) Simulation of the personalized microbial community models under different diet regimes, e.g. using flux variability analysis ( Heirendt  et al. , 2017 b). The differences between maximal uptake and secretion fluxes provide a metabolic profile for each microbial community sample, which can be analyzed using classical multidimensional scaling analyses. Diet-specific constraints (e.g. obtained from  https://vmh.life/#nutrition ) can be applied to the corresponding diet exchange reactions. 3 Implementation The Microbiome Modeling Toolbox is written in MATLAB (Mathworks, Inc.) and accompanied with comprehensive documentation and tutorials. The toolbox allows for the integrative analysis of any number of reconstructions, including the human metabolic reconstruction ( Brunk  et al. , 2018 ). Metabolic reconstructions can be obtained from, e.g. the VHM ( https://vmh.li fe), BioModels ( https://www.ebi.ac.uk/biomodels-main/ ) and the KBase ( https://kbase.us/ ). A uniform nomenclature of reaction and metabolite abbreviations across the reconstructions is required. The implemented diet constraints require VMH abbreviations. To use higher taxonomical levels create pan-reconstructions ( createPanModels ). For larger datasets and/or bigger microbial community models, we recommend the use of the MATLAB command line or.m files and of a high-performance computing cluster. 4 Discussion The Microbiome Modeling Toolbox enables the user to investigate microbial interactions at a large scale ( Heinken  et al. , 2013 ;  Magnúsdóttir  et al. , 2017 ). Moreover, metagenomically derived data can be integrated with microbial metabolic reconstructions permitting the prediction of altered functional assessment of different microbial communities, e.g. in health and disease ( Heinken  et al. , 2018 ;  Thiele  et al. , 2018 ). Funding This study received funding from the Luxembourg National Research Fund (FNR), through the ATTRACT program [FNR/A12/01], and the OPEN grant [FNR/O16/11402054], as well as the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation program [grant agreement No 757922]. 
 Conflict of Interest : none declared. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>PanGP: A tool for quickly analyzing bacterial pan-genome profile</Title>
    <Doi>10.1093/bioinformatics/btu017</Doi>
    <Authors>Zhao Yongbing, Jia Xinmiao, Yang Junhui, Ling Yunchao, Zhang Zhang, Yu Jun, Wu Jiayan, Xiao Jingfa</Authors>
    <Abstract>Summary: Pan-genome analyses have shed light on the dynamics and evolution of bacterial genome from the point of population. The explosive growth of bacterial genome sequence also brought an extremely big challenge to pan-genome profile analysis. We developed a tool, named PanGP, to complete pan-genome profile analysis for large-scale strains efficiently. PanGP has integrated two sampling algorithms, totally random (TR) and distance guide (DG). The DG algorithm drew sample strain combinations on the basis of genome diversity of bacterial population. The performance of these two algorithms have been evaluated on four bacteria populations with strain numbers varying from 30 to 200, and the DG algorithm exhibited overwhelming advantage on accuracy and stability than the TR algorithm.</Abstract>
    <Body>1 INTRODUCTION Since the conception ‘pan-genome’ was introduced in 2005 ( Medini  et al. , 2005 ), pan-genome had shed light on the genomic diversity and evolution of bacterial genome. To make pan-genome analyses more expediently, several software tools were designed. Panseq could identify single nucleotide polymorphism on core genome and strain’s specific region ( Laing  et al. , 2010 ), and PGAP was designed to perform five analytic functions with only one command ( Zhao  et al. , 2012 ). At the same time, a series of mathematic models had been proposed to characterize pan-genome profile of bacteria, such as the  Streptococcus agalactiae  pan-genome model ( Tettelin  et al. , 2005 ), the  Haemophilus influenzae  pan-genome model ( Hogg  et al. , 2007 ), heaps law model ( Tettelin  et al. , 2008 ) and infinitely many genes model ( Baumdicker  et al. , 2010 ). All current models could well depict the pan-genome profile of a launch of bacteria with their time complexity O( ), where  n  is the strain number. In this article, we developed two sampling algorithms to efficiently perform pan-genome profile analysis for larger scale genomes. These two algorithms were integrated in the software with a graphic interface, PanGP. 2 METHODS 2.1 Test dataset Gene clusters from seven bacterial populations were used to test the performance of sampling algorithms; the detailed strain list and gene clustering method were introduced in  Supplementary Material . 2.2 Sampling algorithm To analyse the pan-genome profile of  N  strains, the pan-genome size and core genome size of  n  strains (1 ≤  n  ≤ N) would be calculated. If a population has  N  strains, there are   combinations. When   grows to a large number, two sampling algorithms, totally random (TR) and distance guide (DG), would be available to sample  s  ( s : sample size) combinations consisted of  n  strains. For the TR algorithm : when   ( r : sample repeat times), randomly sample  s  non-redundant combinations from the total   combinations. The average pan-genome size and core genome size of these  s  non-redundant combinations were record as   and  . For the DG algorithm : when  , sample   ( k : amplification coefficient, which could be modified to control sample size for evaluating the genome diversity of the total combinations) non-redundant combinations from the total   combinations. Calculate the Dev_geneCluster value of these   combinations, sort all the   combinations by their Dev_geneCluster value and sample  s  combinations from end to end with the same interval. The average pan-genome size and core genome size of these  s  combinations were record as   and  . The above sample process would repeat  r  times for both TR and DG algorithms, and their average value (  and  ) would be taken as the pan-genome size and core genome size of  n  strains, respectively. 2.3 Genome diversity characterizing models in the DG algorithm In the DG algorithm, genome diversity was taken as a criterion to sample a certain number of strain combinations. To figure out a suitable model to characterize genome diversity, three models were proposed. In the following models,  G ( n ) represents a combination consisted of  n  strains.  i  and  j  represent the  i th and  j th strain in  G ( n ). In the population with  N  strains, there are  
 G ( n ) combinations. Model A:  Genome diversity was characterized by the discrepancy of evolutionary distance on phylogenetic tree. Branch length ( ) and node depth ( ) between the  i th and  j th strain on phylogenetic tree were calculated respectively (Calculating method for   and   were introduced in  Supplementary Fig. S1  in  Supplementary Material ). The average branch length ( ) or node depth ( ) of every strain pair in a given combination  G ( n ) represented genome diversity of the combination  G ( n ) and were calculated using the following formulas, respectively.
 
 
 In model A, six methods were provided, which were introduced in  Supplementary Material . Model B : Genome diversity was characterized by the difference of each strain’s gene number. For each combination  G ( n ), the total gene number of all strains (marked as Sum_geneNum) and the variance of gene numbers among those strains (marked as Dev_geneNum) were calculated by two different formulas, shown as follows.
 
 
 Note :   means the Sum_geneNum value;   means the Dev_geneNum value;   is the gene number of the  i th strain. Model C:  Genome diversity was characterized by the discrepancy among gene clusters. For each combination  G ( n ), the average number of the different gene clusters between two strains (marked as Dev_geneCluster) was calculated to represent genome diversity in a given combination  G ( n ). The value of Dev_geneCluster ( ) is calculated by the following formula.
 
 Note :   is the number of different gene clusters between the  i th and  j th strains. 2.4 Evaluate the performance of sampling algorithm To evaluate the accuracy of sampling algorithm, the pan-genome size and core genome size of  n  (1 ≤  n  ≤ N) strains were calculated by TR and DG algorithms. Real data were calculated without sampling; amplification coefficient  k  was set to 4; and the simulation were carried out with the sample size set to 300, 500, 800, 1000, 2000, 3000, 5000, 8000 and 10 000. For each population, the simulation for two sampling algorithms was repeated 100 times respectively. The accuracy for every simulation result was measured by root mean square (RMS) value, which calculated the deviation between sample result (  and  ) and real data (  and  ) at each point.
 
 The stability for every simulation result was also measured by RMS value, which calculated the deviation between the  i th simulation result (  and  ), and the  j th simulation result (  and  ) at each point (1 ≤  i &lt; j  ≤ 100).
 
 2.5 Nonlinear fitting for the pan-genome profile When the pan-genome size and core genome size of  n  strains (1 ≤  n  ≤ N) were available, a series of mathematics models ( Supplementary Material ) were used to depict pan-genome size, core genome size and new gene size. 3 RESULTS The performance of sampling algorithms . It was found in  Supplementary Fig. S1  that genome diversity, characterized by Dev_geneCluster method in Model C, exhibited strong correlation to pan-genome size, and difference in gene cluster number may primarily affect pan-genome size. Hence, we take Dev_geneCluster as the method to characterize genome diversity in the DG algorithm. The performance of two sampling algorithms was estimated from three aspects—accuracy, stability and time cost. Seen from  Supplementary Figs S2–S4 , the result from the DG algorithm was closer to the real data and more stable than the TR result with the same sample size. For the same species, population size almost did not affect the accuracy and stability of the two sampling algorithms. When the sample size was set to 500, the error proportion in the total gene clusters, either between the DG result and the real data or between any two sample results from the DG algorithm, were &lt;0.1% in all test populations. Regarding time cost, the DG algorithm took slightly more time than the TR algorithm with the same sample size ( Supplementary Table S1 ). When sample size was set to 500, the analysis for ≤50 genomes could be finished within 3 min. Application for PanGP . Among pan-genome analysis tools, PGAP, a comprehensive pan-genome analysis pipeline with five modules, could only deal with small-scale genomes in the pan-genome profile analysis module, while PanGP was a highly efficient tool for large-scale bacterial pan-genome profile analysis with sampling algorithms. Besides TR and DG algorithms, another one named traverse all algorithm (TA algorithm) was also provided in PanGP. When TA algorithm was selected, the pan-genome profile would be analysed without sampling. PanGP required orthologs information as the input data, which could be generated by series of software, such as PGAP ( Zhao  et al. , 2012 ), OrthoMCL ( Li  et al. , 2003 ) and PanOCT ( Fouts  et al. , 2012 ), with the format introduced in the online manual. When sampling finished, the pan-genome profile would be present as curves images (shown as  Supplementary Fig. S5 ). The curves images could be revised and exported via the user-friendly graphic interface, and non-linear fitting with three mathematic models was also available for pan-genome size, core genome size and new gene size. More usages about PanGP were provided in the online manual. Funding : The  National Basic Research and Development Program  ( 973 Program ;  2010CB126604  to J.X.);  National Programs for High Technology Research and Development  ( 863 Program ;  2012AA020409 ), the  Ministry of Science and Technology of People’s Republic of China ;  Key Program of Chinese Academy of Sciences  grant ( KSZD-EW-TZ-009-02  to J.X). Conflict of Interest : none declared. 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Minimum message length inference of secondary structure from protein coordinate data</Title>
    <Doi>10.1093/bioinformatics/bts223</Doi>
    <Authors>Konagurthu Arun S., Lesk Arthur M., Allison Lloyd</Authors>
    <Abstract>Motivation: Secondary structure underpins the folding pattern and architecture of most proteins. Accurate assignment of the secondary structure elements is therefore an important problem. Although many approximate solutions of the secondary structure assignment problem exist, the statement of the problem has resisted a consistent and mathematically rigorous definition. A variety of comparative studies have highlighted major disagreements in the way the available methods define and assign secondary structure to coordinate data.</Abstract>
    <Body>1 INTRODUCTION Periodic hydrogen-bonding patterns in globular proteins give rise to elements of secondary structure—helices and sheets. The α-helix and β-sheets were among the first structural motifs predicted from first principles of stereochemistry by  Pauling and Corey (1951 ). We now know these specific motifs are almost ubiquitous across the corpus of known structures. Eventually, other repetitive motifs were also identified, and the alphabet of secondary structures was expanded to include 3 10 -helix, π-helix, β-turn, γ-turn, Ω-turn and β-bulges, among other minor elements. In what follows, we use the term secondary structure to include both the classical helices and sheets, and other common substructural elements. Accurate assignment of secondary structure of proteins from coordinate data is an important and a challenging problem ( Andersen and Rost, 2009 ). Secondary structure underpins the architectural organization in proteins. It simplifies the complex atom-level description of proteins and is therefore the key to generation of schematic diagrams of their three dimensional (3D) folding patterns ( Lesk and Hardman, 1982 ;  Richardson, 1981 ). They are central in training methods geared for predicting secondary structure from amino acid sequence ( Andersen and Rost, 2009 ). They form a linchpin to efficient methods for structural comparison and analysis ( Kamat and Lesk, 2007 ;  Konagurthu  et al ., 2008 ). Over the last 30 years, many programs were developed to address the problem of assigning secondary structure to protein coordinate data. A broad classification can be made of the assignment strategies: (i) methods that use distance and angle profiles of local fragments; (ii) methods that detect hydrogen bonds between backbone atoms; (iii) methods that use 3D geometry of local fragments; and (iv) methods that approximate the backbone trace with a set of straight lines. The following reviews some of the major earlier contributions to the literature of this problem.  Levitt and Greer (1977 ) were the first to generate an automatic method for secondary structure assignment, based on distance and dihedral angle profiles of C α  atoms over a sliding window of four residues. P-SEA ( Labesse  et al ., 1997 ) is another method in this category which assigns secondary structural states using a short C α  distance mask and two C α  dihedral angle criteria. PROSS ( Srinivasan and Rose, 1999 ) proposes an assignment based solely on backbone dihedral angles. Xtlsstr ( King and Johnson, 1999 ) calculates backbone dihedral angles and distances and assigns secondary structural types that would be consistent with interactions of amide-amide groups observed from circular dichroism of a protein in ultraviolet range ( Andersen and Rost, 2009 ). More recently, PALSSE ( Majumdar  et al ., 2005 ) was designed to delineate protein structure into helices and strands, mainly using distance and torsion angle constraints to identify core elements which are later extended to longer segments. KAKSI ( Martin  et al ., 2005 ) is based on C α  distances and backbone dihedral angles and designed primarily to show concordance with the manual assignments found in the protein data bank (PDB). The most popular method in this space is ‘Dictionary of Secondary Structure of Proteins’ (DSSP) developed by  Kabsch and Sander (1983 ). DSSP is based on detecting hydrogen bonds between nitrogen and carbonyl groups along the protein polypeptide chain using a Coulomb approximation of the hydrogen-bond energy function ( Andersen and Rost, 2009 ). Many now consider this method a standard for secondary structural assignment ( Martin  et al ., 2005 ). Since DSSP was published, several methods have been designed that rely on computing the hydrogen-bond energy between backbone atoms. STRIDE ( Frishman and Argos, 1995 ) is among the successful variants of DSSP which uses a modified hydrogen-bond energy function as well as backbone dihedral angles to compute its assignment. SECSTR ( Fodje and Al-Karadaghi, 2002 ) is another variant which improves the detection and assignment of π-helices which both DSSP and STRIDE have difficulty characterizing ( Martin  et al ., 2005 ). There are other methods which assign secondary structure using 3D features in a protein structure.  Richards and Kundrot (1988 ) describe a method, DEFINE-S, to assign secondary structure using local geometry of ideal secondary structures. The P-CURVE ( Sklenar  et al ., 1989 ) algorithm uses an helicoidal axis approach derived from a series of peptide planes to assign secondary structure. The VoTAP ( Dupuis  et al ., 2004 ) algorithm relies on Voronoi tessellation of a residue contact map and then matching the contact map profiles to a consensus assignment of secondary structures by methods like DSSP and STRIDE. In the last category are indirect methods, such as STICK ( Taylor, 2001 ) and PMML ( Konagurthu  et al ., 2011 ) which work by approximating the C α  spatial trace using a set of lines. These methods seek the best approximation of the protein backbone using piecewise lines. Only as a post-process to this approximation, each line segment is indirectly attributed a secondary structural type based on criteria such as the average rise and pitch of the C α  atoms within the segment. These approaches solve a related yet different problem, namely ‘the best line approximation of the protein chain’. Consistent with this large number of proposed methods, assignment of secondary structure has been recognized to be an ‘inexact process’ ( Cuff and Barton, 1999 ). Previous comparative studies have highlighted the difficulties of existing programs to assign secondary structure consistently ( Andersen and Rost, 2009 ;  Colloc'h  et al ., 1993 ;  Cuff and Barton, 1999 ;  Martin  et al ., 2005 ;  Zhang  et al ., 2008 ). These disagreements can be major as shown by  Colloc'h  et al . (1993 ) where the percentage of agreement between DSSP, DEFINE-S and P-CURVE was only 63% on a residue basis. It has been observed that most disagreements arise in the terminal regions of the assigned secondary structural elements. Reflecting on this problem  Robson and Garnier (1986 ) comment [as quoted by  Martin  et al . (2005 )]: ‘In looking at a model of a protein, it is often easy to recognize helix and to a lesser extent sheet strands, but it is not easy to say whether the residues at the ends of these features be included in them or not. In addition, there are many distortions within such structures so that it is difficult to assess whether this represents merely a distortion, or a break in the structure. In fact, the problem is essentially that helices and sheets in globular proteins lack the regularity and clear definition found in the Pauling and Corey models.’ Given the complexity of the details of individual protein structures, it is not surprising that the secondary structure assignment problem has resisted a mathematically rigorous definition. The effect can be seen in the use of a variety of definitions by the existing tools, although all of them are reasonable. In this study, we describe an approach,  SST , to the secondary structure assignment problem using minimum message length (MML) inference ( Wallace and Boulton, 1968 ). Linking statistical inference with data compression, the goal is to communicate losslessly the coordinates of a protein using a two-part message. The first part transmits the secondary structure assignment as a  hypothesis  about the coordinates. The second-part transmits the details of coordinates not explained by the hypothesis. This gives rise to statistically robust objective function to optimize: find the best hypothesis on the coordinate data that minimizes the total two-part message length. SST  assigns secondary structure segments of the following types: α, 3 10  and π-helix (including left-handed versions of all these helices when they occur), sharp turns, β-strands and others (coil).  SST  in a post-processing step merges consecutive structures where appropriate, and groups all strands of a sheet, identifies β-bulges, to convert the results to a molecular biologist's conventional secondary structure description, and produces a PyMol script to visualize the secondary structural assignments. ( Fig. 1 .)
 Fig. 1. SST assigned secondary structure to coordinates of a 1.6Å crystal structure, Ornithine decarboxylase from mouse. 2 OVERVIEW OF MML CRITERION The MML criterion provides an information–theoretic objective for problems of inference where the goal is to find the best  explanation  (or  theory ,  hypothesis ,  model ) for a set of observed data ( Wallace and Boulton, 1968 ). MML relies on quantifying the amount of information  required  to convey losslessly the observed data in an  explanation message . The best hypothesis is the one which can convey the entire data set in the shortest possible explanation message. More formally, for some observed data  D  and a hypothesis  H  that offers an explanation of the data  D , Bayes's theorem ( Bayes and Price, 1763 ) gives
 
where  P ( H ) is the prior probability of hypothesis  H ,  P ( D ) is the  prior  probability of data  D ,  P ( H | D ) is the  posterior  probability of  H  given  D , and  P ( D | H ) is the  likelihood . Using Shannon's mathematical theory of communication ( Shannon, 1948 ), the amount of information for an explanation of the data  D  with the hypothesis  H  is given by
 
where  I ( x )=−log 2 ( P ( x )) gives the optimal code length to convey some event  x  whose probability is  P ( x ). This immediately gives an objective means to compare competing hypotheses. For hypotheses  H 1  and  H 2  on the same data  D , we have
 
It follows that the best hypothesis  H *  over all competing hypotheses is the one where the expression  I ( H * )+ I ( D | H * ) is minimized. A concrete realization of the MML framework comes from describing it as a communication process between an imaginary transmitter (Alice) and receiver (Bob) connected over a Shannon channel. Alice's objective is to send the observed data  D  using an explanation message in a form such that Bob can receive and decode the data  D  precisely as Alice sees it. Alice and Bob agree on a  codebook  containing the general rules of communication composed solely of common knowledge about typical, hypothetical data. Anything that is not a part of the codebook must be strictly transmitted as a part of the message. If Alice can find the best hypothesis  H *  on the data, Bob will receive a decodable explanation message most economically: The best inference about the data is the hypothesis that minimizes the total message length. Alice sends the explanation message of  D  in two parts. In the first part, she transmits the best hypothesis,  H * , she could find on the data  D  taking  I ( H * ) bits. In the second, she transmits the details of the observed data  D  not explained by  H * , taking  I ( D | H * ) bits (i.e. the deviations from  H * ). Notice that MML inference gives a natural trade-off between hypothesis complexity ( I ( H * )), and its goodness of fit to the data ( I ( D | H * )). For a comprehensive resource on MML see  Wallace (2005 ). 3 THE DESIGN OF THE COMMUNICATION FRAMEWORK Protein coordinates for a single-polypeptide chain are represented as an ordered set of 3D points of the form  ={ p 1 ,···,  p n }, where any  p i  corresponds to the  i th C α  coordinate along N- to C-terminus of the protein chain. Each  p i  defines a 3D real-valued vector ( p i x ,  p i y ,  p i z ) in Angstrom (Å) units, where each component of the vector comes specified (in the PDB) to three positions after the decimal place. Therefore, in this work, we treat the accuracy of measurement of the data as ϵ=0.001 Å (independent of the actual accuracy of the experimental structure determination). The transmitter (Alice) has to send a message to the receiver (Bob) who will then be able to reconstruct the original data from the encoded explanation message  exactly . For coordinate data from the PDB, Bob will reconstruct each coordinate of each atom to the original precision of three digits after the decimal point. 3.1 Null model description of a protein coordinate data MML gives a natural hypothesis test: The null-model corresponds to transmitting the data raw. If any hypothesis  H  on the data takes longer than the null model, then clearly  H  is unacceptable. However, the statement of the raw null model message (without any hypothesis) has to be economical; it must not be willfully inefficient. The construction of an efficient null model for protein coordinates relies on the observation that the distance between successive C α  atoms in a protein chain is highly constrained at about 3.8Å with only small deviations from this value. The method starts with the transmission of the first C α  coordinate  p 1  in any choice of encoding that both transmitter and receiver agree on. (Stating  p 1  simply adds a constant overhead to the message length, whether transmitted via a null model message or an explanation using a hypothesis. A simple way to do away with this overhead is for Alice to translate   such that  p 1  becomes the origin.  p 1  then need not be transmitted explicitly in the message and can be treated implicitly a part of the codebook.) Alice then computes the observed distance  r  between the successive C α  coordinates  p 1  and  p 2 . This distance  r  can be communicated efficiently using an encoding over a normal distribution  (μ, σ) with a certain fixed mean (μ) and a small standard deviation (σ) around it. Based on the prior knowledge of C α –C α  distances between successive atoms, these values are set to μ=3.8Å and σ=0.4Å and are considered to be part of the codebook. The probability density of a random variable  x  over a normal distribution with mean μ and a standard deviation σ is given by:
 
Therefore, the probability of stating any distance  r  to an accuracy of ϵ (given ϵ≪σ) using the above normal distribution is  P ( r )=ϵ× ( x = r ; μ, σ). This implies
 
The optimal code length to transmit  r  is given by −log 2 ( P ( r )) bits:
 (1) Note that Bob will not be able to recover  p 2  simply from the transmitted information of  p 1  and the distance  r  between  p 1  to  p 2 .  p 2  can lie anywhere on the surface of a sphere of radius  r  centered on  p 1 . The precise location of  p 2  stated to ϵ can be transmitted by first dividing the surface area of this sphere into cells each of area ϵ 2 . This results in 4π r 2 /ϵ 2  such cells distributed uniformly on the surface. These cells can be numbered using a convention that Alice and Bob both agree upon (as a part of the codebook). On the basis of this discretization of the sphere's surface area, Alice transmits the cell number  c  in which the observed  p 2  falls within. Assuming uniform probabilities, the probability that the point  p 2  falls in a cell number  c  is given by  P ( c )=ϵ 2 /4π r 2 . Following from this, the code length to state the cell number is:
 (2) 
Bob now has all the information to reconstruct  p 2  to the precision of ϵ using the information he has received. With  p 2  known at Bob's end, Alice can proceed to encode in the same fashion  p 3  with respect to  p 2 , then  p 4  with respect to  p 3  and so on until all the points in   are transmitted. Let  r i  (∀1≤ i &lt; n ) denote the observed distance between any two successive C α  coordinates  p i  and  p i +1 . Let  c i  (∀1≤ i &lt; n ) denote the cell number on the surface of the sphere of radius  r i  centered on the point  p i  in which the point  p i +1  falls. The message length to transmit the entire C α  coordinate data in   is therefore
 (3) 
where  O (1) denotes the constant number of bits to state  p 1  (0 bits if Alice translates the coordinates such that  p 1  lies on the origin). 3.2 Models to describe segments of proteins The secondary structure elements are used as a hypothesis to explain the coordinates. Here, we consider eight models to describe any contiguous stretch of C α  atoms (of arbitrary length) along the protein chain: (i) a right-handed α-helix; (ii) a left-handed α-helix; (iii) a right-handed 3 10 -helix; (iv) a left-handed 3 10 -helix; (v) a righ-handed π-helix; (vi) a left-handed π-helix; (vii) an extended β-strand; and (8) coil. The Helical (1–6) and strand (7) models follow ideal Pauling–Corey geometry ( Pauling and Corey, 1951 ) and are of arbitrary length. We term these seven collectively  ideal models . (Pauling–Corey models are common knowledge and taken to be in the codebook.) The coil model (8) is treated simply as a model that describes a segment of a protein raw, using the null model approach described above in  Section 3.1 . 3.3 Describing a protein segment using an Ideal model Assume that at some stage of the transmission Bob has received C α  coordinates up to an intermediate point  p i , that is he has received coordinates ( p 1 ,  p 2 ,···,  p i ) ( i &lt; n ). Alice now will transmit a contiguous segment of coordinates  p i  to  p j  (1≤ i &lt; j ≤ n ) using one of the ideal (helical or strand) models. If it is a good model, then the coordinates can be transmitted cheaply. (The discussion of the optimal choice is given in  Section 4 .) The number of points to be transmitted in this segment is  j − i  since the start point of the segment  p i  is already known at the receiver's end. The remaining points  p i +1 ,···,  p j  are transmitted as follows: 3.3.1 Transmitting the end point of the segment The end point of the segment ( p j ) is transmitted using the sphere approach similar to the one described in  Section 3.1 . Instead of the distance between successive C α  coordinates, Alice transmits the distance  d ij  between the start ( p i ) and end ( p j ) points. This is encoded using a normal distribution where the mean (μ) is taken as the distance ( d * ) between the start and end points from the ideal model containing  j − i +1 points. The standard deviation σ of the end point is set to min(( j − i )×0.2Å, 3Å) based on the length of the segment being transmitted and this rule is taken to be a part of the codebook. On the basis of Equation  1 , the code length to state  d ij  to the accuracy of ϵ is given by
 (4) On the basis of equation  2 , given the start point of the segment  p i  and distance  d ij  of the end point  p j , the end point can lie anywhere on a sphere with radius  d ij .  p j  can therefore be stated by specifying the cell number  c ij  on the surface of this sphere in
 (5) 3.3.2 Encoding the interior points With the start and end points already known, there are  j − i −1 interior points of the segment,  p i +1 ,···,  p j −1 , yet to be transmitted. These points can be transmitted cheaply if the chosen ideal model agrees with the observed points in the segment. Alice uses the following procedure to transmit the interior points given a chosen ideal model. (Details of how the optimal choice is made appear in  Section 4 .) Consider an ideal model containing  l = j − i +1 points, denoted formally as  ={ q 1 ,  q 1 ,···,  q l }. The coordinates in   are orthogonally transformed to  ′ ={ q ′ 1 ,  q ′ 2 ,···,  q ′ l } such that:
 q ′ 1  is same as the start point  p i  of the segment; the direction cosines of the vector connecting the start and end points of the ideal model  q ′ l − q ′ 1  and the direction cosines of the vector connecting the start and end points of the observed segment  p j − p i  are the same; and the sum of the squared error of the ( l −2) interior points of the segment with the corresponding interior points of the ideal model is minimized. That is, ∑ 1≤ k ≤ l −2  | p i + k − q ′ 1+ k | 2  is minimized, where |.| denotes the Euclidean vector norm. 
Such a spatial transformation is related to the more general superposition problem that minimizes the sum of the squared distance between two corresponding vector sets ( Kearsley, 1989 ). However, the transformation is further constrained such that the first points of the two sets are the same (Constraint 1) and the rotational axis for the ideal model is the vector between the start ( p i ) and end ( p j ) points of the segment (Constraint 2). The first two constraints can be achieved using elementary translation and rotation of the ideal coordinates. Once   is transformed such that the first two constraints are realized, the best rotation θ *  of   about the  p j − p i  axis has to be found so that Constraint 3 is realized. With an approach similar to the generalized superposition problem between two vector sets ( Kearsley, 1989 ), this minimization problem can be solved analytically as an eigenvalue decomposition of a 2×2 square symmetric matrix in quaternion parameters of the corresponding points. (The detailed proof of the analytical method is too long for the main text and hence is provided as  Supplementary Material .) Once the transformation of   to  ′  is achieved as described above, Alice can transmit the interior points of the segment,  p i +1 ,···,  p j −1  by:
 transmitting the best rotation about the  p j − p i  axis of the ideal model. (Note, Bob has already received the start and end points,  p i  and  p j .); transmitting the interior points  p i +1 ···  p j −1  as spatial deviations from their corresponding transformed interior points of the ideal model. (Bob already knows  p i  and the coordinates of   of the ideal model from the codebook. After he receives the end point of the segment  p j  (using the sphere approach described above), the ideal coordinates can be transformed such that Constraints 1 and 2 of the transformation discussed above are realized. After Bob receives the rotation θ * , the ideal coordinates are rotated by that angle around the axis  p j − p i  whose information he already has. Once Alice sends the spatial deviations of interior points  p i +1 ,···,  p j −1  with respect to the transformed ideal coordinates, Bob can reconstruct the observed interior points of the segment.) 
 3.3.3 Transmitting the rotation Rotation θ *  is transmitted using a uniform distribution over a circle whose radius  r θ *  is the farthest distance of an interior point of the ideal model from the axis of rotation. Note that  r θ *  need not be transmitted because it is a property of the coordinates of the ideal model which the receiver already knows as a part of the codebook. The rotation is transmitted by dividing the circumference of a circle of radius  r θ *  into arc segments of length ϵ and stating the segment number in which the rotated coordinate with the farthest radius to the axis falls. Thus, the code length of stating θ *  is
 (6) 3.3.4 Transmitting the interior points as spatial deviations Let any error vector of an interior point of the segment with respect to the corresponding transformed interior point of the ideal model,  e k ≡( p i + k − q ′ 1+ k ), 1≤ k ≤ l −2 have the vector components (Δ x k , Δ y k , Δ z k ). Each Δ x , Δ y  and Δ z  of an interior point is transmitted using a normal distribution with a μ of 0 and a standard deviation σ set to the sample standard deviation computed from these error components. Wallace (2005 ) gives the MML estimate of code length to transmit a set of independent data (Δ x 1 , Δ y 1 , Δ z 1 ), (Δ x 2 , Δ y 2 , Δ z 2 ), ··· (Δ x l −2 , Δ y l −2 , Δ z l −1 ) using a normal distribution as:
 (7) 
where  M =3×( l −2) is the total number of components of the error vectors  e k  being transmitted,  R σ  gives the prior knowledge of the limits to log 2  σ and κ 1 ≈1/12 denotes the constant corresponding to quantizing lattices proposed by  Conway and Sloane (1984 ). In this study, we assume that σ is bounded by 3Å because this is consistent with the limits of utility of root-mean-squared-deviation (RMSD) in superposition as a measure to estimate protein structural similarity. Therefore, combining code lengths from equations  4 – 7 , the code length required to transmit coordinates of a segment of a protein using any ideal model is give by
 (8) 3.4 Describing a protein segment using the coil model When transmitting a segment of a protein  p i ,···,  p j  as a coil, the coordinates are stated raw in that range using a null model ( Section 3.1 ). Therefore, the code length of stating a segment  p i ,···  p j  as a coil is
 (9) 
where  I ( r k ) and  I ( c k ) are code lengths given in equations  1  and  2 . 3.5 Describing the protein as a collection of segments Having laid the foundations of encoding segments of a protein using one of the ideal models or the coil model ( Sections 3.3  and  3.4 ), this section deals with describing the entire protein coordinates as a collection of segments of a model type. The main idea here is to find the best decomposition of points   of a protein into segments where each segment is described using exactly one of eight potential models. Note that the decomposition, with the associated model descriptors, gives a secondary structural hypothesis of a protein. Formally, a segmentation of  ={ p 1 ,···,  p n } gives an ordered subset of points  ′ ={ p ′ 1 ≡ p i 1 ,  p ′ 2 ≡ p i 2 ,···,  p ′ m ≡ p i m } where 1= i 1 &lt; i 2 &lt;···&lt; i m = n . Each successive pair of points in  ′ , 〈 p ′ 1 ,  p ′ 2 〉, 〈 p ′ 2 ,  p ′ 3 〉,··· 〈 p ′ m −1 ,  p ′ m 〉, defines the start and end points of a segment. (Notice that  ′  gives  m −1 segments of P, where end point of one segment is same as the start point of the next.) Associated with each segment 〈 p ′ k ,  p ′ k +1 〉 of length  l k = i k +1 − i k +1 is a model type  t k , 1≤ k ≤ m −1. A secondary structural assignment of   is given by the segmentation { p ′ 1 ,···,  p ′ m } and its corresponding model assignment { t 1 ,···,  t m −1 }. We note that for  n  points in  , there are   possible segmentations – the first and last points of  ′  are the same as those in  . Since each segment of any segmentation can be assigned to any of the eight possible model types, the total possible secondary structural assignments is given by the formula:  . For an average protein, this gives a massive search space. (An efficient dynamic programming method to find the best secondary structural assignment is detailed in  Section 4 .) Any given segmentation  ′  of   and its associated model types acts as a secondary structural hypothesis of the given coordinate data. Alice can describe and transmit the coordinates in   using this hypothesis over a two-part message. 3.5.1 First part of the explanation message In the first part of the message, Alice communicates the segmentation  ′ ={ p ′ 1 ,···,  p ′ m } and its corresponding model assignments { t 1 ,···,  t m −1 } as the hypothesis on the observed coordinate data in  . This part of the message will be composed of the following:
 the number of segments ( m −1) in  ′  and for each segment  p ′ k  (1≤ k ≤ m −1), communicate
 the length of the segment  l k = i k +1 − i k +1 and the model type  t k  to encode the points in that segment. 
 
 The number of segments ( m −1) is an integer transmitted using a log *  distribution assuming a universal prior on the distribution of numbers.  Rissanen (1983 ) gives the code length of transmitting any integer  n &gt;0 as
 (10) 
where log 2 * ( n )=log 2 
 n +log 2  log 2 
 n +··· (over all +ve terms). Next, the lengths of the segments are positive integers. Although these integers can also be transmitted using a log *  distribution, it is rather inefficient because in practice the lengths of helices, strands and coils are constrained. Therefore, in this work, we, encode the lengths of the segments using a Poisson distribution with a predefined mean of λ for each model type: (The parameters λ for each of the eight types of models are treated to be a part of the codebook. In this work we empirically set λ=4 for coil and λ=5 for strands. The lengths of helices are transmitted using a mixture of two Poisson distributions with means 4 and 8.)
 
The code length to state any integer  n &gt;0 using this distribution is:
 (11) Finally, each model type  t  (encoded as an integer 0≤ t ≤7) of any segment is stated using a uniform distribution (uniform is the simplest choice. Since some models as more probable than others (e.g. α-helices and strands are significantly more probable than other models), a more elaborate coding scheme can also be considered taking into account the empirical distribution of various models.) in
 (12) In summary, combining equations  10 – 12 , the code length of the first part of the message proposing the secondary structural hypothesis on   is
 (13) 3.5.2 Second part of the explanation message In the second part of the message, Alice sends the actual details of the coordinates in   economically given the hypothesis  ′ . The procedure to transmit coordinate data of a segment of a protein using the ideal and coil models has been discussed in  Sections 3.3  and  3.4 . Using the notations in  Section 3.5 , the hypothesis received in the first part of the message is of the form 〈 p ′ k ,  p ′ k +1 〉, with each segment of some length  l k  and type  t k . The message length to transmit the coordinates given the above segmentation is:
 (14) 
where  O (1) is the constant number of bits to state the first C α  coordinate  p 1  ( Section 3.1 ),  I model ( p ′ k ,···,  p ′ k +1 )= I ideal ( p ′ k ,···,  p ′ k +1 ) if 0≤ t k ≤6 and  I model = I coil ( p ′ k ,···,  p ′ k +1 ) if  t k =7. The total message length of communicating   with  ′  comes from combining equations  13  and  14 
 (15) 3.6 Problem Statement From the earlier section, the problem of inferring the best secondary structural assignment can now be stated formally as follows: given   containing  n  points, find the secondary structural segmentation  ′  and its corresponding model assignment such that the total message length to transmit   losslessly,  I total  ( &amp; ′ )= I first ( ′ )+ I second ( | ′ ) is minimized. 4 INFERENCE OF SECONDARY STRUCTURE This section describes the search method to find the best MML secondary structural segmentation from given coordinate data. 4.1 Constructing the code length matrices Equation  15  gives the total message length of communicating coordinates in   using a segmentation  ′ :
 For a given protein, any pair of points can potentially be the start and end points of a segment. At the same time, a segment can be described using any of the eight models considered here. Therefore, the procedure to assign secondary structure to C α  coordinates { p 1 ,···,  p n } in a given protein begins by constructing a set of eight code length matrices, one for each model type  t  (0≤ t ≤7):
 (16) 
where any cell ( i ,  j ) 1≤ i &lt; j ≤ n  of the matrix for type  t  gives the code length of stating the segment  p i  to  p j  using the model  t . 4.2 Finding the best secondary structural assigning The segmentation of   using various model types enforces a strict ordering constraint, satisfying the requirements for a solution by dynamic programming, even though the search space is huge as discussed in  Section 3.5 . Let any  D ( i ) store the optimal message length of transmitting points  p 1 ,···,  p i , for all 1≤ i ≤ n . With the boundary condition of  D (1)=0, the dynamic programming recurrence to find the optimal assignment is given by
 The above recurrence is used to fill the array  D  iteratively from 1 to  n . On completion, the best secondary structure assignment can be derived by remembering the index  i  and type  t  from which the optimal  D ( j ) is computed. 5 POST-PROCESSING In the post-processing step, the above-defined successive segments of same type (helical or strand) from the MML inference are examined for moderate curvature. Further, sharp turns are identified and distinguished from coil assignments. Finally, β-sheets are identified by grouping together the assigned strands. Checking for moderate curvature in helix and strand : The MML inference automatically gives the best (in the information–theoretic sense) piecewise approximation of a curved helix or strand. In a single pass through the secondary structural assignment generated by our method, we check for such curvatures and merge successive segments to form a larger segment. Each helix (1≤ t ≤6) and strand ( t =7) is represented by a vector. For a helical segment, the vector is the axis of the helix. For the strand segment, the vector is the least-square line fitting its C α  atoms. Two successive segments (of the same model type) are joined to form a single segment if the orientation angle of their vectors is within 30 ○ . [For a detailed description of the finding the axes and orientation angle, see  Konagurthu  et al . (2008 )] Identifying sharp turns : sharp turns often have geometries that are conformationally similar to a turn of ideal helical models considered in this work. Any assignment of a helical segment of length less than or equal to four residues that is preceded and succeeded by other assigned segments is relabeled as a sharp turn. However, α-helices often fray or tighten at their N- or C-terminal ends giving a short stretch of π or 3 10  helical segment. In order not to incorrectly assign these ends as sharp turns, the orientation of a candidate segment is checked against the preceding and succeeding segments and only assigned a sharp turn if the segment's orientation exceeds 45 ○  relative to its neighbouring segments. Grouping strands forming β-sheet : all strand segments are extracted from a given assignment. A strand adjacency matrix is computed where two strands are treated to be adjacent if and only if they are in parallel (with orientation angle in the range ± 45 ○ ) or anti-parallel (with orientation angle in the range [135 ○  to 180 ○ ] or [−135 ○  to −180 ○ ]) orientation and there exist Van der Waals interactions between at least two pairs of atoms from the segment. Strands from a β-sheet are then identified and grouped using a complete depth-first search on the strand adjacency matrix. 6 RESULTS Implementation : a program ( SST ) implementing the method described in the previous section has been developed in  C++  programming language. The program accepts protein coordinates in the Brookhaven PDB format and outputs the secondary structure assignment both at a segment level (stating the start and end point of each secondary structural segment) as well as at a residue level. The program also generates a PyMol script, which allows users to visualize the secondary stucture assignment. ( Fig. 1 ). Datasets and comparison methods : To study the performance of  SST , we consider a dataset of 1737 PDB structures. These structures are the same as the dataset considered by  Martin  et al . (2005 ), excluding the structures which have been deprecated or those structures that failed running on any one of the considered methods. (See below.) These structures are divided into four datasets: high-resolution (HRes) dataset with 631 crystal structures solved to 1.7 Å or better; medium resolution (MRes) dataset with 582 crystal structures with resolution between 1.7 and 3 Å resolution; low-resolution (LRes) dataset with 306 structures with resolution &gt;3 Å and finally, a dataset of 218 NMR structures. In this work, we mainly compare  SST  with  DSSP  ( Kabsch and Sander, 1983 ) and  STRIDE  ( Frishman and Argos, 1995 ) exhaustively on the large structural dataset described earlier. Although there are other programs for secondary structure assignment, we had difficulty finding distributions that we could download. For those we managed to download, we could not install the programs due to unresolvable dependencies in their source code. However, we use a web server  2Struct  ( Klose  et al ., 2010 ), which allows manual submission of queries with a single point of access to a variety of secondary structure assignment methods beyond  DSSP  and  STRIDE  such as  KAKSI  ( Martin  et al ., 2005 ),  PALSSE  ( Majumdar  et al ., 2005 ),  STICK  ( Taylor, 2001 ),  XTLSSTR  ( King and Johnson, 1999 ) and  P-SEA  ( Labesse  et al ., 1997 ). To facilitate comparisons with other methods, we randomly selected 30 low-resolution structures from the LRes dataset and manually collected the secondary structure assignments from the server. The list of structures used in this experiment can be download from  http://www.csse.monash.edu.au/~karun/sst . Comparison : we first assess the composition of the assigned regular secondary structures (helices and strands) using  DSSP ,  STRIDE  and  SST  over the four datasets described earlier. Overall  DSSP  assigns 34.5% of residues to helices and 19.9 of residues to strands.  STRIDE  assigns 37.3 and 27.4%, and  SST  assigns 40.4 and 25.3% of residues to helices and strands, respectively. In general,  DSSP  is conservative in assigning regular secondary structures resulting in shorter elements compared with output from  STRIDE  and  SST . Examining the lengths of various secondary structural segments, we observe that the average length of a helix and strand segment assigned by  SST  is 12.1 and 6.1 residues, respectively. Unlike  SST , which states the residue start and end points of each segment, computing the lengths of secondary structural elements from  DSSP  and  STRIDE 's output is problematic and error-prone ( Majumdar  et al ., 2005 ). Table 1  shows the comparison between  SST ,  DSSP  and  STRIDE  over high, medium, low resolution and NMR structures. To undertake this comparison, the assignments of the three programs are grouped into three classes: helix (of all types), strand and other.  DSSP  and  STRIDE  use similar methods for assignment based on detecting hydrogen bonds. Therefore, as one would expect, their assignments are highly similar.  SST  largely agrees with  DSSP  and  STRIDE  when assigning helices. Strands, however, show some disagreement. Visually examining several instances, we find that in many cases  SST  assigns longer strands than the other two methods. Agreement between strand assignments on NMR structures among the three methods is rather poor. Surprisingly, even  DSSP  and  STRIDE  differ enormously in this class even though their assignment methods are quite similar.
 Table 1. Performance of  SST  compared with  DSSP  and  STRIDE  on four datasets: HRes, MRes, LRes and NMR HRes (% Agreement) MRes (% Agreement) Helix Strand Total Helix Strand Total SST  versus  DSSP 97.6 81.9 84.1 97.6 83.4 83.9 SST  versus  STRIDE 97.1 80.8 84.3 97.2 82.3 84.3 STRIDE  versus  DSSP 99.4 98.5 96.7 99.4 98.9 96.9 LRes (% Agreement) NMR (% Agreement) Helix Strand Total Helix Strand Total SST  versus  DSSP 97.7 84.3 82.7 98.4 53.8 51.5 SST  versus  STRIDE 97.2 82.3 83.8 94.4 78.9 83.9 STRIDE  versus  DSSP 99.3 98.3 96.0 99.6 64.6 68.7 Columns labelled ‘Helix’ and ‘Strand’ give the percentage agreement of residues assigned as helix and strand, respectively, between the two methods. Column ‘Total’ gives the percentage agreement over three classes: helix, strand and others. 
 Table 2  extends the comparison of  SST  with other popular methods for assignment on low-resolution structures. The table shows the percentage agreement of secondary structural assignments between the methods. Consistent with previous comparative studies ( Colloc'h  et al ., 1993 ;  Martin  et al ., 2005 ), we see considerable differences in the assignments. In the absence of a universally acknowledged  gold standard  for assignment, it becomes very difficult (if not impossible) to objectively validate one method to be truly better than the other. The observed differences mainly arise from the different criteria used by the methods. However, manually examining many cases where the methods differ, we find that most disagreements appear at the ends of various (helical or strand) segments. We will use a simple example to highlight the most common type of differences.  Figure 2  gives the overall residue-level secondary structure assignment across different methods for a flavodoxin structure from  Clostridium beijerinckii  (wwPDB ID: 5NLL ).  DSSP  and  STRIDE  assignments are nearly identical to each other. From the figure, small disagreements between methods can be seen around the start and end points of various segments demarcated by  SST s segment view (labelled  SST(SEG) ). A major difference between  DSSP  and  SST  is the region Lys28…Asn34, which  SST  assigns as a strand.  DSSP  starts the segment three residues further at Asn31. Inspecting the structure, we find a backbone hydrogen bond between Asp29 and Met1. This might suggest the start of the strand at either Asp29 or one residue upstream at Lys28 as identified by  SST . Also, in the region Glu62… Ile73, only  SST  correctly assigns a π-helical cap (Glu62… Phe66) leading into a α-helix.
 Fig. 2. Residue-level secondary structure assignment of a 1.75 Å flavodoxin structure from  Clostridium beijerinckii .  SST  residue-level assignment and the segment boundaries are shown in addition to the assignments across multiple methods. For details of the secondary structure codes, see  http://www.csse.monash.edu.au/karun/sst/codes.html 
 Table 2. Pairwise comparison between secondary structure assignment methods DSSP  (%) STRIDE  (%) KAKSI  (%) PALSSE  (%) P-SEA  (%) STICK  (%) XtlSStr  (%) SST 77.4 75.3 74.3 80.7 53.9 69.0 74.9 DSSP 86.1 76.9 76.0 57.1 71.8 77.6 STRIDE 67.8 74.8 48.8 64.5 73.8 KAKSI 75.4 67.4 79.4 72.9 PALSSE 50.9 69.9 70.8 P-SEA 66.6 54.1 STICK 66.7 Each cell in the upper-triangular matrix gives the percentage agreement of the residue-level assignments between a pair of methods indicated in the first row and column. The agreement is measured over all three classes: helix, strand and other. 
 To evaluate the consistency of  SST s secondary structural assignments on coordinates solved at different resolutions, we randomly selected 15 protein structures for which both the superseded low-resolution coordinates and the new high-resolution coordinates were available.  Table 3  gives the list of considered structures along with the percentage agreement between  SST s assignment at different resolutions. The results indicate that  SST  produces consistent results on structures determined at different resolutions. The &lt;10% differences ( Table 3 , last column) in agreement on the chosen structures may well represent genuine structural differences rather than shortcomings of the algorithm.
 Table 3. SST  assignment sensitivity to changes in coordinate resolution. Resolution numbers marked with  *  are taken from the original papers Structure name LRes PDB ID HRes PDB ID %Agree Lysozyme 2LZH (6.0  Å) 2ZQ3 (1.6  Å) 95.3 Ferrochelatase 1LD3 (2.6  Å) 1DOZ (1.8  Å) 97.4 Glutamate Dehydrogenase 1AUP (2.5  Å) 1BGV (1.9  Å) 90.6 Pseudomonas Cytochrome 151C (2.0  Å) 351C (1.6  Å) 93.9 Bence-Jones Protein 1BJL (2.9  Å) * 3BJL (2.3  Å) 90.2 Concanavalin A 4CNA (2.9  Å) * 5CNA (2.0  Å) 91.8 Endochitinase 1BAA (2.8  Å) 2BAA (1.8  Å) 95.5 Ferredoxin Reductase 1FNR (2.6  Å) 1FND (1.7  Å) 95.9 Endonuclease III 1ABK (2.0  Å) 2ABK (1.6  Å) 97.6 Myohemerythrin 1MHR (2.9  Å) * 2MHR (1.3  Å) 92.4 Phosphofructokinase 5PFK (7.0  Å) * 6PFK (2.6  Å) 95.3 Serine Protease Inhibitor 1QLP (2.9  Å) 2PSI (2.0  Å) 95.4 Dimeric Hemoglobin 1SDH (2.4  Å) * 3SDH (1.4  Å) 98.4 Glutathione Reductase 1GRS (3.0  Å) * 3GRS (1.5  Å) 94.4 Calmodulin Fragment TR2C 1TRC (3.6  Å) 1FW4 (1.7  Å) 93.9 
 Further, to illustrate the reliable segmentation produced by  SST  on structures with long, curved helices and strands, we chose two structures: Leucine zipper protein (wwPDB:  1NKP ) composed of very long helices and Sucrose-specific porin protein (wwPDB:  1A0S ) composed of long, curved strands forming β-barrels. Although  SST  initially breaks the curved segments into smaller pieces, the post-processing step explained in  Section 5  reconstitutes these pieces back correctly into fuller segments. [ Fig. 3  gives  SST s assignment on the porin protein ( 1A0S ). The figure shows that the curved strands of the β-barrels have been reconstituted and grouped reasonably well in the post-processing step.]
 Fig. 3. Automatically generated PyMol image of  SST s secondary structural assignment on sucrose-specific porin (ScrY) from  Salmonella typhimurium  (wwPDB:  1A0S ) Finally, as a difficult case we consider the 10 Å resolution protein coordinates of Elongation Factor Tu (GDB.Kirromycin) from  Escherichia coli  (wwPDB:  1qzd ) solved using Cryo-Electron Microscopy. Its wwPDB file contains only C α  coordinate information.  DSSP ,  STRIDE  and  P-SEA  fail to process such information as the coordinates of other atoms are needed to decipher Hydrogen bonds.  KAKSI  and  XTLSSTR  are able to process this structure but assign all residues in the chain to coil. Of the considered methods, only  SST ,  PALSSE  and  STICK  assigned any secondary structure. For lack of space, the overall residue-level assignment across these three methods are presented in  Figure 1  of the supplementary text. Examining the structure,  PALSSE  consistently overestimates the regular secondary structural regions by a large margin.  STICK  performs well, especially in identifying β-strands. However, it miscalculates several secondary structural elements. In comparison,  SST  produced the most reasonable segmentation of the three methods on visual inspection of the structure. 7 CONCLUSION Reliable secondary structure assignment is an important problem. We have developed a novel information theoretic method to address this problem using the Bayesian framework of MML inference. Careful examination of the results over a large number of structures suggests that our method gives consistent assignments even on low-resolution data. We note that our method uses a dictionary of models composed of ideal secondary structural elements. The details of the models are explicit and open to scrutiny. It is likely that these models can be improved. (‘Essentially, all models are wrong, but some are useful.’—George Box.) However, modification to the models is an improvement if, and only if, it yields extra compression. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Accelerating Bayesian inference for evolutionary biology models</Title>
    <Doi>10.1093/bioinformatics/btw712</Doi>
    <Authors>Meyer Xavier, Chopard Bastien, Salamin Nicolas, Kelso Janet</Authors>
    <Abstract/>
    <Body>1 Introduction Model inference and hypothesis testing is nowadays widely performed using Bayesian inference. The surge of popularity of this statistical method is tightly linked with the theoretical and computational developments of Markov chain Monte Carlo methods (MCMC;  Hastings, 1970 ;  Metropolis  et al. , 1953 ). Indeed, MCMC made possible the numerical approximation or sampling of high-dimensional posterior distribution, thus broadening the inference of more complex statistical models. Evolutionary biology particularly benefited from this progress and a large number of applications, ranging from phylogenetic reconstructions ( Lartillot  et al. , 2013 ;  Ronquist  et al. , 2012 ), divergence time analyses ( Drummond  et al. , 2006 ), molecular evolution ( Dib  et al. , 2014 ), comparative methods ( Beaulieu  et al. , 2012 ;  FitzJohn, 2012 ) or population genetics ( Kuhner, 2006 ;  Fischer  et al. , 2011 ), makes extensive use of these MCMC approaches. While popular, MCMC is limited by its inherent sequential nature and its dependency on user defined transition kernels. Moreover, complex models require costly computational evaluations of their likelihood function and their high-dimensional parameter-space is usually difficult to explore efficiently. A wide variety of approaches have been proposed to improve the sampling of some of these complex models. These solutions range from avoiding the evaluation of the likelihood function ( Marjoram  et al. , 2003 ;  Mengersen  et al. , 2013 ) to using sequential Monte Carlo ( Andrieu  et al. , 2010 ;  Cappe  et al. , 2007 ) or Hamiltonian Monte Carlo methods ( Duane  et al. , 1987 ;  Neal, 2011 ). However, these methods suffer from limitations that make them less generally applicable. For example, some have to be applied on a model-specific basis, may prove difficult to apply to high-dimensional parameter-space or require continuous parameters. These limitations are typical of Bayesian inference in phylogenetics. Indeed, the computational complexity of tree building grows with the length of the sequence data available as well as with the number of taxa, which also defines the amount of parameters and potential phylogenetic trees to consider ( Felsenstein, 2004 ). Moreover, it is well known that sampling from this discrete space of phylogenetic trees is a particularly difficult task ( Bouchard-Côté  et al. , 2012 ;  Lakner  et al. , 2008 ). Current approaches to build phylogenetic trees therefore still heavily relies on MCMC and any enhancements of these methods would be extremely useful for evolutionary biologists. Several studies focused on the sampling effectiveness (or mixing) of the MCMC process and helped to propose theoretical guidelines to specify the optimal size for transition kernels based on normal distributions ( Gelman  et al. , 1996 ;  Roberts  et al. , 1997 ). These guidelines were used to develop the adaptive Metropolis algorithm ( Haario  et al. , 2001 ), which aims at optimally tuning a transition kernel using the observed empirical covariance of the Markov chain. The adaptive Metropolis method has been further improved by using component-wise scaling ( Haario  et al. , 2005 ), adaptively tuning the proposal size to target an optimal acceptance rate ( Andrieu and Thoms, 2008 ) or exploiting the parameter covariance ( Roberts and Rosenthal, 2009 ;  Vihola, 2012 ). Some of these improvements have been implemented in the main software to build phylogenetic trees ( Aberer  et al. , 2014 ;  Drummond  et al. , 2012 ;  Ronquist  et al. , 2012 ). However, few strategies were explored to improve MCMC efficiency by exploiting parallel computing. While some of these methods offer interesting properties, they usually suffer from limitations that hinder their general use on applied problems. For example, prefecthing ( Brockwell, 2006 ) aims at predicting the future states of the Markov chain to pre-process them in parallel. This method is however limited by the difficulty of accurately predicting the path of the Markov chain. We refer the reader to the recent review of  Green  et al.  (2015)  for an extensive state-of-the-art description of this domain. We present here a novel combination of enhancements of the Metropolis-Hastings (M-H) framework specifically designed to obtain an efficient sampling of parameter-rich and complex models. We first present a multivariate adaptive proposal that coerces the acceptance rate, balances the mixing among all parameters and exploits their potential correlations. We then show that under a precise coupling of both adaptive MCMC and pre-fetching, we can achieve synergetic performance gains that exceed the sum of its parts. Finally, the framework properties are validated on various test cases and is then challenged on two real biological applications: a macroevolutionary model ( Silvestro  et al. , 2014b ) and on a model for Bayesian inference of the phylogeny ( Felsenstein, 2004 ). 2 Methods 2.1 Background and notation Starting from data  X  and a model  M  with unknown parameters  Θ ∈ ℝ d , we are interested in sampling the posterior distribution
 p ( Θ | X ) = f ( Θ ) · f ( X | Θ ) ∫ f ( Θ ) · f ( X | Θ ) 
or its unnormalized density  π ( Θ ) ∝ f ( Θ ) · f ( X | Θ ) . Sampling this distribution can be done by setting a Markov chain with  π ( Θ )  as stationary distribution. This is done by considering a reversible Markov chain whose transition kernel  p ( Φ , Θ )  satisfies the detailed balance equation
 π ( Θ ) p ( Θ , Φ ) = π ( Φ ) p ( Φ , Θ )     ∀   ( Θ , Φ ) The kernel  p  is based on two functions: an arbitrary transition kernel  q ( Φ , Θ ) , also known as proposal window, and an acceptance probability  α ( Θ , Φ ) . The probability of leaving the state Θ is given by
 p ( Θ , Φ ) = q ( Θ , Φ ) α ( Θ , Φ ) 
with  Θ ≠ Φ , while the probability of staying in the same state is then given by
 p ( Θ , Θ ) = 1 − ∫ q ( Θ , Φ ) α ( Θ , Φ ) d Φ 
 Hastings (1970)  proposed the following acceptance ratio
 α ( Θ , Φ ) = min ( 1 , π ( Φ ) q ( Φ , Θ ) π ( Θ ) q ( Θ , Φ )   ) which simplifies to the original algorithm of Metropolis et al. ( Metropolis  et al. , 1953 ) if the transition kernel  q  is symmetrical. 2.2 An efficient multivariate adaptive proposal Our framework uses its own multivariate adaptive proposal specifically designed to maintain a low computational cost while exploiting the potential correlations between parameters in complex and parameter-rich models. The original adaptive method presented by  Haario  et al.  (2001)  defines the proposal as  Φ ∼ N ( Θ , λ Σ )  with  λ  being the optimal scaling factor ( Gelman  et al. , 1996 ) and Σ the observed empirical covariance of the Markov chain. While being based on the same concepts, our adaptive process approximates the covariance matrix Σ using stochastic approximation methods and optimizes the mixing by coercing the acceptance rate ( Andrieu and Thoms, 2008 ). Given the sample  Θ t  observed at iteration  t  of the MCMC process, the global scaling factor  λ , the parameter mean  Θ ¯  and covariance Σ are updated as follow
 (1) Θ ¯ ( t + 1 ) = Θ ¯ ( t ) + γ ( t ) · ( Θ ( t ) − Θ ¯ ( t ) ) Σ ( t + 1 ) = Σ ( t ) + γ ( t ) · [ ( Θ ( t ) − Θ ¯ ( t ) ) ⊗                                             ( Θ ( t ) − Θ ¯ ( t ) ) − Σ ( t ) ]   log   ⁡ ( λ ( t + 1 ) ) =   log   ( λ ( t ) ) + γ ( t ) ( α ¯ ( t ) − α ⋆ ) 
with  α ⋆  defining the target acceptance rate that optimizes the mixing ( Roberts  et al. , 1997 ) and  α ¯ ( t )  being the average observed acceptance rate after  t  iterations. The step size γ impacts the convergence of the approximation process and must have the following two properties ( Robbins and Monro, 1951 ):  ∑ t = 0 ∞ γ ( t ) = ∞  and  ∑ t = 0 ∞ ( γ ( t ) ) 2 &lt; ∞ . Sequences defined as  γ ( t ) = C / t β  with  β ∈ [ ( 1 + η ) − 1 , 1 ]  satisfy these conditions and their rate of convergence is determined by the two user-defined constants  C  and  η . Coercing the global scaling factor λ guarantees that the overall proposal acceptance rate is optimal by taking advantage of the information made available by the MCMC process. However, it does not ensure a balanced mixing over all parameters. Component-wise scaling is able to solve this problem at the cost of  d  additional likelihood evaluations ( Haario  et al. , 2005 ). The new component-wise or local scaling factors  Λ = d i a g ( λ 1 , … , λ d ) , estimated separately for each parameters, can then be used to scale the proposal over each dimension (corresponding to a parameter) as follows:
 Φ ∼ N ( Θ , Λ 1 / 2 Σ Λ 1 / 2 ) This approach guarantees a more balanced mixing over all parameters while inducing a significant increase in computational cost. A second drawback is the loss of control on the global acceptance rate. Indeed, independent parameters would lead to a global acceptance rate defined as  α = ∏ i = 1 d α i . However, in the more frequent case of correlated parameters, the relation between local acceptance rates α i  and the global acceptance rate α remains undefined. In order to efficiently balance the mixing along all parameters, our adaptive proposal combines both local and global scaling variants. This proposal maintains the coercion of the global acceptance rate by using the global scaling λ, which is updated at each iteration. Component-wise updates are made periodically and the normalized scaling factors  w i = λ i / ∑ k = 1 d λ k  form the local scaling matrix  W = d i a g ( w 1 , … , w d ) . The proposal then uses the global scaling to target the overall size of the move while the normalized local scaling ensures a balanced mixing over all directions.
 (2) Φ ∼ N ( Θ , λ W 1 / 2 Σ W 1 / 2 ) The method that we propose can still be improved when the parameters Θ are correlated ( Gilks  et al. , 1995 ). These correlations are exploited by using the geometric interpretation of the multivariate normal distribution. Since this distribution belongs to the family of elliptical distributions, it can be expressed by the directions of the principal axes of the ellipsoids. The spectral decomposition of the covariance matrix
 Σ = Q E Q ′ = Q E 1 / 2 ( Q E 1 / 2 ) ′ 
gives these directions as the eigenvectors  Q = ( V 1 , .. , V d )  and their scaling as the eigenvalues  E = d i a g ( e 1 , .. , e d ) . We can then sample from the distribution
 X ∼ N ( μ , Σ ) ≡ μ + Q E 1 / 2 N ( 0 , I ) 
The component-wise scaling factors are used in this case to scale the size of the moves along the direction of the eigenvector  V i  and the scaled eigenvectors  ( λ W ˜ E ) 1 / 2  can be used to build the following proposal
 (3) Φ ∼ Θ + Q ( λ W ˜ E ) 1 / 2 N ( 0 , I ) Our method is still limited by its additional computational cost that grows polynomially with the number of dimensions  d . The covariance matrix is costly to learn since it requires linear algebra functions with complexity  O ( d 2 ) . In addition, the generation of random moves based on multivariate distributions requires the Cholesky or eigen-decomposition of the scaled covariance matrix at a computational cost of  O ( d 3 ) . The decomposition must be updated whenever Σ and Λ change. We created smaller blocks of parameters to reduce the computational burden and monitored the convergence of the adaptive phase (see supporting materials). Moreover, our adaptive proposal evolves with the information contained in the covariance matrix. In the initial phase, parameters are considered independent. The covariance matrix is learned as usual but moves are independently generated from
 (4) Φ ∼ N ( Θ , λ W 1 / 2 Σ ˜ W 1 / 2 )       with       Σ ˜ = d i a g ( σ 1 2 , … , σ d 2 ) , 
which avoids the Cholesky or eigen-decomposition step. Upon convergence detection of Σ, the observed degree of correlation between parameters is assessed. If no significant correlations are detected, the training phase ends. Otherwise the proposal is switched to a proposal based on the full Σ matrix ( Eqs. (2)  or (3)) and the training phase is prolonged until a second convergence is reached. 2.3 Securing an optimal pre-fetching Our adaptive proposal greatly improves the mixing properties of the MCMC, but it can also enhance the performances of a parallel MCMC method. Markov chains are an inherently sequential process due to the dependencies between states in two subsequent iterations of a chain. Obtaining speed improvements by using parallel computing techniques is therefore an important challenge. Pre-fetching ( Brockwell, 2006 ) overcomes this limitation by pre-processing the possible paths that the Markov chain could take during a set of iterations. The future path of a MCMC can be represented as a decision tree ( Fig. 1 ). Given that the chain at time  t  is in state  Θ ( t ) , a new state  Φ ( t )  is proposed. This new state is then accepted or rejected with probability  α ( Θ ( t ) , Φ ( t ) ) . Branches of the tree represent these two possible paths of the MCMC process. Each state, or node, leads to two subtrees corresponding to either an acceptance or a rejection. Fig. 1 Markov chain’s decision tree of depth  k  = 3 The likelihood computation of the future possible states  Φ  can then be distributed over multiple processors. Given that several samples are generated during the same amount of time that it previously took for one, this results in a significant speed up. However, given the tree structure of the possible paths of the Markov chain, an exhaustive approach to get  D  =  k  useful samples, or draws, per pre-fetching iterations would require  n p = 2 k − 1  processors. This strategy scales poorly since the number of wasted likelihood computations corresponding to unvisited state grows exponentially with  k . Various strategies ( Strid, 2010 ) aiming at determining the most probable path in the decision tree have been proposed to improve the scaling of this method. One of these strategies uses the mean observed acceptance rate of the chain  α ¯  as a predictor for the most probable paths. As defined  in (Strid, 2010 , Eq. 8), the efficiency of this method can be estimated by a performance model that depends on the acceptance rate and the number of available processors:
 (5) E ( α , n p ) = E 1 ( α ) · D ( α , n p ) 
where  E 1 ( α )  defines the mixing efficiency for an i.i.d normal distributed proposal with  d → ∞  ( Strid, 2010 , Eq. 7) and  D ( α , n p )  is the expected number of draws per pre-fetching iterations ( Strid, 2010 ,  Eq. 1 ). The expected number of draws  D ( α , n p )  is equal to  n p  and thus optimal for α = 0 or α = 1. With such  α , pre-fetching is indeed able to exactly predict the path of the Markov chain by considering moves as being always rejected, respectively accepted. Such prediction would be represented by the right most, respectively left most, branch in the decision tree ( Fig. 1 ). The expected number of draws  D ( α , n p )  reduces as α approaches 0.5, since predictions are less accurate. When α reaches 0.5, any path in the decision tree is equally probable and thus pre-fetching is back to using the inefficient exhaustive approach of computing all the possible paths ( D = log ⁡ 2 n p ). On the other hand, the mixing efficiency of a proposal  E 1 ( α )  peaks at an optimal α value of 0.234 with  d → ∞  ( Roberts and Rosenthal, 2001 ). Therefore, the optimal acceptance rate  α ⋆  for a given number of processors can be derived from  equation 5  and expresses the optimal trade-off between an efficient mixing and accurate predictions for pre-fetching. In others words, the efficiency of both method is then controlled by the average acceptance rate α of the proposals and is optimal for  α ⋆ . Therefore, we coupled pre-fetching to our adaptive proposals by coercing their acceptance rate to the value  α ⋆  optimizing  equation 5  through the global scaling factor λ. Although we do not know a priori the acceptance rate of the MCMC process, this approach ensure that it will quickly reach the vicinity of the target acceptance rate  α ⋆ . This combination of methods will optimize the efficiency of the pre-fetching method by finding the optimal size of the proposal window in function of the number of available processors and regardless of the model. In return of providing these optimal conditions for the pre-fetching method, the adaptive proposal also benefits from this coupling. During a pre-fetching iteration, only a certain number of predicted states are retained, while the remaining unused ones are discarded. However, our adaptive proposal takes advantage of these wasted likelihood evaluations. During one iteration, all the acceptance rates  α  estimated to predict the chain path in the decision tree offer usable information that can help the updating of λ. The advantage is that the training process becomes more accurate and adapts more quickly to the parameter space, because λ is adapted more frequently. This results in a better sampling efficiency and reduces the time required to reach the equilibrium of the Markov chain, also known as burn-in phase. These improvements apply equally well, if not better, to the local scaling factors Λ by processing the unidimensional moves in parallel and thus offering a nearly ideal speed-up of this costly operation (see Algorithm 1 in the  supplementary materials  for an outline of the main steps taking place during an iteration of the presented method). 2.4 Assessing the framework performance In a first phase, we compare, using simple models, multiple strategies to highlight the contributions of each enhancement that are at the core of our new framework. Two strategies using non-adaptive random walk M-H with pre-fetching method are used as reference: the  PF  method emulates user-defined proposal windows by using relevant but sub-optimal size of the proposals, while  OPPF  uses  optimal  proposal windows that are scaled based on  Eq. (5)  and guidelines from  Roberts and Rosenthal (2001 ) to target the optimal acceptance rate  α ⋆ . These two methods are compared with variants of our new framework based on pre-fetching and adaptive proposal scaled locally and globally. The three variants  STD ,  MIXED  and  PCA  correspond to the three proposals described by  Eqs. (4) ,  (2)  and  (3) , respectively. These experiments and their results are detailed and discussed in the  supplementary materials . In a second phase, we challenge our best strategy  PCA  on a macro-evolutionary model ( Silvestro  et al. , 2014a ) on a large empiric dataset. Then, our framework is compared with the widely used  MrBayes  software ( Ronquist  et al. , 2012 ) on a codon-substitution model with fixed tree topology as well as on the task of inferring trees with a nucleotide model. The complexity of these models and the fact that  MrBayes  already incorporate a form of adaptive proposals offer a more instructive benchmark (all measures were made using  MrBayes  3.2.5 compiled with SSE support and MPI enabled). Furthermore, while  MrBayes  was chosen as a reference, the performance gains measured in this benchmark result from the presented enhancements of the MCMC method and thus should apply similarly to any alternative software independently of the efficiency of the likelihood computations. 2.4.1 Performance measures We measured the efficiency in two ways. We first estimated the sample size (ESS;  Liu, 2008 ) that gives an indication of the potential number of usable samples. This number is defined as the ratio between the total number of samples and their integrated auto-correlation time (ACT). Then, in order to properly assess the performance of our framework on the inference of trees, we measured the convergence rate of the phylogenetic tree distribution using the average standard deviation of split frequency (ASDSF;  Lakner  et al. , 2008 ). The overall performance of the framework is measured using the speedup  S  defined as
 (6) S = g ¯ ( n p , M ) t ¯ ( n p , M ) ÷ g ¯ ( 1 , M R ) t ¯ ( 1 , M R ) 
with the  g ¯ ( n p , M )  being the averaged measures (i.e. ACT or ASDSF),  t ¯ ( n p , M )  the averaged run time for  n p  processors and the method  M . We mainly compare the measures of each method  M  to the reference method  M R  with 1 processor. The speedup indicates the gain provided by the increase in mixing or rate of convergence in addition of the loss caused by the additional time spent during the adaptive phase plus the overhead of the communications between processors. 2.4.2 Hierarchical bayesian model The model  PyRate  ( Silvestro  et al. , 2014a ) is used to illustrate the actual performance of our methods on simulated and real datasets. This hierarchical Bayesian model analyses speciation and extinction rates of large collections of fossils and estimates large numbers of parameters including the preservation rate, the time of speciation and extinction of each species and the speciation and extinction rates with their variation trough time. This model has a complexity order of  O ( N )  and only few parameters are correlated. These properties are particularly interesting because the relatively inexpensive likelihood will highlight the overhead of the framework, while the low amount of correlations will illustrate its ability to exploit correlations. Our framework was challenged on a large dataset of plant fossils ( Silvestro  et al. , 2015 ). This dataset contains 22 415 fossil occurrences assigned to 443 plant genera. It spans over a hundred millions of years divided in 31 predefined epochs, which were defined by the stratigraphic geological time scale. 2.4.3 Codon-substitution model We use the well-known model of codon substitutions  M2a , which was developed to identify positive selection on protein coding genes, to illustrate the mixing performance of our framework on methods for molecular evolution ( Yang  et al. , 2000 ). Given a fixed phylogenetic tree and a set of protein coding sequences, the  M2a  model estimates the selective pressure on codon sites through the use of a mixture of three site-classes having different synonymous to non-synonymous substitution rates  ω : purifying selection ( ω 0 &lt; 0 ), neutral evolution ( ω 1 = 1 ) and positive selection ( ω 2 &gt; 1 ). Beside the estimation of the parameter values for each site-class  ω  and their respective proportions, this model estimates the overall transition-transversion rate and the branch lengths of the tree. The computational cost of the  M2a  model exceeds significantly the cost of the  PyRate  model since it requires matrix exponentiations and matrix-matrix operations (both having a complexity order of  O ( N 3 ) ) that depends on the alignment length and the tree size. It thus offers a completely different sampling challenge than the one of  PyRate . The performance of our framework was tested by computer simulations. We used two simulated datasets created using  INDELible  ( Fletcher and Yang, 2009 ). These datasets represented alignments formed of 100 codons simulated under mild purifying selection ( ω = 0.8 ) on phylogenetic trees having 16 taxa ( Dataset 1 ) and 32 taxa ( Dataset 2 ). We took particular care to ensure an informative comparison of our implementation with  MrBayes . The simulated phylogenetic tree was used as a fixed tree topology and the MC 3  method was disabled. Under this circumstance, our implementations and  MrBayes  sampled the same set of parameters and both used adaptive proposals. 2.4.4 Estimating phylogenetic trees distribution As a final illustration, we challenged our framework with the estimation of the posterior distributions of phylogenetic trees with their branch lengths and parameters of a general time reversible (GTR) model of nucleotide substitution ( Tavaré, 1986 ). This substitution model presents a lesser computational challenge than the codon model  M2a . However, the major difficulty resides in properly sampling the space of potential phylogenetic trees. This tree space grows exponentially with the number of taxa and can only be explored with specific tree proposals ( Felsenstein, 2004 ). For that matter, we implemented two widely used tree proposals: the Stochastic Nearest Neighbor Interchange (stNNI) and the Extending Subtree Pruning and Regrafting (eSPR) in our framework. We compared our framework performance to  MrBayes  under two different settings. In the first setting, later referred as  Ref MrBayes , only the stNNI and eSPR tree proposals were enabled to mimic our own implementation. In the second setting, later referred as  Full MrBayes , the default configuration of  MrBayes  was used. All the aforementioned settings (including our framework) were compared with and without the MC 3  method on two empirical DNA datasets used in  Lakner  et al.  (2008)  and available in TreeBASE ( http://www.treebase.org ). The first one,  M2017  (legacy ID M336) has 27 taxa and 1949 sites, while the second dataset,  M2152  (legacy ID  M520 ) has 67 taxa and 1098 sites. Finally, to give an insight of our method potential on larger dataset, we used a DNA empirical dataset published by  Pyron and Wiens (2011)  representing over 2800 species of amphibians with more than 10 000 sites. 3 Results and discussion 3.1 Performance gain on  PyRate  model We challenged our new framework by analysing a large dataset of plant fossils ( Silvestro  et al. , 2015 ) with  PyRate . The complex and heterogeneous parameter space of the empirical model highlighted the full potential of our new framework. Adaptive methods revealed speedups between 2 to over 35 times when compared to the non-adaptive  OPPF  method depending on the number of processors used ( Fig. 2a ).  PCA  surpassed the other adaptive methods by exploiting the small amount of correlation that existed in the model. Fig. 2 Result for  PyRate  model on plant fossils data. The figure on the left shows the speedup of each method using  OPPF  with 1 processor as the reference. The right figure shows the parallel scalability of the framework when compared or combined to a parallel computation of the  PyRate  likelihood. Settings for these simulations were  d ≈ 1000 ,   8 × 10 6  iterations and 3 runs In this application, the number of free parameters to estimate was approximately 1000, which makes it almost impossible for a user to guess a sensible set of proposal distribution for this real dataset. Indeed, the mean variance of the observed chain for species birth and death times ranged from 1 to 425 with average value of 138 and standard deviation of 110. Using the average variance to define the proposal distribution would result in some parameters being nearly never sampled. Actually moves configured with the observed average variance would be rejected almost all the times when applied to parameters with variances one hundred-fold smaller. We therefore fixed an arbitrary size for proposal distribution of  ≈ 5  for  OPPF , which allowed each parameter to be sampled. To illustrate the gains of the adaptive methods, let us consider the case where we would like to sample this model with an average ESS of 500 per parameter. The runtime of such a task can be estimated by adding the time  T BI  spent in the burn-in to the estimated sampling time  T S . The former is directly measurable, while the latter can be estimated based on the desired number  N ESS  of ESS and the average observed time  T ESS  required to produce one ESS. Therefore, the time to produce an average of 500 ESS using the  PF  method with one processor would be given by
 T P F = T B I + T S = T B I + N E S S · T E S S = 1600 + 500 · 47 = 2.35 · 10 4   seconds   
(i.e. 7 h). Switching to the  PCA  method, it would take only two and a half hours to reach the same level of ESS on one processor ( T B I = 2670 , S = 12 ) and the time would reduce to 17 minutes if 16 processors ( T B I = 230 , S = 1.6 ) would be used. Beside the differences in computation time, the  OPPF  method would further produce a highly uneven sampling of all parameters due to the suboptimal proposal distributions. This was illustrated by the variation of ACT over all parameters. We observed an average ACT of 740 with standard deviation of 877 for the  PF  method. The  PCA  method produced in contrast a far more reliable quality of sampling and its ACT variation reached only 193 ±273 for 1 processor and 219 ±177 for 16 processors. 3.1.1 Comparison with parallel likelihood We measured the raw parallel performance gain of our framework when confronted or combined to a parallel computation of the  PyRate  likelihood. We thus compared the performance of the  PCA  adaptive proposal combined with  a)  a parallel likelihood,  b)  pre-fetching (our framework) and  c)  both methods. Our framework performance (variant  b ) clearly surpassed the parallel likelihood one. It showed a nearly linear speedup up to 8 processors and notable gains up to 48 processors, while the parallel likelihood (variant  a ) showed a nearly linear speedup up to 4–6 processors before reaching a plateau with no gain in performance ( Fig. 2b ). The combination of both methods (variant  c ) showed the potential of coupling model dependent improvements with our framework by greatly increasing the parallel scalability and reaching speedups of  ≈ 40  for 64 processors and  ≈ 60  for 128 processors. For this measures, we used the  PCA  proposal as the reference method in order to measure the parallel scalability without being biased by the gain induced by our adaptive proposals. Thus the speedup was measured according to  Eq. (6)  with  M R = PCA  using the  PyRate  model on the plant fossil dataset. 3.2 Performance gain on phylogenetic models 3.2.1 Mixing on codon-substitution models We assessed the performance of our framework on the  M2a  model of codon substitutions with simulated data. As a reference, we compared a sequential execution of  STD  with one parameter per block against the adaptive MCMC based algorithm implemented in  MrBayes . The average ESS obtained were comparable, yet slightly better for our  STD  variant ( Fig. 3a and c ). We then compared our best method,  PCA , with 12 parameters per block and showed an improvement in sampling performances. Indeed, the increase of average ESS when compared to  MrBayes  went from more than a twofold factor on a sequential execution up to a twentyfold factor with 32 processors. Fig. 3 Result for the ESS and ESS speedup comparison with MrBayes on the  M2a  codon substitution model with simulated data. ( a ) and ( c ) The average ESS per parameter on the first, respectively second, datasets for MrBayes, the  STD  method using single parameter moves and the  PCA  method with multiples parameters per block. ( c ) and ( d ) The ESS speedup for the same methods on the same datasets using an hypothetical MrBayes implementation as reference. Simulations were run for  2 · 10 5  iterations While representing the mixing efficiency of these methods, the ESS does not take into account the added computational complexity brought by moving multiple parameters at once. Indeed, Bayesian software for phylogenetic inference such as  MrBayes  are taking advantage of the possibility to partially update the likelihood when one or few parameters are changed. The computational cost during an iteration is then linked to the amount of parameters updated. Therefore, using the ESS speedup defined by  Eq. 6  is more appropriate given that it encompasses these increases in computational cost by normalizing the ESS by the execution time. However, since our implementation outperformed  MrBayes  on this model (see supporting materials), we had to define as reference,  M R , an  Hypothetical MrBayes  by defining the average reference ESS,  g ¯ ( 1 , M R ) , as the one of  MrBayes  and the average runtime,  t ¯ ( 1 , M R ) , as the one of our  STD  implementation. The reference thus represents the effective sample size of  MrBayes  normalized by the runtime of our method with settings closest to  MrBayes  (i.e. updating only one parameter per iteration). The obtained ESS speedups ( Fig. 3b and d ) show that our implementation were still outperforming the hypothetical reference method from close to a twofold factor on a sequential execution and up to approximately a tenfold factor with 32 processors. However, in the sequential case, the simple  STD  method was more efficient than the  PCA  variant. This is explained by considering the overhead of more costly partial likelihood computations and the added cost of learning parameters correlation. However, as soon as several processors were used, and thus our coupling of the adaptive proposals with the pre-fetching method was employed, the  PCA  variant was more efficient by proposing bolder moves and using the parallel resources to learn correlations more accurately and quickly. 3.2.2 Convergence on phylogenetic tree distributions Our framework was finally evaluated on the sampling of the posterior distributions of trees and parameters of the model of nucleotide substitution. At this task, our best method,  PCA  with 32 processors, was able ( Fig. 4 ) to converge towards the posterior distribution up to 10-20 times faster than its equivalent  Ref MrBayes . When compared to the default settings of  MrBayes  ( Full MrBayes ) with more advanced tree proposals, our method still showed faster convergence rate as soon as 4–8 processors were used. Against this  MrBayes  setting, the convergence speedup reached approximately a fourfold factor with 32 processors. Fig. 4 Results for the ASDSF convergence speedup comparison with MrBayes when estimating the posterior distribution of the phylogenetic tree and the nucleotide substitution parameters on empiric datasets (TreeBASE M2017 and M2152). ( a ) and ( c ) The ASDSF speedup the first, respectively second, datasets for two settings of MrBayes and our  PCA  method with multiples parameters per block. ( c ) and ( d ) illustrate the ASDSF speedup on the same datasets when the previous methods are augmented with  PCA 
 Figure 4a and c  illustrate the speedup in ASDSF convergence on datasets M2017 and M2152, respectively, using  Ref MrBayes  setting without MC 3  as reference. Indeed, the  Ref MrBayes  setting showed similar execution time and convergence performance as our  STD  variant with updates on one parameter per iteration (see appendix).  PCA  slightly outperformed  Ref MrBayes  in the sequential case but was still far from the performance offered by  Full MrBayes . However as the number of processors used increased, the performance of  PCA  exceeded both  MrBayes  settings. To highlight the versatility of our framework, we applied the same experiment but with MC 3  enabled on our framework and  MrBayes .  Figure 4b and d  show the speedup in ASDSF convergence on both datasets with four parallel tempered chains. The trend shown in this second experiment is consistent with our previous results and the speedup of  PCA  increased with the number of processors used and surpassed the performance of  Ref MrBayes  and  Full MrBayes . The improvement on convergence rates brought by our framework on this problem is linked with the difficulty of moving through the phylogenetic tree space. Tree proposals are usually suffering from a low acceptance rate and are thus frequently rejected. Indeed, in our experiment, the acceptance rates for tree proposals were lower than 0.05. Such moves are not currently adaptive, but they remain easily predictable and therefore exploitable by the pre-fetching method. While this assumption of low acceptance rate on tree proposals is true for most datasets, it might falter on some of them. To get around this limitation, tree proposals could be made adaptive by tuning the extension probability (e.g. for eSPR) or the proposals on branch lengths that are jointly applied to tree modifications. Our results suggest that the approach that we propose has large potentials on more challenging datasets, but the datasets used in previous experiments were easily analyzable using existing software. We therefore used a large dataset representing more than 2800 species with 12 genes of the amphibian family ( Pyron and Wiens, 2011 ) to challenge both  Full MrBayes  and  PCA . For this experiment, we compared four separate runs of  Full MrBayes  using each four tempered chains with four separate runs of our best method,  PCA , with 4 tempered chains having each 32 processors dedicated to our coupling of adaptive MCMC and pre-fetching. While  Full MrBayes  took more than five days to approach a likelihood plateau ( Fig. 5 ),  PCA  reached it in less than one day. It is worthwhile to mention that if  PCA  would be augmented with the advanced tree proposals present in  Full MrBayes , further performance gains could be observed. Fig. 5 Trace of the log likelihood when estimating the posterior distribution of the phylogenetic tree, branch lengths and nucleotide substitution parameters on an empiric dataset having 2800 species with 12 genes of the amphibian familiy. Average trace and 95% confidence interval are shown for 4 independent runs with of 4 parallel tempered chains of Full MrBayes and PCA with 32 processors 4 Conclusion Building an efficient MCMC sampler for parameter-rich and complex models is challenging given their inherent expensive computational cost and high parameter-dimension  d . Indeed, most of the existing MCMC enhancements may prove inefficient due to their increase in computational effort caused by additional likelihood evaluations or complex operations depending on  d  ( O ( d x ) , x &gt; 1 ) (e.g.  Haario  et al. , 2005 ). We presented a M-H framework that overcomes this difficulties by using the coupling of a novel adaptive proposal and pre-fetching. We showed that our new framework improved the mixing, reduced the burn-in phase and speed up the sampling by an order of magnitude when applied to several models of varying complexity including the difficult task of estimating the posterior probability of phylogenetic trees using evolutionary models. Model dependent enhancements can be used to further improve the performance gain highlighted, as demonstrated by the addition of parallel likelihood computations ( Fig. 2b ) or MC 3  ( Fig. 4 ). Therefore existing evolutionary biology state-of-the-art software, such as  MrBayes  ( Ronquist  et al. , 2012 ) or ExaBayes ( Aberer  et al. , 2014 ), could directly benefit from this framework. Furthermore, since this method is based on speeding up a single M-H process, it could be used as the core of more advanced MCMC methods such as trans-dimensional MCMC ( Sisson, 2005 ) or even Bayes factors computation using thermodynamic integration ( Lartillot and Philippe, 2006 ). Further capabilities could be built on top of this framework by exploring new approaches such as adaptive tree proposals or an automatic clustering of parameters into optimal blocks. Therefore, beside broadening the range of evolutionary hypothesis tractable using Bayesian inference, our parallel M-H framework provides a potential basis for a new generation of efficient parallel MCMC samplers for parameter-rich models such as those that are currently developed in biological studies. Supplementary Material Supplementary Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Customizable views on semantically integrated networks for systems biology</Title>
    <Doi>10.1093/bioinformatics/btr134</Doi>
    <Authors>Weile Jochen, Pocock Matthew, Cockell Simon J., Lord Phillip, Dewar James M., Holstein Eva−Maria, Wilkinson Darren, Lydall David, Hallinan Jennifer, Wipat Anil</Authors>
    <Abstract>Motivation: The rise of high-throughput technologies in the post-genomic era has led to the production of large amounts of biological data. Many of these datasets are freely available on the Internet. Making optimal use of these data is a significant challenge for bioinformaticians. Various strategies for integrating data have been proposed to address this challenge. One of the most promising approaches is the development of semantically rich integrated datasets. Although well suited to computational manipulation, such integrated datasets are typically too large and complex for easy visualization and interactive exploration.</Abstract>
    <Body>1 INTRODUCTION 1.1 Systems biology and integrative bioinformatics Systems biology considers multiple aspects of an organism's structure and function at the same time, using the plethora of data that is publicly available online. Biologists have access to heterogeneous data covering many different aspects of biology; 1230 entries are listed in the 2010 database issue of  Nucleic Acids Research  ( http://nar.oxfordjournals.org/content/38/suppl_1 ). For model organisms such as  Saccharomyces cerevisiae , the abundance of freely available data has long since exceeded the point at which it can be analysed manually. Most data sources still exist in isolation, each with its own specialization and focus ( Stein, 2002 ). In many cases, databases lack semantic links to each other, even when they are providing data about the same entities. This partitioning of knowledge is a particular problem for systems biology, hampering the examination of the emergent behaviours inherent in complex biological systems. The problem of using distributed, heterogeneous datasets is addressed by the subdiscipline of integrative bioinformatics. There are many different approaches to the integration and querying of large heterogeneous datasets. Early XML-based approaches ( Achard  et al. , 2001 ) were soon replaced by more sophisticated methods. Federated approaches, such as the distributed annotation system (DAS) ( Prlić  et al. , 2007 ) establish a central view on the data by translating each internal query into a set of external queries in order to retrieve the required data from the outside sources on the fly ( Heimbigner and McLeod, 1985 ). Semantic web approaches, as endorsed by the Open Biological and Biomedical Ontologies foundry (OBO) ( Smith  et al. , 2007 ) or as implemented in YeastHub ( Cheung  et al. , 2005 ), establish a network of interlinked ontologies using a set of controlled vocabularies ( Brinkley  et al. , 2006 ). Data warehousing approaches such as BioMART ( Haider  et al. , 2009 ) EcoCyc ( Keseler  et al. , 2010 ) and SAMBA ( Tanay  et al. , 2004 ), convert and incorporate data from their external sources into their own database schema and provide custom queries. 1.2 Ondex Ondex is a graph-based data integration framework ( Kohler  et al. , 2006 ), which takes a semantic warehousing approach. Links between entities in different datasets may be directly extracted from the available data sources, inferred as part of the integration, or generated by external tools, such as BLAST ( Altschul  et al. , 1990 ). These data can subsequently be matched and interlinked with each other semantically. The result is a semantically enriched dataset, with which users can interact and also visualize. Ondex incorporates data into a network of entities termed ‘concepts’, connected by ‘relations’, all of which can carry ‘attributes’. All concepts, relations and attributes have ‘types’, which are organized in a hierarchical fashion. For example, the concept type  Protein  is a subtype of  Molecule , which is itself a subtype of  Thing . This hierarchy means that every  Protein  concept is also a  Molecule  and a  Thing . Similarly, the relation type  catalyzes  is a subtype of  actively_participates_in . Therefore, every statement that  p :: Protein   catalyzes r :: Reaction  means that  p actively_participates_in r . Adding this type of information means that the computer stores not only data, but also its meaning, providing a ‘semantic representation’ of the data. A special kind of attribute that all concepts in the an Ondex graph have is the cross-reference (called ‘concept accession’ in Ondex). Concept accessions make it easy to connect concepts originating from different sources. For example, if an Ondex dataset contains two  Gene  concepts which have been imported from two different data sources, but which share the same concept accession, Ondex can connect these concepts with a new relation of type  same_as , and can at a later time merge these concepts into a single concept. All concepts and relations also have attached provenance information stating their origin and any associated evidence codes. The process of integrating data into this data structure is performed by the Ondex workflow engine, which employs a plugin architecture. Parsers, mapping methods and other plugins can be developed using an open API ( Taubert  et al. , 2007 ) and can subsequently be used in workflows. In this work, we describe a mechanism to collapse groups of concepts into a simpler, more easily visualized and conceptualized representation. We have implemented this mechanism as a plugin for Cytoscape, called OndexView, which facilitates the focused analysis of parts of a large, complex network. We demonstrate the value of this approach by applying it to an investigation into the systems biology of telomere maintenance in the baker's yeast  S. cerevisiae . 1.3 Telomere maintenance and  BMH 1/2 Telomeres are structures composed of the ends of eukaryotic chromosomes together with their capping nucleoprotein complexes. These structures appear to play a major role in the ageing process ( Blasco, 2007 ;  Cheung and Deng, 2008 ). Without the capping proteins, chromosome ends appear as double-stranded breaks to the cell's DNA damage detection mechanism, triggering a checkpoint response and ultimately cell cycle arrest ( Longhese, 2008 ;  Sandell and Zakian, 1993 ). With each cell division, telomeres shorten ( Longhese, 2008 ). When telomere length falls under a certain threshold, the checkpoint response is triggered. This mechanism appears to contribute to the establishment of a cell's finite lifespan. Saccharomyces cerevisiae  is an excellent subject for studying telomere biology, because it is one of the simplest and most well-studied eukaryotic model organisms, and telomere biology is highly conserved across eukaryotes. In  S. cerevisiae , an important component of the telomere maintenance mechanism is the protein Cdc13, which binds to the single-stranded DNA overhangs of telomeres. Cdc13 has two major functions: capping the telomeric DNA and recruiting telomerase ( Garvik  et al. , 1995 ;  Nugent  et al. , 1996 ), which is part of the telomere repair mechanism. CDC13  is an essential gene, so it is difficult to characterize using knock-out mutants. However, there is a temperature-sensitive mutant called  cdc13-1 . This mutant has a wild-type phenotype below 26 ○ C, but above this temperature telomeres become uncapped, the checkpoint response is induced and the cells stop dividing ( Weinert and Hartwell, 1993 ). Epistatic interactions between  cdc13-1  and every non-essential gene of the  S. cerevisiae  genome have been studied ( Addinall  et al. , 2008 ) using a high-throughput synthetic genetic array (SGA) assay ( Tong  et al. , 2001 ). Addinall and co-workers identified a large number of genes which appear to genetically interact with  cdc13-1 . The biological function of many of these genes is already understood, but the role of others requires further investigation. A particularly challenging problem is the phenotype of the two paralogues  BMH1  and  BMH2 . The deletion of the gene  BMH1  strongly suppresses the  cdc13-1  phenotype, while the deletion of its paralogue  BMH2  suppresses  cdc13-1  rather weakly.  BMH1  and  BMH2  share 91.6% sequence identity. They both encode 14-3-3 proteins, a class of proteins which usually occur as dimers and which bind to phosphoproteins ( Chaudhri, 2003 ). Members of the 14-3-3 family have a variety of different functions in eukaryotes, including directly modifying the functionality of their target proteins, mediating and controlling transport processes between the cytoplasm and different organelles and serving as scaffolds for interactions between different proteins ( Tzivion  et al. , 2001 ). An important role for  BMH1  and  BMH2  in the regulation of carbohydrate metabolism has also been suggested ( Bruckmann  et al. , 2007 ). It is not immediately obvious why  BMH1  and  BMH2  behave differently despite their close homology. Clearly, the difference in phenotype between two such closely homologous genes cannot be understood by studying the genes in isolation; a systems biology approach is essential. In order to investigate this problem, we used Ondex to integrate five publicly available data sources as well as a homology dataset generated from BLAST results. We then enriched this data with semantic links between the concepts from the different data sources. In addition, we developed a novel, view-based visualization approach which we used to analyse this large, complex dataset. This analysis produced several testable hypotheses. 2 METHODS 2.1 Data sources We integrated six different data sources using Ondex. Genomics data, as well as the latest Gene Ontology ( Ashburner  et al. , 2000 ) annotations, were obtained from the  Saccharomyces  Genome Database (SGD) ( Cherry  et al. , 1997 ). A yeast regulatory network was acquired from  Balaji  et al.  (2006 ). A curated model of the yeast metabolic network was sourced from  Herrgard  et al.  (2008 ). A yeast protein–protein interaction (PPI) network and a network of known genetic interactions (GI) were taken from the BioGRID database ( Stark  et al. , 2006 ). The BioGRID data includes the GIs reported by  Addinall  et al.  (2008 ). Homology links between genes were created using BLAST ( Altschul  et al. , 1990 ) with an 85% sequence identity threshold ( Table 1 ). Further information regarding the data sources can be found in the  Supplementary Material .
 Table 1. Data sources used in this work Data Source Version/date Genome SGD 11/02/2010 GO annotations SGD 11/02/2010 Interactome BioGRID v2.0.61 Regulatory network Balaji  et al.  (2006 ) NA Metabolic network Herrgard  et al.  (2008 ) v1.0 Homology BLAST (id. &gt;85%) NA NA = not applicable. 
 2.2 Integration The source datasets were integrated to form a combined Ondex  Saccharomyces  knowledge network. A metadata model was designed for the knowledge network to capture the semantics of concept and relationship types found within the different datasets ( Fig. 1 ). This model provides information about how certain concept types inherit features from one another (for example, an  Enzyme  is always a  Protein ), and allows such concepts to be treated as both concept types in the downstream workflow.
 Fig. 1. Simplified schematic of the data structure underlying the Ondex  Saccharomyces  knowledge network. Circles represent concept types, and lines represent relations between them. Ondex parsers for each data source produce an Ondex-compatible representation of the data. For example, the BioGRID parser creates an appropriately typed concept for every reported gene, protein and RNA in the database, and an interaction concept of a corresponding type for every interaction. All genes, proteins and RNAs that take part in an interaction are linked to that interaction concept with a  participates_in  relation. A mapping algorithm based on matching cross-references was used to find identical concepts in the Ondex graph. For example, the protein encoded by the ORF  YDL220C  from BioGRID matches a concept with the same accession number in the metabolic network. When the procedure identifies a group of matching concepts, it merges them into one single concept that captures all of the information that was represented by the group members. The procedure checks whether all concepts in a group have compatible types. If so, it automatically uses the most specific type present in the group for the merged concept. Otherwise it reports the inconsistency. For example, a concept cannot be both a  Gene  and a  Pseudogene , but a concept can be both a  Protein  and an  Enzyme , in which case  Enzyme  is the more specific type. The semantic model reflects current understanding of molecular cell biology: concepts of type  Gene  have  transcription  relations linking them to  mRNA  concepts, which in turn connect to  Polypeptide  concepts via  translation  relations.  Polypeptides  can be  part_of   Proteins .  Enzymes , a subtype of  Proteins , may  catalyze   Reactions , which  consume  and  produce   Molecules  such as  Metabolites  and  Proteins . Transcription_factors , a subtype of  Protein , connect to  Genes  via  regulation  relations.  Genes  in turn are  Nucleotide_features , which connect via  adjacency  or  overlap  relations, establishing genomic context. All  Nucleotide_features  can  participate_in  various  Genetic_interactions . Similarly,  Molecules , such as  RNAs  or  Polypeptides  can  participate_in   Physical_interactions . Non-physical data can also be associated with existing concepts.  Nucleotide_features  can contribute to  Biological_processes .  Proteins  can have a  Molecular_function . Various parts of the knowledge network can also be associated with  Publication  concepts. All of the parsers and mapping methods, as well as the metadata that were created for this work, are available as part of the Ondex suite, which is licensed under the GNU GPL v3, and is downloadable from  http://www.ondex.org/ . Further details regarding the integration process can be found in the  Supplementary Material . 2.3 A novel visualization strategy The graph produced by the integration is stored in the Ondex XML format OXL ( Taubert  et al. , 2007 ) and can be browsed in Ondex. The resulting network is large and complex, and contains many types of concepts and relations. In order to present a more succinct and biologically focused view of the data, we developed a plugin for the network visualization tool Cytoscape ( Shannon  et al. , 2003 ) called OndexView. This new visualisation allows the user to define ‘views’ on the data. A view focusses on one type of concept. All concepts of that type found in the knowledge network are visualized as network nodes. The user can query the knowledge network for associations between the concepts. These associations will be visualized as edges between the corresponding nodes. Querying for associations is accomplished by specifying a set of metadata motifs. The instances of these motifs found in the underlying knowledge network are then used to create corresponding associations in the view. Metadata motifs are alternating sequences of concept types and relation types that begin and end on the same concept type ( Fig. 2 E). A modified depth-first search algorithm is used to extract motif instances from the underlying Ondex knowledge network. At each depth level, the algorithm checks whether the currently explored path matches the target motif. For example, a user interested in proteins and metabolic pathway relationships between them could specify a motif representing this type of interaction. Applying the algorithm using this motif produces a view on the graph, containing only those elements in which the user is interested ( Fig. 2 ). It is possible to combine multiple motifs to form a view as long as the motifs share the same start point concept type.
 Fig. 2. Schema illustrating two examples for constructing associations based on metadata motifs. ( A ) The motif shown selects all paths from a polypeptide that is part of a transcription factor which regulates a gene that is transcribed to an mRNA which is translated into another polypeptide. ( B ) The motif from (A) is shown in context of the complete metadata structure. ( C ) A small example subnetwork to which the motif can be applied. Finding the motif (A) in the subnetwork, we identify three matching paths. Each element in these paths matches its corresponding element in the motif. ( D ) The view resulting from collapsing the paths identified in (C) according to motifs from (A) contains three edges; one for each matching path. ( E ) The motif selects all paths from a protein that modifies a process that produces a metabolite which is then consumed by another process back to another protein that modifies that process. Such a motif could be described as ‘precedence in a metabolic pathway’. ( F ) The motif from (E) is shown in context of the complete metadata structure. ( G ) A small example subnetwork to which the motif can be applied. Finding the motif (E) in the subnetwork, we identify two matching paths. Each element in these paths either matches or is a subtype of the corresponding element in the motif. ( H ) The views resulting from collapsing the paths identified in (G) according to motifs from (E) contains two edges; one for each matching path. 2.4 Using OndexView with the  Saccharomyces  knowledge network for generating hypotheses A semantically collapsed view of a complex network facilitates hypothesis generation by providing a simple representation of genes of interest and their interactions. A subnetwork consisting only of these genes and their neighbours can be generated, and inspected to identify edges of potential interest. Examination of the annotations attached to these edges and their adjacent nodes in the Ondex network provides links to available knowledge about the underlying biology. The semantically collapsed view also provides a way of perusing relevant literature in a focused manner, making hypothesis generation considerably more efficient than the alternative of trawling through large databases of publications. Using OndexView, we defined five motifs over  Gene  concepts covering genetic and physical interactions, homology, regulation and metabolic precedence ( Table 2 ).
 Table 2. Motif definitions used in this work Association name Motif homologue Gene is_homologue Gene metabolic path Gene encodes Polypeptide is_part_of Protein participates_actively_in Process gives Metabolite taken_by Process has_active_participant Protein has_part Polypeptide encoded_by Gene ph. interaction Gene encodes Polypeptide participates_actively_in Physical_Interaction has_passive_participant Polypeptide encoded_by Gene regulation Gene encodes Polypeptide is_part_of Transcription_Factor regulates Gene gen. interaction Gene participates_actively_in Genetic_Interaction has_passive_participant Gene 
 We created a view using all five motifs, querying for the immediate neighbourhood of  BMH1 ,  BMH2  and  CDC13 . We then laid out the view, separating all neighbouring genes into groups: Exclusive neighbours of  BMH1 , exclusive neighbours of  BMH2 , joint neighbours of  BMH1  and  BMH2  and others. 3 RESULTS AND DISCUSSION The work described here has four major outcomes: a knowledge network for the yeast  S. cerevisiae ; an algorithm for semantic collapsing of a network; a Cytoscape plugin for visualization of the knowledge network implementing this algorithm; and a set of hypotheses relating to telomere maintenance in  S. cerevisiae , generated using this approach. 3.1 The Ondex  Saccharomyces  knowledge network Using the Ondex data integration platform, we integrated five publicly available data sources covering various aspects of  S.cerevisiae  biology into a semantically enriched and interlinked knowledge network. The resulting knowledge network consists of a total of 240 964 concepts of 59 different types, connected by 754 391 relations of 28 types. 3.2 OndexView: a Cytoscape plugin We implemented a new method for semantic simplification as the Cytoscape plugin OndexView, that uses views on the underlying data. Users can load integrated Ondex networks into OndexView and open views on them. A user can invoke predefined views over the knowledge network or define custom views. A built-in motif editor is included in OndexView for this purpose. To apply a view, a concept type is selected from the knowledge network. The user then chooses one or more motifs for the selected concept type. OndexView extracts all the required data from the underlying Ondex graph and constructs associations according to the algorithm outlined in  Section 2.3 . The program then offers a query interface to the user, which can be used to focus on specific nodes (such as genes) or collections of nodes (such as pathways and complexes) and their neighbourhoods. Once the query has been processed, OndexView displays the selected view in the main Cytoscape window. 3.3 Telomere maintenance in  S. cerevisiae BMH1  and  BMH2 , despite their 91.6% sequence identity, have very different interactions with the temperature-sensitive telomere uncapping mutant  cdc13-1 . Our analysis of the Ondex  Saccharomyces  knowledge network, conducted using OndexView, led to the formulation of several hypotheses to explain the different interaction profiles of  BMH1  and  BMH2 . From the semantically collapsed graph, it is apparent that the neighbourhoods of  BMH1  and  BMH2  have only 23.5% of genes in common;  BMH1  and  BMH2  have completely separate sets of transcriptional regulators. This information is not readily apparent in the uncollapsed graph. Analysis of the node description fields reveals that 19.3% of the two genes' combined neighbourhood are cell cycle-related genes, while 10.1% are glucose metabolism related genes. Of the genes that physically interact with both  BMH1  and  BMH2 , 14.3% are histone related ( Table 3 ). An annotated screenshot of this view is available as  Supplementary Figure S2 .
 Table 3. Gene groups in the neighbourhood of  BMH1  and  BMH2  in the created view and their cardinalities Total Cell cycle related No. (%) Glucose metabolism related No. (%) Histone related No. (%) Neighb. of  BMH1 68 15 (22.1) 7 (10.3) 4 (5.9) Neighb. of  BMH2 79 15 (18.9) 8 (10.1) 4 (5.1) Intersection of  BMH1 / 2  neighbourhoods 28 7 (25.0) 3 (10.7) 4 (14.3) Union of  BMH1 / 2  neighbourhoods 119 23 (19.3) 12 (10.1) 4 (3.3) 
 The over-representation of genes involved in regulation of the cell cycle 1  in the joint neighbourhood of  BMH1  and  BMH2  suggests that  BMH1  and  BMH2  may function as cell cycle regulators. Notably, Rad53, a key element of the cell's checkpoint signalling pathway, interacts physically with Bmh2. Further, two genes that have previously been identified as suppressors of  cdc13-1 :  BNR1  and  CYK3 , are also present in the neighbourhood of  BMH1  and  BMH2 . The joint neighbourhood of  BMH1  and  BMH2  also contains a relatively large number of genes related to regulation of glucose metabolism, 2  indicating that the pair of genes may also play a major role in the regulation of glucose metabolism ( Bruckmann  et al. , 2007 ). 3.4 Differential regulation of  BMH1  and  BMH2 The semantically collapsed view of the neighbourhood of  BMH1  and  BMH2  ( Section 2.4 ) shows that Bmh2 physically interacts with the protein Rad53. After selecting Rad53 in the view, we can learn from its description field that Rad53 mediates the activation of the cell-cycle checkpoint ( Schwartz  et al. , 2002 ), thus interacting with the  cdc13-1  phenotype. Examination of the interaction between Bmh2 and Rad53 reveals an annotation indicating that this interaction was originally reported by  Usui and Petrini (2007 ). These authors showed that both Bmh1 and Bmh2 directly bind to the active (phosphorylated) Rad53 protein, thus enhancing its signalling effect. An edge between  BMH1  and  RAD53  is absent in the Ondex network, since Usui and Petrini experimentally verified only the physical interaction between Bmh2 and Rad53. However, they hypothesise that Bmh1 interacts with Rad53 in the same way that Bmh2 does. Accepting Usui and Petrini's assumption, a significant difference in protein abundance during the time of checkpoint induction could explain the different effects of  BMH1  and  BMH2  deletions. The collapsed network neighbourhood of  BMH1  and  BMH2  shows clearly that the two genes are regulated completely independently from one another. Furthermore, according to their description fields in the network, two of  BMH1 's transcriptional regulators,  ACE2  and  SWI4 , are known to be active during G 1  phase ( Andrews and Moore, 1992 ;  McBride  et al. , 1999 ). This observation leads us to conjecture that  BMH1  could be more strongly expressed than  BMH2  during the G 1  phase, resulting in a higher abundance of Bmh1 protein during and after G 1  phase. We hypothesize that due to its independent transcriptional regulation,  BMH1 's products are present at higher levels than those of  BMH2  at checkpoint time. Thus, deleting  BMH1  would remove the majority of the Bmh1 and Bmh2 protein available as binding partners for Rad53, severely impairing the cell's checkpoint response in the face of uncapped telomeres. Removing  BMH2 , in contrast, removes a smaller proportion of binding partners for Rad53, thus resulting in a milder suppression effect on  cdc13-1 . The differential regulation hypothesis has been summarized in  Figure 3 A, by generating a neighbourhood graph of  BMH1  and  BMH2  and removing all nodes except for those mentioned in the above discussion.
 Fig. 3. Screenshots from OndexView, summarizing the hypotheses presented. Edges represent physical interactions (blue), homology (red), regulation (green) and epistasis (yellow). Nodes represent genes, some of which are weak (light red) and some strong (dark red) suppressors of  cdc13-1 . Genes which show lethal or slow growing phenotypes upon deletion are marked with dashed outlines, as knowledge of their epistatic behaviour can be expected to be incomplete. Experimental validation of the differential regulation hypothesis could be performed in several ways. Expression profiling of  BMH1  and  BMH2  over the course of the cell cycle could be performed. Consultation of pre-existing data has so far been inconclusive: a time course microarray performed by Spellman and colleagues showed  BMH1  expression levels to be slightly higher than those of  BMH2  throughout the the cell cycle and spiking 4-fold during G 1  phase ( Spellman  et al. , 1998 ) ( Supplementary Fig. S2 ). However, due to the low resolution of the assay, the observed peak is represented by only one datapoint, rendering its significance rather doubtful. Another possibility would be the examination of an  ace2 Δ swi4 Δ double mutant, which under the above hypothesis should replicate the  bmh1 Δ phenotype regarding suppression of  cdc13-1 . 3.5 Phosphorylation of Cdc13 A different hypothesis was formed after further examining the OndexView network neighbourhood of  BMH1  and  BMH2 . The network shows a set of edges, indicating that Bmh1 and Bmh2 physically interact with Snf1, Snf4 and Gal83, which according to their node description fields, are members of the AMP-dependent kinase (AMPK) complex. Snf1 also interacts with Cdc13 and Stm1, another telomere-capping protein. The publication linked in the Ondex knowledge network reveals that this interaction is a phosphorylation, ( Ptacek  et al. , 2005 ). Publications linked from the nodes reveal that the AMPK complex is a heterotrimer composed of the α-subunit Snf1 (the actual kinase), its activating γ-subunit Snf4 and a third component β-subunit that tethers Snf1 and Snf4 together. Sip1, Sip2 and Gal83 compete for the place of this third component ( Jiang and Carlson, 1997 ). These three competing proteins have been shown to determine the AMPK's substrate specificity and its cellular localization ( Lin  et al. , 2003 ;  Schmidt and McCartney, 2000 ). For example, the Gal83 variant of the AMPK complex has been shown to be able to enter the nucleus ( Vincent  et al. , 2001 ). In the network, two edges exist indicating that Bmh1 and Bmh2 both physically interact with Snf1 ( Elbing  et al. , 2006 ). However, Bmh1 alone interacts with the γ-subunit Snf4 ( Gavin  et al. , 2002 ), while Bmh2 alone interacts with the β-subunit Gal83 ( Krogan  et al. , 2006 ). The nature of these interactions is currently unknown, but 14-3-3 proteins have been reported to affect kinases in several different ways, including scaffolding and direct alteration of the target's function ( Tzivion  et al. , 2001 ). If Bmh1/2 are involved in scaffolding for the formation of different AMPK variants, then the deletion of  BMH1  and the subsequent over-representation of Bmh2 dimers could favour the formation of Gal83-AMPK variants which can enter the nucleus to phosphorylate Cdc13. We, therefore, hypothesize that the phosphorylation of Cdc13 could potentially affect its temperature sensitivity and thus the  cdc13-1  phenotype. The phosphorylation hypothesis has been summarized in  Figure 3 B, by generating a neighbourhood graph of  BMH1  and  BMH2  and removing all nodes except for those mentioned in the above discussion. Experimental validation of the phosphorylation hypothesis is more difficult. An examination of the phenotype of  gal83 Δ mutants could offer further insight. If such a test would corroborate the importance of the AMPK complex for the observed phenotype, one could examine whether  in vitro  phosphorylation of Cdc13 by the AMPK is possible. However, without further evidence we have to consider the effect of a potential phosphorylation of Cdc13 on its temperature sensitivity in particular to be mere speculation. 3.6 Impact of noisy data Like any other data integration approach, the Ondex  Saccharomyces  knowledge network is limited by the state of knowledge contained in its data sources. There are two main types of errors that affect the system. (i) Incorrect information from the databases, such as curator errors, will also be present in the integrated network, unless detectable by Ondex's inconsistency checks as discussed in  Section 2.2 . For example, contradictions between data sources can be detected and rectified. (ii) Information that is missing from the source databases will also be missing in the integrated dataset. A particular problem in this respect is the lack of negative knowledge recording throughout the systems biology community. In many sources, it is not possible to decide if non-existence of a database entry is indicative of the subject being known not to exist or not being known to exist. It is obvious that such problems also impact downstream analyses with OndexView. However, by enabling the user to review the evidence underlying the integrated data, she/he can be pointed at the original publications that can be consulted. For example, as described in  Section 3.4 , the publication linked from the physical interaction edge between Bmh2 and Rad53 clarified the circumstances around the non-existence of an edge between Bmh1 and Rad53. So it can be argued that performing visual analyses with OndexView on the Ondex  Saccharomyces  knowledge network also impacts on missing data, as it allows for clarifications and corrections. 3.7 Comparison to related works While the Ondex  Saccharomyces  knowledge network in conjunction with OndexView's semantic simplification method shows parallels to previous data integration approaches, there are a number of important differences. Like the work presented in this article, semantic web approaches, such as YeastHub ( Cheung  et al. , 2005 ) store their integrated data in a machine-interpretable fashion, thus providing flexible platforms that can be adapted for various purposes. However, unlike OndexView, such systems require complex querying language constructs to access them. Data warehousing approaches such as SAMBA ( Tanay  et al. , 2004 ) and EcoCyc ( Keseler  et al. , 2010 ), on the other hand, are related to the presented work in that they collect the contents of heterogeneous data sources in one centralized repository. Unlike the Ondex  Saccharomyces  knowledge network, they offer web interfaces, which makes them very easy to query for standard use. However, they are based on less semantically rigorous data structures, rendering them less flexible. In summary, the Ondex  Saccharomyces  knowledge network combines aspects from both semantic web approaches and data warehousing approaches; featuring both their strengths, but also some of their weaknesses. 4 CONCLUSIONS The data produced by high-throughput approaches has the potential to revolutionize our understanding of biology, but can do so only if the computational techniques necessary to visualize and analyse the data can scale with the amount of data generated. Data integration methods have proven to be a successful way to face this challenge. We have shown that a view-based approach can be a powerful tool to simplify the visualization of the complex knowledge networks generated by semantic data integration, without loss of the underlying information. The view-based approach provides concise visualizations tailored to providing only the information relevant to a particular investigation. Biological phenomena such as the different phenotypes arising from deletion of  BMH1  and  BMH2  emerge from the interplay of several independent molecular biological networks. OndexView enables users to visualize not only these networks but also the ways in which they dovetail. This visualization serves as a starting point for the user, who can now easily explore genes, their various relationships and the underlying evidence, thus gathering inspirations facilitating the generation of testable hypotheses regarding the functions of these genes. 4.1 Outlook There are a number of improvements that could be applied to OndexView and the presented knowledge network, as well as a number of ideas that build upon this work. The exploration of evidence trails behind the simplified edges in OndexView could be made more easily accessible. Rather than showing evidence in a textual representation in graph attributes, an option to visualize it graphically on demand would further increase the tool's usefulness. However, due to the limitations of the Cytoscape API, which renders graph views immutable, such new features will more likely be included in a re-implementation of OndexView for Ondex's own graph visualization frontend. Furthermore, the Ondex  Saccharomyces  knowledge network could be enriched with probabilities on the connections between concepts, which could be inferred from the original data sources as well as the evidence coverage. Then, if more experience can be gathered on molecular pathways that potentially qualify for explaining observed epistasis effects, they could be generalized into a collection of semantic motifs. Instances of these motifs in the graph could be ranked according to their overall probability. The generation of such ranked lists could further assist biologists with the formation of hypotheses. 
          Supplementary Material 
          
             
                Supplementary Data 
             
             
             
          
       </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>A likelihood ratio-based method to predict exact pedigrees for complex families from next-generation sequencing data</Title>
    <Doi>10.1093/bioinformatics/btw550</Doi>
    <Authors>Heinrich Verena, Kamphans Tom, Mundlos Stefan, Robinson Peter N, Krawitz Peter M, Valencia Alfonso</Authors>
    <Abstract/>
    <Body>1 Introduction In recent years, high-throughput sequencing approaches have been successfully applied to identify thousands of novel pathogenic mutations in genetic disorders. However, due to the high variability of the human genome, there are several hundred variants of unclear significance in each individual and the analysis of such high dimensional data is still challenging. In order to reduce the search space, related individuals are usually sequenced for filtering purposes or linkage analysis, when studying unknown disorders. Even in routine diagnostics sequencing of additional family members is common practice, whenever the disorder is highly heterogeneous and  de novo  mutations are the most promising candidates ( Veltman and Brunner, 2012 ). However, these approaches rely on correct pedigree information and thus there is a great need for robust and easily manageable methods for checking the relationship between samples. So far it is common practice to infer relatedness with different kinds of genetic markers and a comprehensive overview of the existing methods is given by  Blouin (2003)  and  Pemberton (2008) . Most of the approaches that are based on likelihood ratios (LRs), test different pre-defined relationship models and have already been thoroughly discussed by others ( Aoki  et al. , 2001 ;  Brenner, 1997 ;  Marshall  et al. , 1998 ). In our work, we study the effectiveness of pedigree predictions when also rare markers are used, that become only accessible by direct sequencing. In general, the accuracy of the prediction increases with the information content of the available markers. Microsatellites are very variable in length and are thus highly informative. With 20–30 unlinked microsatellites it is usually possible to achieve accurate predictions about the relationship between samples. In contrast, SNVs are mostly biallelic and in exomes their mean heterozygosity is only around 0.3. On the other hand, large numbers of SNVs are detected in high-throughput sequencing projects and we will study in this work how the accuracy of pedigree predictions scales with the number of such markers. All models that have been developed for the prediction of relationships also have to account for genotyping errors. Microsatellites are more difficult to genotype than SNVs ( Pompanon  et al. , 2005 ). Additionally, the intrinsically higher rates of mutations can introduce biases in analyses that rely on identity by state, IBS ( Hardy  et al. , 2003 ). Due to these technical challenges the evaluation of microsatellite data requires a high human expertise to avoid false parentage exclusions as discussed in  Dakin and Avise (2004)  and it is also more difficult to automate these methods computationally. In most approaches that are used to predict relationships, marker sets are defined prior to screening the samples. In order to maximize the probability that a marker is informative, SNPs or SSRs with a high heterozygosity are therefore usually chosen. However, certainly bi-allelic markers with a high population frequency may also be IBS by pure chance. With whole exomes and other types of reference-guided resequencing data, there is no need for choosing marker loci a priori, as variant calls are plentiful. Instead we can simply consider all positions where the genotype of at least one of the samples differs from the reference sequence. The potential of such rare SNVs for pedigree prediction has not been studied so far. Most existing methods that work with genome data rely on haplotype reconstruction and can certainly achieve high precision ( He  et al. , 2013 ). Unfortunately, for exome data and other enrichment based Next generation sequencing (NGS) data sets, the derivation of long haplotypes is often not possible. We analyzed systematically likelihood-based approaches to reconstruct entire pedigrees that also take into account rare markers and we found that especially rare variants are well suited for assessing second-order relations. By this means we achieve high precision in pedigree predictions and present a tool that can easily be integrated in existing analysis pipelines to ensure quality and avoid sample mix-ups. 2 Approach We developed a method to reconstruct and visualize complex pedigree structures for any given number of individuals that is based on likelihood ratios of different models of relationship types for any combination of two samples (dyad). For any possible dyad we test the following models of kinship: 0 unrelated 1 technical/biological replicates (or identical twins) 2 full siblings 3 parent–child 4 second-order relationship The different degrees of relationship reflect the expected values for the proportion of the genome that is shared between two individuals. This can also be expressed by the coefficient of relationship which defines the probability that an allele at an arbitrary locus in the genome originates from the same common ancestor ( Wright, 1922 ). For instance, we would expect that a parent and a child as well as siblings share 50% of their genetic material and thus the same coefficient of relationships. In second order relationships as e.g. in a grandparent–grandchild, or uncle-nephew dyad only 25% of the alleles are identical by descent (IBD), in an outbred population. On the contrary, in highly inbred populations and consanguineous marriages the expected coefficients of relationship of 0.5 and 0.25 may deviate upwards substantially. In the following we will also use the coefficient of relationship to discriminate between different degrees of kinship and refer to  parent–child  and  full siblings  as first-order relationship and classify half-siblings, grandparent–grandchild and aunt/uncle-nephew/niece as  second-order  relationships. Additionally we are defining the model  technical/biological replicates  to identify samples that have a relationship coefficient close to 1, which is either the case if the same sample has been processed twice or if two samples are from identical twins. For each possible dyad all these models are tested and compared to a null hypothesis that assumes the individuals are unrelated, which is explained in more detail in the next section. The computation of the probabilities of all hypotheses requires knowledge about the expected allele frequencies in a population. In this work, if an allele was observed at least once in the 1000 genomes project ( Altshuler  et al. , 2010 ;  The 1000 Genomes Project Consortium, 2012 ) we use the respective frequency. The most likely relatedness class is then identified by the largest likelihood ratio among all hypotheses for one dyad. We provide a tool for our algorithm that works on genotype data presented in VCF format ( Danecek  et al. , 2011 ) and that generates an intermediate output that we refer to as extended ped format: for each individual the most likely relationships to the remaining individuals are recorded. In addition to mother and father, also children and sibling relationships are listed (see  Supplementary Materials ). An R Script takes the predicted classes of relationship as input and visualizes the result in a pedigree graph. We tested our method on family exome data from the 1000 genomes project (referred to as 1KG data in the following), as well as on in-house data. For the systematic analysis of performance at different degrees of inbreeding as well as increasing degrees of error rates we also simulated samples based on sequencing data from real individuals. The java application, as well as the visualizing R script, is integrated in GeneTalk ( Kamphans  et al. , 2013 ), and also available as a standalone application at  www.gene-talk.de/vcf2ped . We analyzed the performance of our approach for a decreasing number of exomic and genomic markers. For exomic markers that are mostly unlinked we achieved high accuracy for all relationships (first and second order) with a minimum of 10 000 markers. The same accuracy can be achieved with a comparable number of genomic markers when randomly distributed over the genome (data not shown). 3 Materials and methods 3.1 Data processing and simulation of technical replicates The implementation was tested on publicly available data of the 1KG project as well as on data that had been sequenced in-house over the past years with the approval of the ethical Board of the Charité. All in-house samples were sequenced on an Illumina HiSeq 2000 and the resulting reads were mapped to the hg19 reference genome using BWA-MEM ( Li, 2013 ). Multi-sample variant calling was performed on 39 families with different population backgrounds, downloaded from the ftp server of the 1KG project, as well as on in-house families, including three families which show a high degree of consanguinity within the pedigree, with GATK ( Mckenna  et al. , 2010 ) (version 2.7-2) using standard Best Practices recommendations ( DePristo  et al. , 2011 ). To guarantee a fair comparison between all analyzed sequencing samples we restricted all variant calls to the pilot 3 consensus exonic target region defined by the 1KG project. We defined a minimum coverage of five reads per base in a homozygous state and a minimum of 5 reads per allele for heterozygous variants. All individuals of the  1KG  project are subdivided into single families of 2, 3 or 4 family members, which are connected via different degrees of relationship ( siblings ,  one parent—one child ,  trio: two parents—one child ,  quartet: two parents—two children  and  second-order ). in-house families can be grouped in  trios ,  quartets  and families ranging over three generations (see  Fig. 1 ), including second-order relationships. Additionally we simulated technical replicates of one individual of the 1KG project ( NA06986 ) by randomly reducing the coverage of the original alignment yielding a mean per-base coverage of 313, 140, 120, 80, 50 and 30 reads. Fig. 1 Family structure of individuals that were sequenced in-house including three generations with different degrees of relationship (first- and second-order). Circles (female samples) and rectangles (male samples) with labels indicate that genotype data was available 3.2 Likelihood ratio analysis Likelihood analysis is used to evaluate the goodness of fit of a hypothesis  H i  relative to another hypothesis  H j , i ≠ j , (e.g. the  null- hypothesis  H 0 ), depending on the same underlying data  D . The likelihood ratio can be expressed as follows:
 LR ( H i , H j | D ) = Pr ⁡ ( D | H i ) Pr ⁡ ( D | H j ) For every two individuals ( x 1  and  x 2 ) we consider five different hypotheses  H i , i ∈ I = { 0 , 1 , 2 , 3 , 4 } : H 0 : Sample  x 1  and  x 2  are unrelated H 1 : Sample  x 1  and  x 2  are technical/biological replicates H 2 : Sample  x 1  and  x 2  are full siblings H 3 : Sample  x 1  is a parent of sample  x 2 H 4 : Sample  x 1  and  x 2  have a second-order relationship As already introduced and extended in previous works (including  Aoki  et al. , 2001 ;  Brenner, 1997 ;  Marshall  et al. , 1998 ;  Thomas  et al. , 2002 ) we estimate the probability of the combination of two genotypes of two individuals with the additional use of population data ( Table 1 , see  Supplemental Material s for a detailed derivation of the formulas). In this study, we calculated allele frequencies per position from 2535 unrelated individuals which were recently updated by the KG project ( Altshuler  et al. , 2010 ). The likelihood ratios for the combinations of genotypes ( g t ∈ { a n , a m } , a n , m ∈ { A , C , G , T } ) for all variable positions,  k ∈ K , can be combined for unlinked marker loci. Due to the small probabilities it is computationally more efficient to work with logarithms of the likelihood ratios:
 1 K   log 10 LR ( H i , H j | D ) = 1 K   log 10 ∏ k ∈ K LR k ( H i , H j | g t x 1 ( k ) , g t x 2 ( k ) ) = 1 K ∑ k ∈ K   log ⁡ 10 P r ( g t x 1 ( k ) , g t x 2 ( k ) | H i ) P r ( g t x 1 ( k ) , g t x 2 ( k ) | H j ) Table 1. Likelihood ratios for all subject-query genotype combinations  gt  for different hypotheses g t x 1 g t x 2 H 1 ∼ H 0 H 2 ∼ H 0 H 3 ∼ H 0 H 4 ∼ H 0 H 3 ∼ H 2 a 1 a 1 a 1 a 1 1 f 1 2 ( f 1 + 1 ) 2 4 f 1 2 1 f 1 f 1 + 1 2 f 1 4 f 1 ( f 1 + 1 ) 2 a 1 a 1 a 1 a 2 e 2 f 1 3 f 2 f 1 + 1 4 f 1 1 2 f 1 2 f 1 + 1 4 f 1 2 f 1 + 1 a 1 a 1 a 2 a 2 f 1 2 f 2 2 1 4 e f 1 2 f 2 2 1 2 4 e f 1 2 f 2 2 a 1 a 2 a 1 a 2 1 2 f 1 f 2 1 + f 1 f 2 4 f 1 f 2 1 4 f 1 f 2 4 f 1 f 2 + f 1 + f 2 8 f 1 f 2 1 1 + f 1 f 2 a 1 a 1 a 2 a 3 e 4 f 1 2 f 2 f 3 1 4 e 4 f 1 2 f 2 f 3 1 2 e f 1 2 f 2 f 3 a 1 a 2 a 1 a 3 e 8 f 1 2 f 2 f 3 2 f 1 + 1 8 f 1 1 8 f 1 4 f 1 + 1 8 f 1 1 1 + 2 f 1 a 1 a 2 a 3 a 4 e 8 f 1 f 2 f 3 f 4 1 4 e 8 f 1 f 2 f 3 f 4 1 2 e 2 f 1 f 2 f 3 f 4 The frequencies,  f n  refer to the allele frequency of allele  a n  and are pre-calculated using data from the 1KG project and we assume  ∑ n f n = 1 . Combinations of genotypes that do not occur for certain relationships (‘Mendelian error’) could still be observed due to e.g. erroneous genotyping ( e  = 0.001). For some relatedness classes, the probability of having one genotype in one sample and another genotype in a second sample would be zero, assuming perfect data quality and no  de novo  mutations. With these prerequisites it is, for instance, not possible that a parent has genotype  a 1 a 1  while the child has genotype  a 2 a 2  at the same position. This could yield infinitely large likelihood ratios. In this case, we use an additional parameter  e  = 0.001, accounting for sequencing errors and  de novo  mutations. 3.3 Infering relatedness classes from log likelihood ratios For each dyad we computed the LRs for all variable positions and hypotheses  H i  versus the null hypothesis (not related). As the number of variable positions might differ between dyads we used the mean LR per position for a better comparison ( Fig. 2 ). The violin plots visualize the distributions of the LRs for different hypothesis comparisons for unrelated dyads. LR tests for most of the related individuals (circles) yield positive values and the correct hypothesis maximizes the likelihood ratio,  max { LR ( H i , H 0 ) } i ≥ 0  For large sample sizes the likelhood ratio test statistics approximate a  χ 2  distribution and may also be used for a probabilistic interpretation (Wilks’s theorem). Fig. 2 Comparison of the mean likelihood ratio LR per position for different relationships between pairs of individuals (dyads). The likelihood for all relationship models  H i  versus the null hypothesis (not related) were computed for all dyads from the 1000 genomes project and additional family data. LRs of all dyads that are not related are depicted as background distribution (gray violin plots) whereas dyads with a relationship are illustrated as circles. The true relationship is color-coded and maximazes the LR for the correct hypothesis. Exemplarily this is shown for a dyad of each relationship (crosses). However, misclassifications can occur especially in replicate identification for low-quality samples with a high genotyping error rate (orange asterix) (Color version of this figure is available at  Bioinformatics  online.) In families of more than two members and even highly complex sub-structures, such as shown in  Figure 1 , the whole pedigree is reconstructed from the predicted relationships of dyads. All relationship hypotheses are not directed. This means, when the parent–child relationship is detected in a dyad we don’t know who is the father and who is the son. However, the directionality can be clarified by the relationships of additional family members. The sex of each individual is determined by the ratio of heterozygous variants versus all variants on the X chromosome and is also used in the pedigree reconstruction. 4 Results 4.1 Separation efficiency of log likelihood ratios We tested different hypotheses on related individuals from the 1KG data ( Fig. 2 ) and used these data to infer correct relatedness states to families with different substructures ( Fig. 2 ). The comparison of likelihood ratios of most models show a segmentation of the dyads corresponding to their kinship coefficients. For instance, in  Figure 2 , we see for the comparison  H 3 ,  H 0  technical replicates ( H 1 ) with a kinship coefficient close to 1 at the right end, dyads of siblings ( H 2 ) or parent–child ( H 3 ) in the middle- and second-order relationships ( H 4 ) at the left side. Dyads that are not related are visualized as a gray background distribution (violin plots). An interesting exception is  LR ( H 3 , H 2 ) , where two models with the same expected kinship coefficient (0.5) are compared. This likelihood ratio is well suited for discriminating between parent–child and sibling relationships. Since the probability that a parent and a child share the same allele is the same as in siblings (50%), it is hard to distinguish between these two relationship states and lead to a low variance in likelihood ratios, as seen in the likelihood ratios for  LR ( H 3 , H 2 )  in  Figure 2 . A similar kinship coefficient is also the reason why the LR values for parent–child and full siblings are nearly on the same level for the comparisons  LR ( H 3 , H 0 )  and  LR ( H 2 , H 0 ) . This leads to the overall rule: the discriminatory power to distinguish between different hypotheses gets lower if the underlying assumed relationship types become more similar. Especially with increasing error rates the identification of identical twins/technical replicates becomes more difficult as the expected kinship coefficient of one drop. Although all simulated replicates form a distinct cluster in the comparison of  LR ( H 1 , H 0 )  in  Figure 2 , there are three dyads (*) that yield a LR &lt; 0. These false classifications can be avoided if the error rate  e  is adjusted appropriately. 4.2 Precision depends on the number of markers and heterozygosity We analyzed the effect of the number of variants on the precision of the classification process by reducing the amount of markers. Besides restricting to smaller randomly chosen subsets we also studied the performance for subsets of loci with high heterozygosity. Such subsets are more similar to the markers used in SNP-arrays and allow a comparison with existing tools. Heterozygosity is defined as  h = 1 − ∑ n f n 2  and relates to the information content of a marker. The average heterozygosity of a SNV in our exome data was 0.3. In contrast most existing tools use polymorphic sites with considerably larger  h . The positive predictive value (precision) of parent–child and full sibling comparisons starts to drop when using &lt;500 randomly selected markers ( Fig. 3 ). The correct assignment of second-order relationships requires substantially more markers for a comparable precision. However, the performance for the second-order classifications is good for whole exome sequencing data comprising several thousands of variants. Only a reduction to fewer loci, as encountered in gene panels, will lead to an excess of false positive predictions and therefore to lower precision values. Especially the number of unrelated dyads that are erroneously classified as second-order increases. Fig. 3 The precision of the prediction of all relationships decreases with the number of available markers and their heterozygosity. The number of markers was either randomly reduced or restricted to a subset of highly informative markers. The positive predictive values for parent–child as well as for full sibling relationships start to drop when using &lt; ∼500 markers. At a comparable number of markers, second-order relationships are more difficult to distinguish from unrelated controls. The mean heterozygosity of biallelic variants in an exome is 0.3 and represents the information content of a marker. A higher precision can be achieved when choosing markers with a heterozygosity above this average (unfilled shapes) (Color version of this figure is available at  Bioinformatics  online.) We hypothesized that increasing the heterozygosity of these small marker sets might improve the precision. When choosing only loci with a heterozygosity above average, the precision for all relationship models increased (unfilled shapes in  Fig. 3 ). This is in good agreement with the results from other studies that achieved a high precision for predictions that were based on markers with high information content ( Epstein  et al. , 2000 ). The most polymorphic site accessible via exome sequencing is the human leukocyte antigen (HLA) cluster on chromosome 6 with more than 15 000 different alleles known up to date ( Robinson  et al. , 2016 ). Recently, bioinformatics tools became available that allow HLA typing from exome data and can be used additionally to rule out relatedness in questionable dyads ( Szolek  et al. , 2014 ). However, a general comparison between the predictive power of multiple biallelic markers such as most SNVs and a single polymorphic marker such as the HLA shows that the discriminatory power is limited: Assuming equal likelihoods for all alleles, the information content,  IC = − ∑ n f n   log 10 f n , of 10 unlinked biallelic markers ( n   = 2) would be comparable to the IC of the HLA locus ( n  = 15 000). The contributions of the different genotype combinations to the log-likelihood ratios also depend on the heterozygosity. For instance in  LR ( H 3 , H 0 ) , for common variants the major contribution will come from the genotype combinations  a 1 a 2 ∼ a 1 a 2  and  a 1 a 1 ∼ a 2 a 2 . If  H 0  is the true hypothesis the later,  a 1 a 1 ∼ a 2 a 2 , will push  LR ( H 3 , H 0 )  below zero ( Supplementary Fig. S1d ). However, for smaller  f 1 , the choice of  e  will influence this result ( Supplementary Fig. S1a ). On the other hand, if  H 3  is true then the ratio  1 / 2 f 1  ( Table 1  for  a 1 a 1 ∼ a 1 a 2 ) will shift  LR ( H 3 , H 0 )  to positive values. The contribution of  a 1 a 1 ∼ a 1 a 2  to  LR ( H 3 , H 0 )  is positive for frequencies  f 1 &lt; 0.5  and thus in favor of  H 3 . This combination will more likely occur for small values of  f 1  and an disproportionate high occurence of this term will indicate relatedness. Interestingly, we found most misclassified second-order relationships within the same sub-populations, after randomly reducing the number of variants. That also emphasizes the importance of using rare low-frequency variants, that are specific within families rather than using polymorphisms that are more suitable to discriminate between sub-populations. 4.3 Directionality of parent–child relationships The directionality of a parent–child connection can be solved by considering additional relationships if more family members are available ( Thomas and Hill, 2000 ). ( Fig. 4a ) exemplifies a case where both, mother and father, are present in the available family tree (referred to as  trio ). The clear assignment of  mother–child  and  father–child  can be resolved by the additional information that both parents are not related. In another example, ( Fig. 4b ), the directionality between parent and child can be resolved by the additional knowledge, that two of the three available samples in a pedigree ( II1  and  II2 ) have a sibling relationship. A more detailed description for the pedigree reconstruction that is based on this rule set, can be found in the supplemental material and is also encoded in our tool. Fig. 4 The parent–child directionality can be resolved with at least three available samples, either due to the knowledge that both parents are unrelated  (a)  or with additional information about siblings  (b) . If just two sequenced samples are available in the analysis, the directionality cannot be resolved  (c) . Panel (d) shows an illustrated example of two different populations (Nos. 1 and 2) with two exemplarily chosen individuals ( mother  and  father ). The shading colors (orange and green) are illustrating the population specific variants that do not appear in the other group respectively. Ideally, an offspring of two members of these distinguishable groups would share half of the population specific variants of the mother and half of the father. A simulated offspring of two individuals with different ethnic backgrounds, YRI (orange) and CHS (green), has an increased proportion of heterozygous variants (e) and the mean minor allele frequency ( MAF ) is located between the MAF of the parents (f) (Color version of this figure is available at  Bioinformatics  online.) From a theoretical point of view, we wondered whether it is possible to infer the directionality of a parent–child relationship, if only a dyad is given ( Fig. 4c ). There is one scenario in which the directionality can be resolved by data of the dyad itself. If both parents have a different ethnic background the offspring will show two sets of population specific SNPs ( Fig. 4d ). Furthermore the heterozygosity of the offspring will be higher than of each of the parents, which is illustrated in  Figure 4e , where we simulated an offspring of two 1KG individuals from distinguishable populations ( CHS  and  YRI ). The frequency for many polymorphisms differs depending on the population background. In an offspring from two different ethnic backgrounds there will be a movement towards the joint mean, similar to an increasing entropy in thermodynamics when two isolated systems are mixed ( Fig. 4f ). 4.4 Influence of inbred structures within complex families In highly consanguineous families the prediction of the exact relationships becomes more difficult as there is a stronger deviation from the null model that is based on the allele frequencies of an outbred population. In addition to exomes from highly consanguineous families, we investigated the influence of inbred structures by simulating offspring from related 1KG dyads. For each related pair, we randomly chose one allele from each of both assumed parents to create a new genotype at each autosomal position. We used the individual inbreeding coefficients,  f , for each offspring, to quantify the extent of consanguinity. The inbreeding coefficient is defined as the ratio of the genome that is IBD ( Wright, 1922 ) and was computed as described by  Gazal  et al.  (2014) . As expected, the simulated offspring with closely related parents show higher individual inbreeding coefficients. These higher inbreeding factors correlate with lower likelihood ratios between relationship models of the same order, such as parent–child versus siblings ( Fig. 5 ). Fig. 5 Classification of relationships in highly consanguineous families. In addition to real offspring from related parents, we also simulated offspring for 1KG dyads with different degrees of relatedness. Individual inbreeding coefficients were calculated for all offspring and log likelihood ratios for all parent–child pairs. For offspring with a higher inbreeding coefficient log likelihood ratios of parent–child versus siblings approach decrease. However, for exome data a correct classification of all parent–child pairs is still possible (Color version of this figure is available at  Bioinformatics  online.) When using entire exome data sets a correct classification was still possible for all tested cases, even for families with a high degree of consanguinity. 5 Discussion In this work, we analyzed the performance of reconstructing family structures with genotype data that become available in NGS studies, including rare variants. We were able to derive pedigrees with high precision for publicly available samples of families, as well as for in-house data, even for as little as a few hundred markers. Our method is based on comparisons between likelihood ratios for different kinship classes and—as expected—the closer the coefficients of relationship were between two models, the more difficult it became to differentiate between them. The most challenging discrimination is between unrelated individuals in a highly inbred population and samples that are related by second-order relationship. So far highly polymorphic marker loci such as microsatellites have been used in paternity testing and to detect distant relationships. However, ancestry can also effectively be identified with the use of rare and family specific single nucleotide variants. We observed the biggest variance in likelihood ratios for all dyads for the comparison of the hypothesis  H 3 ,  H 0  (parent–child ∼ unrelated). One drawback of our approach is that different quality levels of the data are not considered. We simulated technical replicates with a high error rate by decreasing the coverage per base in the alignments and studied the influence on the classification process. Some replicates with a large difference in coverage and quality were misclassified as full siblings instead of technical replicates, as visualized in  Figure 2  (violet dots for  LR ( H 1 , H 2 ) ). With decreasing coverage, the error rates—especially for heterozygous calls—increase. However, the site-specific error probabilities that are reported by standard genotype callers are usually vastly underestimated for rare variants. Allegedly small error rates pose a problem especially in the identification of replicates. Another approach to estimate genotyping errors on exome variant lists, that also considers allele frequencies from population studies, reported a mean genotyping error rate of  e =  0.001 as more accurate for most current exomes ( Heinrich  et al. , 2013 ).This error rate also yielded the best performance in our tests and is therefore suggested as a default setting. The consequences of an unsuitable choice of this parameter on the likelihood ratios are discussed in more detail in the supporting information. In summary, we have shown that genotype data from NGS studies can also be used to deduce pedigree information with high precision. Our approach doesn’t require any additional generation of data and will thus be easy to integrate into existing analysis pipelines. This will help to identify sample mix-ups at an early stage and improve the overall quality in the diagnostic procedure. Supplementary Material Supplementary Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>girafe – an R/Bioconductor package for functional exploration of aligned next-generation sequencing reads</Title>
    <Doi>10.1093/bioinformatics/btq531</Doi>
    <Authors>Toedling Joern, Ciaudo Constance, Voinnet Olivier, Heard Edith, Barillot Emmanuel</Authors>
    <Abstract>Summary: The R/Bioconductor package girafe facilitates the functional exploration of alignments of sequence reads from next-generation sequencing data to a genome. It allows users to investigate the genomic intervals together with the aligned reads and to work with, visualise and export these intervals. Moreover, the package operates within and extends the ever-growing Bioconductor framework and thus enables users to leverage a multitude of methods for their data in order to answer specific research questions.</Abstract>
    <Body>1 INTRODUCTION Next-generation sequencing (NGS) technologies provide users with millions of comparatively short RNA or DNA  reads  from biological samples of interest. The first step in the analysis of these data usually is to align the reads to the chosen reference genome, using powerful, specific alignment tools. For the secondary analysis that follows, there is a need for an integrated work environment which allows users to explore and annotate the genome intervals with the aligned reads. Besides providing functionality for data exploration, this environment must also provide interfaces to other available tools, especially as the field of NGS analysis software is comparatively new and rapidly evolving. Previously described, excellent tools that provide frameworks for working with aligned reads include  Galaxy  (Giardine  et al. ,  2005 ) and  SAMtools  (Li  et al. ,  2009 ). Here, we describe the R/Bioconductor package  girafe  that enables users to investigate the genome intervals with the aligned reads, henceforth referred to as  aligned intervals , and to work with, visualise and export these aligned intervals. One advantage of  girafe  over other tools for working with aligned reads is that the package operates within the open source, open development and constantly growing Bioconductor framework (Gentleman  et al. ,  2004 ). Thus, this package enables users to leverage a multitude of methods in the analysis of their data in order to answer specific research questions. 2 AVAILABLE FUNCTIONALITY In the following, we present some functionalities of  girafe . The package is built on, and greatly enhances, the functionalities of the Bioconductor packages  genomeIntervals  and  ShortRead  (Morgan  et al. ,  2009 ). For this demonstration, we use example data downloaded from the Gene Expression Omnibus database (Edgar  et al. ,  2002 , GSE10364). The data are Solexa reads obtained from small RNA profiling of mouse oocytes (Tam  et al. ,  2008 ). Importing aligned reads : The reads were mapped to the mouse genome (assembly  mm9 ) using the  Bowtie  aligner (Langmead  et al. ,  2009 ). The resulting file can be read into R, using the  ShortRead  package, and converted into an object of class  AlignedGenomeIntervals , the core class of package  girafe . Exploring aligned intervals : Objects of this class can easily be explored using standard R functions to obtain summary statistics answering questions, such as (i) how long are the reads aligned to specific intervals? or (ii) how many intervals are located on each chromosome? Processing the aligned intervals : Basic interval operations, such as sorting, shifting and determining intersections and unions of interval sets, are readily supported. Moreover, the function  reduce  provides a flexible way to combine, or merge, aligned intervals. One intention could be to combine aligned reads at exactly the same position, which only differ in their sequence due to sequencing errors. Another example objective could be to combine overlapping short reads that may be (degradation) products of the same primary transcript. Visualisation : The package  girafe  contains functions for visualising aligned intervals with the powerful plotting facilities of R. These visualisation functions are a flexible alternative to those provided by genome browsers and may be especially relevant for sequencing data from organisms which are not well represented in genome browsers.  Figure 1  shows aligned intervals from the example data in a 500 bp region on the X chromosome. The reads aligned in this region correspond to two miRNAs reported to be highly expressed in these data (Tam  et al. ,  2008 ).
 Fig. 1. Visualisation of genome intervals with aligned reads. Two miRNAs (shown in pink) have been annotated on the Crick strand within this region on the X chromosome. Reads were exclusively aligned to the Crick strand in this region and mostly correspond to the mature and star sequences of the miRNAs. Intervals with uniquely aligned reads are shown in blue while grey marks non-unique reads. For  mmu-mir-503  (left), the original data are shown while for  mmu-mir-322  (right) the overlapping  AlignedGenomeIntervals  have been merged using the function  reduce . Summarising the data using sliding windows : The data can be searched for genome regions of defined interest using a sliding-window approach. For each window, the number of intervals with aligned reads, the total number of reads aligned, the number of unique reads aligned, the fraction of intervals on the Watson strand, and the higher number of aligned reads at a single interval within the window are reported. Overlap with annotated genome features : A frequent task is to determine the overlap of the aligned intervals with genome elements that are described in databases, in order to annotate the aligned reads.  girafe  includes functions for efficiently determining these overlaps and allows the user to specify custom requirements, such as a minimum fraction of the total interval length, for considering intervals and features to be truly overlapping. Exporting the data : The  girafe  package contains methods for exporting the data into tab-delimited text files, which can be uploaded to genome browsers for further visualisation and exploration. Currently supported formats include ‘bed’, ‘bedGraph’ and ‘wiggle’. Vignette : The package vignette provides more detailed examples together with the corresponding R source code and discusses memory usage and interactions with other Bioconductor packages. 3 CONCLUSION The R/Bioconductor package  girafe  provides users with a powerful, flexible and extensible framework to explore NGS data, following alignment of the reads to a reference genome. The package interacts with other Bioconductor packages and allows export of the data in various formats for exploring them in other software, with the aim of not restricting the user to a limited set of analysis tools. The field of NGS analysis software is growing rapidly. Thus, future developments of the package will include adding further methods for working with aligned genome intervals, reducing the memory footprint, and providing additional interfaces to other R/Bioconductor packages and other software. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Learning signaling networks from combinatorial perturbations by exploiting siRNA off-target effects</Title>
    <Doi>10.1093/bioinformatics/btz334</Doi>
    <Authors>Tiuryn Jerzy, Szczurek Ewa</Authors>
    <Abstract/>
    <Body>1 Introduction Signaling networks consist of interconnected proteins that transmit information inside the cell. External or internal stimuli trigger signaling cascades, which are implemented by consecutive kinases post-transcriptionally modifying one another, for example by phosphorylation. The signal is in this way transmitted down to the nucleus, where the transcriptional machinery regulates adequate functional response of the cell to the signal. Technological advances such as gene silencing using RNA interference (RNAi) ( Agrawal  et al. , 2003 ), gene knock-out using CRISPR-Cas ( Hsu  et al. , 2014 ) or perturbations by the means of drug treatment ( Molinelli  et al. , 2013 ) allow experimental interventions on cellular networks and measurement of their effects. For example, a small interfering RNA (siRNA) perturbation screen in human cells was performed in a study of pathogen infection ( Ramo  et al. , 2014 ). The aim of the study was to identify genes of the host that are involved in the networks that become activated when the cells are infected by pathogens. The measured phenotype was the level of infectivity by the pathogen upon siRNA-mediated knock-down. The ultimate goal in understanding cellular networks, however, is not only to recognize the genes involved in the network, but also to (i) resolve its structure of interconnections (network edges), and to (ii) understand the way the network members contribute to the observed phenotype, for example how the individual kinases regulate the infectivity. Such perturbation studies have the potential to address both these questions. Despite, however, the perturbation technology becoming ever more advanced and increasingly better understood ( Lisitskaya  et al. , 2018 ;  Mohr and Perrimon, 2012 ;  Terns, 2018 ), computational inference of signaling networks from perturbation data remains a challenge. The first problem faced by signaling network modeling is the so-called perturbation-effect gap ( Markowetz, 2010 ). Although the interventions target the network (layer 1 in  Fig. 1A ), the resulting states of the network nodes are not measured. The observed variables reside in the layer of the measured phenotype downstream (layer 3 in  Fig. 1A ), only indirectly connected with the network via the middle regulatory layer (layer 2 in  Fig. 1A ). Second, the network nodes are not only targeted directly by the experiments, but also via propagation in the network. Once a given network node is inactivated by the experiment, the signal is blocked and the nodes downstream also become inactivated. Thus, perturbations propagate in the network along its unknown edges, in the same way as the signal. The third problem is the complexity of the hidden and unknown regulatory layer. The measured phenotype is rarely an effect of a single member of the network. Instead, it is a combination of contributions from all perturbed (directly or via propagation in the network) nodes. These contributions may be positive and enlarge the measured phenotype, or negative and decrease it. Fourth, interventions using common technique of siRNA-mediated knock-down, although intended only to target single genes, the so-called on-targets, at the same time perturb multiple other genes, the so-called off-targets ( Jackson  et al. , 2003 ). A given siRNA, it affects its off-target genes using the microRNA pathway, binding by complementarity of its seed region (positions 2–8) to the 3′ untranslated regions of the transcripts of the genes ( Sigoillot and King, 2011 ). Consequently, the measured phenotype is confounded by the combinatorial effects from the off-targets and cannot be interpreted as the result of perturbing the on-target alone. Finally, there is the problem of model identifiability from perturbation data. The key question here is, how many and what combinations of perturbations are required to uniquely and accurately infer the model?
 Fig. 1. Enhanced linear effects model. ( A ) Three layers of the system: (1) perturbed signaling network, (2) intermediate regulatory layer, (3) observed effects  Y . Genes (circles, here 1, 2, 3) are directly or indirectly (via propagation in the network) perturbed in experiments. Bold arrows indicate how perturbations propagate within the network. Dashed arrows show the individual contributions of the genes to the observed perturbation effects  Y . LEM assumes that  Y  is normally distributed around the mean equal to the weighted sum of individual gene effects (here β 1 , β 2 , β 3 ), with weights set to perturbation states. The difference between the enhanced LEM and the previous model is that the contributions do not have to be positive, and can take any real value but not zero. ( B ) Example means ( y -axis) for all possible perturbation experiments ( x -axis), as expected in the enhanced LEM with network structure as in (A), for  β 1 = − 4 ,   β 2 = 1  and  β 3 = 2 . Whiskers indicate example error  Nested effects models (NEMs) ( Fröhlich  et al. , 2008 ,  2009 ;  Markowetz  et al. , 2005 ,  2007 ;  Tresch and Markowetz, 2008 ) and their extensions ( Anchang  et al. , 2009 ;  Fröhlich  et al. , 2011 ;  Pirkl  et al. , 2016 ;  Siebourg-Polster  et al. , 2015 ;  Srivatsa  et al. , 2018 ) specifically address the perturbation-effect gap problem. NEMs represent the network structure by directed graphs. The crucial assumption behind NEMs is that perturbation effects show a nested subset hierarchy, which reflects the hierarchy of nodes in the signaling network. As a graphical model of signaling networks, probabilistically inferred from the observed effects, NEMs constitute an attractive approach for solving problem of learning network structures (layer 1 in  Fig. 1A ). These models, however, have a simplified representation of the regulatory layer (2 in  Fig. 1A ), and assume that each effect is regulated only by a single gene in the network. Other previous computational approaches to signaling networks concentrated solely on solving problem of elucidating the link between the network and the observed effects (layer 2 in  Fig. 1A ) ( Gat-Viks and Shamir, 2007 ;  Szczurek  et al. , 2009 ,  2011 ). These approaches assume they are given a known network graph, and aim at either small refinements to the given graph, or resolving the detailed mechanisms governing the regulation of the downstream targets by the network components. Recently, we have introduced linear effects models (LEMs), aiming to address the above-mentioned problems ( Szczurek and Beerenwinkel, 2016 ). LEM is a model where layer 1 of the signaling network is represented by a graph with nodes corresponding to the signaling genes. Edges of that graph correspond to the way the perturbation effects propagate in the network. For two nodes 1 and 2, an edge (1, 2) indicates that perturbation of gene 1 affects also gene 2. The downstream regulation layer 2 is modeled by a vector of model parameters, with entries corresponding to individual contributions to the observed effects. LEMs are inferred from the data, which is the phenotype (layer 3) measured under the perturbation experiments. With this formulation, LEMs aim both at learning the structure of interactions within the network and at deconvolution of the contributions of its components to the observed perturbation effects. The main drawback of the previously introduced model, however, is the limited expressiveness of the allowed contribution values. The model parameters are assumed to be strictly positive. As such, they are interpreted as only the magnitudes (absolute values) of the contributions to the measured effects. Thus, the original LEM cannot model both positive and negative contributions, such as down- or up-regulation of the measured phenotype. Here, we extend the previously proposed LEM by allowing negative contributions for the network genes, which are now assumed to take any real, nonzero values. This allows realistic modeling of the phenotype as a combination of up-and down-regulatory contributions from the perturbed genes. At the same time, this enhancement of the model raises nontrivial question of model identifiability. For the original LEMs we proved that a set of experiments perturbing all single and all pairs of nodes is required for their identifiability ( Szczurek and Beerenwinkel, 2016 ). The main contribution of this work is a proof that enhanced LEMs are identifiable from data measured under experiments where single, double and triple genes are perturbed. Moreover, we show that small enhanced LEMs, with less than six nodes, are identifiable from the same set of experiments, targeting only single and pairs of nodes, as the original LEMs. In this manuscript, two inference approaches for the enhanced LEMs are compared, namely Bayesian linear regression, referred to as Bayesian approach, and its time-efficient approximation, referred to as the Bayesian Information Criterion (BIC) approach. We perform comprehensive simulations to demonstrate that both approaches yield excellent accuracy of parameter estimation and network structure recovery of the enhanced LEMs, and to track the run times of the two approaches. Although the Bayesian approach performs slightly better in model inference, its run times are much longer than of the BIC approach. The curse of siRNA off-targets can be turned into a blessing with the use of computational tools of microRNA target predictions such as TargetScan ( Lewis  et al. , 2005 ). Using these tools, we can identify which genes are off-targeted by the siRNA, and treat both the known on-target and the predicted off-targets as a set of genes that are combinatorially perturbed within the same experiment. Recently,  Srivatsa  et al.  (2018)  demonstrated the applicability of the siRNA on- and off-targets as combinatorial perturbations, allowing to learn signaling networks using their pc-NEMs models. They did not, however, account for the fact that the set of network genes is only a subset of all genes that are perturbed by the siRNA, and that the perturbation of the remaining genes may also have their effect on the phenotype. Importantly,  Schmich  et al.  (2015)  showed that a phenotype measured under siRNA screens can successfully be modeled as a linear combination of contributions of the on- or off-targeted genes. Their approach was applied to model the infectivity phenotype in the siRNA screen of  Ramo  et al.  (2014) , demonstrating dramatically increased correlation of the inferred gene contributions compared to the confounded raw phenotypes between different siRNA libraries. Their model, however, did not account for any possible network connections between the perturbed genes, treating them as isolated nodes. Since LEM can be thought of an extension of this simple model, which accounts for the structure of the signaling network and the way perturbations propagate along its edges, we reasoned that LEMs can be particularly well suited to model signaling networks from combinatorial perturbations in siRNA screens. To this end, we correct the phenotype for the contributions of perturbed genes which are not part of the modeled network. Indeed, application of LEMs to inference of the  Bartonella henselae  (shortly B.  henselae ) infection network from infection kinome screen ( Ramo  et al. , 2014 ) demonstrates excellent recovery of known interactions. In summary, the present work introduces a more realistic and highly expressive model for learning signaling networks and their regulation of downstream phenotypes, and comes with proven identifiability constraints ensuring accurate model inference. 2 Enhanced linear effects models A  linear effects model  (LEM) is defined by a triple  M = ( G , β , c ) , where  G  is a finite, transitively closed, directed acyclic graph (DAG) with  n  nodes,  β ∈ { R ∖ 0 } n  is a vector of nonzero real values, henceforth called  admissible vectors , and  c  &gt;   0 is a real number called  precision parameter . The graph  G = ( V , W )  is defined by the set of nodes  V = { 1 , … , n }  and a directed, and transitively closed set of edges  W . The assumption  β g ≠ 0 , for all  g ∈ { 1 , … n } , and  G  being a transitively closed DAG is motivated by model identifiability, as shown below. We write  a → G b  to indicate that there is an edge in  G  from vertex  a  to vertex  b , and we call this edge  outgoing  from  a  and  incoming  to  b . By a  root  in  G  we mean any node  a  with no incoming edges. By a  leaf  in  G  we mean any node with no outgoing edges. The graph  G  corresponds to a signaling network, with nodes interpreted as genes and the edges representing the way perturbations of nodes propagate within the network. For a node  a  of  G  we let  V a G = { a } ∪ { b ∈ V   |   a → G b }  to be the set of all nodes with edges in  G  outgoing from  a , plus node  a . Moreover for  X ⊆ V , we let  V X G = ∪ a ∈ X V a G . The interpretation of the set  V X G  is that if  X  is a set of genes that are targeted directly by a given perturbation experiment, then  V X G  consists of all genes that are perturbed directly or via propagation in the network  G . LEMs are inferred from data  Y ∈ R m  measured under perturbation experiments described by a  m  ×  n  binary  perturbation  matrix  E . For an experiment  e  and a gene  g , entry  E e , g = 1  indicates that  e  directly perturbs gene  g , and  E e , g = 0  otherwise. The perturbation matrix  E , specifying the experiments, and the network graph  G , together determine a binary matrix  S ( E , G ) , called a  design matrix . We drop the arguments when they are clear from the context. For a given experiment  e  and gene  g , entry  S e , g = 1  if gene  g  is perturbed directly or via propagation along the network, and  S e , g = 0  otherwise. Thus, for  e ∈ { 1 , … , m } , if  X e  denotes the set of genes that are targeted as a result of performing the experiment  e , then for every  g ∈ { 1 , … , n } (1) S e , g = { 1   if     g ∈ V X e G , 0 otherwise .   The data  Y  quantify the perturbation effects, with  Y e  recording the effect of experiment  e . For example,  Y  could measure expression of a certain gene regulated by the network. In contrast to the original LEM formulation ( Szczurek and Beerenwinkel, 2016 ), the values of  Y  are not restricted to the absolute magnitudes of effects, but correspond to the actual values of these effects, which can be positive or negative. Finally, an admissible vector of parameters  β = [ β 1 , … , β n ] T , represents the individual contributions of the members of the network to the observed perturbation effects. In contrast to the original formulation of LEMs, the here proposed version of the model allows that the contributions need not be only positive, but also can take negative values. For the gene regulation example,  β g &gt; 0  would indicate that the perturbation of a given node  g  in the network contributes to activation of the regulated gene. On the other hand,  β g &lt; 0  would indicate that perturbation of  g  contributes to repression of the measured gene. Note, that this should be interpreted as that  g , when not perturbed, individually has a positive contribution to regulation of the measured gene. Moreover, it is important to distinguish the individual contribution β g  from the total effect of perturbing gene  g , which is given by the sum of its own contribution and the contributions from the nodes downstream of  g  in the network, i.e.  ∑ a ∈ V g G β a . In particular, even when the individual contribution β g  is negative, the total effect of perturbing  g  can be positive. Remaining assumptions are the same as in the original LEM, namely, that  Y  is a random variable, normally distributed around a linear combination of the individual gene contributions, with weights set to their perturbation states ( Fig. 1B )
 (2) Y e = ∑ g S e , g β g + ϵ e = S ( e , − ) β + ϵ e , 
where ϵ e  stands for measurement error,  ϵ ∼ N ( 0 , c − 1 I ) , with  c  denoting the precision parameter (inverse variance), and where  S ( e , − )  denotes the  e th row of matrix  S .  Equation (2)  can be read as the linear regression equation with design matrix  S  and coefficients β. With these assumptions, the log-likelihood function for the LEM  M = ( G , β , c )  and the data  Y  with perturbation matrix  E  is given by ( Szczurek and Beerenwinkel, 2016 )
 (3) L ( M ) = ln ( p ( Y | S , β , c ) ) = ∑ e = 1 m ln ( f ( Y e | S ( e , − ) β , c − 1 ) ) , 
where  f ( Y e | S ( e ,− ) β ,  c −1 ) is the gaussian probability density function with mean  S ( e ,− ) β  and variance  c −1 . For any two positive integers  k ≤ n , we consider a special perturbation matrix  E ( k , n )  that includes all possible experiments that target directly at most  k  genes out of given  n -element set of genes. Hence  E ( k , n )  has  m = ∑ i = 1 k ( n i )  rows and  n  columns, and for each  i -element subset  X  of genes,  E ( k , n )  has a row with 1s in exactly positions that correspond to genes in  X , and 0s in the remaining positions. In reality it may happen that the modelled biological network contains a cycle. When we take a transitive closure of such a graph  G , the cycle turns into a clique. It follows that the design matrix  S  has identical columns that correspond to the genes belonging to this clique. Consequently, Theorem 3 stated below fails for graphs  G  that contain such cliques, causing a problem with identifiability of LEMs. Such genes belonging to the same clique behave in an identical way under all perturbation experiments. Therefore we collapse the whole clique into a single node. This procedure eventually leads to a DAG over a smaller set of nodes. The node that corresponds to the collapsed clique represents a set of genes of the clique, rather than a single gene. 2.1 Learning the network structure We are given a  m  ×  n  perturbation matrix  E  and a vector  Y ∈ R m  as the observed data. To learn a LEM  ( G , β , c )  from observed data, we need to infer the graph  G , corresponding to the signaling network structure, and the vector of contributions  β . To search the graph space we use the two procedures developed and described previously ( Szczurek and Beerenwinkel, 2016 ), and either exhaustively evaluate all possible small graphs of up to five nodes, or greedily search for DAGs that maximize the evaluation score by iteratively adding or removing edges from currently considered graph. In the case when adding an edge results in a cycle, the cycle is collapsed into a single node and in the corresponding matrix  S  the columns for the nodes in the cycle are replaced by a single column. This latter step is sound since, by transitivity, all columns of  S  that correspond to nodes from the cycle are equal to each other. The greedy search is initialized from an empty graph with unconnected nodes, and an additional number of initializations from randomly sampled initial graphs can be set as a parameter by the user. For enhanced LEMs proposed here, we propose two alternative procedures for evaluating the candidate graphs, referred to as  Bayesian  and  Bayesian Information Criterion  approach, respectively. The Bayesian graph evaluation procedure is the same as proposed for original LEMs. We score the graphs using marginal likelihood for Bayesian linear regression ( Bishop, 2006 ). We employ a flat prior on all possible graphs, and assume that the precision parameter  c  is a constant, while the prior distribution of the β parameters, denoted  p ( β | b ) , is a zero mean isotropic Gaussian with precision  b ,  β ∼ N ( 0 , b − 1 I ) . Assuming an empirical Bayes approximation, we take point estimates  b ^ , c ^  of the hyper parameters, and for a candidate graph  G  as its evaluation score we compute the marginal likelihood function  p ( Y | S , b ^ , c ^ ) , which involves integrating over only the parameters β. The point estimates are obtained by maximizing the marginal likelihood in an iterative procedure described previously ( Szczurek and Beerenwinkel, 2016 ). The marginal likelihood used in the Bayesian approach allows comparing models with different number of nodes. For BIC evaluation, we use least squares to solve the linear regression problem [ Equation (2) ] for a given candidate graph  G . Equivalently, we estimate the parameters  β ^  and  c ^  that maximize the likelihood  p ( Y | S , β , c )  [ Equation (3) ]. Due to the fact that cycles are collapsed into single nodes, we need to assure that the evaluation score does not favor larger models, with a larger number of parameters. Each candidate graph is thus scored using the negative BIC,
 2 ln ( p ( Y | S , β ^ , c ^ ) ) − log ( m ) k , 
where  m  is the number of experiments and  k  is the number of nodes after collapsing. Notably, the BIC score is an approximation of the marginal likelihood used in the Bayesian approach ( Bishop, 2006 ). Since its computation requires evaluation of the likelihood only once, it is more time efficient than the iterative procedure required to compute the marginal likelihood. Both scores are maximized in the search. 2.2 Parameter inference For the Bayesian approach, we estimate the contributions β as the mean of their posterior distribution inferred using the Bayesian procedure described previously ( Szczurek and Beerenwinkel, 2016 ). For the BIC approach, we use the least squares estimates. In the case when more than one effect is measured in the experiment (e.g. expression changes of many genes), the evaluation procedure can easily be extended to deal with multidimensional data  Y = { Y 1 , … , Y D }  by assuming independence of the parameters for the  D  different phenotype vectors  Y d ∈ Y . Each  Y d  is assumed to be generated from a shared network structure but with a different contribution vector, following the procedure of  Szczurek and Beerenwinkel (2016) . 2.3 Enhanced model identifiability Before proving identifiability conditions for LEMs, we show several important properties for pairs of DAGs  ( G 1 , G 2 )  over the same set  V  of nodes. Given such a pair and a positive integer  k , we consider a system  Σ ( G 1 , G 2 ) k  of equations of the form
 ∑ x ∈ V X G 1 v x = ∑ x ∈ V X G 2 u x , 
where  X ⊆ V  ranges over all subsets with at most  k  elements. Here variables  v x  and  u x  have indices  x  ranging over genes. If the set  V  has  n  elements, then a solution of the system of equations  Σ ( G 1 , G 2 ) k  is a pair of vectors  v , u ∈ R n  that satisfies these equations. A solution  v ,  u  is said to be  admissible  if for all  a ∈ V ,   v a ≠ 0  and  u a ≠ 0 . Let  k ≥ 1 . We say that two DAGs  G 1 , G 2  are  k - distinguishable  if the system  Σ ( G 1 , G 2 ) k  has no admissible solutions. The reason for considering systems of equations  Σ ( G 1 , G 2 ) k  is the following. Consider two DAGs  G 1  and  G 2  over the same  n -element set of vertices and a perturbation matrix  E  that addresses at most  k  element sets of genes being directly targeted by perturbation experiments. In the case when two vectors  β , γ ∈ R n  form an admissible solution of  Σ ( G 1 , G 2 ) k , we have that  S ( E , G 1 ) β = S ( E , G 2 ) γ , and the LEMs  ( G 1 , β , c )  and  ( G 2 , γ , c )  would not be distinguished by the given data. This follows from the fact that LEMs assume that  Y  is normal distributed around the mean given by the product of the design matrix and the contribution vector, and in this case it would be the same for the two different models. Thus, for a shared parameter  c , these two models would obtain identical likelihood (3). For the sake of space, all proofs of the results presented here are moved to  Supplementary Material . We have the following immediate observation which is used throughout the proof of the main result. 
 Proposition 1. For a subset  X ⊆ V , let  Λ X G = ∩ a ∈ X V a G . For each  k , the system  Σ ( G 1 , G 2 ) k  is equivalent to the system that consists of all equations of the form
 ∑ x ∈ Λ X G 1 v x = ∑ x ∈ Λ X G 2 u x , 
where  X ⊆ V  ranges over all subsets with at most  k  elements. 
 
 Example 1.  In contrast to the situation when the contributions are strictly positive (and all DAGs are then 2-distinguishable, as it was proved for previous LEMs), when we relax this assumption, we have to consider perturbation experiments that target at most three element sets of genes. The following example justifies this claim and the following theorem states it in general. Consider the two DAGs  G 1 , G 2  on a six element set of vertices  V = { 1 , 2 , 3 , 4 , 5 , 6 }  depicted in  Figure 2 . The system  Σ ( G 1 , G 2 ) 2  has  6 + ( 6 2 ) = 21  equations, where each equation corresponds to one perturbation experiment and includes variables perturbed in this experiment (directly or via propagation in the pathway graph) with coefficient 1. Substituting the following values into these equations, it is easy to verify that it is a solution of the system  Σ ( G 1 , G 2 ) 2 :
 (4) v 1 = v 2 = v 5 = v 6 = u 1 = u 2 = u 5 = u 6 = 1 , and     v 3 = v 4 = u 3 = u 4 = − 1. Since all these values are non-zero, it is an admissible solution. In order to distinguish these two DAGs, we need to consider perturbations of three genes. Here we have  V 5 G 1 ∩ V 6 G 1 ∩ V 1 G 1 = { 1 } , but  V 5 G 2 ∩ V 6 G 2 ∩ V 1 G 2 = ∅ . Hence, by Proposition 1, we conclude that every solution has to satisfy  v 1 = 0.  Therefore it is not admissible. 
 Fig. 2. An example pair of two DAGs that are not 2-distinguishable. The DAGs  G 1  and  G 2  are transitive closures of the shown graphs (the transitive edges are not drawn for clarity) 
 Theorem 1.  For every  n ≥ 1 , any pair of two different DAGs on the same  n -element set of vertices is 3-distinguishable. The reader may find it interesting that the above example is the smallest possible. Namely it can be proved that for all  n ≤ 5 , all DAGs over  n -vertex set of genes are 2-distinguishable. 
 
 Theorem 2.  For every  5 ≥ n ≥ 1 , any pair of two different DAGs on the same  n -element set of vertices is 2-distinguishable. We need the following result on uniqueness of solutions. 
 
 Theorem 3. For every  k ≥ 1 , for every finite DAG  G  having  n  vertices, and for every perturbation matrix  E ( k , n ) , the design matrix  S = S ( E ( k , n ) , G )  has rank  n . Therefore, if  m = ∑ i = 1 k ( n i ) , then for every vector  γ ∈ R m , the system of equations  Sx = γ  has at most one solution. We previously showed that the originally introduced LEMs, which assume all contributions  β  are strictly positive, are identifiable from data measured under experiments  E ( 2 , − ) , i.e. with perturbations of all single and all pairs of nodes ( Szczurek and Beerenwinkel, 2016 ). We say that a LEM  M  is  identifiable  from data  Y  if any other LEM  M ′  over the same set of vertices has a different likelihood, i.e. we have  M ≠ M ′  iff  L ( M ) ≠ L ( M ′ ) . 
 
 Theorem 4.  Every LEM is identifiable from data  Y  measured under perturbation experiments  E ( 3 , − ) . LEMs over less than 6 vertices are identifiable from data measured under perturbation experiments  E ( 2 , − ) . Moreover, for identifiability of LEMs it is necessary to assume that the underlying graphs are transitively closed and acyclic. 
 3 Sensitivity of enhanced LEMs to noise and experimental setup We previously demonstrated on simulated data that only extreme levels of noise and for few experimental repeats are an issue for parameter estimation and graph structure learning of original LEMs ( Szczurek and Beerenwinkel, 2016 ). There, the data were simulated according to the assumption that the individual contributions of the perturbed genes to the measured phenotype are strictly positive. Here, we perform similar simulations for enhanced LEMs, but simulating much more realistic, both positive and negative contributions. The aim of this experiment is to show first, that despite the significantly enlarged space for allowed parameter β values, the enhanced LEMs can also be accurately inferred from data. Second, to compare the performance of the two alternative approaches to parameter estimation and graph scoring, Bayesian and BIC (Sections 2.1 and 2.2). Finally, to motivate the new approach by demonstrating dramatically decreased performance of previous LEMs when both positive and negative contributions are allowed. To this end, we generated two test datasets. First, we generated all 28 possible network structures  G  and their corresponding contribution vectors β for LEMs over a set of three nodes {1, 2, 3}. These networks include also graphs where there is a cycle involving two nodes, and they are thus clumped into one node. Consequently, the data simulated in this dataset allows testing whether structure learning in LEMs has the ability to detect cycles and to return graphs with correctly collapsed nodes. To assess the variability of parameter estimates due to different values of β, for each graph we simulated 50 different random β vectors. Each entry β g  was drawn from the Gamma distribution  Γ ( 10 , 9 )  (with parameters shape equal 10 and rate equal 9), and multiplied by –1 or 1, each with probability 0.5. This assured that the simulated β contributions can be both positive and negative, with most values close to 1 or −1 and not 0. For all simulated models, we simulated five versions of the phenotype data vectors  Y , each with a different level of noise [ σ = c − 1 ∈ { 0.01 , 0.05 , 0.1 , 0.25 , 0.5 } , where σ denotes standard deviation of error terms in  Equation (2) ], for five different experimental setups, where the number of times each experiment was repeated was equal  1 , 2 , 3 , 4  or 5. Note that simulating multidimensional  Y = { Y 1 , … , Y D }  would not change the results of parameter estimation, as different parameters are estimated for each  Y d ∈ Y . The performance of structure learning is expected to increase, since the different  Y d  are assumed to be generated from the same structure and increasing  D  would increase the power of structure learning. Hence, one-dimensional vector  Y  can be considered the worst case scenario for structure learning. For the first dataset, we simulated experiments  E ( 2 , 3 )  (with all possible experiments targeting up to two from the three nodes in the graph), which by Theorem 4 allows model identifiability in this case. This dataset was used to evaluate the two enhanced LEM approaches and the previous LEMs performance when model learning is performed using exhaustive search, where all possible model structures can be evaluated. Second, we simulated graphs and their contribution vectors for sets of 10 nodes  { 1 , … , 10 } . The graphs were sampled at random using the network generation function from the nem R package as follows: for each node, the number  k  of outgoing edges between 0 and the total number of nodes (here, 10) was chosen according to a power law distribution with parameter  γ = 2.5 . We then selected  k  nodes having at most 10 ingoing edges and connected the node to them. Finally the graph was transitively closed. For each graph, we simulated 50 corresponding β vectors with entries from the same  ± Γ ( 10 , 9 )  distribution as in the first dataset. For such simulated models, one-dimensional data  Y  was simulated using the same procedure as for the first dataset, with five different noise levels and five different numbers of experimental repeats. Here, however, we simulated experiments  E ( 3 , 10 )  (with all possible experiments targeting directly up to three from the set of 10 nodes in the graph). By Theorem 4, this assured model identifiability from the data. The second dataset was used to assess the performance of learning larger models, which in LEM is performed using the heuristic of greedy search over the large space of possible graphs (exhaustive search is computationally intractable for graphs with 10 nodes). Greedy search was performed with three random initializations in addition to the initialization from an empty graph. 
 Figure 3  summarizes performance of the Bayesian approach of enhanced LEMs to parameter estimation and network structure learning on both datasets (using exhaustive and greedy structure learning). When simulated LEMs with three nodes and reasonable noise levels ( σ &lt; 0.25 ) are inferred, the correlation between the estimated and true  β  values is concentrated at 1 for almost all simulated models, with just a few outliers with lower correlation. Larger noise does not change the distribution of obtained correlations, but only results in a larger number of outliers. Increasing the number of repeated experiments reduces the number of outliers with low correlation for all levels of noise ( Fig. 3A ). Both sensitivity (fraction of edges present in the simulated network graph that are correctly identified as such;  Fig. 3B ) and specificity (fraction of absent edges that are correctly identified as such;  Fig. 3C ) of exhaustive search are almost perfect already for one experimental repeat; with just a few outliers for extreme noise levels. Again, the number of outliers with lower sensitivity and specificity values decreases with more experimental repeats. For large noise, sensitivity is a bit lower and is more affected by repeat number than specificity. Compared to these results, for simulated LEMs with 10 nodes and graph inference using greedy search, the distribution of correlation between true to estimated β values over simulations has more outliers with low correlation. Still, the median of this distribution, even for very large noise and low number of repeats, is close to 1 ( Fig. 3D ). Similarly, the distributions of sensitivity ( Fig. 3E ) and specificity ( Fig. 3F ) have more outliers with lower values, but the medians remain close to 1.
 Fig. 3. Accurate parameter estimation and network structure learning for enhanced LEMs using the Bayesian approach. ( A – C ) Performance for three-node LEMs and exhaustive search in graph space. (A) Box plots summarizing distribution (showing 25th, 50th and 75th percentiles: horizontal bars, and 1.5 interquartile ranges: vertical line ends) of the correlation between the true β values used to simulate the data and the estimated values ( y -axis) for increasing number of experimental repeats ( x -axis) and for increasing noise (colors). The estimated β values are very close to the true ones for almost all simulations, with only a few outliers. Both sensitivity (B) and specificity (C) of true edge recovery are close to 1 for almost all simulated graphs, and are lowered only for extreme noise values and for few experimental repeats. ( D – F ) The same performance analysis as for 3-node LEMs in (A–C) but for 10-node LEMs using greedy search in graph space. For larger graphs and greedy search, the performance of parameter estimation decreases only slightly, with median correlation remaining close to 1. Compared to exhaustive search, the sensitivity and specificity of edge recovery in graphs is also only slightly lowered, and has more outliers. The median values of both sensitivity and specificity are close to 1 Compared to the Bayesian approach, the BIC approach shows similarly excellent performance of parameter estimation, as well as sensitivity and specificity of graph learning, for both datasets ( Supplementary Fig. S2 ). Only a very detailed comparison would reveal that the distribution of sensitivity values for the exhaustive search across three node graphs extends toward slightly lower values and has more outliers with low values in the case of the BIC approach. The difference between the Bayesian and the BIC approaches, however, becomes more apparent when fraction of perfectly learned simulated graphs is compared, and becomes very important when the run time is considered ( Fig. 4 ). We define a graph to be learned perfectly, when the set of inferred edges is identical to the set in the simulated graph. Compared to the BIC approach, using exhaustive search on small graphs, a larger fraction of graphs is learned perfectly with the Bayesian approach ( Fig. 4A and B ). The advantage of the Bayesian approach is especially prominent for low numbers of experimental repeats. The difference between the approaches is not apparent when greedy search over larger graphs is performed ( Fig. 4D and E ). In this case for both approaches and all experimental setups the fraction of perfectly learned graphs is decreased compared to exhaustive search over small graphs. Still, very high sensitivity and specificity results visualized in  Figure 3  demonstrate that the identified graphs are close to the true simulated graphs, also for greedy search. Thus, overall, the differences in performance between the Bayesian and the BIC approaches are due to the BIC approach more often missing or inserting only a small number of edges.
 Fig. 4. Comparison of the Bayesian and the BIC approaches of enhanced LEMs, and the previous LEMs. Fraction of simulated three-node graphs that were recovered perfectly (with no missing and no added edges) using exhaustive search and the Bayesian ( A ) approach of enhanced LEMs is larger than when the BIC approach ( B ) was applied, especially for low number of experimental repeats ( x -axis) and large noise (marked with colors). Fraction of simulated 10-node graphs that were recovered perfectly using greedy search and the Bayesian ( D ) approach is similar to the fraction when the BIC approach ( E ) was applied. The fraction of both 3-node graphs ( C ) and the 10-node graphs ( F ) that were perfectly recovered from the same data using previous LEMs is significantly lower than for the enhanced LEMs. Both the run time in CPU sec ( y -axis) of 3-node model inference using exhaustive search across 50 simulations ( G ) and the run time of 10-node enhanced LEM inference using greedy search ( H ) is much larger for the Bayesian approach, compared to the BIC approach The run time comparison, on the other hand, shows a huge advantage of the BIC approach ( Fig. 4G and H ). To compare the run times, we simulated data for and inferred 50 random LEMs with 3 nodes, and 50 random LEMs with 10 nodes. The median CPU time used for exhaustive search over the small models with the Bayesian approach is 1.5 CPU seconds and is 15.4 times longer than the median time (0.0985 s) used when the BIC approach is applied ( Fig. 4G ). For greedy search (with one initialization from empty graph) and the Bayesian approach, the median CPU time is 28.612 s and is 6.4 times longer than the median 4.466 s of CPU time needed for the BIC approach ( Fig. 4H ). In summary, the two approaches perform similarly well in terms of parameter estimation and structure learning, with slight advantage of the Bayesian approach, but their run times largely differ, with the advantage of the BIC approach. Finally, we assessed the performance of previous LEMs ( Szczurek and Beerenwinkel, 2016 ) using exhaustive search over 3-node graphs and greedy search over 10-node graphs, on the two simulated datasets, respectively ( Supplementary Fig. S3 ). Importantly, both datasets were generated allowing both positive and negative values of the parameters corresponding to the gene contributions to the phenotype, which is against the assumptions of the previous version of LEMs. Previous LEMs assumed that these values are strictly positive and that they can be interpreted as absolute magnitudes of the contributions instead of their actual values. Consequently, given the simulated data vectors  Y , which contained both positive and negative entries, we first transformed them into their absolute values  | Y |  prior to application of previous LEMs. Compared to excellent results of enhanced LEMs on this data, the previous LEMs perform poorly ( Supplementary Fig. S3 ). As can be expected, with wrong assumption about the parameter values, parameter estimation using previous LEMs fails, with median correlation between the true and the estimated contribution values equal 0 ( Supplementary Fig. S3A  and D). The performance of structure learning is also poorer than that of the enhanced LEMs, with the greedy search performing worse than exhaustive search. When exhaustive search over three-node graphs is applied, median sensitivities are close to 1 for lower values of noise ( σ ≤ 0.1 ; Supplementary Fig. S3B ). The specificity, however, is considerably lower than obtained by enhanced LEMs, regardless of noise and experimental repeat values ( Supplementary Fig. S3C ). The fraction of three-node graphs learned perfect by previous LEMs from this data is much lower than obtained when enhanced LEMs were applied ( Fig. 4C ). Compared to these results, when greedy search over 10-node networks is performed, sensitivity drops drastically ( Supplementary Fig. S3E ), while specificity increases ( Supplementary Fig. S3F ), indicating that for previous LEMs the greedy search identifies structures with too few edges. Consequently, the fraction of structures learned perfect drops down to zero ( Fig. 4F ). These results indicate very clearly the need for introduction of enhanced LEMs when more realistic data are to be modeled accurately. 4 Application to infection network inference from siRNA screening data To demonstrate the performance of enhanced LEMs on real data, we utilized on- and off-targets of siRNA interventions as combinatorial perturbations. We analyzed siRNA kinome screen by  Ramo  et al.  (2014)  carried out in the HeLa ATCC-CCL-2 cell line using Qiagen (Human Kinase siRNA Set V4.1) reagents, with four different siRNAs per on-target gene. The measured phenotype was infectivity of the cells with the pathogen B.  henselae , corresponding to a rate of infection per well, derived from image features collected under the experiments ( Ramo  et al. , 2014 ). Before modeling, we removed readouts from control and bad quality wells and filtered out siRNAs which target essential genes (cell killers). Next, the data were normalized using the B-scoring algorithm ( Brideau  et al. , 2003 ) in order to remove systematic within-plate effects. At the final preprocessing step, the data were Z-scored to eliminate experimentally introduced cross-plate biases. Prediction of off-targets of the siRNAs on all genes was performed using TargetScan ( Lewis  et al. , 2005 ). In any siRNA experiment, the measured phenotype can be assumed to be a combination of contributions of all perturbed genes, i.e. also all remaining genes outside of the network. Therefore to be able to apply LEMs to infer a signaling network of interest (here the infection network), the phenotype needs to be corrected by removing the contributions from the remaining genes. To this end, we used the Lasso ( Tibshirani, 1994 ) to estimate contributions of all genes to the phenotype measured in all experiments, assuming the genes were independent (not connected in any network). Since only the infection network was stimulated in the study ( Ramo  et al. , 2014 ), this assumption is valid for all remaining genes outside of the infection network. Next, the phenotype was corrected by subtracting the estimated contributions of those remaining genes which were perturbed in each experiment. Formally, this procedure is motivated in  Supplementary Material . We next constructed the experiments matrix to contain only eight columns, corresponding to the same eight genes in the B.  henselae  infection network studied by  Srivatsa  et al.  (2018) , and 44 000 rows, corresponding to such siRNAs that either on- or off-targeted any of the eight genes. Note our approach to constricting the experiments matrix is different from  Srivatsa  et al.  (2018) , who used only data from 35 experiments corresponding to siRNAs on-targeting the genes in the network, and is intended to increase the power of our analysis. Finally, we applied enhanced LEMs using greedy search with 100 random initializations (in addition to the initialization from an empty graph) using Bayesian and BIC approaches to infer interactions in the infection network from the corrected phenotype ( Fig. 5A and B ). Using the Bayesian approach, the algorithm identified a network containing a cycle involving CDC42, RAC1, and TLN1, with the connection between CDC42 and RAC1 known to be involved in the regulation of actin cytoskeleton ( Verma and Ihler, 2002 ) ( Fig. 5A ). Transitive closure results in a clique connecting all of these genes together, which is collapsed into a single node. Out of eight identified internode interactions, seven found support in the literature. Interactions between Paxillin (PXN) and talin1 (TLN1) and between Paxillin (PXN) and vinculin (VCL) were assigned high evidence scores in the curated network of integrin anhesome ( Horton  et al. , 2015 ). Interaction between focal adhesion kinase (FAK) and Cdc42 were determined to play a role in the context of cellular motility by several studies ( Ito  et al. , 1982 ;  Zhang  et al. , 2004 ;  Zhao and Guan, 2011 ). Src-dependent activation of Rac1 was studied in the context of glioma tumorigenesis ( Feng  et al. , 2011 ). The recovered interaction between FAK and integrin β1 (ITGB1) is known to be active in adhesion signaling ( Huveneers and Danen, 2009 ). The identified network contains also the established interaction FAK– Src ( Mitra  et al. , 2005 ). In addition to the previously established interactions, our model contains a novel one, VCL → ITGB1, constituting a novel hypothesis about the mechanisms of B.  henselae  infection.
 Fig. 5. Structure and parameters of the B.  henselae  infection network inferred using LEMs from the siRNA screen data of  Ramo  et al.  (2014) . Nodes are labeled with gene names of the network members. The nodes labeled by several gene names represent cliques involving these genes. The color of bold edges indicates support found in the literature. The dashed edges represent the individual contributions of the network nodes to the infectivity phenotype, and are labeled by their values, scaled by 10 3 . ( A ) Enhanced LEM model learned from this data using the Bayesian approach. ( B ) Enhanced LEM model learned using the BIC approach. ( C ) Previous LEM model learned from this data Enhanced LEMs using the BIC approach identified a network that is similar to the network found using the Bayesian approach, but more cyclic ( Fig. 5B ). The network has two disjoint cycles, which were collapsed into two unconnected clique nodes due to transitive closure. One cycle contains ITGB1 and FAK, which are known to interact in adhesion signaling ( Huveneers and Danen, 2009 ). An edge between FAK and ITGB1 was also found using the Bayesian approach. The second cycle contains all remaining genes of the B.  henselae  infection pathway. This cycle contains the smaller cycle containing CDC42, RAC1 and TLN1, suggested also by the Bayesian approach. In addition, BIC placed in this cycle also VCL and PXN, which are known to interact with each other. Finally, the previous LEM model applied to the same data found a structure also containing two cycles, but containing different subsets of genes. To be able to apply previous LEMs, we transformed the infectivity phenotype  Y , which had both negative and positive values, to its absolute value  | Y | . Since the Bayesian approach had a slightly better performance in our simulation study (Section 3), and much better performance than previous LEMs, we consider the model found using the Bayesian approach ( Fig. 5A ) most likely to be the closest to the true biological network. In contrast to other effects models, LEM infers the contributions each signaling node has to the measured phenotype upon its perturbation. Additionally, in contrast to the previous LEMs, the enhanced LEMs account for the positive or negative direction of these contributions. In application to the B.  henselae  network, using both the Bayesian and the BIC approaches, the enhanced LEMs estimated the individual contributions of FAK and ITGB1 to infectivity are strongly negative and most significant among the genes in this network. This indicates that FAK and ITGB1 play the most important roles for successful infection by the pathogen, in agreement with previous findings ( Truttmann  et al. , 2011 ). 5 Discussion and conclusions This paper contributes two main results. First, it introduces an important extension to the previously proposed LEMs. By allowing both negative and positive contributions of the network members to the measured phenotype, model assumptions are much closer to reality than in the original model formulation. The enhanced model expresses how network members jointly regulate the downstream effects, where some of the members may up- and others may down-regulate these effects. Thus, using the enhanced LEMs, we can now both learn the graph representing the network structure, and a more involved representation of the regulatory layer than before. Second, the paper brings a proof of identifiability of enhanced LEMs from combinatorial experiments where single, pairs, and triplets of network nodes are performed. For small enhanced LEMs, with up to five nodes, we show identifiability with perturbations of only up to two nodes. We meant here that the comparison of the way the regulatory layer is modeled in NEMs ( Markowetz  et al. , 2005 ), original LEMs ( Szczurek and Beerenwinkel, 2016 ) and the here introduced enhanced LEMs reveals that the models become increasingly expressive. The binary information of whether a certain effect is regulated by a certain single network gene or not, modeled in NEMs, was replaced by modeling the effects as linear combinations of positive contributions from all network genes in LEMs, with which the current model can be any real values but not zero. Increase in expressiveness, however, clearly comes with a price of larger data required for model learning. While NEMs are inferred from only perturbations of all single network nodes, LEMs already require combinatorial perturbation data of single and double nodes. Enhanced LEMs with more than five nodes need not only single and double, but also triple perturbations. We anticipate that such combinatorial experiments will become increasingly available. Technological advances in automated RNAi screening ( Lambeth  et al. , 2010 ) make such combinatorial experimental regimes more and more efficient. As our and others ( Srivatsa  et al. , 2018 ), examples of successful inference of interactions in the B.  henselae  infection network from siRNA screening data suggest, also the off-target perturbations can be utilized as a rich source of combinatorial interventions on signaling networks. This approach, however, depends on the ability to accurately predict the off-targets, which can be very challenging. Currently, our model does not account for false positive and false negative predictions. Finally, it considers only binary perturbation states and is not applicable to pooled siRNA reagents. Taking account of the prediction errors as well as strength of the perturbations would be a valuable extension of the approach. Although enhanced, the LEMs introduced in this paper still have several limitations. First, LEM assumes that all network members have nonzero contribution to the observed downstream phenotypes, which is biologically less likely especially for the kinases high up in the signaling cascade, or the signaling receptors, corresponding to the roots of the network graph. Second, LEM is applicable only to small networks. Efficiency of the greedy model search procedure could be improved by maintenance of tabu lists and avoiding recently visited neighbors, as well as storage of only the minimum necessary information about the neighbors. Third, the requirement of combinatorial perturbations being available for all triplets of genes in the network is a limiting factor in the applicability of LEM to larger networks. One way to deal with this problem is to explore the equivalence classes of LEMs, which could be obtained when fewer combinations of perturbations would be available, and allow the method to return more than one equally scoring network. Another way, as proposed in this work, is to take advantage of the combinatorial perturbations resulting from multiple off-targets of siRNAs. Still, even with these limitations, the enhanced LEMs do bring significantly improved expressiveness of the model in comparison not only to the original LEM, but also to other methods. With the proved identifiability requirements, it is clear which experiments need to be done to be able to reliably, and, as we show in simulations and in application to infection network recovery from siRNA screening data, accurately infer both network structure and the regulatory layer from the phenotypes measured downstream. Funding This work was supported by the National Science Centre, Poland [2015/19/P/NZ2/03780 to J.T. and E.S.]. This project has received funding from the European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No. 665778. 
 Conflict of Interest : none declared. Supplementary Material btz334_Supplementary_Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Geometric potentials from deep learning improve prediction of CDR H3 loop structures</Title>
    <Doi>10.1093/bioinformatics/btaa457</Doi>
    <Authors>Ruffolo Jeffrey A, Guerra Carlos, Mahajan Sai Pooja, Sulam Jeremias, Gray Jeffrey J</Authors>
    <Abstract/>
    <Body>1 Introduction The adaptive immune system of vertebrates is responsible for coordinating highly specific responses to pathogens. In such a response, B cells of the adaptive immune system secrete antibodies to bind and neutralize some antigen. The central role of antibodies in adaptive immunity makes them attractive for the development of new therapeutics. However, rational design of antibodies is hindered by the difficulty of experimental determination of macromolecular structures in a high-throughput manner. Advances in computational modeling of antibody structures provides an alternative to experiments, but computations are not yet sufficiently accurate and reliable. Antibody structure consists of two sets of heavy and light chains that form a highly conserved framework region ( F c ) and two variable regions responsible for antigen binding ( F v ). The structural conservation of the  F c  is functionally significant, enabling the recognition of different antibody isotypes by their receptors and the  F c  lends well to homology modeling. The  F v  contains several segments of sequence hypervariability that provide the structural diversity necessary to bind a variety of antigens. This diversity is largely focused in six β-strand loops known as the complementarity determining regions (CDRs). Five of these loops (L1–L3, H1 and H2) typically fold into one of several canonical conformations ( Chothia  et al. , 1989 ) that are predicted well by existing methods ( North  et al. , 2011 ). However, the third CDR loop of the heavy chain (H3) is observed in a diverse set of conformations and remains a challenge to model ( Almagro  et al. , 2014 ;  Berrondo  et al. , 2014 ;  Fasnacht  et al. , 2014 ;  Maier and Labute, 2014 ;  Shirai  et al. , 2014 ;  Weitzner  et al. , 2014 ;  Zhu  et al. , 2014 ). Although the CDR loops are sometimes flexible and context-dependent, the change is typically small (&lt;1 Å) between bound and unbound forms ( Sela-Culang  et al. , 2012 ). Because each antibody CDR H3 sequence evolves in an individual organism, evolutionary sequence history is not generally available [although there are exceptions ( Eshleman  et al. , 2019 ; Wu  et al. , 2011)]. Application of deep learning techniques has yielded significant advances in the prediction of protein structure in recent years. At CASP13, AlphaFold ( Senior  et al. , 2020 ) and RaptorX ( Xu, 2019 ) demonstrated that inter-residue distances could be accurately learned from sequence and coevolutionary features. Both approaches used deep residual network architectures with dilated convolutions to predict inter-residue distances, which provide a more complete structural description than contacts alone. trRosetta built on this progress by expanding beyond distances to predict a set of inter-residue orientations ( Yang  et al. , 2020 ). This rich set of inter-residue geometries allows trRosetta to outperform leading approaches on the CASP13 dataset, even with a shallower network ( Yang  et al. , 2020 ). The effectiveness of inter-residue orientations for discriminating protein structures has also recently been demonstrated by methods such as SBROD and KORP ( Karasikov  et al. , 2019 ;  López-Blanco and Chacón, 2019 ). SBROD is a single-model quality assessment function that considers inter-residue interactions, backbone atom interactions, hydrogen bonding and solvent-solute interactions. Those features are extracted from a set of decoys from various CASP experiments and the SBROD scoring function is trained via ridge regression ( Karasikov  et al. , 2019 ). KORP is a knowledge-based potential constructed from a set of six inter-residue geometric descriptors similar to those of trRosetta ( López-Blanco and Chacón, 2019 ). Structures are scored according to a 6D joint probability distribution extracted from a database of non-redundant protein structures. Our work expands on the progress in general protein structure prediction by applying similar techniques to a challenging problem in antibody structure prediction. Specifically, we propose DeepH3, a deep residual network that learns to predict inter-residue distances and orientations from antibody heavy and light chain sequence alone. When compared to state-of-the-art scoring methods, DeepH3 can identify near-native CDR H3 loops more accurately. When used for  de novo  prediction of CDR H3 loop structures, DeepH3 produces lower-root-mean-squared distance (RMSD) structures than existing methods. 2 Materials and methods 2.1 Overview DeepH3 is a deep residual network ( He  et al . 2016 ) that learns to predict inter-residue distances and orientations from antibody heavy and light chain sequences. The architecture of DeepH3 draws inspiration from RaptorX ( Wang  et al. , 2017 ;  Xu, 2019 ), which performed well on general protein structure prediction at CASP13. The relative scarcity of structural data for antibodies compared to general proteins presents challenges (as in any subproblems of structure prediction). We alleviate this limitation by reducing the depth of our network compared to previous methods, and we verify the generalization by examining performance on a highly diverse benchmark dataset. The outputs of DeepH3 are converted into geometric potentials in order to better discriminate between CDR H3 loop structures (decoys) generated using a standard homology modeling approach ( Marze  et al. , 2016 ) and to predict new CDR H3 loop structures  de novo . 2.2 Antibody structure datasets 2.2.1 Benchmark dataset The Rosetta antibody benchmark dataset consists of 49  F v  structures with CDR H3 loop lengths ranging from 9 to 20 residues ( Marze  et al. , 2016 ;  Weitzner and Gray, 2017 ). These structures were selected from the PyIgClassify database ( Adolf-Bryfogle  et al. , 2014 ) based on their quality, with each having resolution of 2.5 Å or better, a maximum  R  value of 0.2 and a maximum  B  factor of 80.0 Å 2  for every atom ( Marze  et al. , 2016 ;  Weitzner and Gray, 2017 ). The diversity of the set is enhanced by ensuring that no two structures share a common CDR H3 loop sequence, but the set is limited by the restriction to structures from humans and mice. 2.2.2 Training dataset The training dataset for this work was extracted from SAbDab, a curated database of all antibody structures in the Protein Data Bank ( Dunbar  et al. , 2014 ). We enforced thresholds of 99% sequence identity and 3.0 Å resolution to produce a balanced, high-quality dataset. This high sequence identity cutoff was chosen due to the high conservation of sequence characteristic of antibodies. In cases where multiple chains existed for the same structure, only the first chain in the PDB file was used. Finally, any structures present in the Rosetta antibody benchmark dataset were removed. These steps resulted in 1433 structures, of which a random 95% were used for model training and 5% were used for validation. This small validation set was found to be sufficient to control for overfitting. Note that testing is carried out on an independent benchmark sharing no structures with the training/validation sets. 2.3 Learning inter-residue geometries from antibody sequence 2.3.1 Input features Unlike most comparable networks, DeepH3 relies only on amino acid sequence as input. For general protein structure prediction, current methods typically utilize some combination of multiple sequence alignments (MSAs), sequence profiles, coevolutionary data, secondary structures, etc. ( Senior  et al. , 2020 ;  Wang  et al. , 2017 ,  2018 ;  Xu, 2019 ;  Yang  et al. , 2020 ). While these additional input features provide rich information for general protein structure predictions, each antibody evolves independently in one single organism, and we rarely have relevant evolutionary histories for CDR H3 loop sequences. Thus, we omit sequence alignment data like MSAs. DeepH3 takes as input a one-hot encoded sequence formed by concatenating the target heavy and light chains ( F v ) sequences. A chain delimiter is added to the last position in the heavy chain, resulting in an input of dimension  L  × 21, where  L  is the cumulative length of the heavy and light chain sequences. 2.3.2 Inter-residue geometries In addition to inter-residue distances, DeepH3 is also trained to predict the set of dihedral and planar angles previously proposed for trRosetta ( Yang  et al. , 2020 ). For two residues  i  and  j , the relative orientation is defined by six parameters [ d , ω, θ ij , θ ji , φ ij  and φ ji ,  Figure 1A and B , adapted from ( Yang  et al. , 2020 )]. The distance ( d ) is defined using C β  atoms or for glycine residues, C α . Distances were discretized into 26 bins, with 24 in the range of [4, 16 Å] and two additional bins for all distances below 4 Å or above 16 Å. The dihedral angle ω is formed by atoms C α i , C β i , C β j  and C α j , and the dihedral angle θ ij  is formed by atoms N i , C α i , C β i  and C β j . Both dihedral angles were discretized into 26 equal-sized bins in the range of [–180, 180°]. The planar angle φ ij  is formed by atoms C α i , C β i  and C β j . Planar angles were discretized into 26 equal-sized bins in the range of [0, 180°]. Orientation angles were not calculated for glycine residues, due to the absence of the C β  atom.
 Fig. 1. Architecture of DeepH3 deep residual neural network. ( A ) Illustration of the distance  d  and dihedral ω for two residues. ( B ) Illustration of the dihedrals θ 12  and θ 21  and planar angles θ 12  and φ 21  for two residues. ( C ) Architecture diagram of residual neural network to learn inter-residue geometries from concatenated antibody  F v  chain sequences 2.3.3 Network architecture DeepH3 applies a series of 1D and 2D convolutions to the aforementioned sequence input feature to predict four inter-residue geometries, as diagrammed in  Figure 1C . The first 1D convolution (kernel size of 17) projects the  L  × 21 input features up to an  L  × 32 tensor. Next, the  L  × 32 tensor passes through a set of three 1D residual blocks (two 1D convolutions with kernel size of 17), which maintain dimensionality. Following the 1D residual blocks, the sequential channels are transformed to pairwise by redundantly expanding the  L  × 32 tensor to dimension  L  ×  L  × 32 and concatenating with the transpose, resulting in a  L  ×  L  × 64 tensor. This tensor passes through 25 2D residual blocks (two 2D convolutions with kernel size of 5 × 5) that maintain dimensionality. Dilation of the 2D convolutions cycles through values of 1, 2, 4, 8 and 16 every five blocks (five cycles in total). Each of the preceding convolutions is followed by a batch normalization. Next, the network branches into four paths, which each apply a 2D convolution (kernel size of 5 × 5) to project down to dimension  L  ×  L  × 26 (for 26 output bins). Symmetry is enforced for the  d  and ω branches after the final convolution by summing the resulting tensor with its transpose. The four resulting  L  ×  L  × 26 tensors are converted to pairwise probability distributions for each output using the softmax function. DeepH3 was implemented using PyTorch ( Paszke  et al. , 2019 ) and is freely available at  https://github.com/Graylab/deepH3-distances-orientations . 2.3.4 Training Categorical cross-entropy loss was calculated for each output tensor and the resulting losses were summed with equal weight before back propagation. The Adam optimizer was used with an initial learning rate of 0.01 and reduction of learning rate upon plateauing of total loss. Dropout was used after the last 2D residual block, with entire channels being zeroed out at 20% probability. The network was trained using 95% of antibody dataset described above (1388 structures) for 30 epochs. Each epoch utilized the entire training dataset, with a batch size of 4. Training lasted about 35 h using one NVIDIA Tesla K80 GPU on the Maryland Advanced Research Computing Center (MARCC). 2.4 Network predictions as geometric potentials 2.4.1 Implementation We applied DeepH3 to each sequence in the Rosetta antibody benchmark dataset to produce pairwise probability distributions for the four output geometries. Distributions for pairs of residues that did not include a member of the CDR H3 (according to Chothia number) loop were discarded. Additionally, pairs of residues for which the maximum probability bin of the distance output was greater than 12 Å were discarded to focus on local interactions that are likely to carry biophysical meaning. We also disregarded those predicted distributions that were not informative enough, chosen as those with a maximum probability below 10%. The remaining distributions were converted to potentials by taking the negative natural log of each output bin probability. Continuous, differentiable Rosetta constraints (AtomPair for  d , Dihedral for ω and θ and Angle for φ) were created for each potential using the built-in spline function. Within Rosetta, a histogram corresponding to each pairwise potential is fit to a cubic spline. These constraint functions are used calculate the DeepH3 energy term for each structure. 2.4.2 CDR H3 loop discrimination To test the effectiveness of predicted geometric potentials for discriminating between near-native CDR H3 loops, we collected a set of 2800 decoy structures generated by RosettaAntibody for each of the 49 Rosetta antibody benchmark targets ( Marze  et al. , 2016 ). These structures were generated by homology modeling, with decoys for each target assuming various heavy/light-chain orientations and non-H3 CDR loop conformations ( Marze  et al. , 2016 ;  Weitzner  et al. , 2017 ). After scoring each structure with DeepH3, we compared the discrimination performance to three other state-of-the-art scoring methods: SBROD ( Karasikov  et al. , 2019 ), KORP ( López-Blanco and Chacón, 2019 ) and the ref2015 full-atom energy function (referred to as Rosetta energy) ( Alford  et al. , 2017 ). 2.4.3 Discrimination score The discrimination score is a common metric for measuring the success of structure prediction calculations by assessing whether the minimum energy structures are near-native, with a lower value being indicative of a more successful prediction ( Weitzner and Gray, 2017 ). In order to compare between different energy schemes, we first scale the scores for all decoy structures such that the 95th percentile energy has a value of 0.0 and the 5th percentile energy has a value of 1.0. The discrimination score is then calculated as ( Conway  et al. , 2014 ):
 (1) D = ∑ r ϵ { 1,1.5,2 , 2.5,3 , 4,6 } min i , RMSD ( i ) ϵ [ 0 , r ] ⁡ E i - min i , RMSD ( i ) ϵ [ r , ∞ ] ⁡ E i where  r  is the RMSD cutoff in Å,  E i  is the scaled energy for the  i -th decoy structure, and the discrimination score,  D , is the sum of the energy differences for the best scoring decoys above and below each RMSD cutoff. 2.5  De novo  prediction of CDR H3 loop structures 2.5.1 DeepH3 prediction on crystal  F v  framework We applied the Rosetta LoopModeler protocol ( Mandell  et al. , 2009 ;  Stein and Kortemme, 2013 ) to each target in the Rosetta antibody benchmark to build the CDR H3 loop onto the  F v  crystal structure (script provided as  Supplementary Material ). Prior to modeling, the crystallographic loop was extended by setting ϕ and ψ angles to 180° to emulate a blind prediction. Throughout the modeling process, the KIC algorithm was guided only by DeepH3 energy, with all Rosetta energy function terms disabled. For each target, 500 decoys were generated. We elected to use a relatively low number of decoys after observing faster convergence with DeepH3 energy than is typical for Rosetta energy. 2.5.2 TrRosetta heavy chain prediction The most similar approach to DeepH3 is trRosetta for general protein structure prediction. To better understand the impacts of designing a network specifically for antibody structures, we tested the performance of trRosetta on the Rosetta antibody benchmark using the public trRosetta server ( Yang  et al. , 2020 ). Because trRosetta was designed to predict the structure of single-chain proteins, we submitted only heavy chain sequences (i.e. omitting the light chain). The five resulting structures were aligned to the heavy chain in the crystal structure to measure the RMSDs of the CDR H3 loop heavy atoms. 3 Results 3.1 DeepH3 accurately predicts inter-residue geometries To evaluate the accuracy of DeepH3’s predictions, we applied our model to the entire Rosetta antibody benchmark dataset (not seen during training or validation). For residue pairs involving a CDR H3 loop residue, the predicted values for each geometry are plotted against experimental structure values in  Figure 2 . We limit our analysis to pairs including an H3 loop residue to ensure that DeepH3 is effectively learning the most variable regions of the antibody structure, rather than just the conserved framework. DeepH3 displays effective learning across all outputs; the Pearson correlation coefficients ( r ) for  d  and φ were 0.87 and 0.79, respectively, and the circular correlation coefficients ( r c ) for dihedrals ω and θ were 0.52 and 0.88, respectively.
 Fig. 2. Accuracy of predicted inter-residue geometries. Pearson correlation coefficients (for  d  and φ) and circular correlation coefficients (for ω and θ) are calculated between DeepH3 predictions and experimental values 3.2 Geometric potentials discriminate near-native CDR H3 loop structures To evaluate the effectiveness of DeepH3 energy for identifying near-native structures, predicted DeepH3 geometric histograms were converted to potentials (Section 2) that were then evaluated on RosettaAntibody generated structure decoys. Reported RMSD values are measured between the heavy atoms of CDR H3 loops after aligning the  F v  backbone heavy atoms. When the best-scoring structures (top 1) by Rosetta energy and DeepH3 energy were compared, DeepH3 selected better-, same- and worse-RMSD structures for 33, 6 and 10 out of 49 targets, respectively, with an average RMSD improvement of 1.4 Å ( Fig. 3A ). When the set of five best-scoring structures (top 5) by Rosetta energy and DeepH3 energy were considered, DeepH3 energy identified a better-, same- and worse-RMSD structures for 24, 16 and 9 out of 49 targets, respectively, with an average RMSD improvement of 0.8 Å ( Fig. 3B ). We also compared the ability of Rosetta energy and DeepH3 energy to discriminate between decoys for each benchmark target ( Fig. 3C ,  Table 2 ). The mean discrimination scores for Rosetta energy and DeepH3 energy across the benchmark were 1.7 and –12.2, respectively, indicating that DeepH3 was much more successful in general. When individual targets are considered, DeepH3 energy was successful in discriminating between decoys for 36 out of 49 targets, while Rosetta energy was successful for only 15 out of 49 targets.
 Fig. 3. Effectiveness of predicted inter-residue geometries for decoy discrimination. ( A ,  B ) Comparison of the quality of structures selected by Rosetta energy and DeepH3 energy (using all geometric potentials). The quality of structures is considered the same if the difference in RMSD is within ±0.25 Å, indicated with dashed lines. (A) DeepH3 energy selected better-, same- and worse-RMSD structures for 33, 6 and 10 out of 49 targets, respectively, when the best-scoring structures were compared (top 1). (B) When the set of five best-scoring structures were considered (top 5), DeepH3 energy identified better-, same- and worse-RMSD structures for 24, 16 and 9 out of 49 targets, respectively. ( C ) Comparison of the discrimination scores for Rosetta energy and DeepH3 energy Table 2. Discrimination score metrics for DeepH3 energy and several state-of-the-art energy functions Energy terms Successful Unsuccessful Mean  D SBROD 8 41 3.7 KORP 21 28 0.2 Rosetta 15 34 1.7 DeepH3 36 13 –12.2 d 32 17 –7.4 ω 32 17 –7.8 θ 38 11 –15.6 φ 36 13 –9.6 
 Notes : DeepH3 energy is further divided into individual inter-residue geometries. Negative discrimination scores,  D , are considered successful and positive are considered unsuccessful. Table 3. Performance of geometric potentials versus Rosetta energy function for selecting low-RMSD antibody decoys Energy terms Top 1 Top 5 Better Same Worse ΔRMSD Better Same Worse ΔRMSD d 27 9 13 –1.1 22 14 13 –0.5 ω 30 8 11 –1.3 26 14 9 –0.4 θ 31 7 11 –1.5 23 13 13 –0.7 φ 29 7 13 –1.4 26 14 9 –0.8 
 Notes : Top-1 metrics compare the RMSD of the best-scoring structure by Rosetta energy against that of a given DeepH3 potential. Top-5 metrics compare the lowest-RMSD structure among the five best-scoring structures selected by Rosetta energy and that of a given DeepH3 potential. The average difference in RMSD between the structures selected by a given DeepH3 potential and Rosetta energy is reported as ΔRMSD (Å). To compare against alternative state-of-the-art methods, we also scored the RosettaAntibody decoy using SBROD ( Karasikov  et al. , 2019 ) and KORP (L ó pez-Blanco and Chac ó n,  2019 ) ( Tables 1  and  2 ). In a comparison of the top-rated structures from the decoy set, DeepH3 demonstrated improvements over SBROD (38 targets were better, 6 same and 5 worse; average ΔRMSD of –1.8 Å). The comparison of the five top-scoring structures was similar (35 better, 11 same and 3 worse; ΔRMSD = –1.1 Å). In general, SBROD was unsuccessful in discriminating near-native decoys, with only 8 out of 49 benchmark targets having a negative discrimination score and an average  D  of 3.7. DeepH3 also outperformed KORP among best-scoring structures (32 better, 10 same and 7 worse; ΔRMSD = –0.9 Å) and when comparing the lowest-RMSD structure among the five best-scoring decoys for each target (25 better, 18 same and 6 worse; ΔRMSD = –0.6 Å). KORP was generally unsuccessful in discriminating near-native CDR H3 loop decoys, with only 21 out of 49 targets having negative discrimination scores and an average  D  =   0.2.
 Table 1. Performance of DeepH3 energy versus alternative methods for selecting low-RMSD antibody decoys Energy function Top 1 Top 5 Better Same Worse ΔRMSD Better Same Worse ΔRMSD SBROD 38 6 5 –1.8 35 11 3 –1.1 KORP 32 10 7 –0.9 25 18 6 –0.6 Rosetta 33 6 10 –1.4 24 16 9 –0.8 
 Notes:  Top-1 metrics compare the RMSD of the best-scoring structure by DeepH3 energy against that of a given energy function. Top-5 metrics compare the lowest-RMSD structure among the five best-scoring structures selected by DeepH3 energy and that of a given energy function. The average difference in RMSD between the structures selected by DeepH3 energy and a given energy function is reported as ΔRMSD (Å). ‘Better,’ ‘Same’ and ‘Worse’ indicate the number of targets that achieve a lower, same, or higher RMSD, respectively, when scored by DeepH3. To provide a better understanding of how predicted geometric potentials improve discrimination between CDR H3 structures, we detail two case studies: anti-ALOX12 sc F v  (sc F v  of mouse antibody with a 12-residue CDR H3 loop, PDB ID: 4H0H) and anti-dansyl mAb (humanized mouse antibody with a 12-residue CDR H3 loop, PDB ID: 1DLF) ( Weitzner and Gray, 2017 ).  Figure 4A–C  shows energy funnels for anti-ALOX12 and anti-dansyl, respectively, with the discrimination score calculated for each. For anti-ALOX12, Rosetta energy displays little ability to discriminate with structures ranging from 2 to 8 Å RMSD ( D  = 10.0). DeepH3 energy, however, earns a negative discrimination score ( D  = –3.7), indicating an ability to easily distinguish the near-native structures. The best scoring anti-ALOX12 decoy structures as selected by Rosetta energy (orange, 7.2 Å RMSD) and DeepH3 energy (violet, 1.6 Å RMSD) are shown in  Figure 4B .
 Fig. 4. Results for two Rosetta antibody benchmark targets. ( A ) Plots of Rosetta energy and DeepH3 energy versus RMSD from the experimental structure for 2800 decoy structures for anti-ALOX12 sc F v . The five best-scoring structures in each funnel plot are indicated in red. Five relaxed native structures are plotted as orange triangles. ( B ) Experimental structure of anti-ALOX12 sc F v  (green) with best-scoring structures by Rosetta energy (orange, 7.2 Å RMSD) and DeepH3 energy (violet, 1.6 Å RMSD). ( C ) Plots of energy versus RMSD from the experimental structure for anti-dansyl mAb. ( D ) Experimental structure of anti-dansyl mAb (green) with best-scoring structures by Rosetta energy (orange, 2.5 Å RMSD) and DeepH3 energy (violet, 4.0 Å RMSD) For anti-dansyl, Rosetta energy is generally unsuccessful in discriminating between decoys ( D  = 0.6), again with minor energetic differences across a wide range of RMSD values. DeepH3 energy appears to converge to an alternative loop conformation around 4 Å RMSD, resulting in a poor discrimination score ( D  = 3.8).  Figure 4D  shows the best-scoring anti-dansyl decoy structures as selected by Rosetta energy (orange, 2.5 Å RMSD) and DeepH3 energy (violet, 4.0 Å RMSD). 3.3 Longer loops remain a challenge The Rosetta antibody benchmark dataset encompasses a diverse set of CDR H3 loop lengths. Longer loops introduce greater degrees of freedom (two DOFs per residue), and thus present additional challenges to effective sampling and discrimination. To investigate the performance of DeepH3 across loop lengths, we sub-divided the benchmark targets by length and compared to three alternative scoring methods: SBROD, KORP and the Rosetta energy function ( Fig. 5 ). For nearly every loop length considered, DeepH3 identified the lowest RMSD structures according to the top-1 and top-5 criteria (see above). For several loop lengths, DeepH3 identified decoys near the lowest-RMSD for particular targets in the dataset, as indicated by the shaded region. In general, the average RMSD increased with loop length across all four methods, though DeepH3 displayed notable consistency across loop lengths according to the top-5 criteria.
 Fig. 5. Performance of DeepH3 and alternative methods across various loop lengths. ( A  and  B ) Comparison across loop lengths of the error in structures selected by SBROD (green), KORP (blue), Rosetta energy (orange) and DeepH3 score (violet). The shaded areas show the range of lowest RMSD values sampled for targets across loop lengths. (A) Average RMSD of the best-scoring structure (top 1). (B) Average of the lowest-RMSD present within the five best-scoring structures (top 5) 3.4 Orientation potentials are more effective than distance potentials We also evaluated the utility of individual geometric potentials for selecting low-RMSD decoys ( Table 3 ). Notably, when DeepH3 distance potentials alone were used, performance was only moderately better than Rosetta energy. When the best-scoring structures by Rosetta energy and distance potentials were compared, distance potentials selected better-, same- and worse-RMSD structures for 27, 9 and 13 out of 49 targets, respectively, with an average RMSD improvement of 1.1 Å. When the set of five best-scoring structures by Rosetta energy and distance potentials were considered, DeepH3 energy identified better-, same- and worse -RMSD structures for 22, 14 and 13 out of 49 targets, respectively, with an average RMSD improvement of 0.5 Å. Individual orientation potentials were more effective at selecting low-RMSD decoys than distance, even matching or outperforming the total DeepH3 energy by some metrics. We also calculated discrimination scores for each geometric potential ( Table 2 ). Distance and ω orientation potentials displayed the weakest performance among geometric potentials but still showed significant improvement over Rosetta energy, with 32 out of 49 simulations being successful for both. The other orientation potentials produced more successful simulations and lower mean discrimination scores. 3.5 DeepH3 effectively predicts new CDR H3 loop structures  de novo The ultimate goal of DeepH3 was to improve the  de novo  prediction of CDR H3 loops. Towards this end, we used DeepH3 to create potentials that we then used in Rosetta for  de novo  structure prediction of the CDR H3 loops (Section 2). The average (± SD) RMSD of the best-scoring structures generated with DeepH3 potentials for each target (top 1) was 2.2 ± 1.1 Å. When the set of five best-scoring structures for each target were considered (top 5), the average RMSD fell to 1.9 ± 0.9 Å. We compare the best-scoring structures generated with DeepH3 potentials to those published by Weitzner  et al.  ( Weitzner and Gray, 2017 ) ( Fig. 6A ) and find effectively equivalent performance ( Δ RMSD &lt; 0.1 Å) (Top-5 metrics were not reported by Weitzner  et al. ). The recently published trRosetta provides another deep learning prediction method to compare. trRosetta is trained broadly on diverse protein structures, and DeepH3 has fewer input features (just sequence). trRosetta is designed for single-chain proteins, so we omitted the light chain and predicted structures for the heavy chain alone. On the same benchmark, trRosetta achieves average accuracies of 4.7 ± 1.4 Å (top 1) and 4.3 ± 1.3 Å (top 5,  Fig. 6A ). Compared to trRosetta, DeepH3’s top-1 and top-5 metrics are 2.5 Å and 2.4 Å RMSD better, respectively.
 Fig. 6. Performance of DeepH3 for  de novo  CDR H3 loop structure prediction. ( A ) DeepH3 achieves lower average RMSD (2.2 ± 1.1 Å) than trRosetta (4.7 ± 1.4 Å) and ties Weitzner  et al.  (2.2 ± 1.5 Å) ( Weitzner  et al. , 2017 ) when the best scoring structures for each target were compared (top 1). When the lowest-RMSD structure among the five best-scoring structures were considered (top 5), DeepH3 (1.9 ± 0.9 Å) outperformed trRosetta (4.3 ± 1.3 Å). Top-5 metrics were not available for Weitzner  et al.  ( B ) Comparison of the minimum RMSD sampled by DeepH3 to the RMSD of the best-scoring structure (top 1) for each target. ( C ) Comparison of the minimum RMSD sampled by DeepH3 to the lowest RMSD within the set of five best-scoring structures (top 5) for each target. To better understand the sampling performance of DeepH3, we compared the lowest-RMSD decoy sampled to the best-scoring (top 1,  Fig. 6B ) and the lowest-RMSD among the five best-scoring (top 5,  Fig. 6C ). DeepH3 samples structures with sub-angstrom RMSD for 38.8% of the targets and 95.9% for &lt;2 Å. On the other hand, DeepH3 is able to identify a sub-angstrom decoy as the best-scoring structure (top 1) for 14.3% of targets and 55.1% for &lt;2 Å. When considering the set of five best-scoring decoys (top 5), DeepH3 identifies a sub-angstrom decoy for 18.4% of targets and 63.2% for &lt;2 Å. These results are promising and point to possibility of further refining the DeepH3 geometric potentials for  de novo  prediction. 4 Discussion The results here suggest that the significant advances by deep learning approaches in general protein structure can also be realized in subproblems in structural modeling. Specifically, we demonstrate that a deep residual network can effectively capture the local inter-residue interactions that define antibody CDR H3 loop structure. DeepH3 achieves these results without MSAs and coevolutionary data, while using significantly fewer residual blocks (3 1D + 25 2D blocks) than similar networks, such as AlphaFold (220 2D blocks) ( Senior  et al. , 2020 ), RaptorX (6 1D + 60 2D blocks) ( Wang  et al. , 2017 ;  Xu, 2019 ) and trRosetta (61 2D blocks) ( Yang  et al. , 2020 ). Fewer blocks may suffice because we limited our focus to antibodies, which are highly conserved, rather than the entire universe of protein structures. By omitting MSAs and coevolutionary, we demonstrate that these features, which have seemed essential to the advances in general protein structure prediction, may not be necessary for some subproblems. In the future, similar specialized networks could achieve enhanced performance in other challenging domains of protein structure prediction, but further research is required. Breakdown of DeepH3 energy into individual geometric potentials revealed that inter-residue orientations were significantly more effective for scoring CDR H3 loop structures than distances. This finding was surprising, given the improvements that distances alone have enabled in general protein structure prediction. This observation could also underlie the improved performance of trRosetta compared to methods that do not use orientations. Or possibly distance restraints are effective at placing residues globally, but local interactions in loops are captured by inter-residue orientations. Application of DeepH3 to  de novo  prediction of CDR H3 loop structures highlights the promise of deep learning in this challenging area. Comparison with the results from Weitzner  et al. , which leveraged an explicit H3-kink geometric constraint ( Weitzner and Gray, 2017 ), demonstrates that DeepH3 effectively learned challenging features of H3 loop structure. While this work focused only on the CDR H3 loop, we anticipate that applying DeepH3 to other aspects of antibody structure prediction may yield further advances. Because DeepH3 learns from full  F v  heavy and light chain sequences, the current network may already capture other critical aspects of antibody structure prediction [V L –V H  orientations ( Marze  et al. , 2016 ), non-H3 CDR loop conformations ( North  et al. , 2011 ) etc.], though future work will be necessary to explore these areas. Funding This work was supported by National Institutes of Health grants R01-GM078221 and T32-GM008403 and National Science Foundation Research Experience for Undergraduates grant DBI-1659649. Computational power was provided by the Maryland Advanced Research Computing Cluster (MARCC). 
 Conflict of Interest : none declared. Supplementary Material btaa457_Supplementary_Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Predicting and understanding the stability of G-quadruplexes</Title>
    <Doi>10.1093/bioinformatics/btp210</Doi>
    <Authors>Stegle Oliver, Payet Linda, Mergny Jean-Louis, MacKay David J. C., Leon Julian Huppert</Authors>
    <Abstract>Motivation: G-quadruplexes are stable four-stranded guanine-rich structures that can form in DNA and RNA. They are an important component of human telomeres and play a role in the regulation of transcription and translation. The biological significance of a G-quadruplex is crucially linked with its thermodynamic stability. Hence the prediction of G-quadruplex stability is of vital interest.</Abstract>
    <Body>1 INTRODUCTION Understanding biological sequences and predicting the functional elements they determine are widely studied themes in computational biology. Examples of well-established problems are gene finding and the prediction of protein structure from its amino acid sequence. Computational methods addressing such challenges helped to gain insights into interesting biological phenomenon. However, other information encoded in the DNA sequence remains to be explored. Recently, it has been found that particular G-rich DNA (and RNA) sequences are capable of forming stable four-stranded structures known as G-quadruplexes (Burge  et al. ,  2006 ; Huppert,  2008 ; Neidle and Balasubramanian,  2006 ). G-quadruplexes have been shown to be relevant in a number of biological processes (Patel  et al. ,  2007 ). They are an important component of human telomeres (Oganesian and Bryan,  2007 ), and play a role in regulation of transcription (Qin and Hurley,  2008 ; Siddiqui-Jain  et al. ,  2002 ) as well as translation (Kumari  et al. ,  2007 ). Structurally, intramolecular G-quadruplexes consist of a square arrangement of four guanines (a tetrad) in a planar hydrogen bonded form. At the centre of the tetrads is a monovalent cation, e.g.  K + , that further stabilizes the structure. The core guanines are linked by three nucleic acid sequences (loops) of varying composition and topology.  Figure 1  shows a schematic picture of a G-quadruplex together with the hydrogen bond pattern.
 Fig. 1. ( a ) Hydrogen bond pattern in a G-tetrad. A monovalent cation occupies the central position. ( b ) Schematic diagram of an intramolecular G-quadruplex, with three G-stacks. 
 An obvious challenge is to predict which sequences will form these G-quadruplexes. A necessary condition for G-quadruplex formation is the presence of core guanines and loop sequences. These basic requirements can be used to identify putative G-quadruplexes using a simple pattern-based rule, matching sequences of the form
 (1) 
where  G N G  are the guanine cores that can occur with different numbers of G-stacks,  N G =2, 3, 4. The symbol  N  denotes any nucleotide. The loop sequences (L 1 , L 2 , L 3 ) have varying length, where  N L =7 is a typical choice for the maximum length. For very long loops, G-quadruplexes are unlikely to form as their stability decays with the total sequence length (Bugaut and Balasubramanian,  2008 ; Hazel  et al. ,  2004 ). Similar rules have been widely used in the literature e.g.(Huppert and Balasubramanian,  2005 ) and demonstrated to work well in practice. However, they are not exhaustive, for example some structures with much longer loops can be formed (Bourdoncle  et al. ,  2006 ). The most important limitation of pattern-based sequence rules is that they do not predict the thermodynamic stability, a key property of the G-quadruplex. In order for the G-quadruplex to have a biologically meaningful role, it needs to be stable enough to form a structure at body temperature. Furthermore, it has been speculated that G-quadruplexes that are metastable at body temperature carry the most significant role, as their influence on transcriptional processes can be active or inactive depending on other factors. This motivates the problem of predicting the G-quadruplex melting temperature as a proxy for stability from its sequence alone. In contrast to simpler systems such as DNA duplexes (SantaLucia,  1998 ), sequence differences in G-quadruplexes affect thermodynamic stability in a non-linear fashion, hence rendering this prediction task challenging. The nearest neighbour approaches that have been so successful for predicting duplex stability, such as from (SantaLucia,  1998 ), are not applicable to folded-back structures such as G-quadruplexes. It is relatively straightforward to experimentally determine the thermodynamic stability for specific G-quadruplexes using ultraviolet (UV) melting (Mergny  et al. ,  1998 ). In a UV melting experiment, the absorbance of a guanine-rich oligonucleotide is recorded as a function of the temperature. This allows the melting temperature of the G-quadruplex to be deduced. However, no one has managed to extrapolate generalized energy parameters to each component of the structure. Instead, empirical rules and intuition have been built up based on small-scale studies with a few dozen G-quadruplex sequences. Various details have been discovered, establishing the importance, in particular, of the loops that join the core guanines together (Bugaut and Balasubramanian,  2008 ; Hazel  et al. ,  2004 ; Lane  et al. ,  2008 ). Although it is still in the early days of our understanding of G-quadruplex stability, it is clear that both loop length and loop composition are important. The stability of G-quadruplexes is also strongly influenced by the surrounding solution providing the monovalent cation that sits inside the structure, typically between the G-tetrad stacks ( Fig. 1 ). For instance, K +  is strongly favoured over Na +  or Li +  and hence leads to more stable structures. In this work, we propose a computational prediction method for the stability of G-quadruplexes based on Gaussian process (GP) regression. This includes a special purpose covariance function that allows sequence features potentially affecting the G-quadruplex stability to be flexibly incorporated. The inference procedure automatically determines the relevance of sequence features and yields predictions with error bars. Using a heavy-tailed likelihood, our model gains additional robustness with respect to outliers. The presented framework can also handle experimental data that merely set a maximum or minimum range on the melting temperature rather than an explicit value. This situation occurs if a structure is found to be stable at all experimentally accessible temperatures. We demonstrate the accuracy of the prediction method on previously unseen sequences and compare it to alternative methods. Finally, we consider an active learning procedure and apply the methodology to assess the stability of G-quadruplexes in gene promoters, comparing them to other G-quadruplexes. 2 QUADRUPLEX PREDICTIONS USING GP The prediction of G-quadruplex stability can be cast as a regression problem. For a given training dataset with observed G-quadruplexes, 𝒟={ x n ,  t n } n =1 N , the task is to infer a latent function  f  :  x → t , mapping from a G-quadruplex input  x  to its melting temperature  t . The main determinant of G-quadruplex stability is the sequence information. However, the cation nature and salt concentration also have an effect on the stability of the resulting G-quadruplex. Our G-quadruplexes were measured at different concentration levels, which must be taken into account when making predictions. We assume that inputs  x ={ s ,  c } consist of the quadruplex sequence  s  and a vector of log-salt concentrations  c . To apply the GP machinery, all we need is a positive definite covariance function defined (kernel) between pairs of G-quadruplex inputs. Given a training dataset 𝒟 the posterior distribution over latent function values  f  is
 (2) 
where  θ K  and θ L  are hyperparameters of the kernel (K) and the likelihood (L), respectively. We use  X  to denote the set of all training inputs,  X = x 1 ,…,  x N . The covariance matrix  K X ,  X ( θ K ) is derived from the covariance function  k ( x ,  x ′|θ K ) which specifies how function values at two inputs  x ,  x ′ covary. The noise model  p L ( t n | f n ,  θ L ) relates function values  f n  and the corresponding noisy observations  t n . For simplicity let us first assume standard Gaussian noise,  p L ( t n | f n ,  θ L )=𝒩 t n | f n , σ 2 ) with noise level σ. In this case, the predictive distribution for an unseen input  x ⋆  is a Gaussian again (Rasmussen and Williams,  2006 ), where  t ⋆ ∼𝒩(μ ⋆ ,  v ⋆ ) and
 (3) 
A Bayesian network representation of this model is shown in  Figure 2 . A comprehensive introduction to GPs can be found in Rasmussen and Williams ( 2006 ).
 Fig. 2. Bayesian network representation of a GP regression model. The model relates observed independent input/output pairs { x n ,  t n } n =1 N . The thick lines couple the latent function value { f n }, illustrating the smoothness assumptions introduced by the GP prior. The parameters  θ K  and  θ L  denote hyperparameters of the kernel and likelihood, respectively. 
 Hyperparameters: a GP is a non-parametric model. The only explicit parameters of the model are hyperparameters  θ L  and  θ K , all other parameters can be integrated out and are not represented explicitly. In a GP model the posterior probability of the hyperparameters is
 (4) 
The log of the first term, ℒ( θ K ,  θ L ) (marginal likelihood), can again be computed in closed form for a Gaussian-noise model (Rasmussen and Williams,  2006 ). Gradient-based optimizers can be used to then determine the most probable setting of the hyperparameters
 (5) 
 2.1 Covariance function and hyperpriors An important design choice for using a GP is a suitable covariance function. We use a product of covariance functions to combine kernels evaluated on the sequence  s  and solution concentrations  c 
 (6) 
where  k s  is the sequence kernel and  k c  the concentration kernel. The product expresses the belief that both kernels must assign high similarities for covariation of function values. The squared exponential concentration kernel decays exponentially with log-concentration difference
 (7) 
where  A c  determines the typical amplitude of deviations from the mean and { c i } are log salt concentrations in mM of Na + , K + , NH 4 +  and Mg 2+ , respectively. These are the four most common stabilizing cations for G-quadruplexes; the nature of the anion does not seem to play a role. The lengthscale parameters  l c  determine the significance of the associated concentration parameters where large lengthscales correspond to less relevant parameters and short length scales to more relevant ones. To make the lengthscale comparable, the individual input dimensions are linearly rescaled such that observed training inputs fall into a set range, here −5 to 5. The sequence kernel,  k s , is a sum of two covariance functions. The first covariance is designed to specifically incorporate existing beliefs about characteristic sequence features that are likely to determine the stability of the G-quadruplex (Lane  et al. ,  2008 ). For flexibility, we consider G-quadruplexes that contain either two, three or four stacked tetrads and hence have the equivalent number of guanines in each run. From the raw sequence information of a G-quadruplex with the form
 (8) 
a set of features  f  is extracted:
 L total  – total length of the sequence (in bases) N G  – number of G-tetrad stacks (2, 3 or 4) L 1  – length of the first loop (from the 5 ′  end, in bases) L 2  – length of the second loop L 3  – length of the third loop F A  – relative frequency of adenine in the sequence F C  – relative frequency of thymine F T  – relative frequency of cytosine 
The loop lengths determine the number of bases between the guanine stacks, N 1− N L . The relative frequency of the adenine, thymine and cytosine are calculated as  , where  N A  denotes the total number of adenines in the sequence (similarly for thymine and cytosine). Again, a squared exponential kernel is used to combine these features
 (9) 
where  f i  denotes the  i -th of the eight sequence features. The parameters have the same interpretation as for the concentration kernel. As before, input dimensions are rescaled and the lengthscale parameters  l f  was adjust the relevance of the sequence features. The second sequence covariance function is ignorant to the biological meaning of the G-quadruplex sequence and merely treats it as character string. We can construct a spectrum kernel (Leslie  et al. ,  2002 ), that is sensitive to common k-mers present in two sequences  s  and  s ′
 (10) 
where Φ k ( s ) maps the sequence  s  to a vector of counts with the number of occurances for each  k -mer in  s . The number of possible k-mers in a nucleotide sequence scales as 4 k  and hence only small orders  k  are practical. In experiment, 1  we consider k-mers up to an order of  k =4. Due to this low order of  k , this spectrum kernel is  local  in that it is not sensitive to long common substrings. In contrast, the feature kernel captures  global  sequence characteristics and hence both sequence kernels complement each other. Finally, all three kernels are combined in
 (11) 
The relative weights of the individual kernels are controlled by the amplitude parameters  A c ,  A f  and  A s . Hyperpriors: priors on all kernel- and likelihood-hyperparameters { θ K ,  θ L } are Gamma distributed. The prior on the expected amplitudes of the squared exponential kernels  A f  and  A c  is Γ(2, 10) with an expected value of 20. The amplitude of the string kernel has a prior  A s ∼Γ(2, 0.5). The prior on the noise level σ is Γ(2, 0.5), which corresponds to an a priori uncertainty of ±1 ○ C about the measured G-quadruplexes' melting temperatures. The lengthscale parameters of the feature and concentration kernels have a prior of Γ(4, 10), which favours long lengthscales (mean 40) encouraging irrelevant features to be switched off. 2.2 Robust likelihood The presentation of the GP model so far makes the simplifying assumption that observation noise is Gaussian. For our full model, we use a heavy-tailed noise model which acknowledges that a small fraction of the data points can be extremely noisy (outliers) while others are measured with considerably more precision. The ‘two model’ (Jaynes and Bretthorst,  2003 ) reflects this belief,
 (12) 
Here, π 0  represents the probability that a datum is a regular observation and (1−π 0 ) is the probability of an outlier observation. The variance of the outlier component, σ 2 ∈ f , is much larger than for regular observations, σ 2 , which allows the model to effectively discard outlier observations. When using this likelihood model, the posterior in Equation ( 2 ) is no longer computable in closed form. To overcome this problem, we use Expectation Propagation (EP) (Minka,  2005 ) for approximate inference. The goal of EP is to approximate the exact posterior with a tractable alternative of the form
 (13) 
where  g n ( f n | C n , μ n , ν n ) denote approximate factors. Following Rasmussen and Williams ( 2006 ) we choose unnormalized Gaussians
 (14) 
which results in a GP for the approximate distribution again. The idea of EP is to iteratively update one approximate factor at a time, leaving all other factors fixed. This is achieved by minimizing the Kullback–Leibler (KL) divergence, a distance measure for distributions (Kullback and Leibler,  1951 ). The update for the  i -th approximate factor is performed by minimizing
 (15) 
with respect to the  i -th factor's parameters μ i , ν i  and  C i . This is done by matching the moments between the two arguments of the KL divergence which can then be translated back into an update for factor parameters. There is no convergence guarantee for EP but in practice it is found to converge for the likelihood model we consider (see also Kuss  et al. ,  2005 ). The fact that the mixture of Gaussian likelihood is not log-concave represents a problem as it may cause invalid EP updates, leading to a covariance matrix that is not positive definite. We avoid this problem by damping the updates as suggested by Kuss  et al.  ( 2005 ) and Seeger ( 2005 ). EP also yields an approximation of the log marginal likelihood which can be used to determine the setting of hyperparameters
 (16) 
where Σ=diag({ν n } n =1 N ). In addition to the noise level σ ( Section 2.1 ), the robust likelihood includes a parameter σ ∈ f  and the mixing proportion π 0 . The parameter π 0  is optimized together with the remaining hyperparameters. The noise level of outliers, σ ∈ f , is set to 10 4 . After convergence of EP, we obtain a GP as approximate posterior distribution (Equation  13 ). Predictions from this model follow analogous to the standard GP (Equation  3 ). A comprehensive overview on EP approximations for GP models can be found in Rasmussen and Williams ( 2006 ); robust GP regression has been previously applied to biological time series in Stegle  et al.  ( 2008 ). 2.3 Constrained likelihood In addition to ‘normal’ observations of sequence/temperature pairs, our G-quadruplex measurements also include a small fraction of sequences where only a bound on the melting temperature was determined. For example, if a G-quadruplex is so stable that it does not complete its melting transition within the experimentally accessible range (typically 10–85 ○ C), one can only deduce that the melting temperature is larger than this threshold value. Such observations can be included using a theta likelihood function. For instance, for an observed lower bound  t n 
 (17) 
where  . These non-Gaussian likelihood terms can be dealt with using an EP approximation similar to the one used in ( 12 ), where exact likelihood terms are approximated by Gaussian approximate site functions. 2.4 Active learning In addition to predicting G-quadruplex melting temperatures, it is possible to use the GP framework for experimental design, i.e. to choose which of a set of candidates to measure. Suppose that we would like to optimally expand a training dataset 𝒟, such that we can make most informative predictions about a test set 𝒟 test . A naive approach would be to randomly draw a subset of the sequences in 𝒟 test , measure their melting temperatures and use them as additional training data. Alternatively we can consider active learning, choosing this set using an information criterion as proposed by MacKay ( 1992 ), or in the context of GP discussed by Seo  et al.  ( 2000 ). A practical objective function is the mean marginal information gain over the set of interest, here 𝒟 test ={ x ′ m ,  t ′ m } m =1 M . If the predictions are Gaussian, the mean marginal entropy is entirely determined by the predicted variances σ t ′ m 2 
 (18) 
To decide which sequence to measure and add to the training data, we iterate through all candidate test inputs  x ′ m ∈𝒟 test , choosing the one which minimizes  S M . The mean entropy  S M  can be efficiently evaluated as predictive uncertainties of a GP, σ t ′ m 2 , only depend on the training inputs (Equation  3 ) and hence candidate sequences can be scored before knowing their melting temperature (Seo  et al. ,  2000 ). Once a measurement has been taken, the new input/target pair   is added to the training dataset and hyperparameters are optimized again. 3 EXPERIMENTS To evaluate the proposed method, we applied the GP predictor to a meta dataset summarizing major G-quadruplex experiment data available as of today. In total, this dataset consists of 260 G-quadruplex structures which have been experimentally tested with varying salt concentrations. All of the considered sequences were of the form described by the pattern in Equation ( 8 ). Hence the covariance function as introduced in  Section 2.1  was applicable. 3.1 Predictive performance on observed data To assess the accuracy of the GP method, the model was trained on subsets of all 260 G-quadruplexes. Subsequently, the trained model was used to predict melting temperatures of G-quadruplexes in the remaining test set, and predictions were compared with the true observed melting temperatures. This predictive test was repeated for different training/split ratios and multiple random splits. 3.1.1 Mean prediction We first investigated how well we were able to predict real data using our model.  Figure 3 a shows marginal GP test predictions versus the true melting temperatures for a representative 50:50 training/test split. The plot illustrates that the GP has estimated appropriately sized error bars. A histogram view of the differences of the true melting temperatures and the predictions is shown in  Figure 3 b. The results show that most of the experimental data was predicted within a 5 ○ C error margin, a reasonable standard of accuracy. Indeed, across 100 random 50:50 training/test splits, on average 80% of the predictions were within ±5 ○ C of the experimentally determined values.
 Fig. 3. Accuracy of GP predictions for a representative 50:50 training/test split (260 total measurements). ( a ) True measured melting temperatures (green) and marginal GP predictions with ±2 SDs error bars (blue). ( b ) Prediction errors Δ. ( c )  Z -Scores for the predicted values,  . 
 We then compared the performance of our model with alternative methods. This comparison includes the proposed GP model (GP robust), a simpler variant of the model without the robust and constrained likelihood (GP standard), Bayesian linear regression on the sequence features  f  (Linear regression, Bishop ( 2006 )) and a support vector machine (SVM, Fan  et al. , ( 2005 )). The SVM was applied with the same kernel as used in the GP models. For the standard GP, linear regression and the SVM, sequences where the data only supplied an upper or lower bound on the melting temperature (i.e. the sequence was too stable to measure under these conditions) had to be excluded. In total, this reduced the size of the training dataset from 260 to 256 sequences. Figure 4 a, shows the root mean squared error on the test dataset for different algorithms as a function of the relative test set size. As expected, the performance of all algorithms decreased with growing test set and therefore shrinking training set sizes. The GP methods outperformed the SVM, and linear regression. Our robust GP model performed marginally but consistently better than the standard GP.
 Fig. 4. Comparative predictive performance of different algorithms evaluated as a function of the relative test-set size (260 total measurements). ( a ) Root mean squared error on the test set. ( b ) Mean log probability of the test data under the predictive distribution. Error bars show 1SD estimated from 100 random training/test splits. 
 3.1.2 Variance prediction As a second criterion, we assessed the mean log probability of the test data under the predictive distribution given by different models. Bigger predictive probability indicates that a method not only is accurate in estimating the mean but also yields appropriately sized error bars. For this analysis, the results from the support vector machine had to be excluded as the method does not yield a predicted uncertainty. The results in  Figure 4 b mirror the comparison of the root mean squared errors. However, using this probabilistic performance measure, the robust GP performed significantly better than the standard GP variant. This suggests that the robust likelihood model helps to ensure appropriate predictive uncertainties. The quality of these error bars is also supported by  Figure 3 c, which shows  Z -scores of test predictions for a 50:50 training/test split. The number of data points within a ±2 SDs margin is in line with the expected number hence showing that the robust GP model ‘knows what it knows’. This is an important and powerful feature for making useful predictions, and will be relevant in the genome-wide G-quadruplex study in  Section 4 . 3.2 Determining causal features of the G-quadruplex sequence To understand the mechanisms of G-quadruplex stability it is useful to be able to analyse which sequence features play a role in determining the stability of a G-quadruplex. Such insights can be gained from observing the optimized hyperparameters of the feature kernel  k f . As the lengthscale parameter  l f i  indicates the relevance of a particular feature  i , this can be regarded as a form of feature selection. A related approach has been described by Chu  et al.  ( 2005 ) who used GP for biomarker discovery in microarray experiments. The string covariance function  k s ( s ,  s ′) explains part of the sequence similarity and thus makes the relevances of the sequence feature kernel difficult to interpret. Hence the string covariance was excluded for this evaluation.  Figure 5  shows the inverse lengthscale parameters of the sequence kernel optimized on the full G-quadruplex dataset. The results were averaged over 100 independent optimizations with random starting points. The results show that the relevance of features varied significantly. The most important features were the length of the middle sequence ( L 2 ), the total loop length ( L total ) and the number of guanine stacks ( N G ). Among the parameters for base composition frequency, the adenine frequency appeared to be most important. Both observations are in line with previously observed characteristics of G-quadruplexes (Lane  et al. ,  2008 ). However, it had been expected that  L 1  and  L 3  would also have a large effect. In this context, it is interesting to note the strong fluctuation of the significances of the outer loop lengths  L 1  and  L 3  as indicated by the error bars in  Figure 5 . A possible explanation for this effect is that there are dependencies between these parameters such that either one or the other feature is needed to explain G-quadruplex stability. Obviously, there is an underlying relationship between  L total ,  N G  and  L 1…3 . As a result of this interaction, independent restarts might then explore different modes of the hyperparameters' posterior distribution.
 Fig. 5. Optimized inverse lengthscale hyperparameters. The plot shows empirically estimated means and ±1 SD error bars estimated from 100 restarts of the optimization procedure. Larger bars indicate more important parameters. 
 To better understand the posterior over hyperparameters, we employed a Hamiltonian Monte Carlo sampler (e.g. MacKay,  2003 ) to draw samples from this distribution.  Figure 6  shows the correlations between hyperparameters of the feature kernel as a Hinton diagram. The correlation coefficients have been calculated from 500 MCMC samples (500 burn-in). This figure shows that the relevances of  L 1  and  L 3  were indeed anti-correlated. This observed anti-correlation can be explained by positive correlations between the corresponding features in the training dataset, causing that either  L 1  or  L 3  is sufficient to predict the melting temperature. A strong positive correlation of hyperparameters was observed between the loop length  L 2  and the number of G-stacks  N G .
 Fig. 6. Correlations between inferred hyperparameters illustrated as Hinton diagram. Correlation coefficients were estimated from 500 Monte Carlo sample. The size of the squares denote the strength of the correlation, where white squares indicate positive correlation and black squares negative correlation. 
 4 GENOME-WIDE ANALYSIS OF G-QUADRUPLEX CANDIDATES We applied the GP predictor to human genome-wide G-quadruplex candidates downloaded from the quadruplex.org database (Wong  et al. ,  2008 ). The database contains candidate structures extracted from sequence information using the pattern-based rule from Equation ( 8 ), considering quadruplexes with three or more G-stacks ( N G ≥3). Using this rule a total of 359 548 G-quadruplex candidates with precisely 3 loops have been identified genome-wide, from a total of 373 k predicted sequences, some of which contain several possible G-quadruplexes, and hence cannot be predicted with the available data. Following Huppert and Balasubramanian ( 2007 ), we also extracted those G-quadruplexes found in the promoters of human genes, looking at the 200 bp upstream of the transcription start site. Again restricting to 3-loop G-quadruplexes there were 10 987 quadruplexes in human promoter regions. All computational predictions for these G-quadruplexes were made for a solution containing 100 mM K + , which roughly approximates physiological conditions and has become something of a standard for experimentation. 4.1 Active learning for promoter G-quadruplexes Given the large number of genomic sequences and the relatively small number of data points, it is necessary to be efficient with data collection, so as to maximize the information derived from each new experiment. We therefore developed a method of active learning such that we can predict which experimental data (i.e. melting temperatures of sequences) would be most useful to collect. As a preliminary case study of the usefulness of active learning, we considered the set of promoter G-quadruplexes and applied the active learning strategy outlined in  Section 2.4 . Given the training dataset, we selected the subset of the 10 most informative G-quadruplexes in promoter regions, assessed by the marginal information gain. The melting temperatures of the corresponding sequences were experimentally determined and added to the training set. As an alternative, we did the exact same experiment but selected 10 randomly chosen sequences instead. Again the sequences were experimentally characterized and added to the training set. In each case, the sequences were prepared at 4 μM concentration in a Tris–HCl buffer at pH 7.4 with 100 mM KCl. A Varian Cary 300 spectrophotometer was used to measure the absorbance at 295 nm over repeated slow heating/cooling cycles (Mergny  et al. ,  1998 ). Melting temperatures were determined by the derivative method.  Figure 7  shows the average predictive uncertainty for all promoter quadruplexes as a function of the number of additional measurements. Results for the physical measurements are indicated as red and black crosses. Lines show the expected uncertainties obtained from the model without conducting any physical measurement.
 Fig. 7. Average predictive uncertainty for promoter G-quadruplexes as a function of the number of additional measurements. Compared are two random measurement sequences (black) and the active learning strategy (red). The red and black cross indicate the average predictive uncertainty after physically measuring 10 actively (red) or randomly (black) chosen G-quadruplexes. 
 It is apparent that very few additional measurements can significantly reduce the predictive uncertainty. This observation can be explained by the sequence homology present in the G-quadruplexes found across the genome (Huppert and Balasubramanian,  2005 ; Todd  et al. ,  2005 ). The active selection performed significant better than the randomly selected sequences. Active learning allows a feedback cycle to be developed, where after each set of data is added, new learning can be performed to optimize the next data collection, resulting in efficient experimentation. The average uncertainties resulting after real measurements were higher than the model expectations. This discrepancy is because the theoretical calculations are approximations based on fixed hyperparameters, whereas for the physical measurements the hyperparameters were re-optimized ( Section 2.4 ). However, we did clearly observe a substantial reduction in uncertainty using the experimental data. These results are supportive and encouraging that active learning in the context of G-quadruplex structures is a helpful tool, although clearly more than 10 further data points are required to make a substantial difference to the predictive power of the model. 4.2 Study of genome-wide G-quadruplex candidates We also performed predictions on all 360 k G-quadruplexes genome-wide. The predictive uncertainty for those G-quadruplexes varied significantly.  Figure 8  shows a histogram of the predictive uncertainty in SD for the entire set of all G-quadruplex sequences. For 90% of the sequences this uncertainty was &lt;14 ○ C. At a more stringent cut off level, still 63% of the sequences could be determined within ±10 ○ C and 6% within ±5 ○ C. This highlights the need for further data collection and the active learning methodology previously described, as well as highlighting the usefulness of predictive uncertainties.
 Fig. 8. Predictive uncertainty for genome-wide G-quadruplex candidates shown in standard deviations in degree celsius. 
 4.2.1 Quadruplexes in promoters Previous analysis of G-quadruplexes suggests that G-quadruplexes are likely to play a widespread regulatory role, supporting experimental demonstrations. It has been shown that G-quadruplexes are over-represented inside promoter regions compared to elsewhere in the genome, by about an order of magnitude (Huppert and Balasubramanian,  2007 ). However, so far it has not been possible to assess whether these quadruplex structures have different stabilities. Here, we use the developed GP predictor to investigate whether there are systematic differences of G-quadruplex stability inside and outside of promoter regions.  Figure 9  directly compares the predictive mean melting temperature for G-quadruplex structures inside promoter regions with G-quadruplexes elsewhere in the genome. For this analysis, we restricted the considered sequences to those that could be predicted with at most a 5 ○ C standard deviation error margin yielding a total of 17 006 G-quadruplexes out of which 235 were in promoter regions. The plots suggest that the statistics of melting temperature might indeed be different for promoter G-quadruplexes. The significance of the difference between the two distributions, melting temperatures of promoter G-quadruplexes and non-promoter quadruplexes, was assessed by a Kolmogorov–Smirnov test. A two-sided test on the predicted mean temperatures for promoter and non-promoter G-quadruplexes found the difference was significant ( P =4.05 × 10 −5 ). This result suggests that G-quadruplexes found in gene promoters are likely to be more stable than those found in the bulk of the genome.
 Fig. 9. Mean predictions of the melting temperature in 100 mM KCl for genome-wide G-quadruplex candidates with a predicted uncertainty &lt;5  ○ C. ( a ) Histograms for promoter and non-promoter quadruplexes. ( b ) Cumulative distribution functions. 
 5 DISCUSSION AND CONCLUSION We have here presented a robust and sensitive method for inferring the stability of G-quadruplexes from the sequence information. Our approach is robust with respect to outliers, allows constraints to be incorporated as observations and automatically determines relevant sequence features. We have further demonstrated how active learning can be used to perform experimental design to guide the choice which sequences of a set of candidates to measure. We demonstrated as proof of principle that we can apply this approach to determine features of biologically important G-quadruplexes, selecting as our example G-quadruplexes found in the 200 bp region upstream of known human gene transcription start sites, a region containing much promoter activity. We have shown previously that G-quadruplexes are concentrated in this region (Huppert and Balasubramanian,  2007 ), and a number of individual studies have confirmed that these can have transcriptional regulatory ability (Qin and Hurley,  2008 ). From the results shown here, we can now conclude that the G-quadruplexes in promoters are likely to be more stable than in the genome as a whole, further supporting the hypothesis that they play an important general role in transcriptional control. The precise mechanistic details of how G-quadruplexes regulate transcription are not entirely clear, but the current model is that their formation disrupts the binding of the normal transcriptional machinery (Qin and Hurley,  2008 ). This approach can be further extended to other regions where G-quadruplexes are found to investigate other functional roles. Several interesting and fruitful extensions to our proposed method could be considered. The sizes of currently available G-quadruplex datasets is very limited. As more data becomes available it would be possible to apply more general sequence kernels characterizing similarity of the loop sequences. Such an approach might yield novel insights into how the sequence composition influences the stability of G-quadruplex structures. We are currently in the process of scaling available G-quadruplex data to significantly larger datasets using the active learning approach proposed in this work to efficiently explore the phase space available. Once the amount of available data goes beyond 1000 examples, it would be helpful to explore sparse approximations to the proposed GP scheme (for instance Snelson and Ghahramani,  2006 ). We will also arrange a data store for other researchers to contribute experimental data they have collected. We plan to have discussions with other researchers to establish a standard for experimental measurements, as well as standards for the quality and style of data provided, which should include measurements of ΔG(37 ○ C), ΔH and ΔS as well as the melting temperature. This would allow us to predict these parameters in addition to the melting temperature alone. We intend to provide a web-enabled version of these predictions. Links to these resources, source code and  Supplementary Material  are available online. 2  The field of G-quadruplexes has grown rapidly in recent years, and we anticipate that the ability to predict their thermodynamic properties will be useful to many in the field, and accelerate the rate of discovery of new functional roles for these fascinating structures. Conflict of Interest : none declared </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>RefProtDom: a protein database with improved domain boundaries and homology relationships</Title>
    <Doi>10.1093/bioinformatics/btq426</Doi>
    <Authors>Gonzalez Mileidy W., Pearson William R.</Authors>
    <Abstract>Summary: RefProtDom provides a set of divergent query domains, originally selected from Pfam, and full-length proteins containing their homologous domains, with diverse architectures, for evaluating pair-wise and iterative sequence similarity searches. Pfam homology and domain boundary annotations in the target library were supplemented using local and semi-global searches, PSI-BLAST searches, and SCOP and CATH classifications.</Abstract>
    <Body>1 INTRODUCTION Evaluation and improvement of protein sequence similarity searches, using algorithms such as BLAST or Smith-Waterman (SSEARCH) and more sophisticated searches such as PSI-BLAST or HMMER (Altschul  et al. ,  1997 ; Durbin,  1998 ; Smith and Waterman,  1981 ), require query sequences and reference sets curated to accurately reflect homology relationships. Because structural similarity is preserved well beyond sequence similarity (Gibrat  et al. ,  1996 ), protein structures are often the gold standard for annotating homology relationships. Although both structure-based homology annotations and manually annotated protein sequence relationships can very accurately record homology relationships, they do not reflect common practice in protein similarity searching, which is to characterize unknown proteins by searching large, comprehensive protein sets such as RefSeq (Pruitt  et al. ,  2007 ) and UniProt (Consortium,  2009 ). To better characterize similarity searching strategies, in particular, PSI-BLAST performance, against comprehensive protein databases, we identified a set of diverse protein domains from Pfam (Finn  et al. ,  2010 ) v. 21 to use as queries against a set of real proteins containing those domains. Our query domain families are taxonomically broad (to provide ‘harder’ homology detection cases), and have long models (to better simulate full-length protein searches). Although we cannot be certain that all homologs have been found, we believe that statistically significant pair-wise alignments are annotated correctly. 2 DATABASE ASSEMBLY Evaluation datasets : from 681 initial Pfam (v. 21) families that met criteria for: (i) domain length (&gt; 200 residues); (ii) taxonomic diversity (present in two of bacteria, archaea and eukarya); (iii) family size (&gt; 100 instances); and (iv) available structure, we selected 344 query Pfam families after merging families that belonged to the same clan (Gonzalez and Pearson,  2010 ). In this initial set, 81 families belonged to distinct clans, while 263 families did not have an associated clan. This set was reduced to 320 non-homologous domains using information from Pfam (v. 23) (by Pfam v. 24, these domains belonged to 112 distinct clans with 168 families not in clans, for 280 non-homologous domains). The target library was built from 234 505 full-length UniProt proteins (excluding viral sequences) containing Pfam v. 21 homologs to the original 320 Pfam families together with 1627 other domain families. Two query sets were constructed and the members of these sets evaluated further: (i) a challenging query subset (50 hard) with the lowest family coverage with BLAST; and (ii) a randomly sampled representative query set (50 sampled with replacement). Annotation extensions : when the original Pfam v. 21 annotations were used to characterize searches with our  hard  and  sampled  queries against the target library, thousands of alignments to very similar UniProt sequences (e.g.  E () &lt; 10 −80 , with &gt;95% identity) were annotated as partial homologs or non-homologs. To correct these conservative annotations, we compared the bare domain query sequences to the target library using SSEARCH and GLSEARCH (a program that produces an alignment that is global in the query sequence but possibly local in the target or library sequence). We identified all the significantly similar sequence regions ( E () &lt; 0.001) with SSEARCH that were either shorter or unannotated in Pfam v. 21 and calculated the boundaries using GLSEARCH. We extended annotations on 2106 partial domains and added 24 604 domain homology annotations based on SSEARCH alignments, 13 574 of which were included in Pfam v. 24. RefProtDom describes relationships and alignment boundaries between query domains and the target library homologs according to Pfam v. 21, Pfam v. 24 and the SSEARCH/GLSEARCH alignment boundaries. Although SSEARCH/GLSEARCH searches against the target library dramatically reduced the number of apparent false positives with very low  E ()-values, additional searches with PSI-BLAST using the queries sometimes found ‘unrelated’ UniProt sequences with significant ( E () &lt; 10 −40 ) scores. We analyzed all significant ( E () &lt; 10 −4 ) ‘non-homologous’ alignments found in the first three iterations of PSI-BLAST for the 100 queries (94 distinct families). Non-homologous alignments to regions with no annotated Pfam domains were used as queries in reciprocal PSI-BLAST searches for three iterations. Reciprocal searches that recovered at least 25% of a domain family were annotated as homologous, yielding 375 additional homology annotations across 33/94 families. Structures of significant ‘non-homologs’ that mapped to unrelated Pfam families were examined in SCOP and CATH; if they shared the same SCOP fold or CATH topology they were annotated as homologs. For example, Pfam annotates a PF00346 domain for residues 295–537 on the Q8ZMJ0_SALTY sequence. RefProtDom also annotates PF00374 in the same region (390–535) because both domains share the same SCOP fold and superfamily classifications (e.18.1). This structural annotation is also reported by SCOOP (Bateman and Finn,  2007 ). We found structural evidence to add 37 additional clans; Pfam v. 24 matches 14 of those 37 structural clans. These additional clans would reduce the number of non-homologous domains in our query set from 94 to 90, but the families were not combined, the cryptic homology was simply annotated. Structure classifications yielded 2124 additional homology annotations across 16/94 queries. 3 SUMMARY Iterative similarity searches are usually performed against full-length proteins with complex domain architectures. Evaluating similarity-searching methods against benchmarking sets with incomplete or missing annotations can introduce dramatic statistical inaccuracies. RefProtDom's greatest strength is its use of a taxonomically diverse set of full-length, multi-domain, proteins in the target library. Searches against RefProtDom resemble searches against SwissProt, UniProt or RefSeq (though those databases are much larger). Moreover, the query sequences are evolutionarily independent; based on structural comparisons, 90 query domains are non-homologous. Thus, RefProtDom can simulate searches against comprehensive sequence databases while evaluating success on challenging homologies. Pfam is a powerful resource for identifying homology relationships and domain boundaries, but strategies that use a single hidden Markov model (HMM) to identify every homolog will be challenged by distant sequences at the detection horizon for the model. For many families, Pfam has addressed this problem by grouping families into clans. But, sometimes homologs are missed; sequences that share strong similarity across the length of a domain to an annotated homolog are surely homologous, even if they do not produce a significant score against the HMM model. The RefProtDom query and target libraries seek to reduce the number of un-annotated homologies with statistically significant similarities, and to more accurately estimate homologous domain boundaries. Although our curation may have missed some homologs, we are confident in the homologies we annotate. Homology is annotated for domains that share significant pair-wise similarity, show significant family coverage after three PSI-BLAST iterations, or when they share structures. Domain boundaries were revised based on significant local or global similarity. By combining single domain queries with full-length, multi-domain proteins, RefProtDom can highlight alignment errors and evaluate improvements in alignment accuracy. Accurate boundary annotation has been largely overlooked in pair-wise sequence comparison, because incorrect alignment boundaries rarely detract from the identification of homologous proteins. Pfam's annotations are now generated with HMMER3, which only performs local alignments (Finn  et al. ,  2010 ). Therefore, one might expect that future Pfam annotations may have an even harder time at identifying complete domains, and thus, should continue to benefit from the extension curation provided by RefProtDom. Nonetheless, HMMER3 compensates with increased sensitivity as a result of better statistics. In fact, 59% of the domain extensions and 55% of the missed homologs added to Pfam v. 21 using our protocol were incorporated in Pfam v. 24. Thus, Pfam v. 24, using HMMER3, has independently addressed many of the missing annotations, validating our approach. However, we believe that the problems inherent in using a single model for diverse protein family searches will always miss homologs and domain boundaries that can be found with individual domains across the family's phylogenetic tree. We plan to continue to update the homology relationships and boundary assignments in RefProtDom. For iterative sequence comparison methods, alignment accuracy is crucial; inaccurate alignments can cause non-homologous domains to be included in the profiles and decrease their specificity in subsequent iterations. Using RefProtDom's annotations, we identified a previously unrecognized alignment overextension error in PSI-BLAST responsible for the corruption of its PSSMs and its poor specificity (Gonzalez and Pearson,  2010 ). Additional evaluations with RefProtDom revealed that while JACKHMMER (HMMER3's iterative implementation) is susceptible to the same error, it overextends more slowly and, thus, shows better performance than unmodified PSI-BLAST (M.W.G. and W.R.P., manuscript in preparation). Domains are the basic units of protein function and evolution; thus, improved homology detection requires improved domain alignment accuracy. Large-scale automatic annotation of gene function is limited by local alignments' incomplete motif matches and fuzzy domain boundaries (Kann  et al. ,  2007 ). Establishing homology is central to a wide array of bioinformatics methodologies; improved domain alignments can improve 3D protein structural predictions that use homology modeling, and also clarify how protein domain networks interact to generate disease phenotypes. RefProtDom provides a comprehensive set of full-length UniProt proteins that can be used to evaluate domain alignment accuracy. Funding : National Library of Medicine, grant LM04969. Conflict of Interest : none declared. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Serial dilution curve: a new method for analysis of reverse phase protein array data</Title>
    <Doi>10.1093/bioinformatics/btn663</Doi>
    <Authors>Zhang Li, Wei Qingyi, Mao Li, Liu Wenbin, Mills Gordon B., Coombes Kevin</Authors>
    <Abstract>Reverse phase protein arrays (RPPAs) are a powerful high-throughput tool for measuring protein concentrations in a large number of samples. In RPPA technology, the original samples are often diluted successively multiple times, forming dilution series to extend the dynamic range of the measurements and to increase confidence in quantitation. An RPPA experiment is equivalent to running multiple ELISA assays concurrently except that there is usually no known protein concentration from which one can construct a standard response curve. Here, we describe a new method called ‘serial dilution curve for RPPA data analysis’. Compared with the existing methods, the new method has the advantage of using fewer parameters and offering a simple way of visualizing the raw data. We showed how the method can be used to examine data quality and to obtain robust quantification of protein concentrations.</Abstract>
    <Body>1 INTRODUCTION The reverse phase protein array (RPPA) is an emerging high-throughput technique in proteomics (for reviews, see Borrebaeck and Wingren,  2007 ; Charboneau  et al. ,  2002 ; Lv and Liu,  2007 ; Poetz  et al. ,  2005 ; Sheehan  et al. ,  2005 ). This technology has been successfully applied in a number of basic and clinical studies (Amit  et al. ,  2007 ; Aoki  et al. ,  2007 ; Fan  et al. ,  2007 ; Pluder  et al. ,  2006 ; Sahin  et al. ,  2007 ; Tibes  et al. ,  2006 ; Yokoyama,  et al. ,  2007 ). A single array slide can be used to measure hundreds of samples for a protein. The protein level across the slide is detected by binding of a highly specific and sensitive primary antibody followed by detection using amplification linked to fluorescence, dye deposition, near infrared or nanoshells. Because protein concentrations can vary over many orders of magnitude in patient or cell line samples, it is desirable to have accurate measurements of protein concentrations over a wide dynamic range. To extend the dynamic range of the measurements, each sample is diluted multiple times successively and spotted on an RPPA slide so that if a protein concentration in the original sample is close to saturation, the sample can still be measured at diluted spots. Multiple methods are available for analysis of RPPA data (Hu  et al. ,  2007 ; Kreutz  et al. ,  2007 ; Mircean  et al. ,  2005 ). Typically, the methods are based on modeling the response curve, which describes the relationship between the observed signal and the protein concentration. Mircean  et al.  ( 2005 ) realized that since it is the same protein being measured for all the samples spotted on an RPPA slide, the same response curve should be suitable for all these samples. Based on this assumption, Microean  et al.  proposed a robust linear-square method to quantify the protein levels. However, an obvious drawback of the method is that it fails to recognize saturation effects for proteins at high levels. Recently, Hu  et al.  ( 2007 ) developed an alternative method using a non-linear, non-parametric approach to model the response curve. In this study, we show an alternative approach to RPPA data analysis. Instead of modeling the response curve, we construct a new model, serial dilution curve, which characterizes the relationship between signals in successive dilution steps. The advantage of this approach is two fold: (i) the signals in successive dilutions can be related to each other in explicit formula in which the underlying unknown protein concentrations do not appear. This allows a low-dimensional non-linear optimization to estimate the key parameters of the map between protein concentration and signal intensity. The estimated map can then be applied to the observed signals to estimate the underlying abundances; (ii) it leads to an intuitive display of raw data, which is very useful for checking data quality and interpreting the model. 2 METHODS 2.1 Serial dilution curve Our new method is based on the recognition that the relationship between signals in successive dilution steps uniquely determines the response curve. Typically, a response curve is a monotonic, s-shaped curve. It can be described by the Sips model (Sips,  1948 ):
 (1) 
where  a  is the background noise;  b  is the response rate in the linear range;  M  is the maximum or saturation level,  x  is the concentration of the protein. Sips model has been widely used to describe adsorption including binding of DNA (Glazer  et al. ,  2006 ) and proteins on solid surface (Vijayendran and Leckband,  2001 ). Generally, γ≠1 applies to conditions in which the free energy of binding of the solute molecules can take a range of values instead of a unique value (Sips,  1948 ), i.e. there is some hereterogeneity in the solute molecules or the surface receptors. When the range of the free energy of binding shrinks to a singular point, γ approaches to 1, in which case it is equivalent to the conventional Langmiur model. With RPPA technology, one can only determine the relative protein concentration. Thus,  x  can be chosen on an arbitrary scale. For simplicity, we set  x  on a scale (i.e. a physical unit of  x ) so that  b =1. Thus,
 (2) 
On this scale, protein concentration equals the background subtracted signal ( S − a ) when γ=1 and saturation effect can be ignored. Starting from Equation ( 2 ), we can see that if the protein concentration is diluted from  x  to  x / d k  at the  k -th dilution step, where  d &gt;1, the expected signal would be:
 (3) Combine the cases for  S k +1  and  S k  and eliminate  x , we have:
 (4) Equation ( 3 ) describes  S k  as a function of  S k +1 , which we call the serial dilution curve, with three unknown parameters:  a ,  M  and γ ( d  is known). These parameters have graphical interpretations from the plot. As shown in  Figure 1 , the curve has two intersection points with identity line: one at background level,  S k = S k +1 = a , the other at the saturation level,  S k = S k +1 = M . At the left side in  Figure 1 , the saturation effect is of no concern and the relationship between  S k  and  S k +1  is approximately linear,
 (5) 
 Fig. 1. Serial dilution plot. Each point in the serial dilution plot is composed of an observed signal  S k  at dilution step  k  (on  x -axis) and a corresponding signal  S k +1 of the same sample at the dilution step  k +1 (on  y -axis). The curve was produced using Equation ( 3 ). The curve has two intersection points with the identity line: ( a ,  a ) and ( M ,  M ). Thus,  d γ  corresponds to the slope in the linear range in the serial dilution plot. Equation ( 4 ) suggests a new model for displaying and analyzing RPPA data. It is important to note that Equation ( 4 ) does not contain protein concentration. Thus, it permits an appealing way of displaying the raw data without model specification or parameterization. Based on the plot like  Figure 1 , we can infer the parameters ( a ,  M  and γ) from the graph or through model fitting without knowing the protein concentrations in the samples. Model fitting with Equation ( 4 ) is relatively simpler than that with model fitting with Equation ( 2 ), which involves much more unknown parameters as in the existing methods of RPPA data analysis. Altogether, the number of unknown parameters in the model with Equation ( 2 ) is three plus the number of protein samples (each dilution series count as one sample), which can be in the hundreds. In contrast, Equation ( 4 ) only involves three unknown parameters. 2.2 Parameterization of the serial dilution curve To find the optimal parameters, we used a weighted non-linear regression model using Equation ( 4 ) as the model and taking  a ,  D = d γ ,  M  as parameters. We assumed the observed signals have multiplicative errors except for the signals close to zero. The weight used in the regression model is 1/( m +| S |), where  m =5, which is taken as the minimal error from signal quantification from the scanner used to obtain RPPA data. The starting values of  a ,  D  and  M  were taken to be max( m , min(S)),  d , max( S ), respectively. The  nls  function implemented in R-language (Ihaka and Gentleman,  1996 ) was used to optimize the parameters. The  m  is set to be the lower bound of  a . 2.3 Estimating protein concentrations Given the parameters in Equation ( 4 ) and signals of a dilution series of a particular sample (let these be  S 0 ,  S 1 ,  S 2 ,…,  S K ), to obtain protein concentration   in the original undiluted sample, we used the following procedure. First, if all these signals are greater than  M / r , the protein concentration   is marked to be saturated. This threshold value of  M / r  is set according to an approximate estimate of the 95% confidence interval (CI) of the signals at the saturated spots. Under multiplicative error model, assume that the error rate of the observed signals is ɛ=10%, and the saturation level is  M , we expect the CI to be [ M /(1+2×ɛ),  M (1+2×ɛ)]=[ M /1.2, 1.2 M ]. Similarly, at background level  a , we expect the 95% CI to be [ a /(1+2×ɛ),  a (1+2×ɛ)]=[ a /1.2, 1.2 a ]. In general,  r  should be &gt;1 and can be reduced if precision of signals is improved. If all the signals except one are &gt; M / r  and the exception is not  S K ,   is also marked to be saturated. Similarly, if all the signals are &lt; ar ,   is marked to be undetected. If all of them except one are &gt; M / r  and the exception is not  S 0 ,   is also marked to be undetected. The minimum and maximum of   are set to be
 (6) 
 (7) 
respectively. The above steps were taken to stabilize the protein concentration estimates for out of linear-range measurements. If   is not marked saturated or undetected, we proceed to make an estimate of   We choose to remove signals &gt; M / r  or &lt; ar . Then, we convert each of the remaining signals  S j  to  x j  as
 (8) 
where  j  denotes the  j -th dilution step. To remove outliers among  x j s, we identify an outlier among  x j s as
 
where mad( x ) is the median absolute deviation of  x . Here,  x  is the vector of all  x j s. Note that the outliers can also be identified from the serial dilution plot as points far away from the dilution curve (e.g.  Fig. 3 A). Finally, we give the estimate of the dilution series as a weighted average of  x j s:
 (9) 
where
 (10) 
the partial derivatives are derived and computed according to Equation ( 8 ); Δ a, Δ M and  Δγ are standard deviations of  a , M , γ, respectively, which are obtained from the  nls  function in R. The estimated error of   is obtained from (Σ w j ) −1/2 . 3 RESULTS To test the utility of the serial dilution curve for analyzing RPPA data, we first applied the method to simulated data, which was composed according to the Sips model [See Equation ( 2 ) in  Section 2 ], with background level  a =100, saturation level  M =50 000 and γ=1, dilution factor  d =2. We added multiplicative noise (error rate=0.15) to nominal signals and generated data as shown in  Figure 2 A. The multiplicative error model has been previously suggested (Kreutz  et al. ,  2007 ). The samples were diluted to 1/2, 1/4 and 1/8 of their original concentrations serially.  Figure 2 B shows the serial dilution plot, which contains all data in the dilution series. Each point in the serial dilution plot is composed of an observed signal at dilution step  k  (on  y -axis) and a corresponding signal of the same sample at the dilution step  k +1 (on  x -axis).
 Fig. 2. Computer simulations. ( A ) Computer generated data with serial dilutions. Red, yellow, green, blue represent undiluted concentrations, 1/2, 1/4, 1/8 original concentrations, respectively. ( B ) Serial dilution plot. The blue line shows the estimated serial dilution curve. ( C ) The estimated versus the ‘true’ concentrations. The dashed lines show the upper (shown in green) and lower (shown in blue) bounds of the estimated concentrations according to Equations ( 5 ) and ( 6 ). The red line shows the identity lines. ( D ) Estimated error rates. CV=estimated error/estimated concentration. ( E ) Signal versus estimated concentrations. Red, yellow, green, blue represent undiluted concentrations, 1/2, 1/4, 1/8 original concentrations, respectively. We found that our algorithm was able to recover the ‘true’ parameters from the simulated signals accurately. The values of  a ,  M  and γ were found to be 98± 5, 49 800± 520, 1.05±0.01, respectively. The estimated protein concentrations are also accurate ( Figure 2 C), except for the cases which are clearly out of the linear range. The lower and the upper bound of the range were calculated using Equations ( 6 ) and ( 7 ) and shown as dashed lines in  Figure 2 C. Note that setting the lower and upper bound helps to stabilize the estimates of protein concentration on logarithm scale, so that small changes in observed signals do not incur large changes in the estimates. Compare  Figure 2 A and C, one can also see that the linear range is much wider in the latter, showing that the dilution series can greatly expand the linear response range of the measurements. We have also tested our algorithm with experimental data.  Figure 3  shows a typical example of RPPA dataset. The experimental methods used to produce the array data were described by Fan  et al.  ( 2007 ). From the serial dilution plot ( Fig. 3 A), we notice many outliers (marked by red plus signs) near both  x - and  y -axis. Inspection of the original scanned image revealed that these outliers were produced by a faulty background subtraction method that extracted signals from the scanned image. The image quantification method took median pixel intensities from local regions outside the spotted area as the background level. However, occasionally the protein samples seemed to spill over the spotted area, which caused grossly overestimated background levels, which in turn led to grossly underestimated signals.
 Fig. 3. Example of a practical dataset. The measured protein is beta actin, which serves as a control standard for measurements. ( A ) Serial dilution plot. Points shown in red were regarded as outliers or saturated (circled). ( B ) Signal versus estimated concentration. The signals of undiluted samples are shown in red, 1/2 diluted samples in green and 1/4 diluted samples in blue. ( C ) Estimated error rates. CV=estimated error/estimated concentration. Each point represents result from one serial dilution. ( D ) Estimated protein concentrations from replicated dilution series of the same samples. Figure 3 A also showed that all the signals are bounded below 65 000 (the points close to the upper bound are marked by the red circles). This was caused by imaging software that set the maximum pixel intensity to be 65 536. Thus, the real signals must have been truncated for these spots. We therefore removed the points shown in red in  Figure 3 A before fitting the serial dilution curve. The estimated parameters are  a =5,  M =63 602, γ=0.57. The estimated protein concentrations were shown in  Figure 3 B. Sometimes RRPA experiment may fail to yield meaningful measurements of proteins. In  Figure 4 , we show an example that has quality problems. The experimental methods used to produce the array data was described by Tibes  et al.  ( 2006 ). Using methods as described in  Section 2.2 , the background was estimated to be 1000, saturation level: 4751, dilution factor: 1.11. The black line is the identity line and the blue line is the serial dilution curve. The serial dilution curve (blue) is very close to the identity line (black), indicating that after dilution, the signals tend to stay at the same levels as before. This implies that the dilution had failed to produce the expected reduction of signals. The exact cause of this effect is unclear. From our observations, such pattern often occurs in the slides that have faint signals. Furthermore, because the serial dilution curve is approximately linear, the saturation level cannot be accurately determined.
 Fig. 4. Example of data with quality problems. This is a serial dilution plot. The measured protein is GAPDH. The red symbols show the outliers. The background is estimated to be 1000, saturation level: 4751, dilution factor: 1.11. The black line is the identity line and the blue line is the serial dilution curve. To evaluate data quality on an array, we find the following two measures to be most important according to our empirical experience.
 V1=Percentage of data points in linear range (as defined by the interval [ ar ,  M / r ]) of all data points on the array, where  a  is the background level,  M  is the saturation level,  r  is the threshold value (as described earlier). High V1 value indicates good quality of data. When V1 is low, the data points are out of the linear range, in which cases extra manipulation of protein concentration in the samples is needed prior to hybridization on arrays. Alternatively, the level of antibody can be adjusted so that more data points will may fall in the linear range. In addition, note that the distribution of the data points can also inform the significance of non-linear effects. When most data points are far below the saturation level, the serial dilution curve approaches a straight line, in which case the saturation level is uncertain (for example, see  Fig. 4 ). V2=median CV on an array, where CV=estimated error/estimated protein concentration. V2 represents estimated error rate. High precision of protein concentration measurements is represented by low V2 values. 
 4 DISCUSSION Graphical display of data plays a very important role in data analysis. For RPPA data, it is conventional to plot the observed signals against the estimated protein concentrations. However, because the estimated protein concentrations depend on the models as well as the estimated parameters, when the signals seem to fit poorly to the estimated concentrations, it is not clear whether it is due to a suboptimal model or to noisy data. Making the serial dilution plot  per se  requires no model selection or parameter fitting. The plot presents the entire set of observables on an array in their original values. From the plot one can identify the background level, saturation level, which signals are in the linear range, and which signals are outliers (as in  Fig. 3 A). Fitting a serial dilution curve needs only three parameters, which is much simpler than fitting the response curve, which requires estimating the protein concentrations as additional parameters. From simulated RPPA data, we showed that our algorithm can yield robust and accurate estimates of protein concentrations. From practical RPPA data, we saw some of the data points did not follow the serial dilution curve. There may be multiple causes of the abnormal points, such as saturation or failure of binding. It should be noted that the response curve in RPPA technology is sensitive to a large number of factors, including the amount and duration of sample incubation, specific and non-specific interactions of reporter molecules and surface chemistry in the microarrays (Seurynck-Servoss  et al. ,  2007 ). These factors complicate the interpretation of RPPA data. Non-parametric models (Hu  et al.   2007 ) take fewer assumptions about the hybridization kinetics in RPPA technology. Hence, the non-parametric models are more flexible, and in some cases they may fit better with observed RPPA data. The disadvantage of non-parametric models is that the parameters are less interpretable, while the parameters in Sips model are physically meaningful and can be used to optimize the conditions for RPPA experiments. We believe the method developed in this study will have broad utility in RRPA applications. Funding : M. D. Anderson Cancer Center start-up fund; MDACC Institutional Research Grant (to L.Z.). Conflict of Interest : none declared. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>RapMap: a rapid, sensitive and accurate tool for mapping RNA-seq reads to transcriptomes</Title>
    <Doi>10.1093/bioinformatics/btw277</Doi>
    <Authors>Srivastava Avi, Sarkar Hirak, Gupta Nitish, Patro Rob</Authors>
    <Abstract>Motivation: The alignment of sequencing reads to a transcriptome is a common and important step in many RNA-seq analysis tasks. When aligning RNA-seq reads directly to a transcriptome (as is common in the de novo setting or when a trusted reference annotation is available), care must be taken to report the potentially large number of multi-mapping locations per read. This can pose a substantial computational burden for existing aligners, and can considerably slow downstream analysis.</Abstract>
    <Body>1 Introduction The bioinformatics community has put tremendous effort into building a wide array of different tools to solve the read-alignment problem efficiently. These tools use many different strategies to quickly find potential alignment locations for reads; for example, Bowtie ( Langmead  et al. , 2009 ), Bowtie 2 ( Langmead and Salzberg, 2012 ), BWA ( Li and Durbin, 2009 ) and BWA-mem ( Li, 2013 ) use variants of the FM-index, while tools like the Subread aligner ( Liao  et al. , 2013 ), Maq ( Li  et al. , 2008 ) and MrsFast ( Hach  et al. , 2010 ) use k-mer-based indices to help align reads efficiently. Because read alignment is such a ubiquitous task, the goal of such tools is often to provide accurate results as quickly as possible. Indeed, recent alignment tools like STAR ( Dobin  et al. , 2013 ) demonstrate that rapid alignment of sequenced reads is possible, and tools like HISAT ( Kim  et al. , 2015 ) demonstrate that this speed can be achieved with only moderate memory usage. When reads are aligned to a collection of reference sequences that share a substantial amount of sub-sequence (near or exact repeats), a single read can have many potential alignments, and considering all such alignment can be crucial for downstream analysis (e.g. considering all alignment locations for a read within a transcriptome for the purpose of quantification,  Li and Dewey (2011) , or when attempting to cluster  de novo  assembled contigs by shared multi-mapping reads,  Davidson and Oshlack, 2014 ). However, reporting multiple potential alignments for each read is a difficult task, and tends to substantially slow down even efficient alignment tools. Yet, in many cases, all of the information provided by the alignments is not necessary. For example, in the transcript analysis tasks mentioned above, simply the knowledge of the transcripts and positions to which a given read maps well is sufficient to answer the questions being posed. In support of such ‘analysis-efficient’ computation, we propose a novel concept, called quasi-mapping, and an efficient algorithm implementing quasi-mapping (exposed in the software tool RapMap) to solve the problem of mapping sequenced reads to a target transcriptome. This algorithm is  considerably  faster than state-of-the-art aligners, and achieves its impressive speed by exploiting the structure of the transcriptome (without requiring an annotation), and eliding the computation of full-alignments (e.g. CIGAR strings). Further, our algorithm produces mappings that meet or exceed the accuracy of existing popular aligners under different metrics of accuracy. Finally, we demonstrate how the mappings produced by RapMap can be used in the downstream analysis task of transcript-level quantification from RNA-seq data, by modifying the Sailfish ( Patro  et al. , 2014 ) tool to take advantage of quasi-mappings, as opposed to individual k-mer counts, for transcript quantification. We also demonstrate how quasi-mappings can be used to effectively cluster contigs from  de novo  assemblies. We show that the resulting clusterings are of comparable or superior accuracy to those produced by recent methods such as CORSET ( Davidson and Oshlack, 2014 ), but that they can be computed  much  more quickly using quasi-mapping. 2 Methods The quasi-mapping concept, implemented in the tool RapMap, is a new mapping technique to allow the rapid and accurate mapping of sequenced fragments (single or paired-end reads) to a target transcriptome. RapMap exploits a combination of data structures—a hash table, suffix array (SA) and efficient rank data structure. It takes into account the special structure present in transcriptomic references, as exposed by the SA, to enable ultra-fast and accurate determination of the likely loci of origin of a sequencing read. Rather than a standard alignment, quasi-mapping produces what we refer to as fragment  mapping  information. In particular, it provides, for each query (fragment), the reference sequences (transcripts), strand and position from which the query may have likely originated. In many cases, this mapping information is sufficient for downstream analysis. For example, tasks like transcript quantification, clustering of  de novo  assembled transcripts and filtering of potential target transcripts can be accomplished with this mapping information. However, this method does not compute the base-to-base alignment between the query and reference. Thus, such mappings may not be appropriate in every situation in which alignments are currently used (e.g. variant detection). We note here that the concept of quasi-mapping shares certain motivations with the notions of lightweight-alignment ( Patro  et al. , 2015 ) and pseudoalignment ( Bray  et al. , 2016 ). Yet, all three concepts—and the algorithms and data structures used to implement them—are distinct and, in places, substantially different. Lightweight-alignment scores potential matches based on approximately consistent chains of super-maximal exact matches shared between the query and targets. Therefore, it typically requires some more computation than the other methods, but allows the reporting of a score with each returned mapping and a more flexible notion of matching. Pseudoalignment, as implemented in Kallisto, refers only to the process of finding  compatible  targets for reads by determining approximately matching paths in a colored De Bruijn graph of a pre-specified order. Among compatible targets, extra information concerning the mapping (e.g. position and orientation) can be extracted  post hoc , but this requires extra processing, and the resulting mapping is no longer technically a pseudoalignment. Quasi-mapping seeks to find the  best  mappings (targets and positions) for each read, and does so (approximately) by finding minimal collections of dynamically sized, right-maximal, matching contexts between target and query positions. Quasi-mapping is inspired by both lightweight-alignment ( Patro  et al.  (2015) ) and pseudoalignment ( Bray  et al. , 2016 ), and while each of these approaches provide some insight into the problems of alignment and mapping, they represent distinct concepts and exhibit unique characteristics in terms of speed and accuracy, as demonstrated below (We do not compare against lightweight-alignment here, as no stand-alone implementation of this approach is currently available). 2.1 An algorithm for Quasi-mapping The algorithm we use for quasi-mapping makes use of two main data structures, the generalized SA ( Manber and Myers, 1993 ) SA[T] of the transcriptome  T , and a hash table  h  mapping each k-mer occurring in  T  to its SA interval (by default  k   =  31). Additionally, we must maintain the original text  T  on which the SA was constructed, and the name and length of each of the original transcript sequences.  T  consists of a string in which all transcript sequences are joined together with a special separator character. Rather than designating a separate terminator  $ i  for each reference sequence in the transcriptome, we make use of a single separator $, and maintain an auxiliary rank data structure, which allows us to map from an arbitrary position in the concatenated text to the index of the reference transcript in which it appears. We use the rank9b algorithm and data structure of  Vigna (2008)  to perform the rank operation quickly. Quasi-mapping determines the mapping locations for a query read  r  through repeated application of (i) determining the next hash table k-mer that starts past the current query position, (ii) computing the maximum mappable prefix (MMP) of the query beginning with this k-mer and then (iii) determining the next informative position (NIP) by performing a longest common prefix (LCP) query on two specifically chosen suffixes in the SA. The algorithm begins by hashing the k-mers of  r , from left-to-right (a symmetric procedure can be used for mapping the reverse-complement of a read), until some k-mer  k i —the k-mer starting at position  i  within the read—is present in  h  and maps to a valid SA interval. We denote this interval as  I ( k i ) = [ b , e )   . Because of the lexicographic order of the suffixes in the SA, we immediately know that this k-mer is a prefix of all of the suffixes appearing in the given interval. However, it may be possible to extend this match to some longer substring of the read beginning with  k i . In fact, the longest substring of the read that appears in the reference and is prefixed by  k i  is exactly the MMP ( Dobin  et al. , 2013 ) of the suffix of the read beginning with  k i . We call this MMP i , and note that it can be found using a slight variant of the standard SA binary search ( Manber and Myers, 1993 ) algorithm. For speed and simplicity, we implement the ‘simple accelerant’ binary search variant of  Gusfield (1997) . Because we know that any substring that begins with  k i  must reside in the interval [b,e), we can restrict the MMP i  search to this region of the SA, which is typically small. After determining the length of MMP i  within the read, one could begin the search for the next mappable SA interval at the position following this MMP. However, though the current substring of the read will differ from all of the reference sequence suffixes at the base following MMP i , the suffixes occurring at the lower and upper bounds of the SA interval corresponding to MMP i  may not differ from each other (see  Fig. 1 ). That is, if  I ( MMP i ) = [ b ′ , e ′ )    is the SA interval corresponding to MMP i , it is possible that  | LCP ( T [ SA [ b ′ ]   ]   , T [ SA [ e ′ − 1 ]   ]   ) )   | &gt; | MMP i |   . In this case, it is most likely that the read and the reference sequence bases following MMP i  disagree as the result of a sequencing error, not because the (long) MMP discovered between the read and reference is a spurious match. Thus, beginning the search for the next MMP at the subsequent base in the read may not be productive, as the matches for this substring of the query may not be informative—that is, such a search will likely return the same (relative) positions and set of transcripts. To avoid querying for such substrings, we define and make use of the notion of the NIP. The notion of the NIP in the algorithm we present for quasi-mapping is motivated by the ‘k-mer skipping’ approach adopted in Kallisto ( Bray  et al. , 2016 ), though the manner in which this information is obtained is different (as are actual positions themselves), since the NIPs computed for quasi-mapping depend on the preceding matching context, which is of a dynamic and variable length. For a MMP i , with  I ( MMP i ) = [ b ′ , e ′ )   , we define  NIP ( MMP i ) = | LCP ( T [ SA [ b ′ ]   ]   , T [ SA [ e ′ − 1 ]   ]   ) )   | + 1 . Intuitively, the NIP of prefix MMP i  is designed to return the next position in the query string where a SA search is likely to yield a set of transcripts different from those contained in  I ( MMP i ) . To compute the LCP between two suffixes when searching for the NIP, we use the ‘direct min’ algorithm of  Ilie  et al.  (2010) . We found this to be the fastest approach. Additionally, it does not require the maintenance of an LCP array or other auxiliary tables aside from the standard SA.
 Fig. 1. The transcriptome (consisting of transcripts  t 1 , … , t 6 ) is converted into a  $ -separated string,  T , on which a suffix array, SA[T], and a hash table,  h , are constructed. The mapping operation begins with a k-mer (here,  k  = 3) mapping to an interval  [ b , e )    in SA[T]. Given this interval and the read, MMP i  and NIP(MMP i ) are calculated as described in section 2. The search for the next hashable k-mer begins  k  bases before NIP(MMP i ) 
 Given the definitions we have explained above, we can summarize the quasi-mapping procedure as follows (an illustration of the mapping procedure is provided in  Fig. 1 ). First, a read is scanned from left to right (a symmetric procedure can be used for mapping the reverse-complement of a read) until a k-mer  k i  is encountered that appears in  h . A lookup in  h  returns the SA interval  I ( k i )  corresponding to the substring of the read consisting of this k-mer. Then, the procedure described above is used to compute MMP i  and  ℓ = NIP (MMP i ). The search procedure then advances to position  i + ℓ − k  in the read, and again begins hashing the k-mers it encounters. This process of determining the MMP and NIP of each processed k-mer and advancing to the NIP in the read continues until the NIP exceeds position  l r − k  where  l r  is the length of the read  r . The result of applying this procedure to a read is a set  S = { ( q 0 , o 0 , [ b 0 , e 0 )   ) , ( q 1 , o 1 , [ b 1 , e 1 )   ) , … }  of query positions, MMP orientations and SA intervals, with one such triplet corresponding to each MMP. The final set of mappings is determined by a consensus mechanism. Specifically, the algorithm reports the intersection of transcripts appearing in all hits—i.e. the set of transcripts that appear (in a consistent orientation) in every SA interval appearing in  S . These transcripts, and the corresponding strand and location on each, are reported as  quasi-mappings  of this read. This lightweight consensus mechanism is inspired by Kallisto ( Bray  et al. , 2016 ), though certain differences exist (e.g. quasi-mapping requires all hits to be orientation-consistent, and, since transcript identifiers are obtained from generalized transcriptome positions via a rank calculation, the mapping positions for each hit—and therefore, each read—are immediately available, rather than decodable as auxiliary information). These mappings are reported in a samtools-compatible format in which the relevant information (e.g. target id, position, strand, pair status) is computed from the mapping. We note that alternative consensus mechanisms, both more and less stringent, are easy to enforce given the information contained in the hits (e.g. ensuring that the hits are co-linear with respect to both the query and reference can be done by passing RapMap the -c flag, and δ-consistency (Patro et al., 2015) can also be easily enforced). However, below, we consider this simple consensus mechanism. Intuitively, RapMap’s combination of speed and accuracy result from the manner in which it exploits the nature of exactly repeated sequence that is prevalent in transcriptomes (either as a result of alternative splicing or paralogous genes). In addition to efficient search for MMPs and NIPs, the SA allows RapMap to encode exact matches between the query and many potential transcripts efficiently (in the form of ‘hits’). This is because all reference locations for a given MMP appear in consecutive entries of the SA, and can be encoded efficiently by simply recording the SA interval corresponding to this MMP. By aggressively filtering the hits to determine the set of ‘best’ matching transcripts and positions, RapMap is able to quickly discard small matches that are unlikely to correspond to a correct mapping. Similarly, the large collection of exact matches that appear in the reported mapping are likely to appear in the alignment (were the actual alignments to be computed). In some sense, the success of the strategy adopted by RapMap further validates the claim of  Liao  et al.  (2013)  that the seed-and-vote paradigm can be considerably more efficient than the seed-and-extend paradigm, as RapMap adopts neither of these paradigms directly, but its approach is more similar to the former than the latter. In the next section, we analyze how this algorithm for quasi-mapping, as described above, compares with other aligners in terms of speed and mapping accuracy. 3 Mapping speed and accuracy To test the practical performance of quasi-mapping, we compared RapMap against a number of existing tools, and analyzed both the speed and accuracy of these tools on synthetic and experimental data. Benchmarking was performed against the popular aligners Bowtie 2 ( Langmead and Salzberg, 2012 ) (v2.2.6) and STAR ( Dobin  et al. , 2013 ) (v2.5.0c) and the recently introduced pseudoalignment procedure used in the quantification tool Kallisto ( Bray  et al. , 2016 ) (v0.42.4). All experiments were scripted using Snakemake ( Köster and Rahmann, 2012 ) and performed on a 64-bit linux server with 256 GB of RAM and 4 × 6-core Intel Xeon E5-4607 v2 CPUs running at 2.60 GHz. Wall-clock time was recorded using the time command. In our testing we find that Bowtie 2 generally performs well in terms of reporting the true read origin among its set of multi-mapping locations. However, it takes considerably longer and tends to return a larger set of multi-mapping locations than the other methods. In comparison with Bowtie 2, STAR is  substantially  faster but somewhat less accurate. RapMap achieves accuracy comparable or superior to Bowtie 2, while simultaneously being much faster than even STAR. Kallisto is similar to (slightly slower than) RapMap in terms of single-threaded speed, and exhibits accuracy similar to that of STAR. For both RapMap and Kallisto, simply writing the output to disk tends to dominate the time required for large input files with significant multi-mapping (though we eliminate this overhead when benchmarking). This is due, in part, to the verbosity of the standard SAM format in which results are reported, and suggests that it may be worth developing a more efficient and succinct output format for mapping information. 3.1 Speed and accuracy on synthetic data To test the accuracy of different mapping and alignment tools in a scenario where we know the true origin of each read, we generated data using the Flux Simulator ( Griebel  et al. , 2012 ). This synthetic dataset was generated for the human transcriptome from an annotation taken from the ENSEMBL ( Cunningham  et al. , 2015 ) database consisting of 86 090 transcripts corresponding to protein-coding genes. The dataset consists of  ∼ 48  million 76 bp, paired-end reads. The detailed parameters used for the Flux Simulator can be found in  Supplementary Appendix 1.2 . When benchmarking these methods, reads were aligned directly to the transcriptome, rather than to the genome. This was done because we wish to benchmark the tools in a manner that is applicable when the reference genome may not even be known (e.g. in  de novo  transcriptomics). The parameters of STAR (see  Supplementary Appendix 1.1 ) were adjusted appropriately for this purpose (e.g. to dis-allow introns). Similarly, Bowtie 2 was also used to align reads directly to the target transcriptome; the parameters for Bowtie 2 are given in  Supplementary Appendix 1.1 . 3.1.1 Mapping speed We wish to measure, as directly as possible, just the time required by the mapping algorithms of the different tools. Thus, when benchmarking the runtime of different methods, we do not save the resulting alignments to disk. Further, to mitigate the effect of ‘outliers’ (a small number of reads which map to a large number of low-complexity reference positions), we bound the number of different transcripts to which a read can map to be 200. Additionally, we have also benchmarked Kallisto, but have not included the results in  Figure 2 , as the software, unlike the other methods, does not allow multi-threaded execution if mappings are being reported. Thus, we ran Kallisto with a single thread, using the –pseudobam flag and redirecting output to /dev/null to avoid disk overhead. Kallisto requires 17.87 m to map the 48M simulated reads, which included &lt;1 m of quantification time. By comparison, RapMap required 11.65 m to complete with a single thread.
 Fig. 2. The time taken by Bowtie 2, STAR and RapMap to process the synthetic data using varying numbers of threads. RapMap processes the data substantially faster than the other tools, while providing results of comparable or better accuracy 
 Finally, we note Kallisto, STAR and RapMap require 2–3× the memory of Bowtie 2, but all of the methods tested here exhibit reasonable memory usage. The synthetic set of 48 million reads can be mapped to an index of the entire human transcriptome on a typical laptop with 8 GB of RAM. As  Figure 2  illustrates, RapMap outperforms both Bowtie 2 and STAR in terms of speed by a substantial margin, and finishes mapping the reads with a single thread faster than STAR and Bowtie 2 with 10 threads. We consider varying the number of threads used by RapMap and STAR to demonstrate how performance scales with the number of threads provided. On this dataset, RapMap quickly approaches peak performance after using only a few threads. We believe that this is not owing to limits on the scalability of RapMap, but rather because the process is so quick that, for a dataset of this size, simply reading the index constitutes a large (and growing) fraction of the total runtime (dotted line) as the number of threads is increased. Thus, we believe that the difference in runtime between RapMap and the other methods may be even larger for datasets consisting of a large number of reads, where the disk can reach peak efficiency and the multi-threaded input parser (we use the parser from the Jellyfish ( Marçais and Kingsford, 2011 ) library) can provide input to RapMap quickly enough to make use of a larger number of threads. Because running Bowtie 2 with each potential number of threads on this dataset is time-consuming, we only consider Bowtie 2’s runtime using 10 threads. 3.1.2 Mapping accuracy Because the Flux Simulator records the true origin of each read, we make use of this information as ground truth data to assess the accuracy of different methods. However, as a single read may have multiple, equally good alignments with respect to the transcriptome, care must be taken in defining accuracy-related terms appropriately. A read is said to be correctly mapped by a method (a true positive) if the set of transcripts reported by the mapper for this read contains the true transcript. A read is said to be incorrectly mapped by a method (a false positive) if it is mapped to some set of 1 or more transcripts, none of which are the true transcript of origin. Finally, a read is considered to be incorrectly un-mapped by a method (a false negative) if the method reports no mappings, but the transcript of origin is in the reference. Given these definitions, we report precision, recall, F1-Score and false discovery rate (FDR) in  Table 1  using the standard definitions of these metrics. Additionally, we report the average number of ‘hits-per-read’ (hpr) returned by each of the methods. Ideally, we want a method to return the smallest set of mappings that contains the true read origin. However, under the chosen definition of a true-positive mapping, the number of reported mappings is not taken into account, and a result is considered a true positive so long as it contains the actual transcript of origin. The hpr metric allows one to assess how many  extra  mappings, on average, are reported by a particular method.
 Table 1.  Accuracy of aligners/mappers under different metrics Metric Bowtie 2 Kallisto RapMap STAR Reads aligned 47 579 567 44 804 857 47 613 536 44 711 604 Recall 97.41 91.60 97.49 91.35 Precision 98.31 97.72 98.48 97.02 F1-score 97.86 94.56 97.98 94.10 FDR 1.69 2.28 1.52 2.98 Hits per read 5.98 5.30 4.30 3.80 As expected, Bowtie 2— perhaps the most common method of directly mapping reads to transcriptomes— performs well in terms of precision and recall. However, we find that RapMap yields similar (in fact, slightly better) precision and recall. STAR and Kallisto obtain similar precision to Bowtie 2 and RapMap, but have lower recall. STAR and Kallisto perform similarly in general, though Kallisto achieves a lower (better) FDR than STAR. Taking the F1-score as a summary statistic, we observe that all methods perform reasonably well, and that, in general, alignment-based methods do not seem to be more accurate than mapping-based methods. We also observe that RapMap yields accurate mapping results that match or exceed those of Bowtie 2. Additionally, we tested the impact of noisy reads (i.e. reads not generated from the indexed reference) on the accuracy of the different mappers and aligners. To create these background reads, we use a model inspired by ( Gilbert  et al. , 2004 ), in which reads are sampled from nascent, un-spliced transcripts. The details of this experiment are included in  Supplementary Appendix 1.3 . 3.2 Speed and concordance on experimental data We also explore the concordance of RapMap with different mapping and alignment approaches using experimental data from the study of  Cho  et al.  (2014)  (NCBI GEO accession SRR1293902). The sample consists of  ∼ 26  million 75 bp, paired-end reads sequenced on an Illumina HiSeq. Because we do not know the true origin of each read, we have instead examined the agreement between the different tools (see  Fig. 3 ). Intuitively, two tools agree on the mapping locations of a read if they align/map this read to the same subset of the reference transcriptome (i.e. the same set of transcripts). More formally, we define the elements of our universe,  U , to be tuples consisting of a read identifier and the set of transcripts returned by a particular tool. For example, if, for read  r i , tool  A  returns alignments to transcripts  { t 1 , t 2 , t 3 }  then  e A i = ( r i , { t 1 , t 2 , t 3 } ) ∈ U . Similarly, if tool  B  maps read  r i  to transcripts  { t 2 , t 3 , t 4 }  then  e B i = ( r i , { t 2 , t 3 , t 4 } ) ∈ U . Here, tools  A  and  B  do not agree on the mapping of read  r i . Given a universe  U  thusly defined, we can use the normal notions of set intersection and difference to explore how different subsets of methods agree on the mapping locations of the sequenced reads. These concordance results are presented in  Figure 3 , which uses a bar plot to show the size of each set of potential intersections between the results of the tools we consider. In  Figure 3  the dot matrix below the bar plot identifies the tools whose results are intersected to produce the corresponding bar. Tools producing mappings and alignments are denoted with black and red dots and bars, respectively. The left bar plot shows the size of the unique tuples produced by each tool (alignments/mappings that do not match with any other tool). The right bar plot shows the total number of tuples produced by each tool, and well as the concordance among all different subsets of tools.
 Fig. 3. Mapping agreement between subsets of Bowtie 2, STAR, Kallisto andRapMap. 
 Under this measure of agreement, RapMap and Kallisto appear to agree on the exact same transcript assignments for the largest number of reads. Further, RapMap and Kallisto have the largest pairwise agreements with the aligners (STAR and Bowtie 2)—that is, the traditional aligners exactly agree more often with these tools than with each other. It is important to note that one possible reason we see (seemingly) low agreement between Bowtie 2 and other methods is because the transcript alignment sets reported by Bowtie 2 are generally larger (i.e. contain more transcripts) than those returned by other methods, and thus fail to qualify under our notion of agreement. This occurs, partially, because RapMap and Kallisto (and to some extent STAR) do not tend to return sub-optimal multi-mapping locations. However, unlike Bowtie 1, which provided an option to return only the best ‘stratum’ of alignments, there is no way to require that Bowtie 2 return only the best multi-mapping locations for a read. We observe similar behavior for Bowtie 2 (i.e. that it returns a larger set of mapping locations) in the synthetic tests as well, where the average number of hits per read is higher than for the other methods (see  Table 1 ). In terms of runtime, RapMap, STAR and Bowtie 2 take 3, 26 and 1020 min, respectively, to align the reads from this experiment using four threads. We also observed a similar trend in terms of the average number of hits per read here as we did in the synthetic dataset. The average number of hits per read on these data were 4.56, 4.68, 4.21 and 7.97 for RapMap, Kallisto, STAR and Bowtie 2, respectively. 4 Application of quasi-mapping for transcript quantification While mapping cannot act as a stand-in for full alignments in all contexts, one problem where similar approaches have already proven useful is transcript abundance estimation. Recent work ( Bray  et al. , 2016 ;  Patro  et al. , 2014 ,  2015 ;  Zhang and Wang, 2014 ) has demonstrated that full alignments are not necessary to obtain accurate quantification results. Rather, simply knowing the transcripts and positions where reads may have reasonably originated is sufficient to produce accurate estimates of transcript abundance. Thus, we have chosen to apply quasi-mapping to transcript-level quantification as an example application, and have implemented our modifications as an update to the Sailfish ( Patro  et al. , 2014 ) software, which we refer to as quasi-Sailfish. These changes are present in the Sailfish software from version 0.7 forward. Here, we compare this updated method to the transcript-level quantification tools RSEM ( Li  et al. , 2010 ), Tigar2 ( Nariai  et al. , 2014 ) and Kallisto ( Bray  et al. , 2016 ), the last of which is based on the pseudoalignment concept mentioned above. 4.1 Transcript quantification In an RNA-seq experiment, the underlying transcriptome consists of  M  transcripts and their respective counts. The transcriptome can be represented as a set  X = { ( t 1 , … , t M ) , ( c 1 , … , c M ) } , where  t i  denotes the nucleotide sequence of transcript  i  and  c i  denotes the number of copies of  t i  in the sample. The length of transcript  t i  is denoted by  l i . Under ideal, uniform, sampling conditions (i.e. without considering various types of experimental bias), the probability of drawing a fragment from a transcript  t i  is proportional to its nucleotide fraction ( Li  et al. , 2010 ) denoted by  η i = c i l i ∑ j = 1 M c j l j . If we normalize the  η i  for each transcript by its length  l i , we obtain a measure of the relative abundance of each transcript called the transcript fraction ( Li  et al. , 2010 ), which is given by  τ i = η i l i ∑ j = 1 η i l i . When performing transcript-level quantification,  η  and  τ  are generally the quantities we are interested in inferring. Because they are directly related, knowing one allows us to directly compute the other. Below, we describe our approach to approximating the estimated number of reads originating from each transcript, from which we estimate  τ , and subsequently transcripts per million (TPM). 4.2 Quasi-mapping-based Sailfish Using the quasi-mapping procedure provided by RapMap as a library, we have updated the Sailfish ( Patro  et al. , 2014 ) software to make use of quasi-mapping, as opposed to individual k-mer counting, for transcript-level quantification. In the updated version of Sailfish, the index command builds the quasi-index over the reference transcriptome as described in Section 2. Given the index and a set of sequenced reads, the quant command quasi-maps the reads and uses the resulting mapping information to estimate transcript abundances. To reduce the memory usage and computational requirements of the inference procedure, quasi-Sailfish reduces the mapping information to a set of equivalence classes over sequenced fragments. These equivalence classes are similar to those used in  Nicolae  et al.  (2011) , except that the position of each fragment within a transcript is not considered when defining the equivalence relation. Specifically, any fragments that map to exactly the same set of transcripts are placed into the same equivalence class. Following the notation of  Patro  et al.  (2015) , the equivalence classes are denoted as  C = { C 1 , C 2 , … } , and the count of fragments associated with equivalence class  C j  is given by  d j . Associated with each equivalence class  C j  is an ordered collection of transcript identifiers  t j = ( t j 1 , t j 2 , … ) , which is simply the collection of transcripts to which all equivalent fragments in this class map. We call  t j  the  label  of class  C j . 4.2.1 Inferring transcript abundances The equivalence classes  C  and their associated counts and labels are used to estimate the number of fragments originating from each transcript. The estimated count vector is denoted by  α , and  α i  is the estimated number of reads originating from transcript  t i . In quasi-Sailfish, we use the variational Bayesian expectation maximization (VBEM) algorithm to infer the parameters (the estimated number of reads originating from each transcript) that maximize a variational objective. Specifically, we maximize a simplified version of the variational objective of  Nariai  et al.  (2013) . The VBEM update rule can be written as a simple iterative update in terms of the equivalence classes, their counts and the prior ( α 0 ). The iterative update rule for the VBEM is:
 (1) α i u + 1 = α 0 + ∑ C j ∈ C d j ( e γ i u 1 l i ^ ∑ t k ∈ t j e γ k u 1 l k ^ ) , 
where
 (2) γ i u = Ψ ( α 0 + α i u ) − Ψ ( ∑ k α 0 + α k u ) 
and  Ψ ( · )  is the digamma function. Here,  l i ^  is the  effective  length of transcript  t i , computed as in  Li  et al.  (2010) . To determine the final estimated counts— α — Equation (1)  is iterated until convergence. The estimated counts are considered to have converged when no transcript has estimated counts differing by &gt;1% between successive iterations. Given  α , we compute the TPM for transcript  i  as
 (3) T P M i = 10 6 α i l i ^ ∑ j α j l j ^ . 
 Sailfish outputs, for each transcript, its name, length, effective length, TPM and the estimated number of reads originating from it. 4.3 Quantification performance comparison We compared the accuracy of quasi-Sailfish (Sailfish v0.9.0; q-Sailfish in  Table 2 ) to the transcript-level quantification tools RSEM ( Li  et al. , 2010 ) (v1.2.22), Tigar 2 ( Nariai  et al. , 2014 ) (v2.1) and Kallisto ( Bray  et al. , 2016 ) (v0.42.4) using six different accuracy metrics and data from two different simulation pipelines. One of the simulated datasets was generated with the Flux Simulator ( Griebel  et al. , 2012 ), and is the same dataset used in Section 3 to assess mapping accuracy and performance on synthetic data. The other dataset was generated using the RSEM simulator via the same methodology adopted by  Bray  et al.  (2016) . That is, RSEM was run on sample NA12716_7 of the Geuvadis RNA-seq data ( Lappalainen  et al. , 2013 ) to learn model parameters and estimate true expression. The learned model was then used to generate the simulated dataset, which consists of 30  million  75 bp paired-end reads.
 Table 2.  Performance evaluation of different tools along with quasi-enabled sailfish (q-Sailfish) with other tools on synthetic data generated by Flux simulator and RSEM simulator Metric Flux simulation RSEM-sim simulation Kallisto RSEM q-Sailfish Tigar 2 Kallisto RSEM q-Sailfish Tigar 2 Proportionality corr. 0.74 0.78 0.75 0.77 0.91 0.93 0.91 0.93 Spearman corr. 0.69 0.73 0.70 0.72 0.91 0.93 0.91 0.93 TPEF 0.77 0.96 0.60 0.59 0.53 0.49 0.53 0.50 TPME −0.24 −0.37 −0.10 −0.09 0.00 −0.01 0.00 0.00 MARD 0.36 0.29 0.31 0.26 0.29 0.25 0.29 0.23 wMARD 4.68 5.23 4.45 4.35 1.00 0.88 1.01 0.94 We measure the accuracy of each method based on the estimated versus true number of reads originating from each transcript, and consider six different metrics of accuracy: proportionality correlation ( Lovell  et al. , 2015 ), Spearman correlation, the true positive error fraction (TPEF), the true positive median error (TPME), the mean absolute relative difference (MARD) and the weighted mean absolute relative difference (wMARD). Detailed definitions for the last four metrics are provided in  Supplementary Appendix 1.5 . Each of these metrics captures a different notion of accuracy, and all are reported to provide a more comprehensive perspective on quantifier accuracy. The first two metrics—proportionality and Spearman correlation—provide a global notion of how well the estimated and true counts agree, but are fairly coarse measures. The TPEF assesses the fraction of transcripts where the estimate is different from the true count by more than some nominal fraction (here 10%). Unlike TPEF, the TPME metric takes into account the direction of the mis-estimate (i.e. is it an over or under-estimate of the true value?). However, both metrics are assessed only on truly expressed transcripts, and so provide no insight into the tendency of a quantifier to produce false positives. The absolute relative difference (ARD) metric has the benefit of being defined on all transcripts as opposed to only those that are truly expressed and ranges from 0 (lowest) to 2 (highest). Because the values of this metric are tightly bounded, the aggregate metric, MARD, is not dominated by high expression transcripts. Unfortunately, it therefore has limited ability to capture the magnitude of mis-estimation. The wMARD metric attempts to account for the magnitude of mis-estimation, while still trying to ensure that the measure is not completely dominated by high expression transcripts. This is done by scaling each ARD i  value by the logarithm of the expression. Table 2  shows the performance of all four quantifiers, under all six metrics, on both datasets. While all methods seem to perform reasonably well, some patterns emerge. RSEM seems to perform well in terms of the correlation metrics, but less well in terms of the TPEF, TPME and wMARD metrics (specifically in the Flux Simulator-generated dataset). This is likely a result of the lower mapping rate obtained on this data by RSEM’s strict Bowtie 2 parameters. Tigar 2 generally performs well under a broad range of metrics, and produces highly accurate results. However, it is  by far  the slowest method considered here, and requires over a day to complete on the Flux simulator data and almost 7 h to complete on the RSEM-sim data given 16 threads (and not including the time required for Bowtie 2 alignment of the reads). Finally, both quasi-Sailfish and Kallisto perform well in general under multiple different metrics, with quasi-Sailfish tending to produce somewhat more accurate estimates. Both of these methods also completed in a matter of minutes on both datasets. One additional pattern that emerges is that the RSEM-sim data appears to present a much simpler inference problem compared with the Flux Simulator data. One reason for this may be that the RSEM-sim data are ‘clean’—yielding concordant mapping rates well over 99%, even under RSEM’s strict Bowtie 2 mapping parameters. As such, all methods tend to perform well on these data, and there is comparatively little deviation between the methods under most metrics. For completeness, we also provide (in  Supplementary Appendix 1.4 ) the results, under all of these metrics, where the true and predicted abundances are considered in terms of TPM rather than number of reads. We find that the results are generally similar, with the exception that TIGAR 2 performs considerably worse under the TPM measure. 5 Application of quasi-mapping for clustering  de novo  assemblies Estimating gene-expression from RNA-seq reads is an especially challenging task when no reference genome is present. Typically, this problem is solved by performing  de novo  assembly of the RNA-seq reads, and subsequently mapping these reads to the resulting contigs to estimate expression. Owing to sequencing errors and artifacts, and genetic variation and repeats,  de novo  assemblers often fragment individual isoforms into separate assembled contigs.  Davidson and Oshlack (2014)  argue that better differential expression results can be obtained in  de novo  assemblies if contigs are first clustered into groups. They present a tool, CORSET, to perform this clustering, and compare their approach to existing tools such as CD-HIT ( Fu  et al. , 2012 ). CD-HIT compares the sequences (contigs) directly, and clusters them by sequence similarity. CORSET, alternatively, aligns reads to contigs (allowing multi-mapping) and defines a distance between each pair of contigs based on the number of multi-mapping reads shared between them, and the changes in estimated expression inferred for these contigs under different conditions. Hierarchical agglomerative clustering is then performed on these distances to obtain a clustering of contigs. Here, we show how RapMap can be used for the same task, by taking an approach similar to that of CORSET. First, we map the RNA-seq reads to the target contigs and simultaneously construct equivalence classes over the mapped fragments as in Section 4. We construct a weighted, undirected graph from these equivalence classes as follows. Given a set of contigs  c  and the equivalence classes  C , we construct  G = ( V , E )  such that  V = c , and  E = { { u , v } | ∃ j : u , v ∈ t j } . We define the weight of edge  { u , v }  as  w ( u , v ) = R u , v min ⁡ ( R u , R v ) . Here  R u  is the total number of reads belonging to all equivalence classes in which contig  u  appears in the label.  R v  is defined analogously.  R u , v  is the total sum of reads in all equivalence classes for which contigs  u  and  v  appear in the label. Given the undirected graph  G , we use the  Markov Cluster Algorithm , as implemented in MCL ( Van Dongen, 2000 ), to cluster the graph. To benchmark the time and accuracy of our clustering scheme compared with CD-HIT and CORSET, we used two datasets from the CORSET paper ( Davidson and Oshlack, 2014 ). The first dataset consists of 231 million human reads in total, across two conditions, each with three replicates (as originally described by  Trapnell  et al. , 2013 ). The second dataset, from yeast, was originally published by  Nookaew  et al.  (2012)  and consists 36 million reads, grown in two different conditions with three replicates each. For both of these datasets, we consider clustering the contigs of the corresponding  de novo  assemblies, which were generated using Trinity ( Grabherr  et al. , 2011 ). To measure accuracy, we consider the precision and recall induced by a clustering with respect to the true genes from which each contig originates. Assembled contigs were mapped to annotated transcripts using BLAT ( Kent, 2002 ), and labeled with their gene of origin. A pair of contigs from the same cluster is regarded as true positive (tp) if they are from the same gene in the ground truth set. Similarly, a pair is a false positive (fp) if they are not from same gene but are clustered together. A pair is a false negative (fn) if they are from same gene but not predicted to be in the same cluster and all the remaining pairs are true negatives (tn). With these definitions of tp, fp, tn and fn we can define precision and recall in standard manner. As shown in  Table 3 , when considering both precision and recall, RapMap (quasi-mapping) enabled clustering performs substantially better than CD-HIT and similar to CORSET. RapMap enabled clustering takes 8 min and 2 min to cluster the human and yeast datasets respectively—which is substantially faster than the other tools. To generate the timing results above, CD-HIT was run with 25 threads. The time recorded for CORSET consists of both the time required to align the reads using Bowtie 2 (using 25 threads) and the time required to perform the actual clustering, which is single threaded. The time recorded for RapMap enabled clustering consists of the time required to quasi-map the reads, build the equivalence classes and construct the graph (using 25 threads), plus the time required to cluster the graph with MCL (using a single thread). Overall, on these datasets, RapMap-enabled clustering appears to provide comparable or better clusterings than existing methods, and produces these clusterings much more quickly.
 Table 3.  Performance of CORSET, CD-HIT and RapMap enabled clustering (R-CL) on yeast and human data Metric Human Yeast CORSET CD-HIT R-CL CORSET CD-HIT R-CL precision 0.96 0.96 0.95 0.36 0.41 0.36 recall 0.56 0.37 0.60 0.63 0.36 0.71 time (min) 957 268 8 23 5 2 6 Discussion and conclusion In this article we have argued for the usefulness of our novel approach, quasi-mapping, for mapping RNA-seq reads. More generally, we suspect that read  mapping , wherein sequencing reads are assigned to reference locations, but base-to-base alignments are not computed, is a broadly useful tool. The speed of traditional aligners like Bowtie 2 and STAR is limited by the fact that they must produce optimal alignments for each location to which a read is reported to align. In addition to showing the speed and accuracy of quasi-mapping directly, we apply it to two problems in transcriptome analysis. First, we have updated the Sailfish software to make use of the quasi-mapping information produced by RapMap, rather than direct k-mer counts, for purposes of transcript-level abundance estimation. This update improves both the speed and accuracy of Sailfish, and also reduces the complexity of its codebase. We demonstrate, on synthetic data generated via two different simulators, that the resulting quantification estimates have accuracy comparable with state-of-the-art tools. We also demonstrate the application of RapMap to the problem of clustering  de novo  assembled contigs, a task that has been shown to improve expression quantification and downstream differential expression analysis ( Davidson and Oshlack, 2014 ). RapMap can produce clusterings of comparable or superior accuracy to those of existing tools, and can do so much more quickly. However, RapMap is a stand-alone mapping program, and need not be used only for the applications we describe here. We expect that quasi-mapping will prove a useful and rapid alternative to alignment for tasks ranging from filtering large read sets (e.g. to check for contaminants or the presence or absence of specific targets) to more mundane tasks like quality control and, perhaps, even to related tasks like metagenomic and metatranscriptomic classification and abundance estimation. We hope that the quasi-mapping concept, and the availability of RapMap and the efficient and accurate mapping algorithms it exposes, will encourage the community to explore replacing alignment with mapping in the numerous scenarios where traditional alignment information is unnecessary for downstream analysis. 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Pepitope: epitope mapping from affinity-selected peptides</Title>
    <Doi>10.1093/bioinformatics/btm493</Doi>
    <Authors>Mayrose Itay, Penn Osnat, Erez Elana, Rubinstein Nimrod D., Shlomi Tomer, Freund Natalia Tarnovitski, Bublil Erez M., Ruppin Eytan, Sharan Roded, Gershoni Jonathan M., Martz Eric, Pupko Tal</Authors>
    <Abstract>Identifying the epitope to which an antibody binds is central for many immunological applications such as drug design and vaccine development. The Pepitope server is a web-based tool that aims at predicting discontinuous epitopes based on a set of peptides that were affinity-selected against a monoclonal antibody of interest. The server implements three different algorithms for epitope mapping: PepSurf, Mapitope, and a combination of the two. The rationale behind these algorithms is that the set of peptides mimics the genuine epitope in terms of physicochemical properties and spatial organization. When the three-dimensional (3D) structure of the antigen is known, the information in these peptides can be used to computationally infer the corresponding epitope. A user-friendly web interface and a graphical tool that allows viewing the predicted epitopes were developed. Pepitope can also be applied for inferring other types of protein–protein interactions beyond the immunological context, and as a general tool for aligning linear sequences to a 3D structure.</Abstract>
    <Body>1 INTRODUCTION Mapping the epitope of an antibody on its corresponding antigen is fundamental to our understanding of the mechanisms of molecular recognition and provides the basis for rational vaccine and drug design (Irving  et al. ,  2001 ). Solving the three-dimensional (3D) structure of an antibody–antigen complex is the most accurate way to determine an epitope. When such a complex is not available (or cannot be obtained), a customary approach is to map the location of the epitope using a phage display library (Barbas  et al. ,  2001 ). In this technology, peptides that bind the antibody of interest with high affinity are selected from a large pool of random peptides. These affinity-selected peptides are assumed to mimic the genuine epitope in terms of their physicochemical properties and spatial organization. Hence, a resemblance of a selected peptide to a linear region of the antigen is often indicative of the genuine epitope location. However, epitopes are often conformational, being comprised of a number of discontinuous segments of the antigen brought together via folding. Thus, an affinity-selected peptide may have similarity only to non-linear fragments of the antigen. Advanced computational methods are hence needed to correctly infer the epitope from the selected peptides. Here we present the Pepitope server, which implements three algorithms for epitope mapping: PepSurf, Mapitope, and a combination of the two. Thus allowing users to run and compare two epitope prediction algorithms that rely on two different methodological approaches via a single web platform. These algorithms were proved successful in predicting discontinuous epitopes based on affinity-selected peptides (Bublil  et al. ,  2006 ,  2007 ; Enshell-Seijffers  et al. ,  2003 ; Mayrose  et al. ,  2007 ; Tarnovitski  et al. ,  2006 ). Furthermore, the combined algorithmic option executes both PepSurf and Mapitope and combines their results into a single epitope prediction. Pepitope enables the alignment of short, hypothetical or synthesized, sequences to a 3D structure and is thus not confined to mapping random peptides derived from phage-display experiments. Pepitope also provides a powerful visualization tool to examine the predicted epitopes and each of the peptide-to-structure alignments. It allows non-expert users to predict epitopes with no previous algorithmic knowledge, yet incorporates advanced options for expert users to fine-tune their predictions. Several algorithms and web servers for epitope mapping were previously developed (Castrignano  et al. ,  2007 ; Halperin  et al. ,  2003 ; Moreau  et al. ,  2006 ; Schreiber  et al. ,  2005 ). The main differences between Pepitope and the other servers are described at the ‘OVERVIEW’ web section. Although some of the features of Pepitope were previously suggested, it is the only tool that uniquely combines all of the following features: (1) It takes into account both identities and similarities, e.g. using the BLOSUM matrix, when aligning linear peptide sequences to the antigen 3D structure. (2) It allows any number of gaps in the alignment and distinguishes between terminal gaps and non-terminal ones. (3) It clusters the resulting matches on the protein surface, thus providing an epitope prediction, which is based on the entire set of peptides collectively. (4) It optionally accounts for the number of times a peptide was selected in the phage-display experiment. A short description of the Pepitope methodology is provided herein, and a more detailed description is available under the ‘OVERVIEW’, ‘GALLERY’ and ‘QUICK HELP’ web sections. The ease of using Pepitope is exemplified by predicting the human monoclonal antibody (mAb) Bo2C11 epitope for coagulation factor VIII (FVIII). 2 METHODS The PepSurf algorithm (Mayrose  et al. ,  2007 ) maps each of the affinity-selected peptides onto the surface of the antigen. This is done by efficiently searching virtually all possible 3D paths (i.e. a sequence of neighboring residues on the surface of the antigen structure) for those that exhibit high similarity to the peptide sequences. Thus, the best alignment of each peptide to antigen residues brought to proximity by folding is obtained. The resulting most significant alignments are then clustered and the epitope location is inferred. The Mapitope algorithm (Bublil  et al. ,  2007 ) is based on an alternative computational approach in which epitope determinants shared by the entire set of peptides are detected. Specifically, each peptide is first deconvoluted to amino acid pairs. Mapitope then identifies pairs of residues that are significantly overrepresented in the panel of peptides. It then searches for a cluster of these enriched pairs on the antigen 3D structure to obtain predicted epitope regions. The minimal input requirement for both algorithms is a set of affinity-selected peptide sequences and a PDB identifier of the antigen. Using these two inputs the following steps are carried out: (1) the exposed residues of the given structure are determined using the Surface Racer program (Tsodikov  et al. ,  2002 ). (2) A graph representing neighboring surface residues is created. (3) The epitope prediction algorithm (PepSurf, Mapitope, or both) is executed. (4) The predicted epitopes are projected and visualized on the 3D structure via a specifically designed 3D structural viewer. 2.1 Pepitope options Similar to classical sequence alignment algorithms, PepSurf relies on a specified amino acid similarity matrix and a gap penalty. The user can choose the gap penalty as well as a number of substitution matrices. The difference between PepSurf calculations using different matrices tends to be small but not negligible (see the ‘GALLERY’ web section). PepSurf's alignment algorithm guarantees finding the best alignment between each peptide and the PDB structure only up to a certain probability, which can also be set by the user. For Mapitope, a detailed description of its option parameters is available under the ‘OVERVIEW’ and ‘QUICK HELP’ web sections. 2.2 Pepitope output A graphic visualization of the results using the FirstGlance in Jmol interface ( http://firstglance.jmol.org ) is available by clicking ‘View Pepitope Results with FirstGlance in Jmol’. For the PepSurf algorithm, the corresponding alignment of each peptide to the 3D structure is also presented using the 3D viewer. The predicted clusters can be viewed in a text format under the ‘Predicted Clusters’ link. The link ‘Resulting alignments for each peptide’ leads to a textual listing of the top-scoring alignments for each peptide. Pepitope also supplies a RasMol command script for viewing the results locally with the RasMol program (Sayle and Milner-White,  1995 ). 3 EXAMPLE: COAGULATION FACTOR VIII EPITOPE We illustrate the mapping of the human mAb Bo2C11 epitope on the 3D surface structure of its antigen, FVIII. Using the Contact Map Analysis server (Sobolev  et al. ,  2005 ), 19 surface residues of FVIII were identified as comprising the ‘genuine epitope’ and were used as the benchmark epitope when determining the prediction accuracy. Bo2C11 was used to screen a combinatorial phage-display library resulting in a set of 27 affinity-selected peptides (Villard  et al. ,  2003 ).  Figure 1  illustrates the main Pepitope output. Pepitope predicts an epitope that significantly overlaps the benchmark epitope. The predicted epitope is comprised of 30 residues, of which 12 are part of the genuine epitope. The 18 remaining predicted residues, although false-positive predictions, are located in close proximity to the genuine epitope and may also contribute to the mAb binding.
 Fig. 1. The epitope prediction obtained by the PepSurf algorithm for the Bo2C11−FVIII complex (PDB id 1IQD). The FVIII antigen is shown as a space-filled model with the predicted epitope in red. The Bo2C11 antibody Fab fragment is shown as a backbone model. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>HiC-spector: a matrix library for spectral and reproducibility analysis of Hi-C contact maps</Title>
    <Doi>10.1093/bioinformatics/btx152</Doi>
    <Authors>Yan Koon-Kiu, Yardımcı Galip Gürkan, Yan Chengfei, Noble William S, Gerstein Mark, Berger Bonnie</Authors>
    <Abstract/>
    <Body>1 Introduction Genome-wide proximity ligation assays such as Hi-C have emerged as powerful techniques to understand the 3D organization of the genome ( Kalhor  et al. , 2011 ;  Lieberman-Aiden  et al. , 2009 ). Although these techniques offer new biological insights, they demand different data structures and present new computational questions ( Ay and Noble, 2015 ;  Dekker  et al. , 2013 ). For instance, a fundamental question of practical importance is, how can we quantify the similarity between two Hi-C data sets? In particular, given two experimental replicates, how can we determine if the experiments are reproducible? Data from Hi-C experiments are usually summarized by so-called chromosomal contact maps. By binning the genome into equally sized bins, a contact map is a matrix whose elements store the population-averaged co-location frequencies between pairs of loci. Therefore, mathematical tools like spectral analysis can be extremely useful in understanding these chromosomal contact maps. Our aim is to provide a set of basic analysis tools for handling Hi-C contact maps. In particular, we introduce a simple but novel metric to quantify the reproducibility of the maps using spectral decomposition. 2 Algorithms We represent a chromosomal contact map by a symmetric and non-negative adjacency matrix  W . The matrix elements represent the frequencies of contact between genomic loci. Recent single-cell imaging experiment suggests that the frequency serves as a reasonable proxy of spatial distance ( Wang  et al. , 2016 ). In principle, the larger the value of  W i j , the closer is the distance between loci  i  and  j . The starting point of spectral analysis is the Laplacian matrix  L , which is defined as  L = D - W . Here  D  is a diagonal matrix in which  D i i = ∑ j W i j  (the coverage of bin  i  in the context of Hi-C). The Laplacian matrix further takes a normalized form  ℓ = D - 1 / 2 L D - 1 / 2  ( Chung, 1997 ). It can be verified that 0 is an eigenvalue of  ℓ , and the set of eigenvalues of  ℓ  ( 0 ≤ λ 0 ≤ λ 1 ≤ … ≤ λ n - 1 )  is referred to as the spectrum of  ℓ . Given two contact maps  W A  and  W B , we propose to quantify their similarity by decomposing their corresponding Laplacian matrices  ℓ A  and  ℓ B  respectively and then comparing their eigenvectors. Let  { λ 0 A ,   λ 1 A , … ,   λ n - 1 A }  and  { λ 0 B ,   λ 1 B , … , λ n - 1 B }  be the spectra of  ℓ A  and  ℓ B , and  { v 0 A ,   v 1 A , … , v n - 1 A }  and  { v 0 B ,   v 1 B , … , v n - 1 B }  be their sets of normalized eigenvectors. A distance metric  S d  is defined as
 (1) S d ( A , B ) = ∑ i = 0 r − 1 ∥ v i A − v i B ∥ . 
Here  ∥         ∥  represents the Euclidean norm. The parameter  r  is the number of leading eigenvectors picked from  ℓ A  and  ℓ B . In general,  S d   provides a metric to gauge the similarity between two contact maps.  v i A  and  v i B  are more correlated if  A  and  B  are two biological replicates as compared with the case when they are two different cell lines (see  Supplementary Fig. S1 ). For the choice of  r , like any principal component analysis, the leading eigenvectors are more important than the lower ranked eigenvectors. In fact, we observe that the Euclidean distance between a pair of high-order eigenvectors is the same as the distance between a pair of unit vectors whose components are randomly sampled from a standard normal distribution (see  Supplementary Fig. S2 ). In other words, the high-order eigenvectors are essentially noise terms, whereas the signal is stored in the leading vectors. As a rule of thumb, we found the choice  r = 20  is good enough for practical purposes. Furthermore, as the distance between a pair of randomly sampled unit vectors presents a reference, we linearly rescale the distance metric into a reproducibility score  Q  ranges from 0 to 1 (see the  Supplementary Material ). We used HiC-spector to calculate the reproducibility scores for more than a hundred pairs of Hi-C contact maps. As shown in  Figure 1 , the reproducibility scores between pseudo-replicates are greater than the scores for real biological replicates, which are greater than the scores between maps from different cell lines (see the  Supplement ). It is worthwhile to point out that two contact maps can be compared in terms of features like topologically associating domains (TADs) and loops. It depends strongly on the choices of methods and parameters. Nevertheless, what we refer to, as ‘reproducibility’ is a direct comparison of the contact maps. Fig. 1 Reproducibility scores for three sets of Hi-C contact maps pairs. Contact maps came from Hi-C experiments performed in 11 cell lines. Biological replicates refer to a pair of replicates of the same experiment. Pseudo replicates are obtained by pooling the reads from two replicates together performing down sampling. There are 11 biological replicates, 33 pairs of pseudo replicates, and 110 pairs of maps between different cell types. Each box shows for a pair the distribution of Q in 23 chromosomes, with crosses as the outliers Mathematically there are different ways to compare two matrices. For instance, one could assume all matrix elements are independent and define a distance metric using Spearman correlation. The intuition behind  S d  is essentially a better way to decompose a contact map. The normalized Laplacian matrix is closely related to a random-walk-process taking place in the underlying graph of  W .  The leading eigenvector refers to the steady state distribution; the next few eigenvectors correspond to the slower decay modes of the random walk process and capture the densely interacting domains that are highly significant in contact maps. In fact, HiC-spector can better separate biological replicates and non-replicates compared with the correlation coefficient (see  Supplementary Fig. S3 ). Apart from the reproducibility score, HiC-spector provides a number of matrix algorithms useful for analyzing contact maps. For instance, to perform a widely used normalization procedure for contact maps ( Imakaev  et al. , 2012 ), we include the Knight-Ruiz algorithm ( Knight and Ruiz, 2012 ), which is a newer and faster algorithm for matrix balancing. Also, we have included the functions for estimating the average contact frequency with respect to the genomic distance, as well as identifying the so-called A/B compartments ( Lieberman-Aiden  et al. , 2009 ) using the corresponding correlation matrix. 3 Implementation and benchmark HiC-spector is a library written in Julia, a high-performance language for technical computing. A Python script for the reproducibility score is also provided. The bottleneck for evaluating  Q  is matrix diagonalization. The runtime is very efficient but depends on the size of contact maps (see  Supplementary Fig. S5  for details). 4 Materials and methods Hi-C data are generated by the ENCODE consortium (see the  Supplementary Material ). Contact maps were generated using the tool cworld ( https://github.com/dekkerlab/cworld-dekker ). Supplementary Material Supplementary Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>LCR-eXXXplorer: a web platform to search, visualize and share data for low complexity regions in protein sequences</Title>
    <Doi>10.1093/bioinformatics/btv115</Doi>
    <Authors>Kirmitzoglou Ioannis, Promponas Vasilis J.</Authors>
    <Abstract>Motivation: Local compositionally biased and low complexity regions (LCRs) in amino acid sequences have initially attracted the interest of researchers due to their implication in generating artifacts in sequence database searches. There is accumulating evidence of the biological significance of LCRs both in physiological and in pathological situations. Nonetheless, LCR-related algorithms and tools have not gained wide appreciation across the research community, partly due to the fact that only a handful of user-friendly software is currently freely available.</Abstract>
    <Body>1 Introduction During the past 30 years, the main focus of research related to regions of local compositional extremes (low complexity regions; LCRs) was their identification for the purpose of sequence masking ( Altschul  et al. , 1994 ;  Wootton and Federhen, 1993 ;  Ye  et al. , 2006 ) for eliminating spurious hits in database searches ( Promponas  et al. , 2000 ;  Tsoka  et al. , 1999 ). Several studies have been published showcasing the abundance and importance of such regions at the molecular/structural (e.g.  Radivojac  et al. , 2006 ;  Tamana  et al. , 2012 ), functional (e.g.  Andrade  et al. , 2001 ;  Haerty and Golding, 2010 ), organismic (e.g.  Miskinyte  et al. , 2013 ;  Pizzi and Frontali, 2001 ) and habitat level (e.g.  Nandi  et al. , 2003 ). Despite the apparent biological importance of LCRs there’s a distinct lack of tools or services capable of helping biologists to study them in depth. Most of the methods capable of detecting LCRs were developed for the sole purpose of masking them and are meant to be used from the command line as part of a sequence analysis or search pipeline. While some tools, such as SEG ( Wootton and Federhen, 1993 ), CAST ( Promponas  et al. , 2000 ) or BIAS ( Kuznetsov and Hwang, 2006 ) do offer more advanced reports as an option, their results are mostly meant to be parsed by a computer software and not a biologist. In this work, we present LCR-eXXXplorer, an online service to search, visualize and share LCRs in protein sequences. We highlight its unique features that may facilitate research efforts towards understanding the biological roles of proteins with LCRs. 2 Functionality 2.1 General description LCR-eXXXplorer is built upon a customized instance of GBrowse ( Stein  et al. , 2002 ) modified to properly work with protein sequences. It currently contains 545 000 sequences (retrieved from UniProt/SwissProt) annotated with over 16 million LCR-related annotations. Along with information about sequence complexity, LCR-eXXXplorer displays external annotations from UniProt, as well as predicted disordered and binding regions by utilizing IUPRED ( Dosztányi  et al. , 2005 ) and ANCHOR ( Dosztányi  et al. , 2009 ;  Mészáros  et al. , 2009 ) respectively. Data are stored in a MySQL database, using a database schema based on the SeqFeature schema internally used by GBrowse (see  Supplementary Methods  and  Supplementary Fig. S1 ). 2.2 Key functionality A basic keyword-based search functionality (allowing wildcards) is available for retrieving protein sequences with matching UniProtKB Accession(s)/Entry Name(s) or gene name(s). Moreover, the ‘Advanced Search’ option (specifically implemented for this process as a custom-made GBrowse plug-in) facilitates more fine-tuned queries. Using the basic search mode, users are able to retrieve up to 500 entries using simple keyword search (e.g. with a single UniProt identifier or accession number). An ‘Advanced Search’ may be initiated by querying a suitable combination of UniProt fields (e.g. gene or protein name, source organism) or LCR properties (e.g. type of LCR, percent of masked residues)—yet, only the AND Boolean operator is currently supported for combining search criteria. Under this mode, batch search functionality is also available using a list of UniProt accession numbers: this feature enables users to take advantage of the powerful UniProt search engine and come up with a list of entries specifying complex search criteria. Results can be displayed in the browser (with a limit of 15 000 entries) or downloaded in a plain text tab-delimited formatted file providing statistics on the LCR content for further processing (with a limit of 50 000 entries). Different options of masking protein sequences are provided for each individual sequence from the graphical GBrowse ‘protein details’ view and sequences are available in FASTA format. The Downloads section offers LCR-eXXXplorer the option of downloading the complete set of sequences in FASTA formatted files masked for LCRs, the complete set of annotations in GFF3 format or a CSV formatted table with LCR statistics for each sequence in the database. Users may also search for data in LCR-eXXXplorer using BLASTP ( Ye  et al. , 2006 ) powered by the user-friendly SequenceServer (Priyam  et al. , manuscript in preparation). Three underlying databases (unmasked, SEG or CAST masking with default parameters) are provided, with the masked databases being a unique feature of this service; this configuration is shown to improve database search results ( Kirmitzoglou, 2014 ; Kirmitzoglou  et al. , in preparation). Furthermore, users may initiate BLASTP searches against the sequence databases hosted at the NCBI web servers ( http://www.ncbi.nlm.nih.gov/ ) using as input query the currently displayed sequence; several options of applying masking using any combination of amino acid residue types and detection algorithm are available. The main strength of LCR-eXXXplorer—setting it apart from similar services—is its visualization capabilities. Displaying LCRs in a protein sequence is more informative when information regarding other functional or structural features is also shown ( Supplementary Fig. S2 ). By taking advantage of the underlying GBrowse capability to display features stored on a remote web accessible server, LCR-eXXXplorer incorporates selected annotations from UniProt into the main browser interface. UniProt annotations displayed in LCR-eXXXplorer are of two major types: (i) general annotations associated with the protein sequence (e.g. protein name, gene ontology terms, PDB accession IDs) and (ii) position-specific annotations, which may include domains, sites, secondary structure etc. These annotations are fetched from UniProt/SwissProt on-the-fly for the protein sequence of interest. This is facilitated by a custom-designed cgi-bin script and the retrieved features are further post-processed to a format suitable for the LCR-eXXXplorer. Using the same underlying mechanism, LCR-eXXXplorer can display tracks generated by another instance of GBrowse, a Distributed Annotation System (DAS) server or valid GFF3 files generated by the user. The only requirement is that the remote tracks must use the same coordinates system, which in the case of LCR-eXXXplorer is the protein sequence itself. Thus, users may practically display results from any LCR-detection tool (or any other protein sequence analysis tool) alongside the data provided by LCR-eXXXplorer. 2.3 Comparison to similar services Two services for providing access to protein sequence LCR-related data are currently available online. The one most closely related to LCR-eXXXplorer is LPS-annotate ( Harbi  et al. , 2011 ), which identifies LCRs based on the LPS algorithm ( Harrison and Gerstein, 2003 ), compared to SEG. These LCR annotations are accompanied with disordered region predictions by DISOPRED ( Buchan  et al. , 2010 ). Even though LPS-annotate is an invaluable resource for researchers interested in compositionally biased proteins, its main drawback is the lack of any effective visualization options. Moreover, the underlying database (according to data available at the LPS-annotate website) has not been updated since 2009. Recently, the HRaP server ( Lobanov  et al. , 2014 ) was developed, specializing in the study of homopolymeric repeats, which comprise a highly specialized case of LCRs, thus it is not further discussed herein. A detailed presentation of web-based services providing information related to LCRs is presented in  Kirmitzoglou (2014) . 3 Future Developments The current version of the LCR-eXXXplorer web server offers several tools for facilitating research on proteins with LCRs, including BLAST search and interactive visualization by exploiting inherent GBrowse features. Given the genuine interest of our research group in LCR-containing proteins, we plan to expand this service in the near future. More specifically, we are in the process of automating the LCR-eXXXplorer update procedure to regularly synchronize with UniProt updates. Moreover, the customizations performed on different GBrowse modules require some additional work (and appropriate documentation) for enabling full programmatic access to our service through the REST interface already available for GBrowse. An important improvement destined for the next version of LCR-eXXXplorer is enabling full support of Boolean queries against fields in the underlying database. The modular (both in terms of data and software) architecture of LCR-eXXXplorer enables easy incorporation of novel datasets (e.g. complete genome sequences) and LCR detection tools in future versions. 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Flower: extracting information from pyrosequencing data</Title>
    <Doi>10.1093/bioinformatics/btr063</Doi>
    <Authors>Malde Ketil</Authors>
    <Abstract>Summary: The SFF file format produced by Roche's 454 sequencing technology is a compact, binary format that contains the flow values that are used for base and quality calling of the reads. Applications, e.g. in metagenomics, often depend on accurate sequence information, and access to flow values is important to estimate the probability of errors. Unfortunately, the programs supplied by Roche for accessing this information are not publicly available. Flower is a program that can extract the information contained in SFF files, and convert it to various textual output formats.</Abstract>
    <Body>1 INTRODUCTION After pyrosequencing was first introduced ( Margulies  et al. , 2005 ), the use of this technology has grown rapidly. Data from pyrosequencing can be analyzed with traditional tools for sequence analysis (typically by first converting data into the Fasta format), but native formats usually contain more information and for many analyses this information can be used to produce more accurate results. Pyrosequencing data from the Roche 454 sequencing technology comes in the form of SFF files, a relatively compact, binary format. This format contains the sequence of  flow values  for each read, corresponding to the light intensities, resulting from the ligation process during sequencing. In addition, the SFF file contains the base-called sequence with associated quality information, and an index linking each called base to the corresponding flow value. The SFF file also contains clipping information for each read, and various metadata. Roche provides some utilities for working with SFF files, and these have so far been the most convenient option for pipelines like Pyronoise ( Quince  et al. , 2009 ) and QIIME ( Caporaso  et al. , 2010 ) that need to access the raw flow values and other information from SFF files. But as Roche's tools are not publicly available and may not be modified or redistributed, this complicates distribution of integrated pipelines. As a consequence, usage and development of more effective analysis tools is hampered. Other tools like  sff_extract  (Jose Blanca, unpublished), a utility supplied with the Mira assembler ( Chevreux  et al. , 1999 ), is limited to extracting sequence information. Flower is an efficient, portable and freely distributable software utility that reads SFF files, and outputs information in textual form. It uses an independent reimplementation of the SFF format based only on published documentation ( Roche, 2009 ), and is freely redistributable and modifiable under the General Public License. 2 FEATURES Flower is implemented as a command line tool that reads one or more SFF files, and produces output in one or more formats, either to files or to standard output. The default output format is a textual representation of each read in the SFF file. The information is presented as one field per line, a tab character separates the label from the contents. Some fields are optional, so if for instance the read name does not follow 454 encoding conventions, no  Info  field will be present. An example of this output (truncated in length) is shown in  Figure 1 .
 Fig. 1. Example textual output for one read. The  Info  field displays the information embedded in the read name, and includes date, time and picotiter plate region and coordinates. An alternative output is the tabular flowgram output (‘-F’ option), shown in  Figure 2 , that generates one line per flow value, with read name, flowgram position, nucleotide and quality calls. Although verbose, this makes it easy to link quality calling with the corresponding flow value.
 Fig. 2. Example tabular output. The columns are read name, flow number, nucleotide (lower case if masked), flow value and list of associated quality values. Flower can also extract the sequence data to the Fasta format (using the  -f  option), associated qualities ( -q ) and the Fastq ( Cock  et al. , 2010 ) format using either Phred ( -Q ) or Illumina ( -I ) type qualities. Summary information about the contents of SFF files is available as either the header information (with the  -i  option) or one line per read (with  -s ). One common graphical representation of a sequencing run is a histogram of flow values in the data (an example is shown in  Fig. 3 ). Although this is straightforward to calculate from the textual output, flower has specific functionality for accumulating these. Since the output size is greatly reduced, this is faster.
 Fig. 3. Plot showing the flow values before and after trimming, using the  -h  option. SFF files contain  trimming  information, indicating parts of each read that are either low quality or part of synthetic sequences like adapter sequences or the four-letter key that prefixes each read. Flower will by default output the trimmed sequence parts as lower case letters and the untrimmed as upper case, but there are also options for removing the unwanted sequence parts, with the corresponding quality and flowgram information. The  –trim  option removes all trimmed sequence, and the  –trimkey  option removes only the key. This affects all output options. 3 DISCUSSION Running time depends to a large degree on the output formats chosen, as some formats like textual or tabular generate a large amount of output, and this dominates running time. For example, converting an 2.1 GB SFF file (containing ∼630 000 reads) to Fasta or FastQ format takes about 20 s on a 3.4 GHz Xeon, processing the SFF file at about 100 MB/s, which is faster than a typical hard disk can deliver it. Generating text output for the same file is substantially slower at 476 s. 1 Although it is likely that these numbers can be improved somewhat, in most cases any subsequent analysis will be much more computationally expensive. The memory footprint is generally low (less than 5 MB resident size as measured by  top  in the experiments described above) and, as flower works incrementally (streaming), memory consumption is independent on total input or output size. 4 AVAILABILITY Flower is available under the General Public License, and is freely available for any purpose. It may be modified, and the modified version is redistributed according to the conditions in the license. Flower is implemented in Haskell, and the source code is available from the Hackage repository at  http://hackage.haskell.org/package/flower . Further documentation can be found at  http://biohaskell.org/Applications/flower </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics (Oxford, England)</Journal>
    <Title>Optimized design and assessment of whole genome tiling
arrays</Title>
    <Doi>10.1093/bioinformatics/btm200</Doi>
    <Authors>Gräf Stefan, Nielsen Fiona G. G., Kurtz Stefan, Huynen Martijn A., Birney Ewan, Stunnenberg Henk, Flicek Paul</Authors>
    <Abstract/>
    <Body>1 Introduction 1.1 Background Genome sequences are providing an important template on which we seek to understand biological function. This template has been exploited in a number of ways to guide biological investigation at unprecedented resolution. A number of recent results have demonstrated the utility of tiled microarrays for unbiased biological discovery. In contrast to gene expression microarrays, which seek to measure the relative abundance of a specifically targeted set of sequences (e.g. expressed mRNAs in a specific cell type or experimental condition), tiling microarrays are composed of a large number of probes from a contiguous region of the genome selected so that they are immediately adjacent to (or overlap) one another. In this way, analysis with a tiling microarray can, for example discover transcribed regions of the genome outside of any known annotation ( Bertone  et al ., 2004 ;  Kapranov  et al ., 2002 ). Tiling arrays have also been extensively used to localize DNA–protein interactions identified with chromatin immunoprecipitation (ChIP). In this technique (known as ChIP-chip), DNA fragments isolated in the ChIP step are labeled and hybridized to tiling microarrays ( Buck and Lieb, 2004 ). ChIP-chip has been used to create a genome-wide map of active human promoters ( Kim  et al ., 2005b ), and is one of the major experimental techniques adopted by the ENCODE project in its attempt to determine all of the functional elements in the genome ( ENCODE Project Consortium, 2004 ). Moreover, combining tiling array data with other genome-wide data sources has the potential to dramatically increase our understanding of genome function ( Guezennec  et al ., 2005 ). 1.2 Tiling array platforms A number of unique tiling array platforms using both PCR products and short oligonucleotide probes have been created for a variety of applications in mammalian genomes, including unbiased regional or whole-genome arrays and specifically targeted arrays encompassing certain classes of genomic regions such as known promoters or other genomic features. Tiling arrays based on PCR fragments have successfully mapped DNA–protein interactions for selected regions of the human genome ( Kim  et al ., 2005a ;  Rada-Iglesias  et al ., 2005 ). PCR arrays have also been successful at mapping interactions that are more widespread in their genomic extent, such as histone modifications ( Koch  et al ., 2007 ). However, whole genome oligonucleotide tiling arrays, which are commercially available from a number of companies, outperform PCR fragment based tiling arrays, at least for transcriptional mapping ( Emanuelsson  et al ., 2006 ). This result may apply to transcription factor binding experiments as well since transcription factor binding sites are generally short sequence motifs. Ultimately, the effort and expense required to create PCR product tiling arrays will likely limit the extent of their coverage to relatively small regions of the genome. Previous whole genome tiling array designs have generally excluded or made little use of the repetitive portions of large vertebrate genomes ( Bertone  et al ., 2006 ). However, with this strategy DNA–protein interactions will be invisible if they occur in regions identified by programs such as RepeatMasker ( Smit  et al ., 2004 ). Indeed, DNA–protein interactions within repeats have been especially important in epigenetic studies ( Martens  et al ., 2005 ) and direct measurement and localization of these interactions by ChIP-chip analysis could be important to understanding the function of genomic repeats. Some tiling array design methods allow for tiling through repeat features for exactly this purpose ( Ryder  et al ., 2006 ). 1.3 Problem definition Whole genome tiling array designs must balance competing interests. The ability to discover new biology in an unbiased fashion with the highest possible resolution requires maximal coverage of the genome at the highest possible density. In large mammalian genomes, data quality is significantly impacted by the presence of repetitive sequence because of the potential for cross-hybridization. Thus, most previous whole genome tiling array designs have concentrated only on the non-repetitive portion of the genome to ensure uniqueness. Another significant consideration is the cost of the experiment including both the array manufacturing cost and the expense of reagents for multiple slide hybridization experiments. Moreover, current array manufacturing techniques prevent some specific sequences from being synthesized. Finally, any array design should aim at facilitating optimal analysis of the resulting data. 1.3.1 Probe uniqueness Methods for determining unique oligonucleotides are critical for accurate results from microarray experiments since non-unique sequences are likely to cross-hybridize. Enrichment from unexpected portions of the genome, especially when this enrichment is distributed across probes in an unknown way, will give rise to significant experimental noise, which may compromise analysis and limit conclusions that can be drawn from the experiment. As noted above, the simplest possible approach to the problem of repetitive sequences in large genomes is to ignore those sequences that are annotated as repetitive. This naive approach ignores both those ancient repeats that have significantly diverged over evolutionary time and those ‘non-repetitive’ sequences that, in fact, occur many times in the genome due to their presence in gene families or other multicopy, but not repetitive sequence as traditionally defined. A more direct approach used by previous oligonucleotide design methods addresses the problem of probe uniqueness using an alignment-based approach in which prospective probes are tested for uniqueness using a procedure based on elongation of exact matches (with BLAST or a similar algorithm) against a sequence database representing the possible sequences that a probe on the array will encounter ( Bertone  et al ., 2006 ;  Wang and Seed, 2003 ). 1.3.2 Biochemical properties Tiling array design must take into account a number of biochemical and biophysical properties of oligonucleotide sequences that have the potential to interfere with hybridization. These properties fall into two distinct groups: first, several properties affect the performance of the probes during the hybridization experiment including probe melting temperature and probe self-hybridization potential ( Bloomfield  et al ., 2000 ). Second, the manufacturing of the arrays requires specific chemistry which may damage longer probes and renders certain nucleotide sequences difficult to accurately manufacture ( Lockhart  et al ., 1996 ). 1.3.3 Analysis considerations The analysis to be performed is largely determined by the experimental use of the tiling array. Transcriptional mapping is most successful with overlapping probes or tiling arrays with high probe density ( Emanuelsson  et al ., 2006 ;  Huber  et al ., 2006 ), while array-based approaches for copy number variation commonly use comparably sparse tiling arrays ( Graubert  et al ., 2007 ). ChIP-chip applications, in which we are most interested, require relatively dense arrays. Additionally, for ChIP-chip approximately uniform spacing of probe enrichment data has been shown to be effective in localizing DNA–protein interactions ( Qi  et al ., 2006 ). 2 Models and Algorithms 2.1 Defining the uniqueness score We are interested in unique substrings for a hypothetical genome sequence  G  of ~3 · 10 9  nucleotide which contains significant repetitive sequences (e.g. a mammalian genome). Uniqueness always refers to the complete genome sequence including both the forward and reverse strand of the assembled chromosomes and additional, but not yet assembled, sequence. Thus, we assume that this complete genome sequence set is on the order of 6·10 9  characters and is referred to as  GS  in the following. Of course, if we make a substring of  G  only large enough, it will become unique. So as a representation of unique substrings, we are interested only in the minimum unique substrings. A substring  x  of  G  is a minimum unique substring if  x  occurs exactly once in  GS  and each proper substring of  x  occurs more than once in  GS . As shown in  Figure 1A , we divide  G  into non-overlapping substrings  s  of length  ℓ  (which we refer to as a  unit ). For a given probe length  h , we shift a window of size  h  over  s  and determine the uniqueness score  U ( s ,  r ) at all possible offsets  r , 1 ≤  r  ≤  ℓ  −  h  + 1.  U ( s ,  r ) is defined as the number of minimum unique substrings of length ≤ K  ending in some position  j ,  r  ≤  j  ≤  r  +  h  − 1. To efficiently determine  U ( s ,  r ), we compute minimum unique prefixes at all possible positions in  s : For each  i  ∈ [1,  ℓ ],  mup ( s ,  i ) is defined by the following two statements:  If  s [ i .. ℓ ] occurs more than once as a substring in  GS , then  mup ( s ,  i ) is undefined, denoted by  mup ( s ,  i ) = ⊥. If  s [ i .. ℓ ] does not occur more than once in  GS , then  mup ( s ,  i ) =  m , where  m  is the smallest positive integer such that  i  +  m  − 1 ≤  ℓ  and  s [ i .. i  +  m  − 1] occurs exactly once as a substring in  GS . Here,  s [ i .. i  +  m  − 1] denotes the substring of  s  from position  i  to position  i  +  m  − 1. 
 The substring  s [ i .. i  +  mup ( s ,  i ) − 1] is denoted minimum unique prefix at position  i . To explain the relationship between minimum unique prefixes and minimum unique substrings, we consider the set  φ ( j ) of start positions of minimum unique prefixes ending at  j , i.e.  φ ( j ) = { i  |  i  ≥ 1,  i  +  mup ( s ,  i ) − 1 =  j }. The following lemmata show the relationships: Lemma  Let  φ ( j ) ≠ ∅ and  i  = max  φ ( j ). Then  s [ i .. j ] is a minimum unique substring. Proof:  By definition  j  =  i  +  mup ( s ,  i ) − 1, i.e.  s [ i .. j ] is the minimum unique prefix at position  i . It occurs once in  GS . By definition,  s [ i .. j  − 1] occurs more than once in  GS . Since  i  = max  φ ( j ), we conclude  i  + ∉  φ ( j ). Hence,  i  + 1+  mup ( s ,  i  + 1) − 1 ≠  j . Now suppose  i  + 1 +  mup ( s ,  i  + 1) − &lt; j . This implies that  s [ i  + 1.. i  + 1 +  mup ( s ,  i  + 1) − 1] is a proper prefix of  s [ i  + 1.. j ]. Hence,  s [ i .. i  + 1 +  mup ( s ,  i  + 1) − 1] is a proper prefix of  s [ i .. j ] and so  s [ i .. i  + 1 +  mup ( s ,  i  + 1) − 1] occurs more than once in  GS . Hence,  s [ i  + 1.. i  + 1 +  mup ( s ,  i  + 1) − 1] occurs more than once in  GS . This is a contradiction. Thus the assumption  i  + 1 +  mup ( s ,  i  + 1) − 1&lt; j  was wrong, and we conclude  i  + 1 +  mup ( s ,  i  + 1) − 1&gt; j . Hence,  s [ i  + 1.. j ] occurs more than once in  GS . Now consider a proper substring  p  of  s [ i .. j ].  p  must be a substring of  s [ i  + 1.. j ] or  s [ i .. j  − 1]. Since these occur more than once in  GS , so does  p . Hence, all proper substrings of  s [ i .. j ] occur more than once in  GS , which means that  s [ i .. j ] is a minimum unique substring. Lemma  Let  s [ i .. j ] be a minimum unique substring. Then  φ ( j ) ≠ ∅,  i  = max  φ ( j ) and  j  =  i  +  mup ( s ,  i ) − 1. Proof:  By definition,  s [ i .. j ] occurs once in  GS . Suppose that  mup ( s ,  i ) is undefined. Then  s [ i .. ℓ ] occurs more than once in  GS . Since  j  ≤  ℓ ,  s [ i .. j ] occurs more than once in  GS . This is a contradiction. Hence  mup ( s ,  i ) is defined. Suppose  j &lt; i  +  mup ( s ,  i ) − 1. Then  s [ i .. j ] occurs more than once in  GS , a contradiction. Suppose  j  +  mup ( s ,  i ) − 1. Then  s [ i .. i  +  mup ( s ,  i ) − 1] is a proper prefix of  s [ i .. j ] which occurs once in  GS . This contradicts the fact that any proper substring of  s [ i .. j ] occurs more than once in  GS . Thus we conclude  j  =  i  +  mup ( s ,  i ) −  i  which also implies  φ ( j ) ≠  ∅ . Obviously  i  ∈  φ ( j ). Suppose  i &lt; i ′ where  i ′ = max  φ ( j ). Then  j  =  i ′ +  mup ( s ,  i ′) − 1. By definition,  s [ i ′.. j ] only occurs once in  GS . But since  s [ i ′.. j ] is a proper suffix of  s [ i .. j ], it occurs more than once in  GS . This is a contradiction. Hence  i  = max  φ ( i ). As a consequence, there is a one-to-one correspondence between the minimum unique substrings and the distinct end positions of minimum unique prefixes ( Fig. 1B ). In other words, counting the distinct end positions of minimum unique prefixes is equivalent to counting the number of minimum unique substrings. This holds for each unit  s  as well as for the entire genome under consideration. We can thus compute  U ( s , r ) = | { i + m u p ( s , i ) − 1 | i ∈ [ 1 , ℓ ] , m u p ( s , i ) ≤ K } ∩ { r , … , r + h − 1 } |  Given the  mup ( s ,  i )-values for a unit  s , we can compute all values  U ( s ,  r ) in time proportional to  ℓ . 2.2 Computing minimum unique prefixes Our uniqueness problem involves the comparison of 3 · 10 9 / ℓ  units (each with  ℓ  positions) against a sequence of 2 × 3 · 10 9  characters. The huge number of uniqueness queries against the same data set requires us to preprocess  GS  into a string index. The most well known string index structures are suffix trees ( Gusfield, 1997 ;  Weiner, 1973 ) and suffix arrays ( Manber and Myers, 1993 ). The standard suffix-tree/suffix-array-based algorithms for string searching can easily be adopted to determine the sought minimum unique prefix lengths for a given unit. While suffix trees deliver the minimum unique prefixes in optimal time (i.e. time proportional to the length of a unit), a suffix array (in its simplest form) requires time proportional to  ( ∑ i = 1 ℓ | m u p ( s , i ) | ) .  log  n , where  s  is the given unit and  n  is the total length of all sequences in  GS . However, the reduced running time for a suffix-tree-based solution is at the cost of a larger space consumption. While the simplest form of suffix arrays for  GS  can be implemented in  1 8 n ⌈ log ⁡ 2 n ⌉  bytes, the most space efficient implementations for suffix trees ( Giegerich  et al ., 2003 ;  Kurtz, 1999 ) require about three times more space. This led us to the conclusion that we are not able to solve our uniqueness problem with suffix trees. A suffix array for  GS  requires  1 8 n ⌈log 2 
 n ⌉ = 4.125 n  = 24.75 · 10 9  bytes, which conveniently fits into the 48 GB RAM of the machine we had available for this task. However, our first program to solve the uniqueness problem was based on the suffix arrays implemented in the software package  Vmatch  ( http://www.vmatch.de ). This program uses 64-bit integers for representing numbers larger than 2 32  − 1, resulting in a space requirement of 8 n  = 48 · 10 9  GB. Given that we additionally have to represent the sequence set  GS , we are not able to store all required information in the given amount of memory. Note that it is also not obvious how to solve the uniqueness problem by a divide and conquer approach, i.e. solving it for disjoint subsets of  GS  and combining the results. For these reasons, we have developed a solution based on a compressed index structure, namely the FMindex, originally proposed by  Ferragina and Manzini (2000) . The FMindex is based on the Burrows–Wheeler transform ( Burrows and Wheeler, 1994 ), which is known from data compression. The simplest way of explaining the concept of the FMindex is via suffix arrays. So let us first define these. Suppose that we have reversed all sequences from  GS  and concatenated them into one very long string  S  with a unique separator symbol between adjacent sequences and a final unique sentinel symbol following the last sequence. Let  n  be the length of  S . By  S i  =  S [ i .. n ], we denote the suffix of  S  beginning at position  i . Now let  S i 1 ,  S i 2 , . . . ,  S i n −1 ,  S i n  be the sequence of all suffixes of  S  sorted in lexicographic order, i.e.  i j  ≠  i k  for  j  ≠  k  and  S i j  is lexicographically smaller than  S i j +1  for each  j , 1 ≤  j  ≤  n  − 1. Obviously, we can represent the sequence of ordered suffixes by the array [ i 1 ,  i 2 , . . . ,  i n −1 ,  i n ] of start positions. This array is termed suffix array. The Burrows–Wheeler transform  T  is a sequence of length  n  storing the character to the left of each suffix in the order, the suffixes are sorted. That is, for each  j , 1 ≤  j  ≤  n ,  T [ j ] =  S [ i j  − 1] if  i j &gt;1 and  T [ j ] is undefined if  i j  = 1.  T  is a permutation of  S  allowing us to search all substrings occurring in  S . The search requires to implement a table  C  and a function  Occ  defined as follows:  C  is an array of length 4 where  C [ a ] is the total number of occurrences of characters in  T  which are alphabetically smaller than  a . Occ ( a ,  q ) is a function delivering the number of occurrences of character  a  in the prefix  T [1.. q ]. 
 The substrings of  S  can be searched in reverse order, from right to left. Thus, the reversed strings in  S  (i.e. the original strings from  GS ) can be searched from left to right, which allows to conveniently compute the  mup -values. Here is a simple algorithm to compute  mup ( s ,  i ) for a given unit  s  of length  ℓ  and all  i  in the range [1,  ℓ ] is shown by  Algorithm 1 : Recent advances show that the FMindex can be implemented such that  Occ ( a ,  q ) can be computed in constant time ( Ferragina  et al ., 2006 ). As a consequence,  Algorithm 1  runs  ∑ i = 1 ℓ | m u p ( s , i ) |  where  ℓ  = 100. This is faster by a factor log  n  compared to the suffix-array-based solution. Our implementation is based on a simpler technique similar to  Navarro (2004) , but tailored for processing DNA sequences. It also has some features in common with the technique described in  Healy  et al . (2003) , but we use less space. Algorithm 1 Compute  mup(s, i)  for a given unit s of length  l 1:  for 
 i  = 1 to  ℓ 
 do 2:    first  = 1; 3:    last  =  n ; 4:    j  =  i 5:    while 
 j ≤ ℓ  &amp;&amp;  first  &lt;  last 
 do 6:        a  =  s [ j ]; 7:        first  =  C [ a ] +  Occ ( a ,  first  − 1) + 1; 8:        last  =  C [ a ] +  Occ ( a ,  last ); 9:        j  =  j  + 1; 10:    end while 11:    if 
 first  ==  last 
 then 12:       printf (“mup %d=%d\n”, i , j  −  i ); 13:    end if 14:  end for The Burrows–Wheeler transform  T  is stored uncompressed in (2 +  δ ) n  bits where  δ  ≤ 1 depends on the number of positions in  S  not containing a base  a ,  c ,  g  or  t . Besides  T  we need  n b + 8 n b 2  bytes to implement function  Occ , where  b  is some user defined constant smaller than 256.  Occ ( a ,  q ) is computed in  O ( b ) time. Since the Burrows–Wheeler transform is based on the suffix array for  S , we first construct this. As described above, we cannot construct the entire suffix array in memory. Therefore, we choose the following approach: we divide the original sequences used to create  GS  into a small number  A  of disjoint subsets (e.g. one for each chromosome), each only containing the original sequence (no reverse complemented sequences). The subsets are small enough such that we can compute the suffix arrays in main memory. Then, for each of the  A  subsets, we construct two suffix arrays using the program  mkvtree  from the  Vmatch  software package: one suffix array for the reverse of the input sequences, and one for the complement of the input sequences. Note that the complement of the input sequences is the same as the reverse of the reverse complement of the input sequences. In this way, we obtain 2 ×  A  suffix arrays, which are all stored in different files. In a second step, we use a multiway merging procedure which simultaneously reads all suffix arrays from left to right. With each merging step, we obtain the next suffix in the sorted order of all suffixes of  S  and obtain the corresponding character of the Burrows–Wheeler transform plus the remaining information comprising the FMindex. Note that the merging step does not require us to have the suffix arrays in main memory, because they are not randomly accessed. However, the sequences, for which the suffix arrays are constructed must be stored in the RAM. Once we have the FMindex stored on file, we can solve our uniqueness problem. 2.3 Validating the uniqueness score To validate the uniqueness score, we used a collection of nearly 670 000 50mer probes from the NimbleGen whole genome tiling array specifically designed for human chromosomes 22 and X. For each of these probes, we determined both  U  as defined above and the number of hybridization-quality alignments (see  Methods  Section) for each probe to the genome using BLAT ( Kent, 2002 ). More than 91% of the probes we tested aligned only once and 97.5% aligned no more than twice.  Figure 2  is a box and whiskers plot clearly showing that probes with a single BLAT alignment to the genome have significantly higher values for  U  than probes that align to the genome more than once. In fact, only one quartile of probes with exactly two BLAT alignments in the genome have  U &gt;15 and the median value of  U  for probes that align three or more times is zero. 2.4 Probe selection algorithm As shown in  Figure 1 , for a given probe length  h  we determine  U ( s ,  r ) for all  r , 1 ≤  r  ≤  ℓ  −  h  + 1, and sort the values by decreasing uniqueness scores. We say that probe  p  =  s [ r .. r  +  h  − 1] has uniqueness score  U ( s ,  r ). From the sorted uniqueness scores, we determine the optimal probe in the unit using a greedy selection strategy, which addresses the biochemical properties of oligonucleotide probe design by considering a number of additional constraints on our probes. These are described below and shown graphically in  Figure 3 . Starting with the probe  p  that has the highest uniqueness score  U , we ensure that the uniqueness score is higher than a given  U  threshold. If this is not the case, we will not place a probe in this unit-sized window. Otherwise, we will calculate the melting temperature using the following salt-adjusted approximation ( Sambrook  et al ., 1989 ):  T m = 81.5 + 16.6 × log ⁡ ( c ( Na + ) ) + 0.41 × f GC − 600 / N ,  where  c (Na + ) is the salt concentration,  f GC  the frequency of Gs and Cs and  N  the overall number of nucleotides in the sequence. If the melting temperature is within our defined range, we test the sequence for specific composition filters. In particular, we make sure that it does not contain any specific runs of oligonucleotides that are known to be difficult to manufacture (e.g. more than 6 consecutive Gs). Finally, we check that the sequence does not contain more than a given percentage of palindromic sequence to exclude probes with significant self-hybridization potential. In a final test, we may reject a probe that exceeds a given maskless array synthesis (MAS) manufacturing cycle limit. If one of these tests fails, we adjust the sequence either by growing or shrinking by a given step size within a given length range. This changes the sequence characteristics which are subsequently reassessed with the tests described above to find an alternative solution. If we are unable to find a probe with a uniqueness score greater than some threshold  U , and satisfying all of the additional requirements, we will not place a probe in that unit-sized window.  Figure 3  shows a flowchart of the probe selection algorithm. 3 Methods The algorithm was implemented in a combination of C (FMindex creation and  Algorithm 1 ) and Perl (probe selection algorithm). We have constructed our whole genome tiling array designs based on NCBI Build 36 of the mouse genome sequence downloaded from release 42 of Ensembl ( ftp://ftp.ensembl.org ). To test the relationship between  U  and the number of BLAT alignments, the default settings were used to match the probe sequences from a non-isothermal, repeatmasked, 50mer NimbleGen whole genome tiling array design against build NCBI35 of the human genome. Alignments that met the following criteria were deemed unlikely to hybridize well to the probes and were ignored: (a) matches with more than one gap; (b) matches of length &lt;30 (i.e. matching &lt;60% of the probe); (c) matches with gap length &gt;3 and (d) matches with more than three mismatched bases. The number of remaining matches was defined as the number of hybridization quality alignments for the probe. 4 Results and Assessment We have used our algorithm to create whole genome tiling array designs for the 2.6 gigabase-pair mouse genome as a typical example of a repeat-rich mammalian genome. The mouse was the second mammalian genome sequenced ( Mouse Genome Sequencing Consortium, 2002 ) and is a rich resource for biomedical research. Additionally, mouse whole genome tiling arrays are available from a number of commercial providers. 4.1 Computational requirements The time required for completion of indexing steps is dependent on the number of repetitive sequences in the input sequence length. For the case of the mouse genome, and considering both strands simultaneously, the input sequence length is ~5.2 · 10 9  characters. Approximately 1.7 h on a single 2.2 GHz AMD Opteron processor are required to create suffix arrays for all of the chromosome sequences (and the additional sequences that have not yet been confidently placed on the mouse genome assembly as of NCBI build 36). The multi-way merging procedure requires a further 6 h using the same configuration. This is a considerable improvement in construction time over  Healy  et al . (2003) . The entire FMindex is computed in 5 GB of memory. After building the index, creation of one complete tiling array design from a typical parameter set, which sets  ℓ  = 100 and  K  = 30, takes ~22 h on a single processor. 4.2 Tiling array designs To test the performance of the design algorithm, we have chosen a number of specific parameters for consideration. Specifically, we considered designs limited by two values of uniqueness score,  U &gt;0 and  U &gt;15; two values of initial seed length,  h  = 50 and  h  = 80; two ranges for probe hybridization temperature, 73 ≤  T m  ≤ 76 and 77 ≤  T m  ≤ 80; and two values for probe self-hybridization potential (i.e. palindromic content),  P sh  ≤ 30% and  P sh  ≤ 50%. For all designs, we limited the length range of the final probes to  h  ± 15. Two additional filters on the array design based on considerations of hybridization potential and manufacturing efficiency. These additional filters prohibit any probe with six or more consecutive guanine bases and limit the number of synthesis cycles that would be needed to manufacture the design using the maskless array synthesis. Cycle limits of 148 and 186 were tested. Figure 4A and B  shows the effect of the two uniqueness score parameter settings on the final design of the tiling array. 4.2.1 Effect of algorithm parameters on design characteristics As shown in  Figure 3 , each probe is chosen to maximize  U  while respecting the other parameters of the design. This leads to a direct trade-off between the coverage of the genome (the fraction of unit-sized windows in which probes are successfully placed) and minimum allowed uniqueness score. In  Figure 4B , we show the effect of removing probes from the design which have  U  ≤ 15. For probes with an initial seed length of 50 and regardless of the allowable  T m  range, setting  U &gt;15 results in preservation of ~80% of the probes with a uniqueness score U&gt; 0, while removing those most likely to participate in cross-hybridization reactions. Increasing the initial seed length to 80, while retaining all of the other parameters constant results in a larger fraction of windows retaining probes after the filter for uniqueness score. However, as expected, the number of synthesis cycles must be increased to achieve comparable genome coverage with the longer probes (data not shown). Because standard oligonucleotide tiling arrays do not include the repetitive regions of the genome, we were interested in the uniqueness score of probes that are placed in regions annotated as repeats. To address this, we divided the probe sets into those that were placed in repetitive and non-repetitive regions based on a standard set of comprehensive repeat identification procedures designed to facilitate genome annotation ( Curwen  et al ., 2004 ).  Figure 4A and B  shows that a much lower fraction of the probes with  U &gt;15 are placed in repetitive regions than probes limited only to  U &gt;0. In fact, the uniqueness score per base (described below) for probes in the non-repetitive regions with  U &gt;0 is the same as the mean uniqueness score per base for the probes in the full design with  U &gt;15. Compared to  U , filters for palindromic content have a relatively minor effect on the number of probes potentially removed from the design. Surprisingly, the palindromic filter removes a greater fraction of probes with  U &gt;15, than with  U  ≤ 15 showing that palindromic sequences are more unique in the genome. 4.2.2 Design coverage A primary goal of our work is to create a tiling array with the highest possible coverage of the genome while maintaining the maximum possible uniqueness. To adequately assess the coverage of our tiling array designs, we consider multiple measures of coverage. The first, and most simple, measure is the number of genomic base pairs actually present on the tiling arrays. This value can theoretically be larger than 100% for the case of overlapping probes. We have also measured the number of base pairs in unit-sized windows for which a probe was successfully placed. Finally, we measure the extent of regions in which probes are placed in continuous unit-sized windows (we require four or five consecutive windows to have probes for a region to be called continuous). This measure is based on the common analysis technique of using sliding window analysis to determine the positive regions on the tiling arrays (e.g.  Bertone  et al ., 2004 ;  Buck  et al ., 2005 ). Coverage values for our array designs are shown in  Table 1 With a goal of increasing coverage, we tested  ℓ  values of 75, 100, 150 and 200 bp. As expected the fraction of windows in which we are able to place a probe increases with window size, while the total number of probes placed falls. Base-pair coverage does increase marginally with smaller  ℓ  values. For example, using the  U  &gt; 15 design described in  Table 1 , base-pair coverage increased from 28.4% for  ℓ  = 100 to 30.9% for  ℓ  = 75. However, this increase does not effect array resolution which is based on probe density ( Emanuelsson  et al ., 2006 ). As described above, our interest in ChIP-chip applications demands relatively dense tiling arrays which explains our choice of  ℓ  = 100 to match the resolution of the NimbleGen designs that have been used successfully for this technique. 4.3 Array design comparison We sought to compare our tiling array design with standard catalog tiling array designs available from Affymetrix and NimbleGen. Each of these designs has characteristics that make exact comparison difficult. For example, both the Affymetrix and NimbleGen designs used fixed-length oligonucleotide probes (25 and 50 bases, respectively) and both designs seek to have a uniform distribution of the probes within tiled regions. Neither explicitly considers the hybridization temperature. Consideration of these differences allows us to assess the effect of  T m  optimization in our algorithm and led us to generalize the uniqueness score metric as described below. In general, uniqueness score is proportional to probe length. Thus to compare tilling array designs with differing probe lengths we compute the normalized uniqueness score for each probe, which is defined as the uniqueness score divided by the probe length. For these comparisons listed below, we only consider our designs with an MAS synthesis cycle limit of 148 to be comparable with the NimbleGen catalog tiling array design. Although we use the same cycle number limitation as the NimbleGen design, our probes are shorter, on average, due principally to the effect of the  T m  optimization. Probes in the  U  &gt; 15 high-uniqueness design have a mean length of 48.0 bp and SD of 5.85 bp. For the  U  &gt; 0 high-coverage design these values are fractionally smaller. Figure 4C and D  shows the uniqueness score per base, and the  T m  distribution for the NimbleGen and Affymetrix whole genome tiling array designs. When we compare these to the designs produced by our algorithm, we see that our high-uniqueness design contains nearly the same number of probes as the NimbleGen design, but with higher coverage and less  T m  variability. As expected, Affymetrix’s shorter probes are less unique and have a lower average  T m  that the other designs. 5 Discussion In this article, we have described an algorithm for the efficient design of whole genome tiling arrays created from large mammalian genomes. A key feature of this design is the definition of a new measure of probe quality that we have termed the  uniqueness score . We have demonstrated that  U  can be efficiently calculated using an FMindex data structure and that probes with high uniqueness scores can be found in regions of the genome annotated as repetitive by programs such as RepeatMasker and therefore missing from standard whole genome tiling array designs. The approach differs fundamentally from the  Bertone  et al . (2006)  approach which presents an optimal solution for placing tiles larger than 300 bp in a RepeatMasked genome.  Bertone  et al . (2006)  also discuss the effect of parameters such as probe uniqueness and  T m  for oligonucleotide tiling arrays similar to the ones considered in  Section 4.3 , but do not present an algorithm for this case. Our algorithm was designed to support and improve data analysis techniques. For example, by creating high-quality and consistent probes within unit-sized windows, we anticipate that the experimental results from each probe will serve as a proxy for the response of the entire window. By selecting only probes with maximal uniqueness scores and, perhaps more importantly, knowing the uniqueness of each probe on the array, we are better able to estimate the potential for experimental noise caused by cross-hybridization. Our approach to scoring uniqueness is considerably different from existing approaches which either are based on aligning the probes to the genome or counting short sequence frequencies, e.g. summing the genome-wide frequency of  k mers occurring in an probe sequence. One example of the latter technique used an algorithm for counting genome-wide occurrence of 15mers developed by  Healy  et al . (2003)  to find the best 70 bp probe in each of collection 200–1200 bp fragments created by restriction digest.  Lucito  et al . (2003)  used these probes on an array designed to find genome copy number variation. In this case, the variable window lengths are much larger than the windows that we use and, although appropriate in their CNV study, do not provide the needed resolution for ChIP-chip. Aligning probes and counting short sequence frequencies are actually estimates of similarity rather than of uniqueness. Put another way, both alignment and frequency counting approaches estimate the potential for a probe to cross-hybridize by determining the extent of the rest of the genome that is very similar to the probe. Our uniqueness score, on the other hand, is an estimation of how unlike the probe is from everything else in the genome. Our assumption is that the more unique the probe, the lower the probability that it will be able to hybridize to any other sequence in the genome. We believe there are several significant advantages to our approach over methods that use alignments to estimate the cross-hybridization potential: first, the alignment approach depends heavily on the alignment method used and the parameter settings for acceptable alignments. Additionally, the resulting alignments do not have a one-to-one correspondence with possible hybridizations. The uniqueness score, on the other hand, can be considered a well-defined property of any subsequence of the genome, independent of the choice of implementation. Second, our method is significantly faster than any similar alignment-based approach, since the score is pre-calculated over the entire genome sequence and defined for any subsequence of the genome. We can, therefore, optimize our choice of probes to the positions with maximum uniqueness score. A similar optimization with an alignment-based approach would require a new genome-wide alignment for each potential probe sequence. For our design with  h  = 50 and final probe lengths limited to  h  ± 15 this corresponds to calculating more than 8 · 10 10  genome-wide alignments in the worst case. Our designs, created with a sensible set of parameters, are more unique than the Affymetrix or NimbleGen designs (as measured by the uniqueness score), and have a more consistent  T m  distribution while notably increasing coverage. We have not compared our designs to iso-thermal tiling designs available from any manufacturer as these are currently limited to select regions of the genome, such as promoter arrays. Finally, we observe probes placed in windows for which no probe can be placed in the window to either side of the placed probe. These ‘singleton’ probes are more common in repeat regions. This is a potentially important feature for the analysis of DNA–protein interactions in repetitive regions with techniques such as ChIP-chip, although fairly sophisticated data analysis methods will be required for these cases. Properly constructed tiling arrays including singleton probes in repetitive regions are potentially useful for assessing copy number variation and certain repeat polymorphisms. 6 Conclusion We propose  uniqueness score  as a general measure of probe quality for tiling array designs. The uniqueness score is based on the content of shortest unique substrings in the probes and measures how unlike the probe is from anything else in the genome. We use  U  at the heart of an algorithm to efficiently design whole-genome tiling arrays with relatively modest computational requirements. Our code implementing this algorithm is provided under an open source license. In collaboration with NimbleGen, we are in the process of testing these designs with ChIP-chip experiments across a significant portion of mouse chromosome 17. We expect the results of these tests to guide further improvements to tiling array design. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>DASMI: exchanging, annotating and assessing molecular interaction data</Title>
    <Doi>10.1093/bioinformatics/btp142</Doi>
    <Authors>Blankenburg Hagen, Finn Robert D., Prlić Andreas, Jenkinson Andrew M., Ramírez Fidel, Emig Dorothea, Schelhorn Sven-Eric, Büch Joachim, Lengauer Thomas, Albrecht Mario</Authors>
    <Abstract>Motivation: Ever increasing amounts of biological interaction data are being accumulated worldwide, but they are currently not readily accessible to the biologist at a single site. New techniques are required for retrieving, sharing and presenting data spread over the Internet.</Abstract>
    <Body>1 INTRODUCTION Molecular interactions are of fundamental importance to many biological processes (BPs). In recent years, the amount of interaction data has increased substantially due to growing attention by the scientific community as well as the widespread use of high-throughput techniques that afford screening of vast numbers of molecules. Nowadays, large-scale protein interaction maps are available for model organisms like yeast, fly and worm (Goll and Uetz,  2007 ), and the current research focus is shifting towards interaction screens for human (Cusick  et al. ,  2005 ; Stelzl and Wanker,  2006 ). In addition, computational methods have been developed for predicting molecular interactions, some of which reach prediction quality comparable to that of experimental high-throughput data (Ramírez  et al. ,  2007 ; Shoemaker and Panchenko,  2007 ). However, this rapid accumulation of interaction data makes it difficult for scientists to keep track of all available information and data sources. The unification of heterogeneous and decentralized interaction data is thus a prerequisite for an effective study of interactomes (Brazma  et al. ,  2006 ). Molecular interactions can be studied at different levels of detail ( Fig. 1 ). In general, physical and non-physical interaction types can be distinguished. While a physical interaction implies a real contact between the interacting molecules (interactors), the other interaction type denotes a purely functional association between them. For instance, such associations can be based on similar genomic contexts, coexpression analyses or literature relationships (Jensen  et al. ,  2006 ). Physical interactions between proteins may involve two and more proteins, forming binary interactions and protein complexes (Frishman  et al. ,  2009 ). In particular, protein–protein interactions are formed by the physical contact of binding sites, which are frequently evolutionarily conserved in domains of protein families (Finn  et al. ,  2008 ). Further protein interactions exist with other ligands, for instance, nucleic acids, lipids and certain small molecules in signaling or metabolic pathways. Techniques like X-ray crystallography, NMR spectroscopy or 3D structure modeling can provide even more molecular details by identifying the interacting atoms or residues in the protein binding sites (Aloy and Russell,  2006 ; Finn  et al. ,  2005 ).
 Fig. 1. Levels of molecular interactions. Physical and non-physical interaction types can be distinguished: ( a ) Non-physical interactions are based on functional associations. ( b ) Physical interactions imply a direct physical contact between the interacting molecules like proteins, domains or small ligands. ( c ) Raising the level of detail to 3D structural data, the interacting atoms or residues of binding sites can be identified. A number of databases keep track of experimentally determined protein interactions (Bader  et al. ,  2003 ; Breitkreutz  et al. ,  2008 ; Chatr-Aryamontri  et al. ,  2007 ; Güldener  et al. ,  2006 ; Kerrien  et al. ,  2007a ; Keshava Prasad  et al. ,  2009 ; Salwinski  et al. ,  2004 ). Such databases are essential components of interactomics, however, each of them contains information not found in other databases (Mathivanan  et al. ,  2006 ). The IMEx consortium formed by eight major interaction data providers aims at overcoming the fragmentation by sharing the curation effort and exchanging curated protein interaction records among its members (Orchard  et al. ,  2007 ). However, IMEx and its member databases are restricted to experimentally determined protein interactions, which cover only a fraction of the estimated interactomes (Stumpf  et al. ,  2008 ). Voluminous data on predicted protein and domain interactions that have been made publicly available by different research groups (Schelhorn  et al. ,  2008 ; Schlicker  et al. ,  2007 ; Shoemaker and Panchenko,  2007 ) are not included. To provide broader data access to currently available interactomes, several integration frameworks have appeared recently. We will refer to the methodology underlying these frameworks as static data integration because either the user is assisted in building a local data warehouse (Aragues  et al. ,  2006 ; Lee  et al. ,  2006b ; Shah  et al. ,  2005 ; Shannon  et al. ,  2006 ) or the software facilitates access to central data repositories via web interfaces (Birkland and Yona,  2006 ; Breitkreutz  et al. ,  2008 ; Cerami  et al. ,  2006 ; Chaurasia  et al. ,  2009 ; Goll  et al. ,  2008 ; Hoffmann and Valencia,  2004 ; Jensen  et al. ,  2009 ; Pagel  et al. ,  2008 ; Prieto and Rivas,  2006 ; Raghavachari  et al. ,  2008 ; Tarcea  et al. ,  2009 ) or by software plugins, for instance, as available for Cytoscape (Avila-Campillo  et al. ,  2007 ; Cerami  et al. ,  2006 ; Hernandez-Toro  et al. ,  2007 ; Shannon  et al. ,  2003 ; Tarcea  et al. ,  2009 ). However, static integration has the drawback of providing only a snapshot of a fixed number of data sources at a certain point of time. Once the data have been included into the central repository, curation efforts are required to keep them up to date and in sync with the original data source. This permanent update problem can be aggravated by possible format changes of the source, which hampers further data processing. Apart from that, these integration frameworks are rather rigid because the inclusion of additional datasets like new experimental data or the results of a novel prediction method can normally be accomplished solely by the central authority. A data fragmentation situation similar to the current diversity of interaction data arose with genomic data several years ago. One possible solution to the integration of genomic data and their annotations was the Distributed Annotation System (DAS) (Dowell  et al. ,  2001 ). In general, it is anticipated that decentralization will become an important data-sharing concept in the future (Murray-Rust,  2008 ; Stein,  2008 ; Thorisson  et al. ,  2009 ). DAS is based on a client–server architecture in which numerous decentralized servers offer annotations of a reference entity provided by another server. The combination and visualization of a reference entity and its annotations is performed in a DAS client. We aim to overcome the shortcomings of the aforementioned static data integration frameworks by adopting and extending a DAS-based approach for the exchange of molecular interaction data and their annotations. Instead of unifying all available interaction data into a central database, the interaction data remain with their original providers and are retrieved and integrated online on request. This eliminates the issue of centralized data maintenance and ensures that the interaction data are always kept up to date. Our system, named DAS for Molecular Interactions (DASMI), is sufficiently generic to support all types of interaction data described above. It is not restricted to protein interactions and considers both experimentally determined and predicted interactions. This is the main distinguishing feature from interaction repositories like HPRD (Keshava Prasad  et al. ,  2009 ) or IntAct (Kerrien  et al. ,  2007a ). Instead of competing with them, we want to supplement their data with additional sources and help the user to assess the available information. In the following section, we will introduce the distributed architecture of DASMI and describe its components: the specification of a DAS extension for the exchange of interaction data and their annotations as well as the software libraries that implement the new specification in servers and clients. Finally, we will demonstrate the exemplary use of DASMI for protein and domain interactions. 2 METHODS 2.1 Distributed architecture DAS (Dowell  et al. ,  2001 ) is a data integration approach with the main goal of replacing central data repositories with distributed storage systems. DAS is built on a client–server architecture, consisting of two types of servers, namely, reference and annotation servers, and a client for visualization purposes. Reference servers provide the biological reference entity, for example, a nucleotide or peptide sequence. Annotation servers make additional information available that is related to the reference sequence, for instance, information on exons or protein domains. Coordinate systems are used to define the entities that a DAS server provides or annotates, for example, chromosomes, genes, protein sequences or protein structures (Prlić  et al. ,  2007 ). A data exchange specification handles the communication between DAS clients and DAS servers by prevalent techniques, namely HTTP URL requests and XML responses. Originally, DAS was designed for the exchange of annotations of DNA sequences. In recent years, several extensions to the protocol have widened its use to other areas (Jenkinson  et al. ,  2008 ): Protein DAS affords the exchange of protein sequence annotations and alignments (Jones  et al. ,  2005 ), 3D-DAS utilizes DAS for the annotation of protein structure alignments (Prlić  et al. ,  2005 ) and 3D-EM DAS for electron microscopy (Macías  et al. ,  2007 ). In addition to these DAS extensions, a registry has been developed that maintains a list of available DAS servers and thus allows DAS clients finding suitable servers ( http://www.dasregistry.org/ ). DASMI aims at resolving the problems of current integration frameworks for interaction data by transferring the idea of DAS into the field of molecular interactions. This includes the specification of a DAS extension defining the data exchange between servers and clients as well as reference implementations of servers and clients ( Fig. 2 ).
 Fig. 2. Schematic DASMI system architecture. The DASMI architecture is similar to the original DAS architecture (Dowell  et al. ,  2001 ). Interaction servers provide interactions (Interaction Server 1) and, optionally, additional information like experimental conditions (Interaction Server 2). Confidence servers provide confidence scores for interactions (Confidence Server 1). DASMI clients query interaction servers and combine their results. The DAS registry maintains a list of available DAS servers. As in the original DAS architecture, there are different server types in the DASMI framework. The majority of servers provide interaction data and optionally additional information; these are the equivalents of DAS reference servers. Examples of additional information, which can be associated specifically with an interaction, include the known or predicted interaction regions, the strength and type of the interaction, or the conditions in which the interaction occurs. Confidence servers are comparable with DAS annotation servers and provide reliability scores for potential interactions. Notably, the HUPO-PSI community wants to utilize our distributed scoring architecture for a common confidence scoring system for protein–protein interactions (Orchard  et al. ,  2008 ). Each interaction or confidence server belongs to a certain coordinate or identifier system, which specifies how interactions can be requested and how they are returned. For instance, a data source with the Entrez Gene identifier system may be queried for interactions by using gene identifiers, another data source with the UniProtKB identifier system by using protein identifiers. DASMI clients thus need to transform the results of servers from different identifier systems in order to unify them. 2.2 DASMI data exchange specification Data exchange between interaction servers and clients requires a DAS URL and XML specification. An advantage of a well-defined data exchange specification is the resulting modularity and extensibility of the system. New servers and clients can be readily incorporated if they follow the specification and thus communicate with the existing parts in a well-defined manner. We extend the DAS specification by the new  interaction  command and the associated DASINT XML response format. This extension is part of the DAS 1.53E specification (Jenkinson  et al. ,  2008 ).  Figure 3  shows an  interaction  request and the associated DASINT response for an exemplary protein–protein interaction.
 Fig. 3. Exemplary interaction request and DASINT response. The communication between DASMI server and client is performed using formatted URL requests and XML responses. ( a ) Interactions are requested using the  interaction  command. In the example shown here, all protein interactions involving the proteins P09497 and O60828 and annotated with a BPscore would be retrieved. ( b ) The server response is a DASINT XML format. ( c ) Overview of the DASINT XML Schema Definition. Mandatory elements are marked using solid frames, optional elements have dashed frames. Requests to a DASMI server are issued in the same form of a formatted URL request as those to a standard DAS server ( Fig. 3 a). The new command for requesting interactions is  interaction  and offers additional query parameters of three types:  interactor ,  operation  and  detail . Please refer to  Supplementary Data File 1  for the full data exchange specification. The response of a DASMI server to an  interaction  request is a DASINT XML document ( Fig. 3 b). In contrast to the widely adopted PSI-MI XML2.5 format (Kerrien  et al. ,  2007b ), which provides an extensive specification with numerous elements and a deeply branched hierarchy, DASINT uses a concise and flexible document format. PSI-MI XML2.5 and DASINT can thus be regarded as complementary approaches: whilst PSI-MI XML2.5 has the goal of describing experimentally determined interactions in detail, naturally resulting in very complex documents, DASINT provides a lightweight intermediate exchange format, which facilitates fast communication between clients and servers. In this regard, DASINT is comparable with MITAB2.5 (Kerrien  et al. ,  2007b ), the simplified tabular version of PSI-MI XML2.5. However, DASINT is more versatile because it supports, for example, the representation of protein complexes without the need of transforming the data into a spoke or matrix model. A more detailed differentiation of DASINT from alternative data exchange formats can be found in  Supplementary Data File 1 .  Figure 3 c shows an illustration of the DASINT XML Schema Definition. The complete definition of the proposed DASINT XML format can be found in  Supplementary Data File 2 . 2.3 DASMI server A DASMI server responds to an interaction request by providing interaction data in the DASINT XML format defined above. One of the objectives of DAS and thus of DASMI is to make the setup of DAS servers easy. To achieve this aim, three versatile open-source DAS server libraries are available, Dazzle ( http://www.biojava.org/wiki/Dazzle ) and MyDas ( http://code.google.com/p/mydas/ ) for Java, and ProServer (Finn  et al. ,  2007 ) for Perl ( http://www.sanger.ac.uk/Software/analysis/proserver/ ). We provide DASMI reference server implementations by extending Dazzle and ProServer, while MyDAS is being extended by the DAS community. The stand-alone servers Dazzle and ProServer both work in a modular fashion. They consist of a server core, which provides basic functionalities like handling requests and responses, and components, which manage specific DAS commands and data storage formats. Existing DataSource classes (Dazzle) and Transport modules (ProServer) act as brokers between the underlying interaction data and the modules that build the DASINT XML response. This simplifies access to a range of data storage formats, for instance, PSI-MI XML2.5 documents (Kerrien  et al. ,  2007a ) or flat files in the Simple Interaction Format (SIF) defined for Cytoscape (Cline  et al. ,  2007 ; Shannon  et al. ,  2003 ). In order to set up a new server, data providers need only to use an existing or implement a new module that is tailored to the specifics of their molecular interaction data. 2.4 Interaction datasets DASMI has been developed to support molecular interactions at different levels ( Fig. 1 ). To demonstrate its use, we set up DASMI servers for a collection of protein–protein interaction datasets: two large-scale experimental datasets [CCSB-HI1 (Rual  et al. ,  2005 ) and MDC (Stelzl  et al. ,  2005 )], four curated experimental datasets [DIP (Salwinski  et al. ,  2004 ), HPRD (Keshava Prasad  et al. ,  2009 ), IntAct (Kerrien  et al. ,  2007a ) and MINT (Chatr-Aryamontri  et al. ,  2007 )] and six predicted datasets [Bioverse (McDermott  et al. ,  2005 ), HiMAP (Rhodes  et al. ,  2005 ), HomoMINT (Persico  et al. ,  2005 ) OPHID (Brown and Jurisica,  2005 ), POINT (Huang  et al. ,  2004 ) and Sanger (Lehner and Fraser,  2004 )]. More information on these datasets is found in Ramírez  et al.  ( 2007 ). In addition, several domain–domain interaction datasets are offered by DASMI servers: three experimental datasets derived from 3D structures obtained by X-ray crystallography or NMR spectroscopy [3did (Stein  et al. ,  2009 ), iPfam (Finn  et al. ,  2005 ) and PiNS (Bordner and Gorin,  2008 )] and 11 predicted datasets (Chen and Liu,  2005 ; Guimarães  et al. ,  2006 ; Jothi  et al. ,  2006 ; Lee  et al. ,  2006a ; Liu  et al. ,  2005 ; Ng  et al. ,  2003 ; Pagel  et al. ,  2008 ; Riley  et al. ,  2005 ; Schelhorn  et al. ,  2008 ; Wang  et al. ,  2007 ; Wuchty,  2006 ), see  Supplementary Data File 3 . Moreover, we set up two confidence servers, FunSimMat (Schlicker and Albrecht,  2008 ; Schlicker  et al. ,  2006 ) and Domain support (Finn  et al. ,  2005 ; Ramírez  et al. ,  2007 ), which can be used to assess the reliability of protein interactions. Of course, our current selection of data sources, with the majority of them temporarily maintained at our institute, serves only as a prototype for the capabilities of our system because it necessitates the replication of some interaction datasets, resulting in the same update problem the aforementioned central repositories are facing. However, for the near future, we already know from other scientists that new sources for interactions and confidence measures will be made available at other institutions. 2.5 DASMI client A DASMI client offers the user an easy way of communicating with various DASMI servers without having to know any data exchange specification details. Subsequent to a user request, a DASMI client will contact all DASMI servers, retrieve and unify the interaction data and present the results to the user. A list of all publicly available DASMI servers is provided by the DAS registry (Prlić  et al. ,  2007 ). To facilitate the development of new DASMI clients, the two existing open-source DAS client libraries, Dasobert ( http://www.spice-3d.org/dasobert/ ) in Java and Bio-Das-Lite ( http://search.cpan.org/dist/Bio-Das-Lite/ ) in Perl, have been upgraded to support our interaction extension. 2.5.1 Identifier mapping Proteomics affords a substantial diversity of object identifiers, ranging from RefSeq and Entrez Gene identifiers to UniProtKB accession numbers. Accordingly, protein interaction datasets use different identifier systems to describe their interactions. In order to unify them, a DASMI client has to convert between various systems to incorporate servers that have different identifier systems. This mapping procedure can produce considerable computational overhead. For instance, while there is usually a one-to-one mapping from UniProtKB to Entrez Gene identifiers, mapping in the opposite direction may produce multiple results as one gene can be responsible for several protein variants or fragments. Therefore, a DASMI client might need to issue multiple queries to retrieve all protein–protein interactions for one gene. Furthermore, the mapping procedure implemented by a DASMI client determines if it is able to distinguish splice variants. In contrast, the identifier diversity for domain interaction datasets is less problematic as stable Pfam identifiers (Finn  et al. ,  2008 ) are predominantly used. 3 RESULTS On the basis of the client libraries Dasobert and Bio-Das-Lite, two DASMI clients have been developed to illustrate the potential of our new system: the DASMIweb client as an entry gate to various protein–protein and domain–domain interaction datasets and the iPfam graphical domain interaction browser that uses DASMI to incorporate predicted domain–domain interactions into its results. 3.1 DASMIweb for protein and domain interactions The DASMI client DASMIweb is publicly accessible at  http://www.dasmi.de/web/ . The aim of DASMIweb is to establish a starting point for interactome studies by consolidating protein and domain interaction data from various sources. 3.1.1 User interface The DASMIweb user interface is designed to be clear and intuitive ( Fig. 4 a). The screen window is divided into several panels; permanent panels are the Query Panel in the top left corner of the window and the Information Panel in the top right corner. Interactions are presented within the Interaction Panel, located in the central part of the window. The configuration of DASMIweb can be managed in the optional Source Configuration Panel. The DASMIweb user interface relies heavily on the use of Asynchronous JavaScript and XML (AJAX) (Jimenez  et al. ,  2008 ; Sagotsky  et al. ,  2008 ). This technique is required to present interactions to the user as soon as they are received from a DASMI server. The asynchronous communication is provided by the Java framework Direct Web Remoting (DWR,  http://getahead.org/dwr/ ).
 Fig. 4. DASMI clients DASMIweb and iPfam. ( a ) DASMIweb is an online tool for dynamically unifying protein and domain interaction data and additional annotations. The results are presented in tabular form: each column represents a DASMI server, each row contains an interaction partner, and squares at the intersections of rows and columns indicate interactions. In this figure, the interaction squares are colored according to the functional similarity between the interacting proteins, from dark blue for high to white for low similarity. ( b ) The iPfam client tool combines domain–domain interactions from several sources with interactions reported in the iPfam database. The results are presented in graphical form; protein domains are depicted as ovals and interactions as edges that connect ovals. Different edge colors distinguish individual data sources. 3.1.2 Querying To make querying DASMIweb as intuitive as possible, the Query Panel contains only a single search field. There is no need for the user to specify the type of the query. The system will use internal identifier mapping tables derived from iProClass (Huang  et al. ,  2003 ) and Pfam (Finn  et al. ,  2008 ) to automatically determine whether the input is a gene, protein or domain identifier. If the identifier cannot be mapped unambiguously, the user is asked to refine the query. Furthermore, DASMIweb will map the query to all compatible identifier systems in order to maximize the number of data sources that can be used to answer the user query. For instance, if the user searches by an Entrez Gene identifier, DASMIweb will not only query all data sources in the Entrez Gene identifier system, but will also try to convert the identifier to UniProtKB, GeneInfo, RefSeq and Ensembl identifiers to cover all data sources in the respective identifier systems. If a mapping results in multiple identifiers, for instance, when mapping from gene to protein identifiers, all combinations of identifiers are used. A more detailed description of the mapping procedure and exemplary mappings can be found in the online documentation at  http://www.dasmi.de/ . 3.1.3 Presentation of results Interactions are presented to the user in tabular form within the Interaction Panel. In the central table, columns represent data sources that have been contacted for the user query, rows correspond to different interactions, and squares in the intersections of rows and columns indicate particular interactions reported by a specific source ( Fig. 4 a). If an interaction is binary, the row contains the interaction partner of the query interactor, but if the interaction is complex, all interaction partners are presented in a single row that is highlighted. This tabular representation affords an intuitive, visual judgment of the results since an interaction reported by multiple sources, as shown by several squares in the same row, may be more likely to be accurate. A more detailed assessment of the interactions can be performed by applying confidence measures as described below. The user can also request further information about an interaction, such as experimental details and confidence scores, by clicking on the associated ‘interaction square’. These annotation details are an optional feature provided by the individual data sources. Therefore, they might not be available for all interactions. 3.1.4 Source configuration and data export In its initial configuration, DASMIweb incorporates all available data sources that are compatible with a user query. The user can change this selection in the Source Configuration Panel. This panel lists all known data sources that are registered in the DAS registry with their identifier systems. The user can integrate additional data sources that are not contained in the DAS registry by using the ‘Add new source’ tab of the Source Configuration. After providing all required information like the name, URL and identifier system of the new source, it will be included in all future queries. Another way of adding new interaction data is by creating a DASMI server from an existing PSI-MI XML2.5 file. After uploading the file into DASMIweb, the interactions will be made temporarily available as a new DASMI server. This procedure enables users to compare their own interactions with existing datasets or to assess them by confidence servers. In order to analyze the results with external applications such as the network visualization software Cytoscape (Shannon  et al. ,  2003 ), the user can export the results from the web client into file formats like the PSI-MI tabular format MITAB2.5 (Kerrien  et al. ,  2007b ) or the SIF (Cline  et al. ,  2007 ). 3.1.5 Quality assessment Current protein interaction networks are still incomplete to a large extent and are prone to bias and errors (Ramírez  et al. ,  2007 ). To address this problem, DASMIweb offers useful options to assess the reliability of individual interactions. The following datasets of confidence scores can be requested and selected in the header of the Interaction Panel and are applied to color the interaction squares:
 FunSimMat: Interaction partners frequently share similar functions. FunSimMat provides scores that measure the functional similarity of both partners (Schlicker and Albrecht,  2008 ; Schlicker  et al. ,  2006 ). The BPscore is based on the BP annotation in the Gene Ontology, the CCscore on the cellular component (CC) annotation and the MFscore on the molecular function (MF) annotation. Domain support: Some protein–protein interactions may be traced to the underlying domain–domain interactions. Domain support offers two subsets: domain interactions that have been derived from crystal structure analyses and domain interactions that have been computationally predicted by different methods (see  Supplementary Data File 3 ). 
In addition, the user can display the original confidence scores that are contained in the source datasets. 3.2 iPfam graphical domain interaction browser iPfam (Finn  et al. ,  2005 ) is a database of Pfam domain interactions derived from proteins with an experimentally determined 3D structure. Integrating information about domain interactions from various sources enables one to address several questions. For instance, datasets generated using different methods can be compared and structurally known domain interactions provided by iPfam or 3did (Stein  et al. ,  2009 ) can be used to verify predicted domain interactions. To this end, the iPfam database has developed a client tool that graphically integrates domain interaction information from one or more DASMI servers ( http://ipfam.sanger.ac.uk/graph ) including an own server for structural interactions. For a user-selected domain, the iPfam tool retrieves data about interacting domains and represents them as a graph. Each domain is depicted as an oval node within the graph, and interactions are represented by graph edges. Different colors are used to distinguish interactions from individual data sources. Clicking on a domain will center the graph on the interactions for that domain, which supports the visual exploration of the domain interaction network. 3.3 Comparison with existing interaction repositories DASMI clients may be compared with interaction databases like HPRD (Keshava Prasad  et al. ,  2009 ) or IntAct (Kerrien  et al. ,  2007a ). However, DASMI does not want to compete with such databases, but intends to complement their results with interaction data from other datasets. For example, the results of computational predictions as available in Bioverse (McDermott  et al. ,  2005 ) are not included into databases of experimental protein interactions, though they might give scientists new insights into the function of proteins (Sharan  et al. ,  2007 ). Providing more interaction datasets is not only a goal of DASMI, but also the motivation for composite databases like MiMI (Tarcea  et al. ,  2009 ) for protein–protein interaction or DOMINE for domain–domain interactions (Raghavachari  et al. ,  2008 ). In contrast to DASMI, these composite databases combine several datasets into a central repository, which renders it difficult to ensure that the interaction data they provide is kept in sync with the original sources. IntAct, for instance, has a daily release cycle, implying daily update processes of the composite databases. DASMI avoids this problem by leaving the interaction data with its original providers. In addition, DASMI fosters the inclusion of novel interaction data and interaction confidence scoring methods. There is no central authority that decides which data resources are included and which are not. By setting up a new DASMI server and registering it at the DAS registry, the interaction data or confidence scoring routine will automatically be available to all users ( Fig. 2 ). Moreover, the setup of an own DASMI server without publishing the server address allows for integrating confidential data into other DASMI clients. 4 DISCUSSION AND CONCLUSION We have introduced DASMI, a new framework for the dynamic exchange and integration of different types of interaction data. The DAS protocol extension DASMI is based on a client–server architecture and comprises three main components: data exchange specification, interaction servers and integration clients. Open source server and client libraries are available for the programming languages Java and Perl. Due to its distributed architecture, DASMI is easily extensible, for instance, by including new servers or developing additional clients. By avoiding a central interaction data repository, DASMI bypasses the problem of update cycles that static integration frameworks face. As a prototypic application, we set up several DASMI servers and developed web clients for the exchange of protein and domain interactions. The client DASMIweb dynamically gathers interactions from various servers and integrates their results into a unified view. In addition, the reliability of interactions can be assessed by confidence measures. Furthermore, DASMI is used by an iPfam client to integrate predicted domain–domain interactions into iPfam results. The development of DASMI will be continued and further extensions will be included. Future plans include more DASMI sources for interaction datasets and confidence measures by external providers. Additional proxies will allow incorporating servers into the DASMI system that do not use DASINT but PSI-MI XML2.5 or other XML formats. The DASMI clients DASMIweb and iPfam will be equipped with new features like a graphical network representation for DASMIweb. Additionally, new DASMI clients like a Cytoscape plugin are under development. 
          Supplementary Material 
          
             
                [Supplementary Data] 
             
             
             
             
             
             
          
       </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Author Index</Title>
    <Doi>10.1093/bioinformatics/btu543</Doi>
    <Authors/>
    <Abstract/>
    <Body>Ahn,T. i422 Alexandrov,L.B. i617 Ali,W. i430 Alkhnbashi,O.S. i489 Allison,L. i512 Atias,N. i445 Bačák,M. i534 Backofen,R. i489 Basu,M.K. i572 Baumbach,J. i631 Bayani,N. i468 Bayzid,Md. S. i541 Bellos,E. i639 Benner,P. i534 Biasini,M. i505 Block,I. i631 Bourguignon,P.-Y. i534 Brass,A. i601 Brayet,J. i364 Campillos,M. i579 Capra,J.A. i408 Capriotti,E. i572 Cavalcante,R.G. i393 Chauve,C. i519 Choi,S. i453 Christiansen,H. i631 Clark,W.T. i609 Coin,L.J.M. i639 Collier,J.H. i512 Cortini,R. i587 Costa,F. i489 Dawson,K.J. i617 Deane,C.M. i430 Delattre,M. i386 Devignes,M.-D. i345 Dimitrova,V. i356 Dittrich,P. i475 Dondelinger,F. i468 Dondi,R. i519 Du,L. i564 Duck,G. i601 Eichmüller,S.B. i401 Eils,R. i401 El-Mabrouk,N. i519 Elofsson,A. i482 Frasconi,P. i587 Friedberg,I. i609 Gönen,M. i556 Garcia de la Banda,M. i512 Garnett,M.J. i617 Garrett,R.A. i489 Gershenzon,M. i445 Gligorijević,V. i594 Gobbi,A. i617 Gonzalez-Perez,A. i549 Gray,J.W. i468 Hashemifar,S. i438 Hauswedell,H. i349 Hayat,S. i482 Helms,V. i415 Holtgrewe,M. i356 Huang,H. i564 Huh,N. i422 Hwang,D. i453 Iannello,G. i587 Ibrahim,B. i475 Iorio,F. i617 Israeli,D. i364 Jang,J.-H. i453 Janjić,V. i594 Jeanson-Leh,L. i364 Jiang,Y. i609 Jurman,G. i617 König,R. i401 Kallioniemi,O.P. i497 Kaski,S. i461 Kaski,S. i497 Khan,S.A. i497 Kim,S. i564 Kim,Y. i453 Konagurthu,A.S. i512 Korkola,J. i468 Kostka,D. i408 Kratsch,C. i527 Kreyssig,P. i475 Kuleshov,V. i379 Lévy-Leduc,C. i386 Labazin,K. i445 Lafond,M. i519 Lee,C. i393 Lee,E. i422 Lesk,A.M. i512 Linial,M. i624 Linial,N. i624 List,M. i631 Liu,X. i579 Lopez-Bigas,N. i549 Lopez-Bigas,N. i617 Margolin,A.A. i556 Marks,D.S. i482 Mary-Huard,T. i386 Mazrouee,S. i371 McHardy,A.C. i527 Michel,M. i482 Mirarab,S. i541 Mollenhauer,J. i631 Moore,J.H. i564 Moreau,Y. i345 Mukherjee,S. i468 Nenadic,G. i601 Niu,S. i356 Oates,C.J. i468 Oswald,M. i401 Park,T. i422 Patil,S. i393 Pavone,F.S. i587 Pedersen,M.L. i631 Peter,S. i475 Poso,A. i497 Pržulj,N. i594 Radivojac,P. i609 Rappoport,N. i624 Reaz,R. i541 Reinert,G. i430 Reinert,K. i349 Reinert,K. i356 Richard,H. i356 Risacher,S.L. i564 Rito,T. i430 Robertson,D.L. i601 Robin,S. i386 Rogers,S. i461 Rubio-Perez,C. i549 Saez-Rodriguez,J. i617 Sander,C. i482 Sartor,M.A. i393 Saunders,S.J. i489 Saykin,A.J. i564 Schacht,T. i401 Schmidt,S. i631 Schroeder,M.P. i549 Schulz,M.H. i356 Schwede,T. i505 Scott,L.J. i393 Shah,S.A. i489 Sharan,R. i445 Shen,L. i564 Silvestri,L. i587 Singer,J. i349 Skwark,M.J. i482 Soda,P. i587 Stern,A. i624 Stevens,R. i601 Studer,G. i505 Sun,F. i430 Suvitaival,T. i461 Swenson,M.S. i541 Tahi,F. i364 Tamborero,D. i549 Tamborero,D. i617 Tan,Q. i631 Thomassen,M. i631 Tian,R. i572 Veloz,T. i475 Virtanen,S. i497 Wang,W. i371 Warnow,T. i541 Wedge,D.C. i617 Weese,D. i356 Welch,R.P. i393 Wennerberg,K. i497 Weymouth,T. i393 Will,T. i415 Wozar,C. i475 Xu,J. i438 Yan,J. i564 Zehraoui,F. i364 Zimmermann,T. i541 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Revisiting amino acid substitution matrices for identifying distantly related proteins</Title>
    <Doi>10.1093/bioinformatics/btt694</Doi>
    <Authors>Yamada Kazunori, Tomii Kentaro</Authors>
    <Abstract>Motivation: Although many amino acid substitution matrices have been developed, it has not been well understood which is the best for similarity searches, especially for remote homology detection. Therefore, we collected information related to existing matrices, condensed it and derived a novel matrix that can detect more remote homology than ever.</Abstract>
    <Body>1 INTRODUCTION Protein sequence comparison methods are fundamental tools in contemporary biology. Currently they are used widely in various fields of bioinformatics such as computational genomics and proteomics, and computational evolutionary biology. Pairwise alignment is the basis of protein sequence comparison methods. Consequently, improving pairwise amino acid sequence comparison methods can engender improved quality of studies in the field of computational biology. A few factors are necessary to improve pairwise amino acid sequence comparison methods. Minimum units that govern the methods consist of comparison algorithms and amino acid substitution matrices, also called similarity/mutation/scoring matrices. Development and improvement of a matrix are crucial for improvement of the methods. Many studies have been undertaken since the first compilation of such matrices ( Tomii and Kanehisa, 1996 ) and have been developed and improved along the following three lines. (i) Specific matrices are proteins with distinctive amino acid compositions. Consequently, it is reasonable to construct matrices for distinctive protein classes or proteins encoded in the genome under directional mutation pressures. For instance, matrices specialized for transmembrane regions ( Muller  et al. , 2001 ;  Ng  et al. , 2000 ) and for β-barrel membrane proteins ( Jimenez-Morales and Liang, 2011 ;  Jimenez-Morales  et al. , 2008 ) have been developed. Matrices for particular proteins/organisms have also been constructed ( Ali  et al. , 2012 ;  Brick and Pizzi, 2008 ;  Dimmic  et al. , 2002 ;  Kuznetsov, 2011 ;  Lemaitre  et al. , 2011 ). Aside from those matrices, a general scheme for the compositional adjustment of matrices has been proposed ( Yu  et al. , 2003 ). (ii) Optimized matrices: starting from the existing matrices, some superior matrices have been derived by maximizing the ability to discriminate between homologs and non-homologs ( Hourai  et al. , 2004 ;  Kann  et al. , 2000 ;  Saigo  et al. , 2006 ), and to obtain accurate alignments ( Qian and Goldstein, 2002 ) with optimization methods. (iii) Context-dependent matrices: from the pioneering work of constructing 400 × 400 doublet-type (= dipeptide) substitution matrices ( Gonnet,  et al. , 1994 ), several approaches along this line have been proposed ( Crooks  et al. , 2005 ;  Gambin  et al. , 2002 ;  Huang and Bystroff, 2006 ;  Jung and Lee, 2000 ;  Liu and Zhao, 2010 ). More recently, a novel similarity search method called CS-BLAST, which uses information of neighboring residues extensively to improve similarity search, has been developed ( Biegert and Soding, 2009 ). In this method, the similarity search is performed with no amino acid substitution matrix. CS-BLAST, which is reportedly of high detection performance, presents the possibility of displacement of a traditional amino acid substitution matrix ( Angermuller  et al. , 2012 ;  Biegert and Soding, 2009 ). Although many matrices have been proposed, we found previously that widely used matrices, so-called general purpose matrices such as PAM ( Dayhoff  et al. , 1978 ) and BLOSUM ( Henikoff and Henikoff, 1992 ), have common characteristics in terms of the results of both hierarchical cluster analysis and reproduction from amino acid indices, despite the difference in datasets, methods, and models used for obtaining them ( Tomii and Kanehisa, 1996 ). This fact might imply the existence of common ground among general purpose matrices. It is worth investigating the potential for development. To this end, we explore effective matrices for identifying distantly related proteins based on the prevailing matrices. We explored and identified effective matrice(s) in the PCA subspace through the intensive benchmark analyses, and inferred the most sensitive matrix. That matrix, designated as MIQS, with SSEARCH outperforms the existing matrices and CS-BLAST on an independent dataset, in terms of sensitivity. We argue that substitution matrices are useful for amino acid similarity search. 2 METHODS 2.1 Datasets We used a non-redundant subset of SCOP (1.75 release) ( Andreeva  et al. , 2008 ) domain sequences from ASTRAL ( Chandonia  et al. , 2004 ) for both training and validation of our method. The subset, SCOP20, consists of all SCOP domains with 20% maximum pairwise sequence identity and includes 7074 sequences in total. We divided the sequences randomly into two sets for training and validation. The resulting sets respectively contain 3537 sequences. To investigate the effects of dataset for training of our method, we prepared the other dataset with a higher threshold of sequence identity. The subset, which we call SCOP40-v, consists of SCOP domain sequences, except those included in the validation set above, with 40% maximum pairwise sequence identity. The resulting set contains 8598 sequences. Furthermore, we prepared a test set that does not share homologous sequences with either the training or validation set to ensure the independence of the sets. We created the test set based on ever-growing CATH domain sequences ( Sillitoe  et al. , 2013 ). We first built a subset, the CATH20 dataset, which consisted of CATH domains (ver. 3.5.0) with 20% maximum pairwise sequence identity and which included 8203 sequences, using the PSI-CD-HIT program of CD-HIT (ver. 4.6.1) package ( Fu  et al. , 2012 ). Then, we obtained the test set, called CATH20-SCOP (1754 sequences), by excluding sequences related to entries in SCOP, according to SCOP/CATH mapping ( Lewis  et al. , 2013 ). Coordinate files of CATH domains for alignment quality evaluation were generated from corresponding PDB files using makedomains.perl (ver. 1.1) developed by Dr Martin ( http://www.bioinf.org.uk/faqs/cath/ ). To perform principal component analysis (PCA; see below), we used nine existing substitution matrices, in 1/3-bit units, of the three series, i.e. original BLOSUM, VTML ( Muller  et al. , 2002 ) and matrices developed by  Benner  et al.  (1994) . For brevity, we designate the last ones as BCG below. BCG1, BCG2 and BCG3 correspond to a matrix collected in 6.4–8.7, 22–29 and 74–100 PAM, respectively. The BLOSUM and BCG series are major members of the cluster containing matrices that are widely used in sequence alignments ( Tomii and Kanehisa, 1996 ). To ascertain whether a BLOSUM-type matrix performs well, we derived the BLOSUM20 matrix, which we called BLOP20, using the training set from SCOP20, and tested it. To construct BLOP20, we used the scripts provided by ( Lemaitre  et al. , 2011 ). 2.2 Derivation of matrices from the PCA subspace The substitution matrix processed in this study is symmetric, and it consists of 210 elements. Therefore, the PCA was performed with the variance–covariance matrix of the nine 210-dimensional vectors as an input. Our aim is to explore and to identify sensitive region(s) to identify distantly related proteins in the PCA subspace, and to deduce and obtain the most sensitive matrix. To this end, in the PCA subspace, matrices derived from points around the existing matrices, which perform better among the nine matrices with the training set, were sampled and examined. We can produce systematically arbitrary matrices based on principal component scores (= coordinate as (s 1 , s 2 , s 3 ) in the PCA subspace) as follows:
 (1) 
Therein,  U i T  represents the transpose of eigenvector PC i ;  s i  represents the coordinate on the PC i  axis ( i  = 1, 2, 3). Furthermore,  M  represents a novel matrix;  μ  represents the mean of nine matrices used for PCA. Then, elements in  M  are rounded off to the nearest integer values. We used Kernel Density Estimation (KDE) to infer and confine the most sensitive region in the 3D PCA subspace based on the results of benchmarks for sampled matrices. At the KDE execution, we treated a value of ROC 50  (see below) as a density at each grid point sampled in the PCA subspace. Both PCA and KDE were conducted using R ver. 2.15.0 ( R Development Core Team, 2012 ). 2.3 Gap penalty optimization and sensitivity benchmark of existing and derived matrices To assess the sensitivity of both existing and derived matrices, we used SSEARCH (ver. 36.3.5) ( Pearson, 1991 ) to conduct all-against-all sequence comparison of datasets. The SSEARCH results were sorted according to their statistical significance (E-value), with the most significant hits on top. Each hit is labeled as true or false positive, otherwise unknown. Above the threshold(s), we defined hits from the same  superfamily  with a query in SCOP (or the same  homologous superfamily  in CATH) as true positives, and defined hits from the different  fold  with a query in SCOP (or different  topology  in CATH) as false positives. Hits from the different  superfamily  ( homologous superfamily ), but from the same  fold  ( topology ), were classified as neither a true nor a false positive, but were classified as unknown because it is difficult to determine whether such hits are homologous or not. With the training set from SCOP20, we tested all possible combinations of open gap penalty from −13 to −9 at 1 interval, and −2 and −1 as an extension gap penalty for each matrix. For each matrix and each combination of open and extension gap penalties, ROC 50  (see below) was calculated. Then the best (optimized) combination of open and extension gap penalties and the best ROC 50  value for each matrix was used for the subsequent analyses, i.e. for evaluation and for obtaining the most sensitive matrix. The best combination of the open and extension gap penalty and the corresponding ROC 50  value for the existing nine matrices is shown in  Table 1 .
 Table 1. Optimized gap penalties and benchmark results with the training dataset of the existing nine matrices Matrix Gap penalty ROC 50 BCG1 (−11, −2) 0.0249 BCG2 (−12, −1) 0.0293 BCG3 (−12, −1) 0.0358 BLOSUM80 (−13, −1) 0.0254 BLOSUM62 (−9, −2) 0.0299 BLOSUM45 (−13, −1) 0.0288 VTML160 (−10, −2) 0.0338 VTML200 (−9, −2) 0.0361 VTML250 (−12, −1) 0.0359 Note : Optimized open and extension penalties are shown in parentheses. 2.4 Evaluation Detection sensitivity and selectivity were measured using the receiver-operating characteristic (ROC) curve ( Gribskov and Robinson, 1996 ). In this curve, the number of true positives is shown against the number of false positives with an arbitrary threshold of E-value. To compare and evaluate the performance of matrices (and also methods), ROC 50  was used as in previous reports ( Lee  et al. , 2008 ;  Schaffer  et al. , 2001 ). The ROC score is defined as the normalized area under the ROC curve; therefore, ROC 50  is the normalized area under the ROC curve up to the first 50 false positives, as
 (2) 
 In this equation,  T  represents the total number of true positives in each dataset;  t i  represents the number of true positives up to  i -th false positive. To observe the performance per query, we use ROC 5 . The percentage of all test queries that yield larger ROC 5  than a given value is shown against the given value. Furthermore, to evaluate the detection performance differences between methods statistically, we conducted the bootstrap analysis using the script provided in a report of an earlier study ( Green and Brenner, 2002 ). We also evaluated alignment quality: the alignment sensitivity and precision of matrices and methods. To assess the alignment quality, we compared sequence alignments with the structural alignments generated by Fr-TM-align ( Pandit and Skolnick, 2008 ), which allows flexible alignments, and DaliLite (ver. 3.3) ( Holm  et al. , 2008 ) as reference alignments. The alignment sensitivity, the ratio of correctly aligned residue pairs to structurally equivalent residue pairs, is defined as (N∩S)/S, where N is the number of residue pairs in the sequence alignment and S is the number of ones in the reference alignment. The alignment precision is the ratio of correctly aligned pairs to aligned pairs and is defined as (N∩S)/N. First, we randomly selected a maximum of 10 domain pairs from each family in the CATH20-SCOP test set and structurally aligned each pair with Fr-TM-align and DaliLite. Among the obtained alignments, those with TM-scores &gt;0.6 for Fr-TM-align and those with Z-scores &gt;2 for DaliLite were used as reference alignments, respectively. Results show that reference alignments of 345 pairs from 433 different protein domains were obtained using Fr-TM-align and those of 588 pairs from 670 different domains were obtained using DaliLite. Then, the average alignment sensitivity and precision were computed respectively, and were binned by pairwise sequence identity in the reference alignment. In evaluation of both detection sensitivity and alignment quality, we performed SSEARCH with the nine and obtained matrices, and also performed, with both default and optimized parameter set, SSEARCH, blastpgp ( Altschul  et al. , 1997 ) and the latest version of CS-BLAST with the K4000.crf library ( Angermuller  et al. , 2012 ). We used the –s option to enhance alignment quality by calculating the locally optimal Smith–Waterman alignments with blastpgp and CS-BLAST. 3 RESULTS 3.1 The PCA subspace We performed principal component analysis (PCA) from three series of prevailing substitution matrices, i.e. BLOSUM, VTML and BCG, to uncover a region with higher sensitivity in the PCA subspace. In this study, we used three matrices for each type, i.e. nine matrices in total. Results showed that the first three components are dominant ones to represent the total variance of the nine matrices. The first three components of PCA described ∼92.7% of the total variation. The first (PC1), second (PC2) and third (PC3) components described 61.1, 18.9 and 12.7%, respectively. Other components were responsible for 3.4% or less. Therefore, most necessary factors of the matrices are retained in this subspace consisting of the first three components. These three components can sufficiently explain the relation among the matrices. In this subspace, we found substantial linearity between PC1 and the divergence of matrices for both the BLOSUM and VTML series. For BLOSUM, as the clustering threshold is decreased, their scores (= coordinate values) on PC1 are decreased. Higher-numbered VTML matrices have lower coordinate values of PC1. In fact, eigenvectors of diagonal elements are along PC1 ( Supplementary Fig. S1 ). The divergence of BCG matrices is related mainly to PC2 instead of PC1. As the set of sequences used for constructing matrices is diverged, their coordinate values of PC2 are decreased. 3.2 Derivation of the most sensitive matrix 3.2.1 Grid search To identify a region with high sensitivity in the PCA subspace, the points around the existing sensitive matrices for the training set were sampled based on the results presented in  Table 1 . Among the existing matrices, BCG3, VTML200 and VTML250 are more sensitive than others in terms of ROC 50 . In this study, points from −14 to 4, from −14 to 2 and from −18 to 2 at two intervals were sampled for PC1, PC2 and PC3 axes, respectively. They amounted to 990 (= 10 × 9 × 11) samples ( Fig. 1 A). Matrices were calculated from PCA coordinates of those sampling points (see  Section 2 ). For every matrix sampled in the PCA subspace, we assessed the detection sensitivity in the same manner, using SSEARCH and optimized gap penalties, as existing matrices with the training set. The best ROC 50  value for each matrix was used for the subsequent analysis. According to the result of the grid search, a matrix derived from (PC1, PC2, PC3) = (−6, −6, −6) demonstrated the best performance (ROC 50  = 0.0386). Correspondingly, we also identified the most sensitive point with the SCOP40-v subset as the training set.
 Fig. 1. PCA subspace constructed with the nine existing matrices and the result of grid search. The red, green and blue lines represent BCG, BLOSUM and VTML series, respectively. On the red line, each point represents BCG1, BCG2 and BCG3 in descending order of PC2 coordinate. On the green line, each point represents BLOSUM45, BLOSUM62 and BLOSUM80 in ascending order of PC1 coordinate. On the blue line, each point represents VTML250, VTML200 and VTML160 in ascending order of PC1 coordinate. ( A ) The 990 generated points for the detection performance benchmark. ( B ) Contour plot of the estimated density (= sensitivity). The color box on right side represents a kernel density corresponding to the detection performance. The cross-sectional view, which is parallel to the PC1-PC3 plane, passing through around the highest point is shown 3.2.2 KDE and refinement To elucidate the most sensitive point (= matrix) in the PCA subspace, we performed Kernel Density Estimation (KDE) based on the results of the grid search above. As a density, we used the best ROC 50  value at each point, as described above. According to the result by KDE ( Fig. 1 B), the most densely populated, i.e. most sensitive point, was identified as (PC1, PC2, PC3) = (−4.57, −7.14, −6.57). Subsequently, we conducted grid search again to scrutinize matrices derived from around this point. We sampled and tested points (= matrices derived) from −5.5 to −4, from −8 to −6.5 and from −7.5 to −6 at 0.5 intervals for PC1, PC2 and PC3 axes, respectively. As a result of this grid search, a matrix derived from (PC1, PC2, PC3) = (−5.5, −8, −6.5) showed the best performance (ROC 50  = 0.0395), with gap penalties of −10 for open and −2 for extension, in terms of ROC 50 . We refer to this matrix as a  M atrix to  I mprove  Q uality in  S imilarity search (MIQS). Correspondingly, we also identified the most sensitive matrix and gap penalties for the SCOP40-v subset. We refer to this as MIQS.SCOP40-v. 3.3 Obtained matrix: MIQS The obtained matrix was located between the VTML series and the BLOSUM series on the PC1–PC2 plane and at the lower than most of the existing matrices on the PC3 axis ( Supplementary Fig. S1 ). According to relative entropy, our best matrix, MIQS (0.3004), is more diverse than BLOSUM62 (0.6979), even when compared with BLOSUM45 (0.3795). This might be expected because MIQS was derived using a diverse set of proteins with 20% maximum pairwise sequence identity, although the relative entropy of MIQS is slightly higher than that of BLOSUM40 (0.2851). As presented in  Figure 2  (and  Supplementary Fig. S2 ), diagonal elements, except for Gly, of the best matrix are smaller, from −1 to −4, than those of the popular matrix, BLOSUM62. By contrast, most off-diagonal elements of MIQS are larger, from 1 to 4, than those of BLOSUM62, although some off-diagonal elements of MIQS are smaller, from −1 to −3, than those of BLOSUM62. Notably, values for amino acid pair associated with Trp, such as W–C, W–Q and W–E pairs, are reduced in MIQS. Those differences remind us of the observation that chemical characteristics of amino acids are influential at high divergence ( Benner  et al. , 1994 ). On biplots, the W–C pair is far from 0 associated with PC1, PC2 and PC3, and W–Q and W–E pairs show extremal positions associated with PC3, reflecting the high variance in these mismatch scores between MIQS and BLOSUM ( Supplementary Fig. S1 ).
 Fig. 2. Comparison between obtained matrix and BLOSUM62. The obtained matrix (lower) and difference matrix (upper) obtained by subtracting BLOSUM62 from the obtained matrix are shown 3.4 Performance of obtained matrix We evaluated both the detection performance and alignment quality of nine existing matrices, MIQS, MIQS.SCOP40-v and BLOP20 with their optimized gap penalties for the training set from SCOP20 using SSEARCH. For comparison with standard methods, we measured the performance of SSEARCH, blastpgp and CS-BLAST with both the default parameter set and the optimized gap penalties for the training set from SCOP20 in the same manner as that described above. 3.4.1 Detection sensitivity When we measure the detection performance using ROC 50  with the validation set, as depicted in  Figure 3 A, a matrix that showed the best detection performance among the existing nine matrices was VTML200, which was identical to a result with the training set. In contrast, the detection performance of our novel matrix indicated ROC 50  of 0.0347, which was higher than that of VTML200 by ∼10.4%, and which was almost identical to that of CS-BLAST. MIQS and MIQS.SCOP40-v showed almost identical detection sensitivity. However, ROC 50  of BLOP20 was 0.0281 and it was inferior to MIQS by ∼23.7%. When compared by the  superfamily - weighted ROC 50 , the performance of MIQS was also higher than that of VTML200, by ∼5.1%, and was almost identical to that of CS-BLAST ( Fig. 3 B). 
 Fig. 3. Detection performance of developed matrix. ( A ) A comparison of ROC 50  value of the developed matrix and other existing matrices and other methods. ( B ) ROC curve of the developed matrix and other matrices and methods on the SCOP20 validation set. The purple and bold line represents the developed matrix. The number of true positive relation detected was weighted by the number of superfamily. ( C ) ROC curve on the CATH20-SCOP dataset. ( D ) ROC 5  curve on SCOP20 validation set. In SSEARCH, blastpgp, and CS-BLAST, (def.), present the result with the default gap penalties, and (opt.) shows the one with gap penalties optimized on the training set from SCOP20. BLOSUM50 is used for SSEARCH (def.) and (opt.), and BLOSUM62 is used for blastpgp (def.) and (opt) Inconsistencies in SCOP classification have been noted. For example, the authors of CS-BLAST argue that SCOP has no well-classified  superfamilies  or  folds . Then they avoid judging pairs as true or false within the four- to eight-bladed β-propellers (SCOP fold IDs: b.66−b.70), Rossman-like folds (c.2−c.5, c.30, c.66, c.78, c.79, c.111) and α-helical and 4Fe-4S ferredoxins (a.1.2, d.58.1). The detection performance of MIQS exceeded that of the nine existing matrices and was lower than that of CS-BLAST by ∼5.8% if followed by the standard with  superfamily  weighting. Similarly, the performance of MIQS exceeded that of the existing matrices and was lower than that of CS-BLAST by ∼11.4%, when we used the ruleset for SCOP 1.61 benchmarks ( Gough  et al. , 2001 ). Furthermore, we evaluated the detection performance of the matrices and methods against another test dataset, CATH20-SCOP, to ensure the independence of the dataset from the training set ( Fig. 3 C). Results show that, if compared by ROC 50 , a matrix of the best performance detection among the existing matrices was also VTML200, whereas MIQS was slightly better than VTML200. When compared by the  homologous superfamily - weighted ROC 50 , the performance of MIQS was higher than that of VTML200 by ∼2.9% and was higher than that of CS-BLAST by ∼12.0%. In this case, the CS-BLAST performance was degraded drastically, partly because CS-BLAST can detect many homologies from a few, rather than from various,  superfamilies  (see below). We also compared ROC 5  for search results of individual query ( Fig. 3 D). In this figure, the percentage of queries that exceeds an ROC 5  value is shown against the horizontal axis. This figure demonstrates how effective the testing method is in actual use because the ROC 5  analysis evaluates the detection performance of the testing method when very few false positives are detected: the number of false positives is five in this case. As depicted in  Figure 3 D, the fraction of queries of MIQS was the highest over the entire range of ROC 5  value among all tested matrices and methods: MIQS showed the highest performance for both easy problems (larger ROC 5  on horizontal axis) and difficult problems (smaller ROC 5  on horizontal axis). Furthermore, enhanced evaluation supports our observation. When we perform the bootstrap analysis ( Green and Brenner, 2002 ), at Error Per Query (EPQ) = 0.0285, which corresponds to 50 false positives in ROC curve, we observed that SSEARCH with MIQS was significantly better than CS-BLAST for the CATH20-SCOP dataset, although we were unable to find a significant difference between SSEARCH with MIQS and CS-BLAST for the SCOP20 validation dataset at EPQ = 0.0141, which corresponds to 50 false positives in ROC curve ( Supplementary Fig. S3 ). The same results were observed at EPQ = 0.01 in both cases. We compared the number of true positive relations detected at the number of false positives as 50 with MIQS and the other matrices and methods. In addition to this, we compared the number of true positive  superfamilies  detected with MIQS and the other matrices and methods. Comparison results are portrayed in  Figure 4  as Venn diagrams. The number of true positives detected with MIQS was compared with that of VTML200 ( Fig. 4 A) and CS-BLAST ( Fig. 4 B). The number of true positive  superfamilies  detected by MIQS was also compared with that of VTML200 and that of CS-BLAST. Regarding true positive relations, MIQS was able to detect more unique true relations than VTML200 did, but less than CS-BLAST did. However, MIQS detected the same number of  superfamilies  as VTML200 ( Fig. 4 C) did, but more  superfamilies  than CS-BLAST did ( Fig. 4 D). These results indicate that, compared with VTML200, MIQS can detect more sequences from various different homologous relations. Compared with CS-BLAST, MIQS can detect various sequences from various different homologous relations.
 Fig. 4. Venn diagrams for the number of true positive relations and superfamilies detected. Comparison of true positive relations ( A  and  B ) and superfamilies ( C  and  D ) detected between the developed matrix and VTML200 and CS-BLAST 3.4.2 Alignment quality The alignment quality is assessed using two standard measures: alignment sensitivity and precision, as described in  Section 2.4 . We compared sequence alignments with the structural alignments generated by Fr-TM-align ( Fig. 5 A and B) and DaliLite ( Fig. 5 C and D). Although, compared with the case with DaliLite, greater values of both sensitivity and precision, except for the quite low (5–10%) range of sequence identity, were observed using Fr-TM-align, which allows flexible alignments, for all matrices and methods, overall trends were preserved in both cases.  Figure 5 A and C shows the alignment sensitivity, and  Figure 5 B and D shows precision for various sequence identity bins in reference alignments. Regarding alignment sensitivity, in both cases, MIQS and BLOP20 showed comparable performance with the best one, VTML250, over almost the entire range of sequence identities in this test. In terms of alignment precision, MIQS.SCOP40-v and CS-BLAST are superior to other matrices and methods in the entire range.
 Fig. 5. Alignment quality of matrices and methods. Alignment sensitivity, defined as (N∩S)/S, of the developed matrix, MIQS, and other matrices and methods. Here, N denotes the number of residue pairs in an alignment and S denotes the number of residue pairs in a reference alignment. Therefore, sensitivity measures the fraction of correctly aligned residue pairs in the sequence alignment. Alignment precision, defined as (N∩S)/N, of the developed matrix and other matrices and methods. Precision measures the fraction of correctly reproduced alignment compared with the reference alignment. Reference alignments were generated, respectively, using Fr-TM-align ( A  and  B ) and DaliLite ( C  and  D ) Generally, a tradeoff exists between sensitivity and precision. In terms of alignment sensitivity, SSEARCH with sensitive matrices is better than BLAST-based methods, partly because the BLAST algorithm including CS-BLAST, and also SSEARCH with MIQS.SCOP40-v, tends to generate shorter alignment ( Supplementary Fig. S4 ). Instead, the performance of shorter alignment groups such as BLAST-based methods and SSEARCH with MIQS.SCOP40-v is exceeded in alignment precision comparison. In terms of both alignment sensitivity and precision, MIQS is balanced compared with existing matrix series, which tended to produce longer alignments, i.e. better sensitivity, with more diverged ones, and which tended to generate shorter alignments, i.e. better precision, with less diverged ones. Similar results were obtained when reference alignment was generated from SCOP20 validation dataset instead of CATH20-SCOP test dataset ( Supplementary Fig. S5 ). 4 DISCUSSION In this study, based on a previous finding, we empirically identified a region in the PCA subspace that represents a set of matrices that are suitable for detecting distantly related proteins, by combining benchmarks and PCA of the nine existing matrices from BCG, BLOSUM and VTML series. This approach differs from conventional approaches used to obtain optimized matrices. Consequently, we were able to provide a novel and highly sensitive substitution matrix, which we call MIQS, for distantly related protein sequence comparison. We were able to find that the MIQS performance with SSEARCH is superior to the sophisticated approach, CS-BLAST, in terms of detection sensitivity on an independent dataset, although CS-BLAST is clearly superior to other methods when we consider inconsistencies in the SCOP classification. This finding is expected to have a major influence on any field of protein sequence analysis that uses a substitution matrix, such as multiple alignment, profile–profile alignment and phylogeny inference. We constructed the PCA subspace with the first three PC axes. It was thought to be sufficient for these three axes to present a relation among the existing matrices because the accumulative contribution of these axes reached ∼93% of total variance. It was able to reproduce the existing matrices from the coordinate of the PCA subspace. We found relations between PC1 and the divergence of matrices, and between PC2 and the divergence of the set of sequences used for constructing matrices. We speculate that a relation exists between PC3 and the datasets, models and methods used for constructing matrices. These observations might imply that our projection to the PCA subspace was used to ‘de-noise’ data of amino acid substitutions, and that one can obtain an arbitrary general purpose matrix using the PCA subspace without computing actual amino acid substitutions. Associated with PC3, some mismatch pairs related to rare amino acids, such as Trp and Cys, were far from 0 ( Supplementary Fig. S1 ). This apparently implies that PC3 has a role in adjusting the variation result from low background frequency of rare amino acids. Using our approach, one might also develop an amino acid matrix that is suitable for specific purpose such as a transmembrane matrix, i.e. an AT-biased matrix. Our developed matrix, MIQS, exhibited extremely high-detection performance. In the ROC 5  curve, which is an analytical method suitable for actual sequence similarity search, MIQS showed the best detection performance over the entire range of ROC 5  values. We learned different characteristics of CS-BLAST from matrix-based methods. CS-BLAST can detect many true positive relations from the confined  superfamily  group, rather than from various  superfamilies , as shown in the Venn diagrams ( Fig. 4 ). For instance, CS-BLAST can detect a huge number of true positive relations within the c.37.1 (P-loop containing nucleoside triphosphate hydrolases)  superfamily  in SCOP. As described in  Section 3 , for the CATH20-SCOP test set, the performance of CS-BLAST was degraded drastically. These results suggest that CS-BLAST might be overfitted to the SCOP dataset or to some confined  superfamilies / homologous superfamilies  when its parameters were trained. However, according to the result portrayed in  Figure 4 B, combining CS-BLAST and conventional search with matrices including our developed one for similarity search is expected to be beneficial. In general, detecting the relations within larger  superfamilies  is more difficult, as pointed out in an earlier study ( Green and Brenner, 2002 ). It is noteworthy that CS-BLAST is ∼2-fold faster than SSEARCH when we search query sequences against a large database, here NCBI NR, although CS-BLAST takes time when we perform all-against-all sequence comparison ( Supplementary Fig. S6 ). The detection performance of a method is not necessarily proportional to the quality of sequence alignment ( Vingron and Waterman, 1994 ). In the evaluation of alignment quality with DaliLite, our approach is not the best, but it is well balanced and comparable to the best method(s). Another example shows that MIQS is of good alignment quality. Yu  et al.  performed compositional adjustment of amino acid substitution matrices for better alignment quality. They compared an alignment calculated using their composition-adjusted matrix and the original BLOSUM62 matrix in their report. We also aligned the same sequences they used and obtained a similar, though longer, alignment as they did without compositional adjustment ( Supplementary Fig. S7 ). In addition, results obtained using POP ( Edgar, 2009 ) show that MIQS (and BLOP20) is suitable for pairwise global protein alignments, although we used the Smith–Waterman local alignment method to derive MIQS ( Supplementary Fig. S8 ). This result suggests that multiple alignment methods can be improved using MIQS. The recently developed innovative method, CS-BLAST, does not require the use of any substitution matrix for similarity search. However, as shown in this study, the availability and importance of amino acid substitution matrices have remained. Our novel matrix, MIQS, can be useful for improving the performance of existing methods easily. In addition, strictly speaking, CS-BLAST actually requires an amino acid matrix to construct its context library. Moreover, MIQS might be useful with other advanced methods, such as profile–profile methods, to improve their performance. In the future, we will examine whether our developed matrix, MIQS, can enhance the performance of these methods. 5 CONCLUSION We demonstrated in this study that, using the PCA subspace based on typical existing matrices, we were able to obtain a sensitive novel matrix, MIQS, empirically. Therefore, it is possible to use it to improve the homology detection of proteins, especially in the SCOP and CATH database, compared with existing matrices and CS-BLAST. We argue that MIQS can be useful for other database searches, and that this matrix can be influential for the improvement of sophisticated methods, such as PSI-BLAST and profile–profile comparison methods, in addition to any method using a substitution matrix in the field of bioinformatics. 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Learning structural motif representations for efficient protein structure search</Title>
    <Doi>10.1093/bioinformatics/bty585</Doi>
    <Authors>Liu Yang, Ye Qing, Wang Liwei, Peng Jian</Authors>
    <Abstract/>
    <Body>1 Introduction The comparison of protein structures has been a fundamental and widely applicable task in structural biology. Given a new protein with unknown functions, searching similar protein structures from existing databases is a critical step for predicting its function. It is particularly valuable when the query protein shares little sequence similarity with existing ones, where sequence alignment algorithms, such as BLAST( Camacho  et al. , 2009 ), cannot easily identify evolutionary relationships. Protein structural search against a database is often expensive because performing pairwise structure alignments against all proteins in a database is computationally linear to the number of protein structures in the database, which is usually large. Due to the advances in crystallography and cryo-EM, the Protein Data Bank( Berman  et al. , 2000 ), a database of solved protein structures, has recently been increasing rapidly, with new protein structures solved almost every day, though the number of possible protein folds is thought to be bounded. Structural classifications of PDB, including SCOP( Fox  et al. , 2014 ) and CATH( Sillitoe  et al. , 2015 ), enable us to explore the hierarchical organization of protein structure space and provide useful guidelines on the relationship between protein structure and functions by identifying structural neighbors. However, the classification of structural neighbors may not be optimal, as there are neighbors, though functionally and structurally similar, classified differently. Thus, fast and accurate structural search against a large-scale protein structure database is still a challenge. Arguably the most popular structural comparisons approaches are structural alignment-based methods ( Menke  et al. , 2008 ;  Shindyalov and Bourne, 1998 ;  Wang  et al. , 2013 ;  Ye and Godzik, 2003 ;  Zhang and Skolnick, 2005 ). Pairwise structural alignment algorithms take 3D coordinates of two protein structures and optimize a predefined geometric similarity measure using heuristics, such as simulated annealing, dynamic programming and local search. Although these approaches can provide residue-resolution alignments with high accuracy, they are often computationally expensive. Such computational cost makes large-scale structural search not practical when coping with a very large database since the runtime of such structural alignment algorithms scales linearly to the number of proteins in the structure database. Another entirely different approach for structural search is to represent protein structures using structural features as 1D vectors and then perform similarity calculation of such vectors without performing an alignment. The idea of alignment-free algorithms was initially introduced for protein sequence comparisons. A feature vector is computed to represent each protein sequence for fast cosine- or Euclidean distance-based similarity calculations. For instance, CD-HIT computes a histogram of k-mers for fast sequence clustering ( Li and Godzik, 2006 ). Other examples include k-mer mismatch kernels for homology search and compositional methods for metagenomic binning ( Eskin  et al. , 2002 ;  Luo  et al. , 2016 ). In many applications, these methods can provide fast comparisons with comparable performance to the alignment-based methods. Recently, several alignment-free protein structural comparisons have been proposed ( Budowski-Tal  et al. , 2010 ;  Rogen and Fain, 2003 ;  Zotenko  et al. , 2006 ). By representing each protein as a vector of structure features or fingerprints, we can compute the similarity between vectors to find structural neighbors in a large database. As a notable example, FragBag represents each protein by a ‘bag of fragments’, which is a vector of frequencies of a set of predefined contiguous short backbone fragments ( Budowski-Tal  et al. , 2010 ). Although it achieves comparable accuracy to some structural alignment algorithms, the performance of FragBag is not satisfactory when the database becomes large. A possible reason is that FragBag only considers contiguous backbone fragments, which captures only local property of the whole structure. This limitation is sub-optimal because important long-range interacting patterns are ignored. Here we present a new approach to learning an innovative structural motif presentation using deep learning. Inspired by FragBag, we hope to further generalize it by learning conserved structural motif for protein structure representation. We develop DeepFold, a deep convolutional neural network model to extract structural motif features of a protein structure from its  C α  pairwise distance matrix. This neural network model extracts local patterns from residue contact patterns and composes low-level patterns into high-level motifs from layer to layer. From this neural network, we can represent each protein structure/fold using a vector of structural motifs and perform the structural search by only computing vector similarity. This neural network model is trained in a supervised manner by fitting TM-scores ( Zhang and Skolnick, 2004 ,  2005 ), a structural similarity score, between existing protein structures from a nonredundant SCOP database. We demonstrate that DeepFold substantially outperforms three existing alignment-free methods, FragBag ( Budowski-Tal  et al. , 2010 ), SGM ( Rogen and Fain, 2003 ) and SSEF ( Zotenko  et al. , 2006 ), on protein structure search for a set of newly released PDB structures in 2016. DeepFold achieves improved structural search accuracy and obtains better structural neighbors. We also show that DeepFold not only extracts conserved backbone segments but also identifies important long-range interacting structural motifs in the representation. We expect that DeepFold will provide new insights into the evolution and hierarchical organization of protein structural motifs. 2 Background: protein structure comparison and retrieval 2.1 Structural alignment Pairwise structural alignment algorithms take two protein structures as input and identify geometrically aligned substructures according to a predefined structural similarity score. Notable examples of structural alignment algorithms include TM-align ( Zhang and Skolnick, 2005 ), Combinatorial Extension (CE) ( Shindyalov and Bourne, 1998 ), DeepAlign ( Wang  et al. , 2013 ), Matt ( Menke  et al. , 2008 ) and FATCAT ( Ye and Godzik, 2003 ). It is worth noting that, given a structural similarity score, sophisticated optimization and sampling techniques are usually required to optimize the score and find the best alignment and substructures. As a result, the computation of aligning two structures is expensive. Therefore, using structural alignment algorithms for structural neighbor retrieval would require  O ( n ) pairwise structural alignments, where  n  is the size of the structure database used for retrieval. By February 1st of 2017, there have been more than 117 000 protein structures deposited in the PDB. Even the non-redundant SCOP and CATH include more than 13 000 structural domains. Thus, naively applying structural alignment algorithms for protein structure retrieval is prohibitively expensive. 2.2 Alignment-free methods In contrast to structural alignment algorithms, alignment-free methods have been proposed for accelerating structural neighbor search. Instead of optimizing a complex score for identifying geometrically similar substructures, alignment-free methods represent each protein as a set of structural patterns or fingerprints encoded in a feature vector and compare these representations via very simple similarity calculations like cosine- or Euclidean similarity. For example,  Zotenko  et al.  (2006)  count the frequencies of 1500 secondary structure triplets (SSEF) as a vector for representing a protein structure. Scaled Gaussian Metric (SGM) ( Rogen and Fain, 2003 ) represent a protein structure using 30 global backbone topological measures. Recently, Kolodny and co-workers developed FragBag ( Budowski-Tal  et al. , 2010 ), a ‘bag-of-words’ vector representation including frequencies of local contiguous fragments in the protein backbone. The optimal performance was achieved with a library of 400 backbone segments, each with 11 contiguous residues. Although FragBag shows improved retrieval performance over previously existing alignment-free methods, the accuracy of FragBag is still not satisfactory compared to advanced alignment-based methods. Possible reasons are that i) the backbone fragment library may not be optimal and that ii) the long-range interacting patterns, which are known to be highly important in discriminating different protein fold, were not considered. 3 Learning structural motifs for protein structure representation We propose DeepFold, a novel deep learning based method to project tertiary protein structures into a low-dimensional vector space. Different from conventional protein structure filtering methods like FragBag, our method does not need a predefined fragment library but automatically identifies relevant local structural protein motifs and long-range interacting motifs. Benefiting from the powerful representation learning ability of the deep neural network, DeepFold is able to construct a more powerful protein structure representation, achieving a better performance on the task of protein structural searching. 3.1 Protein structure comparison Suppose we have a query structure  x Q  and two candidate template protein structures  x A ,  x B .  By applying the DeepFold neural network on these structures as a feature extractor, we obtain three corresponding structural fingerprint vectors  v Q ,  v A ,   v B . With these fingerprints, we compute the similarity of each candidate to the target query by taking the cosine similarity of their corresponding vectors and ranking all candidates from a structure database accordingly. A schematic diagram of this procedure can be found in  Figure 1 . Thus, how to effectively represent each protein into a low-dimensional vector space is quite critical for enabling accurate protein structure search. Fig. 1. Alignment-free structure comparison. The query protein structure is mapped into a low-dimensional fingerprint vector. We then compute a cosine similarity score between the query fingerprint vector and the vector of a template structure. Similar structures share high similarity scores, while dissimilar structures share low similarity scores 3.2 Deep convolutional neural network Deep convolutional neural networks (DCNNs) are natural for learning hierarchical representations of image data. Multiple layers of convolutional filters are constructed for identifying local patterns in a nested manner. Inspired by the recent successes of DCNNs in computer vision and image processing, we propose to develop a deep convolutional neural network model, DeepFold, to learn structural motifs for structural comparison. Given a tertiary structure of a protein, we first calculate the  C α  pairwise distance matrix as the raw representation feature, aiming at preserving the geometric information of the input protein structure. We denote the proteins structure as  x  and its pairwise distance matrix as  D , each entry  D i j  being the  C α  distance between  i th residue and  j th residue. Due to the existence of missing residues in protein structures, directly taking the distance matrix as input may later cause numerical issues in the neural network. So instead, we use the following tensor as input.
 (1) X i j k = D i j − 2 k , k = 1 , ... , K 
where  X ∈ R L × L × K  and  K  is an integer indicating the inverse power of the squared distances. It is worth noting that the choice of this inverse power series of distances is similar to several distance-dependent approximations in force field energy functions, such as van de Waals and electrostatic energies ( Brooks  et al. , 1983 ;  Weiner and Kollman, 1981 ). As shown in  Figure 2 , a deep neural network, DeepFold, takes the transformed input feature of  X , followed by a sequence of blocks of transformations. Each block contains the stacked layers including a convolutional layer and a nonlinear transformation ReLU layer. After the last convolutional layer, we apply a mean-pooling layer and L2 normalization to aggregate features and obtain the final fingerprint representation  f ( X )  of the input protein structure. Taken as a whole, our deep net is a non-linear mapping from a high-dimensional structural input  X  to a low-dimensional fingerprint representation  f ( X ) . Specifically, a convolutional layer takes the output from the previous layer  H n − 1 ∈ R L n − 1 × L n − 1 × K n − 1  as input and compute output  H n ∈ R L n × L n × K n  in the following way. For each  k ∈ { 1 , 2 , .. , K n } (2) H n k = R e L U ( W n k * H n − 1 + b n k ) 
where for the  n th convolutional layer,  W n k ∈ R l × l × K n − 1  is a weight tensor of  K n − 1  convolutional filter weights of size  l  ×  l.  The function operator  ( * )  is 2-dimensional convolution operator over the first two dimensions of the input  H n − 1 , and  b n  is a bias term. Intuitively, a filter in the first convolutional layer can be seen as a local feature extractor on the inverse pairwise distance matrix with the goal to capture important local residue contact patterns as a structural motif. In higher layers, a filter can be seen to compose low-level structural motifs into high-level motifs. However, note that the convolutional operation essentially performs dot products between the filters and local regions of the input features. A common implementation of the convolutional layer is to take advantage of this fact and formulate it as matrix multiplications. The output dimensions  L n × L n  depend on the stride which further reduces the feature dimensionality by controlling how the convolutional filters slide across the input of each layer. After each convolutional layer, a Rectified Linear Unit (ReLU) function ( Krizhevsky  et al. , 2012 ) is applied for nonlinear activation. The ReLU can be implemented by simply thresholding a matrix of activations at zero by  R e L U ( h ) = max ⁡ ( 0 , h ) . Given any tensor/matrix/vector input  h , ReLU takes elementwise max operations. Compared to traditional logistic or  tanh ⁡  functions, ReLU is found to greatly accelerate the convergence of stochastic gradient descent due to its linear, non-saturating form. Fig. 2. Deep neural network structure of DeepFold: Given a protein structure, we first compute the raw distance features. Then we feed the features into a deep convolutional neural network that consists of N convolutional blocks. Inside each block, we apply a convolutional layer, followed by a ReLU nonlinear layer. The final output of DeepFold is a vector representation of structural motifs After the last convolutional layer, we get  K N  feature maps with the size of  L N × L N . Then for each feature map  i ∈ { 1 , 2 , .. , K N } , we extract the diagonal elements of each feature map and take the mean pooling (that is, calculating the mean value of diagonal elements). Therefore, we get the  K N -dimensional vector representation of  f ( x ). For optimization efficiency, each vector is normalized with L2 Norm so that all the vectors are projected on a sphere  | | f ( x ) | | 2 = 1 . Similar to the FragBag representation, DeepFold maps an input tertiary structure into a low-dimensional one-dimensional fingerprint vector but this representation is parameterized via a deep neural network, with each dimension encoding a specific structural motif. Therefore, DeepFold is more powerful than FragBag in that it extracts not only contiguous backbone fragments but also the long-range interacting motifs, which are completely omitted by FragBag. One can easily see that FragBag becomes a special case of DeepFold if we only consider the near-diagonal region of the pairwise distance matrix as input. Our in-house experiments showed that the learning convergence of this special case is poor in practice. 3.3 Learning to compare protein structures Given two proteins  x A  and  x B , with their normalized representations  f ( x A )  and  f ( x B ) , we expect the similarity between  f ( x A )  and  f ( x B )  can reflect the structural similarity between  x A  and  x B .  So if two proteins have very similar structures, we should map their features  f ( x A )  and  f ( x B )  to be close to each other, while if  x A  and  x B  have different structures, we should push their fingerprint features far from each other. Although pairwise structural alignments are slow in protein structural retrieval, we can precompute them on a training dataset and use the resultant structural similarity scores to guide the training of DeepFold. So our key idea here is to fit structural comparison scores using powerful deep neural networks, thus enabling fast alignment-free comparison and retrieval for new query proteins. To implement this idea for training DeepFold, we use the cosine similarity score of  f ( x A )  and  f ( x B )  and apply the well-known max-margin ranking loss function ( Wang  et al. , 2016 ) to discriminate structurally similar proteins from dissimilar ones. To apply the max-margin rank loss function, we first define what positive (similar) and negative (dissimilar) pairs of proteins are. In this work, we use the structural alignment program TM-align ( Zhang and Skolnick, 2004 ) to measure the structural similarity of two proteins. Based upon a dynamic programming algorithm, TM-align performs structural alignment of the two input structures to produce an optimal superposition and return a TM-score that scales the structural similarity in the range of  [ 0 , 1 ] . Therefore, for a protein  x A , we define all pairs ( x A ,  x B ) from the database with TM-score higher than  p * T M m a x ( x A )  as positive pairs, where  T M m a x ( x A )  is the maximal TM-score between  x A  and other proteins in the database, and  p  is a hyper-parameter chosen to be 0.9. So only very similar structures in the database are considered to construct positive pairs in training. For all other pairs that have scores smaller than this threshold, we consider them as negative pairs. It is important to note that other structural similarity scores or structural alignment algorithms can be used here. We choose TM-align/score as it has been shown to be both fast and accurate in structural classification and often more robust than many other structural similarity scores ( Zhang and Skolnick, 2005 ). With the defined positive and negative pairs, our target is to make the margin between all positive pairs and negative pairs as large as possible. Formally, for a specific protein  x A , suppose the set of all positive pairs is  X A +  while the negative set is  X A − . Thus, the margin loss could be defined as:
 (3) L ( x A ) = ∑ x + ∈ X A + ∑ x − ∈ X A − max ⁡ ( 0 , cos ⁡ ( f ( x A ) , f ( x − ) ) − cos ⁡ ( f ( x A ) , f ( x + ) ) + m ) 
where  m  is a small positive margin value, which is chosen as 0.1 in this work. Then, the objective is to minimize this margin loss and learn the parameters { W ,  b } in all layers of the neural network as defined in the above. Efficient online training In above  Equation 3 , to optimize the loss function, we need to sum over all positive and negative training pairs and minimize the rank loss function. However, the number of training pairs can be very huge, and thus it is not feasible to optimize them all at one time. In addition, the imbalance of positive and negative pairs pose another challenge in training. Here, we use the stochastic gradient descent to only randomly take a small batch of training samples at in each iteration. Specifically, we use mini-batches to do the feed-forward passes and back-propagation in each iteration. Motivated by ( Schroff  et al. , 2015 ), we design a simple yet effective sampling method to obtain a fast empirical convergence by selecting effective pairs. Given a protein  x A , we consider the most effective positive pairs and negative pairs  ( x A , x a + )  and  ( x A , x a − )  accordingly as:
 (4) x A + = arg ⁡ min ⁡ x ∈ X + cos ⁡ ( f ( x A ) , f ( x ) ) (5) x A − = arg ⁡ max ⁡ x ∈ X − cos ⁡ ( f ( x A ) , f ( x ) ) Intuitively, we identify those positive pairs that are easier to get confused with negative pairs (furthest positive pairs) and those negative pairs that are easier to get confused to positive pairs (nearest negative pairs) within each mini-batch. If mini-batches may not have any positive pair, we add at least one positive pair for each protein in the mini-batch to balance the training. Also, inside each mini-batch, we identify negative targets that are most similar to the query and incorporate the margin between all positive pairs and these k negative pairs to make the training process more effective. 3.4 Implementation details The first convolutional layer consists of 128 filters of size 12 × 12 with a stride 2 × 2. After that, we connect it with the second layer of 256 kernels of size 4 × 4 with a stride of 2 × 2. From the third layer, we stack 3 identical convolutional layers, each with the size of 4 × 4 and a stride 2 × 2. Finally, the output of stacked layers is linked to 400 filters with a size of 4 × 4 and a stride of 2 × 2. The total dimension of 400 is selected to match the size of the vector representation used in FragBag. Based on the features extracted, we capture only diagonal elements and perform mean pooling for each filter. Then we conduct L2 normalization and project all final fingerprints into the space of sphere  | | x | | 2 = 1 . DeepFold is implemented with the Python library Lasagne ( https://github.com/Lasagne/Lasagne ) based on Theano platform ( Theano Development Team, 2016 ). In the optimization, we apply stochastic gradient descent (SGD) with a momentum of 0.9. The learning rate decay scheme is chosen with AdaGrad ( Duchi  et al. , 2011 ) algorithm. The mini-batch size is chosen as 64 considering the memory usage on the graphics card. Within each mini-batch, we sample one positive instance for each protein and compute top 10 hardest negative instances in the margin loss inside each mini-batch. To make the model more robust, we used Dropout( Srivastava  et al. , 2014 ) after each ReLU layer, which could be viewed as an ensemble trick to enhance the generalization ability of the model. During training, we monitor the retrieval accuracy using the validation set as queries and the training set as the database. Totally, we train each model with 100 epochs and select the one with best evaluation accuracy. Then we report the performance tested with testing fold as queries and all of the training data as the database on this selected model. All the experiments are performed on a workstation with 256GB RAM and an NVIDIA Titan X graphics card with 12GB memory. 4 Results 4.1 Comparison to existing alignment-free structural retrieval methods Experimental settings We construct a sequence-nonredundant structure dataset by filtering out protein structures with 40% sequence identity in the latest SCOP database (version 2.06) ( Fox  et al. , 2014 ). The filtered database includes 13 546 representative protein domains, indexed by the manually curated SCOP taxonomy classification. We compare our proposed DeepFold with FragBag ( Budowski-Tal  et al. , 2010 ), the existing state-of-the-art alignment-free method. With the goal of making a fair comparison, we set the dimensionality of DeepFold representation to be 400, which is the same parameter used in the default setting of FragBag. Also, we also compare our approach with two other previous alignment-free methods, including SGM ( Rogen and Fain, 2003 ) and SSEF ( Zotenko  et al. , 2006 ), which were introduced much earlier. Furthermore, to test whether the long-range patterns are helpful, we also build a simplified local DeepFold model as a baseline, which only extracts fragmental features from consecutive residuals, the same as FragBag. This method is denoted by DeepFold(L). For assessment, we pick structures with TM-Scores that are no less than 0.9 of the highest TM-Score achievable by structures in the database as the True structural neighbors to a target structure. It is natural to train and/or test DeepFold with other structural similarity metrics such as GDT_TS ( Zemla  et al. , 1999 ;  Zemla, 2003 ), MaxSub ( Siew  et al. , 2000 ) and lDDT ( Mariani  et al. , 2013 ). To evaluate the accuracy of structural retrieval, we compute the Receiver Operating Characteristic (ROC) curve for each method and calculate the average area under ROC curve (AUROC) of all query proteins in the validation set. A more effective algorithm should have a larger AUROC. In addition to the ROC curve, we also plot the hit @ top- k  curve, which has been widely used for evaluating ranking performance in information retrieval ( Zhai and Massung, 2016 ). The hit@top- K  is calculated as the percentage of queries in which at least one positive template (i.e. TMscore  &gt; 0.9 * T M m a x ( q u e r y ) ) appear in the top- K  retrieved list from the dataset. We choose the top- K  hit rate as the metric motivated by the fact that if there is a very similar structural neighbor ranked within top  K , we can apply a structural alignment algorithm [such as TM-align ( Zhang and Skolnick, 2005 )] to identify it with at most  K  pairwise alignments. Besides the curves mentioned above, we also report the area under the precision-recall curve(AUPRC) along with hit@top-1, hit@top-5, hit@top-10 in the table, providing alternative metrics for comparison. In addition, we also compute hit@top-K accuracy by the SCOP classification, i.e. whether two structures are within the same family classification. Results on SCOP data Since DeepFold is trained by supervised learning, in contrast to other unsupervised alignment-free structural comparison methods, we firstly design a rigorous cross-validation scheme to evaluate the performance on the SCOP structure database more robustly. Specifically, we randomly split all structures from the SCOP database into 5 subsets and perform a 5-fold cross-validation. In each fold, we only use pairwise TM-scores between proteins from the training set as ground truth to train DeepFold model, and for evaluation, we utilize each protein in the validation set as the query to search in the training set and report the retrieval accuracy. We show ROC curves and Hit curves on cross-validation and the comparison results in  Figures 3 ,  4  and  Table 1 , respectively.
 Table 1. Performance comparison on the SCOP data Algorithm Dimension AUROC AUPRC Top1 Acc Top5 Acc Top10 Acc SGM ( Rogen and Fain, 2003 ) 30 0.768*** 0.062*** 0.000*** 0.001*** 0.002*** SSEF ( Zotenko  et al. , 2006 ) 1500 0.807*** 0.004*** 0.001*** 0.006*** 0.016*** FragBag ( Budowski-Tal  et al. , 2010 ) 400 0.904*** 0.359*** 0.496*** 0.610*** 0.653*** DeepFold (L) 400 0.958*** 0.341*** 0.443*** 0.596*** 0.662*** DeepFold 400 0.979 0.498 0.587 0.736 0.784 Note : *denotes  p -value &lt; 5e-3, **denotes  p -value &lt; 1e-5 and ***denotes  p -value &lt; 1e-10. Fig. 3. ROC curves and Hit@Top- k  curves on SCOP data. The performance of DeepFold is compared with FragBag and a simplified version of DeepFold with only local backbone segmental features Fig. 4. Hit@Top- k  according to SCOP family classification. The performance of DeepFold is compared with FragBag and DeepFold(L) According to the average ROC Curve of these algorithms, our method significantly outperforms other alignment-free methods. Our algorithm achieves a much higher true positive rate at the same level of the false positive rate. Also, our algorithm has a higher AUPRC score, indicating DeepFold is better at ranking similar structures on the top. Moreover, according to the top-1, top-5, top-10 accuracy, our algorithm has remarkable improvement compared with the strongest baseline, FragBag. From  Figure 4 , we also observe the significant improvement in the accuracies determined by the SCOP family classification, indicating our method is consistently better and not overfitted to the metric used for training. The improvement we observed is probably because our algorithm could automatically learn structural motif which is hard to be computed by human clustering or there maybe consist of several unknown protein structures which have not be explored by the human before. Furthermore, FragBag treats each structural motif equally which is not consistent with biological insights. Nevertheless, our DeepFold can learn the weight for each filter automatically so that each motif is attached with a weight. In addition, our DeepFold(L) outperforms FragBag according to AUROC but slightly worse according to hit@top-k and AUPRC. We think the possible reason is that Fragbag is constructed on a set of representative protein folds, while DeepFold(L) is only trained on a subset of protein fold space in the cross-validation. Furthermore, we study whether long-range structural motifs are useful for improving retrieval. Both FragBag and DeepFold(L) only consider local structural motif representation, while DeepFold considers both local structural motif and long-range protein contact patterns at the same time, which is harvested by the convolutional structure and non-linearity of the networks. The retrieval accuracy of DeepFold is significantly better than both DeepFold(L) and Fragbag, thus indicating that long-range structural motifs should be considered for representing protein structures. Results on searching recently released proteins in PDB In addition to the cross-validation on SCOP, we hope to further evaluate the generalization performance of DeepFold on the newly release protein structures in PDB. We download all recently released protein structures on the PDB website ( Berman  et al. , 2000 ) from March  1 s t  2016 to May 1 s t  as query proteins and filter them with sequence identity 40%. Furthermore, to avoid potential redundancy between these proteins and the proteins from the SCOP database, we also remove all queries with a sequence identity higher than 40% to any protein in SCOP dataset. Then final query dataset we get has 757 proteins. By searching every query protein against all SCOP protein domains as the database, we report the retrieval performance using the same evaluations as reported above. The final DeepFold model is used by the ensemble of five models trained in the earlier 5-fold cross-validation on SCOP. The results are reported in  Figure 5  and  Table 2 . The curves of SGM and SSEF are not shown in the figure since their accuracies are much worse than DeepFold and FragBag.
 Table 2. Performance comparison on the recently released proteins Algorithm Dimension AUROC AUPRC Top1 Acc Top5 Acc Top10 Acc SGM ( Rogen and Fain, 2003 ) 30 0.698*** 0.048*** 0.0*** 0.0*** 0.003*** SSEF ( Zotenko  et al. , 2006 ) 1500 0.800*** 0.008*** 0.004*** 0.009*** 0.021*** FragBag ( Budowski-Tal  et al. , 2010 ) 400 0.876*** 0.301* 0.450 0.558* 0.587** DeepFold (L) 400 0.925* 0.228** 0.315*** 0.408*** 0.468*** DeepFold 400 0.967 0.375 0.456 0.605 0.663 Note : *denotes  p -value &lt; 5e-3, **denotes  p -value &lt; 1e-5 and ***denotes  p -value &lt; 1e-10. Fig. 5. ROC curves and Hit@Top- k  curves on the recently released proteins Similar to previous results on SCOP data, DeepFold achieves better performance under all evaluation metrics on this independent non-redundant dataset, which means that the learned structural motif in our DeepFold actually can be generalized for newly released proteins that are not included in the training data. As shown in  Figure 5 , our Deepfold outperforms other methods by a significant margin. We also discover the wide margin between our curve and baselines in the hit@top- k  rate. Note that though our DeepFold is only slightly better than FragBag on hit@top-1 accuracy, we are still able to obtain significant improvement on hit@top-5 and hit@top-10, which further demonstrates the effectiveness of DeepFold on protein structure search. The difference of the performance on SCOP and newly release proteins may be caused by different distributions of folds appearing in these two datasets. Computational efficiency For computational efficiency, we evaluate the runtime needed to generate the representations of all 757 protein structures in the recently released structures curated above. On average, FragBag, one of the most efficient fingerprint algorithm before( Budowski-Tal  et al. , 2010 ) took 553.202 s while DeepFold only took 20.128 s. DeepFold is roughly  25 ×  faster than FragBag. We argue that our algorithm is faster because it does not need to perform the expensive local structural alignments between the target protein and fragments in the library. The other reason is that DeepFold fully utilizes the GPU parallelization and the fast CUDA numerical library, even though it has many parameters in the neural network model. On CPUs, DeepFold took 282.801 s, which is still faster. It is worth noting that we tried to implement Fragbag to run on GPUs, but the structure of local structural comparison algorithm makes it tough to fully exploit the parallelism of GPU cores to outperform the implementation running on CPU. 4.2 Why does DeepFold work so well? To interpret the superior performance DeepFold achieves in protein structure search, we analyze the representation learned by the deep neural network model and intend to understand the structural meaning of the representation. To do so, we develop a quantitative approach to visualize the structural motifs learned in this deep neural network. In detail, we first obtain the activation values of filters in each convolutional layer for each protein in a non-redundant protein structure dataset. Then for each filter, we obtain the top proteins with the highest activation values and extract the corresponding protein substructures. These substructures are aligned together, and a central substructure is picked to represent the motif learned by the filter. To visualize structural motifs in a well-organized way, we utilize t-SNE ( Maaten and Hinton, 2008 ) to perform a nonlinear dimension reduction, projecting these motifs into a 2D space.  Figure 6  illustrates the structural motifs we learned in DeepFold, where each point denotes a learned structural motif. The left plot presents all local contiguous structural motifs, while the right one presents long-range interacting motifs. We annotate each point with a specific color indicating the secondary structure composition of the corresponding motif, where blue indicates  α -helix, red indicates  β -strand and the purple one indicates loop. From the left plot, we could observe the local structural motifs include not only  α -helix,  β -strand, but also some mixture of short  α / β  structures. In addition, the long-range structural motifs are mainly  β-β  interacting patterns, under the complicated environment with  β -strand,  α -helix or even loop structure. Furthermore, we observe that the low-level motifs learned by DeepFold can be organized to represent high-level structural motifs by composing local and long-range motifs together. From this visualization, it is clear that DeepFold extracts not only meaningful contiguous backbone fragments but also the long-range interacting motifs, which are entirely ignored in FragBag. It is worth noting that FragBag can be seen as a special case of DeepFold if we only consider the near-diagonal region of the pairwise distance matrix as input. Fig. 6. Structural motifs learned by DeepFold. The left figure shows the local backbone-segment motifs, and the right figure shows the long-range interacting contact motifs. Ten representative motifs are shown with their structures. Each motif is annotated with a specific color indicating the secondary structure composition To compare the structural motifs against the fragment library used by FragBag. We visualize the carbon backbones of the motifs and fragments in  Figure 7 . We observe that the local structural motifs learned by DeepFold are similar to the fragments utilized by FragBag. Also, the DeepFold preserves long-range contacts in the original structures, which contribute to a more powerful structural representation. Fig. 7. Comparison of motifs used by DeepFold and FragBag. The left panel shows fragments used by FragBag. The upper right panel shows local structural motifs, and the bottom right shows long-range contact motifs discovered by DeepFold In addition to visualizing the structural motifs learned by DeepFold, we also check what are the ‘false-positive’ hits found by DeepFold. From the  Figure 8 , we observe that the ‘wrongly’ predicted top structures also share substantial structural similarity with the query proteins, indicating that our DeepFold is potentially capable of finding remotely related structural neighbors which contain similar motif composition. Fig. 8. Some ‘False Positive’ examples discovered by DeepFold. The left panel represents the query structure while the right panel represents some ‘Top False Positive’ structures found by DeepFold 5 Conclusion In this paper, we present DeepFold, a deep-learning approach to building structural motif presentations for better alignment-free protein structure search. We develop DeepFold, a deep convolutional neural network model to extract effective structural patterns of a protein structure from its  C α  trace. Similar to the previous FragBag, we represent each protein structure/fold using a vector of the learned motifs and perform the structure search by only computing vector similarity. This neural network is trained in a supervised manner by discriminating similar template structures from dissimilar structures in a database. We demonstrate that DeepFold greatly outperforms FragBag on protein structure search on SCOP database and a set of newly released PDB structures, regarding both search accuracy and efficiency for computing structural representations. Remarkably, after visualizing the motifs learned by DeepFold, we find that it not only extracts meaningful backbone segments but also identifies important long-range interacting structural motifs for structural comparison. Furthermore, given that the retrieval accuracy of DeepFold is outstanding, we can combine it with structural alignment algorithms in the compressive genomics manner by first applying DeepFold as a ‘coarse search’ step to identify a very small subset of putative similar template structures that are ranked very top, and then performing structural alignment algorithms as a ‘fine search’ step to refine the ranking and obtain more informative residue-level structural alignments for downstream analysis ( Yu  et al. , 2015 ). Finally, we expect that the structural motifs extracted by DeepFold will provide new insights into the evolution and hierarchical organization of protein structure. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>An automated workflow for parallel processing of large multiview SPIM recordings</Title>
    <Doi>10.1093/bioinformatics/btv706</Doi>
    <Authors>Schmied Christopher, Steinbach Peter, Pietzsch Tobias, Preibisch Stephan, Tomancak Pavel</Authors>
    <Abstract>Summary: Selective Plane Illumination Microscopy (SPIM) allows to image developing organisms in 3D at unprecedented temporal resolution over long periods of time. The resulting massive amounts of raw image data requires extensive processing interactively via dedicated graphical user interface (GUI) applications. The consecutive processing steps can be easily automated and the individual time points can be processed independently, which lends itself to trivial parallelization on a high performance computing (HPC) cluster. Here, we introduce an automated workflow for processing large multiview, multichannel, multiillumination time-lapse SPIM data on a single workstation or in parallel on a HPC cluster. The pipeline relies on snakemake to resolve dependencies among consecutive processing steps and can be easily adapted to any cluster environment for processing SPIM data in a fraction of the time required to collect it.</Abstract>
    <Body>1 Introduction The duration and temporal resolution of 3D fluorescent imaging of living biological specimen is limited by the amount of laser light exposure the sample can survive. Selective Plane Illumination Microscopy (SPIM) alleviates this by illuminating only the imaged plane thus reducing photo damage dramatically. Additionally, SPIM achieves fast acquisition rates due to sensitive wide-field detectors and sample rotation enables complete coverage of large, non-transparent specimen. Taken together, SPIM allows imaging of developing organisms  in toto  at single cell resolution with unprecedented temporal resolution over long periods of time ( Huisken  et al. , 2004 ;  Keller  et al. , 2008 ). This powerful technology produces massive, terabyte size datasets that need computationally expensive and time-consuming processing before analysis. Existing software solutions implemented in Fiji ( Preibisch  et al. , 2010 ,  2014 ;  Schmied  et al. , 2014 ; Preibisch, unpublished ( https://github.com/fiji/SPIM_Registration )) or in ZEISS ZEN black are performing chained processing steps on a single computer and require user inputs via a GUI. As the spatial and temporal resolution of the light sheet data increase, such approaches become inconvenient since processing can take days. In controlled experiments, SPIM image processing is robust enough to be automated and key steps are independent from time point to time point. HPC is inherently designed for such time consuming and embarrassingly parallel tasks that require no user interaction. Therefore, we developed an automated workflow with minimum user interaction that is easily scalable to multiple datasets or time points on a cluster. In combination with the appropriate computing resources it enables for the first time processing of SPIM data that is faster than the total acquisition time required for collecting the raw images. 2 Processing workflow The Fiji SPIM processing pipeline uses Hierarchical Data Format (HDF5) as data container for the originally generated TIFF or CZI files by custom made ( Pitrone  et al. , 2013 ) or commercial SPIM microscopes ( Fig. 1A and B ). Following format conversion, multiview registration aligns the different acquisition angles (views) within each time point ( Fig. 1C ), and subsequent time-lapse registration stabilizes the recording over time ( Preibisch  et al. , 2010 ) ( Fig. 1D ). Fusion combines the registered views of one time point into a single volume by averaging or multiview deconvolution ( Preibisch  et al. , 2010 ,  2014 ) ( Fig. 1E and F ). The result is a set of HDF5 files containing registered and fused multiview SPIM data that can be examined locally or remotely using the BigDataViewer ( Pietzsch  et al. , 2015 ).
 Fig. 1.  Automated workflow for multiview processing. Workflow for SPIM image processing ( A – E ) using parallelization ( B, C and E ). Shown on the right yz slices in the BigDataViewer of a  Drosophila  embryo expressing histone H2Av-mRFPruby raw ( A ) registered ( C ) and deconvolved ( E ). Results of deconvolution with xy , xz and xz slices through the fused volume of the same embryo ( F ). Scale bars represent 50 μm 
 All steps are implemented as plugins ( Preibisch  et al. , 2010 ,  2014 ;  Pietzsch  et al. , 2015 ; Preibisch, unpublished ( https://github.com/fiji/SPIM_Registration )), in the open-source platform Fiji ( Schindelin  et al. , 2012 ). We use these plugins by executing them from the command line as Fiji beanshell scripts ( Supplementary Fig. 1 ). To overcome the legacy dependency of Fiji on the GUI we encapsulate it in a  virtual framebuffer  ( xvfb ) that simulates a monitor in the headless cluster environment ( Supplementary Fig. 1 ). To map and dispatch the workflow logic to a single workstation or on a HPC cluster, we use the automated workflow engine  snakemake  ( Köster and Rahmann, 2012 ). The workflow is defined using a  Snakefile  containing the name, input and output file names of each of the processing steps and python code calling the  beanshell scripts  ( Supplementary Fig. 1 ). Upon invocation, the  snakemake  rule engine resolves the dependencies between individual processing steps based on the input files required and the output files produced during the workflow. It also creates the command that fits the input/output rule description and the template command as defined in the  Snakefile . Most importantly, if single tasks on individual files are discovered to be independent, they are invoked in parallel ( Supplementary Fig. 2 ). Each instance of  snakemake  for one dataset is independent and thus the workflow can be applied simultaneously to multiple dataset. The required parameters for processing are collected by the user during GUI processing of an exemplary time point and entered into a . yaml  configuration file ( Supplementary List 1 ). The workflow is executed by passing the . yaml  file to  snakemake  on the command line ( Supplementary Fig. 1 ). Importantly, from the user perspective the launching of the pipeline on a HPC cluster and on a local workstation appears identical and require a single command ( Supplementary List 2 ). If the parameters are chosen correctly and the local or HPC resources are sufficient ( Supplementary Table 1  and  2 ) no further action from the user is necessary. Snakemake  supports multiple back ends to perform the command dispatch: local, cluster and  Distributed Resource Management Application API  ( DRMAA ) ( Köster and Rahmann, 2012 ). The local back end creates a new sub shell and calls the command(s) required. The cluster back end is a general interface to HPC batch systems based on string substitution.  DRMAA  specifies a system library that interfaces all common batch systems based on a generalized task model, thus multiple batch systems are supported through one interface. 3 Results We compared the performance of the pipeline on a 175 GB, single channel SPIM recording of a  Drosophila  embryo consisting of 90 time points and 5 views, processed either on a single computer or on a HPC cluster ( Supplementary Table 1 ). The processing using average fusion takes almost precisely one day on a single powerful computer. In contrast, using the full cluster resource the dataset can be processed in 1 h 31 min, which represents a 16-fold speedup in processing. Since the time-lapse covers 23 h of  Drosophila  embryonic development the processing becomes real time with respect to the acquisition. Using deconvolution on a cluster with only 4 GPUs ( Supplementary Table 1 ) still brings a more than 3-fold speed up ( Supplementary Table 3 ). A dataset of 2.2 TB in size with 715 time points ( Schmied  et al. , 2014 ) would take an estimated week to process on a single computer. Using this method, the processing is reduced to only 13 h with typical cluster workload from other users. 4 Conclusion and outlook The biologist‘s goal is to analyze, for instance, cellular behavior using time-lapse SPIM recordings. The steps between data acquisition and analysis are of rather technical interest. Our pipeline leverages HPC to reduce the notoriously difficult and time-consuming SPIM data processing to a single autonomous command. Similar pipelines have been developed ( Amat  et al. , 2015 ), however in our case the reliance on an open source platform (Fiji) allows us to execute the processing in parallel without any software associated costs. It is also possible to incorporate new algorithms from the Fiji ecosystem into the pipeline ( Schmid and Huisken, 2015  and see  Supplementary Note ). Future improvements of the workflow will provide greater accessibility to novice users by using the UNICORE GUI framework ( Almond and Snelling, 1999 ). Ultimately, we aim for a completely unsupervised automated processing similar to grid computing practiced in fields facing similar big data challenges such as particle physics and molecular simulation ( Bird, 2011 ;  Gesing  et al. , 2012 ) 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Estimating the predictability of cancer evolution</Title>
    <Doi>10.1093/bioinformatics/btz332</Doi>
    <Authors>Hosseini Sayed-Rzgar, Diaz-Uriarte Ramon, Markowetz Florian, Beerenwinkel Niko</Authors>
    <Abstract/>
    <Body>1 Introduction Dissecting the relative contribution of stochastic versus deterministic forces in shaping the outcome of evolutionary processes is a long-standing question of both experimental and theoretical research in evolutionary biology ( Blount  et al. , 2018 ;  Gould, 1990 ;  Lobkovsky and Koonin, 2012 ;  Orgogozo, 2015 ). While stochastic forces (e.g. genetic drift) allow evolution to take place in an undirected manner, deterministic forces (e.g. natural selection) can impose constraints on the potential evolutionary trajectories. Stephen Jay Gould highlighted the problem of chance and necessity by devising the metaphor of ‘replaying the tape of life’. He concluded that the outcome of evolution at large is not likely to be repeatable, because many equally likely evolutionary trajectories may exist ( Gould, 1990 ). However, recent technological advancements in experimental evolution, high-throughput sequencing and modeling of complex biological systems have revealed some repeatable features in diverse evolutionary processes and pervasive evolutionary constraints in various biological systems ( Achaz, 2014 ;  Blount  et al. , 2018 ;  Ferretti  et al. , 2018 ;  Hosseini and Wagner, 2017 ;  Lieberman  et al. , 2011 ;  Miles  et al. , 2011 ;  Poelwijk  et al. , 2007 ;  Salverda  et al. , 2011 ;  Toprak  et al. , 2012 ;  Weinreich  et al. , 2006 ). These repeatable patterns and regularities suggest a predictive theory of evolution, which has been pioneered by studies attempting to predict the future of evolution in various biological systems ( Barton  et al. , 2016 ;  Bull and Molineux, 2008 ;  Cowperthwaite  et al. , 2008 ;  Luksza and Lässig, 2014 ;  Neher  et al. , 2014 ;  Nyerges  et al. , 2018 ). Thus, beyond reconstructing evolutionary history of the past, the task of predicting future outcomes of evolutionary processes has emerged in computational evolutionary biology ( Lässig  et al. , 2017 ). Predictability is tightly linked with controllability ( Fischer  et al. , 2015 ;  Lässig  et al. , 2017 ). Once we can predict the outcome of evolution, we will be able to design specific intervention strategies and manipulate biological systems towards our desired goals. This is where fundamental principles of evolutionary biology come into play in biomedical research, particularly in the diagnosis and treatment of diseases with evolutionary nature such as cancer. Cancer progression can be regarded as an evolutionary process, which is caused by step-wise accumulation of selectively advantageous mutations ( Beerenwinkel  et al. , 2016 ;  Nowell, 1976 ). Like all evolutionary processes, cancer progression is the outcome of events driven by a mixture of both stochastic and deterministic forces. On the one hand, because of the extensive inter- and intra-patient heterogeneity of cancer-associated mutations, the genetic progression of cancer seems to be an unpredictable evolutionary process ( Burrell  et al. , 2013 ;  Lipinski  et al. , 2016 ;  Marusyk and Polyak, 2010 ). On the other hand, a growing body of evidence attests to the predictability of cancer evolution. For example, only a minor fraction of mutations, called drivers, contributes to the malignancy of cancer, while most are passenger mutations with no phenotypic effects ( Vogelstein  et al. , 2013 ), such that, typically, only a handful of genes are frequently mutated among patients with a given cancer type ( Lawrence  et al. , 2013 ;  Vogelstein  et al. , 2013 ). Moreover, the pervasive constraints in the temporal ordering of tumorigenic mutations ( Bagcchi, 2015 ;  Fisher  et al. , 2014 ;  Kent and Green, 2017 ;  Martins  et al. , 2012 ;  Ortmann  et al. , 2015 ), and the repeatability of evolutionary trajectories during cancer progression ( Caravagna  et al. , 2018 ) suggest the predictability of cancer evolution. Nevertheless, these anecdotal examples and sporadic reports on heterogeneity or repeatability are not sufficient for a systematic insight into the extent of evolutionary predictability of different cancer types. Instead, a rigorous quantitative framework is needed for this purpose ( Linnen, 2018 ). Various attempts using different approaches have been made to gain quantitative insights into the predictability of evolution in general ( de Visser and Krug, 2014 ). Whereas experimentally, evolutionary predictability is assessed as the fraction of identical outcomes in replicate evolutionary experiments ( Blount  et al. , 2018 ;  Tenaillon  et al. , 2012 ;  Woods  et al. , 2006 ), theoretical studies of predictability analyze the probabilities of mutational pathways on a given fitness landscape ( de Visser and Krug, 2014 ). A common model is the strong selection and weak mutation rate (SSWM) assumption ( Gillespie, 1983 ;  Orr, 2005 ), which implies successive clonal expansions driven by selectively advantageous mutations. The SSWM assumption allows for computing mutational pathway probabilities based on the fixation probabilities of the mutations ( Weinreich  et al. , 2006 ). It is widely used for analyzing fitness landscapes ( de Visser and Krug, 2014 ;  Weinreich  et al. , 2005 ) and its validity has been confirmed by experimental evolution studies ( Poelwijk  et al. , 2007 ;  Weinreich  et al. , 2006 ). The predictability of evolution is minimal if all mutational pathways are all equally likely ( Fig. 1a ). In contrast, non-uniform distributions of mutation trajectories bias evolution towards specific directions and increase the predictability of evolution ( Fig. 1b and c ). The extent of the non-uniformity can be quantified by the entropy of the pathway probability distribution ( Szendro  et al. , 2013 ).
 Fig. 1. Fitness landscapes and CBN. Upper panels show schematic representations of three different fitness landscapes each including four mutations. Each vertex corresponds to a genotype, which is represented as a binary string and is color-coded according to its fitness. Each fitness landscape is arranged in five columns and each column contains all genotypes with the same number of mutations. The leftmost and the rightmost columns correspond, respectively, to the wild-type and the fully mutated genotype. There is an edge between a pair of genotypes if they differ in exactly one fitness-increasing mutation. A mutational pathway is comprised of a set of edges that connect the wild-type to the genotype with the highest fitness (i.e. the fully mutated one). In panel ( a ), all genotypes belonging to the same column have the same fitness and fitness increases monotonically from left to right. In this fitness landscape, all 4!=24 potential pathways are accessible with equal probability (minimum predictability). In panel ( b ), not all genotypes belonging to the same column have the same fitness, such that evolutionary trajectories are restricted: only five pathways are accessible with different probability (shown as different edge thickness) (intermediate predictability). In panel ( c ), only a single mutational pathway is accessible and predictability is maximal. Each network on the bottom (with green vertices and labeled by the mutation) represents a CBN, whose DAG encodes the order constraints. An edge  a → b  in the DAG means that mutation  a  must occur prior to mutation  b . The graphs on the top are exactly the genotype lattices of the corresponding CBN models on the bottom In practice, however, defining evolutionary predictability based on empirical fitness landscapes is usually unfeasible, because of the high costs associated with experimentally determining the fitness of all possible genotypes ( de Visser and Krug, 2014 ). Moreover, measuring fitness landscapes  in vivo  is impossible and  in vitro  systems have their own limitations. Furthermore, inferring fitness landscapes from genomic data, especially for cancer is also extremely challenging. Although in other systems, such as, e.g. HIV under the strong assumption of equilibrium distribution of the quasispecies model, systematic inference of fitness landscapes has become possible ( Seifert  et al. , 2015 ), for cancer similar attempts are so far limited to small-scale studies, such as estimating the fitness effects of single mutations (e.g. BCR-ABL in chronic myeloid leukemia) ( Traulsen  et al. , 2010 ). Therefore, for quantifying evolutionary predictability of cancer, it is necessary to define an alternative framework, which operates independently of fitness landscapes, and is based solely on cross-sectional mutational patterns, which are abundant. Here, we assess whether cancer progression models, such as conjunctive Bayesian network (CBN) ( Beerenwinkel and Sullivant, 2009 ;  Gerstung  et al. , 2009 ), CAncer PRogression Inference (CAPRI) ( Ramazzotti  et al. , 2015 ) or Oncogenetic Tree (OT) ( Szabo and Boucher, 2008 ), which are probabilistic graphical models used for describing the constraints in the ordering of mutation accumulation events, can be used to quantify the predictability of cancer evolution. Among cancer progression models, CBNs are particularly appropriate for this purpose, because they allow for inferring both elements of predictability directly from mutational data, namely (i) the constraints on evolutionary trajectories encoded in the inferred network structure and (ii) the distribution of pathway probabilities derived from the local probability distributions of the CBN model ( Beerenwinkel and Sullivant, 2009 ). Therefore, we employ CBNs in this study for estimating mutational pathway probabilities. In order to systematically assess the validity of the CBN model for quantifying the predictability of cancer evolution, it is essential to establish that the CBN-based constraints on the ordering of mutations approximate well those based on the underlying fitness landscape and the SSWM assumption ( Fig. 1 ). Leveraging the simulated data of previous work ( Diaz-Uriarte, 2018 ), which has made a connection between CBNs and fitness landscapes, here we quantify the predictability of cancer progression (i) based on fitness landscapes under the SSWM assumption (as the ground truth) and (ii) by applying the CBN model directly to simulated genotypes (our novel CBN-based framework). We show that the collections of feasible evolutionary pathways derived by the two approaches correlate strongly, implying that CBNs can be used to approximate the SSWM- and fitness landscape-based notion of evolutionary predictability, thus offering an alternative way to quantify the predictability of cancer progression that does not require any knowledge of the fitness landscape. Using our robust and scalable CBN-based framework, we systematically compare the predictability of up to 15 different cancer types using TCGA ( Cancer Genome Atlas Research Network et al., 2013 ) and MSK-IMPACT ( Zehir  et al. , 2017 ) data. 2 Materials and methods 2.1 Conjunctive Bayesian networks CBNs are probabilistic graphical models that describe constraints on the ordering of mutations, which occur during mutation accumulation processes such as tumorigenesis ( Beerenwinkel  et al. , 2007 ). A CBN is defined by a set  E = { 1 , … , n }  of  n  mutational events and a partial order ‘ ⪯ ’ on  E . For  i , j ∈ E , we write  i ≺ j  if  i ⪯ j  and  i ≠ j . We represent the partially ordered set, or poset,  ( E , ⪯ )  by its Hasse diagram, the directed acyclic graph (DAG) with vertices  E  and edges ( i ,  j ) for all relations  i ≺ j , such that no  k ∈ E  exists with  i ≺ k ≺ j  ( Fig. 1 , bottom). The genotype lattice  G  is the set of all genotypes compatible with the partial order on  E  ( Beerenwinkel  et al. , 2006 ). It is defined as the set of order ideals, i.e. the subsets  g ⊆ E  for which  j ∈ g  and  i ≺ j  implies  i ∈ g . We identify a genotype  g ∈ G  with the binary string indicating the occurrences of all mutations in  g , e.g. for  n  = 5,  g = { 2 , 3 , 5 }  corresponds to 01101. The genotype lattice is represented by the DAG with vertices  G  and edges  ( g , h )  for all  g , h ∈ G  with  | h ∖ g | = 1  ( Fig. 1 , top). It defines the state space of the evolutionary process and is a subset of the genotype universe, the  n -dimensional hypercube  U = { 0 , 1 } n . In continuous-time CBNs (CT-CBNs), the waiting time for mutation  i ∈ E  to occur is the random variable  T i , defined recursively as
 (1) T i = max j ∈ pa ( i ) T j + Z i 
where  pa ( i )  denotes the set of parents of  i  in the Hasse diagram and  Z i ∼   Exp ( λ i ) ,   i = 1 , … , n  are independent exponentially distributed random variables with rates  λ = ( λ 1 , … , λ n )  ( Beerenwinkel and Sullivant, 2009 ).  Equation (1)  reflects the order constraints of  E : mutation  i  can occur only after all parent mutations  j ∈ pa ( i )  have occurred. The occurrence times  T 1 , … , T n  of mutations are unknown and instead genotypes  G  are observed after a certain random sampling time  T s ∼   Exp ( λ s ) . Mutation  i  is then observed if  T i  &lt;  T s . The CT-CBN model is the model for  G  defined as the marginalization with respect to the waiting times  T 1 , …,  T n ,  T s . The hidden CBN (H-CBN) extends the CT-CBN by additionally allowing observation errors with probability  ε  independently for each mutation, such that the true genotypes become hidden random variables. We performed maximum likelihood inference of the poset structure and the parameters  λ  and  ε  of the H-CBN based on simulated annealing and expectation maximization as described previously ( Gerstung  et al. , 2009 ). For simulated annealing, we used temperature  T  =   1, and as initial poset the DAG inferred for the CT-CBN model with fixed error rate of 0.05 ( Beerenwinkel and Sullivant, 2009 ). We varied the number of simulated annealing steps depending on the number ( n ) of mutations (i.e. for  n ≤ 4 , 100 steps, for  n  =   5, 1000 steps and for  n ≥ 6 , 10 000 steps). 2.2 Mutational pathways A mutational pathway in  U = { 0 , 1 } n  of length  n  is a permutation  π = ( π 1 , … , π n ) ∈ S n , such that mutation  π 1  occurs first,  π 2  second, etc. Equivalently, the mutational pathway  π  is given by the ordered list of  n  +   1 genotypes  g ( π ) = ( g 0 , g 1 , … , g n ) , where  g ( π ) i = ∪ j = 1 i π j , i.e. the genotypes successively accumulating the mutations in  π . For a poset  ( E , ⪯ )  with genotype lattice  G , the mutational pathways in  G  are exactly the linear extensions of the poset, i.e. the total mutation orders that respect the partial order. For example, in  Figure 1b ,  ( 2 , 1 , 4 , 3 ) ∈ S 4  defines a mutational pathway compatible with the order constraints. It is equivalently represented by the ordered genotypes (0000, 0100, 1100, 1101, 1111). Let Π be a collection of mutational pathways in  U . The exit set of a genotype  g  is the set of all genotypes that can be reached from  g  by acquiring one additional mutation along any of the pathways in Π,
 (2) Exit Π ( g ) = { h ∈ U | ∃ π ∈ Π : g , h ⊆ g ( π )   and   | h ∖ g | = 1 } For example, the CBN model in  Figure 1b  defines the mutational pathways  Π ⪯ = { ( 1 , 2 , 3 , 4 ) ,   ( 1 , 2 , 4 , 3 ) ,   ( 2 , 1 , 3 , 4 ) ,   ( 2 , 1 , 4 , 3 ) , ( 1 , 3 , 2 , 4 ) }  and  Exit Π ⪯ ( 1100 ) = { 111 0 ,   11 0 1 } . Each mutational pathway, at each step, realizes exactly one of the options recorded in the exit set. Let  P ( π ) be a probability distribution over a set of mutational pathways Π. As in  Szendro  et al.  (2013) , its entropy is
 (3) H Π = − ∑ π ∈ Π P ( π )   log   P ( π ) We define the predictability of an evolutionary process described by the pathway distribution  ( P ( π ) ) π ∈ Π  as
 (4) ϕ Π = 1 − H Π H max 
where the maximal entropy  H max = log ( n ! )  is attained when all  | S n | = n !  pathways have the same probability. We have  0 ≤ ϕ Π ≤ 1  with  ϕ Π = 0  indicating no predictability (all mutational pathways have the same probability) and  ϕ Π  = 1 indicating maximal predictability (one pathway is taken with probability 1). 2.3 Evolutionary predictability in the fitness landscape-based SSWM model A fitness landscape is a mapping  w : U → R  that assigns to each genotype  g  its fitness  w g . For a mutational pathway  π , we define the selective coefficient of mutation  π i  as the fitness difference
 (5) s π , i = w g ( π ) i − w g ( π ) i − 1 
that it causes along the mutational pathway. In the SSWM regime ( Gillespie, 1983 ;  Orr, 2005 ), mutations are fixed sequentially in a population, resulting in a multi-step evolutionary process along mutational pathways. The probability of a mutational pathway is the product of the fixation probabilities of mutations in each of the  n  steps, where the fixation probability of each beneficial mutation is proportional to its selective coefficient ( Kimura, 1962 ). Under the SSWM assumption, a mutational pathway is accessible if the fitness of its genotypes is monotonically increasing along the pathway. Thus, we define the set of all accessible pathways as
 (6) Π w = { π ∈ S n | s π , i &gt; 0   for   all   i = 1 , … , n } With  Exit w = Exit Π w , the probability of a mutational pathway is
 (7) P ( π ) = 1 C ∏ i = 1 n s π , i ∑ h ∈ Exit w ( g ( π ) i ) w h − w g ( π ) i − 1 
if  π ∈ Π w  and zero otherwise ( Weinreich  et al. , 2006 ), where  C  is the normalizing constant defined as follows:
 (8) C = ∑ π ∈ Π w ∏ i = 1 n s π , i ∑ h ∈ Exit w ( g ( π ) i ) w h − w g ( π ) i − 1 The evolutionary predictability in the fitness landscape-based SSWM model is then  ϕ w = ϕ Π w  ( Equations (4)  and  (3) ), with  P ( π ) given by  Equation (7) . 2.4 Evolutionary predictability in the CBN model We now derive another notion of evolutionary predictability that does not require a fitness landscape, but is based only on genotype data. We assume that a CBN model  ( E , ⪯ )  with genotype lattice  G  and waiting time parameters  λ  has been learned from genotype data (Section 2.1). The feasible mutational pathways in  G  are the linear extensions of the poset,
 (9) Π ⪯ = { π ∈ S n | π i ⪯ π j   for   all   i ≤ j } To compute the probability of a pathway  π , we consider the waiting time process ( Equation (1) ). At each step  i , all possible one-step extensions of  g ( π ) i  are recorded in its exit set. Thus, the pathway probability is given by the product of competing exponentials,
 (10) P ( π ) = ∏ i = 1 n λ π i ∑ h ∈ Exit ⪯ ( g ( π ) i ) λ h ∖ g ( π ) i 
if  π ∈ Π ⪯  and zero otherwise. For each step  i ,  h ∖ g ( π ) i  has cardinality 1 and consists of the possible additional mutation. We have used the abbreviation  Exit ⪯ = Exit Π ⪯ . Note that the above equation does not need to be normalized, because the following equality always holds:
 (11) ∑ π ∈ Π ⪯ ∏ i = 1 n λ π i ∑ h ∈ Exit ⪯ ( g ( π ) i ) λ h ∖ g ( π ) i = 1 The evolutionary predictability in the CBN model is  ϕ ⪯ = ϕ Π ⪯  ( Equation (4) ). The above procedure, however, requires modification to be widely applicable: for large numbers  n  of mutations it becomes increasingly difficult to estimate the CBN model  ( E , ⪯ )  both statistically and computationally. Although the uncertainty in the genotype lattice may be high in this situation, this need not necessarily be the case for the evolutionary predictability. For large  n , we fix a smaller number  n ′ &lt; n , such that CBN learning is feasible on  n ′  mutations, and approximate the evolutionary predictability by averaging over all subsets of mutations of size  n ′ ,
 (12) ϕ ⪯ ≈ 1 ( n n ′ ) ∑ E ′ ⊂ E | E ′ | = n ′ ϕ ⪯ ′ 2.5 Simulated data We leveraged the simulated data of a previous study ( Diaz-Uriarte, 2018 ) both for generating random fitness landscapes and for producing genotypes from evolutionary simulations. For the fitness landscape-based approach (Section 2.3), we used 100 representable and 111 non-representable fitness landscapes, where a landscape is called representable if its support is the genotype lattice of a CBN. Both types of fitness landscapes are derived from an initial DAG of restrictions, and the genotypes are binary vectors of length 7, defined based on the presence or absence of beneficial mutations in seven genes, resulting in 128 distinct genotypes. The fitness to each genotype is assigned based on the restrictions imposed by the DAG and the fitness effects of each individual mutation. If a genotype is not accessible according to the given DAG, its fitness will be zero; otherwise its fitness will be determined based on the set of mutations it contains. In any given fitness landscape, the fitness of the wild-type genotype is 1 and the fitness of accessible genotypes with a single mutation is 1 +  s , where  s  is the fitness effect of the mutation and is chosen from a uniform distribution between 0.1 and 0.7. More generally, the fitness of accessible genotypes with multiple mutations is  ∏ ( i = 1 ) j ( 1 + s i ) , where  s i  is the fitness effect (i.e. selection coefficient) of the  i th  mutation and  j  is the total number of mutated genes in the given genotype. This way of fitness assignment, which is implemented in the representable fitness landscapes, ensures that there will be no reciprocal sign epistasis in the landscape. To introduce reciprocal sign epistasis, in the second type of fitness landscapes (non-representable ones), synthetic lethals or holes are introduced into the landscape by assigning a randomly chosen subset of (accessible) genotypes with two or more mutations to 0.2, which makes the accessibility of the chosen genotypes very unlikely. Using this approach, in the previous study 100 representable and 200 non-representable fitness landscapes has been constructed ( Diaz-Uriarte, 2018 ). Because our analyses are based on the SSWM assumption, and we assume cancer progression as a mutational pathway from the wild-type genotype towards the fully mutated one, we required the fitness landscapes to assign the highest fitness to the fully mutated genotype (as the sole global peak). Moreover, we required that the genotype with the highest fitness to be connected to the wild-type genotype by at least one accessible mutational pathway. All 100 representable and 111 out of the 200 non-representable fitness landscapes fulfilled these requirements, so we used them in this study. Note that in the non-representable fitness landscapes, the fully mutated genotype is the genotype with highest fitness, but it is still possible to see multiple local fitness peaks, which accounts for the ruggedness of the fitness landscape. Moreover, in the previous study ( Diaz-Uriarte, 2018 ) based on an evolutionary model ( McFarland  et al. , 2013 ) implemented in the OncoSimulR package ( Diaz-Uriarte, 2017 ), from each fitness landscape, under different mutation rates (high: 10 – 5  and low: 10 – 6 ) and detection regimes (slow and fast), 20 000 genotypes were generated ( Diaz-Uriarte, 2018 ), which we used for our CBN-based predictability estimation (Section 2.4 and  Supplementary Text S1 ). 2.6 Real data We used cancer genomic data from two distinct sources, referred to as TCGA and MSK-IMPACT, which were collected differently. Whereas TCGA includes samples from primary tumors of untreated patients (Cancer Genome Atlas Research Network et al., 2013), MSK-IMPACT is comprised of sequence data from patients with metastatic cancer under treatment at Memorial Sloan Kettering Cancer Center, 43% of which were obtained from metastatic sites, most commonly liver, lymph node and bone ( Zehir  et al. , 2017 ). We gained access to these datasets through the cBioPortal platform ( Gao  et al. , 2013 ). We used 15 distinct cancer types in our analyses. The number of samples varies based on cancer type and data source from 186 to 836 in TCGA and from 93 to 1357 in MSK-IMPACT ( Supplementary Table S1 ). We determined the genotype of each tumor using a given number  n ≤ 20  of most frequently mutated driver genes ( Supplementary Table S2 ) predicted by Mutsig2CV v3.1, which is a significantly mutated gene-based method that adjusts for known covariates of mutation rates ( Lawrence  et al. , 2013 ). We exclusively focused on 15 cancer types that are (i) frequent enough in both datasets and (ii) are included in the Broad Institute TCGA GDAC Firehose ( http://gdac.broadinstitute.org/ ), where we obtained the driver gene information. 3 Results 3.1 Evolutionary predictability: fitness landscape-based SSWM model versus CBN-based model We first compared the predictability of evolution as quantified either by a fitness landscape  w  using the SSWM assumption or by a CBN model  ( E , ⪯ )  learned from genotype data collected during evolution on the landscape  w . That is, we asked whether  ϕ w ≈ ϕ ⪯ . We used four different simulation conditions (two different mutation rates and two different detection regimes), in 100 representable and 111 non-representable fitness landscapes. For each fitness landscape and each condition, we computed  ϕ w  and learned a CBN model from 20 000 simulated genotypes to compute  ϕ ⪯ . Each genotype is a binary vector of length seven indicating the occurrence of seven different mutations (see Section 2 for more details). We found a strong correlation between  ϕ ⪯  and  ϕ w  in both types of fitness landscapes under low mutation rate and slow detection regime ( Fig. 2 ; Pearson’s  R =0.92,  P  &lt; 10 – 43  in representable fitness landscapes and  R  = 0.86,  P  &lt; 10 – 33  in non-representable ones). Thus, the congruence between the two methods is not limited to representable fitness landscapes, but it also holds for non-representable ones, where pervasive reciprocal sign epistasis causes the fitness landscape to be rugged. However, under high mutation rates (10 – 5 ) or fast detection regimes, where the SSWM assumption is only weakly respected,  ϕ ⪯  starts to deviate from  ϕ w  ( Supplementary Fig. S1 ).
 Fig. 2. Strong correlation between CBN-based and fitness landscape-based quantification of evolutionary predictability. Panels ( a ) and ( b ), respectively, correspond to representable and non-representable fitness landscapes. Each point corresponds to a fitness landscape. The black lines are the identity lines, and the blue lines are the linear regression models surrounded by a shaded confidence interval region. The used genotypes are the outcomes of evolutionary simulations with slow detection and low mutation rate Next, we compared the pathway probability distributions as estimated from the CBN,  P ⪯ ( π )  ( Equation (10) ), to those computed from the fitness landscape directly under the SSWM assumption,  P w ( π )  ( Equation (7) ).  Figure 3  shows that the Jensen–Shannon divergence between the two distributions is smaller than 0.25 for most fitness landscapes (with median 0.045 in representable and 0.146 in non-representable ones), which is strikingly smaller than that of the baseline comparisons to the empty CBN, which assumes independence of mutations (with median 0.318 in representable and 0.432 in non-representable ones) and to the uniform distribution of pathways (with median 0.433 in representable and 0.487 in non-representable ones). These observations highlight the importance of the inferred DAG of restrictions by the CBN model. We also found that departure from the SSWM assumption, e.g. in simulation conditions with fast detection and high mutation rates, increases the divergence between the two distributions ( Supplementary Fig. S2 ). Additional analyses using different metrics further confirmed the similarity between the two approaches ( Supplementary Text S2  and  Figs S3–S7 ). Thus, under the SSWM assumption, the CBN-based approach reliably quantifies the predictability of evolution directly from genotypic data alone.
 Fig. 3. Similarity of the CBN model-based and fitness landscape-based pathway probability distributions. Displayed is the Jensen–Shannon divergence (where a value of 0 denotes the distributions are identical and a value of 1 that distributions do not overlap) between the pathway probability distributions of the fitness landscape approach,  P ( π w ) ( Equation (7) ), and that of the CBN-based approach,  P ( π ⪯ )  ( Equation (10) ), (blue boxes), the empty CBN model (white boxes) and the uniform pathway probability distribution (black boxes) in ( a ) 100 representable and ( b ) 111 non-representable fitness landscapes in the slow detection and low mutation rate condition. Boxes span the two middle quartiles, and whiskers indicate maxima and minima 3.2 Scalability and robustness In order to apply our framework to real cancer genomics data, we next explore the scalability and robustness of estimating  ϕ ⪯ . In our simulations, we had fixed number of genes to  n  = 7, which resulted in 2 7  = 128 possible genotypes and 7! = 5040 mutational pathways. However, the number of driver genes, which are frequently mutated among cancer patients can reach up to 20 resulting in more than a million genotypes and 10 18  distinct mutational pathways, which renders the quantification of  ϕ ⪯  unfeasible. Moreover, in our simulations, we had a large sample size of  N  = 20 000 genotypes, which could mask the potential variability in the estimation of  ϕ ⪯ , as current real datasets are often on the order of 100 to 1000 genotypes. Since the structure learning of CBNs relies on simulated annealing, by increasing the number of genes,  n , the search space grows exponentially in  n , which can lead to increased variability in the estimation of  ϕ ⪯ . We confirmed this by a bootstrap analysis (see  Supplementary Text S3 and Fig. S8 ). To address the challenges of both scalability and robustness, we consider the approximation of  ϕ ⪯  in  Equation (12)  obtained from averaging over all mutation subsets of fixed size  n ′ &lt; n . The idea is to choose  n ′  such that CBN inference becomes reliable for the given amount of data. Rather than trying to assemble the subnetworks into a global model, a common strategy in network inference, we aggregate on the level of predictability motivated by the multiplicative structure of the pathway probabilities ( Equation (10) ). If all consecutive waiting time rates  λ i  differ by the same factor, i.e.  λ i + 1 / λ i  is constant for all  i , then the approximation  ϕ ⪯ ′  ( Equation (12) ) is almost exactly the same as  ϕ ⪯  ( Supplementary Text S4  and  Figs S9 and S10 ). This approximation is indeed also valid for the simulated data to a great extent ( Supplementary Text S5  and  Fig. S11 ). Moreover, in real data including 15 cancer types from TCGA and MSK-IMPACT, where the sample size is substantially smaller than in the simulated data, this approximation still holds strongly ( Supplementary Text S6  and  Figs S12 and S13 ). Based on a bootstrap analysis of the real data, we showed that the approximate formula considerably reduces the variability of the estimated predictability ( Supplementary Text S7  and  Fig. S14 ), and thus it facilitates not only a scalable but also a robust quantification of predictability. 3.3 Cancer progression is remarkably predictable We employed our framework of evolutionary predictability and used  Equation (12)  to estimate  ϕ ⪯  from two real cancer genomics datasets, namely TCGA and MSK-IMPACT (Section 2.6), in order to address our central question on the predictability of cancer evolution and to systematically compare its extent in different cancer types. As  ϕ ⪯  is robust w.r.t.  n ′  ( Supplementary Fig. S13 ), we kept it constant at  n ′ = 4 , but systematically varied  n  from 4 to 20 to assess how  ϕ ⪯  varies as a function of the number of (predicted) driver genes in different cancer types. We observed that although from  n  = 4 to  n  = 10, different cancer types show different trends, from  n  = 10 upwards,  ϕ ⪯  levels off and remains almost constant in all cancer types and datasets ( Supplementary Fig. S15 ). Indeed, the absolute difference between  ϕ ⪯  of consecutive  n ,  | ϕ ⪯ ( n ) − ϕ ⪯ ( n − 1 ) | , for  n ≥ 10 , becomes negligible in both datasets ( Supplementary Fig. S16 ). Moreover, based on a leave-one-out sensitivity analysis, we found that for  n  = 10,  ϕ ⪯  is robust to removal of any driver gene ( Supplementary Text S8  and  Fig. S17 ), such that undetected drivers are unlikely to confound the analysis. We fixed  n  = 10 for all subsequent analyses. Comparing across cancer types in TCGA, we found that predictability of cancer evolution is generally high, but varies considerably, from 0.36 in stomach adenocarcinoma to 0.82 in pancreatic adenocarcinoma ( Fig. 4 ). To further illustrate the extent of predictability and its diversity across cancer types, we use the fact that  ϕ ⪯  is approximately proportional, on a logarithmic scale, to the fraction  α  of feasible, i.e. non-zero probability, pathways ( Supplementary Text S9  and  Fig. S18 ). Indeed, only a tiny fraction of mutational pathways is feasible. Even in the least predictable cancer type with  ϕ ⪯ = 0.36 , only  α = 0.4 %  of the pathways are accessible, while for pancreatic adenocarcinoma,  α = 0.0004 % , which is 1000 times smaller than for stomach adenocarcinoma. Furthermore, we observe that  ϕ ⪯  for the MSK-IMPACT data, which was collected from patients with metastatic tumors, is on average higher than for TCGA ( P =0.032, Mann–Whitney  U  test). Whereas in seven cancer types,  ϕ ⪯  is almost the same in both MSK-IMPACT and TCGA datasets, for eight other cancer types, particularly for cancer types with lower  ϕ ⪯  in the TCGA data, the evolutionary predictability is substantially higher in MSK-IMPACT as compared to TCGA ( Fig. 4 ).
 Fig. 4. Comparison of evolutionary predictability among different cancer types. The vertical axis shows the CBN-based predictability  ϕ ⪯ , computed based on  Equation (12)  for each given cancer type. The error bars indicate the standard deviation of  ϕ ⪯  calculated from 100 bootstrap samples of equal size as the original genotype data. The genotypes for each cancer type are defined based on the mutational data of the corresponding  n =10 most frequently mutated driver genes from TCGA (blue bars) or MSK-IMPACT (red bars). The cancer types are arranged from left to right in ascending order of their  ϕ ⪯  quantified based on TCGA data 3.4 Predictability, mutation frequency and intra-tumor heterogeneity Next, we compared the evolutionary predictability of cancer types with other observable evolutionary traces, namely mutational load and intra-tumor genetic heterogeneity. Both of these parameters indicate lack of predictability and hence are expected to correlate negatively with  ϕ ⪯ . We found that evolutionary predictability of cancer types in TCGA is indeed significantly anti-correlated with the average mutation rate measured by analyzing &gt;3000 samples ( Fig. 5a ) ( Lawrence  et al. , 2013 ). Similarly, our analysis revealed a significant negative correlation between predictability and intra-tumor heterogeneity based on a recent comprehensive pan-cancer inference of intra-tumor genetic heterogeneity ( Raynaud  et al. , 2018 ) ( Fig. 5b ). The results of this study further corroborated our expectation that the average number of clonal and sub-clonal mutations is significantly anti-correlated with the corresponding measure of evolutionary predictability ( Fig. 5c and d ). These negative correlations, albeit to a lesser extent, are also observed for MSK-IMPACT ( Supplementary Fig. S19 ).
 Fig. 5. Predictability, mutation rate and intra-tumor heterogeneity. In all panels, each point corresponds to a given cancer type and the vertical axis indicates the estimated predictability  ϕ ⪯ . The horizontal axis shows ( a ) the average mutation frequency per mega base-pairs [from  Lawrence  et al.  (2013) ], ( b ) the average number of clones per tumor, ( c ) the mean number of clonal mutations and ( d ) the mean number of sub-clonal mutations according to  Raynaud  et al.  (2018) . The blue lines are the linear regression models surrounded by a shaded confidence interval region. Evolutionary predictability is approximated using  Equation (12) , with  n =10 and  n ′ = 4  for the TCGA data. Note that in panel (a) only 11 cancer types are included in this analysis, because  Lawrence  et al.  (2013)  covered only 11 of the 15 cancer types 4 Discussion In this study, we have established a statistical framework based on CBNs to rigorously quantify the predictability of cancer progression directly from cross-sectional genomic data. In particular, our approach does not require measuring or estimating the fitness effects of mutations, which is common practice in evolutionary biology, where the dominating paradigm for studying the predictability of evolution relies on the concept of fitness landscapes. We systematically analyzed the validity of our approach by leveraging the simulated data of a previous study ( Diaz-Uriarte, 2018 ), which has made a connection between CBNs and fitness landscapes. We have shown that CBN-based approach strongly agrees with the fitness landscape approach under the SSWM model in estimating the evolutionary predictability, not only in representable but also in non-representable fitness landscapes, which are deliberately designed to be rugged by having an elevated level of reciprocal sign epistasis and hence are not consistent with the DAG assumption of CBNs. Our results revealed that CBN models, by inferring a maximum likelihood DAG of restrictions, are able to identify similar collections of feasible evolutionary trajectories as the SSWM-based model, although with a tendency of CBN models to allow more evolutionary pathways than the SSWM-based model ( Supplementary Text S2 ). Our comparison of the maximum likelihood CBN model with the empty CBN, in which mutations are assumed to occur independently, further highlighted the fact that the power of the CBN model lies in its ability to capture dependencies among mutations in the inferred DAG, rather than only their marginal frequencies. In fact, a pure frequency-based method (i.e. the empty CBN) distinguishes between mutational pathways almost as poorly as the uniform pathway distribution ( Fig. 3 ) and therefore considerably underestimates the predictability of cancer evolution ( Supplementary Text S10  and  Fig. S20 ). Furthermore, it is important to note that CBNs estimate the joint probability distribution of all genes, including more complex forms of epistasis. Hence, higher order epistasis, which is beyond pairwise epistasis, is captured by the CBN model and implicitly taken into account the final estimate of the predictability of cancer evolution. That being said, we acknowledge that the validity of our approach depends on the accuracy of the SSWM assumption. We do not know to what extent the SSWM assumption is valid for cancer evolution, as we cannot measure the  in vivo  fitness effect of mutations, but departure from the SSWM assumption might be conceivable at least for hyper-mutated tumors with elevated chromosomal instability. Nevertheless, the SSWM assumption with all its potential pitfalls is broadly applied in studying the predictability of evolution in general ( de Visser and Krug, 2014 ;  Weinreich  et al. , 2005 ) and it is well-supported by experimental evidence ( Poelwijk  et al. , 2007 ;  Weinreich  et al. , 2006 ). In order to address scalability and robustness of our framework for analyzing high-dimensional real genomic data, we developed a subsetting scheme ( Equation (12) ), which aggregates the results of smaller mutation subsets. We have shown that this approximation works well for both simulated and real data. It enabled us to cope with the high-dimensionality of the data and ensured robust estimation of the predictability (see  Supplementary Text S7  and  Fig. S14 ) by drastically reducing the network space in the structure learning step of the CBN model. In other words, while evolutionary constraints may be difficult to learn ( Diaz-Uriarte, 2018 ), this does not necessarily imply that predictability cannot be estimated reliably (which is a simpler task addressed and well approximated by the subsetting scheme). The robust estimation conferred by our subsetting scheme partly explains the different conclusion of our study as compared to the previous one ( Diaz-Uriarte and Vasallo, 2018 ), which reports that cancer evolution can be unpredictable for many datasets. In addition, the former study uses the ‘Lines of Descent’ ( Szendro  et al. , 2013 ), instead of the SSWM assumption employed here, such that different evolutionary regimes are analyzed. We observed that cancer evolution is remarkably constrained, as only a tiny fraction of mutational pathways (between 0.4% and 0.0004% depending on cancer type in TCGA data for  n =10 driver mutations) are feasible during the process of tumorigenesis. Furthermore, the analysis of the MSK-IMPACT dataset showed that tumor samples from metastatic sites display an even higher level of predictability, perhaps because in metastatic samples longer tumorigenic pathways have already been traversed or the evolution of metastatic potential is more convergent. This high level of constrained evolution can open a new avenue for further analysis of the feasible mutational pathways towards predictive modeling of cancer progression and calls for further research in the direction of pan-cancer identification of repeatable evolutionary trajectories ( Caravagna  et al. , 2018 ). Proving the usability of our framework for the ambitious goal of predictive modeling of cancer progression, however, would necessitate a rigorous benchmarking with longitudinal data, single-cell or multi-region samples, which is beyond the scope of our current study and calls for future research. However, our present work is still well-supported by empirical data. In line with our expectations, we have observed significant anti-correlation between estimated predictability of cancer types and alternative observable evolutionary traces such as mutational load and intra-tumor heterogeneity. A major limitation of our present study is that mutations used in the CBN model have been restricted to single nucleotide variants and incorporating copy number variations (CNVs) into our framework still remains as an unmet challenge. The reason is that the CBN model estimates the co-occurrence of mutations, but for CNVs, a more sophisticated model is necessary, which accounts for CNVs of varying sizes affecting simultaneously different sets of physically proximate genes. Thus, integration of CNV data in our framework for future applications requires the CBN model to be adapted for such physically correlated mutations. Moreover, it is important to note that our analyses of the real genomic data are based exclusively on frequent driver genes, which probably provide a strong selective advantage, and we have not taken into account rare drivers, which likely provide only a small selective advantage. Including weak drivers may or may not affect the predictability of cancer evolution, depending on how strong they depend on other mutations. In future work, the impact of weak drivers on the predictability of cancer evolution should be further explored. Furthermore, in our study, genotypes were defined exclusively on the level of ‘genes’. It is potentially interesting to estimate the predictability of cancer evolution alternatively on a higher level (e.g. on the level of ‘functional pathways’). A previous study ( Gerstung  et al. , 2011 ) has found stronger evidence for pathway order constraints than for gene order constraints, which indicates that temporal ordering results from selective pressure acting on the pathway level. Therefore, if we estimate predictability of cancer evolution on the level of functional pathways, rather than genes, it is very likely that the predictability of cancer evolution is even higher on the pathway level. Also, a model has been presented for estimating groups of mutually exclusive genes and their dependency structure at the same time ( Cristea  et al. , 2017 ). Using this version of a CBN model, one might arrive at pathway-level estimates of the predictability of evolution. We will explore this approach in future work. On the other hand, we might need to define genotypes on a lower level, e.g. on the level of individual mutations, because different non-silent mutations in a given gene can exert different phenotypic effects. Some mutations in a driver gene may not be driver mutations and some genes may harbor both loss and gain of function mutations. Therefore, another open question, which calls for further research, is how genotypes defined on the level of driver mutations rather than driver genes affects the predictability of cancer evolution. In summary, the key insight of our analyses of real genomic data on the level of driver genes is that cancer evolution is remarkably predictable, and hence there is high potential for systematic discovery of phenotype-determining repeatable evolutionary trajectories, which are of increasing importance in personalized medicine. Whether the relatively high level of predictability we found is driven mostly by known gene–gene interactions or whether many novel interactions contribute to it remains to be analyzed in future studies that likely require larger sample sizes. Supplementary Material btz332_Supplementary_Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Family classification without domain chaining</Title>
    <Doi>10.1093/bioinformatics/btp207</Doi>
    <Authors>Joseph Jacob M., Durand Dannie</Authors>
    <Abstract>Motivation: Classification of gene and protein sequences into homologous families, i.e. sets of sequences that share common ancestry, is an essential step in comparative genomic analyses. This is typically achieved by construction of a sequence homology network, followed by clustering to identify dense subgraphs corresponding to families. Accurate classification of single domain families is now within reach due to major algorithmic advances in remote homology detection and graph clustering. However, classification of multidomain families remains a significant challenge. The presence of the same domain in sequences that do not share common ancestry introduces false edges in the homology network that link unrelated families and stymy clustering algorithms.</Abstract>
    <Body>1 INTRODUCTION 1.1 Family classification Gene families are the basis of phylogenomic inference (Brown and Sjolander,  2006 ), and evolution-based methods for function prediction and annotation transfer (Wu  et al. ,  2003 ). Knowledge of family structure facilitates the study of the processes that drive family evolution (Demuth  et al. ,  2006 ). Whole genome sequencing efforts have inspired the construction of large-scale gene family databases with the goal of characterizing the full complement of homologous families over a broad range of genomes (Crabtree  et al. ,  2007 ; Heinicke  et al. ,  2007 ; Tatusov  et al. ,  2003 ; Wheeler  et al. ,  2008 ). These and other genome-scale applications require methods that support accurate, automated and high-throughput family classification. The goal of gene family classification is to partition a set of unlabeled sequences into homologous families (Fitch,  2000 ), i.e. sets of sequences derived from a common ancestral gene by speciation and gene duplication 1 . We consider gene family classification, with particular attention to the  domain chaining  problem: inappropriate merging of unrelated families due to the presence of the same promiscuous domain in sequences belonging to different families (Heger and Holm,  2000 ). For example, in  Figure 1 , sequence-based approaches will tend to assign  z  to the homologous family, {  w ,  x ,  y  }, because genes  w  and  z  share a homologous domain. This assignment is incorrect; there is no ancestral genome that contains an  entire  gene that is ancestral to both  w  and  z .
 Fig. 1. The evolutionary history of a hypothetical multidomain family showing both gene duplications and domain insertions. Genes  x ,  y  and  w  share a common ancestor but do not have identical domain composition. Gene  z  shares a homologous domain with these genes, but there is no gene that is ancestral to both  w  and  z . 
 1.1.1 Background Most approaches to gene family classification represent the sequence universe as a homology network  G H =( V , E H ) where  V  is the set of all sequences and ( x ,  y )∈ E H ,  iff x  and  y  are homologous. Homology is a transitive property: if  x  and  y  share common ancestry, and  y  and  w  share common ancestry, then  x  and  w  must also share common ancestry. Consequently,  G H  is a transitive graph: ( x , y )∈ E H  and ( y , w )∈ E H →( x , w )∈ E H  (Rahmann  et al. ,  2007 ). Otherwise stated,  G H  is a disjoint union of cliques, in which each clique corresponds exactly to a single gene family. In practice, however,  G H  is unknown, and homology is typically estimated using sequence comparison. This yields a graph  G S =( V , E S ), where  E S ={( u , v )} such that ( u , v )∈ V  ×  V  has a sequence similarity score better than a given threshold. Since sequence similarity is not a perfect predictor of homology,  G S  will not, in general, be a transitive graph. Remote homology will result in missing edges, while spurious similarity, convergent evolution and shared promiscuous domains will introduce false edges. As a result, families no longer correspond to cliques or even to disjoint connected components. The typical solution is to apply a graph clustering algorithm to  G S  to predict families. Efforts to improve family classification fall into two categories. One strategy is to improve homology prediction to reduce noise (i.e. false and missing edges) in  G S  (Altschul  et al. ,  1997 ; Brejova  et al. ,  2003 ; Buhler  et al. ,  2003 ; Weston  et al. ,  2004 ; Zhang  et al. ,  1998 ). A second approach is development of more sensitive clustering algorithms (Bolten  et al. ,  2001 ; Enright  et al. ,  2002 ; Kim and Lee,  2006 ; Krause  et al. ,  2005 ; Rahmann  et al. ,  2007 ; Sasson  et al. ,  2003 ; Weston  et al. ,  2004 ; Wittkop  et al. ,  2007 ). These approaches are interdependent and can be combined. Many clustering algorithms seek specific structural features in graphs, based on the assumption that  G S  still retains clique-like structures corresponding to families, despite noise. Conversely, better pairwise homology prediction methods will yield a network that better approximates transitivity, and is more amenable to clustering algorithms. Although major gains have been made in the area of family classification overall, the problem of domain chaining remains largely unaddressed. The lack of a gold standard dataset that includes complex multidomain homologs has been a major obstacle. Most work on homology prediction has focused on the problem of detecting remote homology without inclusion of chance sequence similarity. A few heuristics to eliminate domain chaining have been proposed (Bjorklund  et al. ,  2005 ; Huynen and Bork,  1998 ; Song  et al. ,  2007 ), but due to the lack of a gold standard, the effectiveness of these approaches could not be evaluated. Recent empirical evaluations of clustering methods show that more sophisticated clustering strategies can substantially improve the classification of single domain families (Paccanaro  et al. ,  2006 ; Wittkop  et al. ,  2007 ). While these studies did not evaluate clustering performance on multidomain sequences, TribeMCL (Enright  et al. ,  2002 ), one of the few methods designed with domain chaining in mind, was not a top performer. In recent, prior work, we hand curated a multidomain benchmark dataset and used it to evaluate the performance of currently used methods for pairwise homology prediction (Song  et al. ,  2008 ). Our results indicate that for multidomain sequences, sequence comparison results in high error rates, as do heuristics designed specifically to eliminate domain chaining. We further introduced a method, Neighborhood Correlation, that exploits the structure of the sequence similarity network to predict homologs. We demonstrated empirically that Neighborhood Correlation dramatically outperforms other methods for pairwise multidomain homology prediction. However, improved performance on pairwise predictions is not a priori evidence for effective family classification. 1.1.2 Contributions For family classification, the biological property of interest (sequences that share common ancestry) corresponds to a precise mathematical construct (cliques). This ability to cast the problem in terms of a mathematical objective guides algorithm design (Rahmann  et al. ,  2007 ; Wittkop  et al. ,  2007 ); methods that add edges to dense subgraphs and remove edges in sparse regions are promising candidates for family classification. It also suggests graph transitivity and cluster density as measures of performance evaluation in the absence of a gold standard. In the current work, we show analytically that a network rescoring method based on local graph structure will increase graph transitivity in an unweighted network, provided that it is not too far from a network of cliques. In addition, we simulate a network of cliques with noise and demonstrate that rescoring restores transitivity. We further evaluate network rescoring on weighted graphs based on biological data. We show that the network structure in the rescored network of mouse and human sequences closely corresponds to families of known common ancestry, yielding a classification with substantial improvements in Precision and Recall compared with sequence similarity. In the yeast network, we show that rescoring improves graph properties associated with high subgraph density, yielding a more compact network well suited to family inference. Finally, selection of a single threshold that removes spurious edges and adds missing homologous edges, while retaining correct relationships, is a key challenge addressed by our methods. For unweighted networks, we suggest an analytical approach to selecting such a threshold. We further discuss empirical approaches to selecting a threshold in weighted networks. We also demonstrate empirically that the optimal classification threshold for the rescored network is much less sensitive to family history than that of the sequence similarity network. 2 MODEL The goal of network rescoring is to decrease scores of unrelated pairs and increase scores of related pairs, such that it is possible to select a threshold that separates these two sets. We show here that Neighborhood Correlation (Song  et al. ,  2008 ), which rescores a network based on its local organization, has this property. Neighborhood Correlation takes a weighted network as input and calculates pairwise scores in the range [−1, 1] between all pairs of nodes. Given a fully connected, weighted network, let  w x  be the vector of similarity scores between  x  and all other nodes in the network; i.e.  w x [ i ]= S ( x , i ), where  S ( x , i ) is the similarity between sequences  x  and  i . We define the Neighborhood Correlation score, NC( x , y ), to be the Pearson correlation coefficient between  w x  and  w y . Formally,
 (1) 
where  N  is the number of sequences in the network, and   is the mean of  w x . Empirical evaluation shows that Neighborhood Correlation exhibits superior performance on the pairwise multidomain homology prediction problem (Song  et al. ,  2008 ). The effectiveness of Neighborhood Correlation can be understood biologically. Local network structure encodes traces of the evolutionary history of sequences because gene duplication and domain shuffling events impose distinct local organization. Neighborhood Correlation exploits this property to accurately identify family structure. The performance of Neighborhood Correlation can also be understood mathematically: since gene families correspond to cliques in  G H , sequences within a family will have numerous edges to other members of the family, and these relationships can be used to support edges missed by sequence comparison. This intuition suggests that a clique may be resolved from noise so long as a sufficient fraction of homologous edges are retained. Conversely, spurious edges will not be supported by the surrounding local network structure. Let  G S  be a network in which the weight of every edge is either zero or one, imposed by selecting edges with a similarity threshold,  t . A weighted network  G NC  is then constructed by rescoring  G S  with Neighborhood Correlation, using Equation ( 1 ), where
 (2) 
 Transitivity may be increased by splitting inappropriately linked families. It may also be increased by adding edges that complete cliques. The following examples motivate selection of a threshold that suits both of these interests. First, an example of two unrelated families linked by a single edge suggests an appropriate threshold.  Figure 2 A shows a connected component consisting of two subgraphs of size ≥3, corresponding to families  f x  and  f y . Node  x  is adjacent to at least two nodes in  f x  (resp.  y ). As  N  becomes large, NC( x ,  y ) approaches 0.5. If  x  has more than two neighbors in its family, then NC( x , y ) will decrease further. A threshold of NC&gt;0.5 will eliminate the spurious edge ( x , y ), correctly splitting the component into two separate families. This suggests that a threshold of 0.5 will separate unrelated families in  G NC .
 Fig. 2. Example graph components for intuition. In ( A ),  x  and  y  are members of families  f x  and  f y , respectively, but joined by a single edge. ( B ) depicts a single family missing two edges, while ( C ) illustrates a case where edge weights must be used to distinguish between edge addition or deletion. 
 We next consider whether this threshold is low enough to restore missing edges to a clique. A family of size 4 is shown in  Figure 2 B. Two additional edges, ( x , u ) and ( v , y ), are needed to form a clique. As  N  becomes large, NC( x , u )=NC( v , y )→0.6, and NC(·,·)→0.8 for all edges already present in the component. With a threshold of NC&gt;0.5, the existing edges will be retained and transitivity is increased by the added edges, completing the clique. In general, for any connected component of size  k &gt;3 with at least than  k ( k −1)/4 edges, more edges will be added with score  NC &gt;0.5, yielding a denser component and increasing network transitivity overall. Formalizing these ideas is an interesting direction for future theoretical work. Our interest here is to investigate the practical consequences of these observations for gene family classification. While the unweighted model is a useful abstraction for theoretical analysis and simulation, a weighted graph based on sequence similarity scores should be used for real data. Consider the example in  Figure 2 C. If  x ,  y  and  z  represent a family, then a third edge, ( x , y ) should be added. On the other hand, if, say,  x  and  z  form a family of size 2 and  y  is unrelated, then ( z , y ) should be removed. In this case, connectivity alone provides no information to make this decision and Neighborhood Correlation can yield no additional confidence. In a weighted graph,  S ( x , z ) and  S ( z , y ) would determine whether the edge should be added or subtracted. Therefore, the information provided by edge weights should be utilized when working with a real sequence similarity network. With a weighted graph, it is no longer possible to select a threshold based on simple geometric arguments. In the following section, we demonstrate that the ideal threshold for a network rescored by Neighborhood Correlation is robust, and may be selected more readily than with other leading methods. Several approaches may be used to infer an appropriate classification threshold. As a rule of thumb, a threshold may be selected by plotting a histogram of Neighborhood Correlation scores. With our human and mouse dataset ( Fig. 3 ), and all other datasets we have considered, this histogram is strongly bimodal. The two peaks, one with scores close to 1.0 and the other close to 0.0, are separated by a broad, low trough at intermediate values. Threshold values selected within this trough are robust and accurately partition the network when compared to gold standards. When a more fine-grained approach is desired, a threshold may be selected by optimizing general measures of network transitivity, as below.
 Fig. 3. Histogram of Neighborhood Correlation scores for the mouse and human dataset. 
 3 RESULTS We evaluate family classification methods using three different approaches: simulation, comparison with a gold standard and via intrinsic measures based on graph transitivity.The most relevant test is comparison of predicted families with known families, when a gold standard is available. We use curated families in mouse and human for this analysis. When a gold standard is not available, family classification can be evaluated using intrinsic measures. Since homology is an intrinsically transitive property, the extent to which  G  approximates a network of cliques is a measure of classification performance. Clustering coefficient and average component density are two measures that assay this property. We use this approach in yeast, for which no gold standard is available. Finally, we use simulation to evaluate the ability of our methods to restore transitivity a network of cliques degraded by noise. With simulation, the true homology network is known and it is possible to control parameters of interest (e.g. clique size, clique number, the number of false and missing edges). All three analyses begin with a graph,  G S  that corresponds to a transitive homology graph,  G H , perturbed by noise. Every pair of nodes ( u , v )∈ V  ×  V  is rescored using Neighborhood Correlation to obtain a rewired graph,  G NC =( V , E NC ). 3.1 Validation Metrics For our application, where homologous gene families correspond to cliques, figures of merit that assess the transitivity of a graph are appropriate internal validation methods. We use two metrics to evaluate how well a graph approximates a set of isolated cliques: the mean clustering coefficient,  C , which reflects local transitivity, and the graph component density,  D , a measure of global transitivity. The clustering coefficient for a single node  i , of degree  k i ≥2, is
 (3) 
where  V i  is set of nodes adjacent to  i .  C i  is the edge density of the subgraph,  V i . It reflects the degree to which the neighbors  j  and  k , of a node  i , are connected to each other. The graph clustering coefficient,  C , is the mean  C i  over all nodes, and is an average measure of local density.  C ( G )=1  iff G  is transitive. The component density of a graph is the weighted average of the density of individual components or
 (4) 
where  d c =2 E c /( L c ( L c −1)) is the density of component  c , with  L c  nodes.  D ( G )=1  iff G  is transitive. Note that  D  is equivalent to the ratio of the total number of edges in  G  to the number of possible edges within components. Given graphs  G S =( V , E S ) and  G NC =( V , E NC ) of equivalent total density (| E S |=| E NC |), the graph with the highest component density most closely approximates a transitive graph. Both  C  and  D  increase with transitivity, reaching unity in a fully transitive graph. Although, in general, high values of  C ( G S ) and  D ( G S ) are evidence that  G S  closely approximates  G H , these measures can be misleading in extremely dense or sparse graphs. In a graph consisting of one, or a very small number, of dense connected components, both  C  and  D  will be close to one. However, this is not a realistic gene family model. At the other end of the spectrum,  D  will be unity in a graph consisting entirely of components of size 2, but these, again, are not typical of gene families in real data. Moreover, the clustering coefficient is not informative for very sparse graphs, since  C  is not defined on connected components of size 2. To ensure that the graphs obtained are not near these extremes, we consider the number of connected components. In the simulated data, where the exact number of cliques in  G H  is known, we also verify that we recover the correct number of connected components. In the analyses of simulation and yeast data, relative transitivity is assessed by comparing the values of  C  and  D  for  G S  and  G NC . These values depend on the choice of edge weight threshold used to sever edges in the graph. To obtain a fair comparison, we select thresholds in  G S  and  G NC  to obtain graphs with the same graph density. Graph density is a suitable basis for normalization, because  D  is directly, and  C  is indirectly, dependent on overall graph density. In the simulation analysis, the density of  G S  is implicitly controlled by the noise model, which, by design, constructs  G S  with the same density as  G H , in expectation. A Neighborhood Correlation score threshold is then explicitly selected to ensure that  G NC  also has the same density. In the yeast studies, the Neighborhood Correlation threshold is treated as an independent variable. For each Neighborhood Correlation threshold considered, the sequence similarity threshold is selected to obtain a graph,  G S  with the same density as  G NC . Neighborhood Correlation performance was also assessed on a curated set of mouse and human families (Song  et al. ,  2008 ). This test set was derived from the set of all 26 197 full length, mouse and human amino acid sequences derived from the SwissProt (version 50.9) database. Twenty families with evidence of common ancestry were considered, including 1577 sequences in all. This set is based on a synthesis of over 70 publications by experts on specific families. The selected families represent seven single domain families, five families of conserved multidomain architecture and eight families of variable architecture. These families also represent a range of sequence conservation. Highly divergent single-domain families, such as the tumor necrosis factors (TNFs) and ubiquitin-specific proteases (USPs), were included to test the performance on remote homology prediction. Details of the family curation procedure are given in (Song  et al. ,  2008 ). 3.2 Simulation In simulation studies, we construct an artificial graph  G H  consisting of a disjoint set of cliques to represent families.  G S  is derived from  G H  by simulating missing and spurious edges that arise from faulty homology prediction. Edges within cliques of  G H  are selected for deletion with probability  p d . Edges not in  G H  are selected for addition with probability  p a , where  p a  is selected such that the expected total density of  G S  is equal to the density of  G H . We considered networks with cliques of varying sizes, because small cliques are more sensitive to noise than large cliques due to the difference in ‘redundant’ connections within the clique.  Figure 4  shows an analysis of a network of 48 cliques: 16 cliques of size 4, and 8 each of sizes 8, 16, 32 and 64. The choice of these parameters was guided by the observed family sizes in our curated mouse and human families. The results obtained for other conformations were similar (data not shown).
 Fig. 4. Component and transitivity measures of simulated networks of cliques degraded by noise; ( A ) connected component count, ( B ) component density and ( C ) mean clustering coefficient. Error bars indicate 1 SD over 100 randomization trials. 
 Figure 4  illustrates that a very small number of mis-assigned edges is sufficient to completely disrupt this family structure, as shown by the low values of both  C ( G S ) and  D ( G S ). In contrast, Neighborhood Correlation is able to completely recover this structure, when  p d ≤0.1. In addition,  G NC  almost always has 48 components, the same number as  G H . This is a strong evidence to show that Neighborhood Correlation is able to perfectly reconstruct  G H  at low error rates. Performance begins to degrade as noise increases. However, even with  p d  as high as 0.2, Neighborhood Correlation is still able to reconstruct more than half of the components and  C ( G NC ) remains &gt;0.8 for the entire range, 0 ≤ p d ≤0.5. 3.3 Mouse and Human After considering the utility of Neighborhood Correlation on simulated data where the true graph  G H  is known, we now turn to real data. A weighted sequence similarity network,  G S , was constructed using all-against-all BLAST (version 2.2.15, default parameters) comparison of the full set of 26 197 mouse and human amino acid sequences in our dataset. A significance  E -value corresponding to 10 matches per sequence was used. A weighted network,  G NC , was constructed from  G S  by calculating Neighborhood Correlation scores using Equation ( 1 ), where
 (5) 
 S ( x , i ) is the normalized bit score (Altschul  et al. ,  1997 ), and  S min  is fixed to 95% of the smallest bit score satisfying  E =10. Families were predicted from both  G S  and  G NC  by applying each of three simple agglomerative clustering variants: single,  complete  and average linkage. Non-overlapping clusters are obtained by cutting the agglomerative tree at a particular threshold. The quality of clustering is evaluated by the correspondence between families and clusters. Given a family  i  and a cluster  j , the Precision,  P ij , is the fraction of elements in  j  that are members of family  i . Similarly, the recall,  R ij , is the fraction of members of family  i  that are found in cluster  j .  F , the harmonic mean of Precision and Recall, reflects the quality of both Precision and Recall simultaneously:
 (6) 
Classification performance on each family was determined using a family-specific  F -measure:  F i =∑ j   n i , j F i , j / n i , where  n i , j  is the number of members of family  i  in cluster  j  and  n i  is the number of sequences in the entire family. The family-specific  F -measure captures the classification quality on individual families, but does not reflect performance on a mix of sequences from families with varied conservation and architecture. To test classification on heterogeneous data, we also calculated the  F -measure on sequences from all families combined ( ALL ) using the weighted average  F =∑ i , j   n i , j F i , j / n , where  n =∑ i   n i . One family, the kinases, is much larger than the others. To avoid bias, we also calculated F for the set of all sequences except the Kinases ( ALL-kin ). We evaluated classification performance of both  G S  ( Fig. 5 A) and  G NC  ( Fig. 5 B), for all possible thresholds. Families are grouped by domain architecture: first, single domain families; then, multidomain families with conserved architectures, followed by families with variable architectures. Classification performance is expressed as a heatmap of the  F -measure, where  F =1 (red) is optimal, and  F =0 (blue) is the worst possible clustering quality.
 Fig. 5. Evaluation of clustering performance of sequence similarity ( A ) and Neighborhood Correlation ( B ) on 20 curated families in mouse and human. This heatmap illustrates  F -measure, where good performance (F=1) is red and low (F=0) is blue, for single-, average-, and complete-linkage agglomerative clustering. Families are ordered by domain structure, where ACSL-WNT are single domain, DVL-TRAF have conserved multidomain architectures and ADAM-TNFR have variable architectures.  ALL  and  ALL-kin  depict weighted averages over the full set, and the set excluding Kinase, respectively. 
 It is immediately clear from inspection of  Figure 5  that a much better classification can be obtained using the rewired network: the  Figure 5 A is mostly blue;  Figure 5 B is mostly red. Near perfect classification of single domain families, with the exception of TNF and USP, can be obtained using either of the scoring systems. Similar behavior is seen with multidomain families with conserved architectures: good classifications can be achieved by either of the methods, although the optimal classification threshold varies substantially from family to family. In contrast, classification with the rewired network shows a dramatic improvement over sequence similarity for families with variable architectures. Classification of TNF and USP, families with low sequence conservation, is also much improved in the rewired network, showing that rewiring restores missing edges due to remote homology, as well as removing edges due to domain chaining. Comparison of the performance of  G S  and  G NC  on the  ALL  and  ALL-kin  datasets reveals that sequence similarity scores are much more family-specific than Neighborhood Correlation scores. The poor performance of single-linkage clustering on  G S  implies that no threshold can give good performance for most families, a fundamental obstacle to obtaining good classifications on heterogeneous data using sequence similarity. Neighborhood Correlation obtains much better performance on these aggregate datasets: the best performance of the rewired network ( F max =0.85, avg. linkage) is substantially greater than that of sequence similarity ( F max =0.42, avg. linkage). This suggests that rewiring is of particular importance for automated, genome-scale analyses. Since the single-linkage metric simply generates connected components for a particular threshold, comparison of the single-linkage heatmaps for  G S  and  G NC  reveals the benefit of rewiring alone, without additional clustering. Neighborhood Correlation rewiring increases transitivity and results in connected components that more closely approximate true families. Both average and complete linkage further improve the classification obtained from  G NC , suggesting that Neighborhood Correlation not only increases graph transitivity, but also that rewiring as a pre-processing step is a promising approach for better classification. Interestingly, neither average- nor complete-linkage much improves performance with  G S . The  F -measure captures overall classification performance, but does not detail the tradeoff between Precision and Recall.  Figure 6  shows the F, Precision and Recall attained with average linkage for  G S  and  G NC  on the  ALL-kin  dataset. On this dataset, sequence similarity can obtain near perfect Precision, but at a cost of missing roughly half of all true positives. In contrast, classification with Neighborhood Correlation delivers both Precision and Recall &gt;80% for Neighborhood Correlation scores ranging from 0.2 to 0.6.
 Fig. 6. F -measure, Precision and Recall of the  ALL-kin  dataset, for ( A ) sequence similarity, and ( B ) Neighborhood Correlation, after average-linkage clustering. 
 3.4 Yeast In a third analysis, we considered how Neighborhood Correlation influences graph properties in the yeast network. We also investigated whether it is beneficial to use additional sequence data when calculating Neighborhood Correlation scores. Since NC( x , y ) effectively compares the relationship between  x  and other sequences with the relationship between  y  and other sequences, we hypothesized that using additional sequences to calculate Neighborhood Correlation would improve its accuracy. All 46 060 amino acid sequences from nine yeast genomes were obtained from the YGOB, version 2 database (Byrne and Wolfe,  2005 ). All-against-all BLAST comparisons were carried out on the set of all genes in  Saccharomyces cerevisiae  alone; in four genomes ( S.cerevisiae ,  Candida glabrata ,  Ashbya gossypii , and  Kluyveromyces lactis ); and in all nine genomes in YGOB2. Neighborhood Correlation scores were then calculated for all pairs in each of these three datasets. From these, we extracted three sequence similarity networks ( G S −1 ,  G S −4  and  G S −9 ) and three Neighborhood Correlation networks ( G NC −1 ,  G NC −4  and  G NC −9 ) for  S.cerevisiae  only; that is, in two of these, multiple genomes were used to calculate the edge weights, but we consider only edges in  G S  and  G NC  between nodes of the 5616  S.cerevisiae  genes in this analysis. We constructed a visual representation of the  G NC  network with a force-based layout calculated with Neato (Emden R. Gansner and Stephen C. North,  1999 ).  Figure 7  shows that Neighborhood Correlation breaks the network into disjoint components. Many of these are cliques. Unfortunately, no rigorously curated gold standard for evolutionary families is available in yeast. However, visual inspection revealed that many of these components correspond to groups of genes that are commonly considered as families. For example, actin and the seven actin-related proteins (ARPs) form an isolated clique and the large cluster in the upper right hand corner corresponds to the kinases. This well-defined component structure was not observed in a similar visual representation constructed from the sequence similarity network ( G S ) (data not shown).
 Fig. 7. Visualization of the  S.cerevisiae  genome after rescoring with Neighborhood Correlation. Edge color signifies the Neighborhood Correlation score, where gray indicates NC≥0.3, violet ≥0.4, green ≥0.6, orange ≥0.8, and yellow ≥0.9. The dense component at top right contains all kinases. Singleton nodes have been omitted for clarity. 
 In the absence of a gold standard, we evaluate the ability of Neighborhood Correlation to restore transitivity in the yeast network using the same network measures used in the simulation analysis, shown in  Figure 8 . The Neighborhood Correlation and sequence similarity axes are aligned such that graph density is constant for NC ≥ 0.15.
 Fig. 8. Component and transitivity measures of the network of  S.cerevisiae  genes for sequence similarity, and Neighborhood Correlation calculated with one, four and nine yeast genomes: ( A ) connected component count, ( B ) component density and ( C ) mean clustering coefficient. 
 The clustering coefficient ( Fig. 8 C) is consistently higher in  G NC  than in  G S . This demonstrates higher local density for components of size ≥ 3. Similarly, component density ( Fig. 8 B) is consistently higher for Neighborhood Correlation than for sequence similarity up to thresholds of NC ≥ 0.9. Taken together, these measures show that in yeast, as in mammalian and simulated networks, Neighborhood Correlation restores transitivity. For all but the highest thresholds, components in  G S  are larger and sparser than in  G NC . This occurs because multiple families are merged into single components in  G S , yet failure to recognize remote homology keeps these components sparse. The component density of  G NC  decreases markedly between thresholds of 0.75 and 0.9. First,∼10% of components of size 2 break up into singletons in this range. Since the density of a two-node component is one, loss of such pairs will substantially reduce the average value of  D ( G ). In addition, large components become sparser as the threshold becomes more stringent. For example, the largest component in the network in this range (the Kinases), decreases in density from 0.85 to 0.33. At very high stringencies, both networks consist almost entirely of singletons, two-node components and a few very small cliques of size ≥ 3, leading to values of  C ( G ) and  D ( G ) close to one. The number of components ( Fig. 8 A) increases from few very weakly connected components at very lenient thresholds to a larger number at more stringent thresholds in both  G NC  and  G S . In  G NC , the component count is stable for thresholds roughly from 0.3 to 0.7, illustrating the robustness of Neighborhood Correlation.  G S  has fewer components at comparable thresholds up to 0.7. Again, this is probably due to larger components containing members of more than one family. At higher thresholds, the number of components decreases in both graphs as components are broken up into singletons, which are not included in the component count. Comparison of  G NC −1 ,  G NC −4  and  G NC −9  shows that including more genomes in the calculation of Neighborhood Correlation further increases transitivity. It is reassuring to note that while the clustering coefficient and component density are higher with more genomes, the overall trends are unaffected. Moreover, the differences between  G NC −1 ,  G NC −4  and  G NC −9  are smaller than the differences between  G NC  and  G S  except at very low thresholds. 4 DISCUSSION Despite advances in gene family classification, classification of multidomain families remains an open problem. Whereas true homology forms a set of disjoint cliques, domain chaining introduces false edges that degrade transitivity. We show empirically through simulation and with real biological data that Neighborhood Correlation captures homology and restores transitivity to the sequence similarity network. From networks degraded by considerable noise, Neighborhood Correlation recovers network clique structure typical of the structure of gene families evolved through vertical descent. When studied from an analytical standpoint, examination of simple graph structures that reflect expected family structure suggest that this is an inherent mathematical property of Neighborhood Correlation. Formalization of this intuition is an interesting area for future development. In addition to improving transitivity overall, we empirically verify that dense subgraphs in the Neighborhood Correlation graph correspond to known homologous families in the mouse and human genomes. Moreover, because Neighborhood Correlation effectively normalizes scores across families, good quality classification can be achieved with a single threshold for all families. Families in the network rescored by Neighborhood Correlation are correctly represented as disjoint components, suggesting that Neighborhood Correlation ameliorates the domain chaining problem and captures remote homology. The application of hierarchical clustering to the rescored network improves performance further. This suggests that while Neighborhood Correlation better estimates homology than sequence similarity, it can also be a useful pre-processing step to more sophisticated clustering algorithms that consider network structure. Empirical evaluation of clustering algorithms on curated multidomain data is a useful direction for future work. Many family classification methods have been shown to work when validated with single domain sequences, domain models, structural similarity and functional data, though it is important to recognize that none of these explicitly test evolutionary relationship, and all are ill-suited to evaluating the question of homology. Use of such validation is partly due to a lack of available curated datasets. We provide our curation dataset for use by others, at  http://www.neighborhoodcorrelation.org . </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>CPSP-web-tools: a server for 3D lattice protein studies</Title>
    <Doi>10.1093/bioinformatics/btp034</Doi>
    <Authors>Mann Martin, Smith Cameron, Rabbath Mohamad, Edwards Marlien, Will Sebastian, Backofen Rolf</Authors>
    <Abstract>Summary: Studies on proteins are often restricted to highly simplified models to face the immense computational complexity of the associated problems. Constraint-based protein structure prediction (CPSP) tools is a package of very fast algorithms for ab initio optimal structure prediction and related problems in 3D HP-models [cubic and face centered cubic (FCC)]. Here, we present CPSP-web-tools, an interactive online interface of these programs for their immediate use. They include the first method for the direct prediction of optimal energies and structures in 3D HP side-chain models. This newest extension of the CPSP approach is described here for the first time.</Abstract>
    <Body>1 INTRODUCTION Lattice models are a common abstraction of protein structures to enable high-throughput studies in sequence and structure space (Huard  et al. ,  2006 ); (Jacob  et al. ,  2007 ); (Wolfinger  et al. ,  2006 ). This is achieved by a simplified representation of amino acids with one or a few monomers each, and a discretization of the structure space that restricts the positions of the monomers to nodes on a regular lattice. The modelling accuracy of structures depends on the underlying lattice (e.g. 3D cubic or 3D face-centered-cubic (FCC) and the number of monomers per amino acid. Arbitrary accuracy comes at the cost of computational complexity. Even in the simplest lattice model, the 2D-square HP model, problems of interest as  ab initio  optimal structure prediction and inverse folding stay computationally demanding (NP-complete) (Berger and Leighton,  1998 ); Berman  et al. ,  2004 ). Solving such problems for complex full atom models is currently completely infeasible. The HP model is widely used in literature and groups amino acids into (H)ydrophobic and (P)olar to focus on hydrophobic forces in protein structures. This is done by maximizing (HH) contacts between neighbored H-monomers resulting in a compact hydrophobic core (Dill,  1985 ). In the original model each amino acid is represented by a single monomer but, as discussed later, different models also representing side chains have been developed. Here, we present a web interface to constraint-based protein structure prediction (CPSP)-tools (Mann  et al. ,  2008b ), a package of exact and complete methods to cope with tasks in the field of HP lattice protein studies. The algorithms are based on the approach by Backofen and Will ( 2006 ) utilizing advanced techniques like constraint programming (see CPSP section). This approach enables the calculation of one or all optimal structures (not possible using stochastic approaches) and has opened up new vistas for protein studies. The CPSP-tools package is designed for fast high-throughput experiments in 3D lattices (Mann  et al. ,  2008a ); Wolfinger  et al. ,  2006 ). All CPSP-tools support 3D lattices such as the 3D-FCC lattice. The latter was shown to yield highly accurate fits of real protein structures (Park and Levitt,  1995 ). Our CPSP-web-tools enable direct instant access to the CPSP-tools when no high-throughput experiments are needed. The package serves as a platform for research and teaching in the field of HP protein models. The combination of interactive result visualization including 3D views of the structures, the interlink of the different tools for chained applications and the fast runtimes of the CPSP-tools results in a useful service for the end-user. In particular, we present the latest extension of the CPSP approach that enables for the first time the prediction of optimal structures in the 3D HP side-chain model (Bromberg and Dill,  1994 ). Here, each amino acid is represented by two monomers, one for the side-chain residue group and one for the backbone atoms. While the backbone is considered to be neutral, the side-chain monomers are distinguished into (H)ydrophic and (P)olar and, following the original model, a maximization on HH-contacts between H-side-chain monomers is sought. This more detailed model is closer to real protein structures, while still focusing on a compact hydrophobic core. Still, the extended CPSP approach is able to tackle the increased complexity and calculates optimal energies and structures within seconds. 2 CPSP-WEB-TOOLS The CPSP-web-tools provide a direct and interactive web platform for the programs contained in the CPSP-tools package (Mann  et al. ,  2008b ). It focuses on  ad hoc  and experimental usage of the programs as needed when one is only interested in a few experiments or teaching. The JavaServer Pages based platform supports workflow oriented operations by providing direct tool application on the results of a previous request. For example, an optimal structure calculated by HPstruct can be forwarded into the sequence design front end of HPdesign to expand afterwards the corresponding neutral network (HPnnet). It is the first and only online service that interfaces programs for these tasks and is able to handle HP sequences of more than 100 amino acids in length. Extensive javascripting is utilized to provide interactive input validation to the user. All CPSP-web-tools provide detailed help on the required parameters. Further help and information concerning the CPSP approach, the tool package itself and general concepts in the field of lattice proteins is provided by a collection of frequently asked questions. The interactive 3D visualization of lattice protein structures utilizes the Jmol molecule viewer (Herrez,  2006 ). For an example see  Figure 1 . Programs and tasks currently part of the CPSP-web-tools interface are:
 HPstruct: optimal energy and structure calculation for backbone and side-chain model (see next section). HPview: interactive visualization of lattice protein structures. HPconvert: conversion between different structure formats like 3D-coordinates, PDB, CML, etc. HPdeg: calculation of a sequence's degeneracy, i.e. the number of optimal structures it can adopt. HPnnet: expansion of the neutral network of a given sequence, i.e. the interlinked fraction of sequence space that stably share a common structure. HPdesign: design of sequences that fold stably into a given structure. 
 Fig. 1. An optimal structure (energy −55) of HPPHHPPPHPHHPHHPPH-PHPPHHHPHHPPHPHPH in the 3D FCC side-chain HP model. The CPSP-web-tools interface is being constantly expanded such that further tools and features will become available. 3 CPSP-APPROACH AND SIDE-CHAIN MODELS The CPSP-approach (Backofen and Will,  2006 ) is based on a database of so called  H-cores  following the observation that optimal structures show a compact placement of H-monomers. An optimal H-core is a set of H-monomer positions allowing for the maximal number of HH-contacts. For a concrete sequence  S  the approach systematically examines the list of H-cores compatible with  S  in decreasing contact number. For each core, it attempts to find a placement of the monomers of  S  in a self-avoiding walk such that all H-monomers are elements of the given H-core and all P-monomers are outside of the core. Since the H-cores are considered in the order of decreasing contacts, the first successful threading results in a structure with global minimal energy. Note that at this point the algorithm has  proven  that there is no structure of  S  that forms more HH-contacts. Technically, the threading of a sequence through a core is performed by a constraint program. A constraint satisfaction problem (CSP) is formulated that constrains the H-monomers to positions in the H-core. Further, it enforces successive monomers along the sequence to be neighbored in the lattice and prohibits the multiple use of a single position. The constraint-programming machinery allows for the enumeration of all valid placements according to the given constraints. In this way, all optimal structures for a given sequence can be calculated. For details of the CSP definition and the mechanisms for solving this model see Backofen and Will ( 2006 ). Here, we describe the newest extension of the CPSP-approach, to enable the prediction of  optimal structures in side-chain HP-models . As introduced above, the side-chain model focuses on a compact placement of H-side-chain monomers. The extension consists of the formulation of a new CSP that constrains only the H-side-chains to positions in the H-core. Additionally, successive backbone monomers as well as the side-chain and backbone monomer of each amino acid have to be neighbored in the lattice. The new CSP is exploited within the original CPSP framework analogously to the standard CSP. Thus, the advanced constraint programming machinery previously developed is reused in the calculation of optimal structures in 3D side-chain HP models. 4 CONCLUSION The CPSP-web-tools are the online interface of the newest version of the CPSP-tools package (Mann  et al. ,  2008b ) including algorithms for lattice protein studies. While the CPSP-tools are made for high-throughput experiments, the web-tools focus on  ad hoc  usage for research and teaching. The user can immediately work with the CPSP-tools ‘out of the box’ which are combined with an interactive, interlinked result management system. The outcome of this is a the first web service that answers the question for optimal energy and structures, degeneracy, neutral networks and other properties in the field of HP-models in 3D lattices. Furthermore, we have presented the latest extension of the CPSP approach that enables the prediction of optimal energy and structures in the side-chain HP-model. This is the first method that is able to calculate all optimal side-chain structures of a given sequence, while proving their optimality. This extension is the base for new studies in more realistic protein models that are still computationally feasible. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>geneCo: a visualized comparative genomic method to analyze multiple genome structures</Title>
    <Doi>10.1093/bioinformatics/btz596</Doi>
    <Authors>Jung Jaehee, Kim Jong Im, Yi Gangman, Hancock John</Authors>
    <Abstract/>
    <Body>1 Introduction Comparative genomics is mainly focused on creating highly detailed visualizations of the common features between organisms. The major principle of comparative genomics is to compare basic biological similarities or differences in genomic features resulting from DNA sequences between organisms at the genetic level. Genomic features include DNA sequences, gene contents, gene order, regulatory sequences and other genomic structures. Therefore, comparative genomic approaches are needed to specifically align genome sequences and to compare genomic features among organisms. Comparative genomics also provides powerful tools to study evolutionary relationships among organisms and to identify genes that are either conserved or represent unique genomic features. Several comparative genomic tools, such as Artemis comparison tool (ACT) ( Barrell  et al. , 2005 ), Mauve ( Darling  et al. , 2004 ), BLAST ring image generator (BRIG) ( Alikhan  et al. , 2011 ) and Circos ( Schnable  et al. , 2009 ), have been developed for multiple genome assemblies. ACT visualizes comparisons of two or more genomes and is most useful for comparing a few DNA sequences, making it easy to spot and zoom in on regions of difference. This tool displays sequence similarities from various allowed input formats, such as GenBank entries or FASTA sequences. Mauve aligns whole genomes and shows output in the form of SNPs, regions of difference and homologous blocks, among others. Mauve can also be used to assess assembly quality against a reference using Mauve Contig Metrics. BRIG gives a global view of whole genome comparisons by visualizing BLAST comparisons with elaborate circular figures. BRIG is suitable for comparing multiple genomes; however, it is difficult to compare more than a dozen or so because each genome must be entered through the GUI. Circos uses plain text files for both input data and configuration, with the latter controlling the placement and format of each data track. The function to generate both data and configuration files automatically makes Circos highly amenable to incorporation in web-based database mining and visualization. Despite the many bioinformatic approaches to compare genomes, the development of tools for comparative DNA analysis remains a challenge. Since, the identification of mismatched genes that may be non-conserved sequences is also meaningful in terms of evolution. A visualization and comparative genomic tool, geneCo, is proposed to align and compare multiple genome structures resulting from user-defined data in the GenBank file format. Information regarding inversion, gain, loss, duplication and gene rearrangement among the multiple organisms being compared is provided by geneCo. Another purpose of geneCo is to provide a web-based user interface that can be comfortably used by biologists, offering easy-to-use options and displaying the results in a web browser. 2 Application 
 Figure 1  shows an overview of geneCo, including the web-based interface, user options and output results.  Figure 1a  is a screen shot of the web page. The main engine of geneCo is implemented in Python, and the web-based interface is implemented in PHP, Python, Javascript and Bootstrap ( Spurlock, 2013 ).  Figure 1b  explains the option values. User configurations and usages are described in detail in  Supplementary Data .  Figure 1c  shows the representative result for both the construction of a genome map and map comparisons. The output generated by geneCo varies in accordance with user options.
 Fig. 1. Overview of the geneCo system (geneCo web page ( a ), option values ( b ) and results of two different types in geneCo ( c )) The main functions of geneCo can be divided into two categories. The first function is ‘the construction of a genome map.’ GenBank files can be used as inputs to generate single and multiple genome maps. OrganellarGenomeDRAW ( Lohse  et al. , 2013 ) functions in a similar manner, but it generates only one gene map based on the plastid or mitochondrial sequence of the organelle. When comparing several genes, OrganellarGenomeDRAW has to generate several sets of individual data. In contrast, geneCo permits a comparison of genome maps in the order in which they appear in GenBank and the generation of outputs designed by the user that can be customized by adjusting genome lengths, intervals, output file formats and a user-defined functional category of configuration files. The second function is a ‘genome map comparison’ between genes from two different genomes based on genomic input sequences in the GenBank flat file format, which compares the matched genes of both genomes with genes that exist only in a single genome. Currently, existing tools either manually construct and compare each genome structure or do not include gene annotation, making it difficult to distinguish between matched or mismatched genes in genome structures at a glance. In contrast, geneCo takes GenBank input files and arranges them as left to right genome pairs according to the order of the input files. Thus, each step in  n -loops compares a pair of Genbank files from the left to right genomes and draws them in accordance with the input settings. The comparative genomic method is a modified local sequence alignment based on the dynamic programming algorithm to align each matched gene name. This method compares the genes from two genomes in the order defined by the user and analyzes missed genes that are not conserved and the genomic features of different genomes in terms of their biological similarities and genetic levels. Matched and mismatched genes are distinguished and identified by geneCo. In addition, geneCo will also enable biologists to repeatedly change input parameters in order to return desired outputs, especially when there is a large amount of genome data to be analyzed. The geneCo method is described in detail in  Supplementary Section S2 . Multiple genome maps can be created by geneCo using the title and input files with additional gene alignment options. The final output is immediately generated in a web browser and supports various vector types as outputs. In addition, different display options for users who want increased precision for adjustments and greater customization based on their preferences are supported. For example, users can either set the color of the map to the default option or change it manually. Furthermore, users can define keywords and set functional categories to match the keywords using various colors to improve visualization. The legend in the output shows the specified functional categories. For analysis within a specific range, users can specify the start and end base-pair in the output using the zoom-in option. Moreover, geneCo also supports various output options for almost all objects. 3 Evaluation Tools, such as BRIG, Mauve, ACT and Circos, can be used for comparative genomics. BRIG selects several query genomes in FASTA, GenBank, or EBML formats and selects only one as a reference and then compares the other genomes against the reference. However, geneCo can set multiple references that are determined by order of the uploaded GenBank files so that mismatched genes between two compared genomes are easily found. To evaluate the performance of geneCo, several mitochondria, nucleomorph chromosome 1 and plastid genomes from multiple species were used as the test dataset ( Supplementary Tables S2 and S3 ).  Supplementary Figures S2–S9  show geneCo outputs with different input options.  Supplementary Tables S4 and S5  show the results obtained with other applications. The most important key feature of geneCo is the identification of mis-matched genes found by comparing two related genomes. Further details are described in the  Supplementary Data . 4 Conclusion Comparative genomics aims to find the common function between genomes to study the evolution of the genome. This study requires tools for comparing and visualizing of genomes. The proposed geneCo method is implemented as a Python-based software that can compare and analyze various genome maps. In the past, users have had to construct individual gene maps manually to compare genome structures. With geneCo, users can easily compare and analyze the position of genes, find common genes between other genomes, and find genes that exist only in one genome using GenBank files as input data with user-defined settings. Various options are available for visualizing elaborate genome structures and generating results specific to the objective of the user. Funding This research was supported by the National Research Foundation (NRF) of Korea funded by the Ministry of Science, ICT &amp; Future Planning, Basic Science Research Program [MSIP; NRF-2016R1C1B1007929] to J.J.; the Ministry of Education [2018R1D1A1B07050727] to J.I.K.; [NRF-2016R1D1A1A09919318, NRF-2019R1F1A1064019] to G.Y. 
 Conflict of Interest : none declared. Supplementary Material btz596_Supplementary_Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>iMapper: a web application for the automated analysis and mapping of insertional mutagenesis sequence data against Ensembl genomes</Title>
    <Doi>10.1093/bioinformatics/btn541</Doi>
    <Authors>Kong Jun, Zhu Fei, Stalker Jim, Adams David J.</Authors>
    <Abstract>Summary: Insertional mutagenesis is a powerful method for gene discovery. To identify the location of insertion sites in the genome linker based polymerase chain reaction (PCR) methods (such as splinkerette-PCR) may be employed. We have developed a web application called iMapper (Insertional Mutagenesis Mapping and Analysis Tool) for the efficient analysis of insertion site sequence reads against vertebrate and invertebrate Ensembl genomes. Taking linker based sequences as input, iMapper scans and trims the sequence to remove the linker and sequences derived from the insertional mutagen. The software then identifies and removes contaminating sequences derived from chimeric genomic fragments, vector or the transposon concatamer and then presents the clipped sequence reads to a sequence mapping server which aligns them to an Ensembl genome. Insertion sites can then be navigated in Ensembl in the context of genomic features such as gene structures. iMapper also generates test-based format for nucleic acid or protein sequences (FASTA) and generic file format (GFF) files of the clipped sequence reads and provides a graphical overview of the mapped insertion sites against a karyotype. iMapper is designed for high-throughput applications and can efficiently process thousands of DNA sequence reads.</Abstract>
    <Body>1 INTRODUCTION Retroviral-based insertional mutagenesis screens in mice have been a valuable tool for the discovery of oncogenes and tumor suppressors in mice (Mikkers and Berns,  2003 ), and also for gene discovery in cultured cells (Du  et al. ,  2005 ). More recently transposon-based approaches such as the use of the  Tc1 -family transposon  Sleeping Beauty  (Collier  et al. ,  2005 ; Dupuy  et al. ,  2005 ) and the  Trichoplusia -derived transposon  Piggybac  (Wang  et al. ,  2008 ) have been developed increasing the repertoire of insertional mutagens available as gene discovery tools in mammals. To determine where in the genome an insertional mutagen has inserted the usual approach is to use a linker based polymerase chain reaction (PCR) method, such as vectorette or splinkerette (Devon  et al. ,  1995 ). For any insertional mutagenesis screen to cover a significant proportion of the genome, it is desirable to perform a screen using hundred of mice, and hundreds if not thousands of cell clones. Thus insertional mutagenesis screens may involve the generation and analysis of tens of thousands of DNA sequence reads from insertion sites. Although linker based PCR methods are generally specific, non-specific PCR products, chimeric sequences and sequences derived from transposon concatemeric arrays can all represents contaminating sequences within pools of insertion site PCR products. Thus without careful processing of DNA sequence data, the direct mapping of insertion site sequence reads to the genome may result in the identification of false-positive insertion sites. To facilitate the analysis of linker mediated insertion site sequences, we have developed a web application called  iMapper  (Insertional Mutagenesis Mapping and Analysis Tool). Using linker based PCR sequence reads as input  iMapper  uses a local sequence alignment algorithm to identify a tag sequence derived from the end of the insertional mutagen ( Supplementary Material ).  iMapper  then scans the downstream sequence for user defined contaminating sequences, processes the sequence to identify the restriction site sequence used for linker ligation during the insertion site PCR, clips out the genomic sequence between the tag and first restriction enzyme cutting site and presents this sequence to a rapid mapping algorithm called sequence search and alignment by hashing algorithm (SSAHA) (Ning  et al. ,  2001 ). Output is then generated in various formats. The main features of  iMapper  include:
 Efficient and accurate processing of insertion site sequence data and analysis against Ensembl human, mouse, rat, zebra-fish,  Drosophila  and  Saccharomyces cerevisiae  genomes. Output of annotated sequence reads in tabular format with links to Ensembl  ContigView  so that insertion sites can be viewed in the context of gene structures and other genomic features. Output of processed sequence data in test-based format for nucleic acid or protein sequences (FASTA) and generic file format (GFF) allowing insertion site sequence data to be analyzed in any sequence analysis package and displayed as a distributed annotation system (DAS) track against an Ensembl genome. Output of a graphical chromosome ‘ KaryoView ’ showing insertion sites against an ideogram of each chromosome. 2 METHODS 2.1 Architecture The  iMapper  interface is web-based ( Supplementary Material ). The sequence analysis module within  iMapper  uses Perl and computer generated imagery (CGI), and for comparison to an Ensembl genome Perl scripts run against the Ensembl application programming interface (Ensembl-API). 2.2 Input format Sequence is imported into  iMapper  in FASTA format. Sequence data in this format can either be pasted into the text box or imported using the file upload option ( Supplementary Material ). 2.3 User defined parameters After sequence input, a user can define the species against which they would like their insertion site data analyzed from human, mouse, rat, zebrafish,  Drosophila  and  S. cerevisiae . The orientation of the tag in the sequence can then be chosen, and the output option selected. Selecting ‘output good sequences’ will exclude those sequences that do not contain the tag sequence, sequences that do not map to the genome, and sequences that are identified to contain contaminating sequences from all output formats. The sequence of the mutagen ‘tag’ sequence can then be specified, or a prevalidated tag sequence can be selected from the drop down menu. We provide tag sequences for the transposons  Sleeping Beauty  and  Piggybac , and for the U3LTR of the MuLV retrovirus. The sequence of the restriction site can then be specified. At this point, the user has defined the boundaries of the sequence which will be mapped to the genome as the sequence between the tag and the restriction site or linker. Advanced options can then be specified. These include the tag alignment parameters and the sequence of contaminating sequences, which will be highlighted in the tabular format ( Supplementary Material ). It is also possible to specify the parameters used by the SSAHA algorithm for matching the genomic sequence between the tag and the restriction site or linker, to the genome. Finally, it is possible to specify the criteria for ‘gene overlaps’. By specifying ‘gene overlaps’, it is possible to vary the spatial criteria for defining what constitutes an insertion event in or near a gene. For example, it may be desirable to identify insertion events that mutate in or upstream of a gene, but not downstream. Sequence processing commences when the ‘Submit Query’ button is selected. 2.4 Sequence processing The procedure used by the code for sequence processing is shown in the  Supplementary Material . 2.5 Tabular format The tabular format is an html-based output of the analyzed sequence data ( Fig. 1 ). Sequences such as the tag sequence, the restriction site and contaminating sequence are highlighted in this view. Links to Ensembl gene pages and to the Ensembl  ContigView  are also provided from this page.
 Fig. 1. iMapper  maps and analyzes insertion site sequence data.  iMapper  generates output in several formats including a tabular format with links to Ensembl  ContigView  and Ensembl gene pages (upper panel), GFF file format, FASTA format and a  KaryoView  format (lower panel) which provides a global overview of all insertion site sequences that were mapped to the genome. Sequence traces displayed in the tabular format are annotated to show the location of the tag sequence (green), the restriction enzyme site (orange) and the mapped genomic sequence (yellow). An alignment of the tag sequence from the trace and the insertion site mapping location is also shown in the tabular format. In the  KaryoView  format, each red triangle indicates an independent insertion site displayed against a genome ideogram. 2.6 FASTA and GFF formats A FASTA file of the processed traces and a GFF file of the data are provided when the analysis run is complete. 2.7  KaryoView To obtain a global overview of the sequence data,  iMapper  has a link to an Ensembl  KaryoView  providing a graphical view of the data against a chromosomal ideogram ( Fig. 1 ). 2.8 Performance The specificity of tag identification depends on the length of the tag sequence entered, and the predefined thresholds specified for sequence tag identification including the percentage alignment threshold, gap penalty, match and mismatch score. Longer tag sequences, higher alignment percentages and more stringent gap and mismatch scores will result in more accurate tag sequence identification. We have tested the optimal tag sequence length and percentage threshold using a dataset of 1920  PiggyBac  insertion site sequence reads (Wang  et al. ,  2008 ). Because  PiggyBac  integrations invariably occur at ‘TTAA’ sites a precisely identified tag sequence will always be followed by the sequence TTAA. As shown in the  Supplementary Material , the minimal advisable tag sequence length is 15 bp. We determined the optimal percentage threshold for sequence tag identification, to be used as the default, and determined this to be 80% ( Supplementary Material ). Finally, we optimized the SSAHA sequence mapping parameters to be used as the default finding that for sequences from splinkerette-PCR reactions containing genomic junction fragments of on average 200 bp in length the optimal SSAHA score is 35. This score should be ideal for insertion site sequences generated by capillary read sequencing but may need to be lowered to 20 for shorter reads such as those generated by 454 sequencing. It is advisable to optimize the SSAHA mapping score for each dataset selecting a score that generates the highest number of uniquely mapped reads. This is important because the default mapping parameters used by  iMapper  are stringent and will return only those reads that map to unique unambiguous genomic locations. We have used  iMapper  to analyze up to 20 000 DNA sequence traces. It takes, on average, 1–2 s for  iMapper  to analyze each DNA sequence trace ( Supplementary Material ) and to return the analyzed data in tabular,  ContigView , GFF, FASTA and  karyoView  formats. 3 SUMMARY iMapper  is a web-based freely accessible solution for the analysis of insertional mutagenesis datasets and should facilitate the many insertional mutagenesis screens that are ongoing worldwide. Funding : Cancer Research-UK (C20510/A6997) and the Wellcome Trust (76943 to D.J.A.); Wellcome Trust Sanger Institute PhD programme (to J.K. and F.Z.). Conflict of Interest : none declared. 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>A system for generating transcription regulatory networks with combinatorial control of transcription</Title>
    <Doi>10.1093/bioinformatics/btn126</Doi>
    <Authors>Roy Sushmita, Werner-Washburne Margaret, Lane Terran</Authors>
    <Abstract>Summary: We have developed a new software system, REgulatory Network generator with COmbinatorial control (RENCO), for automatic generation of differential equations describing pre-transcriptional combinatorics in artificial regulatory networks. RENCO has the following benefits: (a) it explicitly models protein–protein interactions among transcription factors, (b) it captures combinatorial control of transcription factors on target genes and (c) it produces output in Systems Biology Markup Language (SBML) format, which allows these equations to be directly imported into existing simulators. Explicit modeling of the protein interactions allows RENCO to incorporate greater mechanistic detail of the transcription machinery compared to existing models and can provide a better assessment of algorithms for regulatory network inference.</Abstract>
    <Body>1 INTRODUCTION With the increasing availability of genome-scale data, a plethora of algorithms are being developed to infer regulatory networks. Examples of such algorithms include Bayesian networks, ARACNE (Bansal  et al. ,  2007 ). Because of the absence of “ground truth” of regulatory network topology, these algorithms are evaluated on artificial networks generated via network simulators (Kurata  et al. ,  2003 ; Margolin  et al. ,  2005 ; Mendes  et al. ,  2003 ; Schilstra and Bolouri,  2002 ). Since gene regulation is a dynamic process, existing network simulations employ systems of ordinary differential equations (ODEs) that describe the kinetics of mRNA and protein concentrations as a function of time. Some approaches construct highly detailed models, but require large amounts of user-specified information (Kurata  et al. ,  2003 ; Schilstra and Bolouri,  2002 ). Other approaches generate large networks but use simpler models by making the mRNA concentration of target genes dependent upon mRNA concentration, rather than on protein concentration of transcription factors (Mendes  et al. ,  2003 ). In real biological systems, protein expression does not correlate with gene expression, especially at steady state, due to different translation and degradation rates (Belle  et al. ,  2006 ). These approaches also do not model protein interactions edges and, therefore, combinatorics resulting from these interactions. We describe a regulatory network generator, RENCO, that models genes and proteins as separate entities, incorporates protein–protein interations among the transcription factor proteins, and generates ODEs that explicitly capture the combinatorial control of transcription factors. RENCO accepts either pre-specified network topologies or gene counts, in which case it generates a network topology. The network topology is used to generate ODEs that capture combinatorial control among transcription factor proteins. The output from RENCO is in SBML format, compatible with existing simulators such as Copasi (Hoops  et al. ,  2006 ) and RANGE (Long and Roth,  2007 ). Time-series and steady-state expression data produced from the ODEs from our generator can be leveraged for comparative analysis of different network inference algorithms. 2 TRANSCRIPTIONAL REGULATORY NETWORK GENERATOR RENCO works in two steps: (a) generate/read the network topology and (b) generate the ODEs specifying the transcription kinetics (see RENCO manual for details). For (a) proteins are connected to each other via a scale-free network (Albert and Barabasi,  2000 ), and to genes via a network with exponential degree distribution (Maslov and Sneppen,  2005 ). 2.1 Modeling combinatorial control of gene regulation We model combinatorial control by first identifying the set of cliques,  , up to a maximum of size  t  in the protein interaction network. Each clique represents a protein complex that must function together to produce the desired target regulation. A target gene,  g i  is regulated by  k  randomly selected such cliques, where  k  is the indegree of the gene. These  k  cliques regulate  g i  by binding in different combinations, thus exercising combinatorial gene regulation. We refer to the set of cliques in a combination as a  transcription factor complex  (TFC). At any time there can be several such TFCs regulating  g i . The mRNA concentration of a target gene is, therefore, a function of three types of regulation:  within-clique ,  within-complex  and  across-complex  regulation. Within-clique regulation captures the contribution of one clique on a target gene. The within-complex regulation captures the combined contribution of all cliques in one TFC. Finally, the across-complex regulation specifies the combined contribution of different TFCs. We now introduce the notation for ODEs generated by RENCO.  M i  ( t ) and  P i ( t ) denote the mRNA and protein concentrations, respectively, of gene  g i , at time  t .  V i M  and  v i M  denote the rate constants of mRNA synthesis and degradation of   and   denote the rate constants of protein synthesis and degradation. C ij  and T ij  denote a protein clique and a TFC, respectively, associated with  g i . Q i  denotes the set of TFCs associated with  g i .  X ij ,  Y ij  and  S i  specify the within-clique, within-complex and across-complex regulation on  g i . Based on existing work (Mendes  et al. ,  2003 ; Schilstra and Bolouri,  2002 ), the rate of change of mRNA concentration is the difference of synthesis and degradation of  . Similarly for protein concentration,  . The across-complex regulation,  S i  is a weighted sum of contributions from |Q i | TFCs:  , where  w q  denotes the TFC weight. The sum models ‘or’ behavior of the different TFCs because all TFCs need not be active simultaneously. The within-complex regulation,  Y ij  is a product of within-clique actions in the TFC T ij ,  . The product models ‘and’ behavior of a single TFC because all proteins within a TFC must be active at the same time. Finally, the cliques per gene C ij  are randomly assigned activating or repressing roles on  g i . If C ij  is activating, 
 
otherwise, 
 
 Ka ip  and  Ki ip  are equilibrium dissociation constants of the  p th activator or repressor of  g i . All degradation, synthesis and dissociation constants are initialized uniformly at random from [0.01, V max ], where  V max  is user specified. 3 EXAMPLE NETWORK We used RENCO to analyze : (a) mRNA and protein steady-state measurements and (b) combinatorial gene regulation, in a small example network ( Supplementary Material  has details). 3.1 Importance of modeling protein expression The example network has five genes and five proteins ( Fig. 1 a). The gene  G 4  is regulated via different combinations of the cliques { P 2 },{ P 0 , P 1 }. We find that the wild-type time courses of individual mRNA expressions are correlated with corresponding proteins ( Fig. 1 b and c). But because different genes and proteins have different degradation and synthesis rate constants, the mRNA population as a whole does not correlate with the protein population (Spearman's; correlation =0.3). Because of the dissimilarity in the steady-state mRNA and protein expression populations, genes appearing to be differentially expressed at the mRNA level may not be differentially expressed at the protein level. This highlights the importance of modeling mRNA and protein expression as separate entities in the network.
 Fig. 1. ( a ) Example network. Dashed edges indicate regulatory actions. Wild-type gene ( b ) and protein ( c ) time courses. 3.2 Combinatorics of gene regulation We analyzed combinatorial control in our network by generating the  G 4  time course under different knockout combinations of the  G 4  activators,  P 0 , P 1  and  P 2  ( Fig. 2 ). Because all the regulators are activating,  G 4  is downregulated here compared to wild-type. We note that each knock out combination yields different time courses. In particular, knocking out either  G 0  or  G 1  in combination with  G 2  is sufficient to drive the  G 4  expression to 0. This phenomenon is because of the clique,  P 0 , P 1 . This illustrates a possible combinatorial regulation process to produce a range of expression dynamics using a few transcription factors.
 Fig. 2. G 4  time course under knock out combinations of  G 0 ,  G 1  and  G 2 . 4 CONCLUSION We have described RENCO, a generator for artificial regulatory networks and their ODEs. RENCO models the transcriptional machinery more faithfully by explicitly capturing protein interactions and provides a good testbed for network structure inference algorithms. 
 Supplementary Material 
 
 
 [Supplementary Data] 
 
 
 
 
 
 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>RidgeRace: ridge regression for continuous ancestral character estimation on phylogenetic trees</Title>
    <Doi>10.1093/bioinformatics/btu477</Doi>
    <Authors>Kratsch Christina, McHardy Alice C.</Authors>
    <Abstract>Motivation: Ancestral character state reconstruction describes a set of techniques for estimating phenotypic or genetic features of species or related individuals that are the predecessors of those present today. Such reconstructions can reach into the distant past and can provide insights into the history of a population or a set of species when fossil data are not available, or they can be used to test evolutionary hypotheses, e.g. on the co-evolution of traits. Typical methods for ancestral character state reconstruction of continuous characters consider the phylogeny of the underlying data and estimate the ancestral process along the branches of the tree. They usually assume a Brownian motion model of character evolution or extensions thereof, requiring specific assumptions on the rate of phenotypic evolution.</Abstract>
    <Body>1 INTRODUCTION Many biological studies investigate the ancestral states of one or several discrete and continuous characters on a phylogenetic tree ( Felsenstein, 2004 ). Comparative methods correlate the evolution of alleles at different loci with each other or with a trait and thus often require the reconstruction of ancestral values. Typical examples for discrete characters are the reconstruction of the absence, presence or state of genes or traits for the internal nodes of the tree (i.e. for the ancestral organisms), while typical examples for continuous characters are environmental preferences of different species, measures of morphology or physiology or behavioral or metabolic properties ( Nunn, 2011 ). Such reconstructions are also of interest when fossil records cannot be retrieved, when the phenotype of interest cannot be determined from the fossil tissue or when studying the evolution of a gene family across different environmental conditions. Statistical approaches such as regression or correlation can fail to estimate correlations between traits correctly when they assume that closely related species are statistically independent ( Felsenstein, 1985 ;  Grafen, 1989 ;  Harvey and Pagel, 1991 ). Comparative methods account for such dependencies by including estimates of the phylogeny underlying the data into their predictions. In the case of continuous characters, most of these techniques are based on a simple model assuming neutral evolution of the respective character. The Brownian motion (BM) model ( Felsenstein, 1985 ) assumes that the trait of a leaf node in a phylogeny develops as a random walk starting from the ancestral root. The duration of that walk—and therefore the change and variance in the trait—is assumed to be proportional to the change in branch length covered between those two nodes. At each inner node, the random walk bifurcates, creating two dependent processes and thus defining a stochastic distribution for all leaves. Several methods that apply the BM model are available to reconstruct continuous ancestral characters, and implemented in widely used software packages, such as APE ( Paradis  et al. , 2004 ), Geiger ( Harmon  et al. , 2008 ), Phytools ( Revell, 2012 ), Mesquite ( Maddison and Maddison, 2011 ), BayesTrait/Continuous ( Pagel, 1999 ), PAUP* ( Swofford, 2003 ) and Contml ( Felsenstein, 1993 ). One of the simplest ways to reconstruct a continuous ancestral character state was established with Felsenstein’s algorithm for ‘Phylogenetic Independent Contrasts’ ( Felsenstein, 1985 ). In the Phylogenetic Independent Contrasts algorithm, ancestral values are computed recursively as the weighted average of their child values, with the weights set according to the distance (i.e. the branch length) of these children. In addition, branch lengths leading to reconstructed internal nodes are rescaled to account for the uncertainty of the reconstruction. This leads to a maximum likelihood estimation of the ancestral value for the root node alone. Other algorithms estimate the values for the whole tree by re-rooting or by a squared change maximum parsimony approach ( Felsenstein, 2004 ;  Schluter  et al. , 1997 ). Linear regression allows another framework to formulate the reconstruction of ancestral character states, and generalized least squares has been suggested as a technique to reconstruct ancestral values as a weighted average of the values of all extant species, while taking the correlation structure described by the phylogenetic tree into account ( Martins and Hansen, 1997 ). This approach is particularly flexible, as it allows detailed assumptions of the evolutionary process by inclusion of an appropriate covariance matrix. Several possible extensions and modifications of the BM process have been suggested and can be considered for ancestral character state reconstruction. For example, the Ornstein–Uhlenbeck (OU) process models adaptation explicitly by defining a single global optimum ( Hansen, 1997 ) or several local optima ( Butler and King, 2004 ) of directional selection. A major restriction of the BM and the OU models, noted in, for example,  Harmon  et al.  (2010) , is the assumption of a constant rate of trait variation throughout the underlying phylogeny. The early burst model ( Blomberg  et al. , 2003 ;  Freckleton and Harvey, 2006 ) offers an alternative that decreases the rate of evolution exponentially through time, and describes a process of adaptive radiation [ Harmon  et al.  (2010)  provide a detailed comparison]. The ACDC model ( Blomberg  et al. , 2003 ) describes a process of accelerating versus decelerating rates of character evolution toward an optimum, i.e. a combination of OU and early burst. Several methods allow us to estimate BM rates ( Garland, 1992 ;  Lynch, 1990 ;  Martins, 1994 ), to test for variations in that rate or use different rates in different parts of the tree ( McPeek, 1995 ;  O’Meara  et al. , 2006 ;  Revell, 2008 ) or to suggest global branch length transformations in the phylogeny to account for variable rates ( Blomberg  et al. , 2003 ;  Pagel, 1999 ). However, most of the phylogenetic methods aforementioned do not explicitly aim to reconstruct ancestral character states. They are originally concerned with correlations between two or more traits [‘phylogenetic regression’,  Grafen (1989) ;  Felsenstein (2004) ] or with tests for deviation from the assumption of a globally constant rate, and they do not suggest how to infer ancestral character states for more complex scenarios. In addition, critical studies note that deciding on the correct model might be difficult and warn of over-interpreting the phylogenetic patterns ( Blomberg  et al. , 2003 ;  Boettiger  et al. , 2012 ;  Losos, 2011 ;  Price, 1997 ). 2 APPROACH We here describe RidgeRace (Ridge Regression for Ancestral Character Estimation), a new and simple method inspired by the least-squares optimization technique of  Cavalli-Sforza and Edwards (1967)  for the inference of branch weights in a phylogeny via pairwise distances. RidgeRace does not assume certain rates at certain regions of the phylogeny or a particular model of rate change over time. It treats phenotypic measurements at the terminal nodes of a phylogeny as sample observations and relies on a linear regression with L2-Norm regularization, allowing phenotypic rates to vary at each branch. It estimates branch-wise rates and ancestral characters simultaneously, in a way that best describes the phenotypes observed at the terminal nodes. In an extensive simulation study, we evaluated different variations of BM on randomly created trees and show that our method performs equally well as or better than established implementations of state-of-the-art reconstruction algorithms. We suggest using RidgeRace in studies aiming to reconstruct ancestral character states of continuous characters when no definite assumptions can be made about the type of evolutionary process, or when the assumption of a model for phenotypic evolution is not appropriate at all. The latter might, for example, be the case in studies that rely only on a hierarchical clustering of samples instead of phylogenies. Branch weights inferred by the ridge regression based on phenotype measurements can be interpreted as rates of phenotypic change (i.e. phenotypic rates) and provide insights into particularly interesting areas of the phylogeny. They can also be used to judge the phenotypic impact of genetic changes or other types of events associated with branches within the phylogeny [see, for example,  Steinbrück and McHardy (2012) ]. To demonstrate a possible application of RidgeRace integrating phenotypic and genotypic data, we studied an ovarian cancer dataset, created by the Cancer Genome Atlas research network and recently analyzed with network-based stratification ( Hofree  et al. , 2013 ). 3 METHODS RidgeRace estimates ancestral character states on a phylogenetic tree. As in the original BM model, we consider the leaf values to be the result of a weighted sum of intermediate contributions  g i  created along the tree, beginning at the root ( Fig. 1 ). The contributions represent the gain or loss in character value on each branch of the tree so that, for example, the character value of sample  y 4  can be described as follows:
 
where  a ,  b  and  c  represent the branches in the tree, and  g 0  holds a bias term representing the original contribution of the root node. The contribution  g j  of a single branch  j  can be seen as being analogous to the formulation of BM: the gain or loss in the phenotype is dependent on the length  l j  of branch  j  and the speed  β j  of the process, in analogy to the variance term  σ 2  in the BM model:
 
One can then write the solution for the vector of leaf phenotypes  y  in matrix form:
 (1) 
where
 
and β is a vector with a length equal to the number of branches in the phylogeny, including a single virtual branch above the root to account for its original contribution  g 0 . This scheme is overparameterized, as it adds a parameter for each inner branch, and only considers one sample observation for each terminal node. However, it also allows the inclusion of measurements at inner nodes (e.g. from fossil records), and it is suitable for accounting for multiple measurements at single leaf nodes. Such samples can be added by appending rows to  y  and  L .
 Fig. 1. Model of phenotype evolution on a phylogenetic tree. The observed continuous character values at the nodes  y i  are the result of a sum of contributions on ancestral branches. A virtual branch ‘above’ the root node  x 1  contributes the global phylogenetic mean, i.e. the ancestral state of  x 1 Ridge regression  is a simple extension of ordinary least squares regression. As ordinary least squares, ridge regression also aims to minimize the squared error term, but adds a quadratic regularization penalty on large values of the weight vector  β . A tuning parameter  λ  controls the relative impact of both terms. The regularization does not only help to reduce the variance of the model, it also acts as an integrated parameter selection method for overparameterized models [see  Gareth  et al.  (2013)  for details]. We here use ridge regression to estimate a vector  β ^  that explains the known observations  y  best:
 (2) 
The textbook solution ( Gareth  et al. , 2013 ;  Hastie  et al. , 2009 ) to this optimization problem is as follows:
 (3) 
 Equation (2)  shows how the optimization balances the leaf reconstruction error versus the quadratic term that penalizes large variance in the phenotypic rates. A trivial but undesirable solution to the optimization would set the gain at each terminal branch equal to the according terminal node value, leaving all other gains empty and making ancestral reconstruction impossible. We here use quadratic regularization instead of L1 regularization. The latter penalizes the absolute value of the weight vector  β , driving single weights toward zero. This would correspond to a phylogeny with many phenotypic rates at zero and only few branches with rates of high absolute values, describing a rather implausible model for phenotypic evolution. For a given estimate of  β ^  as defined in  Equation (3) , the vector  a ^  containing the phenotypic reconstruction of all inner nodes can then be computed in analogy to  Equation (1) :
 (4) 
where
 
 This formulation is similar to the generalized least squares method proposed by  Martins and Hansen (1997) . Similarly they suggest inferring ancestral character states as the weighted average of leaf contributions, with weights assigned according to the covariance between an ancestor and a leaf [Equation (10) in  Cunningham  et al.  (1998) ;  Martins and Hansen (1997) ]:
 
where the covariance between an inner node  a  and a leaf node  y  is defined as  σ 2 t ( a , y ) , with  t ( a , y )  being the distance between the root of the tree and the most recent common ancestor of  a  and  y . RidgeRace differs in the sense that it allows us to estimate a weight  β j  for each branch instead of assuming a constant rate  σ 2  or, more generally, the predefined covariances between nodes. Extensions of the generalized least squares approach under the BM model use more complex matrices  W . However, the design of  W  has to be defined in advance based on specific model assumptions, whereas RidgeRace is able to estimate rates independently. An important assumption of linear regression is that the standard deviations of the error terms are constant and do not depend on the covariates (here: the branch lengths). This assumption is violated under the BM model, as leaf nodes with a long distance to the root will have a high variance in their trait value and phenotypic measurements will produce larger errors at these nodes compared with the predicted value. The estimation of  β  might thus be biased if the depth of single leaf nodes is large compared with the rest of the tree. We therefore recommend RidgeRace for approximately balanced trees. 3.1 Estimation of the regularization weight The regularization weight parameter  λ  in  Equation (2)  balances the impact of accuracy at the leaves versus the complexity of the model and variance of  β . To find the optimal value of  λ , we performed a leave-one-out iteration over all leaves of the tree. To estimate the goodness of fit of a particular  λ 0 , we iteratively removed a single leaf  x  from the tree, estimated  β  on the remaining tree and used the rate of the branch leading to the parent of  x  as an approximation of the branch rate of the missing node. The leave-one-out error for  x  is defined as the squared difference between the inferred phenotypic value for  x  and the actual value according to the input data. The leave-one-out error for a particular  λ 0  is the sum over all leave-one-out errors for all leaves. Iterating  λ 0 ∈ { 10 − 6 , 10 − 4 , … , 10 + 2 } , we selected the final  λ  to be the one that minimized the leave-one-out error. 3.2 Simulation study We created random trees with an increasing number  N  of leaves using the function rtree in the R-package APE ( Paradis  et al. , 2004 ;  R Core Team, 2012 ). We simulated BM with variation  σ 2  along the branches of the tree, resulting in a character assignment for each inner or leaf node. The process was repeated several times for different trees and different values for the parameters  σ 2  and  N .  Supplementary Text S1  provides details on the simulation algorithm and the parameter settings. The random tree and the simulated values obtained at the leaf nodes were provided as input to RidgeRace and to implementations of the maximum likelihood and generalized least squares algorithms ( Felsenstein, 1985 ;  Hansen, 1997 ) in the APE package for ancestral character state estimation ( Paradis  et al. , 2004 ). The reconstructed values thus obtained were mapped back to the inner nodes of the tree and compared with the simulated ones (leaf nodes were excluded from the comparison), and the mean squared error was computed for evaluation. 3.3 Cancer study A binary matrix describing the absence or presence of non-synonymous mutations in 9850 genes for 325 patients was taken from the  supplementary data  of an ovarian cancer dataset provided by  Hofree  et al.  (2013) . Analogous to the description of the authors in their article and  Supplementary Material , we used their network-based stratification software ( NBS , version 0.2, available at  http://idekerlab.ucsd.edu ) with four clusters, the HM network and default parameters, creating 1000 bootstrap samples. We then inferred a hierarchical clustering (average linkage) on the bootstrap similarity matrix using the methods provided in the scripts of the authors. We used this inferred topology as the input tree for RidgeRace. We then downloaded information on each patient’s survival time from the TCGA database ( Cancer Genome Atlas Research Network, 2011 ). Treating a patient’s survival time as a ‘trait’ of each patient, phenotypic rates were inferred with RidgeRace as described above. The binary genetic profile of each patient was then mapped to the leaf nodes and reconstructed to inner nodes with the Sankoff algorithm implemented in RidgeRace, using a simple 0/1 cost matrix and the ACCTRAN principle in case of ambiguities ( Felsenstein, 1985 ). Changes in the genetic profiles of neighboring nodes were then reconstructed on the branches of the tree. Finally, the tree was visualized using FigTree ( Rambaut, 2013 ). 4 RESULTS 4.1 Simulation study Our evaluation of RidgeRace on data consisting of randomly drawn phylogenetic trees and continuous ancestral characters states created by a simulated BM process showed that the method performs similarly or better than other state-of-the-art techniques. We compared the performance of RidgeRace with generalized least squares ( Hansen, 1997 ) and REML ( Felsenstein, 1985 ), and simulated ancestral character evolution in a BM setting.  Figure 2  shows that all three methods were able to reconstruct ancestral states well, achieving low mean squared errors, even for small trees or high variation values. The variation of the mean squared error of all three methods is large to observe statistically significant differences in the methods; however, on average, RidgeRace performed similar or better in our simulation than the two alternative methods.
 Fig. 2. Mean squared error between the inferred ancestral characters and the true simulated values, when using maximum likelihood reconstruction (yellow), generalized least squares (red) and RidgeRace (light blue). The plot shows ( a ) the dependence of performance on the standard deviation  σ  of the BM process or ( b ) performance when increasing the number of leaf nodes in the tree To show another practical application of the RidgeRace method, we mapped the inferred rates  β  to their associated branches using an arbitrary random tree from the simulation.  Figure 3  shows such a tree with 25 leaves and BM simulated in three different regimes that have the internal rate parameters  σ i  of 5.3, 1.3 and 2.3. Simulated phenotypic values are shown as node labels. The correlation coefficient between the simulated ancestral states and those inferred by RidgeRace was  r  = 0.988. The inferred phenotypic rates vector  β  was plotted at the branches, and the branches were colored according to the relative size of these rates, with blue branches indicating strongly negative weights, red branches indicating strongly positive weights and gray branches indicating weights close to zero. Large changes in the phenotype value mainly occurred in Regime I, which features the largest  σ  parameter. One can observe that the inferred phenotypic rate is large when the absolute change in phenotypic value is large compared with the length of the associated branch. Therefore, plotting the inferred phenotypic rates to the phylogeny can be useful when studying the evolution of a population or a set of species. It will visualize regions in the tree that are associated with rapid phenotypic evolution.
 Fig. 3. Reconstruction of the phenotypic rates  β  along the branches of a random tree with 25 leaves, simulated with three regimes and a hypothetical phenotypic trait that resulted from a BM process with original mean zero and standard deviations  σ I = 5.3 ,  σ I I = 1.3  and  σ I I I = 2.3  in regimes I, II and III. The inferred rates visualize the speed of phenotypic evolution from strongly decreasing (red) to strongly increasing (blue). Absolute phenotypic rates are clearly largest in the regime with the highest  σ  parameter 4.2 Application to ovarian cancer data According to the World Health Organization, cancer is a leading cause of disease-related deaths worldwide and was responsible for 7.6 million deaths in the year 2008 ( WHO, 2013 ). The disease is the result of a complex interplay of genetic preconditions, external influences and interactions with the immune system ( Hanahan and Weinberg, 2000 ,  2011 ). For a wide variety of cancer types, recent studies have identified genes that are significantly associated with cancer risk, onset and progression (e.g.  Cancer Genome Atlas Research Network, 2008 ,  2012a ,  b ,  2013 ;  Kandoth  et al. , 2013 ). Hofree  et al.  (2013)  argue that somatic mutations are likely to contain the causal driver events of tumor progression, and that this type of data provides a promising source of information to identify clinically relevant subclusters. Such subclusters are identified with methods that find groups of samples with significant differences in their allele frequency profile, a process described as  stratification .  Network - based stratification  is a new clustering method that smooths the sparse and diverse genetic profiles with the help of gene interaction networks ( Hofree  et al. , 2013 ), and the authors show that it produces clinically meaningful clusterings. We used a dataset and the software provided by the authors to reconstruct a hierarchical clustering on somatic mutation data of ovarian cancer samples. We thus created a tree structure showing similarities in the genetic profiles of the tumors of ovarian cancer patients ( Fig. 4 ). The tree structure may be error-prone because of the high diversity of genetic aberrations in tumors, but their main branches closest to the root are likely to represent biologically meaningful subclusters (see also argumentation in  Hofree  et al. , 2013 ).
 Fig. 4. Application of RidgeRace to a hierarchical clustering on somatic mutations inferred for an ovarian cancer dataset. Colors on the side of the tree indicate the subtypes inferred with network-based stratification ( Hofree  et al. , 2013 ). Branches are colored according to the phenotypic rate parameter  β ; the thickness of branches is proportional to the number of nodes below them. Branches leading directly to leaf nodes were colored gray for improved visibility. Labels  m1  to  m5  indicate branches with strong changes in patient survival time. Changes in the absence or presence of mutations in the selected genes are indicated on all branches with four or more children It was not possible to determine whether our inferred clustering was completely identical to that of  Hofree  et al.  (2013) , as the exact tree was not provided by the authors. However, we observed similar cluster sizes and distributions of survival time in the clusters. We found that patients assigned to the smallest of the four subtypes showed an increased survival time ( Fig. 4 , green cluster). A RidgeRace analysis of patient survival time as a phenotype consistently showed a strong positive rate increase in the branch leading to that cluster ( Fig. 4 , marker  m1 ). Similarly, RidgeRace inferred a decrease in survival time for the branch leading to the yellow cluster (branch  m2 ). Branch  m3  was associated with a rather small decrease in survival time because the red cluster splits in distinct two subtypes with a successive second increase (branch  m5 ) or a decrease (branch  m4 ) in survival time, with branch  m4  leading to the majority of the red cluster, which had the lowest survival time of all four clusters. RidgeRace reconstruction can be combined with the reconstruction of discrete genetic events. We mapped the binary data encoding the absence or presence of non-synonymous mutations in a selection of genes to the tree. The mapping confirmed the diverse nature of the somatic mutations. Only  P53  was found to be altered in almost all patients and was reconstructed to have mutated at the root of the tree. Beside  P53 , only  TTN  was reconstructed to change on a higher level node: it was ‘gained’ (mutated) at branch  m3  and was present in 83 of 85 patients of the red cluster.  RYR2  was gained on branch  m5  and present in 9 of 85 patients in the red cluster. Besides these changes, no change appeared on a branch higher than five levels below the root. 4.3 System requirements RidgeRace requires only minimal system resources (&lt;100 MB RAM). The C++ implementation relies on the boost ublas library ( BOOST, 2014 ) to solve the ridge optimization [ Equation(3) ]. The running time of a full RidgeRace inference is larger than the time required by comparable methods ( Table 1 , measured using an Intel Xeon X5660 with 2.8 GHz) but still within the range of a few minutes. The majority of the running time for RidgeRace is consumed by estimating the  λ  parameter, performing a leave-one-out iteration over all leaf nodes of the tree and testing  λ ∈ { 10 − 6 , 10 − 4 , … , 10 + 2 } . Decreasing the evaluation range for  λ  or performing the leave-one-out iteration only on a subset of the leaf nodes can considerably decrease the running time for larger trees.
 Table 1. Comparison of average running times in seconds for RidgeRace and the  APE  implementations of GLS and REML, shown for trees of different size, ranging from 100 to 500 leaf nodes Method 100 200 300 400 500 RidgeRace 4 + 0 29 + 1 135 + 1 1074 + 7 3372 + 24 GLS 1 2 3 7 10 REML 2 6 15 38 123 Note:  RidgeRace running time is provided as the running time required for full  λ  inference plus time required for ACR. 5 CONCLUSION We here describe a new method for the inference of ancestral character states for continuous characters by performing a ridge regression between the total branch length reaching from the root to a leaf node and the phenotypic value of that leaf. The inference is made by minimizing the prediction error at extant nodes as well as an additional L2-norm regularization factor. The regularization forces the phenotypic rate to be distributed more equally across the whole tree and to branches shared by several nodes, circumventing the trivial case that explains the phenotype by a gain at the terminal branches owing to the overparameterization. Our evaluation showed that RidgeRace achieved a good congruence between inferred and simulated ancestral character states, and that it performed similarly to or better than two other state-of-the-art methods in terms of the mean squared error. RidgeRace does not assume any underlying model of evolution, and thus, the method allows large flexibility when no definite assumptions can be made about the type of evolutionary process. The formulation of the optimization problem [ Equation(2) ] allows the straightforward inclusion of measurements at ancestral nodes (e.g. from fossil records). In a similar fashion, multiple measurements at a node can be easily included to lend further support to the inference. Such measurements might originate from multiple observations of the same trait for the same species, or they might represent several traits fitted together. Finally, visualization of the inferred branch weights  β  along the tree allows a detailed interpretation of the specific phenotypic rates and can indicate short periods of strong directional selection or of increasingly fast evolution, or regimes of the phylogeny that feature larger variation overall. In our application to the genetic profiles of ovarian cancer data, we demonstrated that RidgeRace was able to reconstruct the main clusters of the phenotype distribution. We also reconstructed changes in the genetic profiles on the branches of the tree. However, no associations between known genetic aberrations and change in survival rate were found for these data. Nevertheless, this study demonstrated the general functionality of the method and suggests future extensions. Patient survival time as a phenotype is a biased measurement, as it is based on the time of diagnosis and the (potential) death of the patient. It may also be dependent on many other factors, such as the patient’s age and the type of therapy received. As RidgeRace can perform a regression on the patient data, such information could easily be included as additional covariates (features) in the regression, if available. This would allow us to control for the influence of such factors and provide insights into their relevance relative to the genetic factors. Funding : This work was supported by  Heinrich Heine University . Conflict of Interest : none declared. 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>PEP_scaffolder: using (homologous) proteins to scaffold genomes</Title>
    <Doi>10.1093/bioinformatics/btw378</Doi>
    <Authors>Zhu Bai-Han, Song Ying-Nan, Xue Wei, Xu Gui-Cai, Xiao Jun, Sun Ming-Yuan, Sun Xiao-Wen, Li Jiong-Tang</Authors>
    <Abstract>
Motivation: Recovering the gene structures is one of the important goals of genome assembly. In low-quality assemblies, and even some high-quality assemblies, certain gene regions are still incomplete; thus, novel scaffolding approaches are required to complete gene regions.</Abstract>
    <Body>1 Introduction Identifying genes is a major goal of genome sequencing projects for downstream functional study and evolutionary analysis. Although large-insert libraries or long single-molecule reads improve the genome N50 size, it remains difficult to complete all gene structures. Thus, new scaffolding approaches are needed to recover the gene regions. Many methods have been developed to increase the continuity of gene regions using transcripts or proteins as guides.  Mortazavi  et al.  (2010 ) utilized paired RNA-seq data to scaffold  Caenorhabditis nematode  contigs. Nevertheless, the accuracy of this strategy and the improvement in the proportion of complete genes were not estimated. Previously, we developed L_RNA_scaffolder using long single-end RNA-seq reads for genome scaffolding, which was highly accurate (93.6%) ( Xue  et al. , 2013 ). However, these strategies require RNA-Seq data, and genes that are not detected by RNA-seq will not be re-built. Different from using transcripts, ESPRIT ( Dessimoz  et al. , 2011 ) and SWiPS ( Li and Copley, 2013 ) used proteins to link contigs. Based on the predicted genes using AUGUSTUS ( Stanke  et al. , 2006 ), ESPRIT identified split protein-coding regions and linked unassembled genomic segments. The accuracy of ESPRIT depends on that of predicted genes. The time-consuming nature of  de novo  gene prediction also limits its application. SWiPS determined coding contigs and exonic regions using tblastn and GeneWise ( Birney  et al. , 2004 ), and then scaffolded contigs by optimization of the overall protein to contig mappings. SWiPS integrated multiple steps to refine the precise protein-contig mapping; consequently, it has long running time. Herein, we present a novel and fast method to scaffold contigs using (homologous) proteins. The PEP_scaffolder has high scaffolding accuracy and is much faster than previous scaffolders. The improved proportion of fully covered genes is close to that of the finished genome. 2 Methods The main steps of PEP_scaffolder are summarized as follows ( Supplementary Figure S1  and  Supplementary Methods  1). Initially, proteins are aligned to contigs using BLAT ( Kent, 2002 ) and then the alignments are subjected to PEP_scaffolder. ‘Guide’ proteins are selected that have high-quality alignments above a certain minimal percent identity (MPI;  Supplementary Methods  1) and are not fully covered under the minimal length coverage (MLC;  Supplementary Methods  1). The longest alignment region in one block is then selected and all blocks are ordered following the alignment positions in the protein. The contigs corresponding to blocks are sorted and oriented following the block orders. If the interval between two blocks is shorter than the maximal intron length (MIL;  Supplementary Methods  1), the connection between two contigs is retained. The optimal connection for each contig is selected and scaffolding paths are built by walking all optimal connections. To assess performance and accuracy, we scaffolded 36 437 human contigs (N50 size of 148 715 bp) with different sources of human proteins. The accuracy of PEP_scaffolder was measured following the Genome Assembly Gold Standard Evaluations pipeline ( Salzberg,  et al. , 2012 ) ( Supplementary Methods  2). The N50 size and corrected N50 size were used as metrics to determine the scaffolding performance ( Supplementary Methods  3). The genome coverage was measured to investigate its effect on scaffolding performance ( Supplementary Methods  3). To evaluate the proportions of fully covered genes, we aligned human Swiss-Prot proteins to three assemblies including the contigs, the PEP_scaffolder assembly and the hg38 assembly ( Speir  et al. , 2016 ), respectively. For each assembly, the proportion of fully covered proteins among all proteins was calculated ( Supplementary Methods  4). To assess the accuracy of the method using non-human homologs, rodent and mammal proteins were used as guides to scaffold human contigs. Human, rodent and mammal proteins were downloaded from the UniProt database ( Consortium, 2015 ). The contigs and hg38 assembly were obtained from NCBI GenBank ( Benson  et al. , 2013 ). We compared our method with SWiPS and ESPRIT. The  Drosophila melanogaster  genome from the Ensembl database ( Cunningham  et al. , 2015 ) was fragmented into contigs of the same length (10 kb). Fly Ensembl proteins were used to scaffold the fly contigs ( Supplementary Methods  5). 3 Results The performance of PEP_scaffolder was measured using the N50 size. Using human Swiss-Prot proteins as test guides, the N50 sizes were saturated when the MPI was &gt;0.9, the MLC &gt;0.9 and the MIL &gt;150 kb ( Supplementary Figures S2, S3 and S4 ). Using these optimal parameters, the final N50 sizes were 171,032 bp (a 15% increase) and 168,047 bp (a 13% increase) for human Swiss-Prot proteins and TrEMBL proteins, respectively ( Supplementary Tables S1 and S2 ). Using human Swiss-Prot and TrEMBL proteins as guides, the contig number was reduced from 36,437 to 30,550. The improvement on N50 size was 16.8% increase at an accuracy of 96.7%, which demonstrated the good performance of PEP_scaffolder. In a revised assembly where the scaffolds were split at error points, the corrected N50 size had an improvement of 16.1% increase. The proportion of fully covered proteins in the PEP_scaffolder assembly (96.8%) was higher than the proportion in the contigs (82.8%) and nearly equal to that for hg38 (99.8%). In particular, the proportion of fully covered proteins over 500 amino acids showed a larger improvement (from 72.3 to 95.5%, a 32% increase) than did shorter proteins, indicating that PEP_scaffolder greatly improved the proportion of complete genes ( Fig. 1a ).
 Fig. 1. Evaluation of PEP_scaffolder performance. ( a ) Completeness of Swiss-Prot proteins in three human assemblies. Swiss-Prot proteins were aligned to three assemblies using BLAT, with a length coverage cutoff of 90%. ( b ) Running time for scaffolding the fly genome PEP_scaffolder was able to scaffold human contigs using orthologs of distant species. Using rodent Swiss-Prot proteins, mammal Swiss-Prot proteins, rodent TrEMBL proteins and mammal TrEMBL proteins, we obtained improvements on N50 size of 6.84, 3.61, 10.84 and 20.84%, respectively ( Supplementary Tables S1 and S2 ). The accuracy was as high as 90.82%, indicating that PEP_scaffolder could utilize orthologs to scaffold a target genome with high accuracy. More genome regions covered by proteins would generate longer scaffolds. To examine the improvement on N50 size with increasing numbers of proteins, we constructed multiple sets of proteins by combining human proteins, mammal proteins and rodent proteins together. The N50 size and corrected N50 size were improved to 182 433 and 176 257 bp (a 22.7% increase and 18.5% increase), respectively, using all proteins as guides ( Supplementary Table S3 ), indicating that an enlarged proteome could increase the proportion of recovered genes. We scaffolded fly contigs using SWiPS, ESPRIT and PEP_scaffolder, respectively. PEP_scaffolder produced the most connections (4191) with the highest accuracy (99.6%) and the shortest running time (27 minutes) ( Fig. 1b  and  Supplementary Table S4 ), suggesting that PEP_scaffolder is superior to the other scaffolders. 4 Discussion We demonstrated that PEP_scaffolder is an efficient and fast scaffolder that improves the proportion of complete genes. The performance of PEP_scaffolder could be improved in several ways. Protein variations between species might influence the accuracy of PEP_scaffolder. We observed higher accuracy using proteins from the target species compared with proteins from close species ( Supplementary Tables S1 and S2 ). The annotations of Swiss-Prot proteins are created by manual analysis, whereas TrEMBL proteins are predicted automatically without manual annotation. Therefore, Swiss-Prot proteins are more credible than TrEMBL proteins. Our results showed that, using Swiss-Prot proteins, the accuracy of PEP_scaffolder was higher than using TrEMBL proteins ( Supplementary Tables S1 and S2 ). To overcome the above limitations, we recommend that more supporting proteins are used to construct more accurate scaffolds ( Supplementary Methods  6 and  Supplementary Data ). As shown in  Supplementary Figure S6 , scaffolding performance is significantly correlated with genome coverage. Therefore, increasing the number of (homologous) proteins would improve the performance. PEP_scaffolder could be useful for the genome analysis of non-model species, which lack high-quality genome assemblies.  Supplementary Material Supplementary Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>mDCC_tools: characterizing multi-modal atomic motions in molecular dynamics trajectories</Title>
    <Doi>10.1093/bioinformatics/btw129</Doi>
    <Authors>Kasahara Kota, Mohan Neetha, Fukuda Ikuo, Nakamura Haruki</Authors>
    <Abstract>Summary: We previously reported the multi-modal Dynamic Cross Correlation (mDCC) method for analyzing molecular dynamics trajectories. This method quantifies the correlation coefficients of atomic motions with complex multi-modal behaviors by using a Bayesian-based pattern recognition technique that can effectively capture transiently formed, unstable interactions. Here, we present an open source toolkit for performing the mDCC analysis, including pattern recognitions, complex network analyses and visualizations. We include a tutorial document that thoroughly explains how to apply this toolkit for an analysis, using the example trajectory of the 100 ns simulation of an engineered endothelin-1 peptide dimer.</Abstract>
    <Body>1 Introduction Molecular dynamics (MD) simulations are a promising method to investigate the dynamical behaviors of various molecular systems with atomic details. Although recent advances in the computer technologies have realized the long-term simulations of large systems, the huge amount of trajectory data thus generated is not easily interpreted. In order to tackle this problem, analyses toolkits have been extensively developed, such as MDAnalysis ( Michaud-Agrawal  et al. , 2011 ), Wordom ( Seeber  et al. , 2011 ) and VMD ( Humphrey  et al. , 1996 ). Long-term trajectories reflect complex behaviors of the local and global conformational changes of molecules. The distributions of atomic coordinates may be unimodal,  i.e.  adequately described by a cluster that is approximated by a single mean and standard deviation, or multi-modal, with several spatially distinct clusters, often slowly interchanging with relatively rapid fluctuations within each cluster. For example, a common multi-modal local motion in proteins results from the rearrangements of hydrogen bonds associated with transient flipping of side chains. The analysis of multi-modal motions, which are not each describable by a single Gaussian distribution, is not straightforward. We previously proposed a new analysis method, named ‘multi-modal Dynamic Cross Correlation (mDCC)’ ( Kasahara  et al. , 2014 ), as a variant of the conventional Dynamic Cross Correlation (DCC) method ( McCammon, 1984 ). Because DCC calculates the correlations between atomic motions, based on deviations from the averaged coordinate of each atom, it does not make sense when atoms undergo multi-modal motions. To characterize such multi-modal motions, the mDCC method takes advantage of a Bayesian statistics-based pattern recognition technique ( Attias, 1999 ), and classifies the distributions of atomic coordinates into some clusters, or modes. We applied this method to analyze transcription factor-DNA interactions, and found that many transient, multi-modal interactions are formed at interfaces between proteins and DNA. See  Supplementary Materials S1 and S2  for details of the method. Here, we present an open source, easy-to-use toolkit for the mDCC method. This toolkit performs the full analysis techniques applied in our previous work ( Kasahara  et al. , 2014 ), and not only covers the correlative coefficients of multi-modal atomic motions, but also enables visualization of the results effectively as a heatmap and a complex network diagram, powered by standard software such as Cytoscape ( Shannon  et al. , 2015 ) and R (R Core Team, 2003). As the output files are simple tab-separated texts, users can apply their favorite software for visualization. Users can easily learn how to use this toolkit via the attached tutorial document, with the trajectory of the 100 ns simulation of an engineered endothelin-1 peptide dimer as an example. 2 Implementation Figure 1  summarizes the mDCC analysis by using  mDCC_toolkit , which is composed of two C ++ programs and several scripts.  mDCC_tools  handles a variety of trajectory file formats, such as Gromacs, AMBER and CHARMM, by taking advantage of MDAnalysis library ( Fig. 1A ). In addition, files in PRESTO format ( Mashimo  et al. , 2013 ) and tab-separated text files are also accepted.
 Fig. 1. Overview of the mDCC analysis.  (A)  Input data for the analysis.  (B)  The pattern recognition on the atomic coordinates.  (C)  Assessing probabilities for each mode.  (D)  Visualization of all-against-all correlation coefficients. Each column and row indicates each residue. The color gradation from blue to red corresponds to negative and positive correlations. The upper- and lower-triangle depict the mDCC and mDCC-DCC values, respectively.  (E)  A network diagram. The edges indicate the contacting residue pairs with positive correlation. The interaction including multi-modal behavior is shown as the red edge.  (F)  An example of multi-modal behavior in engineered endothelin-1 peptide dimer 
 The analysis is performed by the following programs:
 mdcc_learn  ( Fig. 1B ) recognizes the multi-modal motions of each atom from a MD trajectory. By parameter fitting of the Gaussian mixture model, the spatial distribution of the atomic coordinates is classified into some Gaussian functions, each referred to as a ‘mode’. mdcc_assign  ( Fig. 1C ) calculates the probability of the event that an observed atomic coordinate  r i (t)  belongs to a mode  k  (Gaussian element) for all  i  (atom) and for all  k  over the total time  t . cal_mdcc.py  ( Fig. 1D ) calculates the mDCC values between modes. The correlation map shows the maximum mDCC value of each pair of residues (upper triangle). The map also indicates the difference from the conventional DCC values in the lower-triangle, where the residue pairs with large differences from the DCC show multi-modal behaviors. The R-script for drawing this heatmap is included in this package. The network diagram ( Fig. 1E ) provides a bird’s-eye view of the interactions in the molecular system. Each node indicates each residue, and each edge indicates a pair of residues with highly positive mDCC values (≥0.5 is used in this example) and atomic contacts (the minimum distance ≤5 Å). The toolkit generates files readable by Cytoscape, one of the standard programs for complex network analyses. In addition, the importance of each residue can be quantified by the betweenness values, which are calculated by  nx_centrality.py . The betweenness quantifies the centrality of each node in the network. High betweenness values imply that the node plays an important role in the network (see the  Supplementary Material S3 ). The simulation trajectory of the engineered endothelin-1 peptide dimer (PDB: 1t7h) in a 150 mM NaCl solution is included in this package, as a tutorial example. The 100 ns simulation in the NPT ensemble was performed by using Gromacs ( Pronk  et al. , 2013 ). Users can readily trace our analyses starting from the trajectory file, in a step-by-step manner. The analysis revealed the transient interactions between Asp10 in chain A and Arg2 in chain B (the red edge in  Fig 1E ), which had mDCC and DCC values of 0.55 and 0.31, respectively. A transient flipping motion of Asp10 side-chain resulted in breaking the salt bridge with Arg2 ( Fig. 1F ). See our previous publication for more details regarding the theory and application to a more complex molecular assembly, consisting of two transcription factors on a double-stranded DNA ( Kasahara  et al. , 2014 ). Although this method has been tailored for analyses of MD trajectories, it can be applied to any multi-dimensional distribution (see the software documentation). 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Biopython: freely available Python tools for computational molecular biology and bioinformatics</Title>
    <Doi>10.1093/bioinformatics/btp163</Doi>
    <Authors>Cock Peter J. A., Antao Tiago, Chang Jeffrey T., Chapman Brad A., Cox Cymon J., Dalke Andrew, Friedberg Iddo, Hamelryck Thomas, Kauff Frank, Wilczynski Bartek, de Hoon Michiel J. L.</Authors>
    <Abstract>Summary: The Biopython project is a mature open source international collaboration of volunteer developers, providing Python libraries for a wide range of bioinformatics problems. Biopython includes modules for reading and writing different sequence file formats and multiple sequence alignments, dealing with 3D macro molecular structures, interacting with common tools such as BLAST, ClustalW and EMBOSS, accessing key online databases, as well as providing numerical methods for statistical learning.</Abstract>
    <Body>1 INTRODUCTION Python ( www.python.org ) and Biopython are freely available open source tools, available for all the major operating systems. Python is a very high-level programming language, in widespread commercial and academic use. It features an easy to learn syntax, object-oriented programming capabilities and a wide array of libraries. Python can interface to optimized code written in C, C++or even FORTRAN, and together with the Numerical Python project  numpy  (Oliphant,  2006 ), makes a good choice for scientific programming (Oliphant,  2007 ). Python has even been used in the numerically demanding field of molecular dynamics (Hinsen,  2000 ). There are also high-quality plotting libraries such as  matplotlib  (matplotlib.sourceforge.net) available. Since its founding in 1999 (Chapman and Chang,  2000 ), Biopython has grown into a large collection of modules, described briefly below, intended for computational biology or bioinformatics programmers to use in scripts or incorporate into their own software. Our web site lists over 100 publications using or citing Biopython. The Open Bioinformatics Foundation (OBF,  www.open-bio.org ) hosts our web site, source code repository, bug tracking database and email mailing lists, and also supports the related BioPerl (Stajich  et al. ,  2002 ), BioJava (Holland  et al. ,  2008 ), BioRuby ( www.bioruby.org ) and BioSQL ( www.biosql.org ) projects. 2 BIOPYTHON FEATURES The  Seq  object is Biopython's core sequence representation. It behaves very much like a Python string but with the addition of an alphabet (allowing explicit declaration of a protein sequence for example) and some key biologically relevant methods. For example, Sequence annotation is represented using  SeqRecord  objects which augment a  Seq  object with properties such as the record name, identifier and description and space for additional key/value terms. The  SeqRecord  can also hold a list of  SeqFeature  objects which describe sub-features of the sequence with their location and their own annotation. The  Bio.SeqIO  module provides a simple interface for reading and writing biological sequence files in various formats ( Table 1 ), where regardless of the file format, the information is held as  SeqRecord  objects.  Bio.SeqIO  interprets multiple sequence alignment file formats as collections of equal length (gapped) sequences. Alternatively,  Bio.AlignIO  works directly with alignments, including files holding more than one alignment (e.g. re-sampled alignments for bootstrapping, or multiple pairwise alignments). Related module  Bio.Nexus , developed for Kauff  et al.  ( 2007 ), supports phylogenetic tools using the NEXUS interface (Maddison  et al. ,  1997 ) or the Newick standard tree format.
 Table 1. Selected  Bio.SeqIO or Bio.AlignIO  file formats Format R/W Name and reference fasta R+W FASTA (Pearson and Lipman,  1988 ) genbank R+W GenBank (Benson  et al. ,  2007 ) embl R EMBL (Kulikova  et al. ,  2006 ) swiss R Swiss-Prot/TrEMBL or UniProtKB (The UniProt Consortium,  2007 ) clustal R+W Clustal W (Thompson  et al. ,  1994 ) phylip R+W PHYLIP (Felsenstein,  1989 ) stockholm R+W Stockholm or Pfam (Bateman  et al. ,  2004 ) nexus R+W NEXUS (Maddison  et al. ,  1997 ) Where possible, our format names (column ‘Format’) match BioPerl and EMBOSS (Rice  et al. ,  2000 ). Column ‘R/W’ denotes support for reading (R) and writing (W). 
 Modules for a number of online databases are included, such as the NCBI Entrez Utilities, ExPASy, InterPro, KEGG and SCOP.  Bio.Blast  can call the NCBI's online Blast server or a local standalone installation, and includes a parser for their XML output. Biopython has wrapper code for other command line tools too, such as ClustalW and EMBOSS.  Bio.PDB  module provides a PDB file parser, and functionality related to macromolecular structure (Hamelryck and Manderick,  2003 ). Module  Bio.Motif  provides support for sequence motif analysis (searching, comparing and  de novo  learning). Biopython's graphical output capabilities were recently significantly extended by the inclusion of GenomeDiagram (Pritchard  et al. ,  2006 ). Biopython contains modules for supervised statistical learning, such as Bayesian methods and Markov models, as well as unsu pervised learning, such as clustering (De Hoon  et al. ,  2004 ). The population genetics module provides wrappers for GENEPOP (Rousset,  2007 ), coalescent simulation via SIMCOAL2 (Laval and Excoffier,  2004 ) and selection detection based on a well-evaluated  F st -outlier detection method (Beaumont and Nichols,  1996 ). BioSQL ( www.biosql.org ) is another OBF supported initiative, a joint collaboration between BioPerl, Biopython, BioJava and BioRuby to support loading and retrieving annotated sequences to and from an SQL database using a standard schema. Each project provides an object-relational mapping (ORM) between the shared schema and its own object model (a  SeqRecord  in Biopython). As an example,  x BASE (Chaudhuri and Pallen,  2006 ) uses BioSQL with both BioPerl and Biopython. 3 CONCLUSIONS Biopython is a large open-source application programming interface (API) used in both bioinformatics software development and in everyday scripts for common bioinformatics tasks. The homepage  www.biopython.org  provides access to the source code, documentation and mailing lists. The features described herein are only a subset; potential users should refer to the tutorial and API documentation for further information. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>The GMOD Drupal Bioinformatic Server Framework</Title>
    <Doi>10.1093/bioinformatics/btq599</Doi>
    <Authors>Papanicolaou Alexie, Heckel David G.</Authors>
    <Abstract>Motivation: Next-generation sequencing technologies have led to the widespread use of -omic applications. As a result, there is now a pronounced bioinformatic bottleneck. The general model organism database (GMOD) tool kit (http://gmod.org) has produced a number of resources aimed at addressing this issue. It lacks, however, a robust online solution that can deploy heterogeneous data and software within a Web content management system (CMS).</Abstract>
    <Body>1 INTRODUCTION 1.1 Emerging model species and bioinformatics Next-generation sequencing (NGS) technologies have allowed an increasing number of biologists to utilize the -omic experimental strategy and support research programs by searching for statistically significant patterns in large-scale (LS) experiments ( Collins  et al. , 2003 ). Due to the limited number of bioinformaticians and resources, this rapid uptake of -omics is now causing a bioinformatic bottleneck. This bottleneck, which is more pronounced in the Ecological and Evolutionary Functional Genomics (EEFG) community ( Beldade  et al. , 2008 ), ought to be addressed without requiring custom-made and non-integrated solutions. The Generic Model Organism Database tool-kit (GMOD;  http://gmod.org ) is a consortium originally formed from functional genomics model organism communities to produce a standard set of open source software for handling, primarily, genomic data. Since its inception, the consortium has built or incorporated an impressive array of tools and standards. The uptake of GMOD tools and standards has been so successful that GMOD has expanded beyond the functional genomics community and is now been used by EEFG laboratories. Indeed, ‘MOD’ databases are now commonplace in the -omics field ( http://gmod.org/wiki/GMOD_Users ). Until recently, GMOD software had focused on whole-genome sequencing. As researchers from other fields make use of -omic, bioinformatic and artificial intelligence (AI) approaches, GMOD has expanded into other fields such as phylogenetics ( Heinicke  et al. , 2007 ), microarray research ( Day  et al. , 2007 ), molecular ecology (a Chado extension from the National Synthesis Center for Evolution  http://www.nescent.org/informatics/software.php ), transcriptomics without a reference genome ( Papanicolaou  et al. , 2009 ) and others. Further, the cost-effectiveness of NGS does not apply to the downstream cost associated with computational analysis of the data; quite the opposite, in fact. Therefore, there is an ever-growing need for cost-effective and integrated solutions that improve the capabilities of wet-lab biologists to mine their own data before publication. Even though a number of commercial tools exists, some are not affordable. Others are closed-source software and thus cannot be adapted. Moreover, most are not integrated into the larger GMOD framework. Individual attempts within the GMOD consortium have yet to provide a generic visualization front end. The web site creation tool, GMODWeb ( O'Connor  et al. , 2008 ) is of interest but it has limited scope, but it is useful in rapidly generating a web-based front-end for a Chado database ( Arnaiz  et al. , 2006 ;  Mungall  et al. , 2007 ). Tripal ( http://gmod.org/Tripal ) offers an efficient front-end for Chado but no generic framework. InterMine ( Lyne  et al. , 2007 ) is a more powerful graphical user interface (GUI) for a database, driven by lightweight JavaScript but it is a complicated framework to use for development. The Ensembl system ( Hubbard  et al. , 2002 ) is an example of a complete platform for processing genomic data, but it was custom built for the needs of the Sanger Institute rather than a community software. Indeed, most of above software are open source but not necessarily developed for open-development. In order to minimize reliance on continued funding, the community could orientate toward more generic frameworks explicitly designed for open-development. Bioinformatic work-flow visualizations, such as Taverna and Galaxy ( Giardine, 2005 ;  Oinn  et al. , 2004 ), are both geared toward data analysis, even though the latter allows for custom plugins. Although the Galaxy team is working toward a more general framework for data dissemination, for many bioinfomaticians, the Ensembl solution seems more robust. Ensembl is an entire bioinformatic framework with both analysis and dissemination tools, but it has a very steep learning curve and is not a GMOD component. It would be of interest, however, for the entire GMOD community to develop a generic ‘plumbing’ framework so that (i) laboratories can rapidly deploy web sites with data analysis/dissemination tools (such as Taverna or a BLAST server); (ii) bioinformaticians can rapidly program new applications (such as custom front-ends on par with InterMine). 1.2 The Drupal content management system One solution is to use a content management system (CMS) such as Wordpress, Drupal or others. CMSs are platforms for storing, managing, disseminating data of any type. Often they have been used to drive web sites, including ‘blogs’, but research projects such as Scratchpads ( Smith  et al. , 2009 ) have also been successful. Some researchers use CMSs for building their laboratory web sites. Some CMSs support a number of useful concepts such as the RDF, XML and similar protocols, ontologies, controlled vocabularies and the other concepts relating to the Semantic Web. Further, CMSs are often a complete software package with tools for managing community-based data, such as users, roles and fine-grained permissions. Some CMSs are modular, allowing for users to program their own plugins and extend functionalities. Drupal is such a CMS. It is open source and can be downloaded freely from  http://drupal.org . It is written in PHP, a language that is straightforward for nascent bioinformaticians to learn. It supports a number of database engines, including mySQL, Oracle and the GMOD supported PostgreSQL. Further, Drupal is built with security in mind, has powerful user-management tools and is highly modular, allowing for plugins to be developed and deployed in a standardized and streamlined fashion. Importantly, Drupal is popular and well documented. The widespread use has resulted in a large active community of users and developers (e.g. see  http://egressive.com/article/who-uses-drupal ). This article initiates a long-term effort in creating a bioinformatic framework for the Drupal CMS within the specifications of GMOD. We developed three modules for three categories of users. First, GMOD-DBSF is a generic function framework for Drupal developers of bioinformatic tools. Then we built two modules for end users: (i) a powerful similarity-search software (e.g. BLAST) server for wet-lab biologists and system administrators benefiting from a friendly GUI and (ii) an RNAi experiment databasing platform. The latter can be easily modified for other experimental data, but it was developed and used in a community-wide review on failed and unpublished RNAi experiments in Lepidoptera (butterflies and moths; the only taxon where RNAi experiments are often unsuccessful). 2 METHODS We used the Drupal 6 CMS. As the Chado package uses PostgreSQL, this database engine is required. The GMOD-DBSF module is a base module and thus required for all other modules in this framework; the other modules are optional. The GMOD-DBSF base module can utilize an installation of the Chado package but installing it is not necessary as it is not required for the biosoftware_bench module. The BioPerl ( http://bioperl.org ;  Stajich  et al. , 2002 ) package and freely available Perl libraries (from CPAN) are needed along with certain Drupal modules: the Tabs module ( http://drupal.org/project/tabs ) is used to deploy tabular web content; the JQuery module ( http://drupal.org/project/jquery ) to deploy and seamlessly maintain the JQuery JavaScript library. Further, an external JQuery-utilizing library, dynatree ( http://code.google.com/p/dynatree ), is used to produce ‘check-box trees’. Commonly used annotation software, such as BLAST, annot8r, InterProScan and SSAHA2 ( Altschul  et al. , 1997 ;  Ning  et al. , 2001 ;  Schmid and Blaxter, 2008 ;  Zdobnov and Apweiler, 2001 ), were integrated into  biosoftware bench , but using them requires that they are installed on the server (not all software needs to be installed: administrators can select which ones they wish to make available). For sequence retrieval, the fastacmd and Bio::Seq::Index approaches were used for the BLAST and SSAHA2 databases, respectively. To enable job management, Condor ( Thain  et al. , 2005 ) was used as it is simple and can perform well on both a PC-farm and a single multi-core host. Future versions of this framework aim to make use of the Sun Grid Engine. 2.1 Specifications The framework adheres to certain criteria. It (i) is open source under a non-restrictive license and thus can be customized and expanded; (ii) can be integrated with other widely used bioinformatic applications and implement the GMOD standards; (iii) is secure to both the user and the server; (iv) provides GUIs to both end users and administrators; (v) is developer friendly by extending Drupal's application programming interface (API) according to the Drupal community specification. Drupal itself has a powerful API: e.g. the deployment of the third party modules, such as the ones presented here, requires no more than a single line of code. This complements their installation, which is usually ‘point-and-click’. 2.2 Aims The work presented here focused on producing three Drupal modules. The first is GMOD-DBSF, which provides a framework for developing new bioinformatic Drupal modules. It is responsible for (i) importing a subset of the Chado tables to Drupal, (ii) creating new tables in Drupal using Chado conventions; (iii) providing functions to communicate with Chado and Drupal database schemas; and (iv) providing other, generic, functions useful for bioinformatic module development. We also built two example applications. First, a software server with the BLAST, InterProScan, annot8r and SSAHA2 software deployed by default; additional plugins can be generated by the community. Second, a web-based database for storing experimental information from RNAi experiments. We used the Minimum Information Criteria for RNAi experiments (MIARE) as provided by the MIARE working group ( http://miare.sourceforge.net ) and the Lepidoptera RNAi Working Group, an international group composed of 70 scientists from 42 institutions in 21 countries (Terenius  et al. , under review). 2.3 Schema We opted not to use Chado for public data entry and manipulation; in our work, Chado is used as a long-term and secure data warehouse. We prefer not to allow the public to commit changes to the Chado database but still wish to provide a bidirectional user-interface. We, therefore, use Chado for read operations of data residing in the data warehouse, but opted to create a Chado schema within Drupal for read/write operations of user-contributed data. With Chado being a highly generic schema, there are a number of tables unused in this instance of GMOD-DBSF (e.g. the MAGE module). Therefore, we imported only the basic Chado tables in the Drupal database (the feature, organism, cv, dbxref, pub tables and their dependencies). Drupal is then extended with additional tables created using the Chado conventions ( Fig. 1 ). New tables in the ‘resource’ group were created to allow better representation of sequence-less features. Likewise, a software table group is utilized specifically for software variables and is linked with the resource data using the software_resource. Further, a new study group of tables has been created to allow for generalized databasing of wet-lab experimental data. Publications are supported via the Chado pub schema. We used new tables to better integrate authorships using the author and pub_author tables. This implementation allows seamless integration with core Drupal data: e.g. a resource_roles allows linking of the resources with specific Drupal username groupings (roles). All of these tables are installed automatically during the point-and-click installation of GMOD-DBSF. It was expected that certain applications would require the synchronization of data between the Drupal and Chado databases. For example, InsectaCentral requires it for its Community Annotation module. For the security of Chado as a data warehouse, developers should be cautious but secure protocols can be developed using Drupal's features. GMOD-DBSF offers such feature/resource-specific synchronization. In InsectaCentral's implementation, a special administrator user-group is allowed to use these functions and synchronization changes with Chado.
 Fig. 1. Part of the database schema built by GMOD-DBSF. Chado conventions ensure that this schema can interact with an installed Chado database. Some tables and links omitted for clarity. 2.4 RNAi experiment In order to efficiently provide a cataloguing platform for the RNAi experiments, the Lepidoptera RNAi working group used the MIARE ontologies. MIARE is a set of reporting guidelines that describes the minimum information that should be reported about an RNAi experiment to enable the unambiguous interpretation and reproduction of the results. We then built the genes4all_experiment module using GMOD-DBSF and enhanced it via community feedback. Three different datatypes are used: sequence features, sequence-less features and publications. To distinguish between the first two, the latter is called a resource and has a separate set of tables in our schema. Two types of sequence data are used: (i) the target gene, which may be derived from a species other than the one targeted (due to lack of sequence information), and (ii) the RNAi construct. Three types of resources are used: (i) experimental animals, (ii) delivery protocol and (iii) assay protocol. Considering that the genes4all_experiment caters primarily to unpublished research, the publication GUI requests only the communicating author but, as we mentioned above, the schema can handle multiple authors and their details via the author and pub_author tables. 3 RESULTS 3.1 Drupal for bioinformatics using GMOD-DBSF The core Drupal program has limited capabilities for bioinformatics. As a CMS, it is most capable in storing, displaying and organizing data as stored in the so-called ‘nodes’: authored web-pages linked with ancillary data. Extensions, called ‘modules’, extend its functionality. For example, the Tabs module that we use allows for multiple web-pages to appear as tabs. Such modules provide their own API and thus allow other modules to make use of a complicated functionality using only a line of code. The GMOD-DBSF module is one such module. Bioinformaticians can use it to perform an increasing number of operations (see  http://gmod-dbsf.googlecode.com/files/GMOD-DBSF_dev_manual_1.0.pdf ). Indeed, we hope that as the bioinformatics community embraces Drupal, GMOD-DBSF will also expand. Currently, GMOD-DBSF offers a number of functionalities not available in the Drupal core. A set of functions allows a generic interaction with Chado tables. The function  gmod_dbsf_add_cv() , for example, allows for one to add a new Controlled Vocabulary (CV) by providing the name of the CV and an array with the CV terms they wish to add. This function can connect to a Chado database via the  gmod_dbsf_db_execute()  function or operate on the local Drupal database (or make use of the  gmod_dbsf_is_chado()  auto-detect function). Similar functions operate to add, delete and populate the feature, db, pub and other Chado tables. Ancillary Chado tables, such as the featureprop and feature_cvterm tables, often require complicated SQL commands with multiple joins. A number of gmod_dbsf functions cater to simplify manipulating these tables by simply passing the desired variables. For example, a featureprop table can be populated with a single line of code which passes the feature ID or feature name, the CV term and properties one wishes to associate. This approach is the  raison d'etre  of GMOD-DBSF: to allow other modules to query and manipulate Chado in a standardized fashion, and also to accelerates the development of other modules. Other convenience functions allow a developer to install a materialized view, a new table or PostgreSQL function. A few functions aim to provide secure approaches for oft-used tasks. The  gmod_dbsf_create_uid()  function (all non-core functions in Drupal begin with the module's name) creates a unique MD5 identifier, based on a user's session ID, time and optionally a text string, which can be used for file uploads. The  gmod_dbsf_batch_upload_fasta()  function allows users to upload a FASTA file to the server even if it is many megabytes or takes a considerable amount of time. It is used, for example, by the biosoftware_bench software server to allow users to upload datasets for use as query or subject databases. Finally, a few functions have been created to make use of BioPerl functions. For example, one function is responsible for creating and parsing GFF3 files, another, the  gmod_dbsf_get_taxonomy_from_ncbi() , uses Bio::DB::Taxon to query NCBI (via Entrez or via a local NCBI Taxonomy database flatfile) for the taxonomy of a species. In InsectaCentral, this function is used in conjunction with the  gmod_dbsf_get_add_organism()  function to build a GUI for InsectaCentral curators to add new organisms and ancillary phylogenetic information into the Chado database. 3.2 Bioinformatic software bench 3.2.1 Innovations When a laboratory generates multiple pre-publication datasets, a local solution for mining, searching and manipulating the data must be deployed. This leads to cumbersome administration and maintenance and the need for constant bioinformatic support. There are a number of main innovations of biosoftware_bench: (i) graphical administration; (ii) deployment of command-line software; (iii) use of a secure daemon to handle job submissions with the option to use the Condor job management system; and iv) linking datasets with phylogenetic information. Further, the ability to deploy datasets only available to certain users or groups allows for the existence of a single server to handle both public and pre-publication data. As the system is integrated with a laboratory's web site and user authorization is handled by Drupal, the entire process appears seamless to the user. Moreover, the deployed software can also be used by other modules, i.e. without a GUI. By reusing the same biosoftware_bench functions, another module can utilize them to prepare and process software results. For example, a module currently under development allows for community members to submit an open reading frame, which is then automatically processed and annotated with the BLAST, annot8r and InterProScan software, with the resulting data stored first into Drupal and then transferred into Chado by a curator. 3.2.2 Installation of software plugins The module comes with plugins for BLAST, annot8r, InterProScan and SSAHA2, but others can be coded by the community. Bioinformatic software can be installed through the biosoftware_bench module. Two ‘include’ (.inc) files are needed for each software. One file guides the installation, including the use of CV terms to define options. The second file is responsible for the interface and batch jobs. New software can be deployed within a few hours by creating two such files and providing Perl routines to handle any output graphs. Once deployed, administrators have access to a set of options that allows them to select which software they wish to install and if they wish to make use of the Condor job management system. This latter feature allows administrators to utilize a PC-farm or a multi-core server to control job submissions. In both cases, a Perl daemon containing the aforementioned Perl routines, processes the jobs as an unprivileged user. For the software servers, it also post-processes the output of the software search in order to provide the output as a number of file formats. A Bio::Graphics-driven image of an alignment of the hits to the query is also produced and colored according to the significance statistic. 3.2.3 Administration The biosoftware_bench module provides an administrator's GUI to minimize user-errors and reduce the time required to setup and maintain a software server. In Drupal, administrative rights are decoupled for each module and each action. Users with specific administrative rights have a GUI where they can specify the location of datasets, see which ones are available and choose which to deploy. The administrator can provide friendly names and group memberships (e.g. ‘Genomes’, ‘Transcriptomes’, ‘UniProt’, etc.) to assist users selecting an appropriate dataset. System and security checks prevent errors with typing or dataset formatting, and thus ensure that the database is populated only with functional datasets. Further, linking them to a species through the NCBI Taxonomy database can be used to enable the phylogeny-driven dataset selection. One security feature allows the administrator to decide if a dataset is to be made restricted to a specific set of users. This allows for the secure deployment of both public and pre-publication datasets from the same server and interface. In the future, for large web sites biosoftware_bench ought to load the dataset in the database rather than use flatfiles. The current method of providing formatted flatfiles is, however, the most straightforward approach and will suit the bulk of biosoftware_bench administrators, in particular bioinformaticists with limited programming or databasing skills. 3.2.4 End-user capabilities The privacy mechanism allows end users to see only the datasets that their username and role memberships allow. In the BLAST server, they can choose to run multiple BLAST algorithms simultaneously, expand their subject dataset by uploading a multi-FASTA file and use a phylogeny to select species- or taxon-specific subject datasets. Once the search is submitted, a self-refreshing page with a unique submission identifier (SUID) appears and can be used to bookmark the page. The system uses ‘cron’ jobs to purge old files, and administrators can decide when result files are flagged as being old and ready to be deleted. For the BLAST software, the results are first produced as XML but BioPerl modules provide an additional choice of text and HTML output. For other software, when possible, an XML is also provided as well as GFF and/or HTML and text. A Bio::Graphics-driven alignment graph provides an overview of the queries, any hits and their respective scores. The tabular presentation of significant hits allows users to download hits of interest as a FASTA file. 3.3 Experiment module The experiment module was custom built for the International Lepidoptera RNAi Working Group (Terenius  et al. , under review). It utilizes functions provided by GMOD-DBSF. A number of core functions exist and adapting them for other types of experiments is straightforward. The GUI was built to provide a good balance between user-friendliness and data security. Each fully completed submission is live in real time, partial submissions can be continued later and even completed submissions may be edited by authorized individuals. The date and time of the last submissions/edits is stored in the database. The user is requested to first provide a unique name for their submission, their email address (which is not made public) and a non-unique passkey. The passkey can be used by multiple submissions and its purpose is to prevent unauthorized edits. Further, data linked with a passkey can be reused by subsequent submissions and allows for continuing incomplete ones. After providing these credentials, the user is presented with a panel of six tabs. The first tab relates to the sequence-based features: the target gene and RNAi construct. The second tab handles non-sequence data (resources) such as experimental animals and protocols. The third tab contains the publication data, including external database cross-references. Such references are also available for the resources and features, allowing users to identify experimental animal stocks or GenBank gene identifiers. Once all required information is provided, a finalize tabs becomes available and users are able to review their submission prior to storing the study as ‘complete’. At any time, users can stop and continue their submission at a later day by making use of their passkey credentials. In order to reduce work load to curators, much of the data are driven by controlled vocabularies (as provided by the community). Building new modules might be considered time consuming. It might be of interest, therefore, to note that the design and deployment of this module required weeks of full-time equivalent work, excluding a week of responding to community feedback. Using the existing module as a template, however, other types of experiments can be supported in a matter of hours. 4 DISCUSSION 4.1 Utility as a community resource Unlike other software, the work presented here aims to integrate well with an existing laboratory website. This system allows laboratories to deploy software locally. This is especially useful for software that can take advantage of clusters of computers [e.g. the RaxML phylogenetics or PAML molecular evolution software ( Stamatakis  et al. , 2008 ;  Yang, 2007 )]. Further, by utilizing a CMS, laboratories can deploy the biosoftware_bench module via the point-and-click approach. They can, likewise, create their entire web content including feed aggregators (e.g. Atom), blogs and file servers. Indeed, a user-friendly system can be the key to allow a specific -omics community (such as one centered around a taxon or a genome-sequencing project) to develop and interact with a central resource such as a large database supporting that community. Drupal modules offer a straightforward installation and also allow for customization within a variety of existing ‘themes’. It is possible, then, to provide the feeling that the -omic data, BLAST servers and standard web-pages are part of one package. 4.2 Utility as a bioinformatic framework With the explosion of information and the paucity of expertise, Drupal is already being applied across biological disciplines: recent work funded by the European Union Framework 6 has produced Scratchpads, a Drupal project for Natural History collections ( Smith  et al. , 2009 ). With advances in information technology and increased interest in semantic integration, the genomics community will benefit from choosing a diverse and robust system, such as Drupal, for integrating, analyzing and displaying information. With more genome-sequencing project coming to fruition, there will be laboratories focusing on datatypes such as ecological and population data which, thus far, are not part of genome databases. GMOD-DBSF is a step toward addressing these emerging needs without worsening the bioinformatic bottleneck. This new API for Drupal makes the co-existence of Chado and Drupal seamless to the end user and reduces the learning curve for the bioinformatic community. Additionally, a large number of core functions or third party modules are available to be used by the bioinformatic community. One example is Drupal's abilities for data federation. A single settings file (settings.ini) defines the database names and access credentials, allowing for a federated database system in the sense that a single web-page can be served by multiple databases which may reside on multiple hosts. This may be of special interest as such an approach would allow us to build to a heterogeneous system of database engines or gain remote access to other database servers. In InsectaCentral's implementation, for example, we deploy Drupal and Chado as core databases and then a SeqFeature::Store database for each of the 200 hosted species. In future versions of InsectaCentral, a laboratory will be able to deploy a local copy of InsectaCentral, a local copy of a Drupal database and connect to the public Chado and SeqFeature::Store databases. They can then deploy their private data as local Chado and SeqFeature::Store databases so that a mix of private data and data from the up-to-date InsectaCentral is seamlessly served to the end user. Further, the Services module ( http://drupal.org/project/services ) provides the means for integrating multiple interfaces such as XMLRPC, JSON, REST, SOAP, etc., avoiding, thus, the need to set up a separate BioMart instance ( Smedley  et al. , 2009 ). This allows a Drupal site to provide web services to other software via multiple interfaces while using the same callback code. Even though Chado was built to be generic and therefore easy to exchange data between groups, different genome-sequencing teams have implemented it in a slightly different way so that cross-communication is not straightforward and adaptors have to be written. The Drupal CMS can become a solution to this compatibility issue between Chado databases. 4.3 Integrating with other software This generic framework could tap into the concept of bioinformatic work flows, such as those offered by Taverna and Galaxy. This is an interesting possibility to consider and may inspire the EEFG community to use these tools. Meta-servers and software to run bioinformatic applications are constantly being developed. A number of command-line software packages now have their own web servers and a dedicated journal now exists (the annual  Nucleic Acids Research  Web Server issue). The most robust and widely used meta-application amongst these is the Galaxy framework. Even though originally developed for genomic data, it has now expanded to other types of data through an active developer community. Galaxy does not offer the main benefits of a CMS (i.e. ease of customization and a rich API). Further, administration of a multi-lab server can be a daunting task for the often over-worked bioinformatician. The biosoftware_bench approach provides full control of the visualization and processing routines. As Drupal is taken up by the GMOD consortium, bioinformaticians who provide new tools would benefit from preparing a biosoftware_bench.inc file (i.e. their software can be easily deployed and laboratories readily can manage and administer it without requiring access to a dedicated bioinformatician). An increasing number of applications exist for displaying genome data to web-users [e.g. the FlyBase database ( Drysdale and Crosby, 2005 ), the UCSC Genome Browser ( Kent  et al. , 2002 ), the Ensembl system and the ubiquitous GBrowse ( Stein  et al. , 2002 )]. As more laboratory groups generate -omic data, there will be a pressing need to develop more such software. One example is the GMODWeb, which builds a web site for a Chado database using the Turnkey application ( http://turnkey.sourceforge.net ). Like GMODWeb, GMOD-DBSF utilizes an external application to drive content deployment but, instead of Turnkey, it uses Drupal, another open source software. Drupal has the advantages of a broader end-user base and hundreds of developers, and is built to be robust and secure for users and the host server. Because of the large number of functions provided by the core and contributed modules, the Drupal solution will become a powerful tool for bioinformaticians. GMOD-DBSF is the only Drupal application built to a generic GMOD API. Another implementation, Tripal, also a GMOD tool ( http://gmod.org/wiki/Tripal ), is available and in active development. It provides a direct interface with Chado, allowing users to edit the contents of a Chado database. The two modules are not mutually exclusive as GMOD-DBSF is aimed as a base module to facilitate development of other modules. With Tripal and the software presented here, the ‘Drupal solution’ provides a feature set unavailable in any of the other software. Indeed, we could envision multiple Drupal sites linking and sharing their data in a seamless manner. 5 CONCLUSION The software presented here was built specifically for the research communities that are only now emerging into the -omics era. For example, NGS transcriptome data are widely used to address central biological questions in non-model species but many laboratories do not yet have the means to make the best use of these data. Due to funding constraints, these communities also have a paucity of bioinformaticians. Developed tools must, therefore, be general enough so that they can be used between laboratories and also straightforward to customize so that wet-lab biologists with a bit of training in programming can deploy and maintain the software. Supporting this new cadre of ‘bioinformaticists’ is vital in order for the communities of emerging model species to reap the rewards that NGS technologies have to offer. We have shown that our software can assist with development of bioinformatic web services. Because Drupal modules are licensed under the GNU Public license and our software was built to be generic and expandable, it would be of interest to the bioinformatic community to expand it. To assist users and developers, we have provided screencast tutorials via another Drupal project, SciVee ( http://scivee.tv ), which can be accessed via  http://gmod.org/gmod-dbsf . We anticipate that the uptake of the Drupal CMS by the bioinformatic community will result in a powerful new set of tools. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Reconstruction of genealogical relationships with applications to Phase III of HapMap</Title>
    <Doi>10.1093/bioinformatics/btr243</Doi>
    <Authors>Kyriazopoulou-Panagiotopoulou Sofia, Kashef Haghighi Dorna, Aerni Sarah J., Sundquist Andreas, Bercovici Sivan, Batzoglou Serafim</Authors>
    <Abstract>Motivation: Accurate inference of genealogical relationships between pairs of individuals is paramount in association studies, forensics and evolutionary analyses of wildlife populations. Current methods for relationship inference consider only a small set of close relationships and have limited to no power to distinguish between relationships with the same number of meioses separating the individuals under consideration (e.g. aunt–niece versus niece–aunt or first cousins versus great aunt–niece).</Abstract>
    <Body>1 INTRODUCTION Algorithms for relationship inference can greatly benefit association and linkage studies by detecting undeclared or misspecified relatives, and have applications in forensics and wildlife population management. Existing methods ( Epstein  et al. , 2000 ;  McPeek and Sun, 2000 ;  Sun  et al. , 2002 ) based on Hidden Markov Models (HMMs) only consider a small number of relationship types. RELPAIR ( Epstein  et al. , 2000 ) examines eight types of relationships (full siblings, parent–child, avuncular, grandparent–grandchild, half siblings, first cousins, monozygotic twins, and unrelated). PREST-plus ( McPeek and Sun, 2000 ;  Sun  et al. , 2002 ) extends this set of alternative relationships to include the relationship types half avuncular, half first cousins and half-siblings first-cousins. Algorithms for pedigree reconstruction ( Berger-Wolf  et al. , 2007 ;  Koch  et al. , 2008 ;  Riester  et al. , 2009 ;  Riester  et al. , 2010 ) partition the individuals into sets of sibling and parent–child pairs ( Blouin, 2003 ;  Jones and Arden, 2003 ) and are not designed for datasets containing distantly related individuals. Additionally, the methods mentioned above cannot differentiate between  rotated  relationships, that is, between relationships with the same number of common ancestors and the same number of meioses separating the individuals under consideration (e.g. aunt–niece versus niece–aunt or first cousins versus great aunt–niece). Distinguishing between such relationships facilitates pedigree reconstruction from relatively distant relationships. We developed CARROT (ClAssification of Relationships with ROTations), a new framework for relationship inference that leverages linkage information to differentiate between  rotated  relationships. As suggested by  Skare  et al.  (2009 ), we grouped relationships between outbred individuals into three categories, based on whether the individuals under consideration have one or two common ancestors or whether one of them is the ancestor of the other. Building on the ideas of  Stankovich  et al.  (2005 ) and  Bercovici  et al.  (2010 ), we defined three sets of HMMs, one for each category of relationships. CARROT uses a novel heuristic to decide whether one of the two individuals under consideration is closer to the common ancestors than the other and benefits from haplotype phasing in order to distinguish between certain  rotated  relationships. We demonstrated that CARROT achieves higher accuracy than maximum-likelihood approaches, such as RELPAIR and PREST-plus, using simulated data. We also applied CARROT on real data from Phase III of the HapMap Project ( The International HapMap 3 Consortium, 2010 ). In addition to validating relationships reported by  Pemberton  et al.  (2010 ), we identified previously unreported third- and fourth-degree relationships in this dataset. 2 METHODS 2.1 Overview of CARROT CARROT is a framework for predicting the relationship type for a pair of individuals from their genotypes. Similarly to previous work ( Epstein  et al. , 2000 ;  McPeek and Sun, 2000 ;  Sun  et al. , 2002 ), CARROT uses HMMs to compute the likelihood of the genotype data under a set of alternative relationships. Unlike existing methods, which define a different HMM for each relationship type, CARROT defines three HMM templates from which it generates HMMs for a broad range of relationships. CARROT uses the likelihoods computed by all alternative models as features of a classifier together with additional features that quantify the overall genetic sharing between the two individuals. In this work, we use the term  Identity By Descent  or  Identical By Descent  (IBD) to refer to genomic regions inherited from the Most Recent Common Ancestors (MRCAs) of the two individuals. The additional features we consider are the percentage of IBD, the number of transitions between IBD and non-IBD regions and the  haplo-frequencies  of the two individuals in these regions. Roughly speaking, the  haplo-frequency  of an individual  A  in a given genomic region is the fraction of the reference population that has the same haplotype as  A  in that region. The use of phased data enables CARROT to assign different likelihoods to relationships that are indistinguishable by previous methods: in the case of an aunt–niece pair, for instance, segments inherited from the common ancestors can lie on either haplotype of the aunt, but must lie on the same haplotype of the niece. This haplotype model, combined with linkage information captured in the  haplo-frequencies , gives CARROT a significant advantage in differentiating between  rotated  relationships compared with the existing methods. 2.2 HMMs and factorial HMMs A HMM is a probabilistic model for capturing the dependencies of a sequence of observations  G 1 ,  G 2 ,…, G M  on a chain of unknown (or hidden) variables  S 1 ,  S 2 ,…, S M  taken from a set 𝒮. An HMM makes the following conditional independence assumptions: first, given  S k ,  G k  is conditionally independent of all observations and hidden states, that is  P ( G k | S 1 ,…, S k ,  G 1 ,  G k −1 )= P ( G k | S k ). Second, given  S k −1 ,  S k  is conditionally independent of all previous hidden states, that is,  P ( S k | S 1 ,…, S k −1 )= P ( S k | S k −1 ). An HMM is, therefore, defined by a set of transition probabilities  P ( S k | S k −1 ), a set of emission probabilities  P ( G k | S k ) a probability distribution over the initial states. Often, we want to infer the value of the hidden variables from the observed variables. The posterior probability  P ( S i | G ) can be computed using the forward–backward algorithm in time  O ( M |𝒮| 2 ) ( Rabiner and Juang, 1986 ), where |𝒮| is the number of values in 𝒮. In a factorial HMM ( Ghahramani and Jordan, 1997 ), the observation at position  k  depends on multiple hidden variables,  S k 1 ,  S k 2 ,…, S k T , which are assumed to evolve independently, that is:
 A factorial HMM where each hidden variable  S k t  takes values from the set 𝒮 is equivalent to an HMM with hidden variables taking values from the Cartesian product 𝒮 T . Using the latter representation, running the forward–backward algorithm on a factorial HMM requires  O ( M |𝒮| 2 T ) time. However, by taking advantage of the independence assumptions for the hidden variables, the forward–backward algorithm can be modified to run in  O ( MT |𝒮| T +1 ) time, which is a significant improvement when the number of hidden variables  T  is large. 2.3 Likelihood computation assuming linkage equilibrium We want to infer the relationship between two individuals  A  and  B , genotyped at a set of  M  unlinked SNPs. Let  H A 0 ,  H A 1 ,  H B 0 ,  H B 1 ∈{ A ,  C ,  G ,  T } M  be the two haplotypes of  A  and  B , respectively,  G A =( H A 0 ,  H A 1 ),  G B =( H B 0 ,  H B 1 ) be their ordered, or phased, genotypes, and θ k  be the probability of recombination between SNPs  k  and  k +1. Throughout this work, we assume that θ k  is the same for both sexes. Let ℛ be a set of putative relationships for individuals  A  and  B . For any relationship  R ∈ℛ, we want to compute the likelihood of  R , or the probability of the observed genotypes under the assumption that the true relationship between  A  and  B  is  R ,  L R = P ( G A ,  G B | R ). As noted in  Skare  et al.  (2009 ), assuming that  A  and  B  are not inbred, their relationship must fall into exactly one of the following categories:
 A  and  B  share exactly 2 MRCAs (e.g. full siblings, first cousins); A  and  B  share exactly 1 MRCA (e.g. half siblings, half cousins); and A  is the ancestor of  B  or vice versa. 
 We call relationships  R 1  and  R 2   rotated , if  R 1  and  R 2  are in the same relationship category and the total number of meioses between the two individuals is the same in  R 1  and  R 2 . Alternatively, we say that  R 1  is a  rotation  of  R 2 . We defined a set of HMMs for each of three relationship categories similarly to  Stankovich  et al.  (2005 ) and  Bercovici  et al.  (2010 ), who defined HMMs for cousins parameterized by the number of generations between them. Unlike these methods, the state space of our models does not increase with the number of generations of the pedigree. Below, we describe our HMMs for the first type of relationships. The models for the other two cases are derived along similar lines. Given that  A  and  B  have two MRCAs,  C  and  D  ( Fig. 1 ), the hidden state at SNP  k  depends on the following binary variables:
 m C ( k ) and  m D ( k ) indicate whether  C  and  D , respectively, passed the same allele to their immediate descendants  E 1  and  F 1 . For example, if both  E 1  and  F 1  inherited the maternal allele of  C  at position  k , then  m C ( k )=1. If  E 1  received the maternal allele of  C , and  F 1  received the paternal allele of  C , then  m C ( k )=0. m E 1 ( k ) and  m F 1 ( k ) indicate whether  E 1  and  F 1  passed to  E 2  and  F 2 , respectively, the allele of  C  and not the allele of  D . d A ( k ) takes the value 0 if  A  inherited the allele that  E 2  got from  E 1  (which came from either  C  or  D ) and the value 1 otherwise. That is,  d A ( k )=0, if for all  i &gt;2,  E i  got from  E i −1  the allele of  E i −2  and not the allele of  G i −2 . If  d A ( k )=1, we will say that there were off−chain donations in the lineage of  A  at position  k .  d B ( k ) is defined in an analogous way for the lineage of  B . p A ( k ) indicates which of the alleles of  A ,  H A 0 ( k ) or  H A 1 ( k ), comes from  E gen A  and is used to capture phasing errors.  p B ( k ) is defined in an analogous way. 
 Fig. 1. Pedigree for a pair of individuals with two common ancestors: individuals  A  and  B  share two MRCAs,  C  and  D . There are gen A  generations between the MRCAs and  A  (i.e. gen A +1 meioses separating them) and gen B  generations between the MRCAs and  B . The sex of the individuals is arbitrary. Each of these variables refers to a different set of meioses in the pedigree, therefore they all evolve independently from each other. We thus model the process of generating the genotypes  G A  and  G B  as a factorial HMM with hidden state  s ( k )=( m C ( k ),  m D ( k ),  m E 1 ( k ),  m F 1 ( k ),  d A ( k ),  d B ( k ),  p A ( k ),  p B ( k )). The transition probabilities for all the variables are shown in  Table 1 . We now derive the transition probabilities for  d A . Let  gen A  be the number of generations between the MRCAs and  A  ( Fig. 1 ). The number of meioses between  E 2  and  A  is  gen A −1. Assume that  d A ( k )=0, that is, the allele that  E 2  inherited from the MRCAs at locus  k  was passed down to  A . Any recombination between loci  k  and  k +1 would result in  d A ( k +1)=1, therefore  P ( d A ( k +1)=0| d A ( k )=0)=(1−θ k ) gen A −1 . If  d A ( k )=1, then there exists at least one off−chain donation in the gen A −1 meioses between  E 2  and  A . The probability that there are exactly  n  off-chain donations between  E 2  and  A  is  (1/2) gen A −1 . Given that there are exactly  n  off-chain donations at SNP  k , the probability that there are no off-chain donations at SNP  k +1 is θ k n (1−θ k ) gen A −1− n . Therefore:
 
 Table 1. Transition probabilities for the HMMs with two MRCAs. Variable Pr (0→0) Pr (1→0) m C θ k 2 +(1−θ k ) 2 2θ k (1−θ k ) m E 1 1−θ k θ k d A (1−θ k ) gen A −1 p A 1−ω ω P ( i → j ) is the probability that a variable transitions from state  i  at SNP  k  to state  j  at SNP  k +1, gen A  and gen B  are the generations between the MRCAs and each of  A  and  B  ( Fig. 1 ), θ k  is the recombination probability between SNPs  k  and  k +1; and ω is the probability of a phasing error. The transition probabilities for the variables  m D ,  m F 1 ,  p B  and  d B  are derived similarly. Given  s ( k ), we can determine the IBD status at SNP  k  and use population allele frequencies to compute the emission probabilities ( Epstein  et al. , 2000 ). To account for genotyping errors, let ϵ be the probability of a genotyping error, and  f ϵ ( x , y ) be the probability that allele  x  is genotyped as  y :
 
Then:
 
where  q c  is the frequency of allele  c  in the reference population. The rest of the emission probabilities are adjusted in a similar way. 2.4 Relationship notation We refer to relationship types using the notation (mrcas, gen A , gen B ). The variable mrcas is 2 when individuals  A  and  B  share two MRCAs and 1 otherwise. Unless  A  is the ancestor of  B  or vice versa, gen A  and gen B  are the number of generations between the MRCA(s) and  A  and  B , respectively. If  A  is the ancestor of  B , then gen A  is set to −1, and gen B  is the number of generations between  A  and  B . For close relationships, we prefer to use the usual verbal description, unless space is limited.  Table 2  shows the numerical notation for some common relationships.
 Table 2. Numerical notation for some common relationships Degree Relationship (mrcas, gen A , gen B ) 1 Full siblings (2, 0, 0) Parent–child (1, −1, 0) 2 Half siblings (1, 0, 0) Aunt-niece (2, 0, 1) Avuncular (2, 0, 1) or (2, 1, 0) Grandparent–grandchild (1, −1, 1) 3 First cousins (2, 1, 1) Great grandparent–grandchild (1, −1, 2) Great aunt–niece (2, 0, 2) Half aunt–niece (1, 0, 1) 4 Half first cousins (1, 1, 1) For all relationships with gen A ≠gen B , there is a corresponding symmetric relationship, for example (1, 0, −1) denotes a child–parent pair. Note that the term ‘avuncular’ does not specify a direction. The terms ‘aunt’ and ‘niece’ should be read as ‘aunt/uncle’ and ‘niece/nephew’, respectively. 2.5 Incorporating linkage information HMMs that use unlinked markers have limited power to distinguish between relationships of the same degree ( Sun  et al. , 2002 ). Linkage information can help disambiguate such relationships. Assume, for instance, that we want to determine whether the relationship between individuals  A  and  B  is first cousins or great aunt–niece. An IBD block between  A  and  B  implies that they inherited overlapping genomic segments from their MRCAs ( Fig. 2 ). If  A  and  B  are first cousins, the two scenarios of  Figure 2  are equally likely. However, if  A  is closer to the MRCAs than  B , then it is more likely that  A  inherited a larger segment from the common ancestor than  B  [scenario (a)], because we expect fewer recombinations between the MRCAs and  A  than between the MRCAs and  B . Therefore, if we compare the haplotypes of  A  and  B  in a small window around the IBD transitions to the haplotypes of a reference population, we are more likely to find a match for the haplotype of  A , than for the haplotype of  B .
 Fig. 2. Illustration of the IBD sharing between a pair of relatives: the horizontal lines represent two haplotypes of individuals  A  and  B  and the bold blue box is inherited from an MRCA. The overlapping part of the blue haplotypes is IBD in the two individuals. The first scenario is more likely if  A  is closer to the MRCA than  B . To quantify this intuition, assume that there is a transition in IBD status between SNPs  k  and  k +1, and let  H i ( k − w +1.. k + w ) be the haplotype of individual  i  at positions  k − w +1,  k − w +2,…, k + w , that is in a  window  of size 2 w  around  k . The  haplo-frequency  of  A  is defined as:
 (1) 
where the sum is over all haplotypes in the reference population,  N  is the number of such haplotypes, and  f ϵ ( H i ( k − w +1.. k + w ), H A ( k − w +1.. k + w )) is the probability that  H i ( k − w +1.. k + w ) is genotyped as  H A ( k − w +1.. k + w ):
 The positions of transitions in IBD status are estimated from the posterior probabilities of IBD of the HMMs using two cutoffs,  c ibd  and  c non−ibd . Positions with posterior probability for IBD larger than  c ibd  are called IBD, while positions with posterior probability lower than  c non−ibd  are called non-IBD. If position  k  is IBD, position ℓ is non−IBD, and positions  k +1…ℓ−1 have IBD probabilities between  c ibd  and  c non−ibd , then we consider  k − w +1..ℓ+ w −1 as the transition window. 2.6 Classifying relationships Existing methods for relationship inference ( Epstein  et al. , 2000 ;  McPeek and Sun, 2000 ;  Sun  et al. , 2002 ) use HMMs to compute the likelihood of the observed genotypes under a set of alternative relationships and then select the relationship that maximizes that likelihood. As explained in  Section 2.5  and demonstrated in the  Section 3 , the likelihoods assuming unlinked markers have limited power to distinguish between relationships of the same degree. To overcome this problem, we used simulated data to train a multiclass discriminant analysis model with the following features:
 the likelihoods of all the alternative models; the total number of transitions between IBD and non-IBD; the estimate of the IBD sharing, (∑ k 0.5 p 1 ( k )+ p 2 ( k ))/ M , where  p 1 ( k ) and  p 2 ( k ) are the posterior probabilities of sharing one and two alleles IBD at SNP  k , respectively, and the sum is taken over all  M  SNPs; and the percentage  r  of transitions between IBD and non-IBD in which  C A &lt; C B , where  C A  and  C B  are the  haplo−frequencies  of  A  and  B , respectively [Equation  1 ]. As explained in  Section 2.5 , when  A  and  B  are equally distant from their common ancestors,  r  is expected to be 0.5. A value of  r  much larger than 0.5 indicates that  C A  tends to be smaller than  C B , so  A  is more distant from the common ancestors than  B . Similarly, a value of  r  much smaller than 0.5 indicates that  B  is more distant from the common ancestors than  A . 
 2.7 Simulations Phased genotypes of 165 individuals from UT, USA, with ancestry from northern and western Europe (CEU) and 88 Tuscans (TSI) were obtained from release 2 of Phase III of the HapMap Project. Some individuals in the CEU population were connected in parent–child pairs. We removed the offspring of all these pairs and combined the remaining 113 CEU individuals with the TSI individuals. After removing SNPs with minor allele frequency smaller than 1% in the combined CEU-TSI population, we obtained a set of 1 133 686 SNPs, which we call the  linked  set of SNPs. SNPs in the  linked  set that were in linkage disequilibrium (LD) were removed using the variance inflation factor method implemented in PLINK, version 1.07, ( Purcell  et al. , 2007 ) to obtain an  unlinked  set of 79 681 SNPs. The HMMs were run on the  unlinked  set, while the  haplo-frequencies  were computed from the  linked  set. The 201 individuals were divided randomly into two subgroups,  T sim , containing 57 CEU and 44 TSI individuals and  T ref , containing 56 CEU and 44 TSI individuals.  T sim  was used to simulate pedigrees, while  T ref  was used as a reference population, to compute allele and haplotype frequencies. Only autosomes were simulated. Following Haldane's model of recombination ( Haldane, 1919 ), we set the recombination probability between SNPs  k  and  k +1 to (1 −  e −2 g k )/2, where  g k  is the genetic distance between the SNPs. Genetic maps were obtained from the HapMap Project. The genotyping error was set to 0.9%. The window  w  for the computation of the  haplo-frequencies  was set to 20, and the cutoffs,  c ibd  and  c non−ibd , were set to 0.9 and 0.2, respectively. These values were chosen based on the accuracy of capturing the IBD transitions on a set of simulated individuals distinct from the training and testing sets. 2.8 Comparison with RELPAIR and PREST-plus We compared our method to RELPAIR, version 2.0.1 ( Epstein  et al. , 2000 ) and PREST-plus, version 4.09 ( McPeek and Sun, 2000 ;  Sun  et al. , 2002 ) on simulated individuals created as described in  Section 2.7 . PREST-plus was run on the same 79 681 unlinked SNPs that was used to run CARROT's HMMs. RELPAIR cannot handle more than 9999 markers, so, similarly to  Pemberton  et al.  (2010 ), we ran it on 5 random subsets of the unlinked SNPs of size 9961 SNPs each. Each subset was created by dividing the 79 681 SNPs into non-overlapping windows of size 8, and then randomly selecting one SNP from each window. The prediction of RELPAIR for a given pair of individuals was set to the relationship predicted in the majority of the five runs. 2.9 Inference of relationships in Phase III of HapMap Phased genotypes of 83 individuals of African ancestry from the southwestern USA (ASW), 165 CEU individuals, 171 individuals of Mexican ancestry from Los Angeles, CA, USA (MXL) and 167 Yoruba individuals from Ibadan, Nigeria (YRI) were obtained from release 2 of Phase III of HapMap. We inferred relationships in each population separately, since, according to  Pemberton  et al.  (2010 ) there is no evidence for population-labeling errors. For each population, we removed SNPs with minor allele frequency smaller than 1%, SNPs in LD, as well as SNPs that failed the hypothesis test of Hardy–Weinberg equilibrium [ P &lt;10 −4 , computed by PLINK, version 1.07 ( Purcell  et al. , 2007 )], and created  linked  and  unlinked  sets as described in  Section 2.7 . To reduce computation time, we first obtained a rough estimate of the IBD sharing by running the HMM for full siblings for all pairs in each population. Pairs with predicted IBD percentage less than 3% were removed from further consideration. This threshold was selected to exclude most of the fifth-degree relatives, since the accuracy of our method is small in such cases. It is possible that some of the fourth-degree relatives were also excluded during this process. We note, however, that the goal of this analysis was not to exhaustively identify all the relationships in the populations studied, but rather to demonstrate the applicability of our method on a real dataset. For the pairs that passed the above cutoff, we ran our HMMs for all relationships of degree up to 5, using previously reported unrelated individuals of the corresponding population to compute allele and haplotype frequencies. The phasing error parameter ω, was set to 0.1%. To infer the relationships of these pairs, we trained our classifiers on 100 pairs of individuals for each relationship of degree up to 5 and 100 unrelated individuals, simulated as described in  Section 2.7 . We verified our predictions for the first- and second-degree relationships by comparing them with the relationships reported in HapMap and the predictions of  Pemberton  et al.  (2010 ). To verify our predictions for relationships of higher degree, we detected sets of three or more individuals that were all predicted to be related to each other and examined whether our predictions for these individuals were mutually consistent. We were very conservative in our verification process, reporting newly discovered third- and fourth-degree relationships as verified only if they had strong support from relationships of smaller degrees. Pedigrees were drawn using HaploPainter, version 1.043 ( Thiele and Nürnberg, 2005 ). 3 RESULTS 3.1 Results on simulated data Comparison with maximum-likelihood methods: we compared CARROT with two existing methods for relationship inference, RELPAIR ( Epstein  et al. , 2000 ) and PREST-plus ( McPeek and Sun, 2000 ;  Sun  et al. , 2002 ). Both these methods use HMMs similar to those defined in  Section 2.3  to compute the likelihoods of a set of alternative relationships and select the relationship within the set with the maximum likelihood. RELPAIR only considers the relationships such as full siblings, parent–child, avuncular, grandparent–grandchild, half siblings, first cousins, monozygotic twins and unrelated. PREST-plus considers the relationships examined by RELPAIR as well as the relationships half avuncular, half first cousins and half-siblings-first-cousins [see  Sun  et al.  (2002 ) for the definition of that relationship]. Neither RELPAIR nor PREST-plus differentiate between  rotated  relationships, so for example they do not distinguish between a grandparent–grandchild pair and a grandchild–grandparent pair. We first compared CARROT, RELPAIR and PREST-plus on the set of relationships that can be handled by RELPAIR, except twins, which are not considered by CARROT. We simulated 100 pairs of phased individuals for each of the relationships full siblings, parent–child, aunt/uncle–niece/nephew, grandparent–grandchild, half siblings, first cousins and unrelated. We ran RELPAIR and PREST-plus on all pairs using the seven aforementioned alternative relationships. The performance of CARROT was assessed using 10-fold cross-validation: The simulated pairs were divided into 10 subsets each containing 10 pairs from each relationship. CARROT was trained on 9 of the subsets, that is, on 90 pairs from each relationship, and tested on the remaining subset. This process was repeated 10 times and the accuracy was averaged over the 10 runs. Figure 3  shows the number of pairs that were predicted  incorrectly  by each method. Although all three methods achieved excellent accuracy for the first-degree relationships, CARROT clearly outperformed both RELPAIR and PREST-plus on second degree pairs, particularly on half siblings. RELPAIR and PREST-plus classified more than half (56 and 68%, respectively) of the half sibling pairs as avuncular. We note that the difference in performance between RELPAIR and PREST-plus can probably be attributed to the fact that RELPAIR was run on smaller sets of SNPs (see  Section 2.8 ).
 Fig. 3. Comparison between CARROT, RELPAIR and PREST-plus on a set of seven relationships: the height of the bars is the percentage of pairs that were classified  incorrectly , so smaller bars are better. In each relationship, the leftmost bar corresponds to RELPAIR, the middle bar to PREST-plus and the rightmost bar to CARROT. Avg. is the average error over all relationships examined. (2, 0, 0): full siblings; (1, −1, 0): parent–child; (2, 0, 1): aunt-niece; (1, −1, 1): grandparent-grandchild; (1, 0, 0): half siblings; (2, 1, 1): first cousins, unrel: unrelated. We also compared CARROT with PREST-plus on the set of nine relationships which includes the seven relationships above as well as half avuncular and half first cousins. As shown in  Figure 4 , CARROT outperformed PREST-plus for most relationships as well as on average. The only relationship for which CARROT performed worse than PREST-plus was first cousins: 13% of the first cousins pairs were classified as half avuncular by CARROT, probably because the  haplo-frequencies  of these two relationships are similar.
 Fig. 4. Comparison between CARROT and PREST-plus on a set of nine relationships: the height of the bars is the percentage of pairs that were classified  incorrectly , so smaller bars are better. In each relationship, the left bar corresponds to PREST-plus and the right bar to CARROT. Avg. is the average error over all relationships examined. (2, 0, 0): full siblings; (1, −1, 0): parent–child; (2, 0, 1): aunt–niece; (1, −1, 1): grandparent–grandchild; (1, 0, 0): half siblings; (2, 1, 1): first cousins; (1, 0, 1): half aunt–niece; (1, 1, 1): half first cousins, unrel: unrelated. As mentioned above, RELPAIR and PREST-plus use HMMs to compute the likelihoods of a set of alternative relationships and select the relationship with the maximum likelihood. However, direct comparison between these methods and CARROT can only be done for a small set of relationships. To extend this comparison to additional relationships, we implemented a classifier that uses the HMMs of  Section 2.3  for the likelihood computation and then selects the relationship with the maximum likelihood. This classifier can serve as a proxy for any method that maximizes the likelihood of unlinked markers. We compared the maximum-likelihood classifier with CARROT using a set of 100 simulated pairs of phased individuals for each relationship of degree up to five, including all  rotated  relationships. We assumed that the degree of the relationship was known, so predictions were made only within each degree. To assess whether a classification-based approach is better that a maximum-likelihood approach, we also ran CARROT using only the likelihoods as features. We observed that for relationships of degree up to two, the likelihoods are sufficient to differentiate between relationships ( Table 3 ). For higher degrees, the likelihoods become less informative and the additional features of CARROT result in a significant increase in accuracy. Additionally, we notice that CARROT performs consistently better than the maximum-likelihood approach, even when we only use the likelihoods as features. Intuitively, the classifier can capture correlations between the likelihoods of different relationships. We observed, for instance, that when the true relationship is great grandparent–grandchild, the likelihood of the relationship great aunt/niece tends to be increased, but this effect is overlooked when we use the maximum likelihood criterion.
 Table 3. Comparison between CARROT and two other approaches for relationship inference Method/Degree 1 2 3 4 5 Max likelihood 100 98.6 62.29 39.33 26.73 CARROT likelihoods 100 100 74 43.78 29 CARROT 100 100 84 57.56 34.45 max likelihood  selects the relationship with the maximum likelihood;  CARROT likelihoods  uses only the likelihoods as features for the classification. We simulated 100 pairs of individuals from each relationship and performed predictions only within each degree. The number reported is the percentage of pairs classified in the correct relationship. Differentiating between rotated relationships: to evaluate the ability of CARROT to distinguish between  rotations  of relationships, we simulated 100 pairs of individuals for each of the possible relationships of degree up to five, including all possible  rotated  relationships. We first assumed that the degree of each relationship was known, and ran CARROT separately for each degree. We started by examining the ideal case of perfect phasing, so we set the probability of phasing errors, ω, to zero. We assessed CARROT's accuracy using 10-fold cross-validation, as described in the previous section: the simulated pairs were divided into 10 subsets each containing 10 pairs from each relationship. CARROT was trained on nine of the subsets and tested on the remaining subset. This process was repeated 10 times and the accuracy was averaged over all 10 runs. We defined the prediction accuracy as the number of pairs that were classified in the correct relationship. When run on first- and second-degree relatives, CARROT achieved perfect performance. The results for the third- and fourth-degree relationships are summarized in  Tables 4  and  5 , respectively. The average accuracy over all the pairs of the corresponding degree, was 83.86 and 56.89%, respectively. For the fifth-degree relationships, the average accuracy was 34.45% (full results not shown). As expected, the accuracy of our classifiers drops as the degree of the relationship increases. However, even within the same degree, some relationships are much harder to predict correctly than others. For example, the two half-avuncular relationships, (1, 0, 1) and (1, 1, 0), are hard to differentiate from each other and from first cousins, since the difference in the distance of each of the individuals from their MRCA is not enough for the  haplo-frequencies  to distinguish them from a balanced relationship where both individuals are equally distant from the MRCAs. Similarly, although the average accuracy for the fifth-degree relationships was 34.45%, the (2, 4, 0) and (2, 0, 4) pairs were predicted correctly in 48.5% of the cases.
 Table 4. Classification accuracy of CARROT on third-degree relatives Rel. 2,0,2 2,1,1 2,2,0 1,−1,2 1,0,1 1,1,0 1,2,−1 2,0,2 88 – – 5 5 2 – 2,1,1 – 84 1 – 7 8 – 2,2,0 – 1 88 – 2 4 5 1,−1,2 2 – – 96 1 1 – 1,0,1 – 11 – – 68 21 – 1,1,0 – 8 – – 24 68 – 1,2,−1 – – 3 – 1 1 95 The value at row  i  and column  j  is the percentage of pairs of relationship  i  that were predicted to be of relationship  j . (2, 0, 2): great aunt–niece; (2, 1, 1): first cousins; (2, 2, 0): great niece–aunt; (1, −1, 2): great grandparent–grandchild, (1, 0, 1): half aunt–niece; (1, 1, 0): great niece–aunt; (1, 2, −1): great grandchild–grandparent. 
 Table 5. Classification accuracy of CARROT on fourth-degree relatives Rel. 2,0,3 2,1,2 2,2,1 2,3,0 1,−1,3 1,0,2 1,1,1 1,2,0 1,3,−1 2,0,3 76 4 1 – 8 8 3 – – 2,1,2 1 42 24 – – 15 11 7 – 2,2,1 – 24 43 – – 5 13 15 – 2,3,0 – – 4 73 – 1 3 10 9 1,−1,3 4 1 – – 80 13 – 2 – 1,0,2 – 13 9 – – 52 14 12 – 1,1,1 – 8 19 – – 28 14 31 – 1,2,0 – 5 13 – – 13 17 52 – 1,3,−1 – – 1 4 – 2 1 12 80 The value at row  i  and column  j  is the percentage of pairs of relationship  i  that were predicted to be of relationship  j . Predictions across degrees: since in practice the degree of the relationship is not necessarily known, we also performed cross validation on a set of 200 pairs of individuals from each of the relationships of degree up to 5, including  rotated  relationships, as well as 200 pairs of unrelated individuals. The average classification accuracy was 57.5%, varying widely for different degrees: all the first-degree pairs were classified correctly; the average accuracy for the second-degree pairs was 99.5%, for the third-degree pairs 76.57%, for the fourth-degree pairs 46% and for the fifth-degree pairs 23.36%. Finally, 90% of the unrelated individuals were classified correctly. We note that there was a small decrease in accuracy compared with the results of the previous section, because some of the pairs were classified in relationships of the incorrect degree, while this was never the case when only within-degree predictions were made.  Table 6  shows the percentage of pairs that were classified in a relationship of the correct degree for each of the degrees examined. On average, the correct degree was predicted for 89.83% of the pairs.
 Table 6. Ability of CARROT to predict the degree of a relationship Degree 1 2 3 4 5 Unrel 1 100 – – – – – 2 – 99.5 0.5 – – – 3 – 0.57 93.57 5.71 0.14 – 4 – – 3.94 82.06 14 – 5 – – – 12.41 86.64 0.95 Unrel – – – – 10 90 The value in row  i  and column  j  is the percentage of pairs of degree  i  that were classified in a relationship of degree  j . The effect of phasing errors: the phasing error rate is defined as the proportion of successive pairs of heterozygote SNPs that are phased incorrectly with respect to each other. To examine the effect of phasing errors on the classification accuracy of CARROT, we simulated 100 pairs of individuals for each of the third-degree relationships, introduced phasing errors at various rates in the range 0–1% and performed 10-fold cross-validation, as described above. We observed that, as the phasing error increased from 0% to 1%, there was an almost linear decrease in classification accuracy from 84% to 67.57%, although for phasing errors in the range 0.0–0.1%, the accuracy was almost unaffected. Current algorithms for statistical phasing of genotype data, such as BEAGLE ( Browning and Browning, 2007 ), have phasing error rates ranging between 0.05% and 6%, depending on the number of genotyped individuals and markers. Recently, however,  Fan  et al.  (2011 ) and  Yang  et al.  (2011 ) proposed new experimental techniques for whole-chromosome haplotyping, which promise to achieve much smaller error rates than current methods. Given these recent developments in phasing techniques, we believe that the phasing error requirements of our algorithm are realistic. 3.2 Inference of relationships in Phase III of HapMap Phase III of the HapMap dataset contains genotypes of 1184 individuals from 11 populations in more than a million SNPs. To facilitate phasing, many of these individuals are connected in trios (two parents and an offspring) or duos (a parent and an offspring). Recently, however,  Pemberton  et al.  (2010 ) discovered many additional first- and second-degree relationships in the HapMap collection. We applied CARROT on four of the 11 HapMap populations: 83 individuals of African ancestry from the southwestern USA (ASW), 165 individuals from UT, USA, with ancestry from northern and western Europe (CEU), 171 individuals of Mexican ancestry from Los Angeles, CA, USA (MXL) and 167 Yoruba individuals from Ibadan, Nigeria (YRI). These populations were selected because they contained both previously reported first-degree relationships and additional relationships detected by  Pemberton  et al.  (2010 ). Since  Pemberton  et al.  (2010 ) did not report any relationships of degree greater than two, to verify the novel predictions of CARROT, we identified sets of three or more related individuals and examined whether our predictions for these individuals were consistent with each other and with previously reported relationships. Table 7  summarizes our findings. For each population, we report:
 the number of predictions that agreed with previously known relationships; the number of identified pairs for which the relationship predicted by CARROT was inconsistent with previously known relationships but the degree of the relationship was in agreement with previously known relationships; the number of identified pairs for which the degree of the predicted relationship was inconsistent with previously known relationships; and the number of identified pairs for which we could not determine whether CARROT's prediction was correct or not based on previously known relationships. 
 Table 7. Predictions of CARROT on four HapMap populations Pop Degree Reported relatives New relatives Total Correct Degree only Incorrect Correct Unverified Degree only Incorrect CEU 1 97 – – – – – – 97 2 2 – – – – – – 2 3 – – – 2 – – – 2 4 – – – 1 13 1 – 15 Total 99 – – 3 13 1 – 116 MXL 1 56 – – – – – – 56 2 7 – – – – – – 7 3 – – – 2 – 1 – 3 4 – – – 2 – – 1 3 Total 63 – – 4 – 1 1 69 YRI 1 105 – – – – – – 105 2 2 1 – – – – – 3 3 – – – 1 3 1 1 6 4 – – – 1 3 – – 4 Total 107 1 – 2 6 1 1 118 ASW 1 46 – – – – – – 46 2 7 3 3 – – 1 – 14 3 – – – 7 4 1 – 12 4 – – – 1 21 – – 22 Total 53 3 3 8 25 2 – 94 The results are grouped per population and relationship degree. New Relatives are relative pairs identified by CARROT that have not been previously reported. Correct predictions are predictions that agree with previously reported relationships. The column ‘Degree Only’ refers to cases where CARROT predicted correctly only the degree of the relationship. ‘Unverified’ relationships are those for which we could not determine whether CARROT's prediction was correct or not. We note that there are small discrepancies between the numbers of first- and second-degree relationships that we report here and the numbers reported by  Pemberton  et al.  (2010 ). These discrepancies can be partially explained by the fact that  Pemberton  et al.  (2010 ) used release 3 of Phase III of HapMap, while we used release 2. Additionally, unlike  Pemberton  et al.  (2010 ), we did not include in our analysis individuals who failed the quality control filters during phasing. In total, we identified 23 previously unreported third-degree pairs and 44 previously unreported fourth-degree pairs.  Figure 5  shows one of the pedigrees we detected. This pedigree involves three families from the MXL population, M008, M010 and M012. All the depicted first-degree relationships as well as the aunt–nephew relationship between the individuals NA19660 and NA19664 have been reported previously. However, our algorithm correctly identified the first cousin pairs NA19664–NA19685 and NA19664–NA19662, therefore CARROT reconstructs this pedigree even in the absence of genotype data for the mother of NA19685 and NA19662. A list of all the previously unreported relative pairs detected by CARROT is given in the supplement.
 Fig. 5. Example of a pedigree identified by CARROT: the pedigree involves three families from the MXL population, M008, M010 and M012. We note that, although for the CEU and MXL populations, the accuracy of our algorithm is consistent with the simulation results of  Section 2.7 , in the other two populations there was a drop in accuracy. This difference in performance might be a result of an increased phasing error in the YRI and ASW populations. Additionally, in the ASW population, the degree of relatedness is unexpectedly high, with 94 identified relationships in a sample of 83 individuals. We observed, for example, that 14 individuals from eight HapMap families (2483, 2487, 2488, 2489, 2490, 2492, 2495, and 2496) are connected to each other. With less than 50 presumably unrelated individuals [there are 53 individuals reported as unrelated in the HapMap ASW dataset, even if we ignore the relationships reported here and in  Pemberton  et al.  (2010 )], the estimation of allele and haplotype frequencies becomes problematic, especially in an admixed population like ASW. To help determine whether the previously unreported pairs detected by CARROT are predicted correctly, we ran KING ( Manichaikul  et al. , 2010 ) on all the identified third-degree relatives. KING uses identity by state to distinguish relatives of degree up to three from unrelated individuals. Six out of the seven pairs of individuals that were predicted to be third-degree relatives by CARROT and could not be otherwise verified, were also predicted to be third-degree relatives by KING. We thus concluded that these six pairs are most likely true third-degree relatives. We notice that KING correctly predicted only 13 out of the 16 verified third-degree relatives. It is therefore likely that the last unverified third-degree prediction for which there was a disagreement between CARROT and KING is also a true third-degree pair. 4 DISCUSSION We presented CARROT, a novel framework for relationship inference that uses linkage information to infer more distant relationships than existing methods and to distinguish between  rotated  relationships, that is, relationships with the same number of common ancestors and the same number of meioses separating the individuals under consideration (e.g. aunt–niece versus niece–aunt or first cousins versus great aunt–niece). We demonstrated that CARROT achieved superior accuracy on relationships of degree up to four, clearly outperforming previous methods, such as RELPAIR and PREST-plus on simulated data. We also applied CARROT on data from four HapMap populations, ASW, CEU, MXL and YRI, and correctly identified the vast majority of the first- and second-degree relatives recently detected by  Pemberton  et al.  (2010 ). Additionally, CARROT detected 67 previously unreported third- and fourth-degree relative pairs. A possible shortcoming of CARROT stems from its current reliance on phased data. However, state of the art statistical methods for phasing achieve phasing errors smaller than 1% ( Browning and Browning, 2007 ;  Howie  et al. , 2009 ) when large cohorts and dense SNP panels are used. Additionally, novel experimental phasing techniques have achieved haplotyping of whole chromosomes with accuracy of 99.8% ( Fan  et al. , 2011 ;  Yang  et al. , 2011 ). Methods for detecting genealogical relationships and for disambiguating between  rotated  relationships can facilitate pedigree reconstruction from cohorts such as the HapMap and WTCCC ( Wellcome Trust Case Control Consortium, 2007 ) datasets, where most individuals are not expected to be closely related. Existing algorithms for pedigree reconstruction that are based on the detection of parent–offspring and sibling pairs cannot be applied in such datasets. In studies such as WTCCC, that are based on the premise that the individuals in the dataset are unrelated, putative relatives are usually removed or reweighted. Knowledge of the exact pedigrees, however, allows a more informed selection of the individuals to be removed. More importantly, instead of discarding the information captured in related individuals, one can leverage the reconstructed relationships in a meta-analysis step. Combining the linkage analysis from multiple reconstructed partial pedigrees with the association analysis over the remaining unrelated individuals can potentially increase the power of such studies. CARROT was designed with such applications in mind and aims at bridging the gap between algorithms for pedigree reconstruction from close relatives and traditional relationship inference methods. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>UTGB toolkit for personalized genome browsers</Title>
    <Doi>10.1093/bioinformatics/btp350</Doi>
    <Authors>Saito Taro L., Yoshimura Jun, Sasaki Shin, Ahsan Budrul, Sasaki Atsushi, Kuroshu Reginaldo, Morishita Shinichi</Authors>
    <Abstract>The advent of high-throughput DNA sequencers has increased the pace of collecting enormous amounts of genomic information, yielding billions of nucleotides on a weekly basis. This advance represents an improvement of two orders of magnitude over traditional Sanger sequencers in terms of the number of nucleotides per unit time, allowing even small groups of researchers to obtain huge volumes of genomic data over fairly short period. Consequently, a pressing need exists for the development of personalized genome browsers for analyzing these immense amounts of locally stored data. The UTGB (University of Tokyo Genome Browser) Toolkit is designed to meet three major requirements for personalization of genome browsers: easy installation of the system with minimum efforts, browsing locally stored data and rapid interactive design of web interfaces tailored to individual needs. The UTGB Toolkit is licensed under an open source license.</Abstract>
    <Body>1 INTRODUCTION Browsing genomic information has been central to life science analysis since large-scale genomes became available for many species including mammals, invertebrates, plants and insects. To support the task of analyzing genomic information, two categories of genome databases are available. The first category includes generic genome database species maintained by large organizations, such as Ensembl (Hubbard  et al. ,  2009 ), UCSC (Kuhn  et al. ,  2009 ) and NCBI (Wheeler  et al. ,  2007 ). The other category includes species-specific genome databases, such as SGD (Hong  et al. ,  2007 ), FlyBase (Wilson  et al. ,  2008 ) and Wormbase (Rogers  et al. ,  2007 ), which utilize general-purpose genome browsers, such as GBrowse (Stein  et al. ,  2002 ). Both types of genome browser represent centralized resources because of the high cost of building and maintaining web database servers. Due to the limited number of experts available to maintain the servers, updates of these centralized web servers are likely to be slow, which is tolerated because the amount of genomic data output by Sanger sequencers is relatively small, yielding on the order of ∼10 million nt per week. Today, however, the rate at which genome information is produced has outperformed the pace of centralized web browser maintenance. Indeed, the recent advent of high-throughput DNA sequencers (e.g. Solexa/Illumina, SOLiD/ABI and 454/Roche) allows even small groups of people to collect huge amount of genomic data in a fairly short period, billions of nucleotides per week, requiring the data analysis to be done as quickly as possible. In particular, generating tracks for investigating personalized data becomes a crucial step in conducting specific analysis. The UCSC Genome Browser, for example, offers the function of managing custom tracks that upload and display personal data in local disks to the UCSC Genome Browser together with well-annotated existing tracks. These functions in traditional genome browsers are useful but suffer from three major drawbacks. First, the users have to upload their novel and confidential data to the server, though they want to keep these datasets in their local files before publication. Second, although anonymization of the data is partially supported by UCSC, uploading large volumes of data (e.g. 1 GB of Solexa reads data) to the remote web server still needs enourmous amount of time. In order to avoid the costs of data uploads, installing a private version of these genome browsers is desirable; however, the task is extremely hard for non-programming biologists because it demands expertise in programming. Third, personalization of web interface is limited and time consuming, though personalizing web interface through a variety of rearrangements is an important step toward finding new ideas. For example, Affymetrix developed the Integrated Genome Browser (IGB) (Affymetrix,  http://www.affymetrix.com/partners_programs/programs/developer/tools/affytools.affx .) for integrating various types of biological data provided by DAS (Distributed Annotation System) (Dowell  et al. ,  2001 ) servers. We developed the (University of Tokyo Genome Browser) UTGB Toolkit to provide solutions to these three major requirements: browsing locally stored data, ease of system installation with minimum effort and custom web interface design. Installing the UTGB Toolkit locally on one's own computer is quite easy for inexperienced users because the system can be installed with a few steps, and also avoids uploading confidential data to the remote server. Furthermore, the UTGB Toolkit packages ready-to-use functions, such as a stand-alone web server, HTML rendering functionality, database engines, into one component so that these functions can be made available at private sites immediately after the installation.  Figure 1  shows how the UTGB Toolkit facilitates personalization of the web interface in the study of epigenomics. Studying epigenomics involves searching for meaningful combinations from a variety of genome-wide data resources such as DNA methylation, nucleosome positions, transcription start sites, gene expression and evolutionary conservation (Kasahara  et al. ,  2007 ; Sasaki  et al. ,  2009 ), which can now be observed via high-throughput sequencers. Our toolkit makes it quite easy to develop a genome browser with drag-and-drop functions for rearranging, juxtaposing and resizing relevant tracks side-by-side interactively to highlight how epigenetic controls are responsible for gene expression and evolution.
 Fig. 1. Rapid creation of tailored interface. ( A ) A number of tracks in a UTGB genome browser. A long track with a large amount of information can be resized to a shorter track interactively using the scroll bar, which allows the user to browse the content of the original long track. It is also quite easy to eliminate tracks irrelevant to a particular analysis. ( B ) The resulting tracks can be reordered to facilitate the further analysis. ( C ) Tracks can be rearranged using the interactive drag-and-drop interface. ( D ) A genome browser tailored to the analysis of nucleosome positioning surrounding transcriptional start sites and its effect on genetic variation. The track for specificity of short reads is useful in assessing the uniqueness of short-read alignments on the genome. It takes &lt;1 min to perform all the steps. 2 METHODS 2.1 Genome browser interface To enhance the portability of the system, the UTGB Toolkit is implemented in Java, a portable programming language, such that its machine codes run on top of the Java Virtual Machine. Thus, the UTGB Toolkit is executable on most commonly available platforms, including Mac OS X, Windows, Linux, Solaris and FreeBSD. In addition, UTGB Toolkit is designed such that the browser interface runs on most common web browsers, such as IE, Safari, Firefox and Opera, although it is still necessary to settle discrepancies between these web browsers. To achieve this goal, the interface is compiled into JavaScript code via the Google Web Toolkit (GWT) compiler, which is capable of subsuming the differences between JavaScript engines in the individual web browsers. 2.2 Portable web server for quickly browsing local resources UTGB Toolkit contains a portable web server so that the genome browser can be launched from the user's personal computer. To avoid manual installation of the web server program (e.g. Apache), the UTGB Toolkit has an embedded Tomcat web server engine. The Tomcat server in the UTGB Toolkit works as a stand-alone web server that is not resident on the system. UTGB Shell launches an instance of Tomcat with the ‘utgb server’ command, and then deploys the genome-browser program on the local Tomcat server. The portable web server is useful not only for avoiding the installation process, but also for browsing locally stored data resources without losing data privacy. With the web server running on the user machine, no need exists to upload confidential data, such as personal genomic data, to a remote database center. Although anonymization of the data is supported in the UCSC genome browser, the cost of uploading large volumes of data is still prohibitive. As a solution to this problem, the user can utilize the UTGB running on the local machine to simultaneously display local tracks, whose data are kept on the local hard disk, and publicly available tracks. 2.3 Ensuring portability via the embedded database engine The database management system (DBMS) is an essential component of the genome browser used to provide genomic data for drawing tracks. However, its installation and setup are quite complicated tasks even for database experts. To avoid problems in setting up database engines, we embedded the SQLite ( http://www.sqlite.org/ ) database engine into the UTGB Toolkit. Connections to other DBMS, such as MySQL, PostgreSQL, are also supported in the UTGB Toolkit through JDBC (Java Database Connection) ( http://java.sun.com/products/jdbc/ ). However, unlike these DBMS that use several files to store the database contents, SQLite is portable in that it uses a single file with a universal format, which can work across several operating systems. To make the DBMS available in any OS environment, we developed an SQLite JDBC connection library, which packs natively compiled SQLite binaries (SQLite is written in C) for operating systems such as Windows, Mac OS and Linux. To support other operating systems for which the SQLite binary is not available, our SQLite JDBC library also contains a pure Java SQLite database engine, which works in any environment that supports Java. Therefore, even if the computer has no DBMS, running the UTGB browser with database support is possible. In addition, with the embedded SQLite database engine, we can easily port the genome browser to other OS environments; e.g. we can make a clone of the genome browser simply by copying the browser code and database files. This portability of the genome browser is a novel feature made possible by the UTGB Toolkit. 2.4 Server-side programming support The UTGB Toolkit is designed to accommodate various requirements of data visualization, as visualization for genomic data tends to be different for each scientific study. Standard graphical representations provided by existing database centers do not always fulfill user needs. The UTGB Toolkit allows developers to use their own data visualization programs, e.g. CGI-based graphic image generators, HTML content renderer. Although many variations in data visualization exist, a common implementation pattern is used in writing web-based graphic generation programs. For example, a typical pattern of server-side graphic drawing is as follows: the web server receives a user request from the browser, and then issues a query to the database. The results of the query are translated to class objects (e.g. gene objects), and finally, image data for visualizing these gene objects are returned to the browser. This pattern contains three major processes: web request handling, database connection and database object mapping. Here, we describe libraries for supporting implementation of these common tasks, and how the UTGB Toolkit eases server-side programming. 2.4.1 Web Action The web request handler in the UTGB Toolkit is called a web action, which is a Java class for receiving HTTP requests. Web actions in the UTGB Toolkit enable developers to rapidly begin coding web interfaces; the ‘utgb action’ command in UTGB Shell generates a web action instantly. Each web action is directly mapped to a web server URL. For example, a web action named ‘sequence’ corresponds to the URL,  http://(server_base_url)/sequence . Parameter values attached to the URL (e.g. ‘sequence?name=chr1&amp;start=100’) are passed to the web action, and these values are automatically assigned to corresponding variables in the web action class. Data types of the request parameters, such as integer and string, are detected automatically from the class definitions of web action class, and parameter values in the URL, which are merely a string type, are automatically converted into appropriate data types. Individual web actions correspond to the genome browser web API. With the web action mechanism, it is possible to avoid writing repetitive code for request handling and data type conversion. 2.4.2 Database connection The second core component is database connection support. In general, web database development with Java requires a database server installation and complex configuration files. Inclusion of the embedded SQLite database engine removes the requirement for database installation, and connections to SQLite databases or other DBMS are immediately available within the web action codes. UTGB Toolkit supports both local SQLite database files and remote databases connected through JDBC. These databases can be used by specifying their system types and database location (file names or URLs) in the config/track-config.xml file. 2.4.3 Object database mapping Relational database engines serve table-formatted data, and an impedance mismatch occurs between table data and their memory representations in computer programs. To use the database data in a program, it is necessary to convert the table data into a more usable format, such as array or class objects in main memory. The UTGB Toolkit provides the BeanUtil library, which supports translation of table data into class objects. Similar to the web action handling, table format data are converted to appropriate class objects by investigating the Java class definitions. Our matching algorithm translates the table data to a set of class objects by comparing column names in the table data and parameter names in the class definition. The matching algorithm automatically converts the data types between table and class objects using the reflection mechanism in the Java language, which provides the information of parameter names and types in the class definitions. Thus, no manual mapping configuration is required to bind table data to class objects. 2.5 Importing biological data The UTGB Toolkit supports visualiztion of commonly used biological data formats, such as BED, DAS (Dowell  et al. ,  2001 ), etc. To create a track for displaying biological features mapped onto the genome, the user has to specify the data files to load (see the documentation at  http://utgenome.org/toolkit/  for details). The UTGB parses these biological data files in a stream manner and generates graphics that display the features in the region specified in the genome browser. Access to DAS data is also supported in the UTGB Toolkit for utilizing existing biological data resources in conjunction with the user's own tracks. We are continuing the effort of improving the UTGB Toolkit to support other biological data formats, such as AGP, GFF, PSL, AXT (alignment data), etc. 3 RESULTS 3.1 UTGB framework for browsing various data sources A notable feature of the UTGB Toolkit is its framework design for integrating multiple web resources ( Figure 2 ). The interface of the UTGB browser consists of tracks, which display individual data resources. To manipulate a set of tracks at the same time, we provide the notion of the track group, which holds variables that are shared between multiple tracks. For example, the user can relocate the window positions of several tracks by changing their track group state, e.g. by clicking the scroll button. The genome browsers generated by using the UTGB Toolkit can display both public and private data sources on a local machine simultaneously.
 Fig. 2. Illustration of the UTGB framework. The UTGB framework has a two-sided design, client- (browser) and server-side code. For the client side, the UTGB Toolkit generates a web browser interface that consists of a set of tracks. Individual tracks communicate with the web servers, and produce track contents from the received data in the form of, for example, graphics or table data. Track groups, which manage a set of tracks, hold common parameters shared among tracks, such as window location on the genome sequence. On the server side, arbitrary web data sources (e.g. HTML, text, XML, database query results, etc.) can be used to generate track contents using mini-browser (iframe) tracks or web resource adapters. Advanced users can implement tracks in Java, which are compiled into JavaScript code, to provide a more sophisticated user interface. 3.2 Fast and flexible genome browser interface revision The interface of the UTGB genome browser displays a list of tracks, each of which can be dragged to a different location, allowing the user to customize the browser interface online. The track interface of the UTGB Toolkit is implemented using JavaScript technology, which can dynamically rewrite HTML components in the browser window, so track relocation can be performed without accessing HTML pages on the server. This feature greatly reduces user frustration when employing traditional genome browsers that reload the entire pages after the track relocation. Reloading of track contents is independent for each track in the browser. Therefore, even if the response time of the server providing a track contents is slow, other track contents served by other servers can be displayed immediately after the arrival of the data. Therefore, the users can continue to relocate sequence positions on the UTGB genome browser seamlessly, eliminating the latency caused by the slowest server to display tracks. 3.3 UTGB shell for quick genome browser development To facilitate rapid genome browser development, we developed the UTGB Shell, a command-line user interface of the UTGB Toolkit.  Figure 3  shows the overview of the UTGB Toolkit and what can be done with the UTGB Shell. The UTGB Shell has several user commands that support development of a personalized genome browser. For example, the ‘utgb create’ command creates a new genome browser in a few seconds, and the ‘utgb server’ command launches a web server on the local machine, which enables immediate use of the genome browser. For developers, track programs that use, for example, database searches, data visualization, can be implemented with minimal programming effort because ‘utgb create’ command generates a genome browser code that already has features such as database connection support, a rich user interface and standard track implementations.
 Fig. 3. Overview of the UTGB Toolkit. The UTGB Toolkit supports development of personalized genome browsers in various ways. The UTGB Shell generates code templates for handling web requests from the genome browser interface (web action), database access support through SQLite JDBC and mapping support from SQL query results to the specified class objects (BeanUtil). Developers can generate track graphics by using the library included in the UTGB Toolkit or their own programs. The generated graphics (or arbitrary HTML contents) can be displayed as track contents in the genome browser interface. To browse both of the locally and remotely stored data, these steps can be performed in a local user machine by launching a local web server from the UTGB Shell. 3.4 Stand-alone and web mode Genome browsers generated by the UTGB Toolkit work in two modes, stand-alone and web modes. In the stand-alone mode, the genome browser runs as a user program on the local computer, so a privileged account is not required to run the genome browser program. This stand-alone mode is useful in testing the behavior of the browser before publishing. The web mode is for publishing the genome browser on a server machine. The browser code is sent to the Tomcat engine, which is a standard web server program for running web applications written in Java. With ‘utgb deploy’ command in the UTGB Shell, we can immediately deploy the genome browser on the Tomcat server. Even if the server machine already is running another web server, such as Apache, these server programs can coexist by bypassing HTTP requests received by the Apache server to the Tomcat engine through the proxy module, and users can use the genome browser contents as if they were served by the Apache web server. Detailed information regarding such settings is available from  http://utgenome.org/ . 3.5 Personalization of genome browser Maintenance of biological databases consists of data conversions to accommodate site-specific data formats, and data submission to appropriate sites. In general, however, this process is not suited to visualizing the huge amounts of data that are now common in the era of large-scale genome analysis. In addition, site-specific data formats and their graphical representations may not be ready or extendable to publish a variety of research results. The UTGB Toolkit tackles these problems by providing a web browser interface to display tracks hosted by multiple web servers in a single window. A set of standard tracks supporting visualization of data resources is already available. The framework design of the UTGB Toolkit provides users with flexibility to browse their own data using both preinstalled and their own visualization programs. To enhance the user experience, we also incorporated the modern web application technologies, such as the Google Web Toolkit (GWT), AJAX and client-side graphic drawing into our toolkit. These technologies make several features of UTGB, such as drag-and-drops, flexible resizing and smooth scrolling, available to both users and web application developers. 3.6 Efficiency of client-side resource integration The Ensembl's Genome Browser (Hubbard  et al. ,  2009 ) is composed of a single set of image data integrating images of several tracks using server-side programs. However, this architecture suffers from a major drawback: when the user clicks a browser button to move the location to display, the genome browser must redraw the whole contents already shown on the page. This type of implementation, which we call server-side integration, is problematic as the server program has to perform similar database queries and repeatedly draw graphics even for slight window relocation. Without a caching mechanism, this type of implementation cannot work efficiently. The major reason why the Ensembl Genome Browser merges images is that the HTTP protocol is stateless; i.e. the browser cannot remember the presented HTML data when the user clicks a link and moves to the next page. To display the next page, the browser must retrieve the entire contents again from the server as stateless web browsers do not allow partial updating of the genome tracks. With recent advances in browser technology, it has become possible to change the HTML contents dynamically after loading HTML and image data using the JavaScript language. Several other technologies are available for drawing graphical contents in the browser, such as Flash, Adobe Air and Microsoft Silverlight. However, these extensions of the web browser sometimes do not work; e.g. if the browser does not have the required plug-ins to run these extensions or if plug-in installation is not allowed for non-privileged users. Therefore, we chose JavaScript which is commonly supported in most modern web browsers, including Internet Explorer (IE), Firefox, Safari and Opera. These browsers already have an embedded JavaScript engine, and therefore no additional installation process is required. Scripts written in JavaScript language can update the displayed browser content  in situ , and enable the web browsers to remember the state of the web page. Therefore, modern web browsers already have the capability to partially modify displayed content while retaining necessary information in memory on the browser (client) side. The UTGB Toolkit fully utilizes this browser (client)-side memory to preserve track contents, which enables the genome browser to support drag-and-drop and resizing of tracks without reloading. The utilization of client-side memory has attracted a great deal of attention not only for the genome browsers, but also for developing general web applications. In September 2008, Google launched a new web browser, Google Chrome. This browser has its own implementation of the JavaScript engine called V8 with major performance improvements in the JavaScript garbage collection mechanism, which efficiently reuses browser-side memory. The open-source web browser Firefox 3 is also continuing to develop the JavaScript engine, and has achieved faster performance than the previous version, Firefox 2. This trend of improving the JavaScript engine lends marked support for client-side resource integration. 3.7 Server-side and client-side graphic drawing With regard to graphical visualization of genomic data, the UTGB Toolkit provides several ready-to-use graphical representations, such as drawing genes and graph representations. In the UTGB Toolkit, these visualization supports are available in server-side code, and we are continuing to develop other types of data visualization. The UTGB Toolkit also supports browser-side graphic drawing with the canvas tag, which will be a standard of browser-side graphic drawing in HTML 5, the next version of HTML. Several web browsers, including Google Chrome, Firefox, Safari and Opera, support drawing graphics with the canvas tag, with IE being the only major browser not providing support for this tag (as of 2008). However, IE can draw graphics using Vector Markup Language (VML), which has similar functionality to canvas for drawing graphics. A team at Google has developed GWT Canvas, a graphic drawing library, which hides differences between VML in IE and the canvas tag, so developers can draw graphics in both IE and other browsers that support this tag. The UTGB Toolkit uses GWT Canvas to draw custom user tracks. The capability of drawing graphics in the browser has an impact on genome browser design because it simplifies the role of the server; the server machine has no need to generate graphics, but rather its role is to serve data objects that represent genomic information such as genes and CDS. The client (browser) receives these data objects and draws their graphical representation on behalf of the server. This two-sided genome browser design greatly reduces the workload on the server machine and simplifies server-side programming; the server needs only to perform database queries and publish the query results. 3.8 Availability of the UTGB Toolkit The UTGB Toolkit is freely available from the UTGB Project Page ( http://utgenome.org/ ). All source code is managed using the source revision control system Subversion, so the latest code is made available immediately. All of our source code is licensed under the Apache License version 2.0, which is an open-source license allowing free use and distribution for both personal and commercial purposes. The Apache License is applied on a file basis; i.e. modified source files must be licensed under the Apache License Version 2.0, while users are free to apply any license they wish to source code generated by the UTGB Toolkit and user-developed source code that simply uses the UTGB Toolkit. 4 DISCUSSION In general, database integration takes various forms: a portal, compound and fully integrated. A portal collects links to internal or external data sources (e.g. PubMed and ENCODEdb); search engines, such as Google and Yahoo, also belong to this category. The second type, compound, displays several database contents at the same time in the browser window. The interface of the UTGB can support this type of data visualization. The IGB (Affymetrix,  http://www.affymetrix.com/partners_programs/programs/developer/tools/affytools.affx .) is also a compoond browser and supports the integration of genome data resources provided by DAS protocol using the Java-based GUI. Another example is iGoogle ( http://www.google.co.jp/ig ), a web service consisting of user-customizable gadgets, sub windows that can display, for example, news or a calendar. The third type comprises fully integrated databases that merge several databases into a single DBMS to support queries across these databases; this type of integration is common in centralized database centers such as NCBI and Ensembl. These various types of database integration each have their own benefits: portals can be used as directories to data resources, compound browsers can collect resources provided by several organizations and full integration is necessary to process queries that cannot be evaluated without integration of databases. For example, a query listing all SNPs surrounding user-selected genes requires both SNP and gene locus databases. Without integration of these databases, the user must click the browser buttons numerous times to navigate through unintegrated database browsers. Our UTGB framework is not restricted to a certain pattern of database integration, and all of three of the above types can be used as back-end databases. However, to achieve faster query performance and maintenance of the integrated databases, further implementation effort are required, for example, integrated query support, query optimization and user-friendly database management interfaces. Funding : Japan Science and Technology Agency (JST). Conflict of Interest : none declared. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Gracob: a novel graph-based constant-column biclustering method for mining growth phenotype data</Title>
    <Doi>10.1093/bioinformatics/btx199</Doi>
    <Authors>Alzahrani Majed, Kuwahara Hiroyuki, Wang Wei, Gao Xin, Wren Jonathan</Authors>
    <Abstract/>
    <Body>1 Introduction Under a standard lab condition, a vast majority of genes have little to no effect on the normal growth of microorganisms ( Korona, 2011 ). These so-called ‘dispensable’ genes account for over 90% in  E.coli  and  B.subtilis  ( Baba  et al. , 2006 ;  Kobayashi  et al. , 2003 ), while over 80% in yeast ( Giaever  et al. , 2002 ;  Kim  et al. , 2010 ). A molecular-network level understanding of the cause of this gene dispensability has important implications in evolution and systems biology ( Bochner, 2009 ). One theory to explain this phenomenon is mutational robustness, which argues that these genes are dispensable because the genetic architecture has evolved to compensate for gene mutations either by duplicate genes or by backup pathways ( Gu  et al. , 2003 ;  Wagner, 2000 ). Another theory is environment-dependent genetic interaction, which argues that these seemingly dispensable genes are actually essential in other environments as the activation of genetic interactions depends on environmental conditions ( Papp  et al. , 2004 ). Whereas both theories could explain dispensable genes, the latter was shown to provide explanations for a majority of dispensable genes in yeast ( Hillenmeyer  et al. , 2008 ). To advance our knowledge of environment-dependent genetic interactions, one key question to address is how to find  co-fit genes , which are defined to be a group of genes that share similar patterns of conditional essentiality and dispensability across various environmental conditions ( Deutschbauer  et al. , 2014 ;  Hillenmeyer  et al. , 2010 ). The illustration in  Figure 1(a)  shows how similar phenotype patterns could help reveal the underlying organization of the genetic interactions.
 Fig. 1 A minimalist example to illustrate environment-dependent genetic interactions. ( a ): Conditionally essential and dispensable genes. The circle, triangle and square symbols illustrate environmental inputs to the cell, for example input metabolites and ligands. Red, gray and black arrows denote active paths in wild type, inactive paths and active paths in each condition, respectively. The wild type grows normally under each condition, while the deletion of each gene has different effects on fitness under different conditions.  Δ X  denotes the strain of deleting gene  X  ( X ∈ { A , B , C } ). ‘GR’ and ‘NG’ stand for normal growth and no growth, respectively. ( b ): The corresponding growth phenotype data. Blue and red denote low and high fitness, respectively. The constant-column bicluster in the green box captures co-fit genes,  A  and  B , which cannot be captured by any other constant biclusters The recent development in genome-wide growth-phenotype (i.e. fitness) profiling methods enabled the measurement of fitness scores of a large number of gene-deletion strains over many stress conditions ( Bochner, 2009 ;  Giaever  et al. , 2002 ;  Hillenmeyer  et al. , 2008 ;  Nichols  et al. , 2011 ). Importantly, such growth phenotype data can be used to assess the effects of a loss-of-function mutation of each gene on fitness and detect which genes are essential and dispensable under different stress conditions. That is, for a given environmental condition, conditionally essential genes are defined to be those whose loss-of-function mutations have very low fitness values, while conditionally dispensable genes are defined to be those whose loss-of-function mutations have very high fitness values. Thus, we can use such growth phenotype data to systematically identify sets of co-fit genes, allowing us to probe how the genetic interactions are organized and how environmental conditions can change the genetic interactions. Such environment-dependent genetic interactions have been commonly analyzed using flux balance analysis (e.g.  Harrison  et al. , 2007 ;  Papp  et al. , 2004 ;  Segrè  et al. , 2005 ). While flux balance analysis is a powerful method that can predict how metabolic activities may change given various environmental and genetic perturbations, its accuracy depends on prior knowledge about the structure of a given metabolic system and metabolic flux boundaries. Here, we propose an alternative, data-driven approach that can be used for analysis of environment-dependent genetic interactions. In this approach, by representing a growth phenotype dataset by a two-dimensional matrix, whose rows are the gene-deletion strains and columns are the stress conditions, we transform a problem of finding sets of co-fit genes into a constant-column biclustering problem (as illustrated in  Fig. 1(b) ). We argue that in growth phenotype data, finding constant-column biclusters results in detecting more meaningful biclusters, i.e. co-fit genes. There is a fundamental difference in the nature of growth phenotype data and gene expression data, the latter of which was the target for almost all existing biclustering methods. In gene expression data, each row (i.e. a gene) has a reference value, which is the expression level of this gene under the normal condition. Thus, the reference values for different rows are different from each other. Although data normalization or transpose can be done to transform the problem of mining gene expression data into the constant-column biclustering problem, mining other types of biclusters, e.g. constant biclusters or coherent biclusters, is more prevalent in mining gene expression data. In contrast, in growth phenotype data, all rows (i.e. strains) have a same reference value, which is the growth of the wild type (without any knock-out) under the normal condition. Thus, detecting constant-column biclusters in such data can identify co-fit genes because such a bicluster implies the deletion of this group of genes has similar effects on fitness (i.e. similar values in the same column imply similar changes to the reference value) under a subset of stress conditions (as illustrated in  Fig. 1 ). This motivated us to develop a novel biclustering method, Gracob, that is designed to identify constant-column biclusters in growth phenotype datasets. To our knowledge, this is the first work that develops and applies biclustering methods to mining co-fit genes in growth phenotype data. The identification of co-fit genes by such a method can be useful for gaining new insights into the functional organization of genes, which has been commonly analyzed using the pairwise correlation coefficient across all the conditions considered in an experimental setup. This is because a co-fit gene measure can detect a significant local fitness similarity under a subset of conditions, while such strong signals can be diluted in the overall correlation coefficient measure owing to the rest of the conditions. We compared Gracob with 13 representative widely used methods that cover a wide spectrum of algorithms and types of biclusters they can detect. When evaluated on a variety of synthetic datasets, Gracob showed nearly perfect performance with respect to different noise levels and overlapping degrees. We then applied Gracob to three real growth phenotype datasets for  E. coli , proteobacteria and yeast. Gracob was able to identify maximal constant-column biclusters while the existing methods failed to do so. Functional enrichment analysis through KEGG pathways and GO terms demonstrated that Gracob is on average more than twice as precise as the other methods. 2 Related work 2.1 Previous biclustering methods The biclustering problem was first proposed by  Cheng and Church (2000)  to analyze gene expression data. Since then, extensive efforts have been made in both computer science and statistics to develop different types of biclustering methods (e.g.  Bergmann  et al.  2003 ;  Ben-Dor  et al. , 2003 ;  Bozdağ  et al. , 2009 ;  Cho  et al. , 2004 ;  Gu and Liu, 2008 ;  Gusenleitner  et al. , 2012 ;  Henriques and Madeira, 2014 ,  2015 ;  Hochreiter  et al. , 2010 ;  Huttenhower  et al. , 2009 ;  Kluger  et al.  2003 ;  Lazzeroni and Owen, 2002 ;  Liu and Wang, 2003 ;  Li  et al. , 2009 ;  Murali and Kasif, 2003 ;  Pandey  et al. , 2009 ;  Prelić  et al. , 2006 ;  Serin and Vingron, 2011 ;  Sheng  et al. , 2003 ;  Tanay  et al. , 2002 ,  2004 ;  Turner  et al. , 2005 ;  Yang  et al. , 2002 ,  2003 ;  Wang  et al. , 2002 ). Existing methods mainly deal with three types of biclusters ( Madeira and Oliveira, 2004 ), i.e. constant biclusters within which the variation is low, constant-column (or constant-row) biclusters within which the column-wise (or the row-wise) variation is low, and coherent biclusters in which the data generally follow an additive or a multiplicative model. As shown in  Figure 1(b) , the problem of finding co-fit genes is equivalent to finding constant-column biclusters in a growth phenotype data matrix. That is, we are interested in finding a group of genes that, under multiple conditions, have similar fitness to each other. However, despite the success of the existing biclustering methods to analyze gene expression data, to the best of our knowledge, there are no other studies that developed and applied biclustering methods to mining growth phenotype data. Here, we review 13 biclustering methods that are widely used in various comparative studies ( Eren  et al. , 2013 ;  Henriques  et al. , 2015 ;  Prelić  et al. , 2006 ), which will be later compared with our method on both synthetic datasets and real growth phenotype datasets. These methods are CC ( Cheng and Church, 2000 ), Plaid ( Lazzeroni and Owen, 2002 ;  Turner  et al. , 2005 ), FLOC ( Yang  et al. , 2003 ), ISA ( Bergmann  et al. , 2003 ), xMOTIFs ( Murali and Kasif, 2003 ), Spectral ( Kluger  et al. , 2003 ), SAMBA ( Tanay  et al. , 2004 ), Bimax ( Prelić  et al. , 2006 ), BBC ( Gu and Liu, 2008 ), QUBIC ( Li  et al. , 2009 ), CPB ( Bozdağ  et al. , 2009 ), iBBiG ( Gusenleitner  et al. , 2012 ) and BicPAM ( Henriques and Madeira, 2014 ). Since most of the existing methods used different definitions of biclusters and were reported to be general as they are not restricted to certain types of data, it is difficult to clearly categorize them. Here we first group these biclustering methods according to the general types of biclusters such methods used for evaluation in their papers or in comparative studies. A typical class of the existing methods work with ‘constant’ biclusters. Here constant is often defined to be the same value after discretizing the input data matrix into 0’s and 1’s (e.g. Bimax and iBBiG). Another major class of the existing methods have their own definitions of the biclusters they are looking for, which do not directly correspond to constant-, constant-column-, or coherent-biclusters. For example, CC uses the mean squared residue to define a bicluster, which basically measures the variance of the individual data points in the biclusters with respect to the mean of the corresponding rows, the corresponding columns and the entire bicluster. Plaid models the data matrix as a sum of layers and minimizes the fitting error through optimization. Similarly, BBC uses the plaid model of biclusters which defines a bicluster as a combination of the main effect, the gene effect, the condition effect and the noise. FLOC extends the CC algorithm by using a probabilistic model to account for missing values in data. ISA requests that the mean value of each row must be higher than a threshold, and so does each column. CPB defines the biclusters in a similar way, i.e. the Pearson correlation coefficient between columns and rows must be higher than a threshold. Spectral tries to detect checkerboard structures. Therefore, this class of methods can theoretically detect different types of biclusters. A number of methods were developed to (preferably) detect constant-column (or equivalently constant-row) biclusters. SAMBA discretizes the data into different bins and finds biclusters with each column belonging to the same bin. Similarly, xMOTIFs attempts to find biclusters within each of which genes have the same state under different samples. The method picks up randomly sampled subsets over the conditions and chooses the corresponding subsets of genes that satisfy this requirement. However, when the number of conditions is large, the chance of picking the proper subsets of conditions becomes very low. QUBIC thresholds the extreme values (both positive and negative) and detects constant-column and constant-row biclusters on the discretized values only. Recently, BicPAM was proposed to detect both additive and multiplicative coherent biclusters. In terms of the techniques such methods use, they can be classified into iterative methods (i.e. CC, ISA, Bimax, CPB, Plaid, FLOC and iBBiG), matrix decomposition-based methods (i.e. ISA and Spectral), graph-based methods (i.e. SAMBA and QUBIC), sampling-based methods (i.e. xMOTIFs and BBC) and pattern mining-based methods (i.e. BicPAM). The iterative methods either gradually grow biclusters from small seeds, or delete columns or rows that cannot be a part of the biclusters from the original matrix. The decomposition-based methods mainly use different variants of singular value decomposition to reduce the dimensionality in order to better detect biclusters. The graph-based methods model the problem in a bipartite graph, and look for cliques or densely connected subgraphs. The sampling-based methods try to control the way of sampling to increase the probability of finding large biclusters. The pattern mining-based methods rely on frequent itemset mining or association rules to identify biclusters. 2.2 Co-fitness measurement with constant-column biclustering Co-fit genes are traditionally defined using the pairwise correlation coefficient of two genes across all the stress conditions, and hierarchical clustering is used to group co-fit genes together ( Deutschbauer  et al. , 2011 ;  Hillenmeyer  et al. , 2008 ;  Nichols  et al. , 2011 ). However, as mentioned in Section 1, the use of correlation coefficient to measure similarity could miss strong signals detected in a subset of conditions owing to ‘correlation dilution’ through the rest of the conditions. To further elucidate this, let us take the genes LSM2 and LSM3 of  Saccharomyces cerevisiae  ( Hillenmeyer  et al. , 2008 ) as an example. These two genes have a low correlation value,  r  = 0.15, although they share many common functions and high sequence similarity. Both genes are part of one complex that binds to the 3′ end of U6 snRNA, and are responsible for its regulation and stability ( Pannone  et al. , 2001 ). LSM2 and LSM3 are required for pre-mRNA splicing and their mutations inhibit mRNA decapping ( Tharun  et al. , 2000 ). Another study showed that LSM2 and LSM3 form many interactions with each other ( Mayes  et al. , 1999 ). The semantic similarity between their cellular component GO terms is 0.95 as calculated using  Wang  et al.  (2007) . Thus, these two genes are in the same functional organization by definition. However, the correlation coefficient measure cannot capture this. Our method, on the other hand, predicted them as co-fit genes since they were in the same constant-column bicluster based on similar fitness values representing conditional essentiality or dispensability. Specifically, our method detected similar, extreme fitness values between the LSM2- and LSM3-deletion strains for 51 out of 726 different stress conditions in the yeast phenotype profiling data, showing statistically significant association ( P -value =   3.0 × 10 − 6 ), and these deletion strains have very high correlation ( r  = 0.99) over these 51 conditions. Therefore, the co-fitness can only be detected by local measures as they capture the similarity over a subset of conditions. Furthermore, by using biclustering methods to find co-fit genes, it is possible to explicitly identify which subset of genes shares similar patterns of conditional essentiality and dispensability under which subset of stress conditions. By definition of co-fitness, a bicluster of co-fit genes should have similar values in each column of this bicluster, but values across different columns can be very different, which is the same definition as constant-column biclusters. 3 Materials and methods 3.1 Gracob The proposed method, GRAph-based Constant-cOlumn Biclustering (Gracob), is a deterministic graph-based method that is designed to find maximal constant-column biclusters in any given data matrix ( Fig. 2 ), where a maximal bicluster means that it is not possible to extend the bicluster by either rows or columns while keeping the same level of specified similarity. Although most interesting variants of the biclustering problems are well known to be NP-Complete ( Tanay  et al. , 2002 ), the proposed method takes advantage of the sparsity of biclusters. That is, compared to the size of the input data matrix, the number of biclusters in the matrix is small. For the sake of simplicity, here, we define that each row represents a gene-deletion strain and each column represents a condition.
 Fig. 2 Workflow of Gracob. ( a ): The data in each column are transformed using a cumulative distribution function, independently. ( b ): Data values in each column are sorted independently from other columns while keeping track of the original row indexes. ( c ): Nodes are created for each consecutive row subset such that the range of their values is at most  δ  (user defined value for how ‘constant’ each column of desired biclusters should be). A row subset can overlap with other row subsets but cannot be contained by others. ( d ): An edge is created between any pair of nodes if the nodes are from different columns and share at least  r  (user defined threshold for the smallest number of strains in desired biclusters) rows (i.e. strains). ( e ): Nodes with degree less than  c  (user defined threshold for the smallest number of conditions in desired biclusters) are deleted from the graph. ( f ): Each node is used to grow a clique with its connected nodes (orange circles) while thresholds,  r  and  c , are repeatedly checked to detect future failures as early as possible. ( g ): Row and column index information from each clique is used to extract biclusters from the original data matrix The main idea of Gracob is that once the users define how ‘constant’ they want the biclusters to be column-wise in the preprocessed data ( Fig. 2(a) ), Gracob looks at the subsets of strains that maximally satisfy this ‘constant’ requirement inside each independently sorted column. Each of such subsets is defined to be a  block , which is a multi-row one-column vector in the corresponding sorted column. Consequently, any column in any potential bicluster is contained by at least one of these blocks ( Fig. 2(b) and (c) ). We then build a multipartite graph in which each node is a block and an edge is created between two blocks from two different conditions if they share a sufficient number of strains ( Fig. 2(d) ). This number is defined to be the minimum number of strains in a desired bicluster. For instance, if this number is set to be 1, then every single strain constitutes a constant-column bicluster by definition; however, such biclusters are most likely not of interest to the users. If there is a bicluster of  n  conditions, there must exist in the graph a clique of  m  ( m ≥ n ) nodes that contain these  n  blocks ( Fig. 2(e) ). The problem then becomes finding maximal cliques in this multipartite graph. We propose an efficient method to solve this problem. The idea is to divide the problem into smaller ones, and make use of the characteristics of the data and the requirements of biclusters to search for solutions in a reasonable amount of time ( Fig. 2(f) ). Finally, biclusters are identified inside these cliques ( Fig. 2(g) ). Here, we start by formulating the problem and then briefly describing the main steps. The technical details can be found in  Supplementary Materials . Gracob consists of three main phases: (i) the pre-processing phase, (ii) the graph creation phase and (iii) the maximal clique finding phase. 3.1.1 Problem formulation Let  G  be a set of  n  mutant strains, each of which is a single gene knock-out mutation, and  C  be a set of  m  environmental stress conditions. We denote  a ij  as the elements of the growth phenotype data matrix  A ( n × m )  where  a ij  is a real value that represents the growth of the  i th mutant under the  j th stress condition where  i ≤ n  and  j ≤ m . To define a constant-column bicluster, the user has to specify three parameters. The first one is the range threshold,  δ , to define how ‘constant’ each column is in desired biclusters. For example, if  δ  is set to be 0, then the user is looking for biclusters within which each column contains data with exactly the same value. The second one is the row threshold,  r , to define the minimum number of strains (or genes) that each bicluster must have. If  r  is set to be 1, each row becomes a trivial constant-column bicluster because each column for the same row has 0 variance. The third one is the column threshold,  c , to define the minimum number of conditions each desired bicluster must contain. If  c  is set to be 1, the biclusters will be a part of a single column, but is not an interesting one. Once the requirements are provided by the user, let  I ⊆ G  and  J ⊆ C , we say that  I  and  J  specify a desired constant-column bicluster if the following conditions are satisfied:
 (1) | f ( a i 1 j ) − f ( a i 2 j ) | ≤ δ , (2) | I | ≥ r , (3) | J | ≥ c , 
where  i 1 , i 2 ∈ I  and  j ∈ J .  | . |  denotes the cardinality of a set.  f ( . )  is a transformation function that we will specify later.  Eq. (1)  ensures that the values within each column of the bicluster are similar, whereas  Eq. (2)  and  (3)  make sure only non-trivial biclusters are reported. The objective is to find all  I  and  J  that satisfy these conditions, and there is no  I ′  and  J ′  such that  I ⊆ I ′  and  J ⊆ J ′  that satisfies these conditions, i.e. only maximal constant-column biclusters are returned. 3.1.2 Preprocessing phase There are two major steps in this phase, i.e. transforming the data in each condition based on a cumulative distribution and creating blocks (nodes). The input growth phenotype data are often assumed to follow a standard normal distribution where the data have been z-score normalized inside each column ( Nichols  et al. , 2011 ). As most of the outlier data points are distributed along a long range of values, they are considered to show similar phenotypes, i.e. growth is extremely sensitive (negative outliers) or stable (positive outliers) with respect to environment conditions. Thus, there is a need to transform the data into another space which preserves the similarity of these values. We chose to apply CDF (cumulative distribution function) transformation to each column, independently, in the input matrix. Consequently, data points in the tail of each side will be assigned very close values, which satisfies our needs. The right panel of  Figure 2(a)  shows the distribution of the values for a column after the CDF transformation. The second step is to create blocks that are the nodes for the multipartite graph. The idea is to sort ( Fig. 2(b) ) and then linearly scan each column to get all the blocks in which the range of values is at most  δ . These blocks are used as the (unit) nodes for the following phases ( Fig. 2(c) ). Detailed steps can be found in Section S1.2. 3.1.3 Graph creation phase In this phase we create edges between the blocks (unit nodes). The edges are not weighted but rather labeled by the shared subsets of strains. There is no edge created between nodes from the same condition, and the cardinality of the shared subset of an edge must be at least  r . The complexity of such a process is  O ( S 2 )  where  S  is the total number of nodes. With genome-wide growth phenotype data,  S  can be in the order of millions and  O ( S 2 )  runtime becomes infeasible. Therefore, we propose a divide-and-conquer algorithm by repeatedly using the user defined thresholds  c  and  r  to reduce the search space, and thus reduce the practical runtime. The main idea is that we first merge all the blocks inside each column into a super-node and create edges among all these super-nodes. Then we try to divide these super-nodes into non-overlapping child nodes, each of which is a subset of blocks and inherits the edges from its parent node, unless the cardinality (i.e. number of genes) of the edge is below  r , which means this edge will never be a part of a meaningful bicluster. If such a non-overlapping split is not feasible, then we split in the middle. Meanwhile, we delete all the nodes that have degree below  c , which means the blocks in this nodes will never be a part of bicluster with at least  c  stress conditions. We recursively do this splitting until each node is a block. The detailed steps can be found in Section S1.3. Note that although the divide-and-conquer idea has been used in biclustering methods, such as in Bimax ( Prelić  et al. , 2006 ), our divide-and-conquer algorithm is very different from the ones developed in literature. 3.1.4 Maximal clique finding phase The objective of this phase is to find and return all maximal cliques, from which biclusters can be easily extracted. Unfortunately, existing general-purpose maximal clique finding algorithms do not scale up well on our problem. We thus propose a tailored algorithm that starts from each remaining unit node from the previous phase, and sequentially grows cliques seeded from this node by gradually adding connected nodes to the existing cliques. The main idea is to use the minimum row and column thresholds,  r  and  c , to detect future failures as early as possible and to eliminate those cliques that have no hope to grow to the required size. The details of our algorithm can be found in Section S1.4. It is noteworthy that Gracob is an exhaustive algorithm that finds all maximal biclusters in the given growth phenotype dataset, under the given thresholds,  δ ,  r  and  c . Neither the divide-and-conquer algorithm used in the graph creation phase nor the early detection of failures trick used in the maximal clique finding phase affects the optimality of the search. 3.2 Validating Gracob on synthetic data Following  Prelić  et al.  (2006) ;  Li  et al.  (2009) ;  Eren  et al.  (2013) , we validated Gracob by a variety of synthetic datasets, where different types of implanted biclusters, different levels of noise, and different degrees of bicluster overlaps were simulated (Section S2.1). Since the ground-truth biclusters are known for the synthetic datasets, we used recall, precision and F1-score to measure the performance (Section S2.2). Our results (Supplementary Fig. S1) show that among the 14 compared methods, ISA, QUBIC, and Gracob are all able to detect both constant biclusters and constant-column biclusters well. These three methods can also tolerate noise. However, when the overlapping degree of the implanted biclusters is high, Gracob is the only method that can almost perfectly identify all the implanted biclusters (Section S2.3). We conducted the sensitivity analysis on Gracob with respect to the parameters  r  (minimum number of rows for biclusters) and  c  (minimum number of columns), and Gracob shows strong robustness to these parameters (Supplementary Fig. S1(9a)). We further tested the three best performing methods with respect to the increasing size of the input data matrix. In terms of F1-score, Gracob is very stable whereas ISA and QUBIC are less (Supplementary Fig. S1(9b)). In terms of the runtime, Gracob has a similar runtime to QUBIC, while both are faster than ISA (Supplementary Fig. S1(9c)). 3.3 Growth phenotype data To comprehensively evaluate the performance of Gracob, we used three recently measured growth/fitness phenotype datasets. The first one is the genome-wide growth phenotype dataset of  E. coli  ( Nichols  et al. , 2011 ). This dataset consists of fitness data for 3979 mutant strains, each of which was measured under 324 different stress conditions. Each fitness value in the data matrix represents the relative growth rate of a given gene-knockout strain under a given stress condition, which is normalized column-wise to follow the unit normal distribution ( Nichols  et al. , 2011 ).  Figure 3 (1) shows this growth phenotype dataset.
 Fig. 3 Heatmap visualization of the  E. coli  growth phenotype data and the representative biclusters detected by the 11 methods. (1): The capped data matrix for the  E. coli  growth phenotype dataset with 3979 strains and 324 stress conditions. All the values bigger than 3.0 are capped as 3.0 and all the values smaller than -3.0 are capped as -3.0, for visualization purposes. ( 2 )–( 12 ): The representative biclusters detected by BicPAM, Bimax, CC, CPB, iBBiG, ISA, QUBIC, SAMBA, Spectral, xMOTIFs and Gracob, respectively. For each method, the predicted biclusters that have consistent patterns which appear many times in the results of this method are selected. For visualization purposes, rows and columns of each bicluster are organized by hierarchical clustering ( Eisen  et al. , 1998 ). That is, genes with similar values are clustered on the Y-axis and conditions with similar values are clustered on the X-axis The second one is the DNA tag-based pooled fitness assay dataset for  Shewanella oneidensis  MR-1, a Gram-negative  γ -proteobacterium ( Deutschbauer  et al. , 2011 ). The dataset contains the mutant fitness for 3355 nonessential genes under the 195 pool fitness experiments. The third one is the growth response dataset for  Saccharomyces cerevisiae  ( Hillenmeyer  et al. , 2008 ). The dataset contains 5337 heterozygous gene deletion strains over 726 conditions. 3.4 Performance measures The real growth phenotype data do not have known ground-truth biclusters. Thus, to measure the performance of biclustering methods on the real data, we defined four performance measures. Since each biclustering method can discover a large number of biclusters in a given dataset, our measures consider the performance based on multiple biclusters. If the number of predicted biclusters is smaller than 100, we keep all of them. Otherwise, we keep the top 100 largest biclusters for evaluation. In order to reduce the bias caused by highly overlapping biclusters in evaluation, we sorted all the returned biclusters by their size in a descending order. We then applied a greedy approach to go down the list and keep only the biclusters that share less than 30% of the size of this bicluster with any previously selected bicluster, until we selected 100 biclusters. The first measure is the average column-wise standard deviation. We first calculated the mean of the column-wise standard deviation for each bicluster, and then calculated the average of this value over all the predicted biclusters. The second measure is the average size of the predicted biclusters, where the size of a bicluster is measured by the number of rows times the number of columns. Thus, a method that simultaneously reports a small average standard deviation and a large average bicluster size is considered to be useful. Furthermore, each bicluster is subject to two enrichment analyses, using pathway information from the KEGG database ( Kanehisa and Goto, 2000 ) and gene ontology (GO) terms, respectively. For each of the predicted biclusters of a method, we found the set of genes that correspond to the strains of this bicluster, and searched for all the annotated pathways that contain at least one gene from this gene set. Then, we calculated the probability, i.e.  P -value, of randomly finding these genes for each pathway with the hypergeometric calculation ( Li  et al. , 2009 ). The precision of a method is the ratio of biclusters which have at least one significant pathways (i.e.  P -value smaller than a given threshold, e.g.  10 − 7 ,   10 − 6 ,   10 − 5 ,   10 − 4  or  10 − 3 ) to the total number of selected biclusters for that method. The number of selected biclusters for any method is at most 100 as explained above. The same procedure was done for the GO term enrichment analysis, and the GO-level precision for different methods is reported as the fourth measure. 4 Results and discussions We compared Gracob with the 13 representative widely used biclustering methods introduced in the related work. For each experiment, the input data were transformed and preprocessed following the requirements of different methods. The parameter settings for the 13 methods were searched and optimized based on the recommended use from their papers. 4.1 Performance on growth phenotype datasets Some representative biclusters predicted by 11 methods on the  E.coli  dataset are illustrated in  Figure 3 (2)-(12). BBC and FLOC failed to detect any bicluster on these large growth phenotype datasets in 3 hours, and Plaid only predicted less than three biclusters and thus is not included in the analysis for the sake of fair comparison. It is clear that the biclusters detected by Bimax are ‘purely’ constant, whereas the ones detected by CPB, iBBiG, ISA and SAMBA tend to have relatively constant columns, although they are still far less constant than the ones detected by BicPAM and Gracob. Among these four methods, CPB and iBBiG have relatively lower column-wise standard deviation, whereas ISA and SAMBA tend to detect bigger biclusters. It is worth noting that biclusters predicted by Bimax are not only smaller than those predicted by Gracob, but they also contain only large positive values. This is due to the required binary discretization step in Bimax. Among the biclusters returned by Gracob, about 62% consists of only conditionally essential genes (i.e. biclusters in the blue color), 20% consists of only conditionally dispensable genes (i.e. biclusters in the red color), and 18% consists of genes that are essential under certain conditions but dispensable under some other conditions (i.e. biclusters with mixed colors). In terms of the average column-wise standard deviation, as expected, Bimax and Gracob have the lowest column-wise variance, followed by BicPAM ( Fig. 4 (1a)-(3a)). However, the average bicluster size of Gracob is one order of magnitude bigger than that of Bimax ( Fig. 4 (1b)-(3b)). Although ISA, Spectral and xMOTIFs can return large biclusters, they are very impure. Overall, Gracob has a remarkably strong ability to discover maximal constant-column biclusters. As shown in  Figure 4 (1c)-(3c), Gracob has the highest percentage of significantly enriched KEGG pathways among all the 11 methods, under almost all the different significance levels. The only exception is for the  E. coli  dataset, when the significance threshold is below 1E-7, the precision of Gracob is slightly lower than that of Spectral. The average precision of Gracob under the five significance thresholds ( 10 − 3 ,   10 − 4 ,   10 − 5 ,   10 − 6  and  10 − 7 ) are 0.90, 0.82, 0.75, 0.64 and 0.53, respectively, whereas that of the second best method are 0.56 (Bimax), 0.44 (Bimax), 0.32 (QUBIC), 0.27 (QUBIC) and 0.24 (QUBIC), respectively. These results show that for this analysis Gracob is at least 61%, 86%, 134%, 137% and 121% more precise than any other biclustering method in terms of KEGG pathways under the five significance levels, respectively.
 Fig. 4 Performance comparison of the 11 methods on the  E. coli , proteobacteria and yeast growth phenotype datasets. ( 1a )–( 3a ): The average column-wise standard deviation on the three datasets, respectively. ( 1b )–( 3b ): The average size of the returned biclusters on the three datasets, respectively. ( 1c )–( 3c ): The KEGG pathway-level precision under five significance levels on the three datasets, respectively. ( 1d )–( 3d ): The GO term-level precision under five significance levels on the three datasets, respectively Similar conclusions can be drawn on the GO term-level precision. Gracob is clearly more precise than all the other methods under almost all the situations ( Fig. 4 (1d)-(3d)), except for the yeast data when the significance level is  10 − 3 , the GO-level precision of Gracob (0.89) is slightly lower than that of BicPAM (0.91). The average precision of Gracob over the three datasets under the five significance levels are 0.93, 0.84, 0.76, 0.62 and 0.54, which show that for this analysis Gracob is 26%, 71%, 105%, 88% and 108% more precise than the second best method, which are BicPAM (0.74), BicPAM (0.49), QUBIC (0.37), QUBIC (0.33) and SAMBA (0.26). We further analyzed the enrichment over the three branches of GO terms (Biological Process, Cellular Component and Molecular Function). Our results revealed that the highest percentage of enriched GO terms among the co-fit genes detected by Gracob biclusters belong to the Cellular Component (CC) branch in all the analyzed species (Supplementary Fig. S2). This is in agreement with the findings in  Hillenmeyer  et al.  (2010)  that co-fitness is a powerful tool to predict cellular functions. We conducted parameter sensitivity analysis of Gracob over the  E.coli  dataset. Gracob is very stable with respect to the changes of parameters  r  and  δ , while less so when  c  increases (Supplementary Section S4 and Figs S3 and S4). 4.2 Case study on  E. coli  growth phenotype data We now focus on the largest bicluster ( Fig. 3 (12a)) that Gracob detected in the  E. coli  growth phenotype dataset. The bicluster groups 79 gene knock-out strains under 10 stress conditions (see Tables S1&amp;S2 for details). The knock-out of any of these 79 genes leads to significantly reduced cell growth under these 10 conditions, although none of them is an essential gene. The 10 conditions consist of seven carbon-source conditions, one nitrogen-source condition and two ferrous sulfate-source conditions. These sources are known to be transported and metabolized by pathways that require amino acids, purines, pyrimidines and cofactors to be synthesized ( Kim and Copley, 2007 ). Thus, deletions of genes involved in such pathways are expected to impact the cell growth under these conditions ( Gottschalk, 1986 ). Among the 79 genes, there are 74 enzyme coding genes (Section S5). We found that 72 of them are closely connected through KEGG pathways as can be seen in Supplementary Figure S5. In fact, 70 genes (88.6% of the genes in this bicluster) are involved in ‘Metabolic pathways’. This is statistically significant because only 15.2% of the total 3979 genes are known to be involved in metabolic pathways (see Tables S3 and Section S5 for details). The second and third most significant KEGG pathways in this bicluster are ‘Biosynthesis of secondary metabolites’ and ‘Biosynthesis of amino acids’, in which 44 and 41 of the genes are involved, respectively. This is interesting because secondary metabolites generally do not play a role in growth under the normal condition. However, it is discovered that they can be important in survival of organisms because they are involved in physiological functions like stress-response ( Price-Whelan  et al. , 2006 ). Growth phenotype data can be used not only to analyze conditional essentiality and dispensability of genes for specific environmental settings ( Hillenmeyer  et al. , 2008 ;  Nichols  et al. , 2011 ), but also to facilitate computational analysis to gain new insights into the functional organization of genes ( Deutschbauer  et al. , 2011 ;  Giaever  et al. , 2002 ;  Lee  et al. , 2005 ;  Nichols  et al. , 2011 ). Since about one-third of the protein-coding genes are still uncharacterized (i.e. orphan genes) even in  E. coli  ( Blattner  et al. , 1997 ;  Hu  et al. , 2009 )—one of the most well-known biological systems—such analysis is crucial to unraveling how the interplay of genetic and environmental factors orchestrates cellular-level phenotypes. To illustrate this point, we examined the genes in the largest bicluster and analyzed the function of ycdY, which is the only orphan gene in this bicluster. This orphan gene is known to code for a chaperone protein that is suggested to be a redox enzyme maturation protein (REMP) ( Turner  et al. , 2004 ). No functional annotation is defined for ycdY. Surprisingly, ycdY deletion has strong effects on growth under these 10 conditions ( P -value =  3.33 × 10 − 16 ). In order to predict its function we looked for the most significantly enriched GO terms in this bicluster. Seventy-one out of the 79 genes (89.9%) are annotated as ‘organonitrogen compound biosynthetic process’ whereas only 485 genes are annotated as this GO term among all the 3979  E.coli  genes in this dataset, which gives a  P -value of  9.57 × 10 − 55 . Other most significantly enriched GO terms are ‘cellular amino acid biosynthetic process’ ( P -value  = 1.37 × 10 − 49 ), ‘small molecule biosynthetic process’ ( P -value  = 1.13 × 10 − 48 ), ‘cellular amino acid metabolic process’ ( P -value  = 2.18 × 10 − 43 ) and ‘organonitrogen compound metabolic process’ ( P -value  = 2.08 × 10 − 42 ). Therefore, our analysis strongly suggests that the function of ycdY to be associated with these five GO terms. Another case study on a bicluster containing 11 genes that are essential under three dyeing chemical conditions but are dispensable under a cold shock and an antibiotic, Spectinomycin, condition also demonstrates the value of Gracob (Section S6). 5 Conclusion In this paper, we proposed a novel graph-based biclustering method, Gracob, that is developed to discover co-fit genes from large growth phenotype profiling datasets. To our knowledge, Gracob is the first biclustering method developed specifically to mine growth phenotype data. Experimental results from both a variety of synthetic datasets and three genome-scale growth phenotype datasets for  E.coli , proteobacteria and yeast demonstrate the superior performance of Gracob over a great collection of the widely used biclustering methods to discover co-fit genes. Funding The research reported in this publication was supported by the King Abdullah University of Science and Technology (KAUST) Office of Sponsored Research (OSR) under Award No. URF/1/1976-04. NIH U01HG008488, NIH R01GM115833, NIH U54GM114833, NSF DBI-1565137, and NSF IIS-1313606.  
 Conflict of Interest : none declared. Supplementary Material Supplementary Materials Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Error control variability in pathway-based microarray analysis</Title>
    <Doi>10.1093/bioinformatics/btp385</Doi>
    <Authors>Gold David L., Miecznikowski Jeffrey C., Liu Song</Authors>
    <Abstract>Motivation: The decision to commit some or many false positives in practice rests with the investigator. Unfortunately, not all error control procedures perform the same. Our problem is to choose an error control procedure to determine a P-value threshold for identifying differentially expressed pathways in high-throughput gene expression studies. Pathway analysis involves fewer tests than differential gene expression analysis, on the order of a few hundred. We discuss and compare methods for error control for pathway analysis with gene expression data.</Abstract>
    <Body>1 INTRODUCTION DNA microarray technology makes possible the simultaneous monitoring of thousands of gene expression and has been widely applied to detect gene activity changes in many areas of biomedical research. Traditionally, the microarray data were analyzed using a gene by gene strategy, which focused on individual gene's expression change across contrasting conditions. This gene-based approach, while useful to identify the most significant changes, could often neglect the information in genes' joint distribution. From the viewpoint of biology, the biological mechanisms underlying gene activity change are generally the result of interactions between multiple genes in a modular pathway, with most of its members characterized by subtle active changes. Such modest but consistently coordinate effects are difficult to be identified by gene expression analysis. To overcome the limitations of the gene expression analysis, pathway-based enrichment methods have been developed to identify changes in the collections of genes, thought to be involved in the same underlying biological process (i.e. pathway), thus allowing the incorporation and weighting of prior biological knowledge into microarray analysis. The earliest pathway-based approaches followed from Fisher's test, comparing the distributions counts of differentially expressed genes (DEGs) in each pathway with the overall distribution (Khatri and Drghici,  2005 ). Other methods include parametric analysis of gene set enrichment (PAGE) (Kim and Volsky,  2005 ), which attempts to model the changes in DEGs belonging to pathways by a parametric model. The more popular pathway analysis employ stringent permutation-based methods, attempting to account for gene–gene correlation. Examples of these are gene set enrichment analysis (GSEA) (Subramanian,  2005 ) and gene set analysis (GSA) (Efron and Tibshirani,  2007 ). In GSEA, all of the genes are first ranked, by their significance in terms of differential expression, and then, the rankings are tested, to determine whether a particular predefined collection of genes is randomly distributed throughout the ranked list, or enriched at either the top or bottom of the list (Efron and Tibshirani,  2007 ; Subramanian,  2005 ). GSA employs the max–mean statistic, as a measure of significance for each pathway, what Efron and Tibshirani claim is more powerful and discriminatory approach, and uses a permutation-based approach to determine the null distribution. One of the critical assumptions underlying the pathway-based strategy is the availability of a collection of well-curated pathways. These can be obtained through existing annotation information such as that provided by the Kyoto Encyclopedia of Genes and Genomes (KEGG) (Kanehisa and Goto,  2000 ). KEGG is one of the most complete and academically available pathway databases, whose latest release includes about 210 curated non-redundant pathways in human. Although it is possible to create more gene sets  in silico  (e.g. through chromosome neighborhoods, expression neighborhoods and regulatory motif sharing, etc.), the increasing functional heterogeneity associated with such less-curated gene sets make them difficult for subsequent interpretation in terms of biological mechanism. As a result, many published pathway-based application (Jimeno  et al. ,  2008 ; Pangas  et al. ,  2008 ; Setlur  et al. ,  2007 ; Xu  et al. ,  2007 ) or evaluation (Liu  et al. ,  2007 ; Song and Black,  2008 ) in microarray data analysis used a couple of hundred well-curated non-redundant pathways merged from KEGG and other public annotation systems like Biocarta and/or Gene Ontology (GO). Although KEGG is not freely available to all users, our arguments regarding error control for pathway analysis apply to similar databases used to construct the pathways. Both gene- and pathway-based analysis deal with the challenge of multiple testing error control (i.e. multiplicity correction ). Three universal error control strategies are applied in the microarray analysis literature, controlling the false discovery rate (FDR), false positive count or simply choosing the top  K  from a ranked list. The FDR-based method of Benjamini and Hochberg ( 1995 ) has been widely applied in the analysis of gene expression microarray analysis, and is also a popular choice in pathway-based microarray analysis. However, the size of multiplicity is quantitatively different between them. For example, the latest human U133plus 2.0 array contains ∼54 000 probe sets targeting at ∼38 000 human genes, including ∼37 000 probe sets for ∼18 000 curated refseq genes. On the other hand, typically much fewer than 500 canonical pathways (e.g. from KEGG, Biocarta or GO databases, etc.) are available for pathway-based approaches, and less are used after excluding pathways with less than a certain number of genes expressed in the chips. The shift from massive multiplicity in the gene expression approach, to reduced multiplicity in pathway-based methods raises interesting questions about the accuracy of FDR-based procedures in pathway studies. We addressed this issue here by first briefly reviewing the terminology of multiple testing. 1.1 Family-wise error rate Suppose that 10 000 hypothesis tests are performed, each controlled at the 0.05 level. If all null hypotheses are true, on average you can expect 500 false positives, far too many false leads in practice. Carlo E. Bonferroni (1892–1960) suggested controlling the probability of at least one false positive, what he called the family-wise error rate (FWER). The FWER procedure, given  N  hypothesis tests, controls the FWER at level α with  P -value threshold α/ N . Notice that the Bonferroni threshold becomes more stringent, i.e. conservative, as the number of tests  N  increases, diminishing power for microarray analysis. For this reason new methods of error control were devised. 1.2 FDR analysis Multiple statistical testing procedures began to be reexamined in the early 1990s, with the advent of high-throughput genomic technologies, in light of microarray analysis. Benjamini and Hochberg (BH) proposed controlling the FDR, or the expected rate of false test positives (Benjamini and Hochberg,  1995 ). In the BH multiple testing procedure, the FDR is controlled by the following scheme:
 Let  p (1)  &lt; ··· &lt;  p ( N )  denote the  N  ordered  P -values. Let   for the largest  k  such that  p ( k )  ≤ α* k / N Reject all null hypothesis  H 0 i  for which  . 
BH prove that if the above procedure is applied, then  FDR  ≤ α. Storey ( 2002 ) showed, for  P -value threshold  t ,
 (1) 
where π is the probability that an alternative hypothesis is true, and  F ( t ) is the distribution of  P -values given the alternative. FDR performance has been evaluated for gene detection given a variety of scenarios, for examples, in the presence of gene–gene correlation (Benjamini and Yekutieli,  2001 ; Shao and Tseng,  2007 ). FDR analysis does not control what Genovese and Wasserman ( 2004 ) call the  realized  FDR (rFDR), the number of false rejections  V  divided by the number of rejections  R  (assuming at least one rejection), which in fact, can be quite variable, see our  Supplementary Materials . Since researchers do not have knowledge of the truly DEGs, the rFDR is unknown in a real experiment. Several of the multiple testing procedures (MTPs) thus try to control the expectation of the rFDR, (often confusingly abbreviated as FDR). BH control is achieved only in large sample sense, as BH showed that as the number of tests goes to infinity, the expected rate of false positives is bounded above. BH's result controls the FDR ≤(1 − π) α ≤ α as  N  → ∞, through π, the percentage of true (and in practice unknown) positives. In other words, a desired FDR control of 10% might yield less power than hoped for inversely to π (Storey,  2002 ). 1.3 Controlling the number of false positives Lehman and Romano ( 2005 ) proposed controlling against at least  K  false positives, what he called K FWER (KFWER) with  P -value threshold α ·  K / N . Like the FWER control, the KFWER  P -value threshold becomes more stringent as the number of tests  N  becomes larger. We consider the  K  binomial method, KBIN (Miecznikowski  et al. ,  2009 ), to control KFWER. Like Lehman and Romano ( 2005 ), KBIN also controls the probability of at least  K  false positives, although for our purposes can achieve greater power. The KBIN procedure follows from basic probability concepts, to control the probability that the number of false positives  V  is not larger than  k  to be α, that is,
 
where  V  is the number of false positives, assumed to follow a Binomial distribution, with  k  defined by the user. The procedure rejects all hypotheses with  P -values less than α cut , where α cut  is the value of α that solves the equation below
 
where  F  is the cumulative density function for a Binomial distribution with  N  trials and probability of success α and β is usually chosen to be large, e.g. 0.95. KFWER provides a convenient approximation for choosing α cut  in the case that  N  is large. In contrast, KBIN is determined before the data are observed, rather than estimated from data. Given the way the  P -value threshold is estimated, when the number of false positives exceeds the desired bound  K , it tends not to be much larger, see our  Supplementary Materials . One might also consider selecting the top  K  pathways, i.e. with the smallest  P -values. We call this method TopK. TopK and KBIN are less variable than BH for pathway analysis, although they have their own drawbacks such as lacking meaningful control and interpretation. For example, suppose for KBIN  K  = 5, and three pathways are discovered, while in another case 20 pathways are discovered. Clearly the latter seems to lend better interpretation. In practice, it is impossible to predict how many pathways will be detected with KBIN. 2 METHODS 2.1 Simulation studies The theoretical results of BH's FDR control hinge on a large number of tests. This begs the question, when is it inappropriate to report α-control of FDR? In order to answer this question, we illustrate the performance of BH's FDR analysis simulating data under two scenarios. In Simulation 1, we have a relatively large number of significant tests, each with small effects. In Simulation 2, there are fewer relative significant tests, but when significant, the effects are larger. The underlying  P -value distributions in our simulations are described at length in the  Appendix A . In both simulations, we let the number of tests  N  = 250. Simulating from a model provides us with well-defined functions, FDR( t ) and rFDR( t ), of the  P -value threshold  t . We compare the discrimination performance of our candidates: BH, KBIN and TopK, for Simulations 1 and 2, each of the 10 000 iterations. The true positive detection rate (PDR) and the true negative detection rate (NDR) were measured in each simulation as the mean number of positives and negatives, detected at a given threshold. While it is not possible to compare BH, KBIN and TopK directly since they control different quantities, there exists, for example, a pair (α, K ) yielding near identical average PDR performance for both BH and KBIN. Therefore, we consider a range of criteria: for FDR, we let α ranged from 0.01 to 0.99, KBIN  K  ranges from 0 to 250 and TopK,  K  ranges from 1 to 250. We use 20 and 50 values for each method in either simulation, respectively. We compare both the relative PDR/NDR mean and standard deviation (SD) by method across all 10 000 simulated datasets. 2.2 Gene expression data We perform pathway analysis on a subset of the samples in the cervical cancer study of Scotto  et al.  ( 2008 ), contrasting pathways in normal (24 cases) from cervical cancer (33 cases) specimens, on U133A Affymetrix arrays. Unlike simulated data, real data pathway analysis can be influenced by gene–gene correlation. To this end we favor permutation-based methods that attempt to account for gene–gene correlation yielding more robust results. PAGE and expression analysis systematic explorer (EASE), for example, ignore gene–gene correlation in determining  P -values (Hosack  et al. ,  2003 ; Kim and Volsky,  2005 ). We compare the performance of KBIN and BH, for various levels of control, given the nominal  P -values of GSA (Efron and Tibshirani,  2007 ) and GSEA (Subramanian,  2005 ). We modified the original restandardization step in GSA, rescaling by the max–mean scores by the median absolute deviation (MAD). The MAD estimator is more robust to outliers, i.e. significant max–mean statistics and thus yields nominal  P -value distributions more consistent with the underlying assumptions of BH. In the authors' experience, an abundance of positive, or negative, max–mean scores, can inflate the SD estimate for restandardization, and potentially bias the nominal  P -value distribution. This issue is easily rectified with the more robust MAD estimator. Otherwise, we did not alter GSA, nor seek improvements or modifications. For GSEA, we perform the kolmogorov-smirnov (KS) test, i.e. setting  P  = 0 in the original algorithm, and applied BH and KBIN to the nominal  P -values (i.e. we did not apply GSEA's multiple testing procedure, which we comment on below). We repeat analysis on 100 boot strapped versions of the data (Efron and Tibshirani,  1994 ), to contrast the distributions on the results, in particular the (random) number of pathways discovered as enriched for each criterion and method. We repeated the above analysis comparing gene expression in smokers versus non-smokers, from the Spira  et al.  study, on U133A Affymetrix arrays Spira  et al.  ( 2004 ), and differences in acute megakaryoblastic leukemia (AMKL) with and without down syndrome status (Bourquin  et al. ,  2006 ). 3 RESULTS 3.1 Simulation studies 3.1.1 FDR and rFDR There is a discordance between the FDR and rFDR for pathway analysis. In  Figure 1 , the variability in the rFDR for both Simulations 1 and 2 is extreme, and appears to be quite discordant from the FDR control level. Researchers should find this troubling. The frequency histogram of  , shows the sampling variability in the decision rule to reject. This variability can better be illustrated though the FDR function, as function of  , not to be confused with the desired FDR level of control level which is a constant. In the results of Simulation 1, there is a spike in the probability that the  , and a long right tail. Similar results are observed for Simulation 2. This indicates that the method has a tendency to be overly conservative for  N  = 250 tests. Summary statistics for each simulation are listed in  Table 1  of  Supplementary Materials , including results for BH control at α = 0.01. Note that for  N  = 5000, available in the  Supplementary Materials , the discordance between FDR and rFDR disappears.
 Fig. 1. Simulation of sampling variability in FDR and rFDR top row Simulation 1, bottom row Simulation 2, for FDR control level of 10%. 
 Table 1. Mean and SD of counts of cervical cancer pathways detected GSA α 0.01 0.05 0.1 0.15 0.2 0.25 0.3 BH Mean 6.8 8.48 10.26 11.56 13.26 14.72 16.53 SD 2.4 3.27 3.29 3.38 3.33 3.76 4.22 K 1 2 3 4 6 8 9 KBIN Mean 6.8 9.85 10.83 12.27 13.91 15.95 16.57 SD 2.4 2.59 2.49 2.4 2.35 2.55 2.61 GSEA α 0.01 0.05 0.1 0.15 0.2 0.25 0.3 BH Mean 3.73 4.19 5.67 7.09 8.74 10.64 12.21 SD 1.57 2.29 2.7 3.45 4.34 5.41 6.17 K 1 2 3 4 6 8 9 KBIN Mean 3.73 6.27 7.11 8.86 11.12 13.94 14.65 SD 1.57 2.18 2.5 2.78 3.08 3.44 3.54 
 3.1.2 Comparison of error control procedures A method that can achieve, say, on average higher PDR than all other methods, over the range of NDR, would be attractive, although variability can play a role. In  Figure 2 , average PDR, over 10 000 simulations, improves as the threshold becomes more liberal, i.e. leading to negative consequences for the average NDR. Notice, the relative performance of all three methods is on average the same, yet the SD in PDR and NDR, measured over all 10 000 simulated datasets, tends to be higher for BH than KBIN and TopK. The trend in the SD falls as the average PDR or NDR improve, expected since at  P -value threshold extremes there are fewer relative misclassifications, respectively. Practically speaking, this means that KBIN and TopK provide typically less variable results than BH, while delivering the overall expected benefit of error control.
 Fig. 2. Comparison of BH (solid), KBIN (dash) and TopK (dot): top row Simulation 1, bottom row Simulation 2. 
 3.2 Application to real data Since BH and KBIN control different things, the two are not directly comparable. Hence, for a multitude of control levels, we use the mean number of pathways detected, estimated by bootstrap analysis, as the equating metric between BH and KBIN. This allows sufficient comparison of the bootstrap variability in the number of pathways detected, for different control levels between procedures, that on average yield similar results. For any level of control, the method that is most consistent is preferred. Below we list selected results for each dataset, see our  Supplementary Materials  for complete results. 3.2.1 Cervical cancer We found similar results in BH and KBIN for conservative control, the relative variability in BH increased as the control was allowed to be more liberal. For example, BH's with 1% FDR control and KBIN with  K  = 1 yield mean (SD) counts of discovered pathways across bootstrap versions of the dataset of 6.8 (2.4). For GSA and GSEA, respectively, BH controlled at the 10% FDR level, yields mean (SD) counts of 10.26 (3.29) and 5.79 (2.6), and KBIN with  K  = 2 yields 9.85 (2.59) and 6.36 (2.11). The mean (SD) of counts for FDR control at 20% are 13.26 (3.33) and 8.77 (4.24), while KBIN with  K  = 6 yields 13.91 (2.35) and 8.88 (2.69), see  Table 1 . 3.2.2 Smoking-associated changes in gene expression In this dataset, similar to what we found in the Cervical cancer data, we noticed that GSA tended to yield more pathways than GSEA. Given the results of GSA, BH controlled at 10% and 20% FDR yields mean (SD) counts of 5.41 (4.19) and 7.41 (5.6) discovered pathways. In contrast, KBIN with  K  = 2 yields 5.11 (3.18) and  K  = 3, 7.15 (3.95) counts, respectively. Given the  P -values from GSEA, FDR control of 20% yields 1.65 (3.03) counts compared with KBIN with  K  = 2 of 1.44 (1.62) counts, see  Table 2 .
 Table 2. Mean and SD of counts of smoking-related pathways detected GSA α 0.01 0.05 0.1 0.15 0.2 0.25 0.3 BH Mean 3.75 4.28 5.41 5.41 7.41 9.08 10 SD 2.45 3.29 4.19 4.19 5.6 6.57 7.22 K 1 2 3 4 6 8 9 KBIN Mean 3.75 5.11 7.15 7.95 10.01 11.72 12.64 SD 2.45 3.18 3.95 4.19 4.82 5.19 5.32 GSEA α 0.01 0.05 0.1 0.15 0.2 0.25 0.3 BH Mean 0.82 0.86 0.93 1.33 1.65 2.8 3.27 SD 1.12 1.36 1.68 2.07 3.03 4.23 4.68 K 1 2 3 4 6 8 9 KBIN Mean 0.82 1.44 3.04 3.59 5.74 7.55 8.79 SD 1.12 1.62 2.31 2.32 3.26 3.8 4.05 
 3.2.3 Down Syndrome-associated changes in AMKL As shown in  Table 3 , KBIN shows &gt;4-fold less variability than BH in this dataset, for on average comparable results. Fewer pathways were detected with BH, for both GSA and GSEA, over a range of control levels, than KBIN. Controlling at 35% FDR, the mean count of detected pathways is 1.06 (2.47) and 1.15 (2.29), respectively. KBIN,  K  = 2, yields a mean count of 1.02 (1.14) pathways for GSA and 1.25 (1.14) pathways for GSEA.
 Table 3. Mean and SD of counts of AMKL pathways detected GSA α 0.01 0.05 0.1 0.15 0.2 0.25 0.3 BH Mean 0.28 0.28 0.35 0.36 0.56 0.56 0.84 SD 0.6 0.6 0.8 0.82 1.27 1.27 1.79 K 1 2 3 4 6 8 9 KBIN Mean 0.28 1.02 1.43 2.26 3.33 5.1 5.41 SD 0.6 1.14 1.38 1.81 2.26 2.88 3.03 GSEA α 0.01 0.05 0.1 0.15 0.2 0.25 0.3 BH Mean 0.33 0.33 0.33 0.37 0.67 0.68 0.99 SD 0.65 0.65 0.65 0.75 1.26 1.29 1.84 K 1 2 3 4 6 8 9 KBIN Mean 0.33 1.25 1.64 2.51 3.72 5.2 5.62 SD 0.65 1.14 1.37 1.67 2.14 2.54 2.6 
 4 DISCUSSION AND CONCLUSION While there are now many methods for conducting pathway analysis (Huang  et al. ,  2008 ), relatively little is discussed about how to control the error for pathway discovery. We recognize that in practice, a  variety  and/or  composite of  error control methods are applied. For our purposes, we were concerned with comparing the sole performance of representative methods including BH, KBIN and TopK. BH is widely applied, partly because it requires less computation than competing FDR analysis, offering a simple interpretation, i.e.  FDR  ≤ α. In contrast, KBIN bounds the probability of committing more than a specified number of false positives. While KFWER and KBIN offer control of the false positive count, KBIN is more powerful and, thus preferred. Our results demonstrate the sole performance of individual method, which serves for future study of consensus approaches. We found BH more variable than KBIN or TopK for pathway analysis, both in simulation and with three real datasets, from independent labs, addressing different hypotheses. The reason for the variability in BH's FDR analysis is that   is estimated from data, and therefore suffers from sampling variation. Note, when the number of tests is much greater than  N  = 250 tests, say  N  ≥ 5000, the variability in the sampling distribution of the rFDR is considerably reduced, see  Supplementary Materials .FDR control procedures, that attempt to estimate the distribution of the  p -values will suffer from sampling variability as we demonstrated. If one expects to obtain, for example, a list of 10–20 candidate pathways, a liberal FDR control of 30% provides reasonable costs. In our applications, KBIN yields, with comparable results to that of BH's FDR control of 30%, between a 2.25- and 4-fold reduction in variability in the count of detected pathways across three bootstrap analysis of independent datasets. While BH's error control procedure offers interpretation for gene discovery, interpretation is lacking for pathways, due to the variability in the rFDR. We prefer KBIN for pathway analysis, yielding less variability than BH, and better interpretation than TopK. We encourage investigators to consider the relative costs in selecting an error control procedure, taking into account  N , the number of hypothesis tests. Gene annotations will no doubt evolve, eventually calling for new methods for pathway analysis. Procedures that attempt to control error should be examined in the light of what they are controlling and how accurately they control it given what is known about the experiments. 
          Supplementary Material 
          
             
                [Supplementary Data] 
             
             
             
          
       </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>CloudNeo: a cloud pipeline for identifying patient-specific tumor neoantigens</Title>
    <Doi>10.1093/bioinformatics/btx375</Doi>
    <Authors>Bais Preeti, Namburi Sandeep, Gatti Daniel M, Zhang Xinyu, Chuang Jeffrey H, Hancock John</Authors>
    <Abstract/>
    <Body>1 Introduction Mutations in tumor genomes create specific peptide changes that can be recognized by the immune system and influence sensitivity to immunotherapy ( van der Most  et al. , 1996 ;  van Rooij  et al. , 2013 ). The mechanism of action involves binding of native major histocompatibility complex (MHC) class I and II molecules, a.k.a. human leukocyte antigen (HLA) complex I and II molecules, to the novel peptide sequences that result from protein-changing somatic mutations in cancer cells. Cells presenting these neoantigens are recognized as foreign by T-cells, which then selectively destroy them. With the arrival of new next generation sequencing platforms, it has become possible to interrogate the genomes of patient tumors and computationally predict T-cell reactivity against putative mutation-derived neoantigens ( Schumacher  et al. , 2015 ) by estimating the binding of MHC class I molecules to each new peptide sequence. Several bioinformatics tools are routinely used to predict tumor neoantigen—MHC class I binding from sequencing data. For example, HLAMiner ( Warren  et al. , 2012 ) and Polysolver ( Shukla  et al. , 2015 ) are software tools that can predict patient-specific HLA classes I and II typing from RNA sequencing data, and netMHCpan ( Nielsen  et al. , 2016 ) predicts HLA-peptide binding. Prior studies in cancer immunotherapy have successfully used these tools to predict the efficacy of immuno-oncological therapies in a patient-specific manner ( Rizvi  et al. , 2015 ;  Van Allen  et al. , 2015 ), demonstrating the importance of making such methods easily available to the general research community. However, the cost of developing and maintaining the bioinformatics infrastructure to perform this type of analysis is substantial. In particular, research groups are generating increasing amounts of custom sequencing data or investigating massive consortium datasets such as The Cancer Genome Atlas ( Weinstein  et al. , 2013 ), for which data transfer and scalability of computing can be significant obstacles to analysis on local compute clusters. To resolve these problems, we have developed a cloud-based analysis pipeline for tumor neoantigen detection. 2 Description We developed the CloudNeo pipeline on the Seven Bridges cloud platform as part of the National Cancer Institute’s Cancer Genomics Cloud [ http://www.cancergenomicscloud.org/ ] (CGC), which uses Docker containers to execute the tasks in the workflow. Briefly, CloudNeo takes a vcf file (for mutations) and bam file (for HLA typing) as inputs and then outputs HLA binding affinity predictions for all mutated peptides (see  Supplementary Fig. S1 ). A first input to CloudNeo is a list of non-synonymous mutations in vcf file format. There are multiple somatic mutation calling pipelines that can be used to generate and filter this vcf file ( Alioto  et al. , 2015 ), including several which are available through the CGC. The genomic variants are translated into amino acid changes using the VEP tool ( McLaren  et al. , 2010 ) and a custom R script that we have created called Protein_Translator. The output of the custom tool is a list of N-amino-acid-long peptide sequences in a fasta format, such that the single peptide change is in the middle of the N-mer. In parallel, Protein_Translator generates another fasta file for the homologous N-mers with no peptide mutation. Users have options to calculate the HLA types using either HLAminer or Polysolver. Six HLA types are predicted, namely the top two predictions for each of HLA-A, HLA-B and HLA-C. The final step in the pipeline is the NetMHCpan tool, which uses the HLA types and the N-mer mutant peptide sequences to calculate the binding affinities for potential neoantigens. Affinities between the two HLA-A, two HLA-B, and two HLA-C molecules and each of the ([N/2]+1)mer peptide subsequences within the N-mers are computed. The output of the pipeline is a list of peptide subsequences along with the MHC binding affinity scores for each of the six HLA types. Similar results are generated for the homologous unmutated peptide sequences as a comparison. To test this pipeline, we analyzed 23 melanoma tumor samples ( Hugo  et al. , 2016 ) as described earlier using both the HLAminer and Polysolver versions of the pipeline. We then predicted neo-antigens based on criteria of strong mutant-MHC binding affinity (NetMHCpan score &lt; 500), non-zeroexpression of the transcript containing the mutation, and lack of strong affinity between the non-mutated sequence and the MHC (NetMHCpan score for the non-mutant sequence ≥ 500). For each sample we merged the set of neoepitopes predicted across the six HLA types. The neoepitope load ranged from 0 to 1244 with an average of 107.89 using the HLAminer version of the pipeline. For the Polysolver version of the pipeline, the same filtering criteria were used and the neoepitope load was from 0 to 1417 with an average load of 133.53. The differences in the two pipeline results were due to differing HLA type predictions by Polysolver and HLAminer. 16 HLAtype predictions by the tools overlapped with each other, and there were 102 unique HLA predictions from Polysolver and 122 unique predictions from HLAminer. While our HLA type predictions were based on RNA-seq data, CloudNeo can also use DNA data as inputs for HLA calling. The average wall time required to run the pipeline for a given tumor on CGC was 8 h and 2 min for the HLAminer version and 7 h and 25 min for the Polysolver version (see  Supplementary Material  ‘Pipeline Performance’). 3 Discussion Other recent methods, such as ( Hundal  et al. , 2016 ), are similar to CloudNeo in providing a computational pipeline for neoantigen prediction. However, to our knowledge CloudNeo is the only such pipeline that has been developed for cloud computing. This allows users to realize advantages of cloud analysis, including massive computing scalability and access to large datasets on the CGC such as TCGA, as these can be reached without downloading to a local server. This cloud approach also makes CloudNeo easy to match to time and budget restrictions on demand, providing a flexible computational approach for the research community. A version of the CloudNeo pipeline is openly available at the Github site as a Common Workflow Language (CWL) implementation that can be run using Rabix ( Kaushik  et al. , 2016 ), allowing for running on systems including AWS, Google Compute Engine and Azure. Licenses for academically licensed software (HLAminer and NetMHCpan) must be obtained by users, but simple instructions to do so are provided at the Github site. Users with licenses can also contact the authors to request a version with all software integrated. Full versions are available either in CWL or as a workflow on the Seven Bridges implementation of the CGC. The CGC version is recommended, as this provides additional functionality including graphical interfaces for running and editing, simple workflow sharing and version tracking, improved calling of multiple cloud instances, and access to TCGA data. Full details and docs are at  https://github.com/TheJacksonLaboratory/CloudNeo . Supplementary Material Supplementary Figures Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>ProDy: Protein Dynamics Inferred from Theory and Experiments</Title>
    <Doi>10.1093/bioinformatics/btr168</Doi>
    <Authors>Bakan Ahmet, Meireles Lidio M., Bahar Ivet</Authors>
    <Abstract>Summary: We developed a Python package, ProDy, for structure-based analysis of protein dynamics. ProDy allows for quantitative characterization of structural variations in heterogeneous datasets of structures experimentally resolved for a given biomolecular system, and for comparison of these variations with the theoretically predicted equilibrium dynamics. Datasets include structural ensembles for a given family or subfamily of proteins, their mutants and sequence homologues, in the presence/absence of their substrates, ligands or inhibitors. Numerous helper functions enable comparative analysis of experimental and theoretical data, and visualization of the principal changes in conformations that are accessible in different functional states. ProDy application programming interface (API) has been designed so that users can easily extend the software and implement new methods.</Abstract>
    <Body>1 INTRODUCTION Protein dynamics plays a key role in a wide range of molecular events in the cell, including substrate/ligand recognition, binding, allosteric signaling and transport. For a number of well-studied proteins, the Protein Data Bank (PDB) hosts multiple high-resolution structures. Typical examples are drug targets resolved in the presence of different inhibitors. These ensembles of structures convey information on the structural changes that are physically accessible to the protein, and the delineation of these structural variations provides insights into structural mechanisms of biological activity ( Bakan and Bahar, 2009 ;  Yang  et al. , 2008 ). Existing computational tools and servers for characterizing protein dynamics are suitable for single structures [e.g. Anisotropic Network Model (ANM) server ( Eyal  et al. , 2006 ), elNémo ( Suhre and Sanejouand, 2004 ), FlexServ ( Camps  et al. , 2009 )], pairs of structures [e.g. open and closed forms of enzymes; MolMovDB ( Gerstein and Krebs, 1998 )], or nucleic magnetic resonance (NMR) models [e.g. PCA_NEST ( Yang  et al. , 2009 )]. Tools for systematic retrieval and analyses of ensembles of structures are not publicly accessible. Ensembles include X-ray structures for a given protein and its complexes; families and subfamilies of proteins that belong to particular structural folds; or a protein and its mutants resolved in the presence of different inhibitors, ligands or substrates. The analysis of structural variability in these ensembles could open the way to gain insights into rearrangements selected/stabilized in different functional states ( Bahar  et al. , 2007 ,  2010 ), or into the structure-encoded dynamic features shared by protein family or subfamily members ( Marcos  et al. , 2010 ;  Raimondi  et al. , 2010 ;  Velazquez-Muriel  et al. , 2009 ). The lack of software for performing such operations is primarily due to the non-uniform content of structural datasets such as sequence variations at particular regions, including missing or substituted residues, short segments or loops. We developed  ProDy  to analyze and retrieve biologically significant information from such heterogeneous structural datasets.  ProDy  delivers information on the structural variability of target systems and allows for systematic comparison with the intrinsic dynamics predicted by theoretical models and methods, thus helping gain insight into the relation between structure, dynamics and function. 2 DESCRIPTION AND FUNCTIONALITY 2.1 Input for  ProDy The input for  ProDy  is the set of atomic coordinates in PDB format for the protein of interest, or simply the PDB id or sequence of the protein. Given a query protein, fast and flexible  ProDy  parsers are used to Blast search the PDB, retrieve the corresponding files (e.g. mutants, complexes or sequence homologs with user-defined minimal sequence identity) from the PDB FTP server and extract their coordinates and other relevant data. Additionally, the program can be used to analyze a series of conformers from molecular dynamics (MD) trajectories inputted in PDB file format or programmatically through Python NumPy arrays. More information on the input format is given at the  ProDy  web site tutorial and examples. 2.2 Protein ‘dynamics’ from experiments The experimental data refer to ensembles of structures, X-ray crystallographic or NMR. These are usually heterogeneous datasets, in the sense that they have disparate coordinate data arising from sequence dissimilarities, insertions/deletions or missing data due to unresolved disordered regions. In  ProDy , we implemented algorithms for optimal alignment of such heterogeneous datasets and building corresponding covariance matrices. Covariance matrices describe the mean-square deviations in atomic coordinates from their mean position (diagonal elements) or the correlations between their pairwise fluctuations (off-diagonal elements). The  principal modes  of structural variation are determined upon principal component analysis (PCA) of the covariance matrix, as described previously ( Bakan and Bahar, 2009 ). 2.3 Protein dynamics from theory and simulations We have implemented classes for Gaussian network model (GNM) analysis and for normal mode analysis (NMA) of a given structure using the ANM ( Eyal  et al. , 2006 ). Both models have been widely used in recent years for analyzing and visualizing biomolecular systems dynamics ( Bahar  et al. , 2010 ). The implementation is generic and flexible. The user can (i) build the models for any set of atoms, e.g. the substrate or inhibitor can be explicitly included to study the perturbing effect of binding on dynamics, and (ii) utilize user-defined or built-in distance-dependent or residue-specific force constants ( Hinsen  et al. , 2000 ;  Kovacs  et al. , 2004 ).  ProDy  also offers the option to perform essential dynamics analysis (EDA;  Amadei  et al. , 1993 ) of MD snapshots, which is equivalent to the singular value decomposition of trajectories to extract principal variations ( Velazquez-Muriel  et al. , 2009 ). 2.4 Dynamics analysis example Figure 1  illustrates the outputs generated by  ProDy  in a comparative analysis of experimental and computational data for p38 kinase ( Bakan and Bahar, 2011 ).  Figure 1 A displays the dataset of 150 X-ray crystallographically resolved p38 structures retrieved from the PDB and optimally overlaid by  ProDy . The ensemble contains the apo and inhibitor-bound forms of p38, thus providing information on the conformational space sampled by p38 upon inhibitor binding. Parsing structures, building and diagonalizing the covariance matrix to determine the principal modes of structural variations takes only 38 s on Intel CPU at 3.20 GHz.  Figure 1 C illustrate the first principal mode of structural variation (PC1; violet arrows) based exclusively on  experimental  structural dataset for p38.
 Fig. 1. Comparative analysis of p38 dynamics from experiments (PCA) and theory (ANM). ( A ) Overlay of 150 p38 X-ray structures using  ProDy . An inhibitor is shown in space-filling representation. ( B ) Network model (ANM) representation of p38 (generated using  NMWiz  and VMD). ( C ) Comparison of the principal mode PC1 (from experiments; violet arrows) and the softest mode ANM1 from theory (green arrows) and ( D ) overlap of the top five modes. ( E ) Distribution of X-ray structures (blue) and ANM-generated conformers (red) in the subspace spanned by PC1-3. The green ellipsoid is an analytical solution predicted by the ANM. As to generating  computational  data, two approaches are taken in  ProDy : NMA of a representative structure using its ANM representation ( Figure 1 B; color-coded such that red/blue regions refer to largest/smallest conformational mobilities); and EDA of MD trajectories provided that an ensemble of snapshots is provided by the user. The green arrows in  Figure 1 C describe the first (lowest frequency, most collective) mode predicted by the ANM, shortly designated as ANM1. The heatmap in  Figure 1 D shows the overlap ( Marques and Sanejouand, 1995 ) between top-ranking PCA and ANM modes. The cumulative overlap between the top three pairs of modes is 0.73. An important aspect of  ProDy  is the  sampling  of a representative set of conformers consistent with experiments—a feature expected to find wide utility in flexible docking and structure refinement.  Figure 1 E displays the conformational space sampled by experimental structures (blue dots), projected onto the subspace spanned by the top three PCA directions, which accounts for 59% of the experimentally observed structural variance. The conformations generated using the softest modes ANM1-ANM3 predicted to be intrinsically accessible to p38 in the apo form, are shown by the red dots. The sizes of the motions along these modes obey a Gaussian distribution with variance scaling with the inverse square root of the corresponding eigenvalues. ANM conformers cover a subspace (green ellipsoidal envelope) that encloses all experimental structures. Detailed information on how to generate such plots and figures using  ProDy  is given in the online documentation, along with several examples of downloadable scripts. 2.5 Graphical interface We have designed a graphical interface,  NMWiz , to enable users to perform ANM and PCA calculations from within a molecular visualization program.  NMWiz  is designed as a VMD ( Humphrey  et al. , 1996 ) plugin, and is distributed within the  ProDy  installation package. It is used to do calculations for molecules loaded into VMD; and results are visualized on the fly. The plug-in allows for depicting color-coded network models and normal mode directions ( Fig. 1 B and C), displaying animations of various PCA and ANM modes, generating trajectories, and plotting square fluctuations. 2.6 Supporting features ProDy  comes with a growing library of functions to facilitate comparative analysis. Examples are functions to calculate, print and plot the overlaps between experiment, theory and computations ( Fig. 1 D) or to view the spatial dispersion of conformers ( Fig. 1 E). For rapid and flexible analysis of large numbers of PDB structures, we designed a fast PDB parser. The parser can handle alternate locations and multiple models, and read specified chains or atom subsets selected by the user. We evaluated the performance of  ProDy  relative to Biopython PDB module ( Hamelryck and Manderick, 2003 ) using 4701 PDB structures listed in the PDB SELECT dataset ( Hobohm and Sander, 1994 ): we timed parsers for reading the PDB files and returning C α -coordinates to the user (see documentation). The Python standard Biopython PDB parser evaluated the dataset in 52 min; and  ProDy  in 11 min. In addition, we implemented an atom selector using Pyparsing module for rapid access to subsets of atoms in PDB files. This feature reduces the user programming effort to access any set of atoms down to a single line of code from several lines composed of nested loops and comparisons required with the current Python packages for handling PDB data. The implementation of atom selections follows that in VMD. Full list of selection keywords and usage examples are provided in the documentation.  ProDy  also offers functions for structural alignment and comparison of multiple chains. 3 DISCUSSION Several web servers have been developed for characterizing protein dynamics, including elNémo ( Suhre and Sanejouand, 2004 ), ANM ( Eyal  et al. , 2006 ) and FlexServ ( Camps  et al. , 2009 ). These servers perform coarse-grained ENM–based NMA calculations, and as such are useful for elucidating structure-encoded dynamics of proteins. FlexServ also offers the option to use distance-dependent force constants ( Kovacs  et al. , 2004 ), in addition to protocols for coarse-grained generation and PCA of trajectories.  ProDy  differs from these as it allows for systematic retrieval and comparative analysis of ensembles of heterogeneous structural datasets. Such datasets includes structural data collected for members of a protein family in complex with different substrates/inhibitors.  ProDy  further allows for the quantitative comparison of the results from experimental datasets with theoretically predicted conformational dynamics. In addition,  ProDy  offers the following advantages: (i) it is extensible, interoperable and suitable for use as a toolkit for developing new software; (ii) it provides scripts for automated tasks and batch analyses of large datasets; (iii) it has a flexible API suitable for testing new methods and hypotheses, and benchmarking them against existing methods with minimal effort and without the need to modify the source code; (iv) it allows for producing publication quality figures when used with Python plotting library Matplotlib; and (v) it provides the option to input user-defined distance-dependent force function or utilize elaborate classes that return force constants based on the type and properties of interacting residues [e.g. based on their secondary structure or sequence separation ( Lezon and Bahar, 2010 )]. 4 CONCLUSION ProDy  is a free, versatile, easy-to-use and powerful tool for inferring protein dynamics from both experiments (i.e. PCA of ensembles of structures) and theory (i.e. GNM, ANM and EDA of MD snapshots).  ProDy  complements existing tools by allowing the systematic retrieval and analysis of heterogeneous experimental datasets, leveraging on the wealth of structural data deposited in the PDB to gain insights into structure-encoded dynamics. In addition,  ProDy  allows for comparison of the results from experimental datasets with theoretically predicted conformational dynamics. Finally, through a flexible Python-based API,  ProDy  can be used to quickly test and implement new methods and ideas, thus lowering the technical barriers to apply such methods in more complex computational analyses. Funding :  National Institutes of Health  ( 1R01GM086238-01  to I.B. and  UL1 RR024153  to A.B.). Conflict of Interest : none declared. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>MD-TASK: a software suite for analyzing molecular dynamics trajectories</Title>
    <Doi>10.1093/bioinformatics/btx349</Doi>
    <Authors>Brown David K, Penkler David L, Sheik Amamuddy Olivier, Ross Caroline, Atilgan Ali Rana, Atilgan Canan, Tastan Bishop Özlem, Valencia Alfonso</Authors>
    <Abstract/>
    <Body>1 Introduction Molecular dynamics (MD) is used to understand the movement of atoms of macromolecules in a simulated cell environment. MD simulations produce trajectories depicting the motions of atoms by presenting the atomic coordinates at specified time intervals, allowing for investigation into changes over time. Traditionally, these trajectories are used to analyze macromolecule dynamics by calculating well-established measures such as root mean square deviation, root mean square fluctuation (RMSF), radius of gyration and energy-based approaches including the molecular mechanics/Poisson Boltzmann surface area (MM/PBSA) and the molecular mechanics/generalized Born surface area (MM/GBSA) ( Kollman  et al. , 2000 ). Applications of MD simulations are broad, ranging from determining the stability of macromolecular complexes to understanding allosteric behavior of proteins. Although the measures mentioned above are informative, sometimes additional approaches are required. For example, changes in residue interaction networks (RINs) have been investigated in the context of mutation analysis ( Bhakat  et al. , 2014 ;  Doshi  et al. , 2016 ;  Brown  et al. , 2017 ). Another method, perturbation response scanning (PRS), following MD simulations, is used to identify residues important for controlling conformational changes ( Atilgan and Atilgan, 2009 ). Although traditional measures are incorporated into MD programs, they can be limited depending on the research questions. Hence, individual research labs often write custom scripts to further analyze trajectories. This might be challenging to some, especially if it requires mathematical knowledge and complex scripting. To serve this need, we present MD-TASK, an easy-to-use tool suite with detailed documentation, for analyzing MD trajectories. MD-TASK includes what we call dynamic residue network (DRN) (which combines the RINs of the frames in an MD trajectory) analysis, PRS, and dynamic cross-correlation (DCC), none of which are found in commonly used MD packages. 2 Materials and methods 2.1 Implementation MD-TASK was developed using Python for Linux/Unix-based systems. Various non-standard Python libraries, including NumPy, SciPy, Matplotlib ( Hunter, 2007 ), MDTraj ( McGibbon  et al. , 2015 ) and NetworkX, were used in the suite. Thus, MD-TASK supports any formats the underlying Python libraries support including .binpos (AMBER), LH5 (MSMBuilder2), PDB, XML (OpenMM, HOOMD-Blue), .arc (TINKER), .dcd (NAMD), .dtr (DESMOND), hdf5, NetCDF (AMBER), .trr (Gromacs), .xtc (Gromacs), .xyz (VMD) and LAMMPS. Further, the igraph package for R was used to generate residue contact maps. 2.2 Network analysis RINs can be analyzed using graph theory. In a RIN, each residue in the protein is a node in the network. An edge between two nodes exists if the  C β  atoms ( C α  for Glycine) of the residues are within a user-defined cut-off distance (usually 6.5–7.5 Å) of each other. MD-TASK constructs a DRN and uses this to calculate the changes in betweenness centrality (BC) and average shortest path ( L ) to residues over the trajectory. 2.2.1 Betweenness centrality BC is a measure of how important a residue is for communication within a protein. The BC of a node is equal to the number of shortest paths from all nodes to all others that pass through that node. MD-TASK uses an implementation of Ulrik Brandes algorithm in the NetworkX library for calculating BC ( Brandes, 2001 ). 2.2.2 Average shortest path For a given residue,  L  is the sum of the shortest paths to that residue, divided by the total number of residues less one. MD-TASK uses a custom algorithm in the NetworkX library for quickly calculating the shortest path between all residues ( NetworkX Developers, 2017 ). The average shortest path describes how accessible a residue is within the protein. 2.2.3 Residue contact map Residue contact maps are generated by monitoring the interactions of a residue throughout a simulation, yielding a network diagram with the residue of interest [e.g. single nucleotide polymorphism (SNP)] at the center, and residues that it interacts with arranged around it. Edges between the residue of interest and the other residues are weighted based on how often the interaction exists. 2.3 Perturbation response scanning Given the atomic coordinates for initial and target states, together with an equilibrated MD trajectory of the initial structure, the algorithm performs a residue-by-residue scan of the initial conformation, exerting external forces on each residue, and records the subsequent displacement of other residues using linear response theory and a variance-covariance matrix obtained from the MD trajectory ( Atilgan  et al. , 2012 ). The quality of the predicted displacement is assessed by correlating the predicted and experimental difference between the initial and target states. This results in a correlation coefficient for each residue, where a value close to one implies good agreement with the known experimental change. Residues whose perturbation invokes a conformational displacement closest to the target structure are reported as hot residues. 2.4 Dynamic cross-correlation DCC is calculated using the following formula:
 C i j = Δ r i ⋅ Δ r j Δ r i 2 ⋅ Δ r j 2 
with  Δ r i  the displacement from the average position of atom  i , and ⟨⟩ the time average over the whole trajectory ( Di Marino  et al. , 2015 ). MD-TASK generates a heat-map depicting the DCC between the  C α  atoms of selected frames in a trajectory to identify relative residue movements. 3 Performance The results of a performance test are presented in  Table 1 . Tests were conducted on the ‘example_small.dcd’ trajectory, a 599 residue, 2000 frame trajectory provided in the ‘example’ sub-directory of the MD-TASK Github repository. Tools were set to iterate through the trajectory at 100 frame intervals and then executed 10 times. These executions were timed using the Linux ‘time’ utility, which includes the time to start up the Python interpreter, providing an accurate measure of what users will experience in practice. An average time was then calculated for each tool. The PRS script used 50 random forces per residue.
 Table 1. Results of MD-TASK performance tests Script Average time (s) calc_network.py (–calc-L) 37 298 calc_network.py (–calc-BC) 62 109 calc_delta_BC.py 16 852 calc_delta_L.py 1864 avg_network.py 1713 compare_networks.py 4230 delta_networks.py 2289 contact_map.py 19 806 calc_correlation.py 39 703 prs.py 95 480 Tests were conducted on a PC running Ubuntu 16.04 with an Intel Core i5-6300U CPU with 4 logical cores running at 2.4 GHz and 8 GB RAM. 4 Applications The network measures, BC and  L , have been used in minimized protein structures ( Ozbaykal  et al. , 2015 ) for Alanine scanning. One suggested use, here, is SNP analysis over MD simulations. The network analysis scripts of MD-TASK were applied to analyze renin-angiotensinogen system ( Brown  et al. , 2017 ). The data indicated that combination of RMSF values with network analysis can be informative. Further, a SNP analysis protocol combining traditional MD analysis tools with DRN is proposed in a recent review article ( Brown and Tastan Bishop, 2017 ). PRS identifies single residues playing an active role in protein conformational changes between an initial and target state ( Atilgan and Atilgan, 2009 ). Perturbative methods are instrumental in uncovering different protein functions such as the effect of pH on the distribution of conformations of calmodulin ( Atilgan  et al. , 2011 ) and ferric binding protein ( Atilgan and Atilgan, 2009 ). By disclosing nonevident relationships, PRS was used to suggest new experiments to explore allosteric communication, e.g. in HSP70 ( Penkler  et al. , 2017 ). As an example of MD-TASK outputs, we used HIV protease. Sequence of closed conformation of crystal structure (4ZIP) was used as wild type (WT) target sequence to model its structure in open conformation by means of homology modeling. Two major (V32I, I47V) and one minor (V82I) mutations were then introduced, also using homology modeling, to create the open conformation mutant structure. In both cases 1TW7 was used as the template. Network analysis was performed for 40 ns MD runs for both WT and mutant proteins in open conformation ( Fig. 1A ). DCC was calculated for the mutant protein ( Fig. 1B ). For PRS ( Fig. 1C ), an equilibrated 20 ns section of the mutant trajectory was used. The PDB structure, 3S54, which represents the closed conformation with the same mutations, was used as the end state during PRS calculations. Residues having the highest change in reachability (highest Δ L ) during the course of the trajectory are 46–56, comprising the flap. This property gives HIV-protease the flexibility to expand the active site cavity and diminish the effect of inhibitors while staying functional ( Martin  et al. , 2017 ). The peaks in ΔBC correspond to active site residues 25–27, supporting the hypothesis that high BC positions are responsible for interdomain communication ( Ozbaykal  et al. , 2015 ). While these properties are similar in both WT and mutant, the neighborhood structure of position 47 slightly shifts, as shown in the residue contact maps. DCC ( Fig. 1B ) demonstrates that the intrachain motions are highly correlated, while the motions of the chains with respect to each other are anticorrelated (residues 1–99 versus 100–198). The PRS results ( Fig. 1C ) display that there are no single residues whose perturbation directly leads to the conformational change between the equilibrated WT structure at the 20 ns snapshot and the triple mutant, as the maximum correlations are ∼0.6. Nevertheless, highest correlations span residues 60–72 implying an allosteric communication between this region and the flap motions. The specific residues are mapped onto the protein structure in  Figure 1D .  Figure 1  summarizes how MD-TASK provides a means to analyze the trajectories, and gives a bird’s eye view of various factors that may be effective in the dynamics of a protein.
 Fig. 1 Outputs for  (A)  Network analysis: average Δ L ; average ΔBC; residue contact maps (from top to bottom);  (B)  DCC; ( C)  PRS (plot not generated by MD-TASK);  (D)  HIV-protease with significant regions highlighted 5 Conclusion MD simulations have become an important tool in structural bioinformatics. Here, we present a new tool suite for analyzing MD trajectories using DRN analysis, PRS, and DCC. To the best of our knowledge, MD-TASK is the first downloadable tool suite for analysis of different properties along MD simulations not commonly found in other MD packages. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Quality assessment of protein model-structures using evolutionary conservation</Title>
    <Doi>10.1093/bioinformatics/btq114</Doi>
    <Authors>Kalman Matan, Ben-Tal Nir</Authors>
    <Abstract>Motivation: Programs that evaluate the quality of a protein structural model are important both for validating the structure determination procedure and for guiding the model-building process. Such programs are based on properties of native structures that are generally not expected for faulty models. One such property, which is rarely used for automatic structure quality assessment, is the tendency for conserved residues to be located at the structural core and for variable residues to be located at the surface.</Abstract>
    <Body>1 INTRODUCTION The function of a protein is largely determined by its 3D structure. Therefore, the determination of a protein's structure is an important step in understanding how the protein achieves its function, and it can also aid in predicting protein function or designing experiments. However, experimental structure determination can be a long and difficult procedure, and naturally errors may occur (Kleywegt,  2009 ). This was recently demonstrated when several protein structures published in the Protein Data Bank (PDB) were discovered to be erroneous (Chang  et al. ,  2006 ). Even when the structure determination process is correct, the determined structure may adopt a non-physiological fold, for example, due to non-physiological constraints imposed by the crystal in the case of X-ray crystallography. Such errors can cause confusion and mislead further research, so it is important to be able to spot them before the structures are published. Errors are even more frequent in computationally derived structures, which are built either by extrapolating from a homologous protein whose structure is already solved (Fiser and Sali,  2003 ; Ginalski,  2006 ) or by computer simulation (Das and Baker,  2008 ; Zhang and Skolnick,  2004 ). In the latter case, many alternative conformations might be generated during the simulation, and differentiating between erroneous conformations and structures that are more likely to be correct could help guide the simulation and limit the search space. Programs that try to numerically assess the correctness of a given structural model for a protein are called Model Quality Assessment Programs (MQAPs). The need for such programs is widely recognized by the structural biology community, as evidenced by the inclusion of a category for assessing MQAP performance in the biennial Critical Assessment of Techniques for Protein Structure Prediction (CASP) experiment, starting from its seventh round (CASP7; Cozzetto  et al. ,  2007 ). The two pioneering MQAPs, still widely used today, are Verify3D (Eisenberg  et al. ,  1997 ) and ProSa (Wiederstein and Sippl,  2007 ). Both methods check the compatibility between the protein's structure and its sequence. Verify3D, for example, classifies each residue in the protein into one of the 18 classes according to the residue's structural environment in the input model. The propensity of each amino acid to exist in each such structural environment class is calculated according to statistics collected from structures in the PDB, and the final score given to the protein structure is the sum of propensities of the individual residues. Newer MQAPs were recently assessed in the blind experiments of CASP7 (Cozzetto  et al. ,  2007 ) and CASP8 (Cozzetto  et al. ,  2009 ). The models given as input to the MQAPs were the ‘server models’, which are generated by the various servers participating in CASP shortly after the round starts, and long before the native structures are published. Many of the participating MQAPs, including QMEAN (Benkert  et al. ,  2009 ) and MULTICOM-REFINE (Cheng  et al. ,  2009 ), functioned similarly to Verify3D and ProSa, receiving only one structure as input and assigning it a quality score based on the compatibility of various features computed from the sequence with the predicted 3D structure. However, the most successful MQAPs in CASP8 were the consensus-based methods, such as Pcons (Larsson  et al. ,  2009 ) and ModFOLDclust (McGuffin,  2009 ), which used as input the entire decoy set instead of just coordinates of a given model and took a consensus approach to rate each model according to how similar it was to the other structures in the set. This approach, while clearly advantageous in the setting of the CASP experiment, is not applicable in many scenarios in which few structures (possibly only one) are available, or when the decoy set is not likely to contain many correct models (Cozzetto  et al. ,  2007 ; Wallner and Elofsson,  2008 ). The single-structure MQAPs that performed best in CASP8 were LEE and LEE-server. The group produced their own structural model for each target and ranked the decoys according to how similar they were to their model (Cozzetto  et al. ,  2007 ). The models produced by the LEE group were homology based, so at least part of the success of the method could be attributed to the fact that it used additional constraints from structural models of homologous proteins. Two other methods from CASP8, SAM-T08-MQAU and SAM-T08-MQAO (Archie  et al. ,  2009 ), also used such constraints taken from structural homologs, and they also performed significantly better than the rest of the single-structure methods. While homology-based approaches have proven very promising, they are only usable when reliable structural homologs exist. Therefore, there is still a need for devising methods that do not use additional structural information, neither from structural homologs nor from the other decoys. We term such methods ‘pure single-structure MQAPs’. An alternative strategy to that of most single-structure MQAPs is to check the compatibility of the suggested 3D structure with the evolutionary conservation pattern of the sequence. There are various ways to calculate the conservation level (or evolutionary rate) of an amino acid position (Glaser  et al. ,  2003 ; Mihalek  et al. ,  2004 ). A residue that is conserved throughout evolution has undergone strong purifying selection; this suggests that the conserved residue is important for the protein's normal function (Brändén and Tooze,  1999 ). This observation has been used in many applications, such as identifying the active site of a protein, which is usually composed of a patch of clustered residues on the protein's surface (Nimrod  et al. ,  2008 ). An interesting observation is that for most proteins, the structural core is composed mainly of such conserved residues (see for example,  Fig. 1 A). These residues are usually not involved directly in the mechanism of the protein's function. Rather, they are conserved because a mutation in such a buried residue would tend to perturb the architecture. The protein surface, in contrast, is mostly variable. If the association between residue accessibility and conservation level is strong enough, it might be used to differentiate between correct and incorrect model structures, as incorrect structures are unlikely to feature this pattern by chance.
 Fig. 1. Tendency of conserved residues to be buried in correct structures. ( A  and  C ) The native structure for the CASP7 target T0289 (Aspartoacylase, PDB 2gu2A) in (A), and a poor model for the same target (model FPSOLVER-SERVER_TS1, GDT-TS = 7.9) in (C), colored by the ConSurf color map using the ConSurf-DB database. ConSurf colors 1–7 are semitransparent to show the cluster of conserved residues buried at the structural core in the native structure. ( B  and  D ) Distribution of relative residue accessibilities as calculated by Naccess for variable residues (cyan; ConSurf classes 1, 2, 3) and for conserved residues (purple; ConSurf classes 8, 9). The distributions are shown for the native structure in (B) and for the poor model in (D). ( E ) Distribution of relative residue accessibilities for all residues of all structures in the dataset, classified by their ConSurf conservation grades. The different grades are colored according to the ConSurf color scheme. There is a consistent shift to the right, with the most variable residues (ConSurf class 1) being most accessible. Molecular graphic images were generated using UCSF Chimera (Pettersen  et al. ,  2004 ). This conservation pattern has been used for computational modeling of proteins, both manually for checking the validity of a built model (Landau  et al. ,  2007 ) and automatically for generating a Cα model of transmembrane proteins starting from a low-resolution cryo-EM map (Fleishman  et al. ,  2004a ,  b ,  2006 ). Conservation information has also been used for quality assessment in several studies. The first conservation-based approach is to use the observation that conserved residues tend to be clustered in the native structure (Mihalek  et al. ,  2003 ; Muppirala and Li,  2006 ; Schueler-Furman and Baker,  2003 ). This clustering is expected both for structurally conserved residues, as they form the structural core, and for functionally conserved residues, which are usually localized on the surface, at the functional site of the protein. Mihalek  et al.  ( 2003 ) used the evolutionary trace method to collect a set of conserved residues and quantified the set's tendency to cluster using a measure they termed the selection clustering weight (SCW). They applied this method to the Decoys ‘R’ Us decoy set (Samudrala and Levitt,  2000 ) and showed that indeed 78.1% of the decoys in the set were assigned a lower (less favorable) SCW score compared with the native structure. However, the assessment of a method by its ability to rank a native structure higher than decoys has been shown to be problematic (Handl  et al. ,  2009 ). Schuler-Furman and Baker (Schueler-Furman and Baker,  2003 ) took a similar approach, adopting a simpler strategy for selecting the set of conserved residues based on entropy, as well as a different measure for quantifying the clustering. However, they validated their method in a more relevant scenario, showing that when the method is used to select decoys generated by ROSETTA (Das and Baker,  2008 ), there is a statistically significant enrichment in correct models. The second approach to exploit conservation data is to use it initially to make contact predictions and subsequently use the predictions for quality assessment. Olmea  et al.  ( 1999 ) provided a set of contact predictions, using the observation that in pairs of conserved residues, as well in pairs of residues whose mutations are correlated, the members of the pair tend to be spatially close to each other in the 3D structure. They have shown that such predictions are usually more precise for native structures than for deliberately misfolded ones. They also used this information as a post-processing step in a threading method and showed that it improved the method's results. More recently, Miller and Eisenberg (Miller and Eisenberg,  2008 ) built an MQAP based on the agreement between such contact prediction information and the set of contacts in the proposed model. They checked their method on several of the CASP7 targets and proved that it performed significantly better than random. While previous studies have suggested that evolutionary information can be used for quality assessment, the performance of these methods was never compared with that of other MQAPs. Furthermore, these methods only used the tendency of conserved residues to be spatially close to one other, which captures only partially the information that is present in the conservation–accessibility relation. In this study, we present a new very simple MQAP called ConQuass (conservation-based quality assessment), which is based on the correlation between each residue's degree of evolutionary conservation and its accessibility in the structure. We check the performance of ConQuass on the CASP8 dataset, and show our method to be comparable to the other pure single-structure MQAPs that participated in CASP8. We also show that ConQuass is complementary to existing methods and could potentially be integrated with them to improve their performance. 2 METHODS 2.1 Collecting a training set of known structures The PISCES server (Wang and Dunbrack,  2003 ) was used to collect a non-redundant set of X-ray, full-atom protein structures from the PDB that have resolution better than 3.0 Å, R-factor better than 0.3 and sequence identity &lt;25%. This resulted in a set of 6132 protein chains. Of those, we used only the 5648 proteins for which evolutionary conservation information was available in the ConSurf-DB database (Goldenberg  et al. ,  2009 ). We generated three structures for each protein chain. The first contained only the given chain in isolation, without the other chains in the PDB structure. In addition, two versions of the chain in the context of its biological unit were generated using either the protein quaternary structure (PQS) server (Henrick and Thornton,  1998 ) or the progressive iterative signature algorithm (PISA) server (Krissinel and Henrick,  2007 ). Non-protein chains were removed. Finally, we removed each protein whose complex contained &gt;26 protein chains and eliminated protein structures that were too big to run in Naccess (Hubbard and Thornton,  1993 ), leaving a total of 5543 proteins in the final set. 2.2 Features collected for each structure 2.2.1 Conservation We collected the conservation level of each residue from the ConSurf-DB database (Goldenberg  et al. ,  2009 ), which provides precalculated conservation profiles for every structure in the PDB. These profiles assign each residue to one of nine conservation levels, with 9 being the most conserved and 1 being the most variable. For some residues, the information in the multiple sequence alignment is not enough to compute the conservation level (for example, if that position consists mostly of gaps). In these cases, ConSurf-DB assigns the residue value of ‘insufficient data’. 2.2.2 Accessibility We used the program Naccess to calculate the total relative accessibility for each residue (Hubbard and Thornton,  1993 ). We further normalized these accessibility values by transforming them into quantiles, so that the most buried residue in a given protein would get the value 0 and the most exposed would get the value 1. This normalization was done in light of the observation that some protein structures might overall be more accessible than others owing to their geometric properties, but within a single structure conserved residues still tend to be more buried compared with other residues in the same structure. Each residue was then classified into one of ten evenly distributed accessibility classes. 2.2.3 Structure quality features For each structure, we extracted the resolution and the R- and free R-factors from the PDB as measures of the general structure quality. This was done in order to validate our assumption that the correlation between the level of burial and evolutionary conservation of the amino acids would increase with the structure quality (see  Section 3.1.1 ). 2.2.4 Alignment quality features We collected the following measures for each structure: (i) Nseq, the number of homologs in ConSurf-DB's alignment; (ii) Nseq20, the number of homologs in the alignment whose identity is &gt;20% [the level of identity for each homolog is extracted from the PSI-BLAST output (Altschul  et al. ,  1997 ), taken from ConSurf-DB]; (iii) Resnum, the number of residues with significant conservation information; (iv) %insig, the fraction of the protein residues whose conservation level is assigned the value ‘insufficient data’. These features were chosen to reflect the general quality of the alignment and evolutionary rates generated by ConSurf-DB for each protein. 2.2.5 Finding the optimal filtering cutoffs The four measures of alignment quality that we collected could each help predict in advance whether a given protein would be well-suited for use with our method. To find the optimal way to integrate these features, we solved the following optimization problem: given a ratio  X  (called the filtering degree), find the optimal quadruple of cutoffs such that when filtering the dataset according to these cutoffs,  X  of the proteins in the dataset pass the filter, and their average ConQuass score (as defined in  Section 2.3 ) is maximal. This problem was solved for each  X  in 0.01, 0.02,…, 1 using an exhaustive enumeration, enumerating for each cutoff over 50 discrete values distributed evenly across the dataset. In what follows we refer to proteins that passed the filter corresponding to a given filtering degree  X  as having a ‘high-quality alignment, according to the  X -filter’, where a higher filtering degree corresponds to a more stringent requirement. 2.3 The ConQuass score Similarly to Verify3D (Bowie  et al. ,  1991 ), we built a 10 × 9 propensity matrix, where each cell gives the compatibility score for assigning a residue with conservation class  c  an accessibility class  a , as given by the information value (Fano,  1961 ):
 
where  P ( c | a ) is the probability of finding a residue of conservation class  c  in the accessibility class  a , and  P ( c ) is the overall probability of finding a residue in conservation class  c . These probabilities are estimated using the conservation and accessibility levels of the residues in the dataset of known protein structures. The accessibilities were calculated using the biological unit given by PQS. We also tried using the PISA biological unit or the isolated chain, but the propensity matrices generated were very similar (data not shown). The final propensity matrix is shown in  Supplementary Table S1 . ConQuass assigns each structure the average score of its residues:
 
where  C  and  A  are vectors of the same length  L  (number of amino acids in the protein), giving, respectively, the conservation and accessibility classes of the residues. 2.4 Assessment on the CASP dataset In the model quality assessment category of CASP, the participating groups were asked to rank the models built by the participating automatic servers. We downloaded these server models, as well as the predictions of the participating MQAPs, from the CASP web site ( http://predictioncenter.org ). We also downloaded for each server model its global distance test total score (GDT-TS) (Zemla,  2003 ), which is in the range (0, 100] and is the standard quality evaluation score given by CASP. For each CASP target, we downloaded conservation information, if available, from the ConSurf-DB database entry for the native structure. The same conservation information was aligned to all full-atom models of the target, as there is sometimes a shift between the residue sequence numbers in the native structures and in the CASP models. To this end, we ranked each such alignment by giving each column a score of +1 if the residue identity in the native matched that of the model, −2 if the residues did not match, −1 for an insertion/deletion and 0 if the residue was missing in one of the structures. The optimal alignment was then found using the Smith–Waterman algorithm (Smith and Waterman,  1981 ). For each model, we computed the accessibility class of each residue, as was done for the structures in the training set (see  Section 2.1 ). The conservation levels and accessibilities were used to calculate the MQAP score for each model (see  Section 2.3 ). We did not score targets whose native structure had no ConSurf-DB information. When comparing ConQuass to the MQAPs that participated in CASP7, we considered only the 16 MQAPs that had ranked at least 15 000 models. For MQAPs that participated in CASP8, we considered only the 22 pure single-structure methods that had ranked at least 20 000 models. We restricted each analysis to models that had been ranked by all considered methods (including ConQuass), and from this set we eliminated targets for which fewer than 100 ranked models were available. For each MQAP and each target, we calculated the Pearson correlation between the quality scores given by the MQAP and the GDT-TS scores downloaded from the CASP web site. 2.5 Integration of ConQuass with other methods To demonstrate that the conservation information used in ConQuass is complementary to that used by other methods, we built three new MQAPs, integrating the score given by ConQuass ( Section 2.3 ) with the scores given by Circle-QA (Terashi  et al. ,  2007 ), QMEANfamily (Benkert  et al. ,  2009 ) and MULTICOM-REFINE (Cheng  et al. ,  2009 ), respectively. We chose Circle-QA because it was the leading pure single-structure method in CASP7, and we chose QMEANfamily and MULTICOM-REFINE because they were the leading pure single-structure methods in CASP8. For each integrated MQAP, the score we assigned to each model was a simple linear combination of the ConQuass score and the score produced by the other method (the two scores were each assigned a weight of 0.5). The analysis described in  Section 2.4  was repeated for these three MQAPs. We compared the first MQAP (integration with Circle-QA) to MQAPs that had participated in CASP7 and compared the other two (integration with QMEANfamily or MULTICOM-REFINE) to MQAPs that had participated in CASP8. 3 RESULTS AND DISCUSSION 3.1 Experimentally determined structures match their conservation pattern 3.1.1 Examining a dataset of high-quality structures It is widely recognized that residues buried in the protein core tend to be evolutionarily conserved, whereas residues on the surface are usually variable (Brändén and Tooze,  1999 ; Lichtarge  et al. ,  1996 ). This implies that the accessibilities of variable residues should be shifted toward higher values in comparison with those of conserved residues, as indeed seems to be the case for many experimentally solved protein structures we examined (e.g.  Fig. 1 A and B). This characteristic is expected for true protein structures, and we would generally not expect to see it in incorrect models.  Figure 1 C and D show the evolutionary profile of an extremely poor model structure (analysis of an intermediate quality model of the same protein is provided in  Supplementary Fig. S1 ). We first set out to measure the magnitude of this trend in real protein structures. For that purpose, we collected a comprehensive dataset of high-quality experimentally determined structures, which we can reasonably assume to contain mostly ‘correct’ structures ( Section 2.1 ). For this dataset, it is obvious that the more variable residues are consistently more accessible than the conserved residues ( Fig. 1 E). The information in this dataset was used to calculate a propensity matrix, giving the compatibility of each conservation class with each accessibility class ( Section 2.3 , and  Supplementary Table S1 ). The matrix confirmed our intuitive expectations, giving high propensity scores to accessible-variable residues and to buried conserved residues. Consequently, the matrix was used to calculate each protein structure's ConQuass score, which was the average of the propensity scores of the protein's residues. A score was calculated for each structure in the dataset ( Fig. 2 ), using the biological unit complexes as given by PQS (Henrick and Thornton,  1998 ). Only 7.9% of the structures received a negative score, meaning that for most structures the residues' conservation levels tended to be compatible with their accessibility levels. However, when we determined scores for the individual chains in the dataset without the context of the biological unit, more structures were assigned a negative score (12.5%). This was due to monomers exposing conserved interface residues that are actually buried in the physiological complex. We also tried to determine scores for the biological unit complexes given by PISA (Krissinel and Henrick,  2007 ) and the results were very similar to those obtained for the PQS complexes (data not shown). The ConQuass scores also seemed to become progressively higher for structures of higher quality, as measured by various structure quality measures such as resolution, R-factor and free R-factor ( Fig. 3 A).
 Fig. 2. ConQuass scores assigned to experimental structures from the PDB and to a few erroneous models. The scatter plot shows the propensity score of the protein versus the number of residues with ConSurf information for all the structures in the dataset (in gray). Only structures that have ConSurf information for at least 40 residues were included. Also shown are pairs of incorrect (triangle) and correct (circle) structures for EmrE (black, 2f2m and 3b5d), Connexin (gray, 1txh and 2zw3) and MsbA (white, 1jsq and 3b5w). For each of these structures, the ConQuass score was calculated for the residues of all the chains in the biological unit. For models containing only the Cα-trace, the full-atom structures were rebuilt using MaxSprout and SCWRL4. The correct structure of MsbA (3b5w) was truncated to contain the same set of residues as the erroneous structure (1jsq). 
 Fig. 3. Compatibility of the structure with the evolutionary profile of the protein is higher for higher-quality structures or higher-quality multiple sequence alignments, as described by different quality measures. ( A ) The mean ConQuass score of the proteins in the dataset when filtering only for the top  X  proteins ( x -axis), as measured by several crystallographic structure quality measures: the R-factor, free R-factor and the resolution. ( B ) As in (A), but when filtering by non-structural measures: the number of residues (red), the number of homologous sequences in the alignment (black), the ratio of residues with insignificant conservation information as measured by ConSurf (green) and the number of homologous sequences in the alignment with at least 20% identity with the query (blue). Also shown is the optimal ratio achieved by integrating these four measures (gray). The conservation data was calculated according to the multiple sequence alignment generated automatically by ConSurf-DB, and it is possible that a high-quality structure would be assigned a low ConQuass score if an inadequate alignment was used. To discern these cases, we collected four measures that are indicative of the alignment quality or that could otherwise predict an incorrect ConQuass score for a protein model (see  Section 2.2.4 ). As can be seen in  Figure 3 B, the ConQuass score becomes progressively higher as the dataset is filtered to leave only structures whose alignment is of higher quality according to any one of the four measures. Obviously, a better indicator for how suitable a protein is for ranking with ConQuass can be achieved by integrating the different alignment quality measures. We used an exhaustive enumeration to find the optimal way to integrate these measures, each time filtering the database to leave only  X % of the proteins such that the mean ConQuass score of the remaining proteins is maximal ( Section 2.2.5 ). This procedure assumes that after filtering, a higher mean ConQuass score is achieved because the remaining proteins have a higher quality alignment. The integration achieves a much higher mean ConQuass score than does filtering by each measure separately ( Fig. 3 B; gray). The optimal cutoffs found for some selected filtering degrees are shown in  Supplementary Table S2 . 3.1.2 The conservation profile may reveal incorrect structures To test whether the ConQuass score is capable of discriminating incorrect structures, we collected three examples of structural models that had been deposited in the PDB but were later found to be incorrect. All these structures also have corrected versions available, which we also scored using ConQuass ( Fig. 2 ). The first two examples are EmrE ( Fig. 2 ; black) and MsbA ( Fig. 2 ; white). Both structures were determined by Chang and coworkers using a faulty piece of in-house software, which caused the group to misinterpret the crystallization data and eventually yielded false models. Following the detection of the error in the software, the structures were retracted (Chang  et al. ,  2006 ), and corrected versions have since been published (Chen  et al. ,  2007 ; Ward  et al. ,  2007 ). Calculating the ConQuass score for these structures is not straightforward, as they are all Cα-only models, with the exception of the erroneous EmrE structure. However, we were able to apply ConQuass after reconstructing the full-atom models using MaxSprout (Holm and Sander,  1991 ) and SCWRL4 (Krivov  et al. ,  2009 ). Clearly, the correct structures are much more compatible with their conservation pattern than are the incorrect ones ( Fig. 2 ). The third example is the gap junction connexin channel ( Fig. 2 ; gray), which was previously modeled by our group using low-resolution electron cryo-microscopy data (Fleishman  et al. ,  2004b ). The helix assignment of the model recently turned out to be wrong when an experimentally determined high-resolution structure of a homologous protein was reported (Maeda  et al. ,  2009 ). For the purpose of comparing the two structures, we truncated the non-membrane residues from the experimental structure and also removed all non-Cα atoms. This procedure left us with two Cα-only models composed of the same set of residues. We then rebuilt the two full-atom models as above and scored them using ConQuass. While both the truncation of non-membrane residues and the full-atom reconstruction lowered the score for the crystallographic model (data not shown), it was still assigned a much higher score than the erroneous model ( Fig. 2 ). There are some cases in which a correct model seems not to match its conservation pattern, as denoted by a negative ConQuass score. However, a closer examination can usually provide an explanation for the low score. Some representative examples are discussed in  Supplementary Section S1.1 . 3.2 Ranking decoys in CASP ConQuass may also assess how distant a given model is from the native structure. To show this, we checked how ConQuass scores models of varying quality for the same protein. A good source for such models is the biennial CASP experiment (Moult  et al. ,  2009 ), where each round consists of several targets, corresponding to proteins whose structure have recently been solved (but not yet published), and each participant submits computational models in an attempt to predict the structure of each target. At the end of the round, the experimental structures are revealed, and the quality of each submitted model is measured by the similarity measure GDT-TS (Zemla,  2003 ), which is based on the superposition between the model and the native structure. The seventh and eighth CASP rounds included a quality assessment category (Cozzetto  et al. ,  2007 ,  2009 ), in which different MQAPs participated and were consequently evaluated according to their performance. The models scored by the MQAPs were server models that were generated by the structure prediction servers participating in CASP and published shortly after the round began. The MQAPs were evaluated according to the correlation between the scores they gave the different models and the quality of those models as measured by GDT-TS. The scores given by the participating MQAPs are available for download from the CASP web site ( http://predictioncenter.org ), which allowed us to compare them with ConQuass. The results for the CASP8 set are presented below. The analysis of the CASP7 set showed a similar performance, and it is presented in  Supplementary Section S1.2 . To be able to best compare ConQuass with the other pure single-structure methods, we have excluded from our analysis methods that use structural data from the other decoys or from homologs (a comparison of ConQuass with the latter methods is shown in  Supplementary Fig. S2 ). For brevity, we will use the term MQAP in this section to refer only to pure single-structure methods. 3.2.1 Example of the performance on one CASP8 target As an illustrative example,  Figure 4 A shows the GDT-TS values of the server models of CASP8 target T0449, plotted as a function of the assigned ConQuass scores. There is a striking correlation between the score and the structure quality, and the set of highly scored models was enriched with high-quality structures. The Pearson correlation in this case was 0.827, and the score of the native structure ( Fig. 4 A; vertical line) was higher than the scores of all the decoys except three.
 Fig. 4. The ability of the ConQuass score to rank decoys in the CASP8 dataset. ( A ) Demonstration for target T0449. For each decoy, the GDT-TS (similar to the native structure) is plotted versus the ConQuass score. The vertical line is the ConQuass score assigned to the native structure. The Pearson correlation for this target was 0.827. ( B ) Box plots of the correlation values for the 22 MQAPs that ranked at least 20 000 models. The number signifying each MQAP ( x -axis) is the number assigned in the original CASP8 experiment (see  http://predictioncenter.org/casp8 ). Also shown is the box plot for the correlation values of ConQuass (999, gray). The correlations were calculated only for models ranked by all 23 MQAPs. The box plots were sorted by the mean correlation, indicated by the black dots. The figure is cut to show only the correlation range [0.5, 1] in order to make the differences between the methods more apparent (the uncut version is shown in  Supplementary Fig. S3 ). ( C ) Same as (B), when looking only at targets with the highest quality alignment, using the 20% filter. Although ConQuass is ranked first here, the specific ordering of the top ranking methods is irrelevant, as the correlation values achieved by ConQuass are not significantly higher than those achieved by MULTICOM-REFINE (013). 3.2.2 Overall performance on all CASP8 targets In our calculation on the CASP datasets (see  Section 2.4 ), we used the conservation data recorded in the ConSurf-DB dataset. There are cases in which the alignment could have been manually improved in order to achieve a better performance, but we deliberately refrained from doing this to avoid biasing our results. Four CASP8 targets could not be ranked, because their native structures did not have any ConSurf-DB information. This usually happens when ConSurf-DB cannot find enough homologs to construct a meaningful alignment. Ten additional targets were cancelled by CASP8 or had no corresponding native structure listed in the CASP8 web site. A ConQuass score was given to each of the full-atom models of the remaining 114 targets. CASP allowed each participating MQAP to choose to rank any subset of models, for any subset of targets. Indeed, many MQAPs are not applicable for all models. This makes performance comparison problematic. For example, it might be easier to assess the quality of full-atom models, and if so an MQAP (such as ConQuass) that ranks only such models would have an advantage over methods that also rank Cα models. To avoid this problem, we carried out all calculations on the set of 11 686 models and 75 targets that were ranked by all participating MQAPs. To avoid excluding too many models, only the 22 participating MQAPs that scored at least 20 000 models were used. To evaluate the performance of each MQAP, we calculated for each CASP8 target the Pearson correlation between the scores determined by the MQAP and the GDT-TS values of all the models for that target. The sets of correlation values for each MQAP are plotted in  Figure 4 B. Our ranking of the methods is slightly different from the ranking published in the CASP8 proceedings (Cozzetto  et al. ,  2009 ) due to differences in the ranking protocol (see a detailed explanation of the differences in  Supplementary Section S1.3 ). However, as in the CASP8 results, the MQAPs that performed best according to our assessment were the different variants of QMEAN (Benkert  et al. ,  2009 ) and MULTICOM (Cheng  et al. ,  2009 ). The variants with the highest mean correlation were QMEANfamily (082) with a mean correlation of 0.778 and MULTICOM-REFINE (013) with a mean correlation of 0.768. Following the different variants of QMEAN and MULTICOM, the method with the next highest ranking, with a mean correlation of 0.722, was CIRCLE (396), which was the best performing single-structure MQAP in CASP7 (Cozzetto  et al. ,  2007 ). ConQuass (999,  Fig. 4 B; gray) ranked next, with a mean correlation of 0.715. As shown in  Section 3.1.1 , some structures were assigned a low ConQuass score because of a low-quality alignment rather than a low-quality model. Indeed, for some CASP8 targets, the native structure itself scored very low by ConQuass. Such targets were clearly not suitable for use with our method. Many of these cases could be discerned in advance by using the alignment quality measures presented in  Section 2.2.4 . To check how our method performs on more appropriate targets, we used the 20% filtering to select only the 17 targets with the highest quality alignment. The performance of the different methods for this subset of targets is shown in  Figure 4 C. With these targets, ConQuass performs significantly better, and it is ranked first with a mean correlation of 0.844. It is important to stress that the set of targets that are more suitable for use with ConQuass can be selected a priori, as all the features used for the filtering are based on the multiple sequence alignment alone. The Pearson correlations we evaluated were calculated for each target independently, so scores produced by an MQAP that achieves a high correlation value can be used to select among alternative structural models for the same protein. However, in many scenarios one wants to evaluate the absolute quality of a single uncertain structural model without comparing it to other decoys. For these cases, it is informative to know the relation between the MQAP score and the structure quality, as measured by the GDT-TS. This relation, for all models of all CASP8 targets, is shown in  Figure 5 A. The overall correlation between the ConQuass score and GDT-TS is good (0.678), especially when filtering for targets with a high-quality alignment (0.843 if using the 20% filtering,  Fig. 5 A; red). The overall correlation was also compared with that of the other participating MQAPs ( Supplementary Table S3 ).
 Fig. 5. The relation between the ConQuass score assigned to the model and the model's quality. ( A ) Plot of the GDT-TS (similar to the native structure) versus the ConQuass score for the following models: all models in the CASP8 dataset (black, Pearson correlation 0.678); the models with the highest quality alignment by the 50% filter (blue, Pearson correlation 0.780); models that passed the less permissive 20% filter (red, Pearson correlation 0.843). ( B ) Box plots of the GDT-TS values for models in each ConQuass score quartile. For example, the median GDT-TS for the models scoring very low (below −0.017) is 22.8, and 50% of these models have GDT-TS values between 14.8 and 40.6. For the models scoring very high (&gt;0.12), the median GDT-TS is 73.1, and 50% of these models have GDT-TS values between 65.0 and 81.7. Figure 5 B presents these results in a way that is more intuitive for interpreting the score given to a model by ConQuass. If a model is assigned a very low ConQuass score (below -0.017), it is expected to be of rather low-quality (median GDT-TS 22.8, most GDT-TS values in the range [14.8, 40.6]). However, if a model is assigned a very high ConQuass score (&gt;0.12), it will very rarely be a low-quality structure (median GDT-TS 73.1, most GDT-TS values in the range [65.0, 81.7]). 3.2.3 Complementarity to other MQAPs ConQuass uses the evolutionary conservation properties of the protein structure, a feature that is not directly used by any other contemporary MQAP. It therefore seems reasonable to suggest that ConQuass is complementary to the other prevalent methods. To support this claim, we scored the CASP8 models using two new MQAPs that were trivial integrations of ConQuass with, respectively, MULTICOM-REFINE and QMEANfamily, the two best performing single-structure MQAPs in CASP8 (see  Section 2.5 ). The performance of these two integration methods was analyzed using the same procedure described above. The integration with ConQuass significantly improved the correlations achieved by both MULTICOM-REFINE ( P -value ∼ 4.2e-14) and QMEANfamily ( P -value ∼ 1.1e-08); see  Supplementary Section S1.4 . 4 CONCLUSION Here we have presented ConQuass, a very simple MQAP based directly on the compatibility between the conservation and accessibility patterns of a given structural model. We studied the scores that ConQuass assigns to experimental structures, demonstrated its ability to discern erroneous models and checked the relation between the ConQuass scores given to different models and the models' resemblance to the native structure. We have also shown that ConQuass's performance is comparable to that of other pure single-structure MQAPs, despite being much simpler than most. Our approach is different from previous MQAPs that used evolutionary conservation, which were based on the spatial clustering of the conserved residues. We feel our approach is more direct, since this clustering is mostly an effect of the conserved residues' tendency to be buried in the structural core (for a direct comparison with the method developed by Mihalek and coworkers; see  Supplementary Section S1.5 ). ConQuass is also the first conservation-based approach to be rigorously compared with contemporary MQAPs. In addition, our score is based on summation of information that is local in the structure (the propensity of the conservation class of each residue for its accessibility class), so it should be adaptable to provide a local quality score for each residue of the structure, as is done by local quality assessment tools (Fasnacht  et al. ,  2007 ). Preliminary tests for a local MQAP based on summing the propensities over a fixed-width window on the sequence have yielded promising results (data not shown). In this study, we have clearly shown that evolutionary conservation is a powerful property for use in model quality assessment, so it would be advantageous for new MQAPs to integrate this property with other more commonly used properties. Evolutionary conservation is currently not used directly by any MQAP, although some methods, like QMEAN and MULTICOM, use it indirectly by comparing model surface accessibilities with the predicted accessibilities, which are associated, in part, with the evolutionary conservation. However, we have demonstrated that these methods do not use the conservation information to its full extent, as their results improve when their scores are integrated with those of ConQuass. In this work, we have followed a very naïve approach for such an integration, using a simple linear combination. Much better results would doubtless be obtained by a more intricate approach, for example by using the residue conservation as one of the features in a machine learning-based tool. In any case, such integration would have to take into account the quality of the alignment, as the evolutionary conservation property is more indicative for high-quality alignments. The same approach could also be used to integrate conservation in many other practices, such as finding the physiological complex of a crystal structure and scoring docking results. In addition, as the ConQuass score reflects the consistency between the alignment and the structure, its functionality could be reversed to check the quality of an alignment based on a high-quality structure. While ConQuass is not the best performing of the examined MQAPs, many of which use a mixture of complex features including geometric and energetic properties of the structure, it has the advantage of being straightforward and easy to interpret. The conservation pattern of the protein is not used by most modeling and structural determination programs, so ConQuass gives independent support for a structural model, whether experimental or computational. If the model is assigned a low score, it is easy to visualize the discrepancy of the model with the conservation pattern by projecting it on the structure using ConSurf (Glaser  et al. ,  2003 ), as we have done in the examples in  Figure 1  and  Supplementary Figure S4 . This can either yield relevant insights regarding the mechanism associated with the structure (for example, hint that it may bind to another molecule; see  Supplementary Section S1.1 ), lead to a rejection of the model (see  Fig. 1 C) or perhaps in some cases guide further refinements of the model. 
          Supplementary Material 
          
             
                [Supplementary Data] 
             
             
             
          
       </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Interactive microbial genome visualization with GView</Title>
    <Doi>10.1093/bioinformatics/btq588</Doi>
    <Authors>Petkau Aaron, Stuart-Edwards Matthew, Stothard Paul, Van Domselaar Gary</Authors>
    <Abstract>Summary: GView is a Java application for viewing and examining prokaryotic genomes in a circular or linear context. It accepts standard sequence file formats and an optional style specification file to generate customizable, publication quality genome maps in bitmap and scalable vector graphics formats. GView features an interactive pan-and-zoom interface, a command-line interface for incorporation in genome analysis pipelines, and a public Application Programming Interface for incorporation in other Java applications.</Abstract>
    <Body>1 INTRODUCTION The commercial introduction of next-generation sequencing technology has reduced the time and expense of genome sequencing to such an extent that it is now possible for virtually any microbiology lab to generate genome sequences for the microorganisms that they study. Visualizing the genomic features contained in these sequences is important to aid biologists in understanding the genetic basis of microbial traits, and to convey this information to others. A number of software packages already exist for this purpose ( Supplementary Table 1 ). Early software packages like GenomePlot ( Gibson and Smith, 2003 ) and GenoMap ( Sato and Ehira, 2003 ) generate circular genome maps in bitmap (PNG, JPG) formats, but do not support standard sequence file formats and have limited customizability. CGView ( Stothard and Wishart, 2005 ) and Circos ( Krzywinski  et al. , 2009 ) can generate very high quality, highly customizable circular maps in bitmap and scalable vector graphics (SVG) formats, and can be incorporated in pipelines, but have limited interactivity, and like their predecessors, require custom input formats. DNAPlotter ( Carver  et al. , 2009 ) generates interactive, high quality, customizable circular and linear maps, but does not support SVG image output, and cannot save the customizations applied to a genome map for later use. The motivation behind GView is to provide in a single application the best features of all these microbial genome viewing packages: ease-of-use; customizability; interactivity; ability to generate circular and linear maps; ability to read standard sequence file formats; ready incorporation in pipelines and third-party applications; and the ability to generate aesthetically pleasing, publication quality image output. 2 IMPLEMENTATION GView is a complete rewrite of the CGView Java package that preserves the best features of the original application. Genome features and feature labels are displayed in tracks above and below the genome backbone. Graphs can be generated to display feature-level properties (e.g. BLAST atlases) or sequence-level properties (e.g. GC content, GC skew). Optional feature legends can be displayed inline with the map image or as a separate image. Maps can be output in bitmap (PNG, JPG) or vector (SVG) formats for use in web pages and publications, or for additional customization in image editing programs ( Fig. 1 ).
 Fig. 1. ( A ) A circular genome plot for  Roseobacter dentrificans  with tracks showing genes on forward and reverse strands, COG categories, GC skew and automatically generated labels. ( B ) A linear view of a subregion from (A). GView maintains support for pipelining applications via a command-line interface with options to define image dimensions, zoom level and a subregion of the map on which to centre the display. The Application Programming Interface (API) has been substantially reworked to make incorporation in third party Java programs simpler and more straightforward. The new API includes additional support for making selections to the features, tracks and regions of the map. GView also contains substantial new functionality relative to the original CGView. Genome maps can now be generated with circular or linear layouts. A ‘wrapped’ linear layout, similar to that available in DNAPlotter, is planned for a future release. Dynamic panning and zooming allows direct interaction with the map via familiar keyboard and mouse controls, and is useful to quickly navigate to biologically interesting subregions of the map for detailed exploration. To aid in navigating maps at high zoom levels, an optional ‘bird's eye view’ is available that displays and adjusts the current view in the context of the entire genome map. A new feature label placing algorithm keeps label positions stable while zooming in and out of the map ( Been, 2006 ). File compatibility has been improved considerably compared to CGView. GView incorporates the BioJava libraries ( Holland  et al. , 2008 ) to read common genome sequence file formats (EMBL, GFF, Genbank). Backwards compatibility is maintained with the original CGView XML to customize the appearance of the genome map. In addition, GView introduces a new style specification file format—termed ‘Genome Style Sheet’ (GSS)—designed to make genome map customization easier for biologists without computer programming backgrounds and less cumbersome than most GUI-based style customization interfaces. Based on the Cascading Style Sheet specification already in widespread use for applying style information to HTML documents, the GSS specification provides a clean separation of content and style, thus a single GSS file can be used to apply a common style to a set of genome sequences and associated graphs. The GSS format supports the arbitrary grouping of the various map elements into sets, and styles can be quickly applied to that set. By combining this grouping ability with the ability to operate on the feature keys, specifiers, and genome coordinates contained in the genome sequence file (and graph data files), the GSS format provides a highly compact, intuitive and powerful style application ability while retaining the high customizability of the original CGView XML. Funding : Canada's National Microbiology Laboratory. Conflict of Interest : none declared. 
          Supplementary Material 
          
             
                Supplementary Data 
             
             
             
          
       </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Pyicos: a versatile toolkit for the analysis of high-throughput sequencing data</Title>
    <Doi>10.1093/bioinformatics/btr570</Doi>
    <Authors>Althammer Sonja, González-Vallinas Juan, Ballaré Cecilia, Beato Miguel, Eyras Eduardo</Authors>
    <Abstract>Motivation: High-throughput sequencing (HTS) has revolutionized gene regulation studies and is now fundamental for the detection of protein–DNA and protein–RNA binding, as well as for measuring RNA expression. With increasing variety and sequencing depth of HTS datasets, the need for more flexible and memory-efficient tools to analyse them is growing.</Abstract>
    <Body>1 INTRODUCTION Gene expression is regulated through a complex network of protein interactions with RNA and DNA. Recent advances in high-throughput sequencing (HTS) technologies provide a very effective way to obtain information about these interactions at genome-wide level at reasonable cost. ChIP-Seq (chromatin immunoprecipitation followed by HTS) has become the preferred method for current analysis of transcriptional regulation  in vivo , since it provides genome-wide coverage at a high sensitivity and signal-to-noise ratio ( Johnson  et al. , 2007 ;  Robertson  et al. , 2007 ). However, to unravel the complete gene expression network, it is also necessary to systematically identify protein–RNA interactions in cells. HITS-CLIP or CLIP-Seq (cross-linking immunoprecipitation followed by HTS) provides a powerful mean to obtain global information about direct protein–RNA interactions in cells ( Licatalosi  et al. , 2008 ;  Xue  et al. , 2009 ). Additionally, the RNA-Seq experiments provide a way to measure RNA levels at a higher sensitivity and coverage than microarray experiments. Despite the inherent biases, RNA-Seq can overcome some of the array limitations and provides the possibility to measure alternative splicing events ( Pan  et al. , 2008 ;  Wang  et al. , 2008a ) or expression levels ( Pepke  et al. , 2009 ;  Trapnell  et al. , 2010 ), to detect single nucleotide polymorphisms ( Wang  et al. , 2008b ) and to discover novel genes ( Khalil  et al. , 2009 ). Short-read sequencing platforms such as Illumina are very suitable for the study of protein–DNA and protein–RNA interactions, since they can produce enough data to perform accurate quantitative analysis. Furthermore, the resulting reads are of sufficient length to be mapped accurately to a reference sequence, but short enough to define the interaction sites with enough precision. However, not all interactions can be detected at the same coverage. For instance, transcription factors produce generally a clearly localized, or punctuated, signal ( Park, 2009 ), whereas histone marks or RNA polymerase II (RNAPII) produce broad signals, covering more extended regions ( Park, 2009 ). Most of the tools published for the analysis of ChIP-Seq data focus on punctuated signals ( Fejes  et al. , 2008 ;  Nix  et al. , 2008 ;  Park, 2009 ;  Pepke  et al. , 2009 ;  Zhang  et al. , 2008 ), with only few methods devoted to analyse broad signals ( Shin  et al. , 2009 ;  Zang  et al. , 2009 ). However, these methods are not readily applicable to other HTS datatypes, like CLIP-Seq or RNA-Seq, making it necessary to incorporate additional tools to perform gene regulation studies, which generally involve multiple HTS experiments. The increase in variety and sequencing depth of such experiments, as well as the growing need for memory-efficient tools that can be applied to various datatypes, motivated us to develop  Pyicos . In this article, we describe  Pyicos , a versatile toolkit operating on short HTS reads that have already been mapped to a reference coordinate system, like a genome or a transcriptome. We demonstrate its accuracy on punctuated ChIP-Seq data comparing it to MACS ( Zhang  et al. , 2008 ), FindPeaks ( Fejes  et al. , 2008 ) and USeq ( Nix  et al. , 2008 ), using published ChIP-Seq datasets. We also describe the accuracy of  Pyicos  enrichment analysis (EA) to detect differentially expressed (DE) genes from RNA-Seq data by comparing it with the Bioconductor packages DEGseq ( Wang  et al. , 2010 ), DESeq ( Anders and Huber, 2010 ) and edgeR ( Robinson  et al. , 2010 ) using published RNA-Seq data and microarray experiments on the same samples. Furthermore, we illustrate the flexibility of  Pyicos  by showing its effectiveness on other HTS datatypes: broad ChIP-Seq signals and CLIP-Seq data. Finally, we also discuss the performance of  Pyicos  in terms of memory usage and CPU time. Pyicos  provides different protocols to define significant signals from ChIP-Seq (both punctuated and broad), CLIP-Seq and RNA-Seq data; and all operations can be applied independently or combined in a protocol file, which makes  Pyicos  useful for the integrative analysis of a wide variety of HTS data. Moreover, the software has a modular architecture that allows re-usability in other Python applications.  Pyicos  thus offers a useful framework to facilitate the analysis of a variety of HTS datatypes, providing the basis for data integration into gene regulation studies. 2 METHODS Pyicos  is written in Python and is available under the GNU General Public License. It can operate from the command line or as a software library within Python programs (tested on GNU/Linux Fedora 12 and MAC OSX). All operations can be used independently and the parameters can be set by the user; thus providing as much flexibility as possible. Additionally,  Pyicos  can be run using protocols files ( Supplementary Fig. S1  and  Supplementary Material ), which allows the user to apply them directly for standard analyses or to design customized workflows. Pyicos  has its own compressed format for peaks as explained in the  Supplementary Material . This is also used as an internal representation for read clusters, and can be used as input and output formats. Furthermore,  Pyicos  can handle other various formats, providing converter capabilities. It can read eland, SAM, BED and wiggle (fixed and variable step); and write to SAM, BED and wiggle (fixed and variable step). Some formats are obligatory for some of the operations, which are specified in the corresponding command help. 2.1 Callpeaks protocol The  callpeaks  protocol is applied to punctuated ChIP-Seq data and starts by optionally removing duplicated reads, i.e. redundant reads in terms of position and strand in a genome. Next, all reads that fall into a set of regions specified by the user, such as satellites and pericentromeric regions, are removed, as they generally lead to false positives (FPs). Reads are extended to the expected initial fragment size, given as input. Alternatively,  Pyicos  can also estimate this extension using a shift correlation analysis ( Zhang  et al. , 2008 ). Next, reads are grouped into read clusters according to genomic overlap. To follow standard nomenclature, for punctuated data we will refer to read clusters as peaks. When a control experiment is available,  callpeaks  normalizes the sample with respect to control and subtracts the profile of the control from that of the sample with nucleotide resolution ( Supplementary Material ). At this stage,  callpeaks  removes two possible artefacts: peaks that are shorter than a certain length (100 nt by default), which may be produced by the subtraction step; and block-like peaks, which may occur when duplicates are kept. This removal is optional to the user.  Pyicos  also includes a split operation, which allows exploring the possibility that some peaks may describe multiple binding sites. Finally,  callpeaks  calculates the significance for a peak to correspond to a real binding site using Poisson analysis applied to three different parameters: the peak height, the number of reads per peak and the number of reads per nucleotide. For each chromosome, a  P -value is calculated independently for each of these parameters. We also define a peak score as a combination of the Poisson  P -value and the peak parameter  P : peak score=[ P  − log 10 (P-value)]/2, for  P  equal to either the peak height or the peak read count. 2.2 Benchmarking of callpeaks We compared  Pyicos  with MACS ( Zhang  et al. , 2008 ) version 1.3.7.1, FindPeaks ( Fejes  et al. , 2008 ) version 3.3 and USeq ( Nix  et al. , 2008 ) version 7.2, using ChIP-Seq datasets for four different factors: the insulator (CTCF) in K562 cells ( ENCODE Consortium, 2011 ), the CCAAT/enhancer-binding protein alpha (CEBPA) in liver cells ( Schmidt  et al. , 2010 ), the neuron-restrictive silencer factor (NRSF) in Jurkat T cells ( Johnson, Mortazavi,  et al. , 2007 ) and the progesterone receptor (PR) in T47D cells ( Vicent  et al. , 2011 ). We have used various measures to evaluate the quality of peak definition on the punctuated ChIP-Seq data. Two of the descriptors of the peak definition used are the peak length and the distance between the peak centre and summit, i.e. the position with the highest read pileup. We further compared the rankings of the peaks provided by each method: the  peak score  ( Pyicos ), the  peak height  (FindPeaks), the value of −10·log 10 ( P -value) (MACS) and the  ranking  provided by USeq. For the accuracy evaluation, we also used the 83 positive and 30 negative regions validated by ChIP-qPCR for NRSF ( Mortazavi  et al. , 2006 ) as done in  Johnson  et al.  (2007 ). Since peaks differ a lot in their extensions ( Supplementary Fig. S2 ), we took a region of 100 nt centred on the summit as the predicted binding region. We classified the peaks from each method as true positives (TPs) and FPs depending on whether the 100 nt long region centred on the summit overlapped a positive or negative validated region, respectively. The true positive rate (TPR) was calculated as TP over the total number of positive regions and the false positive rate (FPR) was calculated as FP over the total number of negative regions. Receiver operating characteristic (ROC) curves were calculated using incremental subsets of peaks along the ranked peaks from each method, with increasing subset sizes of 500 peaks. An additional measure of the quality of the predictions is the fraction of peaks associated to the expected motif ( Ji  et al. , 2008 ;  Zhang  et al. , 2008 ). We thus measured the fraction of peaks with motif occurrence in the sequence underlying the summit, considering 100 nt for NRSF and PR peaks and 200 nt for CEBPA and CTCF peaks. For the motif analysis, we scanned the selected sequences from the peaks using the matrices for NRSF, CTCF, CEBPA and the three available matrices for PR from TRANSFAC ( Knüppel  et al. , 1994 ) (accession numbers: M00256, M01200, M00116, M00954, M00957, M00960) using a custom script. We used as core similarity cut-off 0.99 and as total similarity cut-off 0.85. As the summit of a peak is associated more strongly with the binding motif than the peak centre ( Supplementary Fig. S3 ), we considered it as a suitable reference position. Additionally, we used as fourth descriptor the spatial resolution, i.e. the distance from the summit position to the centre of the detected motif. Finally, for all four ChIP-Seq datasets, we considered the  Pyicos  rankings using both the peak height and read count scores. For further analyses, we selected the score that gives the best motif content in top-scoring peaks: the peak height score for PR and CEBPA, and the read count score for CTCF and NRSF. 2.3 Enrichment protocol Pyicos  incorporates a method to calculate the significance of the enrichment of the signal between two samples based on the comparison with the distribution of enrichment values on the same regions for experimental or theoretical replicas, similarly to the methods MATR and MARS from DEGseq ( Wang  et al. , 2010 ). Using subsets of ∼5% of neighbouring data points along the axis of average densities  A , the enrichment values  M  calculated for genes between RNA-Seq samples for two liver replicas from  Marioni  et al.  (2008 ) follow a normal distribution ( Supplementary Fig. S4 ). Accordingly,  Pyicos  uses a sliding window along the  A  axis and calculates a  Z -score for the enrichment  M  of each region from the comparison of the replicas (details in  Supplementary Material ). The region is assigned to a background window according to proximity: the region is compared with the values of the window which centre is closest to the region in terms of the value of  A . If no replicas are provided, theoretical replicas are created by randomly rearranging the reads into two subsets, taking into account the relative sizes of the original samples. We integrated the TMM normalization ( Robinson and Oshlack, 2010 ) in  Pyicos  EA. The calculation of the TMM factor on our read count lists yielded a value of 0.68 for liver versus kidney and a value of 1.01 for the comparison of the two liver replicas, which agrees with the results reported in  Robinson and Oshlack (2010 ). Pyicos  EA can have read counts or reads per kilobase per million reads (RPKM) ( Mortazavi  et al. , 2008 ) as input; or alternatively, directly the BED files with regions and read coordinates, overcoming the need of an additional tool to count reads. Additionally,  Pyicos  can calculate enriched regions  de novo  by scanning the samples provided, using two configurable parameters, the proximity between two reads to be taken as part of the same region, and the minimum number of reads in the region to be considered. 2.4 Benchmarking of EA To assess the accuracy of  Pyicos  in the prediction of DE genes, we compared it to three other methods, DEGseq ( Wang  et al. , 2010 ), DESeq ( Anders and Huber, 2010 ) and edgeR ( Robinson  et al. , 2010 ); using published datasets for RNA-Seq and equivalent microarray experiments for liver and kidney samples ( Marioni  et al. , 2008 ). First, we mapped the microarray results to the Ensembl annotation ( Flicek  et al. , 2011 ). Then, using the microarray data, we defined a benchmarking set composed of DE and non-DE genes. DE genes were defined to have False Discovery Rate (FDR) &lt;0.001 and an absolute log 2 -fold change &gt;0.5 ( Marioni  et al. , 2008 ), regardless of whether they are up- or down-regulated. Non-DE genes were defined to have an FDR &gt;0.01 and an absolute log 2 -fold change of at most 0.5. This benchmarking set is composed of 6700 DE genes and 7060 non-DE genes. To predict DE genes, we considered for each Ensembl locus the mean of the values for read counts or RPKM for the corresponding Ensembl transcripts in the locus. The RPKM and read count per transcript were calculated with reads falling within the exons of each transcript. A pseudocount of one read per transcript was added to be able to operate with transcripts whose exons did not have any reads in any of the samples. We estimated the TPs and FPs as the number of predicted DE (enriched or depleted) genes that were annotated as DE or non-DE genes in the benchmarking set, respectively. ROC curves were calculated by considering increasing absolute values of the  Z -score or −log10( P -value) for the predictions for each method. We also calculated precision–recall curves along  Z -score thresholds, where precision is the ratio of TP over the number of predicted cases and recall is the same as the TPR. 2.5 Enrichment on ChIP-Seq data We performed EA on broad ChIP-Seq signals using ENCODE data for H3K36me3 and RNAPII, and correlated the results to the enrichment of RNA-Seq data, for the K562 and NHEK cells ( ENCODE Consortium, 2011 ). The calculation of significantly enriched or depleted regions was based on RPKM values. Replicas 1 and 2 from K562 and replica 1 from NHEK were used to calculate enrichment on three different types of regions from the Ensembl annotation ( Flicek  et al. , 2011 ): for H3K36me3 we used the transcript-body, spanning from the transcription start site (TSS) to the transcription termination site (TTS); for RNAPII we used a window of 4 kb around the TSS; and the transcript exons for RNA-Seq, as before. Significant regions were defined as those with an absolute  Z -score &gt;10. We calculated the Pearson's correlation coefficient of  Z -scores from significant regions between H3K36me3 and RNA-Seq, as well as between RNAPII and RNA-Seq. 2.6 Memory usage and running time benchmarking To test the memory usage and running time of the  callpeaks  protocol, we used ChIP-Seq data for the human transcription factor CEBPA ( Schmidt  et al. , 2010 ). To simulate files of different sizes, we first pooled together all available reads and took random subsets with an increasing size step of 3 million reads, up to 30 million reads, separately for sample and control. In each run, we subsampled two random sets of equal size from the ChIP-Seq reads and from control, using steps of 3 million reads. In order to test the performance of  Pyicos  EA, we compared it to DEGSeq and to a combination of BEDTools ( Quinlan and Hall, 2010 ) with edgeR and DESeq, as these two programs do not accept as input BED files with the regions of interest and the positions of the mapped reads from the samples to be compared. In this way, all four calculations start with the same input: BED files for the samples and the region file. We used the same CEBPA data mentioned before and we calculated the enrichment in the region of 2 kb upstream of the TSS for RefSeq annotated genes. This time we sampled twice per run on the experiment, in order to get a simulated replica, and once for the control. We sampled reads increasing by 3 million reads in each run. 3 RESULTS 3.1 Analysis of punctuated ChIP-Seq data The problem of peak detection entails two challenges: first, to determine significant peaks by distinguishing between real and false binding signal; and secondly, once we have selected the candidate peaks, we need to properly determine the site of interaction. In what follows, we use various methods to establish the accuracy and quality of peak prediction for  Pyicos  and three other published methods for punctuated ChIP-Seq analysis. But first, we assess the effectiveness of  Pyicos  operations for peak definition using ChIP-Seq datasets for NRSF, PR, CTCF and CEBPA. ChIP-Seq analysis often starts by removing duplicated reads, i.e. redundant reads in terms of position and strand in a genome. This has been suggested to eliminate amplification biases ( Zhang  et al. , 2008 ). We observe that keeping duplicates does not produce any improvement on the four sets and that, in fact, the highest fraction of peaks with motifs is achieved when all duplicates are removed ( Supplementary Fig. S5 ). An additional way to remove experimental biases, which may introduce FPs, is the use of a control. When this is available,  callpeaks  subtracts the control from the sample after normalization ( Supplementary Material ). To test whether  Pyicos  subtraction improves peak detection, we generated peaks with and without the subtraction of the control. We observe a great improvement in peak detection for CEBPA ( Fig. 1 a), CTCF and PR ( Supplementary Fig. S6 ), while we only saw a slight increase for the top peaks for NRSF ( Fig. 1 b). Interestingly, the subtraction of the control results into almost no difference in the spatial resolution ( Supplementary Fig. S7 ).
 Fig. 1. Properties of candidate peaks. Cumulative plots of the fraction of  Pyicos  peaks with a motif along the ranking selected by Poisson  P -value cut-offs for peaks with and without subtraction for ( a ) CEBPA and ( b ) NRSF. Cumulative plot of the fraction of peaks with motifs along the ranking for the top 3000 peaks predicted by  Pyicos , MACS, FindPeaks and USeq, for ( c ) PR and ( d ) CTCF. ( e ) Memory performance of the same four methods on the CEBPA ChIP-Seq data. To explore the possibility that some regions of ChIP-Seq signal may describe multiple binding sites,  Pyicos  includes the option to split peaks ( Supplementary Material ). We found that splitting peaks showed improvement only on the CTCF dataset, while spatial resolution was maintained ( Supplementary Fig. S8 ). The protocol  callpeaks  incorporates a Poisson test on the retained peaks to select those that are more likely to be functional ( Section 2 ). Although the Poisson distribution seems not to fit well the background biases of ChIP-Seq experiments ( Zhang  et al. , 2008 ), our analyses indicate that by subtracting the control we remove these biases and Poisson analysis can be applied. Each predicted peak is given a peak-score and a  P -value ( Section 2 ). The peak score gives a ranking for all peaks, whereas the  P -value is used to select a subset of candidate peaks with certain significance. In order to show that the peak score produces an appropriate ranking, we calculated the fraction of peaks containing the expected motif along ranked peaks. We observe that for increasing cut-offs of the  P -value, the fraction of peaks with motifs decreases with the peak score ( Supplementary Fig. 9 ). The selection of candidate peaks by  P -value is therefore consistent with the expected association of significant peaks to functional motifs. The peak score can be calculated using the peak height or read count. On the one hand, we found no differences in peak quality on NRSF and slight differences on CTCF and CEBPA, using peak height or read count. On the other hand, the peak height score for PR results into an increased fraction of peaks with motifs relative to the read count score ( Supplementary Fig. S10 ). 3.2 Comparing peak detection with other methods In order to further assess the quality of  Pyicos  peak calling, we compared  Pyicos  with three other methods: MACS, FindPeaks and USeq. We evaluated the peak definition of the four methods in terms of motif content ( Section 2 ). They all show a similar trend along the ranking, with  Pyicos  and MACS showing higher densities for all ranking positions ( Fig. 1 c and d and  Supplementary Fig. S11 ). Moreover, calculating the spatial resolution ( Section 2 ), the four methods show the largest agreement for the PR dataset, whereas USeq shows the lowest spatial resolution on NRSF, CEBPA and CTCF ( Supplementary Fig. S12 ). We also evaluated the accuracy of peak definition measuring the overlap of the selected peaks with a benchmarking set of positive and negative NRSF binding regions validated by ChIP-qPCR ( Section 2 ). All four methods show high agreement for their top 3000 NRSF peaks in terms of overlap ( Supplementary Table S1 ) and in terms of the pairwise correlations ( Supplementary Table S2 ); hence, we expect they would achieve similar accuracies. Indeed, the four methods perform similarly well with an area under the ROC curve (AUC) between 0.90 and 0.91 ( Table 1 ). We also compared the accuracy of peak definition of the selected peaks by calculating the distances between summit and peak centre as well as the lengths of the peaks. We found that USeq, which produces the shortest peaks ( Supplementary Fig. S2 ), achieves the shortest distance between summit and peak centre, closely followed by  Pyicos  ( Supplementary Fig. S13 ).
 Table 1. AUC for peaks predicted by each tested method, using the ChIP-qPCR-validated NRSF regions for benchmarking Pyicos MACS FindPeaks USeq AUC 0.9 0.9 0.91 0.9 
 Finally, in order to test our software performance, we used ChIP-Seq data for the human transcription factor CEBPA ( Schmidt  et al. , 2010 ) ( Section 2 ). Running all four methods with conditions as similar as possible ( Supplementary Material ), we observed that  Pyicos  is more efficient in memory usage than the other three methods ( Fig. 1 e) and stays competitive in running time ( Supplementary Fig. S14 ). 3.3 EA on RNA-Seq data Methods to measure differential gene expression from RNA-Seq are generally based on EA between samples ( Anders and Huber, 2010 ;  Bullard  et al. , 2010 ;  Oshlack  et al. , 2010 ;  Robinson and Oshlack, 2010 ;  Wang  et al. , 2010 ).  Pyicos  incorporates a method to detect regions of significant enrichment between two samples based on the comparison of the observed enrichment values with those measured between two replicas ( Section 2 ). In order to establish the accuracy of  Pyicos  for detecting DE genes, we compared it first to the methods MATR (with replicas) and MARS (without replicas) from DEGseq ( Wang  et al. , 2010 ), using RNA-Seq data from liver and kidney ( Marioni  et al. , 2008 ). Both methods, DEGseq and  Pyicos , can accept as input read counts or RPKM values for each gene; and they also can estimate a theoretical replica when an experimental one is not provided. We measured the significance of the enrichment of liver over kidney in all genes using four different combinations of input data: replicated count, non-replicated count, replicated RPKM and non-replicated RPKM. Comparing the  Z -scores calculated by  Pyicos  and DEGseq, we observe a high correlation between both methods for all the inputs ( Supplementary Table 3 ). To further estimate the accuracy of  Pyicos  EA, we also ran DESeq ( Anders and Huber, 2010 ) and edgeR ( Robinson  et al. , 2010 ), on the same benchmarking sets, using again the four different input types. Using the microarray experiments from  Marioni  et al.  (2008 ), we created a benchmarking set ( Section 2 ). We found an overall good performance for  Pyicos , with AUCs between 0.791 and 0.813 and a behaviour similar to DEGseq. All four methods agree similarly well with the benchmarking set when replicated read count data is used ( Fig. 2 a). However, we find greater disagreements between methods and lower accuracy for the other three input combinations ( Supplementary Fig. S15 a–c ). Moreover, the precision–recall curves show a similar level of agreement for replicated read count data ( Fig. 2 b) and confirm the good performance of  Pyicos  for all combination of inputs ( Supplementary Fig. S15 d–f ). For DESeq, it was not possible to make calculations on non-integer values, so we restricted the calculations to counts, which resulted in a high AUC when replicas were used. Although edgeR was developed for replicated count data, it was also run with non-replicated count data and performed similarly well.
 Fig. 2. Prediction of DE genes. ( a ) ROC curves for the benchmarking against the microarray data ( Marioni  et al. , 2008 ) for DESeq,  Pyicos , edgeR and DEGseq using read counts and replicated data. ( b ) Precision–recall curves for the benchmarking against microarray data for the same four methods using read counts and replicated data. ( c ) Memory performance of the same four methods on the EA of CEBPA ChIP-Seq dataset on the promoter region ( Section 2 ). DESeq and edgeR are run in combination with BEDTools. ( d ) ROC curves of the different normalization methods: read counts (Counts), TMM-normalized counts (TMM counts), RPKMs and TRPKs, for the microarray benchmarking. ( e ) Absolute differences of the medians from the length distributions of DE and non-DE genes calculate with  Pyicos  using counts, TMM-normalized counts, RPKMs, TRPKs, and the corresponding value from the microarray data. In order to test the memory usage and processing time of  Pyicos  EA calculation, we applied EA from all four methods on the CEBPA dataset to compare the ChIP-Seq sample to the control sample. While  Pyicos  and DEGseq can be run directly on BED files, edgeR and DESeq require count data as input. Hence, we combined these last two methods with BEDTools for the comparison. Since the most time consuming and memory intensive operation is the counting of reads per region, which is done with BEDTools, edgeR and DESeq show the same behaviour. The performance of DEGseq suffers in terms CPU time and especially memory usage. Although  Pyicos  is not as fast as the combination of BEDTools with edgeR or DESeq, it still can handle very large datasets at much lower memory usage ( Supplementary Fig. S16  and  Fig. 2 c). BEDTools algorithm compromises memory usage in exchange of execution time, whereas  Pyicos  algorithm focuses on memory efficiency. 3.4 Normalization of the RNA-Seq data As genes have different lengths and RNA-Seq samples may vary considerably in size, proper normalization of the input data is essential. The trimmed mean of M values (TMM) normalization ( Robinson and Oshlack, 2010 ) aims to correct biases due to differences in samples sizes and expression patterns. TMM normalization on count data correctly places the M median on zero on our gene set ( Supplementary Figs S17a and S17b ). We considered the combination of TMM normalization with RPKM densities, which we hypothesize that it would improve results as it takes into account both gene lengths and sample sizes. We thus define a TRPK density as the TMM-normalized Read Per Kilobase density ( Supplementary Material ). First, as expected, using TRPK we can achieve a correction of the M-median for RPKM ( Supplementary Figs S17c and S17d ). Next, we assessed whether the TMM normalization also results in an improvement of the accuracy calculation using the array results. However, for this particular benchmarking set, we could not observe such an improvement ( Fig. 2 d). Intriguingly, read counts seem to perform slightly better in our benchmarking analysis, as shown above. However, they have been observed to produce length biases in the determination of DE genes ( Oshlack and Wakefield, 2009 ). To explore the correction effect of the various normalizations on the length biases, we calculated the lengths for all DE and non-DE genes predicted by  Pyicos  EA using counts, TMM normalized counts, RPKM and TRPK; and compared them to the lengths of the DE and non-DE genes from the microarray results ( Supplementary Fig. S18 ). We observe a significant difference between the absolute length medians of DE and non-DE genes when normalized counts are not used. However, this difference decreases when we use the RPKM and TMM-normalized read counts, achieving the smallest difference and reaching a similar value to the one found for the microarray when TRPK is used ( Fig. 2 e). 3.5 EA on broad ChIP-Seq data We next provide evidence that  Pyicos  EA approach is also suitable for the analysis of broad ChIP-Seq data when comparing two conditions. Broad ChIP-Seq data does not generally produce clearly delimited regions; hence, one cannot speak of peaks. However, it is interesting to be able to measure how the signal changes between two conditions or cell types in specific regions.  Pyicos  allows the calculation of significant enrichment in predefined regions, e.g. gene-body, promoter regions, etc., which are given as input in a BED file. To show the applicability of  Pyicos  on broad ChIP-Seq data, we tried to reproduce the relation between RNAPII activity and transcription ( Sultan  et al. , 2008 ) and between the H3K36me3 chromatin signal and transcription ( Joshi and Struhl, 2005 ;  Pokholok  et al. , 2005 ). For this, we calculated the correlation of the enrichment of H3K36me3 and RNAPII, with that of RNA-Seq using ENCODE datasets ( ENCODE Consortium, 2011 ). For each datatype, we calculated the EA values of K562 over NHEK, considering the variation between two K562 replicas ( Section 2 ). We selected a cut-off of |Z-score|&gt;10, obtaining 1238 genes with significant changes in RNAPII and RNA-Seq, and 377 genes with significant changes in H3K36me3 and RNA-Seq. For these gene sets, we measured a Pearson's correlation between the enrichment  Z -scores of 0.87 and 0.90, for RNAPII versus RNA-Seq and H3K36me3 versus RNA-Seq, respectively. 3.6 CLIP-Seq data analysis CLIP-Seq reads can be mapped to the genome or to the transcriptome in order to detect RNA binding activity. CLIP experiments normally use a control with an unspecific antibody to check the presence of unspecific binding. However, such a control is not always sequenced; hence, a method to calculate enrichment without a control was proposed ( Xue  et al. , 2009 ;  Yeo  et al. , 2009 ). This method, called modified false discovery rate (modFDR), is included in  Pyicos  ( Supplementary Material ). In order to show  Pyicos  applicability to CLIP-Seq experiments, we reproduced the results from  Xue  et al.  (2009 ), where the interacting RNA sites for the polypyrimidine tract binding protein were mapped to the reference human genome. We used all the mapped reads from this experiment and ran  Pyicos modFDR  operation considering the gene-body of the RefSeq genes as regions to calculate the significant read clusters.  Pyicos  retrieved 92% of the 32 298 regions defined in  Xue  et al.  (2009 ) on the same gene set, which means 91.5% of the reported 10 515 genes having at least one significant cluster ( Fig. 3 a). Moreover, the genes missed by  Pyicos modFDR  were those with a low number of reads: from the 467 genes missed by  Pyicos , 89.7% contained clusters with heights of at most three reads ( Fig. 3 b), suggesting that they are likely FPs. Moreover, there were 898 genes selected by  Pyicos  that were not in the predictions from  Xue  et al.  (2009 ), from which 11.1% had clusters that are &gt;3, indicating that  Pyicos  may still recover real sites that are borderline. Nonetheless, the clusters missed by either case had  P -values closely above the used 0.001 thresholds, indicating that the observed variation was mostly due to the random nature of the background model calculation.
 Fig. 3. Detecting significant clusters in CLIP-Seq. ( a ) Genes with at least one significant cluster using  Pyicos  CLIP-Seq protocol (red) and the results published in  Xue  et al.  (2009 ) (blue). ( b ) Beanplots (Kampstra, 2008) showing the distribution of heights for three subsets of read clusters: the significant clusters exclusively detected in  Xue  et al.  (2009 ) and not by  Pyicos  (Only Xue  et al. ), the significant clusters exclusively found by  Pyicos  (Only  Pyicos ) and all the significant clusters found by  Pyicos  (All  Pyicos ). 4 DISCUSSION We have described  Pyicos , a powerful tool for the analysis of mapped HTS reads.  Pyicos  framework facilitates the analysis of different HTS datatypes. We have described its application to ChIP-Seq, RNA-Seq and CLIP-Seq data using the three corresponding protocols:  callpeaks ,  enrichment  and  clipseq . In  callpeaks , we define a peak score in terms of a Poisson  P -value, which is calculated independently for each chromosome, and in terms of one peak property, either the peak height or read count. The peak score therefore takes into account the differences across chromosomes and the fact that peaks with the same height or read count may have different  P -values depending on the chromosome in which they are located. Using ChIP-Seq data for PR, CTCF, CEBPA and NRSF, we have shown that the peak score provides an appropriate ranking for peaks. Interestingly, using either the peak height or read count can make a difference depending on the dataset, as we found an increase of ∼7% in the fraction of the top 500 PR peaks with motifs, leading to a better peak definition compared with the other methods. We have further shown that the subtraction of the control is effective to increase the fraction of peaks with motif, indicating that it eliminates potential FPs. Pyicos  has the advantage that all operations described are configurable by the user to keep as much flexibility as possible, since not all of them may be applicable to a dataset. For instance, splitting peaks seems to result in improvements only for CTCF. Similarly, although  Pyicos  allows the user to choose the number of tolerated duplicated reads, we found that in all datasets the best peak definition is achieved by removing all duplicates. Furthermore, using various measures, we have compared  callpeaks  with three other methods specifically developed for punctuated ChIP-Seq data: MACS, USeq and FindPeaks. Regarding peak definition, we have found that FindPeaks,  Pyicos  and MACS show very similar spatial resolutions, which are also higher than those for USeq peaks. Furthermore, we observed that peaks ranked by  Pyicos  and MACS show a slightly higher fraction of motif-containing peaks than those ranked by FindPeaks and USeq. Peak detection has been assessed using ChIP-qPCR validated positive and negative regions for NRSF. We found that all methods perform similarly, probably due to high agreement between their generated peaks. The methods with the highest pairwise correlations are  Pyicos , MACS and USeq. FindPeaks showed a lower correlation, probably due to a different handling of the control sample: whereas, FindPeaks compares a peak height with the distribution of peak heights from the control sample, the other three methods compare signal to control locally, using windows (MACS and USeq) or at base pair resolution ( Pyicos ). In summary, our results lead us to conclude that  callpeaks  provides an accurate protocol for peak detection for punctuated ChIP-Seq data. Moreover, due to the more flexible usage of the various operations compared with other methods, it allows to design a customized analysis of the ChIP-Seq data. We have further illustrated  Pyicos  flexibility by applying it to other datatypes. Using data from RNA-Seq and microarray experiments on liver and kidney samples, we have shown that  Pyicos  can recover DE genes with high accuracy and that is in fact comparable to methods specifically designed for differential expression analysis from RNA-Seq. Moreover,  Pyicos  performs well using different inputs: replicated or non-replicated, read counts or RPKM. DEGseq, which can also work with RPKM, shows similar accuracy as  Pyicos  and both correlate well in terms of the predicted  Z -scores. The other two methods, DESeq and edgeR, were not designed to work with RPKM, hence could not be included in the comparison. The fact that  Pyicos  performs well with simulated replicated data presents the advantage of making possible to analyse many of the published datasets that have been produced without replica. It is surprising that the highest accuracy is achieved on count data, since it is known that using counts leads to a length bias in DE gene detection. This is probably because the genes selected from the microarray for benchmarking are not much affected by the length bias, since the accuracy hardly changes when using RPKM. This also suggests that RPKM alone does not provide the optimal normalization method. However, using a new density definition, TRPK, which combines RPKM with the TMM normalization, the length differences between DE and non-DE genes are reduced to a level close to that of the microarray. Nonetheless, the TMM normalization is based on the assumption that the majority of regions tested do not change significantly, which might not hold true for some pairs of samples or for certain sets of regions. Accordingly,  Pyicos  implements this as an option to the user. The flexibility of  Pyicos  is further demonstrated by applying the EA protocol to broad ChIP-Seq data. In particular, using ENCODE ChIP-Seq data for H3K36me3 and RNAPII from the cell lines K562 and NHEK, we obtain a high correlation for the  Z -scores of these signals when compared with the enrichment  Z -scores for RNA-Seq between the same cell lines. Thus, the EA of  Pyicos  using RPKM provides a tool to analyse broad ChIP-Seq data of various sorts, which would otherwise require a combination of approaches to be analysed ( Young  et al. , 2011 ). A further advantage is that  Pyicos  can directly accept BED files with mapped reads and regions of interest, unlike DESeq and edgeR.  Pyicos  can also calculate enriched regions  de novo  genome wide, using the reads from two experiments that are overlapping or sufficiently close in position. This is particularly useful for the analysis of signals for which the user does not know where to expect the enrichment relative to genome annotations, like enhancer elements. We have further shown that  Pyicos  can also be used to process CLIP-Seq data without a control. We expect that many of the basic  Pyicos  operations could be applicable to other datatypes and could possibly be combined to generate new analysis protocols. Some of the operations described can become impracticable for some analysis tools due to the amount of reads produced by a single HTS experiment nowadays. The bottleneck of HTS data analysis mostly lies in the memory usage of the software and in the storage and retrieval of data. Indeed, HTS data generation seems to be outpacing the improvements in CPU and disk storage ( Kahn, 2011 ). Moreover, HTS data has become ubiquitous in genomic research; hence, we should aim to provide software that can be adapted to the available computing resources and, in particular, that can run on the average desktop computer. For these reasons, we developed  Pyicos  minimizing RAM usage and maintaining reasonable CPU time usage.  Pyicos callpeaks  protocol for ChIP-Seq data outperforms the other tested methods concerning memory usage while it stays competitive in running time. FindPeaks and USeq algorithms load entire files in memory, allowing for better time performance. However, this is only practical on a computer with enough RAM. For example, an experiment with 100 million reads using as input a BED file, would require &gt;4 GB available of RAM. For EA, the main bottleneck lies in the calculation of read counts or RPKM on the input regions. Although  Pyicos  is not as fast as BEDTools combined with DESeq or edgeR, its memory performance is far superior, allowing the handling of very large datasets. As made patent in the last few years, read files are increasing enormously in size with the development of the technology, making memory usage a critical feature in HTS analysis software. We conclude that the added value of having a modular tool is not at the cost of accuracy or performance; hence,  Pyicos  provides a useful framework for the analysis and integration of heterogeneous HTS data. Finally, as  Pyicos  is open source, we encourage the addition of new operations in order to combine them with already existing ones and possibly to create new analysis protocols. 
          Supplementary Material 
          
             
                Supplementary Data 
             
             
             
          
       </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>CGAS: comparative genomic analysis server</Title>
    <Doi>10.1093/bioinformatics/btp086</Doi>
    <Authors>Itoh Masumi, Watanabe Hidemi</Authors>
    <Abstract>Summary: Comparative approach is one of the most essential methods for extracting functional and evolutionary information from genomic sequences. So far, a number of sequence comparison tools have been developed, and most are either for on-site use, requiring program installation but providing a wide variety of analyses, or for the online search of user's sequences against given databases on a server. We newly devised an Asynchronous JavaScript and XML (Ajax)-based system for comparative genomic analyses, CGAS, with highly interactive interface within a browser, requiring no software installation. The current version, CGAS version 1, provides functionality for viewing similarity relationships between user's sequences, including a multiple dot plot between sequences with their annotation information. The scrollbar-less ‘draggable’ interface of CGAS is implemented with Google Maps API version 2. The annotation information associated with the genomic sequences compared is synchronously displayed with the comparison view. The multiple-comparison viewer is one of the unique functionalities of this system to allow the users to compare the differences between different pairs of sequences. In this viewer, the system tells orthologous correspondences between the sequences compared interactively. This web-based tool is platform-independent and will provide biologists having no computational skills with opportunities to analyze their own data without software installation and customization of the computer system.</Abstract>
    <Body>1 INTRODUCTION A number of comparative studies of genomic data have shown that sequence comparison is invaluable for obtaining functional and evolutionary information in genomic data. Conservation patterns along genome sequences are well studied with comparative genomics tools, including MultiPipMaker (Schwartz  et al. ,  2003 ) and the UCSC genome browser (Karolchik  et al. ,  2008 ). However, these tools do not directly show the collinearity between the sequences compared, although it is an important information source about genome evolution, such as duplication, inversion, translocation, insertion and deletion. Collinearity is also known as an evolutionary signal of orthology (von Mering  et al. ,  2003 ). To view the collinearity between sequences, we usually use a 2D representation, called dot plot or Harr plot, or a synteny map. A number of tools to draw a dot plot have been developed (e.g. Abril  et al. ,  2003 ; Schwartz  et al. ,  2000 ; Sonnhammer and Durbin,  1995 ; Wilson  et al. ,  2001 ), but except a few, e.g. Dotter (Sonnhammer and Durbin,  1995 ), they do not provide interactive zooming, windowing and/or scrolling functionality. These functions are very useful when we explore a huge dot plot between big sequences like eukaryote chromosomes to identify evolutionary changes. For identification of such changes, the annotation data of and other associating data with the sequences involved in the changes may be a good help. Some interactive dot plot tools show annotation data of the sequences (Engels  et al. ,  2006 ; Jareborg and Durbin,  2000 ; Wilson  et al. ,  2001 ), but they are on-site programs, i.e. the users should install them on the platforms that the developers specify. Here, we newly developed a comparative genomic analysis server, CGAS for viewing similarity relationships between user's sequences with their annotation information. The Asynchronous JavaScript and XML or Ajax technology is integrated into this system to provide a dynamic and user-intuitive interface, which allows the users to interactively compare sequences, viewing the annotation data of the sequences. This system does not require installation of any programs but just a web browser that understands JavaScript. 2 FEATURES AND USAGE The main page of the CGAS consists of the interactive comparison and annotation panes ( Fig. 1 A). Users may upload their BLAST outputs (Altschul  et al. ,  1997 ) to the server to view the alignments between the query and the subject sequences. The server automatically generates a dot plot of the alignments. The comparison pane displays the hits between the query and a subject in a 2D representation. Annotation panes are placed along the left and the upper edges of the comparison pane. They display the annotation data of the query and the subject graphically. User-defined annotation data may be uploaded in the gbk/gbff ( http://www.ncbi.nlm.nih.gov/Sitemap/samplerecord.html ) or the gff data format ( http://www.sequenceontology.org/gff3.shtml ). In the case where the sequence names in the uploaded alignment data are recognized as those defined at NCBI, the system automatically fetches the annotation data from NCBI through the TogoWS system ( http://togows.dbcls.jp/ ).
 Fig. 1. The main page of CGAS, consisting with comparison and annotation panes. This example shows a comparison of the regions including the opsin loci in the X chromosomes of human and chimpanzee ( A : single mode) and macaque ( B : dual mode). Red and blue dots/lines in the comparison pane represent the BLAST hits in the same/different directions between the query and the subject. Users can move about the entire dot plot by dragging the comparison pane and zoom it in/out with the mouse wheel in the same manner as the Google Maps. The navigation controls, placed at the upper-left corner of the comparison pane, may be used to navigate. Diagonal movements are possible using the navigation controls. Thanks to the Google Maps API, the annotation panes are updated synchronously with the comparison pane. The names of the annotated objects are shown in a small box, which can be expanded by moving the mouse cursor on it and shows annotation details along with the link to the original data at NCBI. If the mouse cursor is within the comparison pane, the position of the cursor in each of the query and the subject(s) is indicated by a red thin line in the annotation pane with the position information in base pairs. The current version of CGAS is capable of displaying two subjects side by side ( Fig. 1 B). Two comparison panes are synchronously aligned to the query. This functionality allows the users to compare the differences in collinearity between the subjects. CGAS provides a unique function to align the query and the subject annotation panes based on the putative orthology relationships between the sequences compared. If the user double-clicks in the subject or query panes, the most similar region of the clicked region in the query or subject sequence is centered in the pane, and the comparison pane is updated accordingly. This is useful for tracking the significant hits. The uploaded data are indexed with user's login ID and retained in the server for a week after the last access or until the user deletes them. 3 IMPLEMENTATION The CGAS is a web server with the Ruby on Rails framework with JavaScript. The interface within the dot plot viewer and the annotation viewer was implemented using Google Maps API version 2. Interpretation of biological data and interactive entry fetching from public database were achieved by the BioRuby library ( http://bioruby.org/ ) and the TogoWS system. The documentation of CGAS is provided at  http://cgas.ist.hokudai.ac.jp/help/ . </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>TiSAn: estimating tissue-specific effects of coding and non-coding variants</Title>
    <Doi>10.1093/bioinformatics/bty301</Doi>
    <Authors>Vervier Kévin, Michaelson Jacob J, Berger Bonnie</Authors>
    <Abstract/>
    <Body>1 Introduction Whole genome sequencing (WGS) is assuming a role as the technology of choice for an increasing number of genetic studies. The vast majority of the information yielded by WGS resides in non-coding and poorly-characterized regions of the genome. Recent work in annotating non-coding variation has shown that multiple levels of information—integrated using machine learning algorithms—are required to capture the diverse regulatory potentials in these regions ( Ionita-Laza  et al. , 2016 ;  Kellis  et al. , 2014 ;  Kircher  et al. , 2014 ;  Quang  et al. , 2015 ). However, current state-of-the-art variant annotation methods predict generic pathogenicity, largely avoiding the question of which tissues, organs and systems are most likely susceptible to a particular genetic variation. Projects such as the Genotype-Tissue Expression (GTEx) repository ( GTEx Consortium, 2015 ) and the NIH Roadmap Epigenomics (RME) Mapping Consortium ( Bernstein  et al. , 2010 ), provide clear evidence that a given variant will not necessarily have the same impact on gene expression in different tissues or cell types. Recently proposed approaches, such as GenoSkyline ( Lu  et al. , 2016 ), have employed cross-tissue methylation levels to annotate genetic variations. However, such methods are limited because they were trained using only data uniformly collected across a wide variety of tissues, omitting potentially informative features derived from databases providing unique, tissue-specific information. Such approaches emphasize performance over many tissues, rather than a specific tissue. In this work, we introduce Tissue-Specific Annotation (TiSAn), which combines the power of supervised machine learning with tissue-specific annotations, including genomics, transcriptomics and epigenomics ( http://github.com/kevinVervier/TiSAn  for the latest version). We describe a general statistical learning framework in which researchers can derive a nucleotide-resolution score for the tissues they focus on. We foresee TiSAn’s application as a complement for existing scores that measure generic variant deleteriousness, like CADD or DANN. This combination makes it possible to infer which variants may be most damaging in the context of the tissue of interest. As a proof of principle, we apply our methodology to two human tissues—brain and heart—and we have made available pre-computed genome-wide scores for these tissues. 2 Materials and methods 2.1 Training set definition We identified multiple sources of training examples with respect to a given tissue  T . For deriving a genome-wide predictor, the training set needed to cover both coding and non-coding loci, but also loci related (positive examples) and unrelated to  T  (negative examples). Two types of public databases were used to derive training sets:
 Genotype array loci : Disease-related loci were identified in consortium-developed arrays, designed for targeting specific disorders, such as MetaboChip ( Voight  et al. , 2012 ) for cardiovascular diseases or Illumina Infinium PsychArray Beadchip for psychiatric disorders. Genotyped single nucleotide polymorphisms (SNP) were sorted by confidence level into three classes. First, replication SNPs were selected as follow-up for top association signals from the largest available genome-wide association studies (GWAS) meta-analysis, and contain tissue-related variants (positive examples). Then, fine-mapping SNPs represent associated haplotypes identified in preliminary studies. Finally, backbone SNPs include sex-specific markers, Major histocompatibility complex region and other population variation, which we consider as negative examples if they meet a minimal CADD threshold described in Section 2.9. To ensure the highest quality of positive labels, we only considered the first category in our analysis, as positive examples. Large intergenic non-coding RNAs : Large intergenic non-coding RNAs (lincRNAs) represent a well-studied group of non-coding elements known to regulate gene transcription in a tissue-specific manner ( Popadin  et al. , 2013 ). Databases such as LincSNP ( Ning  et al. , 2014 ), contain disease-related variants that occur in lincRNA loci. This is an important source of non-coding training examples, since non-coding variants are generally less functionally characterized than coding variants. After defining a list of tissue-related disorders, we divided this database into two subsets: one related to tissue  T  (positive examples) and one containing background variants (negative examples), i.e. randomly sampled deleterious variants not related to the tissue at hand. This way, we enriched the training set with putatively functional non-coding positions. We endeavored to ease the training set selection process for users interested in training their own models, by developing a companion tool called TiSAn-build ( http://github.com/kevinVervier/TiSAn/tree/master/TiSAn-build ). Using this Shiny ( Chang  et al. , 2017 ) graphical user interface (GUI), the user can select training positions based on a list of keywords and disease names ( Supplementary Fig. S22 ). The number and breakdown of training examples used for TiSAn brain and heart models can be found in  Supplementary Table S1 . After selection of positive and negative examples, we ensured there was no selection bias between the two groups that would lead to poor generalizability of the classifier. For both brain and heart models, half of the training examples were found within the proximity of a gene (±10 kb) and the remaining 50% were in intergenic regions, with no detectable difference in the spatial distribution between positive and negative examples. The variants found in genic regions covered 7219 and 7625 different genes in the brain and heart training sets, respectively. No significant difference in terms of minor allele frequency (MAF) distribution between positive and negative examples was observed (brain: Wilcoxon test  P  = 1, average MAF = 0.14, heart: Wilcoxon test  P  = 0.8, average MAF = 0.4). No significant difference was observed in terms of variant pathogenicity (CADD Phred score) distribution between positive and negative examples (brain: Student’s test  P  = 1, average CADD = 3.5, heart: Student’s test  P  = 0.66, average CADD = 4.1). 2.2 Feature extraction We represent each genomic position in a functional space defined by hundreds of different annotations. In the following, we describe how such signal can be extracted using publicly available datasets, and provide a comprehensive list of variables used in this study in  Supplementary Table S2 . Most of the feature extraction process can be extended to a wide variety of tissues ( Supplementary Table S3 ), and we have developed a companion tool, called TiSAn-build, aiding users in extracting features for their own models. Users can assemble tissue-specific signal from eQTL ( Supplementary Fig. S19 ), DNA methylation ( Supplementary Fig. S20 ) or literature databases ( Supplementary Fig. S21 ) which the machine learning algorithm uses as reference annotations to extract features on training examples. 2.2.1 Motifs in short nucleotide sequences Nucleotide frequencies are linked to overall regulatory activity (G/C content), and patterns in nucleotide  k -mers are the basis of transcription factor binding site detection ( Zhou and Troyanskaya, 2015 ). Specific patterns have been recently identified to be tissue-specific ( Zhong  et al. , 2013 ), and we incorporate this information by computing frequencies for all  n -nucleotides [ n ∈ ( 1 , 2 , 3 , 4 ) ], found within a ±500 base pair neighborhood around a given genomic position  x . 2.2.2 Distance to annotations We model the impact of a known annotation on a given position  x , as a decreasing function of their distance. The comparison between four different distributions found that the Weibull distribution was the most stable across the considered features ( Supplementary Table S4 ). Therefore, in the following paragraphs, the distance is measured as:
 d ( x , a n n o ) = ( β α ) ( | x − a n n o | α ) β − 1   exp ( − ( | x − a n n o | α ) β ) , 
where  anno  refers to a known annotation position,  α  is a scale factor and  β  is a shape parameter. Parameter fitting was performed separately for each annotation, using the  MASS  R package ( Venables and Ripley, 2002 ). 2.2.3 Expression quantitative trait loci Links between disease traits and tissue-specific gene expression have been reported in studies using the GTEx dataset ( GTEx Consortium, 2015 ). For each genomic location, we extract features based on a position’s distance to known eQTL for tissue  T , as well as for other tissues. The Weibull distribution was used for modeling the minimal distance to a GTEx eQTL ( Supplementary Fig. S1 ). We also derive Boolean features for whether the genomic position is at the exact location of a GTEx eQTL, which puts more weight on known eQTL. 2.2.4 Literature mining for tissue-related genes Although gene expression shows variation across tissues, definitive lists of tissue-specific genes are limited. It has already been shown that text mining techniques may help to extract relationships between genes and disease traits ( Liu  et al. , 2015 ). Therefore, we adapt such methods to identify genes reported to be associated with tissue  T , in the PubMed database (May 2016  gene2ID  database). Genes co-cited at least 3 times in publication’s title/abstract with the tissue name (e.g. ‘brain’ or ‘heart’) are kept as tissue-related genes ( Collier  et al. , 2015 ). For each genomic location, we extract features based on how close the position is to tissue-related genes. The Weibull distribution is used for modeling the minimal distance to a gene ( Supplementary Fig. S2 ). We also derive a Boolean feature for whether or not the genomic position  x  is within the range of a tissue-related gene, which puts more weight on positions in regions well-supported by literature for its association with the tissue of interest. When training the brain model under the Weibull distribution assumption, we observed that around 1000 tissue-related genes represented enough genome coverage to derive a feature based on the proximity between a locus  x  and a gene. Lists of genes identified as tissue-related are provided in  Supplementary Data  S1 (1185 brain genes) and  Supplementary Data  S2 (1721 heart genes). 2.2.5 Differentially methylated regions Epigenomics, and, in particular, methylation profiles, have been integrated to explain tissue-specific regulatory mechanisms ( Miller  et al. , 2016 ). We use the Weibull distribution approach for modeling the minimal distance to a methylated region found in the RME database ( Supplementary Fig. S3 ). If the considered position  x  belongs to a methylated region characterized in the RME project, we also note the average methylation level for samples from the tissue of interest  T , and other samples. 2.2.6 Additional tissue-specific resources In contrast to approaches that rely mostly on RME and/or GTEx, we also consider tissue-specific datasets made available by research projects focusing on a single tissue. For the brain model, we integrate developmentally differentially methylated positions (dDMPs) ( Spiers  et al. , 2015 ) found in the fetal brain, and derive features based on the distance between and the closest dDMP. For the heart model, we use the Heart Enhancer Compendium database ( Dickel  et al. , 2016 ) to identify heart development candidates, and we use the distance to the closest fetal development enhancer as a training feature for TiSAn heart model. 2.3 Supervised machine learning model training Considering the aforementioned training sets, we utilized machine learning approaches, such as logistic regression ( glm  R package), linear support vector machine ( libLineaR  R package) and random forest ( randomForest  R package) ( Lischke  et al. , 1998 ), and compared them based on their 10-fold cross-validated performance (herein, area under the receiver operating characteristic curve, AUC), and selected the random forest algorithm to train the final model ( Supplementary Table S5 ). Using cross-validation, we optimize random forest hyper-parameters and build a model with 1000 trees, where each tree is trained on a bootstrapped sample of the training set, and all the 360 variables are considered (instead of the default value  360 ∼ 19 ). The leaf node size is set, by default, as equal to one, meaning that each root-leaf path is predicting a single example. 2.4 From class probability to rescaled odds ratio Current variant annotation approaches often consider the raw class probability as their functional score, requiring an additional tuning/thresholding step from the user. Herein, we propose to rescale the classifier output into a ready-to-use score. First, we define an optimal cutoff value on the class probability ( Supplementary Figs S4A and S5A ), as the smallest value which reaches a false discovery rate (FDR) of 10%. For instance, this threshold is equal to 0.48 for the brain model and to 0.67 for the heart model. Using FDR as a threshold also accounts for potential mislabeling in the training data, especially in the background/negative examples set. We then rescale the filtered probability to a score between 0 and 1, using the formula:
 max ( 0 , 1 − t h r e s h − P ( x ∉ t i s s u e ) P ( x ∈ t i s s u e ) ) .   The main advantage here is to standardize predictive models, and push non-tissue-related loci to a score strictly equal to 0 ( Supplementary Figs S4B and S5B  and S18). 2.5 Reference genome Analyses performed in this study used the hg19 reference genome. 2.6 GTEx transcriptome data for 44 tissues Median gene expression data were downloaded from the GTEx portal (version 6), and are measured using Reads Per Kilobase, per Million mapped reads (RPKM). We used the median RPKM, as it measures expression at the entire gene scale, and allowed us to correlate expression with a gene-level median TiSAn score. We acknowledge that the use of raw expression data may contain biological and technical confounders. We therefore limited the analysis to the 17 803 genes that were used in the original GTEx eQTL study, after being fully processed, normalized and filtered. 2.7 ENIGMA genome-wide association for brain regions volume Genome-wide association summary statistics from the ENIGMA2 study ( Hibar  et al. , 2015 ) were downloaded from  www.enigma.ini.usc.edu . For the four considered brain regions (accumbens, amygdala, caudate and hippocampus), we annotated each of the 974 045 intergenic loci with a distance to the closest gene of at least 10 000 bp with Tisan-brain. 2.8 GWAS prioritization in coronary artery disease (CAD-GWAS) cohort CARDIoGRAM consortium GWAS meta-analysis summary statistics for 8 443 810 SNPs were downloaded from  http://www.cardiogramplusc4d.org . We carefully removed positions that were found both in the TiSAn training and the GWAS, to avoid an overly-optimistic estimation of performance. A comparison between the TiSAn score and association strength ( Fig. 4 ) was obtained by binning coronary artery disease (CAD)-GWAS SNPs in 100 percentile bins on reported GWAS  P -value. Average score gain is measured for top 1% variants (Bin 1) by comparing their scores against the remaining 99% of the SNPs. Then the top 2% variants (Bins 1 and 2) are compared against the remaining 98%, and so on, until merging all the data, which corresponds to the comparison between 99% of the data against the 1% variants with the highest  P -value. We derived confidence intervals for both TiSAn and GenoSkyline by random permutations on the GWAS  P -values. When ranking variants that might have an impact on the trait, we filtered variants not predicted as tissue-related by either TiSAn (zero score), or by GenoSkyline (score &lt;0.15). 2.9 Variant enrichment in the vicinity of ASD genes Variants found in 960 Simons Simplex Collection (SSC) individuals—including probands and parents—were filtered based on their pathogenicity using CADD score. We estimated different threshold values for coding (&gt;15) and non-coding (&gt;10.7) variants to account for systematic bias in CADD predictions. Those values correspond to the top 10 percentile found in the 1000 Genomes data. We also focused the analysis on variants found in ±50 000 bp windows around well-supported autism spectrum disorder (ASD) genes with more than 20 citations in the June 2016 SFARI gene list at  http://gene.sfari.org/autdb/HG_Home.do  ( Supplementary Table S6 ). The same filters were applied to variants found in 1000 Genomes (1KG) European ancestry population (Phase 3). We carefully removed positions that were found in both the TiSAn training and the sequencing variant call sets (SSC and 1KG), to avoid an overly optimistic estimation of performance. We also controlled for potential linkage disequilibrium (LD) between training data and validation variants. LD correlation (R-squared) was estimated from 1000 Genomes European population (CEU) allele frequencies using SNAP proxy search ( www.archive.broadinstitute.org/mpg/snap/ldsearch.php ). For each validation locus (positive or negative), we considered the maximal correlation value with the training set loci. The distributions obtained for positive and negative examples were compared, and correlation for positive examples was not found to be significantly higher (Wilcoxon rank test:  P  = 0.9971, average LD positive = 0.92, average LD negative = 0.93). Coding and non-coding variants were separated based on their RefSeq ( O’Leary  et al. , 2016 ) function annotation. The relative gain in average score ( Fig. 3A and B ) was calculated by computing the difference between average functional score in SSC and in 1KG for coding and non-coding variants. Cumulative score enrichment for SSC over 1KG variants ( Fig. 3C ) was obtained by binning both SSC and 1KG variants based on their functional score, in 5 percentile groups. Then, the proportion of SSC SNPs and 1KG variants present in each bin was computed, and summed in a cumulative way, from the top 5% bin to all the data (from left to right on the figure). 2.10 Transcription factor binding site enrichment in tissue and cell type The ENCODE project provides a large repository for Transcription factor binding site (TFBS) locations in various cell type contexts. Here, we assembled two databases, both available as UCSC Genome Browser tracks,  factorbookMotifPos  (from factorbookMotif track), which contains the location of more than 2 million TFBS across the genome, and  EncodeRegTfbsClustered  (ENCODE Regulation ‘Txn Factor’ track), which provides information regarding the cell types where TFBS were observed. Overlapping the two databases resulted in 1 514 086 unique TFBS found binding 53 different TF structural families. For each of those TFBS, we expanded their location using a ±500 base pair window centered on the site, and TiSAn heart and brain score profiles were extracted from these windows. Scores were centered and scaled around the center value and show the actual score enrichment along the window. An average profile was computed for all TF structural families and cell types. 2.11 TiSAn use case The main application for TiSAn involves the annotation of large sets of variants, and we recommend the use of scalable tools, such as  vcfanno  ( www.github.com/brentp/vcfanno ) to accomplish this annotation. On the other hand, examination of one or a handful of loci can also be helpful in gaining insights about what features are driving a prediction. Therefore, we developed a GUI tool called TiSAn-view, where features extracted for a single locus are displayed ( Supplementary Fig. S6 ). Users simply need to upload a list of genetic loci (e.g. in bed format), and choose the TiSAn to use. Tutorial and vignettes are available on  http://github.com/kevinVervier/TiSAn . 2.12 Software availability 
 Genome-wide TiSAn score databases (brain and heart) are available in bed format at  http://www.flamingo.psychiatry.uiowa.edu/TiSAn . We have also made publicly available the two companion tools (TiSAn-build and TiSAn-view) at  http://github.com/kevinVervier/TiSAn . GenoSkyline approach: we downloaded brain and heart models from ( www.genocanyon.med.yale.edu ), in November 2016. CADD: Deleteriousness annotations were performed using CADD v1.3 (current version) at  http://cadd.gs.washington.edu DANN annotations were downloaded in April 2015 (original version) at  http://cbcl.ics.uci.edu/public_data/DANN 
 3 Results 3.1 Machine learning for predicting tissue-specific functional annotation The design of TiSAn models is outlined in  Figure 1  (details in Section 2). Taking advantage of publicly available datasets ( GTEx Consortium, 2015 ;  Bernstein  et al. , 2010 ), we extracted more than 350 different genome-wide variables that were used to describe two large sets of disease-related loci. Training a supervised machine learning model requires positive and negative examples: herein, positive examples were nucleotide positions that had been previously linked to a tissue-specific disease, and negative examples were variants that had no established link to the tissue-specific disease in question. Predictive models were trained on the labeled datasets and optimized to achieve high discrimination of tissue-specific loci ( Supplementary Figs S4 and S5 ). Herein, a score equal to 1 indicates a position strongly associated with the tissue, whereas a score of 0 means no association at all; such a position is usually discarded in subsequent analysis.
 Fig. 1. TiSAn framework overview. Each nucleotide position in the genome is annotated with multiple types of genome-scale information, such as sequence content, methylation level, proximity to genes, etc. (see Section 2). This information is extracted for training sets, comprised of deleterious variants with or without an association with the tissue of interest. Using supervised machine learning, specifically a Random Forest (RF), a predictive model combines each feature based on its ability to predict whether a position will be functionally associated with the tissue of interest. Model output consists in a tissue-specific functional score ranging from no functional relevance to the tissue (0) and strong functional relevance to the tissue (1). This score can then be used, for instance, to filter down large lists of candidate variants for further investigation, or to isolate the contribution of different tissues to a complex trait 3.1.1 Impact of algorithm hyper-parameters 
 Feature group importance . The cross-validation procedure indicated a random forest as the most accurate approach for the tissue-specificity task. Interestingly, this analysis suggested that using all the features when considering potential splits leads to superior performance, with the optimal  mtry  parameter being equal to the total number of features. In order to identify the most informative features, we compared the contribution of each feature group to the model’s accuracy, by training suboptimal models with features restricted to a single group. Notably, we found that eQTL are the most informative features for both brain and heart models, whereas DNA methylation shows limited performance ( Supplementary Fig. S7 ). Additional tissue-specific epigenome data (e.g. H3k4me1) were found to have a negligible impact on performance ( Supplementary Fig. S8 ), suggesting that the signal in RME consolidated epigenomes present in TiSAn models is sufficient. 
 Feature interactions . We identified combinations of features that were likely to increase model accuracy using an iterative Random Forest scheme ( Basu  et al. , 2018 ). For the brain model, we observed a strong interaction between  GC  content and proximity with brain genes. In the heart model, interactions between heart eQTL and heart enhancers were the main contributors. A complete list of stable interactions is given in  Supplementary Table S7 . 
 Training set and learning curve . The number of training examples also has an impact on model performance ( Supplementary Fig. S9 ). We found that the current training set size for both brain and heart models achieves the optimal trade-off between required labeled examples and model predictive power. 3.1.2 Region-based analysis In addition to cross-validation performance, we assessed our models’ generalization by holding out two genome regions from the training sets, and confirmed there was no LD correlation between the training set and the tested regions. The 16p11 region (from position 28M to 31M) has been extensively studied in neurodevelopmental disorders, such as ASD ( Weiss  et al. , 2008 ). The 9p21 region (from position 19.9M to 25.4M) has been associated with cardiovascular disease ( Gong  et al. , 2014 ). We extracted the TiSan brain and heart scores within the two regions at a 1 kb resolution, and reported tissue-specific enrichment in  Supplementary Figure S10 . As expected, the brain score is significantly higher than the heart score in the 16p11 region (paired Student’s  P = 1.38 × 10 − 12 ), whereas the heart score is significantly higher in the 9p21 region (paired Student’s  P = 1.21 × 10 − 127 ). Finally, we also used the held-out labeled loci in those regions to estimate the TiSAn models’ sensitivity. Predictions with the brain model showed an AUC of 0.99 for the 16p11 region, and predictions with the heart model showed an AUC of 0.86 for the 9p21 region. These values were comparable to the observed cross-validated AUC for the corresponding models, supporting TiSAn’s generalizability. 3.2 Tissue signal detection in non-disease traits Here we demonstrate that the TiSAn score can identify functional, tissue-specific variants that are not necessarily disease-related. First, we considered 44 tissues characterized by RNA-seq, from the GTEx project, including 2 heart tissues (atrial appendage and left ventricle), and 10 brain tissues. We observed correlation between the measured gene expression and the average corresponding TiSAn score both for gene and flanking regions ( Supplementary Fig. S11 ). We found that TiSAn-brain is strongly associated with 9 out of 10 brain tissues, and TiSAn-heart shows enrichment for not only heart tissues, but also in liver and pancreas, which is reasonable given their known roles in diseases that are risk factors for cardiovascular disorders, such as hypercholesterolemia and diabetes. We also evaluated TiSAn’s capability to properly annotate intergenic loci associated with brain volume. While not a disease trait, brain volume has been associated with various neurodevelopmental and psychiatric disorders ( van Erp  et al. , 2016 ). ENIGMA GWAS found statistical association between a large set of genetic variations and brain volume ( Hibar  et al. , 2015 ).  Figure 2  shows the correlation between the TiSAn brain score obtained for a locus and its association with brain volume. In the four considered brain regions, we observe the strongest signal for the loci within the top 5% GWAS associations, confirming a role for non-coding variation in a non-disease trait. The lack of a similar signal from the TiSAn heart model suggests good specificity ( Supplementary Fig. S12 ). Interestingly, none of the generic pathogenicity scores [CADD ( Kircher  et al. , 2014 ) and DANN ( Quang  et al. , 2015 )] detected a comparable enrichment around the stronger GWAS hits for any brain region (Supplementary Figs S13–S16). This suggests that TiSAn is uniquely able to capture functional, tissue-specific variation that is not necessarily pathogenic.
 Fig. 2. TiSAn-brain annotation of intergenic loci associated with brain volume. Each of 974 045 ENIGMA loci was binned into 100 percentile groups, based on the statistical association strength with brain volume. For each bin, the average TiSAn-brain score enrichment was computed with respect to the average score across the entire set of loci. The right part of each panel corresponds to loci with stronger associations with brain region volume In the following, we demonstrate TiSAn performance in three different settings: (i) evaluating tissue-specific enrichment in case-control cohort, (ii) enhancing discovery in a genome-wide association study (GWAS) and (iii) identifying tissue-specific transcription factors. We also make practical comparisons to a recently proposed tool, GenoSkyline ( Lu  et al. , 2016 ) that provides a genome partition in terms of functional segments using only methylation data. Our approach aims to provide functional prediction at the single nucleotide resolution, because variants found in large predicted functional blocks (as is the case in GenoSkyline) may in fact have different functional effects. 3.3 Brain-specific variant prioritization in a sample with familial risk for autism 3.3.1 Genome-wide enrichment for brain-related variations in affected individuals The SSC ( http://base.sfari.org ) provides WGS for one of the largest ASD cohorts currently available. We hypothesized that deleterious genetic variation (see definition in Section 2) found in the vicinity of ASD-related genes would show higher enrichment in terms of brain-related functional consequences (as measured by the TiSAn-brain and GenoSkyline-brain scores) in the SSC compared to the 1000 Genomes (1KG) ( 1000 Genomes Project Consortium, 2012 ). We further assessed enrichment using the respective heart-specific scores as a form of negative control, since the cardiovascular system has not been found to be a major etiological contributor to ASD. In this analysis, the TiSAn-brain score showed the only positive tissue-specific enrichment, over 50% for coding variants ( Fig. 3A ) and around 10% for non-coding variants ( Fig. 3B ). Notably, there was a significant difference between TiSAn brain and heart scores (Wilcoxon signed-rank test,  P &lt; 2 × 10 − 16 ), suggesting effective tissue specificity, which was not observed for GenoSkyline models (Wilcoxon signed-rank test,  P  = 0.351). Interestingly, we did not observe a differential enrichment between SSC family members (proband, father, mother and sibling), suggesting a familial genetic burden for autism ( F -test,  P  = 0.28).
 Fig. 3. Brain-related functional enrichment in a case-control setting. Comparison of SSC variants with 1KG variants. Coding variants ( A ) and non-coding variants ( B ). Both brain and heart models for TiSAn and GenoSkyline were evaluated. ( C ) Functional score enrichment in SSC variants compared to 1KG variants. After sorting SSC and 1KG variants based on their score, we compute cumulative enrichment for each 5 percentile. Blue bars correspond to significant difference between SSC and 1KG, using the χ 2  test (FDR adjusted  q -value &lt; 0.1) (Color version of this figure is available at  Bioinformatics  online.) 3.3.2 Case-control variants filtering with brain-specific annotation Next, we ranked and binned variants according to their tissue-specific scores (i.e. TiSAn or GenoSkyline) and calculated the enrichment of SSC deleterious variants in each bin, compared to deleterious 1KG variants. Because the SSC is a neurodevelopmental cohort, we expected to see over-representation of SSC variants in the most confidently called brain-related genomic regions. Indeed, significant enrichment of SSC variants was observed in the top quantiles for TiSAn-brain but also, unexpectedly, for GenoSkyline-Heart models ( Fig. 3C ). Surprisingly, the GenoSkyline-heart model reported a more pronounced enrichment than the corresponding brain model, suggesting a potential lack of tissue specificity for GenoSkyline. TiSAn-brain achieved the highest enrichment by ranking 2.5 times more SSC variants in the top 5% than 1KG variants. 3.3.3 Autism and calcium channel genes An autism-related calcium voltage-gated channel gene,  CACNA1C  ( Kabir  et al. , 2017 ), was the gene with the highest TiSAn-brain score enrichment in SSC data, suggesting that deleterious variants at this locus are likely to affect brain function. In particular, we identified 116 non-coding deleterious variants (see Section 2 for CADD thresholds) in CTCF ( Prickett  et al. , 2013 ) transcription factor binding sites proximal to  CACNA1C  in the SSC data, while none were found in the unaffected population. Mutations in the same region also hit non-coding RNAs (ncRNAs) more frequently in the SSC population than in the control population (Fisher’s exact test,  P &lt; 2 × 10 − 16 ) . Interestingly, five of these ncRNAs ( Supplementary Table S8 ) were found in LD with loci associated with autism and Tourette’s syndrome ( Ning  et al. , 2014 ). 3.4 Heart-related signal prioritization in CAD 3.4.1 Genome-wide association strength and annotation score Current approaches to GWAS analysis rely mostly on association strength (e.g.  P -value) to prioritize candidate regions. These variants often belong to large LD blocks, making it difficult to decipher the causal genetic mechanism. Here, we apply TiSAn to the Coronary Artery Disease CARDIoGRAM consortium GWAS meta-analysis ( Nikpay  et al. , 2015 ), and demonstrate that the TiSAn-heart score is significantly higher among the most associated variants [ Fig. 4A, ( Student  t -test,  P &lt; 2 × 10 − 16 )]. Furthermore, the top 100 SNPs (according to their  P -value) with a non-zero TiSAn were all found in LD with genomic regions strongly associated with CAD, demonstrating TiSAn high sensitivity. In this analysis, no significant enrichment was observed for GenoSkyline-heart (Wilcoxon signed-rank test,  P  = 0.12) or brain models ( Fig. 4B ).
 Fig. 4. Genome-wide association signal prioritization for coronary artery disease. Genetic variants were binned by percentiles, based on their association  P -values. In each of these bins, we report average functional scores for heart models ( A ), and brain models ( B ) (blue: TiSAn, gray: GenoSkyline). Shaded areas represent confidence interval for the corresponding method, after GWAS  P -value random permutations (Color version of this figure is available at  Bioinformatics  online.) 3.4.2 Reduction of multiple hypothesis burden in GWAS We filtered tissue-relevant genotyped variants before the GWAS analysis, using the TiSAn-heart model, so that only heart-related variants would be considered in the correction for multiple testing (herein, Bonferroni correction). In the case of CAD-GWAS, this reduced the number of SNPs considered by 75% and narrowed significant loci by 20% on average (paired Student’s  t -test,  P  = 0.019). Furthermore, the overall enrichment in transcription factor binding sites (TFBS) among significant loci was conserved between the original and TiSAn-filtered sets (χ 2  test,  P  = 0.51), suggesting that the regulatory content was preserved after the filtering step (a further analysis of TFBS, provided in the  Supplementary Material , suggests that TiSAn can reveal which TFs have important functional roles in specific tissues). Reducing the number of tested variants directly recalibrated the multiple-testing correction threshold used to determine significant loci from  5 × 10 − 8  to  1.6 × 10 − 7 . Herein, 91 new loci were found significantly associated with CAD, and these show a significant enrichment in EBF1 TFBS (Fisher’s exact test,  P = 3.2 × 10 − 6 ). EBF1 is a transcription factor that has been previously linked to obesity, diabetes, and cardiovascular disease ( Singh  et al. , 2015 ). A similar analysis, using TiSAn-brain, also resulted in a substantial reduction of CAD-GWAS SNPs tested (90.2% reduction). However, consistent with proper tissue specificity, the fraction tested in this case was comparatively depleted for newly significant loci (18 additional loci for TiSAn-brain filtering, versus 90 additional loci for TiSAn-heart,  P &lt; 2.2 × 10 − 16 , χ 2  test). We emphasize again that these 90 additional loci were not included in, or dependent on, the training set used for TiSAn-heart. This result supports the good generalization of the model. 4 Discussion Integrative approaches like TiSAn hold great promise for helping genomics researchers narrow massive lists of variants to focus on those that are most relevant to the tissue or disease at hand. Few such tools currently exist, however, with most development efforts focusing on improving estimators of general (and not tissue-specific) deleteriousness ( Capriotti and Fariselli, 2017 ). GenoSkyLine, a recently developed tool that utilizes genome-scale tissue-specific epigenetic data, allowed us to benchmark TiSAn and demonstrate its effectiveness in prioritizing genetic variants that are most likely to play a role in the tissue-specific disease processes under consideration. Specifically, we showed that individuals with elevated risk for autism (i.e. probands and their family members) had more deleterious WGS variants that were predicted to be brain relevant (by TiSAn-brain) than controls. At the same time, we were unable to show such differences in regions identified as brain-relevant by GenoSkyLine. Additionally, no difference between cases and controls was observed in TiSAn-heart score, demonstrating its specificity. We showed that strongly associated GWAS hits in a study of CAD have a significantly higher TiSAn-heart signal than non-associated SNPs, supporting our method’s ability to correctly prioritize tissue-specific variants. Again, we were unable to observe this difference using the GenoSkyLine score for cardiovascular tissue. We also demonstrated the practical advantages of reducing GWAS multiple testing burden by pre-filtering SNPs on the basis of their estimated tissue relevance. In each of these analyses, TiSAn showed an ability to correctly prioritize variants according to tissue-specific action, while GenoSkyLine, the current state-of-the-art for this application, was unable to do so. TiSAn thus represents an important development towards leveraging the massive amount of underutilized information (i.e. non-coding variation) coming from WGS studies. Several technical points related to the development of TiSAn are worth mentioning. Perhaps most importantly, we demonstrated that combining additional data sources gives TiSAn a higher sensitivity compared to approaches that rely solely on epigenetic data. Consequently, depending on the use case, it may be worthwhile to take advantage of unique data sources that are available only for a tissue of interest, rather than only those that are available in a wide variety of tissues. Second, a comparison between multiple machine learning algorithms ( Supplementary Table S5 ) led us to use random forests, known to better handle non-linearity and correlation between variables. Recently, deep learning has been evaluated in the context of variation effects on chromatin ( Zhou and Troyanskaya, 2015 ), and future analyses will investigate the impact of using this algorithmic framework. Another issue is that supervised learning requires genomic positions with accurate class labels, in this case, known to be either associated with disease in a given tissue or not. However, for most of the available data, such a ‘gold standard’ label does not exist, especially for positive association with a tissue-specific trait. Imbalance-aware machine learning ( Schubach  et al. , 2017 ) could be a solution to efficiently train predictive models in the case of underrepresented classes. Finally, we developed TiSAn to serve researchers with a particular focus on a single tissue by improving performance, perhaps at the expense of broad tissue coverage. Researchers interested in other tissues beyond brain or heart can derive their own functional annotation for a selected tissue of interest, and we have provided thorough documentation and software tools, including tutorials, on how to use TiSAn in typical genome informatics workflows. Supplementary Material Supplementary Data 1 Click here for additional data file. Supplementary Data 2 Click here for additional data file. Supplementary Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>BayesPeak—an R package for analysing ChIP-seq data</Title>
    <Doi>10.1093/bioinformatics/btq685</Doi>
    <Authors>Cairns Jonathan, Spyrou Christiana, Stark Rory, Smith Mike L., Lynch Andy G., Tavaré Simon</Authors>
    <Abstract>Motivation: Identification of genomic regions of interest in ChIP-seq data, commonly referred to as peak-calling, aims to find the locations of transcription factor binding sites, modified histones or nucleosomes. The BayesPeak algorithm was developed to model the data structure using Bayesian statistical techniques and was shown to be a reliable method, but did not have a full-genome implementation.</Abstract>
    <Body>1 INTRODUCTION Chromatin ImmunoPrecipitation (ChIP) experiments produce short DNA fragments, preferentially selected to identify the locations of protein binding sites, histone modifications or nucleosome positions. In the ChIP-seq protocol, as described in  Robertson  et al.  (2007 ), the 5′-end of one strand of each fragment is sequenced, obtaining a ‘read’, and then aligned to a reference genome. These aligned reads form ‘peaks’—localized regions of high read density—along the genome. Determining the locations and magnitudes of these peaks is an active area of research, and a number of tools exist for the so-called ‘peak-calling’, using a variety of methodologies. The algorithm described in  Spyrou  et al.  (2009 ) takes a Bayesian approach to modelling aligned reads from ChIP-seq data. Many peak-callers model read counts with the Poisson distribution, and thus do not allow for the overdispersion seen in practice.  BayesPeak  addresses this issue by using the negative binomial distribution. The method optionally allows for the inclusion of a control sample, which enables us to mitigate the effect of experimental artefacts where protein binding is absent. Additionally, the algorithm was shown (in  Spyrou  et al. , 2009 ) to call merged peaks that are narrower than regions identified by other callers and this, along with inference from posterior probabilities, can improve the efficiency of downstream processing, e.g. motif analysis. We present the R package  BayesPeak , which uses a modified version of the algorithm in  Spyrou  et al.  (2009 ) with a flexible genome-wide implementation. As well as providing compatibility with common input formats and downstream analyses, the  BayesPeak  package adds additional methods for summarizing data, tools for handling overfitting and support for parallel processing. 2 METHODS The  BayesPeak  package, written in R and C, forms part of the BioConductor release branch since version 2.6 ( Gentleman  et al. , 2004 ). The implementation of  BayesPeak  allows it to take advantage of parallel processing, improving its efficiency. We provide optional parallelization support using the  multicore  package ( Urbanek, 2009 ) for Linux/Mac OS X. BayesPeak  can analyse a human genome in under 12 h, when run in parallel on an 8-core 2.5 GHz machine. For a benchmark example in which the treatment and control .bed files totalled 33 million reads (2.3 GB of disk space),  BayesPeak  required not more than 3 GB of RAM (although larger .bed files require more RAM). R version 2.11.0 or later is required. The  BayesPeak  software package fits a hidden Markov model (HMM) to the data (aligned reads) as follows: the genome is divided into ‘jobs’, i.e. short regions on which the algorithm is run independently. By default, jobs are of length 6 Mb (numerical stability may be compromised in larger jobs), and each job region is expanded by 2 Kb in each direction (to allow peaks falling on the boundary between two jobs to be called). Within a job, the region is divided into small bins (each of length 100 bases, by default), and reads are aggregated by the bin in which they start and the strand on which they lie. A two-state HMM is fitted to these aggregate counts. The HMM's hidden states correspond to enrichment or unenrichment for sites of interest. A hidden state produces two negative binomial emissions, each corresponding to a bin count (one on each strand), with enriched states tending to emit larger values. The HMM is fitted through MCMC techniques that sample from the posterior distributions of the parameters. The analysis is performed a second time on the same job region, but with all bins offset by half their width (the ‘offset’ analysis) as illustrated in  Figure 1 . Further details can be found in  Spyrou  et al.  (2009 ). Fig. 1. A schematic representation of a hypothetical peak region, with bins labelled by genomic order. Supposing that each bin has an associated PP value above a threshold (by default 0.5), we would merge these six bins into a peak from 1100 to 1450, with an associated PP value as calculated in  Section 2.1 . The output of each job is the posterior probability (PP) of each site being enriched. The data are summarized to form the final peaks as follows. All bins with PP values greater than a user-specified threshold (by default, 0.5) are collected. Where two adjacent jobs call the exact same bin, the maximal PP value is used—this is a rare occurrence under default settings. 2.1 Merging bins Bins from both the ‘normal’ and ‘offset’ jobs that are adjacent or overlap are merged to form contiguous peak regions. The PP value of the peak can be calculated from the constituent bins by either naively taking the maximal value, or by using the ‘lower bound’ method defined as follows: assign the indices 1,…,  n  to the  n  bins within the peak, in order of genomic location (as in  Fig. 1 ). Note that bins with adjacent indices overlap. Now let the PP value of bin  i  be π i , and define  q i  = 1 − π i  as the probability of no enrichment in bin  i . Let  S n  be the set of all subsequences of {1,…,  n } such that  I  ∈  S n  ⇔  I  contains no consecutive integers ⇔ the bins with indices in  I  do not overlap. Then, for each  I  ∈  S n , a lower bound for the probability of enrichment in at least one of the original  n  bins is  . The ‘best’ (highest) lower bound for the probability of peak enrichment is therefore the maximum of this quantity,
 We can find  Q ( n ) by dynamic programming since, by conditioning on whether  i  ∈  I , we have  Q ( i ) = min( Q ( i  − 1),  q i Q ( i  − 2)). The advantage of using this method over taking the maximum PP value is that it can give an appropriate score to sustained regions of only moderately large PP values, which will be undervalued when taking the maximum. We tested  BayesPeak  on the NRSF/REST ChIP-seq dataset from ( Johnson  et al. , 2007 ), in which a small subset of regions have been experimentally validated, and we compared the findings against other common peak callers. 3 RESULTS We present the peak-caller comparison results in the Supplementary Material.  BayesPeak  demonstrated a competitive sensitivity and specificity on the genome-wide scale and showed a substantial overlap with other peak-callers. The over-fitting correction greatly improved the enrichment for true binding sites in  BayesPeak 's data, as did subsequent filtering by PP value. In its raw output,  BayesPeak  returns PP values for each bin and, for each job, the posterior mean of each estimated parameter (excluding half of the draws as burn-in). As of  BayesPeak  version 1.1.3, MCMC samples of several key parameters are also present, permitting convergence tests such as the Geweke diagnostic in the  boa  ( Smith, 2007 ) or  coda  ( Plummer  et al. , 2010 ) packages. Since the summarized output is in RangedData format, this allows direct analysis of the peaks in any downstream package compatible with IRanges ( Pages  et al. , 2010 ), including those in BioConductor. For example,  ChIPpeakAnno  ( Zhu  et al. , 2010 ) can annotate the output. We have observed some phenomena that occur with lower quality data. For example, over-fitting can occur as follows: the model assumes that for each job there are both enriched and unenriched states. As such, when there are no peaks in a job or when the peaks are extremely weak, these two states are used to explain the natural variance present in the unenriched background. We identify over-fit jobs from their low λ 1  values (where λ 1  is the expected number of counts in an enriched bin), and from their PP values being spread out over [0, 1] rather than tending to be 0 or 1.  BayesPeak  supports the identification and removal of jobs that have encountered this effect (Supplementary Table S2 and Fig. S1). 4 DISCUSSION BayesPeak  provides a Bayesian analysis, with advantages including allowance for overdispersion in read counts and a competitive genome-wide specificity and sensitivity. By anticipating peak structure,  BayesPeak  does not call peaks based on sheer numbers of reads without appropriate read formation. Careful selection of job regions may improve the analysis. For example, we can use prior knowledge to partition jobs in a manner that avoids analysing the centromeres and telomeres, which usually contain no reads. This will prevent unnecessary computation, and may also improve results in the surrounding regions. There is scope for adapting the  BayesPeak  approach to other forms of peak-calling. For example, some histone mark data consist of regions of enrichment containing many peaks and, in BrDU-seq data, peaks are much broader than those in transcription factor data. 
          Supplementary Material 
          
             
                Supplementary Data 
             
             
             
          
       </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>GASSST: global alignment short sequence search tool</Title>
    <Doi>10.1093/bioinformatics/btq485</Doi>
    <Authors>Rizk Guillaume, Lavenier Dominique</Authors>
    <Abstract>Motivation: The rapid development of next-generation sequencing technologies able to produce huge amounts of sequence data is leading to a wide range of new applications. This triggers the need for fast and accurate alignment software. Common techniques often restrict indels in the alignment to improve speed, whereas more flexible aligners are too slow for large-scale applications. Moreover, many current aligners are becoming inefficient as generated reads grow ever larger. Our goal with our new aligner GASSST (Global Alignment Short Sequence Search Tool) is thus 2-fold—achieving high performance with no restrictions on the number of indels with a design that is still effective on long reads.</Abstract>
    <Body>1 INTRODUCTION Next-generation sequencing (NGS) technologies are now able to produce large quantities of genomic data. They are used for a wide range of applications, including genome resequencing or polymorphism discovery. A very large amount of short sequences are generated by these new technologies. For example, the Illumina-Solexa system can produce over 50 million 32–100 bp reads in a single run. A first step is generally to map these short reads over a reference genome. To enable efficient, fast and accurate mapping, new alignment programs have been recently developed. Their main goals are to  globally  align short sequences to  local  regions of complete genomes in a very short time. Furthermore, to increase sensitivity, a few alignment errors are permitted. The  seed and extend  technique is mostly used for this purpose. The underlying idea is that significant alignments include regions having exact matches between two sequences. For example, any 50 bp read alignments with up to three errors contains at least 12 identical consecutive bases. Thus, using the  seed and extend  technique, only sequences sharing common  k mers are considered for a possible alignment. Detection of common  k mers is usually performed through indexes localizing all  k mers. Recently, several index methods have been investigated and implemented in various bioinformatics search tools. The first method, used by SHRiMP (Rumble  et al. ,  2009 ) and MAQ (Li,H.  et al. ,  2008 ), creates an index from the reads and scans the genome. The advantage is a rather small memory footprint. The second method makes the opposite choice: it creates an index from the genome, and then aligns each read iteratively. PASS (Campagna  et al. ,  2009 ), SOAPv1 (Li,R.  et al. ,  2008 ), BFAST (Homer  et al. ,  2009 ) and our new aligner GASSST (Global Alignment Short Sequence Search Tool); use this approach. The last method, used in CloudBurst, indexes both the genome and the reads. Although more memory is needed, the algorithm exhibits better performance due to memory cache locality. Another short read alignment technique, used in Bowtie (Langmead  et al. ,  2009 ), SOAPv2 (Li  et al. ,  2009 ) and BWA (Li and Durbin,  2009 ), uses a method called backward search (Ferragina and Manzini,  2000 ) to search an index based on the Burrows–Wheeler transform (BWT; Burrows and Wheeler,  1994 ). Basically, it allows exact matches to be found before using a backtracking procedure that allows the addition of some errors. Although this technique reports extremely fast running times and small memory footprints, some data configurations lead to poor performances. ( http://bowtie-bio.sourceforge.net/manual.shtml ). Moreover, in order to speed-up computations, some methods restrict the type or the number of errors per alignment to a few mismatch and indel errors. In the building alignment process, computing the number of mismatches requires linear time, whereas indel errors require more costly algorithms such as the dynamic programming techniques used in the Smith–Waterman (Smith and Waterman,  1981 ) or Needleman–Wunsch (NW; Needleman and Wunsch,  1970 ) algorithms. For instance, MAQ, Eland and Bowtie do not allow gaps. EMBF (Wendi  et al. ,  2009 ), SOAPv1 and SOAPv2 allow only one continuous gap, while PASS, SHRiMP, BFAST and SeqMap (Jiang and Wong,  2008 ) allow any combination of mismatch and indel errors. GASSST, as well, considers any combination of mismatch, insertion or deletion errors. In most applications, when reads are very short, dealing with a restricted number of errors is acceptable. On the other hand, when longer reads are processed, or when more distant reference genomes are compared, this restriction may greatly affect the quality of the search. In this article, we introduce GASSST, a new short read aligner for mapping reads with mismatch and indel errors at a very high speed. We show how a series of carefully designed filters allows false positive positions to be quickly discarded before the refinement extension step. In particular, GASSST is compared with similar state-of-the-art programs: BFAST, BWA, PASS and SSAHA2. 2 APPROACH GASSST uses the  seed and extend  strategy and indexes the genome. The  seed  step provides all potentially homologous areas in the genome with a given query sequence. The traditional way to accomplish this step is through a hash-table of  k mers for selecting regions sharing common  k mers with query sequences. To include gaps, the  extend  step is carried out with a dynamic programming algorithm (NW). This approach provides a high degree of accuracy, but is prohibitively expensive due to the high number of costly NW extensions that must be performed. To tackle this issue, several approaches have been proposed. The SHRiMP implementation (Rumble  et al. ,  2009 ) provides an improved  seed  step with spaced seeds and Q-gram filters to restrict the size of the candidate hit space. Then, a carefully optimized vectorized Smith–Waterman extension is run to perform the  extend  step. BFAST (Homer  et al. ,  2009 ) uses long spaced seeds to limit the amount of candidate locations, yet still achieves high sensitivity through the use of multiple indexes. PASS (Campagna  et al. ,  2009 ) implements another solution: a filter is introduced before the full  extend  step to rule out areas that have too many differences with the query sequence. A precomputed table of all possible short words, aligned against each other, is built to perform a quick analysis of the flanking regions adjacent to seed words. The GASSST strategy is similar: a traditional anchoring method is used followed by a NW extension step. However, fast computation is achieved thanks to a very efficient filtering step. Candidate positions are selected through a new carefully designed series of filters of increasing complexity and efficiency. Two main types of filter are used. One is related to the computation of an Euler distance between nucleotide frequency vectors as defined by Wendi  et al.  ( 2009 ). The idea is this: if one sequence has, for example, three more ‘T’ nucleotides than another, then the alignment will have at least three errors (mismatches or indels). The other includes precomputed tables, as in PASS, to produce a score based on the NW algorithm, but brings the strategy to a higher level; instead of addressing large tables, as PASS does, we designed an algorithm able to reuse small tables along the whole query sequence. In this way, the filter is much more selective and discards a very large number of false positives, thus drastically decreasing the time spent in the final  extend  step. GASSST's originality comes then from the use of a small lookup table. More precisely, the precomputed alignment scores of all possible pairs of words of length  w  can be stored in a memory of size 4 2 w  bytes, if a score is memorized in a single byte. For PASS, the size of the lookup table is 4 14  = 256 MB. This fits into any computer's main memory, but not in the first-level CPU cache. Hence, random accessing of the table, even if it avoids many computations, may still be costly. On the contrary, the GASSST algorithm manipulates a small table of 64 KB which easily fits into the cache memory, providing fast filtering compared with non-precomputed calculations. 3 METHODS GASSST algorithm has three stages: (i) searching for exact matching  seeds  between the reference genome and the query sequences; (ii) quickly eliminating hits that have more than a user-specified number of errors; and (iii) computing the full gapped alignment with the NW algorithm. These three steps are referred to, respectively, as  seed-filter-extend . The novelty of the GASSST approach relies on a new highly efficient  filter  method. 3.1 Seed GASSST creates an index of all possible  k mers in the reference sequence and then scans every query sequence to find matching  seeds . When seeds longer than 14 are selected the algorithm uses a simple hash-table mechanism. For each seed position in the reference sequence, the index contains the sequence number, the position where it occurs and the flanking regions (binary encoded). Up to 16 nt are stored on the left and on the right of the seed in order to speed-up the next filtering steps. The size of the index is equal to 16 ×  N  bytes, with  N  the size of the reference sequence. If large reference sequences which exceed the memory size are considered, a simple partitioning scheme is provided; the reference sequence is split into as many parts as necessary to fit in the available RAM. Each part is then processed iteratively. 3.2 Tiled-NW filter A lookup table, called pre-computed score table  PST l , containing all the NW alignment scores of all possible pairs of  l  nt long words is first computed. PASS uses a  PST 7  to analyze discrepancies near the 7 nt long flanking region adjacent to the seeds. The bigger the table, the better the filtering. But unfortunately, the table grows exponentially with  l . To address this issue, GASSST analyzes discrepancies in a region of any length thanks to the reuse of a small  PST 4  table. The goal is to provide a lower bound approximation of the real NW score along the whole alignment. If estimated lower bounds are greater than the maximum number of allowed errors, then alignments are eliminated. If not, alignments are passed to the next step. The following values are used for the NW score computation: 0 for a match, and 1 for mismatch and indel errors. Consequently, the final score indicates the number of discrepancies in the alignment. In the following, only the right side of the seed is considered, the other side being symmetrical. D efinition  1. We call TNW l ( n )  the score of a region of n nucleotides which are adjacent to the right of the seed and which are computed with a PST l .  TNW stands for  Tiled NW.  TNW l ( full )  operates on the maximum length available ,  i.e. from the right of the seed to the end of the query sequence . D efinition  2. If S 1  and S 2  are the two sequences directly adjacent to the right of the seed ,  we call PST l ( i ,  j ) t he pre-computed NW score for the two l nt long words S 1( i ,  i  +  l  − 1)  and S 2( j ,  j  +  l  − 1). If  G m  is the maximum number of allowed gaps,  TNW l ( n ) is computed with the following recursion: If  n  ≤  l  then
 (1) 
Else
 (2) 
With
 
 
 
 Figure 1 B shows a graphical representation of this recursion.
 Fig. 1. Computation of a semi-global alignment (only the query sequence needs to be globally aligned), in the case of a maximum of 1 error. ( A ) Dynamic Programming. With 12 nt long sequences and a traditional dynamic programming algorithm, cell calculations can be limited in a band, but there are still 34 cells to compute. ( B ) Tiled Algorithm. With a precomputed table score of size 4 × 4, the tiled algorithm is performed in only three steps. The first step requires one table access, while the second and third steps both require three table accesses. Here, the score given by the tiled algorithm is the same as the full dynamic programming algorithm—there are two errors in the alignment. In the general case, the tiled algorithm only gives a lower bound of the number of errors present in the alignment. Complexity of the filter.  GASSST uses a  PST 4  lookup table. By storing each score in a single byte, its size is equal to 64 KB. This small size allows the  PST 4  to fit in a CPU cache of today's desktop computers. But if  TNW 4 (4) requires a few clock cycles to be accessed, its filtering power is limited. In the general case, the number of  PST l  accesses  N acc [ TNW ( n )] needed for the computation is in 𝒪( n ). If  G m  is the maximum number of allowed gaps, the exact formula is:
 (3) The filter works with iterative  l -sized levels. The first one takes one access, then each following level requires (1 + 2 ·  G m )  PST l  accesses. Each new level filters more and more false positive alignments. Large  PST l  tables lead to fewer levels and better accuracy, but they imply longer execution times since the number of memory cache misses increases rapidly with bigger tables. 3.3 Frequency vector filter The frequency vector filter of GASSST is the same as the one used in EMBF (Wendi  et al. ,  2009 ). The idea is quite simple: if one sequence has, for example, three more ‘G’ nucleotides than another sequence, then their alignment will have at least three errors. If the user-specified maximum number of errors is 2, then the alignment can be directly eliminated. For a sequence  S  =  s 1 s 2 … n  of characters in the alphabet Σ = { a 1 ,  a 2 ,…,  a p }, the frequency vector  F  = { f 1 ,  f 2 ,…,  f m } is defined as:
 (4) 
With
 (5) The Euler distance has to be computed on similar frequency vectors, i.e. referring to sequences of equal length. The distance is computed with:
 (6) In GASSST, frequency vectors of sequences of up to 16 nt on both sides of the seed are computed. Actually, this value is often limited by the length of the reads, and forbids vectors of the reference sequence to be computed at runtime. Indeed, the size of subsequences depends on the size available on the read, which is often less than 16 and which is only known at compute time. The Euler distance is computed between the frequency vectors of the read and the genome. If the result is greater than the maximum number of errors then the alignment is discarded. The computation of the frequency vectors is vectorized and quickly performed thanks to the binary format of the seed flanking regions. The frequency vectors and the Euler distance computations are both vectorized to benefit from  vector  execution units present in modern processors. Since a 16 nt sequence is 32-bit encoded, counting the frequency of a single nucleotide can be done with bit-level logical operations that operate on traditional 32-bit words. The vectorization consists of simultaneously computing values for the four nucleotides of the alphabet and operates on 128-bit wide words. The frequency distance vectorized filter is referred to as  FD-vec . 3.4 Filters combination The goal of the  filter  step is to eliminate as many false positive alignments generated by the  seed  step as possible, and in the fastest possible way. This is done by ordering the individual filters from the fastest to the most complex and powerful. Algorithm 1 shows the main computation loop of GASSST with the series of filters.  TNW 4 (4) is first since it is the fastest. It is followed by the vectorized frequency filter that rules out remaining false positive alignments. Then a more thorough filter is used ( TNW 4 (16)). The last filter is a  TNW 7 ( full ) filter which is applied on the full length of the sequence and with a  PST 7  in order to eliminate a maximum number of false positive alignments. It is computationally intensive, but it comes at the end when most alignments have already been ruled out. Finally, the true NW alignment is computed on alignments that go through all filters. This combination of filters ensures an efficient and fast filtering in a wide range of configurations, from short to longer reads, with low or high polymorphism. One important point is that these filters only discard alignments which are proven to have too many errors and that would have been eliminated by the NW algorithm. They never eliminate good alignments, hence they do not decrease sensitivity. Moreover, to reduce running time, the search is stopped when a maximal number of occurrences of a seed in the reference sequence has been reached. This kind of limitation is present in most other aligners. The only difference here is that a threshold is also checked in some stage of the filtering process. The threshold is automatically computed according to seed length and reference sequence size. Users can control the speed/sensitivity trade-off of this heuristic through a parameter  s  in 0–5 which modulates this threshold. 3.5 Extend The  extend  step receives alignments that passed the  filter  step. It is computed using a traditional banded NW algorithm. Significant alignments are then printed with their full description. It should be noted that if the  filter  step provides good efficiency, no optimization of the  extend  step is required. Indeed, if most false positive alignments have already been ruled out, the  extend  step should only take a negligible fraction of the total execution time. 4 RESULTS 4.1 Implementation GASSST is implemented in C++ and runs on Linux, OS X and Windows. It benefits from vector execution units with the use of SSE (streaming SIMD extensions) instructions, and is multi-threaded. It performs accurate and fast gapped alignments and allows the user to specify the number of hits given per read (option -h). A tool is provided to convert GASSST results to SAM format and compute GASSST mapping quality. GASSST is distributed under the CeCILL software license ( http://www.cecill.info ). Documentations and source code are freely available from the following web site:  http://www.irisa.fr/symbiose/projects/gassst/ . 4.2 Evaluated programs The performance of GASSST is compared with four other programs: PASS 0.74 (Campagna  et al. ,  2009 ) BFAST 0.6.3c (Homer  et al. ,  2009 ), BWA 0.5.7 (Li and Durbin,  2009 ) and SSAHA 2.5.2 (Ning  et al. ,  2001 ) . PASS indexes the genome and scans reads. It uses a precomputed NW table to filter alignments before conducting the extension with a dynamic programming algorithm. BFAST is currently one of the most popular tools. It relies on large spaced seeds for a fast execution, and on many different indexes for sensitivity. BWA uses another approach, based on the BWT, and is probably one of the fastest aligners to date for alignments with a low error rate. We also tested the BWA-SW variant intended to work best for longer reads. In the following, ‘BWA’ refers to the BWA short read mode and ‘BWA-SW’ to the long read variant. The computer used for the tests is an Intel Xeon E5462 with 32 GB RAM running at 2.8 GHz. Although all programs tested are able to benefit from multi-threaded computations, we choose to compare performance on a single thread, as it is enough to assess their respective strong or weak points. We ran experiments with real datasets to give an indicative behavior. Detailed program analysis was conducted on simulated data where alignment correctness could be assessed. 4.3 Evaluation on real data Performance was evaluated on three real datasets of short reads obtained from the NCBI Short Reads Trace Archive. The three sets contain, respectively, 11.9 million sequences of 36 bases, 6.8 million sequences of 50 bases and 8.5 million sequences of 76 bases of accession numbers SRR002320, SRR039633 and SRR017179. They are all aligned with the whole human genome. BWA short read aligner was run with default options, BFAST was run with its 10 recommended indexes and default options, GASSST and PASS were set to search for alignments with at most 10% errors. We measured the execution time and the percentage of mapped reads having a mappinq quality greater than or equal to 20. The results are presented in  Table 1 .
 Table 1. Evaluation on real data Read Software Index (s) Align (s) Q20% Q20 length Error rate (%) 36 bp GASSST 1712 3211 34.5 0.14 BFAST 520 800 17 520 41.8 0.12 BWA 5158 3739 35.4 0.17 PASS 2312 5072 41.4 a – 50 bp GASSST 1719 4090 73.7 0.04 BFAST 520 800 22 799 80.4 0.10 BWA 5158 3043 74.5 0.17 PASS 2144 5384 79.3 a – 76 bp GASSST 1701 8483 81.3 0.04 BFAST 520 800 161 220 85.4 0.28 BWA 5158 3101 86.4 0.53 PASS 1951 118 541 87.7 a – Datasets consisted, respectively, of 11.9, 6.8 and 8.5 millions of reads of size 36, 50 and 76 bp, of accession numbers SRR002320, SRR039633 and SRR017179. The three datasets were aligned with the whole human genome. The time required to compute the genome index is shown in column 3. For BFAST and BWA, the index was computed only once and stored on disk. Column 4 shows the time required in seconds to align reads, running on a single core of a 2.8 GHz Xeon E4562. Column 5 shows the percentage of reads with a mapping quality greater than or equal to 20 (Q20). Last column is the percentage of Q20 alignments that are probably wrong given the results of other aligners: if a program gives a high mapping quality to a read and another program finds a different alignment of similar alignment score for that read, then the first program is probably wrong. a Since PASS does not compute mapping qualities, fifth column shows for PASS the percentage of reads with a unique best alignment, and error rate is not computed. 
 Evaluation on real data is difficult since true alignment locations are unknown. However, it is possible to compare results of different aligners to estimate accuracy, as it is done by Li and Durbin ( 2010 ) for their evaluation of BWA-SW. If an aligner  A  gives a high mapping quality to a read and another aligner  B  finds an alignment at another position for that same read with an alignment score better or just slightly worse, then  A  alignment is probably wrong. A score for each read is computed as the number of matches minus three multiplied by the number of differences (mismatches and gaps). We say that  A  alignment is questionable if the score derived from  A  minus the score derived from  B  is less than 20. Since this evaluation method is approximate, a full evaluation was conducted on simulated data in  Section 4.5 . On short 36 bp reads, GASSST performance is comparable to BWA. On longer 76 bp reads both PASS anf BFAST becomes very slow. On the other hand, the GASSST combination of filters still works well for the 76 bp dataset. GASSST and PASS currently cannot store the index on disk, yet index computation time is amortized when working on very large sets of reads. BFAST index time is very long because it uses 10 different indexes, however they are computed only once. 4.4 Filter behavior analysis We measured the filtering power of the filter combination on the same three datasets of the previous section to validate their efficiency on different configurations. The allowed similarity rate was set to 90% minimum so the number of allowed errors was 3, 4 and 7, respectively. Other program options are possible and would result in different behavior, so the results presented here cannot be generalized and are only intended to provide an indicative example. Hits coming from the  seed  step are pipelined through the filters. At each filter step, we calculate the filtering percentage of the individual step as well as the cumulative percentage of hits filtered so far. The NW alignment is seen as the last filter and its result is also included.  Table 2  shows the results. The first thing that can be noted is that in all cases the percentage of hits arriving at the NW step is limited to 0.2% of the number of hits generated by the  seed  phase. This means that without filters, this step would take at least 500 times longer.
 Table 2. Filtering rate on three datasets Data Set Filter Step Step filter (%) Total filter (%) 36 bp TNW 4 (4) 64.9 64.9 FD  −  vec 72.8 90.5 TNW 4 (16) 96.6 99.7 TNW 7 ( full ) 45.9 99.8 NW 63.8 99.9 50 bp TNW 4 (4) 41.7 41.7 FD  −  vec 80.4 88.6 TNW 4 (16) 97.5 99.7 TNW 7 ( full ) 88.5 99.97 NW 67.5 99.99 76 bp TNW 4 (4) 0.1 0.1 FD  −  vec 49.8 49.9 TNW 4 (16) 93.7 96.8 TNW 7 ( full ) 94.8 99.8 NW 70.2 99.95 The first column gives the percentage of hits filtered by a given filter. The second column gives the total percentage of hits filtered by the sequence of filters. Reads were aligned with 10% errors maximum, which means a maximum of 3, 4 and 7 errors, gaps or mismatches, respectively. 
 Second, one interesting thing to observe is the number of false positive alignments that passed the filters and still arrive at the NW step. The NW step filters about two-thirds of incoming hits, meaning that previous filters have efficiently ruled out most false positive alignments—one alignment out of three that enters the NW step is valid. As expected, on the last dataset (seven allowed errors), the  TNW 4 (4) filter efficiency is very poor since it can only manage a maximum of four errors on each side of the seed. In the current implementation, this filter is deactivated when the number of errors exceeds 7. To complete this analysis, we measured the total time spent inside each filter. The program was profiled for a typical execution with the 50 bp dataset. For each step, both the computation time of a single hit in nanoseconds and the total execution time of the step in percentage of the total time of the program were measured. The results are shown in  Table 3 . The  General Overhead  mostly represents the cost of line 8 of Algorithm 1, which iterates through hits and loads the associated information. The  TNW 4 (4) filter takes only 5.8 ns, which represents, as expected, only a few clock cycles of the processor. We can also verify that the filters are indeed ordered from the most simple to the most complex. The NW step takes only 3% of the total time, which validates the assessment formulated in  Section 3.5  that we do not need to be concerned about its optimization.
 Table 3. Filters execution time Step Execution time Total percent time for one hit (ns) spent in this step General Overhead 13.9 33 TNW 4 (4) 5.8 14 FD  −  vec 22.4 32 TNW 4 (16) 78.5 15 TNW 7 ( full ) 356.7 3 NW 4486.5 3 Average time taken by each filter step for one hit of a 50 bp query, in nanoseconds. Note that only the times of  TNW 7 ( full ) and NW actually depends on query size since the others only work on a bounded size subsequence. 
 4.5 Evaluation on simulated data We simulated 12 datasets containing one million reads each from the entire human genome, with lengths of 50, 100, 200 and 500 bp and with error rates of 2, 5 and 10%. The error rate is the probability of each base being an error, 20% of errors are indel errors with indel length  l  drawn from a geometric distribution of density 0.7 × 0.3 l −1 . BFAST is run with options − K  8 and − M  1280 with 10 indexes. GASSST was tested in two different configurations, a  fast  one with option − s 0 and an  accurate  one with − s 3, which controls the speed/sensitivity trade-off of the algorithm by setting a maximum number of occurrences per seed. It is roughly the equivalent of options − K  and − M  of BFAST. Other GASSST options such as seed length and maximum number of allowed errors were tuned accordingly to the different datasets. BWA and PASS options were also tuned to allow more mismatches and gaps when necessary, and BWA-SW was run with default configuration. Finding the most efficient set of options for each program and each dataset is a lengthy process, so although we did our best to tune options and provide a fair evaluation, in some cases different options may yield better performance. The results are presented in  Table 4 .  Supplementary Table S1  gives the list of options parameters used.
 Table 4. Evaluation on simulated data Program Mode Metrics 50 bp 100 bp 200 bp 500 bp 2% 5% 10% 2% 5% 10% 2% 5% 10% 2% 5% 10% GASSST fast Align sec 584 781 1720 794 981 1160 2030 2314 3051 6573 8453 11 859 Sensitivity% 45.8 42.8 36.1 54.2 53.3 44.9 58.6 56.3 53.8 61.0 59.8 58.8 Accuracy% 99.2 98.5 93.8 90.5 89.4 86.9 91.9 91.5 89.7 93.0 92.8 91.7 GASSST accurate Align sec 1709 2741 4706 1290 1887 3262 3452 6173 8744 12 864 18 737 34 222 Sensitivity% 46.7 44.6 37.5 51.0 50.3 43.5 53.4 52.8 51.7 57.5 55.7 55.5 Accuracy% 99.8 99.3 93.5 99.7 99.4 97.5 99.9 99.8 99.3 99.9 99.9 99.7 BFAST Align sec 2279 2044 1756 15 263 15 787 11 452 – – – – – – Sensitivity% 46.5 43.0 32.1 52.7 51.0 48.5 – – – – – – Accuracy% 98.8 96.1 85.2 99.0 98.7 95.8 – – – – – – BWA Align sec 792 1392 1572 1862 4941 3364 4660 2145 185 – – – Sensitivity% 48.2 38.6 16.8 54.8 41.0 7.3 53.3 11.9 0.1 – – – Accuracy% 99.2 97.4 93.5 99.7 99.0 97.9 99.8 99.6 96.7 – – – BWA-SW Align sec – – – – – – 4699 3546 2365 13 027 9646 7835 Sensitivity% – – – – – – 54.9 50.3 25.2 57.3 56.1 45.4 Accuracy% – – – – – – 99.4 96.9 85.7 99.2 96.8 85.2 SSAHA2 Align sec – – – 27 740 41 978 45 295 22 285 27 504 65 420 179 095 415 252 275 622 Sensitivity% – – – 45.3 43.5 38.6 53.2 51.4 48.4 59.5 58.8 55.8 Accuracy% – – – 99.8 99.1 95.3 99.8 99.1 96.0 99.8 99.2 95.3 PASS Align sec 2012 2281 5085 14 387 26 033 30 022 103 338 139 436 180 943 – – – Sensitivity% 50.0 43.8 31.3 51.6 37.5 16.4 49.3 16.6 2.8 – – – Accuracy% 96.6 93.2 82.4 98.5 94.0 86.8 97.2 93.6 92.4 – – – Data sets containing one million reads each are simulated from the human genome with different lengths and error rate. Twenty percent of errors are indel errors with indel length  l  drawn from a geometric distribution of density 0.7 · 0.3 l −1 . The alignment time in seconds only includes the fraction of the total time proportional to the number of reads, i.e, not the time spent in computing or loading the index of the human genome, running on a single core of a 2.8 GHz Xeon E4562 . Computed alignment coordinates are compared to the true simulated coordinates to find sensitivity and accuracy. Sensitivity is the percentage of reads correctly mapped, while accuracy is the percentage of mapped reads that are correctly mapped. A read is considered mapped if it has a unique best alignment. A mapped read is considered correct if it is within 10 bp of its true coordinates. We filtered alignments having a mapping quality less than 20, except for PASS, which does not give mapping quality. 
 For each run, we reported accuracy, sensitivity and running time. A read is considered to be mapped if it has a unique best alignment. A mapped read is considered correct if it is within 10 bases of the true location. We filtered all alignments that had a mapping quality less than 20, except for PASS which does not compute mapping quality. Sensitivity and accuracy are then, respectively, defined as the percentage of total or mapped reads that are correct. The running time reported does not include index loading or computing phase. Although this choice penalizes BWA, which is the only one not spending noticeable time recomputing or reloading indexes, it is relevant since time spent for the index of the genome is constant and is amortized when dealing with a very large number of reads. Tests were also conducted with SHRiMP 1.3.1 but not included here since running times were tens to hundreds of times larger than GASSST, rendering evaluation impractical. This corresponds to observations in other studies that had to extrapolate SHRiMP running time (Homer  et al. ,  2009 ). For datasets with a low 2% error rate, GASSST performance is comparable with BWA. For example, on short 50 bp reads, GASSST in fast mode obtained 45.8%/99.2% sensitivity/accuracy in 584 s compared with 48.2%/99.2% in 792 s for BWA. For higher error rates, GASSST becomes better than BWA, for example, on the 100 bp reads with 10% error rate GASSST obtained 43.5%/97.5% sensitivity/accuracy in 3262 s compared with 7.3%/97.9% in 3364 s for BWA. On datasets simulated with a high 10% error rate, GASSST consistently reports better results than other aligners. On the 200 bp 10% dataset, GASSST was the only one able to provide high sensitivity and accuracy within a reasonable amount of time. On the 500 bp dataset, experiments are only conducted on GASSST, BWA-SW and SSAHA2 since other aligners are not designed for this read length. They show that GASSST still performs very well. For example, on the 2% error rate dataset, GASSST obtained results comparable to BWA-SW. With 10% error rate GASSST is eight times faster than SSAHA2 for similar results, whereas BWA-SW accuracy is dropping. BFAST is efficient on short 50 bp reads only, its execution time increases a lot for longer reads. Overall, these experiments show that GASSST performs well on a wide range of configurations, as expected with the design of our series of filters. 5 DISCUSSION In this article, we introduced an original method to speed-up aligner programs. While the BFAST approach is to reduce the number of candidate alignment locations generated in the  seed  step through the use of large spaced seeds, our approach uses a simple indexing scheme generating many candidate alignment locations, but quickly discards most of them through a series of filters. Experiments conducted on simulated data showed that the GASSST approach provides fast and high quality results, and that this new series of filters is indeed more efficient than trying to optimize the NW algorithm further. Even a GPU-based implementation of NW, with an optimistic 20-fold speed-up over the SSE vectorized implementation of SHRiMP, would still be slower than our filtering approach. Moreover, since sequencing technologies are evolving toward longer and longer generated reads, many algorithms designed to work best on short reads will become inefficient. On the other hand, our experiments showed that GASSST is already prepared for this evolution: it is efficient on short 50 bp reads as well as on long 500 bp reads. Table 3  shows that the  General Overhead , the cost of iterating through hits, can represent up to a third of the total execution time. This reveals that for the simple indexing scheme we use our filtering technique is close to the optimal solution possible, in the sense that further improvements of filters would not bring much overall speed-up. However, combining state-of-the-art indexing techniques with our series of filters should achieve even better performance. We could, for example, replace our simple unique index with the multiple indexing technique used by BFAST. It would reduce the number of hits to explore, probably increasing performance even further. GASSST currently uses a simple gap and mismatch scoring scheme, whereas affine gap penalties are sometimes necessary. It could very easily be integrated in the  extend  step. Yet if the  filter  and  extend  step uses different scoring schemes, filters will no longer be guaranteed to discard only false positive alignments. This could be a problem that needs to be investigated. Another solution would be to also include affine gap penalties in the  filter  step. Although apparently problematic for our tiled NW algorithm, approximate solutions might be designed and will be the focus of future work. The experiments conducted demonstrate the efficiency of our new filtering approach. This new approach is compatible with all algorithms based on the  seed-filter-extend  strategy, so other algorithms should be able to use it resulting in a significant performance increase without any decrease in sensitivity. 
          Supplementary Material 
          
             
                Supplementary Data 
             
             
             
          
       </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>CyTOFmerge: integrating mass cytometry data across multiple panels</Title>
    <Doi>10.1093/bioinformatics/btz180</Doi>
    <Authors>Abdelaal Tamim, Höllt Thomas, van Unen Vincent, Lelieveldt Boudewijn P F, Koning Frits, Reinders Marcel J T, Mahfouz Ahmed, Wren Jonathan</Authors>
    <Abstract/>
    <Body>1 Introduction High-dimensional mass cytometry by time-of-flight (CyTOF) ( Bandura  et al. , 2009 ) allows the simultaneous measurement of over 40 protein cellular markers ( Spitzer and Nolan, 2016 ). Several studies have illustrated the value of using such a large number of markers to provide a system-wide view of cellular phenotypes at the single-cell level ( Amir  et al. , 2013 ;  Chevrier  et al. , 2017 ;  Lavin  et al. , 2017 ,  2015 ;  Newell  et al. , 2012 ,  2013 ;  van Unen  et al. , 2016 ;  Wong  et al. , 2016 ). Despite the 3-fold extension in the set of markers profiled with CyTOF compared to flow cytometry (FC), technical challenges in designing CyTOF panels limit the number of markers profiled per panel currently to about 40 markers ( Bendall  et al. , 2012 ). In many cases, the number of proteins required to describe the heterogeneity of cells far exceeds the number of markers that can be measured using a single CyTOF panel ( Bendall  et al. , 2011 ;  Chevrier  et al. , 2017 ). To overcome the limitation in the number of markers that can be measured simultaneously, a sample can be split into multiple tubes which are subsequently measured using different CyTOF marker panels ( Lee  et al. , 2011 ;  O’Neill  et al. , 2015 ;  Pedreira  et al. , 2008 ). Including a shared marker set between all panels allows the combination of measurements from all panels to produce an extended marker vector for each cell. However, there are currently no computational methods available to integrate measurements from multiple CyTOF panels. An implicit combination approach, proposed by  Bendall  et al.  (2011) , allowed the visualization of 49 markers, measured using two CyTOF panels sharing 13 markers. After clustering cells from one panel based on the set of shared markers, they overlaid the unique markers of the second panel over the obtained clusters according to the similarity between cells based on the shared markers set. This approach, however, does not explicitly merge the measurements from both panels since the clustering step is performed only on cells from one panel using the shared markers. Therefore, this approach is prone to misidentify small subpopulations of cells (as we will show later in Section 3.4). In the field of FC, two approaches have been proposed to integrate measurements from multiple FC datasets. A nearest-neighbor algorithm was used to integrate measurements from multiple FC panels assuming that each cell is almost identical to its nearest-neighbor cell, measured with a different panel, based on the overlapping markers, which we denote as the  first-nearest-neighbor  imputation ( Costa  et al. , 2010 ;  Pedreira  et al. , 2008 ;  van Dongen  et al. , 2012 ). However, the first-nearest-neighbor approach is noise-sensitive and can produce false combinations between cells from different panels resulting in artificial clusters ( O’Neill  et al. , 2015 ).  Lee  et al.  (2011)  proposed to overcome this limitation by incorporating a clustering step based on the shared markers before merging the FC measured panels, followed by enforcing the imputation of the missing markers from the same cluster, which we refer to as  cluster-based imputation . However, the larger number of unique markers per panel in the case of CyTOF, compared to FC, can result in a large number of undiscovered clusters if cells are clustered only using the set of shared markers (as we will show later in Section 3.2). An alternative approach is to divide the space of shared markers in each panel by binning biaxial scatter plots of marker pairs, each having a preset number of cells. Bins are then matched across the measured panels, and the missing markers are imputed per bin ( O’Neill  et al. , 2015 ). Although feasible for FC data, applying this method to CyTOF data, which has many more possible shared markers and many more cells, is computationally prohibitive. Moreover, the imputation strongly depends on the binning and matching step in a complex high-dimensional space. We propose a method, CyTOFmerge, that does not depend on a priori clustering or partitioning and extends measurements per cell. Our CyTOF data merging approach is based on the  k -nearest-neighbor algorithm which avoids the noise sensitivity problem by relying on a relatively large number of neighbors. In addition, we propose a method to select the most informative markers from one CyTOF panel, in order to be used as shared markers with other panels. This is particularly important given that the imputation strongly depends on the set of shared markers. By merging measurements from multiple CyTOF panels, we increase the number of markers per cell allowing for a deeper interrogation of cellular composition. 2 Materials and methods 2.1 Approach Given that the maximum number of markers on a single CyTOF panel is  N , the goal of our study is to integrate measurements from two CyTOF panels, panels A and B, given that both panels share at least  m &lt; N  markers. The remaining slots ( N−m ) on each panel can be used to measure markers that are unique to each panel. Both panels A and B measure parts of the same sample. Relying on the similarities between cells in both panels based on the shared marker set  m , we can impute markers that were not measured on panel A using the measurements from panel B, and vice versa. The resulting merged dataset extends the number of markers per cell to  2N−m , on which clustering and cell populations identification can be applied ( Fig. 1 ). We defined a  cell population  as group of cells having similar protein marker expression, these cells can represent either cells with the same type and/or state, according to which protein markers are used ( Wagner  et al. , 2016 ).
 Fig. 1. CyTOFmerge pipeline: split the sample, stain each partial sample with a different marker panel and apply CyTOF to obtain the panels’ measurements. Both panels A and B share a set of markers  m  (green).  L 1 (red) are unique markers of panel A, and  L 2 (blue) are unique markers of panel B. Both panel measurements are combined to obtain an extended markers measurements per cell, which is input to downstream computational analysis as, e.g. clustering in a t-SNE mapped domain shown here A major challenge in this approach is to determine the shared markers ( m ), i.e. which markers can preserve the heterogeneity of cell populations. To address this problem, we propose a data-driven approach ( Supplementary Fig. S1 ). Briefly, for each value of  m , we use a dimensionality reduction technique to select the best set of markers preserving the high-dimensional structure of the data. By simulating the scenario shown in  Figure 1 , the quality of an imputation is evaluated using several quantitative scores capturing clustering and neighborhood preservation, from which the minimum number of shared markers can be deduced. Full details of the selection process are described in Section 2.6. 2.2 CyTOF datasets In this study, we applied our methods to the publicly available HMIS and Vortex datasets. The HMIS dataset profiled the human mucosal immune system by measuring Peripheral Blood Mononuclear Cells (PBMCs) and intestine tissue samples from the duodenum, rectum and fistula ( van Unen  et al. , 2016 ). Using a CyTOF panel with  N  =   28 surface protein markers, a total of ∼5.2 million cells positively expressing CD45 (immune cell marker) were analyzed (3.6 million PBMCs and 1.6 million intestine tissue cells), which they down sampled to ∼1.1 million cells, randomly distributed over all PBMC and tissue cells. The marker panel included lineage markers used to differentiate between major types of immune cells, and non-lineage markers used to distinguish between different subgroups (states) of cells within each lineage. Cells were globally clustered into six main lineages: B cells (∼93 000), CD4+T cells (∼230 000), CD8+T cells (∼460 000), CD3-CD7+Innate lymphoid cells (ILCs) (∼95 000), Myeloid cells (∼117 000) and TCRγδ cells (∼88 000). Each lineage was subsequently clustered independently, resulting in 119 subgroups across all six lineages, including small clusters representing rare cell populations. The Vortex dataset is a publicly available mass cytometry data for 10 replicates of mice bone marrow cells ( Samusik  et al. , 2016 ). A total of ∼840 000 cells were measured using a CyTOF panel of  N  =   39 markers. Three cytometry experts provided a consensus clustering of 24 clusters for only ∼510 000 cells. Prior to any processing, measured marker expressions were transformed using hyperbolic arcsin with a cofactor of 5 for both datasets. 2.3 Simulating two overlapping panels We simulated the scenario of having two overlapping panels by splitting the original dataset ( D o ) into two datasets,  D A  and  D B , each measured using a different (simulated) CyTOF panel ( Supplementary Fig. S1 ). Both panels share  m  markers, and the remaining  N−m  markers from the original panel were randomly divided between the two simulated panels. The first simulated panel (A) contains  m+L 1 markers, whereas the second panel (B) contains  m+L 2 markers, where  L 1 +L 2  = N−m . Each of the two panels measures half the number of cells in the original dataset (randomly chosen without replacement), i.e. the panels measure non-overlapping cells from the original dataset. 2.4 Data imputation Data in both simulated CyTOF panels is imputed using the  k -nearest-neighbor algorithm. For each cell measured by panel A, we find the  k -most-similar cells measured by panel B using the  m  shared markers. Then, for each cell measured by panel A, the values of the missing markers ( L 2) are imputed by taking the median values of those markers from the  k -most-similar cells measured by panel B, resulting in imputed dataset  D A i . The same procedure is used to impute the values of the missing markers  L 1 from panel A to cells measured with panel B, resulting in imputed dataset  D B i . The original dataset is reconstructed ( D i ) by concatenating the two imputed datasets ( D A i  and  D B i ), and thus has the same number of cells and the same number of markers  N  as the original dataset, albeit partly imputed ( Fig. 1  and  Supplementary Fig. S1 ). 2.5 Selection of  m  shared markers Given a dataset with a panel of  N  markers, we follow three steps to choose the  m  shared markers that can be used to design follow up panels for a deeper interrogation of cells ( Supplementary Fig. S1 ): 
 Removing correlated makers.  Pearson correlation over all cells in the original dataset between each pair of markers is calculated. If the absolute value of the correlation of two markers is larger than a specified cutoff (here we use 0.7 and 0.8 as cutoffs, for the HMIS and Vortex datasets, respectively), we remove the marker which has the lower variance across all cells. 
 Dimensionality reduction . To reduce the number of markers we exploited three different dimension reduction techniques: (i) principal component analysis (PCA); (ii) Auto Encoder (AE) and (iii) Hierarchical Stochastic Neighboring Embedding (HSNE). Using PCA ( Shlens, 2005 ), the importance of a marker is based on its contribution (i.e. loading factor) to the first  m  principal components, as follows:
 (1) i p =   ∑ q = 1 m β pq 2 *   λ q 
where  i p  is the importance of marker  p ,  β pq   is the loading of marker  p  to the  q-th  principle component,  λ q  is the variance explained by the  q-th  principle component. All markers are sorted on their importance and the  m  most important markers are chosen. An AE neural network ( Hinton and Salakhutdinov, 2006 ) with one hidden layer containing  m  nodes is trained for a maximum of 50 iterations (using the Matlab toolbox for Dimensionality Reduction, drtoolbox:  https://lvdmaaten.github.io/drtoolbox/ ) until the output of the trained AE is similar (mean squared error &lt;0.75 for all values of  m ) to the original input data. We then calculate the variance of all AE output markers, sort them and select the  m  markers with the highest variance. Using HSNE ( Pezzotti  et al. , 2016 ;  Van Unen  et al. , 2017 ), we project the cells using five hierarchical layers. We represent the dataset using only the landmark cells in the top layer. On these landmark cells we apply the PCA-based reduction scheme to select the  m  markers. 
 Selecting m out of the original N markers.  Using one of the dimension reduction schemes, we select the top- m  markers to be used as shared markers. Based on the simulated datasets, we impute the missing markers in each dataset, which we compare to the original dataset using three quantitative scores introduced in the following section. By evaluating those scores over varying values for  m , we make a choice for the most suitable value of  m . 2.6 Comparing two datasets To evaluate the quality of the imputed dataset ( D i ) compared to the original dataset ( D o ), we use three different scores: (i) how well the clustering is preserved  (cluster score) ; (ii) how close the same cells in the different datasets are to each other  (distance score)  and (iii) how well the neighborhood of each cell is preserved  (nearest-neighbor score) . These scores are defined as follows: 
 Cluster score . We used the adjusted Rand-index to express the correspondence between two clustering. Briefly, it calculates the fraction of pairs of cells that end up in the same (or different) cluster in both clusterings, corrected for the random chance to end up in the same cluster (which is different for differently sized clusters). The final value is between 0 and 1. As clustering more than a million cells is too time consuming, we used an approximate cluster score for experiments where we varied either the number of shared markers ( m ) or neighbors used to impute ( k ). For these experiments, we did not cluster the imputed data  D i  but determined the cluster label of the imputed cell by a majority vote of the  k -most-similar cells in the original dataset  D o . The  approximate cluster score  is then the fraction of cells where the estimated cluster label was the same as the cluster label of the original cell:
 (2) Approximate   Cluster   Score =   number   of   cells   having   matched   cluster   labels total   number   of   cells 
 Distance score .  To evaluate how similar the measurements of cells across two datasets are, we calculate the Euclidean distance, in the full marker space, between the measurements of a cell  c n i , the  n -th cell in the imputed dataset  D i , and the corresponding cell  c n o , the same ( n -th) cell in the original dataset  D o . This is done for all cells, and from that the median distance (md) is taken. To make the score independent of the scale of the original dataset  D o , we compare this median distance (md) to the average distance (ad) between all pairs of cells within the original dataset  D o , as follows:
 (3) Distance   Score =   ( ad   -   md ) ad 
 Nearest-Neighbor score . To evaluate the preservation of the neighborhood of cells across datasets, we measure, for each cell  c n o , the Euclidean distance in the full marker space to the nearest-neighboring cell ( d n ) in the original dataset  D o , and the distance between both representations of that cell,  c n o  and  c n i , in the original  D o  and imputed  D i  datasets ( d p ). The local neighborhood is preserved when the imputed version of the cell  c n i  is closer to  c n o  than its nearest neighbor in the original dataset  D o , i.e.  d p  &lt;  d n . The nearest-neighbor score is then the fraction of cells for which this holds.
 (4) NN   Score =   number   of   cells   where   ( d p &lt;   d n ) total   number   of   cells We used the base 2 logarithm of the Jensen–Shannon divergence (JSD) to quantify the similarity between the distributions of a marker in the original and imputed dataset, resulting in values between zero (identical distributions) to one (totally disjoint distributions). The JSD between two distributions  P ( x )  and  Q ( x )  is:
 (5) JSD = 1 2 ∑ x P x   log 2 P x M x + 1 2 ∑ x Q x   log 2 Q x M x (6) M x = 0.5 * ( P x + Q x ) 2.7 Finding clusters We clustered both datasets, HMIS and Vortex, with Phenograph, a neighborhood graph-based clustering tool designed for automated analysis of mass cytometry data ( Levine  et al. , 2015a ). Phenograph is applied to the original and imputed datasets, using the R implementation with default settings (number of neighbors =30). More fine-grained cluster annotations for the HMIS datasets are acquired using Cytosplore ( www.cytosplore.org ), a tool specifically designed for the analysis of mass cytometry data ( Höllt  et al. , 2016 ;  Van Unen  et al. , 2017 ). Briefly, cells are embedded into a 2D map using t-Distributed Stochastic Neighbor Embedding (t-SNE) ( Pezzotti  et al. , 2017 ;  van der Maaten and Hinton, 2008 ), and subsequently clustered using a density-based Gaussian Mean Shift algorithm ( Comaniciu and Meer, 2002 ) using a relatively small density kernel ( σ  = 20–23), resulting in over-clustering of the data. Clusters are then manually merged when they have highly similar marker expression profiles (median value of each marker per cluster). 3 Results 3.1 Selecting the set of shared markers To determine the shared markers that can be used to combine two CyTOF datasets, we simulated the scenario of having two overlapping panels with different sets of shared markers  m , on which we applied our data imputation approach with different number of neighbors  k  ( Supplementary Fig. S1 ). We investigated how the imputation of the two panels is influenced by: (i) the dimension reduction technique used to select the shared markers, (ii) the data (lineages) used to select the markers, (iii) the number of shared markers ( m ) and (iv) the number of nearest neighbors used during imputation ( k ). In the HMIS dataset, the method used to select the shared markers has limited influence on the results.  Figure 2  shows which markers are selected by the different marker selection schemes (PCA, AE and HSNE) when changing the number of selected shared markers ( m ) from 4 to 25 and applied on the 5.2 million cells. In the preprocessing step, CD8b and CD11b were removed from the selection as they are highly correlated with CD8a and CD11c (correlation of 0.843 and 0.705, respectively), leaving 26 markers to choose from. There are small differences in the selection profiles between the three methods, with a maximum of two mismatches. For 14 &lt;  m  &lt;   17, the same set of shared markers is selected by all three methods. In terms of computation time, PCA outperforms the AE and the HSNE (100× and 480×, faster on the same machine, respectively).
 Fig. 2. Shared markers for the HMIS dataset. The selected markers that can best represent the dataset using ( A ) PCA, ( B ) AE and ( C ) HSNE (marker ordering is based on the PCA selection profile, black is selected, white is not selected) We checked whether the marker selection procedure is influenced by the type of cells. Therefore, we applied the PCA-based marker selection on PBMCs and tissue cells independently.  Supplementary Figure S2  shows that there is little difference in the selected set of markers when using the PBMC, tissue or PBMC+tissue samples. Next, we assessed the quality of the subsequent imputed dataset for each lineage individually, as well as all six lineages together, for  m  =   4–25 and  k  =   50. For all three evaluation scores, the performances improve when the number of shared markers increases ( Supplementary Fig. S3A–C ). All performance scores seem to saturate at  m  =   16 ( Supplementary Fig. S4A–F ), i.e. they exceed 80% of the maximal score.  Table 1  shows the values of the three quality measures at  m  =   16, for each individual lineage and the six lineages together.
 Table 1. Evaluation scores for the 16 selected shared markers for the 1.1 million cells HMIS dataset 
 Approximate cluster score (%) 
 
 Distance score (%) 
 
 Nearest neighbor score (%) 
 CD4+T cells 92.3 84.3 94.5 CD8+T cells 91.9 83.9 93.1 B cells 91.8 82.0 92.8 CD3–CD7+cells 89.3 83.4 92.6 TCRγδ cells 86.2 84.1 94.7 Myeloid cells 86.2 80.4 82.5 All cells 89.4 87.4 91.9 A common measure to assess the quality of imputation is to investigate the correlation between the original and imputed values. However, this approach turned out not to be appropriate for our data since many markers are being expressed only in a specific population of cells. As a result, the correlation is relatively high for markers that are high expressed over multiple cell populations ( Supplementary Figs S5 and S6 ), but the correlation is low for cell-population specific markers (such as, e.g. the CD123 marker which is high expressed only in the CD4+T cells lineage). These cell-population specific markers are imputed correctly (low values for most cells and higher values for the cell-population specific cells), but the noise on the abundant low values dominates, causing a low correlation. Consequently, we decided not to use the correlation as a quantitative score to evaluate how well an imputed dataset resembles an original dataset. We further investigated the distribution of the non-shared (imputed) marker by comparing the distributions of the original values with those of the imputed values for each non-shared marker per cell population, and quantify the similarity using the JSD (Section 2.6). Across all the 12 non-shared markers, we obtained low JSD values (&lt;0.2) showing a high similarity between the original and imputed values ( Supplementary Fig. S7A ). The imputation process does exclude the outlier values, as we use the median value from the 50 most similar cells, which results for some markers, in ‘compressed’ distributions as compared to the original ones ( Supplementary Fig. S7B and C ). Next, we investigated the effect of the choice of the number of neighbors ( k ) used when applying the  k -nearest-neighbor imputation.  Supplementary Figure S4A–F  shows the  approximate cluster score  for  k  = {1, 10, 50, 100, 200, 250, 300, 500, 1000}, with  k  =   50 clearly showing the highest performance across all lineages, even over different numbers of shared markers. We observed similar results when applying all these analyses to the Vortex dataset: (i) small differences between PCA, AE and HSNE when  m  is ranging from 4 to 38 ( Supplementary Fig. S8 ), (ii) improving and saturating performance scores with increasing number of shared markers ( Supplementary Fig. S3D ) and (iii) highest performance when  k  =   50 is used during imputation ( Supplementary Fig. S4G ). The saturation for the number of shared markers occurs at  m  =   11, with the  approximate cluster score ,  distance score  and  nearest-neighbor score  being 95.3, 84.0 and 82.1%, respectively. 3.2 CyTOFmerge reproduces original cell populations and outperforms FC imputation methods To demonstrate the feasibility of our computational method to combine data measured from multiple CyTOF panels, we investigated the quality of the clustering of the imputed dataset. First, the original 1.1 million cells HMIS dataset was clustered on the full marker space using Phenograph, resulting in 52 clusters of cells divided into: 6 B cell populations, 8 CD4+T cell populations, 15 CD8+T cell populations, 6 CD3-CD7+ ILC populations, 7 Myeloid populations, 5 TCRγδ cell populations and 5 unknown populations donated as Others ( Supplementary Fig. S9 ). These 52 clusters are used as a baseline for comparison with the imputed datasets. We applied the panel combination and imputation method using  k  =   50 and  m  =   16, thus imputing 12 markers (6 unique markers for panel A, and 6 unique markers for panel B). The imputed dataset was clustered on the full marker space using Phenograph, resulting (coincidentally) in 52 clusters with slight variation in the number of clusters per cell lineage ( Supplementary Fig. S10A ). To evaluate the imputation, we matched the imputed clusters to the original clusters using the maximum pairwise Jaccard index. The cluster matching shows that all imputed clusters match to original clusters within the same lineage ( Supplementary Fig. S10B ). Next, we calculated the adjusted Rand-index representing how similar both clusterings are ( Table 2 ).
 Table 2. Comparison between CyTOFmerge and FC merging methods on the 1.1 million cells HMIS dataset 
 Adjusted Rand-index 
 
 Distance score 
 
 Nearest neighbor score 
 CyTOFmerge — — —  HMIS,  m = 16,  k =50 0.81 87.4% 91.9%  Vortex,  m =11,  k =50 0.90 84.0% 82.1% First-nearest-neighbor — — —  HMIS,  m =16,  k =1 0.77 83.5% 75.6%  Vortex,  m =11,  k =1 0.93 77.9% 51.6% Shared markers clusters — — —  HMIS,  m =16 0.68 n.a n.a  Vortex,  m =11 0.79 n.a n.a Cluster-based imputation — — —  HMIS,  m =16,  k =50 0.80 87.4% 91.8%  Vortex,  m =11,  k =50 0.84 84.0% 82.1% n.a=not applicable. To compare with the first-nearest-neighbor approach proposed by ( Pedreira  et al. , 2008 ), we applied the imputation method using  k  =   1, using the same set of 16 shared markers. Phenograph clustering of that imputed dataset on the full marker space resulted into 53 clusters ( Supplementary Fig. S11 ) with a lower performance compared to CyTOFmerge using  k  =   50 ( Table 2 ). Next, we compared the performance of CyTOFmerge to that of the cluster-based imputation method proposed by  Lee  et al.  (2011) . In this approach, clusters are first determined using the shared markers followed by imputation of the unique markers in each panel  within  the same cluster. We clustered the cells using the 16 shared markers for the entire dataset using Phenograph and obtained 42 cell clusters, 10 clusters less than the original dataset clustering ( Supplementary Fig. S12 ). When comparing with the original clustering ( Table 2 ), we observed a relatively large drop in the adjusted Rand-index. Hence, clustering based on the shared markers only could not identify a large part of the original clustering using all markers. However, when we performed the combination of the two panels using the cluster-based imputation, we obtained comparable performance with CyTOFmerge ( Supplementary Fig. S13 ,  Table 2 ). We also tested CyTOFmerge on the Vortex dataset, using  m  =   11 shared markers and  k  =   50, now imputing 28 markers (14 unique per panel). Phenograph clustering of the original dataset gave 31 clusters ( Supplementary Fig. S14 ), while clustering the imputed dataset resulted in 28 clusters ( Supplementary Fig. S15 ). The adjusted Rand-index was relatively high, i.e. 0.90 ( Table 2 ). Next, we applied first-nearest-neighbor approach, and we clustered the resulting imputed dataset resulting in 29 clusters. The first-nearest-neighbor has slightly higher adjusted Rand-index compared to CyTOFmerge, however, we observed a large drop in the  distance  and the  nearest-neighbor scores  ( Table 2 ). Moreover, confirming our previous observation, the clustering of the shared markers only produces 23 clusters, 8 clusters less than the original dataset clusters, with a relatively large drop in the adjusted Rand-index when compared to the original clustering. Finally, the cluster-based imputation method produces 29 clusters. Compared to CyTOFmerge, the cluster-based imputation method shows comparable  distance  and  nearest-neighbor scores , but lower adjusted Rand-index ( Table 2 ). To obtain a baseline evaluation for the imputed data clustering performance, we permutated the non-shared markers across all cells, while keeping the shared markers values the same. Next, we clustered this permuted dataset in the full marker space using Phenograph and compared the clustering result with the original dataset clustering. The permuted dataset clustering had an adjusted Rand-index of 0.56 ± 0.02 and 0.50 ± 0.01 (across 10 different random permutation), for the HMIS and Vortex datasets, respectively. These results show that random estimation of the non-shared markers decreases the clustering performance compared to clustering using the shared markers only, i.e. adding more dimensions does not improve the clustering performance. This also implies that CyTOFmerge adds real structure by providing good estimation for the non-shared markers, leading to an improved clustering. 3.3 Reproducible cell populations at a deeper annotation level using CyTOFmerge We proceeded by evaluating the quality of CyTOFmerge when using a fine-grained clustering to investigate whether rare (small) cell populations could be identified from the imputed data. As a baseline for comparison, we clustered the six immune lineages from the original 1.1 million cells HMIS dataset individually, on the full marker space using Cytosplore, resulting in 121 clusters in total, including: 17 CD4+T cell populations, 21 CD8+T cell populations, 16 B cell populations, 34 TCRγδ cell populations, 24 CD3-CD7+ILC populations and 9 Myeloid cell populations ( Fig. 3A ,  Supplementary Fig. S16A ). The imputed dataset (with  m  =   16) was similarly clustered using Cytosplore into the same number of populations (121) for the six immune lineages ( Fig. 3B ,  Supplementary Fig. S16B ).
 Fig. 3. Clustering of the original and the imputed datasets. ( A–C ) t-SNE maps showing the different identified populations in the CD4+T Cells lineage. ( A ) Shows the populations of the original data. ( B ) The populations of the imputed data (for  m  =   16,  L 1   =   6 and  L 2   =   6). ( C ) The mapping of the original clusters labels on the t-SNE map of the imputed data. ( D ) Heatmap of markers expression for the 121 characterized immune cells populations of the original dataset for  m  =   16. Black-to-yellow scale shows the median arcsinh-5 transformed values for the markers expression. Markers colors indicate whether a marker is shared between panels or unique to a single panel, during panels combination (red is shared, green is unique to panel A, blue is unique to panel B) The clusters from the imputed dataset were correctly matched to the baseline clusters for all 121 cell populations across the six lineages, including large clusters as well as small rare clusters, such as: population 16 and 17 in the CD4+T Cells ( Fig. 3A and B ), population 21 in the CD8+T Cells, population 16 in the B Cells, populations 3 and 34 in the TCRγδ Cells and populations 23 and 24 in the CD3-CD7+Cells ( Supplementary Fig. S16A  and B). The imputed expression profiles of the 121 populations are remarkably similar (average correlation of 0.998) to the expression profiles of the corresponding baseline clusters ( Supplementary Fig. S17A  and  Fig. 3D , respectively). Also, the Jaccard index showed a clear diagonal between the original and the imputed clusters ( Supplementary Fig. S18 ). To gain more insight into the distribution of the original cluster labels in the imputed space, we colored each cell in the imputed data according to baseline cluster they belonged to.  Figure 3C  and  Supplementary Fig. S16C  show that the imputed measurements for each cell are indeed faithfully reconstructed, i.e. after mapping them they are distributed similarly as in the original data. More quantitatively, the imputation had an overall adjusted Rand-index of 0.81 for all the 121 cell populations. Per individual lineage, the adjusted Rand-index varied between 0.77 and 0.83 for the different lineages ( Table 3 ). Since we rely on Gaussian Mean Shift clustering in the t-SNE space, part of the error in clustering the imputed data is caused by the stochastic nature of the t-SNE algorithm (due to random initializations). The clustering reproducibility between two t-SNE mappings of the original data ( Table 3 ,  Supplementary Fig. S19 ) varied between 0.82 and 0.96, with variance estimates (when repeating the procedure 10 times) in the order of 8 e –5 ( Table 3 , for Myeloid and TCRγδ cells). Hence, the quality of the imputed clustering is close to the quality of repeated t-SNE mappings, with a difference of 0.06 in the adjusted Rand-index for all cells.
 Table 3. Adjusted Rand-index of the imputed data at  m  =   16 and for repeated t-SNE mappings of the original data Imputed data t-SNE rerun CD4+T cells 0.78 0.86 CD8+T cells 0.79 0.84 B cells 0.83 0.85 CD3–CD7+cells 0.78 0.82 TCRγδ cells 0.77±8 e −5 0.89±1 e −4 Myeloid cells 0.82±7 e −5 0.96±6 e −5 All cells 0.81 0.87 To further evaluate the effects of imputation on downstream analysis, we compared the population frequencies of the 121 cell populations, estimated using both the original and the imputed datasets. The result shows that population frequencies are accurately estimated from the imputed data as compared to the original data, with an overall correlation of 0.985 ( Supplementary Fig. S17B ). 3.4 Imputation improves the differentiation of cell populations We have shown that from the imputed data similar clusters of cells can be found as when using the original data. But, can we find clusters from the imputed data that we cannot find in the two separate panels? Hereto, we overlaid the original cluster labels of the HMIS TCRγδ lineage populations onto t-SNE maps constructed using: (i) only the 22 measured markers of a panel (16 shared+6 unique markers), (ii) the original 28 measured markers and (iii) the imputed dataset (16 shared+6 unique+6 imputed). This was done for both panels A and B separately ( Figs 4 and 5 , respectively).
 Fig. 4. Marker panel extension impact on the identification of distinct populations in the TCRγδ immune lineage—panel A. ( A ) The Reduced t-SNE map using only 22 markers. ( B ) The original t-SNE map using the original 28 markers. ( C ) The imputed t-SNE map using 28 markers of which 6 are imputed from panel B. All three maps are colored with the original population labels. ( D ) Shared and missing markers expression profiles are shown on the original t-SNE map. The map border color indicate whether a marker is shared between panels or unique to a single panel (red is shared, green is unique to panel A, blue is unique to panel B and thus missing markers for panel A).The color bar shows the arcsinh-5 transformed values for the markers expression Fig. 5. Marker panel extension impact on the identification of distinct populations in the TCRγδ immune lineage—panel B. ( A ) The Reduced t-SNE map using only 22 markers. ( B ) The original t-SNE map using the original 28 markers values. ( C ) The imputed t-SNE map using 28 markers of which 6 are imputed from panel A. All three maps are colored with the original populations labels. ( D ) Shared and missing markers expression profiles are shown on the original t-SNE map. The map border color indicate whether a marker is shared between panels or unique to a single panel (red is shared, green is unique to panel A and thus missing markers for panel B, blue is unique to panel B).The color bar shows the arcsinh-5 transformed values for the markers expression For panel A, populations 6 and 8 are merged in one cluster when we map the data using only the 22 panel markers ( Fig. 4A ), whereas the original and imputed data separate those two clusters ( Fig. 4B and C , respectively). To better understand this behavior, we overlaid the expression of the markers across the t-SNE map ( Fig. 4D ). CD8b has higher expression (mean±std =3.205 ± 0.797) for cells in cluster 6 as compared to cluster 8 (0.584 ± 0.663) and is missing in panel A, hence resulting in not being able to separate clusters 6 and 8. For the imputed data, the missing marker for panel A is imputed by its measurements on panel B, with which both clusters can indeed be separated ( Fig. 4C ). Likewise, for the data from panel B, clusters 12 and 31 are merged in one cluster ( Fig. 5A ), because NKp46 is missing on panel B ( Fig. 5D ) with cells having a higher expression in cluster 31 (2.728 ± 0.712) compared to 12 (0.505 ± 0.586). Also, clusters 7 and 14 are merged due to the lack of the TCRγδ marker ( Fig. 5D ). For both situations, the clusters are separated when the data from panel B is imputed with data from panel A ( Fig. 5C ). Similar observations can be made for the other lineages ( Supplementary Figs S20–S24 ). For example, for both the CD8+T ( Supplementary Fig. S20 ) and Myeloid ( Supplementary Fig. S21 ) lineages, the CRTH2 marker makes a difference between clusters based on one panel-only data compared to data from combined panels. For some lineages, the clustering based on individual panels does, however, closely match the clustering on the original data. Either the missing markers are not important (e.g. CD11b in panel A of the CD8+T cells,  Supplementary Fig. S20 ), or they are important but highly correlated with one of the shared markers (e.g. CD14 in panel B of the Myeloid cells,  Supplementary Fig. S21 , has a similar expression to CD38). To quantitatively assess the ability to differentiate between cell populations based on different sets of markers, we tested the ability of a two-class Linear Discriminant Analysis classifier ( Abdelaal  et al. , 2019 ), to differentiate between populations 6 and 8 in the TCRγδ cells. We evaluated Linear Discriminant Analysis’ performance using only the 16 shared markers, all 28 markers from the TCRγδ imputed data and all 28 markers from the TCRγδ original data. We obtained the highest performance using all markers from the original data, with an accuracy of 95.74 ± 0.70%. The lowest performance was obtained when using only the 16 shared markers (accuracy= 70.37 ± 1.07%). Using all markers from the imputed data resulted in an accuracy of 83.46 ± 1.13%, which is less than the original data, as expected, but showing a strong improvement over the shared markers. This confirms our previous conclusion that the imputation improves over the shared markers, despite the fact that the imputation relies on the shared markers. We obtained similar results for populations 12 and 31, and populations 7 and 14 ( Supplementary Fig. S25 ). 4 Discussion We demonstrated the feasibility of combining data from different CyTOF panels with a set of shared markers in common. We showed that by imputing data, the heterogeneity of the data can be better captured than with the individual panels separately. Also, we presented a data-driven approach to select the set of shared markers that are most informative to be used to align panels. The selected set of shared markers can capture the underlying structure of the data. For example, from the HMIS dataset we saw that for small values of  m , the selected shared markers include CD3, CD4 and CD8a which separate the main CD4+ and CD8+T cells immune lineages from the rest of the cell populations. As  m  increases, the selection algorithm starts to include markers that differentiate the different populations within a single lineage. Our selection approach relies on the variation in expression across cells. As a result, CD45, an essential marker which is positively expressed across all immune cells, is never selected due to its low variance. To assess the quality of imputation, we relied on three scores that capture the cluster and neighborhood concordance between the imputed and original data. For the HMIS dataset, we observed prominent discordance when a low number of shared markers is used ( m  &lt;   12), mainly due to exclusion of key lineage specific markers within the set of shared markers resulting in imputation failures. The number of shared markers to properly align panels does depend heavily on the complexity and heterogeneity of the data. For the HMIS dataset, studying PBMCs and tissue samples from patients with three different inflammatory bowel diseases as well as controls, 16 shared markers were needed. Whereas for the Vortex dataset, that replicated mouse bone marrow samples, 11 markers were sufficient. On the other hand, we saw that for both datasets we can capture and reconstruct all cell clusters, despite their number and sizes, suggesting that the imputation is not biased toward the clustering. Although the performances do differ for different settings of the number of shared markers ( m ) and number of neighbors used during imputation ( k ), they are not sensitive to the exact setting, illustrating the robustness of CyTOFmerge. Note that during the shared maker selection procedure we represented highly correlated markers by only one representative marker. We made this choice because highly correlated markers will get the same importance by the PCA selection scheme, and thus might be selected together. Selecting a highly correlated marker as an additional shared marker will, however, not add any information to the shared makers, while, at the same time, occupying a marker slot in the panel. To reduce this redundancy and free as many slots as possible on the panel we made the choice to represent highly correlate makers with only one marker. Clearly, the choice for the threshold plays an important role as when the correlation is lower the markers will also add more distinct information. We have shown that by imputing more markers, it is possible to better differentiate between cell populations, but on the other hand, the imputation of markers does affect the quality of the downstream analysis when compared to non-imputed data. We saw that clustering of the imputed data is not perfectly similar to the original data (adjusted Rand-index &lt;1). Indeed, this is affected by the homogeneity of the dataset, as we saw higher performance for the Vortex datasets compared to HMIS (Vortex being more homogenous). Generally, the number of shared markers will affect the downstream analysis, i.e. increasing the number of shared markers will increase the quality of the imputation, and the downstream analysis will more faithfully resemble analyses done on non-imputed data. But that will also restrict the number of unique marker slots available on each panel. Using less shared markers will increase the number of unique markers, which in turn will increase the capacity to capture more heterogeneity, but at the expense of imputation quality. This trade-off is being influenced by the local structure (homogeneity) in the data, which is, unfortunately, hard (or even impossible) to predict beforehand, in general. Compared to FC methods, CyTOFmerge outperformed the first-nearest-neighbor method, and achieved comparable performance with the cluster-based imputation. The later shows that the pre-clustering step of the shared markers is unnecessary, as the imputation through the entire data using CyTOFmerge produces similar results. Further, we demonstrated that by imputing more markers, we obtained better differentiation between different cell populations. However, the imputation depends on how similar cells are in the shared markers space, indicating that the variation between populations that can only be differentiated based on imputed (non-shared) markers is to some extent retained in the shared markers. To practically apply CyTOFmerge, we recommend the following steps: (i) collect the samples and divide them in two parts. (ii) Design the first marker panel according to the biological question one wants to be answered. The marker panel would probably contain lineage markers, to differentiate between the major cell types, and cell state markers, for more detailed subtyping, and intracellular markers of interest ( Bendall  et al. , 2011 ). (iii) Stain the first part of the samples with the designed marker panel and measure the samples with CyTOF. (4) Apply the marker selection pipeline on the measured dataset using the first panel and obtain the most informative markers (i.e. shared markers). (5) Include those shared markers while designing the second panel of marker. (6) Add extra state or intracellular markers of interest to the second panel. (7) Stain the second part of the samples with the second marker panel and measure the samples with CyTOF. (8) Apply the imputation algorithm to all samples, combining both datasets from both panels, and create the imputed dataset in which each cell is represented by the unique markers from each panel (one of which is imputed), as well as the shared markers. Importantly, we have shown that by combining panels a richer protein profile of cells can be acquired with which it becomes possible to find both abundant as well as rare cell populations. This opens possibilities to merge even more panels based on a common shared marker set as there is no fundamental limit to restrict to the combination of two panels. Funding This work was supported from the European Commission of a H2020 MSCA award under proposal number [675743] (ISPIC). 
 Conflict of Interest:  none declared. Supplementary Material btz180_Supplementary_Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>HTqPCR: high-throughput analysis and visualization of quantitative real-time PCR data in R</Title>
    <Doi>10.1093/bioinformatics/btp578</Doi>
    <Authors>Dvinge Heidi, Bertone Paul</Authors>
    <Abstract>Motivation: Quantitative real-time polymerase chain reaction (qPCR) is routinely used for RNA expression profiling, validation of microarray hybridization data and clinical diagnostic assays. Although numerous statistical tools are available in the public domain for the analysis of microarray experiments, this is not the case for qPCR. Proprietary software is typically provided by instrument manufacturers, but these solutions are not amenable to the tandem analysis of multiple assays. This is problematic when an experiment involves more than a simple comparison between a control and treatment sample, or when many qPCR datasets are to be analyzed in a high-throughput facility.</Abstract>
    <Body>1 INTRODUCTION Quantitative real-time polymerase chain reaction (qPCR) is widely used for the detection of specific nucleic acids, measurement of RNA transcript abundance and validation of high-throughput experimental results. qPCR is often performed in standard 96-well plates, and newer instruments can utilize higher density formats. These include the Roche LightCycler, which can accommodate 384-well thermocycler blocks, and the Applied Biosystems TaqMan machines employing 384-well Low Density Array (TLDA) micro-fluidic cards. The technology relies on fluorescence data as a measure of RNA or DNA template concentration, represented by cycle threshold (C t ) values determined at the initial phase of exponential amplification. The calculation of fold changes between genes often entails only limited comparisons of C t  values across two conditions, and omits statistical testing of the significance of observed differences. We have developed a package for high-throughput analysis of qPCR data ( HTqPCR ) within the R/Bioconductor framework (Gentleman  et al .,  2004 ). The software performs quality assessment, normalization, data visualization and statistical significance testing for C t  values between features (genes and microRNAs) across multiple biological conditions, such as different cell culture treatments, comparative expression profiles or time-series experiments. 2 SOFTWARE FEATURES HTqPCR  is developed for the R statistical computing environment ( www.r-project.org ), will run on all major platforms and is available as open source. Core R and Bioconductor packages are the only software dependencies and the package includes a detailed tutorial. 2.1 Data input requirements The input data format consists of tab-separated text files containing C t  values, feature identifiers (genes, microRNAs, etc.) and other (optional) information. Data files can be user-formatted plain text or the direct output of Sequence Detection Systems (SDS) software. Internally, this information is embodied as instances of the  qPCRset  class, which are analogous to the  ExpressionSet  objects typically used to represent fluorescence data in microarray analyses. 2.2 Visualization features HTqPCR  contains multiple functions for data visualization. Subsets of genes across one or more samples can be represented in bar plots, displaying either absolute C t  values or fold changes compared with a calibrator sample ( Fig. 1 ). Data quality control across samples can be assessed via diagnostic aids such as density distributions, box plots, scatterplots and histograms, some of which can be stratified according to various attributes of the features ( Fig. 2 ). When qPCR assays are performed in multiwell plates or another spatially defined layout, the C t  values can be plotted accordingly to visualize any spatial artifacts such as edge effects ( Fig. 3 ). Clustering of samples or genes can be performed using principal component analysis, heatmaps or dendrograms.
 Fig. 1. Log 2  ratios between the normalized C t  values for four different sample groups, relative to the calibrator (Group 1; ratio=0.0). Error bars indicate the 90% confidence interval compared with the average calibrator C t . 
 Fig. 2. Box plot of C t  values across all samples, stratified based on the class membership of each gene ( A ) and the distribution of C t  values across samples after normalization using three different methods ( B ). 
 Fig. 3. C t  values for a typical qPCR assay performed in 384-well format. Gray wells overlaid with crosses were flagged as ‘Undetermined’. 2.3 C t  quality control Individual C t  values are a principal source of uncertainty in qPCR results. This can arise due to inherent bias in the amplification conditions (variable primer annealing, amplicon sequence content, suboptimal reaction temperature or salt concentration, etc.), or when initial template concentrations are insufficient to generate copy numbers exceeding the minimum detection threshold. In  HTqPCR , the reliability of C t  values can be assessed either individually or across replicates. Through user-adjustable parameters all values are flagged as one of ‘OK’, ‘Undetermined’ or ‘Unreliable’, and this information is propagated throughout the analysis. Non-specific filtering can be applied to remove genes that are marked ‘Undetermined’ and/or ‘Unreliable’ across samples, or those having low variability (i.e. not differentially expressed) after normalization. 2.4 Data normalization The qPCR data are often normalized by subtracting average C t  values from those of predetermined housekeeping genes, producing a Δ C t  readout (Livak and Schmittgen,  2001 ). More sophisticated normalization procedures are also implemented in  HTqPCR , for use when housekeeping genes are not present or not reliably expressed. Three different normalization options are available in  HTqPCR : (i) rank-invariant features across the experiment can be used to scale each sample; (ii) quantile normalization can be performed to produce a uniform empirical distribution of C t  values across samples; and (iii) a pseudo-mean or -median reference can be defined, rank-invariant features for each sample are identified, and a normalization curve is generated by smoothing the corresponding C t  values ( Fig. 2 B). For the rank-invariant methods, low-quality C t  values can also be excluded when calculating a scaling factor or normalization curve, thereby avoiding additional bias. 2.5 Statistical testing Assuming normally distributed C t  values and equal variance across sample groups being compared, fold-change significance can be assessed in two ways: applying a  t -test between two conditions, or using methods from the  limma  package (Smyth,  2005 ) for more sophisticated comparisons. Information about the quality of each feature (‘OK’, ‘Undetermined’ or ‘Unreliable’) across biological and technical replicates is summarized in the final results. 3 CONCLUSIONS Efficient data processing is required for the use of high-throughput qPCR applications.  HTqPCR  is a software package amenable to the analysis of high-density qPCR assays, either for individual experiments or across sets of replicates and biological conditions. Methods are implemented to handle all phases of the analysis, from raw C t  values, quality control and normalization to final results. As the software is R based, it runs on different operating systems and is easy to incorporate into an analysis pipeline, or used in conjunction with other tools available through the Bioconductor project. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>MAJIQ-SPEL: web-tool to interrogate classical and complex splicing variations from RNA-Seq data</Title>
    <Doi>10.1093/bioinformatics/btx565</Doi>
    <Authors>Green Christopher J, Gazzara Matthew R, Barash Yoseph, Hofacker Ivo</Authors>
    <Abstract/>
    <Body>1 Introduction Advances in RNA-Seq technology have led to improved detection and quantification of splicing variations through the use of short reads that span across spliced junctions. Most commonly used AS analysis tools focus exclusively on classical, binary AS events (e.g. cassette exon, alternative 5′ or 3′ splice sites, intron retention, etc.). Recently, we formulated local splicing variations (LSVs) that capture both classical as well as complex splicing patterns (i.e. involving three or more junctions). Briefly, LSVs can be thought of as splits in a gene’s splice graph where exons are nodes and splicing of pre-mRNA segments are edges. In this formulation LSVs capture several optional (alternative) pre-mRNA segments that the spliceosome may splice to a reference exon up or downstream. Figure 1A  illustrates such an LSV with the reference exon marked in gray and several downstream alternative exons along with the matching LSV edges colored in red, blue and green. Such an LSVs is considered complex as it involves more than two alternative junctions. Importantly, we found that over 30% of splicing variations in extensive human and mouse RNA-Seq experiments we interrogated are complex ( Vaquero-Garcia et al. , 2016 ).
 Fig. 1 ( A ) Splice graph representation of LSV within Clta  from mouse cerebellum (top) and adrenal gland (bottom) with reads detected from RNA-Seq data displayed above each junction. Junctions quantified directly within the LSV are colored. ( B ) Primer table (bottom) that suggests possible forward (left) and reverse (right) primers. Additional information for each primer can be displayed by clicking the ‘i’ icon as shown below the black cursor. Various primer filters can be applied (top) for additional, on the fly, filtering. ( C ) UCSC Genome Browser snapshot with custom tracks produced by MAJIQ-SPEL as labeled. SPEL opens those when clicking the Genome Browser logo shown on the left. ( D ) Isoform table that displays PSI ( Ψ ∈ [ 0 , 1 ] ) quantifications (left) and possible isoforms associated with each LSV edge (right). Note that as illustrated here complex LSVs may have a single PSI capturing multiple isoforms and that similarly PSI captures the fraction of a splicing event (edge), not necessarily the fraction of each colored exon. Fraction of each Nucleotide sizes correspond to products produced using the selected primers from (B). ( E ) Representative RT-PCR validation of predicted product sizes and quantification using the primers selected in (B) on total RNA from mouse cerebellum (left) and adrenal gland (right) The pervasiveness of complex splicing variations suggests that accurate interpretation of the underlying isoforms is crucial for experimentally interrogating and understanding the consequences of these splicing changes. We therefore developed MAJIQ and VOILA ( http://majiq.biociphers.org ) to define, quantify and visualize LSVs ( Vaquero-Garcia et al. , 2016 ). LSVs visualization is based on segments of splice graphs as shown in Figure 1A , while quantification is based on PSI (Percent Selected Index, Ψ ∈ [ 0 , 1 ] ) which captures the marginal fraction of each LSV edge (i.e. the fraction of isoforms that utilize this splicing junction). Similarly, changes between experimental conditions are measured by dPSI ( Δ Ψ ∈ [ − 1 , 1 ] ). However, no current tool offers a user-friendly interface to connect LSVs, whether simple or complex, to the underlying known gene isoform and affected protein domains. Also, there is a clear need for automated design and visualization of potential primers that flank an LSV for experimental validation via RT-PCR, the gold standard in the field. Specifically, previous work only allows for design of a single primer pair and focuses on classical, binary AS events ( Tokheim et al. , 2014 ). 2 Results We developed the web-tool MAJIQ-SPEL (MAJIQ for Sampling Primers and Evaluating LSVs) to aid in the visualization, interpretation and experimental validation of both classical and complex splicing variations. Typically MAJIQ and its visualization package VOILA ( Vaquero-Garcia et al. , 2016 ) are executed by users on datasets ranging from just a few to hundreds or thousands of RNASeq samples to detect local splicing variations (LSV) of interest. SPEL can then analyze LSVs of interest from such large executions which quantify PSI (quantification of a single experimental group) or delta PSI (quantification of splicing changes between two experimental groups). MAJIQ-SPEL (or SPEL for short) is implemented on a Galaxy web server ( Afgan et al. , 2016 ) and takes as input the output of VOILA ( Vaquero-Garcia et al. , 2016 ). Specifically, users can now click a button to copy a splice graph and LSV quantification of interest, then paste it into SPEL’s Galaxy input form and run the analysis. SPEL is intended to be used primarily as a Galaxy web-tool SPEL but we also made it available as a stand-alone version. The stand-alone is light on memory and CPU, taking about 0.5 s and 24 MB of memory per job on a standard laptop. MAJIQ-SPEL output contains several components, which we highlight in Figure 1  using a complex LSV within Clta  generated comparing RNA-Seq from mouse cerebellum and adrenal gland ( Zhang et al. , 2014 ). First, colorized representations of the LSV are displayed with junction spanning read counts for each junction quantified directly in the LSV (colored arcs in Fig. 1A ). Also shown are counts for junction spanning reads that occur within the boundaries of the event, but are not part of the LSV quantified (dashed grey arcs). This visualization allows for quick interpretation of which paths are commonly utilized in each sample. We note that the ratio of the colored read counts usually correspond approximately to the expected PSI ( E [Ψ]) but may vary from it due to various normalization factors applied during quantification (GC content, stack removal etc.). Second, SPEL produces a table of putative forward and reverse primers for the 5 ′ - and 3 ′ -most exons within the LSV ( Fig. 1B ). The primers are optimized for validating the given LSV via low-cycle RT-PCR, based on the experimental protocols and primer design factors described in ( Smith and Lynch, 2014 ). In brief, to allow for a stringent RT-PCR assay each primer must have a minimal melting temperature (T m)  of 76 °C by the Marmur formula, have a GC content of between 50 and 60%, and have between 2 and 4 G or C nucleotides at the 3 ′  end. Additionally, the selected primer pair should produce expected products within a certain size range to allow visualization via gel electrophoresis and to reduce bias during reverse transcription ( Smith and Lynch, 2014 ). Importantly, design considerations such as minimum and maximum primer length, product length, GC content, T m  and T m  estimation method can be adjusted under ‘Advanced Options’ in the submission form. All primers that meet these criteria are displayed for users to sample and select a pair that best meets experimental needs. For ease of use, the primer table is searchable and key summary information for each primer can also be displayed. Additionally, a number of filters can be applied to the primer table to further reduce the number of primers shown, on the fly, without re-executing SPEL ( Fig. 1B ). MAJIQ-SPEL also offers UCSC Genome Browser ( Kent et al. , 2002 ) connectivity. Clicking the browser’s logo brings up custom tracks that display the exons, junctions and locations of putative primers to aid in selection of primers for validation ( Fig. 1C ). These tracks also include known isoforms and annotated protein domains [Pfam ( Finn et al. , 2016 )], which can aid in examining the functions of alternative isoforms produced. Finally, MAJIQ-SPEL traverses all possible paths within the splice graph contained in the LSV region based on observed and annotated junctions to create the isoform segments table that links the MAJIQ PSI quantification to the associated isoform(s) ( Fig. 1D ). Importantly, once the user selects a forward and reverse primer pair, this table updates to display the expected product size for each isoform segment for validation. Additionally, once a primer pair is chosen, the user can run In-Silico PCR through UCSC Genome Browser ( Kent et al. , 2002 ) to further validate and check the specificity of the chosen pair. In the example shown, RT-PCR performed using primers generated by MAJIQ-SPEL demonstrates both accurate prediction of all four product sizes and quantification for both cerebellum and adrenal gland ( Fig. 1E ). Beyond handling classic or complex splicing variations, MAJIQ-SPEL also offers researchers fast and accurate primer design for de novo splicing variations not in the annotated transcriptome. In such cases experimental validation is crucial. Such a case is shown in an event in Fubp3  ( Supplementary Fig. S1 ). Since this LSV involves novel exon skipping it will likely not be captured in other tools for splicing quantification and visualization packages that rely only on the annotation database. The Fubp3  and Clta  splicing variations shown here also highlight how MAJIQ-SPEL can aid in functional analysis of LSVs. The combined UCSC Genome Browser tracks show the alternative exons overlap annotated protein domains, suggesting a functional effect. The cassette exon in Fubp3  is not a multiple of three, suggesting a frameshift and the Browser tracks revealed that skipping inserts a premature termination codon (PTC). Future extensions of this work will aim to further integrate these and other functional analyses into MAJIQ-SPEL. Funding This work has been supported in part by the Penn Institute for Biomedical Informatics Pilot Grant and R01 AG046544 to YB. 
 Conflict of Interest : none declared. Supplementary Material Supplementary Fig 1 Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>metaCCA: summary statistics-based multivariate meta-analysis of genome-wide association studies using canonical correlation analysis</Title>
    <Doi>10.1093/bioinformatics/btw052</Doi>
    <Authors>Cichonska Anna, Rousu Juho, Marttinen Pekka, Kangas Antti J., Soininen Pasi, Lehtimäki Terho, Raitakari Olli T., Järvelin Marjo-Riitta, Salomaa Veikko, Ala-Korpela Mika, Ripatti Samuli, Pirinen Matti</Authors>
    <Abstract>Motivation: A dominant approach to genetic association studies is to perform univariate tests between genotype-phenotype pairs. However, analyzing related traits together increases statistical power, and certain complex associations become detectable only when several variants are tested jointly. Currently, modest sample sizes of individual cohorts, and restricted availability of individual-level genotype-phenotype data across the cohorts limit conducting multivariate tests.</Abstract>
    <Body>1 Introduction Most human diseases and traits have a strong genetic component. Genome-wide association studies (GWAS) have proven effective in identifying genetic variation contributing to common complex disorders, including type 2 diabetes ( Mahajan  et al. , 2014 ), cardiovascular disease ( Deloukas  et al. , 2013 ), schizophrenia ( Schizophrenia Working Group of the Psychiatric Genomics Consortium, 2014 ), and quantitative traits, such as lipid levels ( Global Lipids Genetics Consortium, 2013 ;  Surakka  et al. , 2015 ) and metabolomics ( Kettunen  et al. , 2012 ;  Shin  et al. , 2014 ). A dominant approach to GWAS is to test one single-nucleotide polymorphism (SNP) at a time against one quantitative phenotype measure or a binary disease indicator. This univariate approach is unlikely to be optimal when millions of SNPs and a growing number of phenotypes, including serum metabolomic profiles ( Kettunen  et al. , 2012 ;  Shin  et al. , 2014 ), three-dimensional images ( Wang  et al. , 2013 ), and gene expression data ( Ardlie  et al. , 2015 ) become available simultaneously. Indeed, a recent comparison demonstrated that utilizing multivariate phenotype representation increases statistical power, and leads to richer findings in the association tests compared to the univariate analysis ( Inouye  et al. , 2012 ). Moreover, some complex genotype-phenotype correlations can be detected only when testing several genetic variants simultaneously ( Marttinen  et al. , 2014 ), and multi-genotype tests are common practice in rare variant association studies, where statistical power to detect any single variant is very small ( Feng  et al. , 2014 ;  Lee  et al. , 2014 ). Unfortunately, restricted availability of complete multivariate individual-level records across the cohorts currently limits multivariate analyses. Often, only the univariate GWAS summary statistics, i.e. univariate regression coefficients with their standard errors, from individual cohorts are publicly available. Hence, a major question is how we can use these univariate association results to carry out a multivariate meta-analysis of GWAS ( Evangelou and Ioannidis, 2013 ), which is crucial to increase the power to identify novel genetic associations. Recently, two kinds of multivariate testing approaches operating on univariate summary statistics have been introduced: (i) one SNP against multiple traits ( Stephens, 2013 ;  van der Sluis  et al. , 2013 ;  Vuckovic  et al. , 2015 ;  Zhu  et al. , 2015 ) and (ii) multiple SNPs against one trait ( Feng  et al. , 2014 ;  Yang  et al. , 2012 ). We propose a new framework,  metaCCA , that unifies both of the existing approaches by allowing canonical correlation analysis (CCA) of multiple SNPs against multiple traits based on univariate summary statistics and publicly available databases. CCA is a well-established statistical technique for identifying linear relationships between two sets of variables, and has been successfully applied to GWAS ( Ferreira and Purcell, 2009 ;  Inouye  et al. , 2012 ;  Marttinen  et al. , 2013 ;  Tang and Ferreira, 2012 ). Our  metaCCA  method extends CCA to the setting where original individual-level measurements are not available. Instead,  metaCCA  works with three pieces of the full data covariance matrix, and applies a covariance shrinkage algorithm to achieve robustness. We demonstrate the performance of  metaCCA  using SNP and metabolite data from three Finnish cohorts. In summary, this paper makes the following contributions.
 To our knowledge, we provide the first computational framework for association testing between multivariate genotype and multivariate phenotype, based on univariate summary statistics from single or multiple GWAS. Our implementation is freely available. We demonstrate how to accurately estimate correlation structures of phenotypic and genotypic variables without an access to the individual-level data. We avoid false positive associations by a covariance shrinkage algorithm based on stabilization of the leading canonical correlation. Our approach,  metaCCA , is a general framework to conduct CCA when full data are not available, and therefore it is widely applicable also outside GWAS. A detailed discussion on the relationship between  metaCCA  and previously published multivariate association methods can be found in  Supplementary Data . 2 Methods This section is organized as follows. First, Section 2.1 explains univariate GWAS, the results of which, in the form of cross-covariance matrix, constitute an input to  metaCCA  described in Section 2.2; Section 2.3 demonstrates how a meta-analysis of several studies is conducted in our framework; Section 2.4 outlines a procedure for choosing SNPs representative of a given locus; finally, Section 2.5 introduces the data we used to test  metaCCA  in the meta-analytic setting. 2.1 Univariate GWAS Let  X  and  Y  denote genotype and phenotype matrices of dimensions  N × G  and  N × P , respectively, storing the individual-level data;  N  the number of samples;  G  and  P  the number of genotypic and phenotypic variables, respectively. The columns of  X  and  Y  are standardized to have mean 0 and standard deviation 1. Typically, univariate GWAS analysis of quantitative traits tests for an association between each pair of genotype  x g ∈ R N  and phenotype  y p ∈ R N  separately using a linear model:
 (1) y p = α g p + x g β g p + ϵ . 
 Coefficient β gp , corresponding to the slope of the regression line, is the parameter of interest, since it depicts the size of the effect of the genetic variant  x g  on the trait  y p . Parameter α gp  is an intercept on the  y -axis, and ϵ indicates a Gaussian error term or noise. The model is fit by the method of  least squares  that leads to a closed-form estimate for the unknown parameter  β g p = [ x g T y p ] [ x g T x g ] − 1 = [ ( N − 1 ) s x y ] [ ( N − 1 ) s x x ] − 1 = s x y , where  s xy  is a sample covariance of  x g  and  y p , and  s xx  = 1 is a sample variance of  x g . Hence, the cross-covariance matrix Σ XY between all genotypic and phenotypic variables is made of univariate regression coefficients β gp :
 (2) Σ X Y = X T Y N − 1 = ( β 11 β 12 ⋯ β 1 P β 21 β 22 ⋯ β 2 P ⋮ ⋮ ⋱ ⋮ β G 1 β G 2 ⋯ β G P ) . 
 An important note is that if the individual-level datasets  X  and  Y  were not standardized before applying the linear regression, the standardization can be achieved afterwards by a transformation
 (3) β g p STANDR = 1 N   S E g p × β g p , 
where  SE gp  indicates the standard error of β gp , as given by GWAS software. (Typically,  SE g p ≈ σ p / ( N 2 f g ( 1 − f g ) ) , where σ p  is the standard deviation of the trait  p , and  f g  is the minor allele frequency of SNP  g , but uncertainty in genotype imputation causes deviations from this expression.) 2.2 metaCCA Conducting multivariate association tests requires estimates of the dependencies between genotypic and phenotypic variables, denoted Σ XX  and Σ YY,  respectively. Typically, they are calculated based on the individual-level measurements  X  and  Y :
 (4) Σ X X = X T X N − 1 , 
 (5) Σ Y Y = Y T Y N − 1 . 
 metaCCA  operates on the cross-covariance matrix Σ XY ( Equation 2 ), and correlation structures  Σ ˆ X X ,   Σ ˆ Y Y , estimated without an access to the individual-level data  X  and  Y  ( Fig. 1A, B ). To make the resulting full covariance matrix Σ a valid covariance matrix,  metaCCA  applies a shrinkage algorithm ( Fig. 1C ).
 Fig. 1. Schematic picture showing an overview of  metaCCA  framework for summary statistics-based multivariate association testing using canonical correlation analysis. ( A )  metaCCA  operates on three pieces of the full covariance matrix Σ: Σ XY of univariate genotype-phenotype association results, Σ XX  of genotype-genotype correlations, and Σ Y Y of phenotype-phenotype correlations. ( B )  Σ ˆ X X  is estimated from a reference database matching the study population, e.g. the 1000 Genomes, and phenotypic correlation structure  Σ ˆ Y Y  is estimated from Σ XY.  ( C ) A covariance shrinkage algorithm is applied to add robustness to the method. Numbers in brackets refer to subsections in Methods. Meta-analysis of several studies is performed by pooling covariance matrices of the same type, before step (C), as described in Section 2.3. The data reduction achieved by  metaCCA  can be seen in  Supplementary Figure S1 
 The rest of this section describes the details of  metaCCA  framework. 2.2.1 Estimation of genotypic correlation structure Genetic variation is organized in haplotype blocks, whose structure is determined by mutation and recombination events, together with demographic effects, including population growth, admixture and bottlenecks ( Wall and Pritchard, 2003 ). Hence, correlation structure of genetic variants differs between populations, such as, e.g. the Finns, Icelanders or Central Europeans. In  metaCCA ,  Σ ˆ X X  is calculated using a reference database representing the study population, such as the 1000 Genomes database ( 1000 Genomes Project Consortium, 2012 ,  www.1000genomes.org ), or other genotypic data available on the target population. In the Section 3, we demonstrate that estimating  Σ ˆ X X  from the target population (in our case, the Finns) leads to better results than utilizing the data comprising individuals across distinct populations (e.g. the Finns and other Europeans). However, since reference data on the target population may not always be at hand, we also present a robust but less powerful solution to multivariate association testing by simply using genotypes of all individuals from a certain broader geographical region (e.g. a continent) available under the 1000 Genomes Project. 2.2.2 Estimation of phenotypic correlation structure In our framework, phenotypic correlation structure  Σ ˆ Y Y  is computed based on Σ XY.  Each entry of  Σ ˆ Y Y  corresponds to a Pearson correlation between two column vectors of Σ XY - univariate regression coefficients of two phenotypic variables  s  and  t  across  G  genetic variants:
 (6) Σ ˆ Y Y ( s , t ) = ∑ g = 1 G ( β g s − μ s ) ( β g t − μ t ) ∑ g = 1 G ( β g s − μ s ) 2 ∑ g = 1 G ( β g t − μ t ) 2 , 
where μ s  and μ t  are the mean values  μ s = 1 G ∑ g = 1 G β g s  and  μ t = 1 G ∑ g = 1 G β g t . (The detailed justification is provided in  Supplementary Data .) In  Supplementary Table S2 , we demonstrate that the higher the number of genotypic variables  G , the lower the error of the estimate. Thus,  Σ ˆ Y Y  should be calculated from summary statistics of all available genetic variants, even if only a subset of them is taken to the further analysis. 2.2.3 Canonical correlation analysis CCA ( Hotelling, 1936 ) is a multivariate technique for detecting linear relationships between two groups of variables  X ∈ R N × G  and  Y ∈ R N × P , where  X  and  Y  constitute two different views of the same object. The objective is to find maximally correlated linear combinations of columns of each matrix. This corresponds to finding vectors  a ∈ R G  and  b ∈ R P  that maximize
 (7) r = ( X a ) T ( Y b ) ‖ X a ‖   ‖ Y b ‖ = a T Σ X Y b a T Σ X X a b T Σ Y Y b . 
 The maximized correlation  r  is called  canonical correlation  between  X  and  Y . We provide the technical details of the method, as well as its extension to subsequent canonical correlations and their significance testing in  Supplementary Data . 2.2.4 Shrinkage At this point, we have three covariance matrices, namely Σ XY, 
 Σ ˆ X X , and  Σ ˆ Y Y . However, in many cases, the resulting full covariance matrix
 Σ = ( Σ ˆ X X Σ X Y Σ X Y T Σ ˆ Y Y ) 
is not positive semidefinite (PSD), and therefore its building blocks cannot be just plugged into the CCA framework ( Equation 7 ). To overcome this problem, in  metaCCA , we apply shrinkage to find a nearest valid Σ ( Ledoit and Wolf, 2003 ). We use an iterative procedure where the magnitudes of the off-diagonal entries are being shrunk towards zero until Σ becomes PSD (Algorithm 1). Assuring the PSD property of the full covariance matrix is necessary, although, as we demonstrate in the Section 3, not sufficient to obtain reliable results of the association analysis when the estimate  Σ ˆ X X  (and/or  Σ ˆ Y Y ) is noisy. In order to address this issue, we propose a variant of  metaCCA , called  metaCCA+ , where the full covariance matrix Σ is shrunk beyond the level guaranteeing its PSD property. A challenge, however, is to find an optimal shrinkage intensity. Shrinkage applied without any stopping criterion would lead to gradual removal of all dependencies between genotypic and phenotypic variables.  Ledoit and Wolf (2003)  introduced an analytic approach for determining the optimal shrinkage level but it requires the individual-level datasets  X  and  Y . In  metaCCA+ , we monitor the leading canonical correlation value  r , and we continue the shrinkage of the full covariance matrix Σ until  r  stabilizes. Specifically, we track the percent change  pc  of  r  between subsequent shrinkage iterations, and we determine an appropriate amount of shrinkage using an elbow heuristic, similar to the criterion for finding the number of clusters, frequently used in the literature ( Tibshirani  et al. , 2001 ). The idea is that the slope of the graph should be steep to the left of the elbow, but stable to the right of it. We find the elbow, and thus the appropriate number of shrinkage iterations, by taking the point closest to the origin of the plot of  pc  versus iteration number, as schematically shown in  Supplementary Figure S2 . Building blocks  Σ ˆ X Y , Σ ˆ X X , Σ ˆ Y Y  of the resulting full covariance matrix Σ, shrunk until it became PSD or beyond, are then plugged into the CCA framework to get the final genotype-phenotype association result. In practice, in order to protect from false positive signals, the shrinkage mode of  metaCCA+  should be applied whenever  Σ ˆ Y Y  is estimated from summary statistics of a small number of genetic variants, and/or  Σ ˆ X X  is calculated using a generic reference population. Algorithm 1 |   w h i l e   Σ   notPSD |   Σ = 0.999 × Σ ; diag ( Σ ) = 1 ; 2.2.5 Types of the multivariate association analysis We consider the following two types of the multivariate analysis.
 Univariate genotype – multivariate phenotype One genetic variant tested for an association with a set of phenotypic variables (matrix  Σ ˆ X X  not needed). Multivariate genotype – multivariate phenotype A set of genetic variants tested for an association with a set of phenotypic variables. The first type corresponds to a standard multi-trait analysis. The second type takes into account the effects across genomic variants on multiple traits, which are ignored when analyzing only a single SNP or a single trait at a time. 2.3 Meta-analysis metaCCA  allows to conduct summary statistics-based multivariate analysis of one or multiple GWAS. In the meta-analytic setting, covariance matrices  Σ X Y ( i ) ,   Σ ˆ X X ( i ) , and  Σ ˆ Y Y ( i )  corresponding to  i  =   1,…, M  independent studies on the same topic are pooled using a weighted average:
 (8) Σ X Y = ( N 1 − 1 ) Σ X Y ( 1 ) + ⋯ + ( N M − 1 ) Σ X Y ( M ) N − M , 
where  N i  denotes the number of samples in the  i th  cohort, and  N = N 1 + ⋯ + N M . This step is performed before applying the shrinkage to the full covariance matrix. As is typical for a fixed-effects meta-analysis, the weighted average is used in order to account for the varying precision of the estimates. The formulas for  Σ ˆ X X  and  Σ ˆ Y Y  are analogous to (8). However, if all cohorts included in the meta-analysis have the same underlying population, only one genotypic correlation estimate is needed. 2.4 Choosing SNPs representing a locus When analyzing multiple genetic variants together, we use a procedure for selecting from a given locus a set of SNPs that jointly capture a maximal amount of genetic variation in the locus, as measured by a linkage disequilibrium (LD) score. In each iteration, a SNP  g  that maximizes LD-score, which we define as  ∑ k r ˆ g k 2 σ k 2 , is selected, where the sum is over all SNPs  k  that have not yet been chosen;  r ˆ g k  denotes a partial correlation between SNPs  g  and  k ;  σ k 2  indicates empirical variance of the residuals for SNP  k  after the effects of the selected SNPs have been regressed out. The residual variance  σ k 2  gets smaller, if the SNP has already been well explained by the previously chosen ones; hence, highly correlated SNPs will not be selected together. In the first iteration,  r ˆ g k  is the Pearson correlation coefficient between SNPs  g  and  k , and  σ k 2 = 1 , meaning that the starting SNP is the one capturing the highest amount of genetic variation in the region. For each locus, we select the smallest number of SNPs that explain, at median, over 95% of the variance of the remaining SNPs in the locus. 2.5 Datasets In order to test our approach, we used genotypic and phenotypic data from three Finnish population cohorts: the Cardiovascular Risk in Young Finns Study (YFS,  N 1  =   2390;  Raitakari  et al. , 2008 ), the FINRISK study survey of 1997 ( N 2  =   3661;  Vartiainen  et al. , 2010 ), and the Northern Finland Birth Cohort 1966 (NFBC,  N 3  =   4702;  Rantakallio, 1969 ). The detailed description of the cohorts can be found in  Supplementary Data . Our phenotype data consist of 81 lipid measures ( Supplementary Table S1 ) from a high-throughput nuclear magnetic resonance (NMR) platform ( Soininen  et al. , 2009 ,  2015 ). As a pre-processing step, within each cohort, each trait was quantile normalized, and the effects of age, sex and ten leading principal components of the genetic population structure were regressed out using a linear model. All cohorts were genotyped using Illumina arrays, and imputed by IMPUTE2 ( Howie  et al. , 2009 ) using the 1000 Genomes Project reference panel ( 1000 Genomes Project Consortium, 2012 ). In the analyses, we included 455 521 SNPs on chromosome 1 and, additionally, the SNPs in the following 5 genes:
 APOE  (apolipoprotein E), 259 SNPs on chr 19; CETP  (cholesteryl ester transfer protein), 387 SNPs on chr 16; GCKR  (glucokinase (hexokinase 4) regulator), 160 SNPs on chr 2; PCSK9  (proprotein convertase subtilisin/kexin type 9), 265 SNPs on chr 1; NOD2  (nucleotide-binding oligomerization domain containing 2), 145 SNPs on chr 16. We expected that this set of genes would provide a comprehensive spectrum of associations with our phenotypes, since  APOE ,  CETP ,  GCKR , and  PCSK9  have well-known associations to lipid levels, whereas  NOD2  is not known to have such an association (NHGRI GWAS catalogue,  Hindorff  et al. , 2011 ,  www.genome.gov/gwastudies ). All SNPs used were of good quality: IMPUTE2 info ≥0.8 ( Marchini and Howie, 2010 ), and minor allele frequency ≥0.05. For multi-SNP models, we compared the results from Finnish genotype data with those obtained by estimating the genotypic correlation structure  Σ ˆ X X  from the 1000 Genomes Project data on 503 European individuals (release 20130502). For each cohort, genotypic and phenotypic correlation structures computed based on  X ( i )  and  Y ( i ) , as shown in the  Equations (4)  and  (5) , can be found in  Supplementary Figures S3 and S4 . 3 Results 3.1 Performance assessment The purpose of this section is to validate that  metaCCA  applied to summary statistics produces similar results to the standard CCA (MATLAB function  canoncorr ) applied to the individual-level data. For  metaCCA , we always use  Σ ˆ Y Y  estimated by the method described in Section 2.2.2 using summary statistics of the entire chromosome 1. We focus on the effects of (i) the amount of shrinkage applied to the full covariance matrix ( metaCCA / metaCCA+ ) and (ii) estimating  Σ ˆ X X  from the population underlying the analysis (here, Finnish), or from a more heterogeneous panel (here, European individuals from the 1000 Genomes database). 3.1.1 Univariate genotype – multivariate phenotype We conducted a meta-analysis of the three cohorts (YFS, FINRISK and NFBC) by testing associations between each SNP in the five genes (as listed in Section 2.5; 1 216 SNPs in total) with different numbers of traits, ranging from 2 to 50. Multi-trait analyses are most useful for correlated traits ( Stephens, 2013 ). To reflect this, for each SNP, we started with a randomly selected trait, and at each step of the analysis, added the trait mostly correlated with the already chosen ones, excluding correlations with absolute values above 0.95. For each SNP, we repeated the procedure three times with different starting lipid measures. The scatter plot in  Figure 2a  shows that  metaCCA  applied to the cohort-wise summary statistics provides an excellent agreement with the standard CCA of the pooled individual-level data. Thus, in this one-SNP–multi-trait analysis, due to the reliable  Σ ˆ Y Y  estimate used, we can base the inference on  metaCCA , and put less weight on  metaCCA+  ( Fig. 2b ) that, as expected, produces conservative  P -values.
 Fig. 2. Scatter plots of −log 10  P -values between the pooled individual-level analysis of original datasets ( full data CCA ) and  metaCCA  (first row),  metaCCA+  (second row). ( a ,  b )  Univariate genotype – multivariate phenotype ; meta-analysis of NFBC, FINRISK and YFS cohorts; ( c – f )  Multivariate genotype – multivariate phenotype ; meta-analysis of NFBC and YFS cohorts;  metaCCA / metaCCA+  was used with  Σ ˆ X X  computed from FINRISK (FIN; c, d), or from the 1000 Genomes database (1000G, 503 EUR individuals; e, f) In all the cases, lipid correlation structure  Σ ˆ Y Y  was calculated from univariate summary statistics of SNPs from the entire chromosome 1. Single point corresponds to the result of one out of (a–b) 178 752, (c–f) 4050 multivariate tests. Numbers at the top of each plot indicate percentages of at least 0.5 unit overestimated  metaCCA ’s/ metaCCA+ ’s −log 10  P -values in the ranges [0, 10] (purple) or (10, max(−log 10  P -value)] (red). This threshold is represented by purple and red lines.  Supplementary Figure S5  shows these results restricted to the  x -axis range of [0, 10], and  Supplementary Figure S6  illustrates the impact of the number of genotypic and phenotypic features included in the analysis on the accuracy of  metaCCA/metaCCA+ 
 The wide range of the observed −log 10  P -values (0–88) shows that multivariate association tests can be very powerful in realistic settings, and that our example assesses the performance of  metaCCA  throughout the range that is important in practical analyses.  Supplementary Figure S5  further refines the behaviour of  metaCCA  within the range most encountered in genome-wide association studies (0–10). 3.1.2 Multivariate genotype – multivariate phenotype When both genotype and phenotype are multivariate, genotypic correlation structure  Σ ˆ X X  needs to be estimated in addition to  Σ ˆ Y Y . We conducted the meta-analysis of two study cohorts (YFS and NFBC), and computed  Σ ˆ X X  either from FINRISK (FIN) or from a more generic population of the 1000 Genomes European individuals (1000G). ( Supplementary Table S3  shows errors of  Σ ˆ X X  estimates.) We analyzed together between 2 and 10 highly correlated lipid measures, chosen sequentially as in the single-SNP tests in Section 3.1.1. For each of the five genes, we analyzed together between 2 and 10 SNPs that were chosen to be approximately uncorrelated to cover a large proportion of genetic variation within the gene. Each set of SNPs was tested for an association with each group of correlated lipid measures. We repeated the procedure ten times for each gene, with different starting phenotypes and SNPs. The results are summarized in  Figure 2c–f .  Figure 2c  shows that when genotypic correlation  Σ ˆ X X  is estimated from the target population,  metaCCA  produces highly consistent results with the standard CCA based on the individual-level data. When  Σ ˆ X X  is estimated from a less well matching population ( Fig. 2e ), the accuracy is reduced, and some −log 10  P -values become clearly overestimated. In both cases, further shrinkage by  metaCCA+  removes, almost completely, any overestimation ( Fig. 2d, f ). This property is expected to be important in genome-wide association studies, where  metaCCA+  can protect from false positives when genotypic correlation structure cannot be accurately estimated.  metaCCA+  has less statistical power than the individual-level CCA, but it is still able to detect strong true associations. 3.2 Application to summary statistics from SNPTEST In the genetics community, established software packages like SNPTEST ( Marchini and Howie, 2010 ) are used to perform univariate genome-wide tests. In this section, we conduct a meta-analysis of univariate results from standard SNPTEST runs on NFBC and YFS cohorts by  metaCCA . These cohorts have been meta-analyzed previously using standard CCA applied to pooled individual-level genotypes and the same serum metabolomic profiles that we consider here ( Inouye  et al. , 2012 ). This single-SNP–multi-trait GWAS highlighted candidate genes for atherosclerosis, and demonstrated the power of incorporating multiple related traits into the analysis. Here, we show that by  metaCCA  we obtain those same results without the access to the individual-level data, and, in addition to that, we can also analyze multiple SNPs jointly by using only summary statistics from the original studies. We wanted to choose a set of correlated traits for the joint analysis, and therefore we proceeded as follows. By an agglomerative hierarchical clustering (average linkage) of Σ YY (81 traits), we identified groups of related lipid measures. From the largest of 6 distinct clusters, we selected a set of traits in such a way that no pair exhibited correlation above 0.95. We ended up with a group of 9 lipid measures related to 8 VLDL particles of different sizes and one HDL particle (highlighted in blue in  Supplementary Table S1 ). We conducted two types of meta-analyses of NFBC and YFS:
 Univariate genotype – multivariate phenotype Each SNP from chromosome 1 tested for an association with the set of 9 correlated lipid measures. Multivariate genotype – multivariate phenotype For each of the 5 genes (APOE, CETP, GCKR, PCSK9, NOD2), the smallest set of SNPs that explained, at median, over 95% of the variance of the remaining SNPs is chosen (see Section 2.4), and tested for an association with the set of 9 correlated lipid measures. The input summary statistics for  metaCCA  were obtained by performing univariate tests for each SNP-trait pair separately using SNPTEST applied to the individual-level data, and transforming the resulting regression coefficients using (3). The correlation structure of analyzed traits,  Σ ˆ Y Y , was estimated from summary statistics of SNPs across the entire genome. The genotypic correlation structure for multi-SNP analyses,  Σ ˆ X X , was calculated from the FINRISK cohort. We compared the results of  metaCCA  and  metaCCA+  with the pooled individual-level CCA of original datasets.  Figure 3  shows scatter plots of − log 10  P -values for 455 521 SNPs from chromosome 1. The results of  metaCCA  demonstrate an excellent agreement with the original  P -values, validating that  metaCCA  can conduct reliable multivariate meta-analysis from standard univariate GWAS software output. As anticipated,  metaCCA+  produces conservative  P -values. Here,  metaCCA  is indeed the method of choice in practice, due to the high quality of covariance estimate used. Manhattan plots illustrating  P -values along the chromosome are shown in  Supplementary Figure S7 . Genome-wide significant associations (at the threshold of  P = 5 × 10 − 8  standard in the field) are located within two regions:  USP1/DOCK7  and  FCGR2A/3A/2C/3B , which are known to be associated with lipid metabolism (NHGRI GWAS catalogue,  Hindorff  et al. , 2011 ).  metaCCA  identified both regions, and  metaCCA+  found the stronger out of the two signals ( DOCK7/USP1 ). For top-SNP in  FCGR2A/3A/2C/3B ,  metaCCA+ ’s −log 10  P -value is 6.11, compared to 7.73 produced by CCA on the individual-level data.
 Fig. 3. Scatter plots of −log 10  P -values from the pooled individual-level CCA of NFBC and YFS and ( a )  metaCCA , ( b )  metaCCA+ . Each point corresponds to one genetic variant from the chromosome 1, tested for an association with the group of 9 correlated lipid measures. In total, 455 521 SNPs were analyzed. Red lines indicate the significance level of  5 × 10 − 8  (7.301 on −log 10 scale) 
 Figure 4  summarizes the results of the multi-SNP–multi-trait meta-analysis, and shows the performance of  metaCCA  when different numbers of SNPs, from 2 up to 25, representing a gene, are tested jointly for an association with the group of 9 related lipid traits. Numbers of SNPs that are chosen by our approach (Section 2.4) are marked with  x .  Figure 4  validates that by using this protocol, a gene is described well, since when adding more SNPs no clear power gain is observed. Both  metaCCA  and  metaCCA+  ( Fig. 4 ,  Supplementary Table S4 ) produced very accurate  P -values. For the largest signals ( APOE ,  CETP ), −log 10  P -values are less than one unit overestimated by  metaCCA , and underestimated by  metaCCA+ . These differences would be unlikely to lead to false inferences when a reference significance level in a gene-based analysis was set to  0.05 / 20000 = 2.5 × 10 − 6 , i.e. 5.61 on − log 10 scale, based on there being about 20 000 protein-coding genes in the human genome. At this level, both  metaCCA  and  metaCCA+  found an association between  APOE ,  CETP ,  GCKR  and the network of VLDL and HDL particles studied. For  APOE  and  CETP , gene-based signals are clearly higher than the univariate ones, even before accounting for different numbers of tests. Moreover, in case of  APOE , the multi-SNP–multi-trait signal is nearly 4.5 units higher than the single-SNP–multi-trait one. Note that  NOD2  has no (known) association with metabolic traits, and therefore it serves as a negative control  Figure 4  and  Supplementary Table S4 .
 Fig. 4. Multi-SNP–multi-trait  analysis: −log 10  P -values of CCA on pooled individual-level datasets (NFBC + YFS), and the meta-analyses conducted using  metaCCA , as a function of the number of SNPs representing a gene. Sets of 2–25 SNPs were tested for an association with the group of 9 related lipid measures. In practice, the smallest number of SNPs that explain, at median, over 95% of the variance of the remaining SNPs would be chosen to represent a gene, and is marked with  x . The evolution of the median variance explained versus the number of SNPs is shown in  Supplementary Figure S8 . For each gene, the largest −log 10  P -value from single-SNP–single-trait tests ( top univariate ) is represented by a dashed line. The largest single-SNP–multi-trait −log 10  P -values are 11.54 for  APOE , 23.77 for  CETP , 9.64 for  GCKR , 6.58 for  PCSK9  and 0.97 for  NOD2 . The values are summarized with details in  Supplementary Table S4 . The number of tests in each gene is 1 for multi-SNP,  G  for single-SNP–multi-trait, and  9 × G  for single-SNP–single-trait tests, where  G  is the number of SNPs in that gene 
 4 Discussion The advantage of multivariate testing of genetic association is well reported in the literature ( Inouye  et al. , 2012 ;  Stephens, 2013 ), and also demonstrated in our results (e.g.  CETP  in  Supplementary Table S4  that has multivariate  P -value 13 orders of magnitude smaller than any of the univariate  P -values). Optimal use of correlated traits is becoming increasingly important as high-throughput phenotyping technologies are being more widely applied to individual study cohorts and large biobanks ( Soininen  et al. , 2015 ). We introduced  metaCCA , a computational approach for the multivariate meta-analysis of GWAS by using univariate summary statistics and a reference database of genetic data. Thus, our framework circumvents the need for complete multivariate individual-level records, and tackles the problem of low sample sizes in individual cohorts by a built-in meta-analysis approach. To our knowledge,  metaCCA  is the first summary statistics-based framework that allows multivariate representation of both genotypic and phenotypic variables. In large meta-analytic efforts, the ability to work with summary statistics is beneficial, even when there is an access to the individual-level data. For example, with a study design of the  Global Lipids Genetics Consortium (2013) , we estimate that the reduction in the size of input data between  metaCCA  and standard CCA could be over 750-fold ( Supplementary Figure S1 ). We provided two variants of the algorithm:  metaCCA  and  metaCCA+ . Based on our results,  metaCCA  is the method of choice when the accuracy of estimated correlation matrices  Σ ˆ X X  and  Σ ˆ Y Y  is good, i.e.  Σ ˆ X X  estimated from genetic data on the target population, and  Σ ˆ Y Y  estimated from at least one chromosome. In such cases,  P -values from  metaCCA  were very accurate, meaning that false positive and false negative rates are close to those of standard CCA applied to the individual-level data. We emphasize that  metaCCA  should not be used when the quality of  Σ ˆ X X  and/or  Σ ˆ Y Y  estimates is reduced, i.e. when a generic reference population and/or summary statistics of only a small number of genotypes are available. In such cases,  metaCCA+  proved useful to protect from an increase of false positive associations ( Fig. 2  and  Supplementary Figure S9 ). This is important in GWAS context, where false positives could lead to considerable waste of resources in subsequent experimental and functional studies. A topic for future work would be to further develop our current heuristic stopping criterion of  metaCCA+  to decrease its false negative rate without sacrificing its good false positive rate. We derived the framework assuming that all traits within each cohort have been measured on the same number of individuals ( N ). We note that the distribution of the test statistic depends on  N  ( Supplementary Data ), as do the effect size transformation ( Equation 3 ) and meta-analysis approach (Section 2.3). While a small proportion of missing data for each trait could be handled by statistical imputation methods, further work is required to study how  metaCCA  should be used when the sample sizes between the traits vary considerably. However, with high-throughput phenotyping technologies, we believe that  metaCCA  can be applied to many existing and forthcoming studies. For multivariate phenotype data, several types of association tests are possible. Natural question is which one should we prefer in practice. It is evident that single-SNP–multi-trait tests can detect much stronger signals at some SNPs than any of the univariate tests separately (e.g.  CETP  in  Supplementary Table S4 ), and identify associations not found by univariate approach ( Inouye  et al. , 2012 ). On the contrary, for some other SNPs, the highest univariate signal may be clearly higher than the multi-trait one, even after accounting for the increase in the number of tests. For example, in  GCKR  ( Supplementary Table S4 ), the top SNP’s (rs1260326) association was explained already by one of the traits individually (M.VLDL.FC). Given the difference in degrees of freedom of the tests, this led to a 4.6 units higher −log 10  P -value in the univariate test compared to the multivariate one. Thus, for single-SNP analysis, univariate and multivariate tests complement each other and neither should be excluded from consideration. When also genotypes are multivariate, even more possibilities for association testing emerge. To illustrate our multi-SNP approach, we proposed a procedure for selecting, for each gene, the smallest number of SNPs that explained, at median, over 95% of the variance of the remaining SNPs in the locus. We demonstrated that testing multiple SNPs jointly can be more powerful than single-SNP–single-trait ( APOE ,  CETP  in  Fig. 4  and  Supplementary Table S4 ) and single-SNP–multi-trait tests ( APOE  in  Supplementary Table S4 ). Moreover,  metaCCA  could equally well incorporate any other way of choosing the SNPs, for example, motivated by functional annotations ( ENCODE Project Consortium, 2012 ), known expression effects ( Ardlie  et al. , 2015 ) or previous GWAS results on other traits ( Hindorff  et al. , 2011 ). A topic for further research could be to extend the covariance matrix-based analyses from CCA to dynamic approaches that learned from the data the set of variants and traits to be considered together. This would circumvent the need to restrict the subset of variables before the analysis. We envision that multivariate association testing using  metaCCA  has a great potential to provide novel insights from already published summary statistics of large GWAS meta-analyses on multivariate high-throughput phenotypes, such as metabolomics and transcriptomics. Finally, we hope that our work helps extending the application area of CCA to summary statistics data also in other data-rich fields outside genetics. 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>RANGER-DTL 2.0: rigorous reconstruction of gene-family evolution by duplication, transfer and loss</Title>
    <Doi>10.1093/bioinformatics/bty314</Doi>
    <Authors>Bansal Mukul S, Kellis Manolis, Kordi Misagh, Kundu Soumya, Kelso Janet</Authors>
    <Abstract/>
    <Body>1 Introduction Duplication-Transfer-Loss (DTL) reconciliation is widely recognized as one of the most powerful computational techniques for understanding the evolution of microbial gene families ( Kamneva and Ward, 2014 ). DTL reconciliation works by comparing a given gene tree (for the gene family of interest) against the corresponding species tree and postulating gene duplication, horizontal gene transfer and gene loss events to explain the evolution of that gene tree inside the species tree. The result of DTL reconciliation is a mapping of the nodes of the gene tree to nodes (or edges) of the species tree, showing the embedding of the gene tree inside the species tree, as well as a labeling of each internal node of the gene tree as either a speciation, duplication, or transfer event. Such detailed knowledge of gene family evolution has many important biological applications, and the DTL reconciliation problem has therefore been extensively studied, e.g. ( Bansal  et al. , 2012 ,  2013 ;  David and Alm, 2011 ;  Doyon  et al. , 2010 ;  Jacox  et al. , 2016 ;  Kordi and Bansal, 2016 ;  Sjostrand  et al. , 2014 ;  Stolzer  et al. , 2012 ;  Szollosi  et al. , 2012 ;  Tofigh  et al. , 2011 ). While probabilistic models of DTL evolution also exist ( Sjostrand  et al. , 2014 ;  Szollosi  et al. , 2012 ), we focus here on parsimony-based models of DTL reconciliation which are much more scalable and require fewer parameters. Parsimony-based DTL reconciliation is also known to be highly accurate in practice; see Section S3 in the  Supplementary Material  for a detailed discussion on accuracy. A preliminary version of RANGER-DTL (short for Rapid ANalysis of Gene family Evolution using Reconciliation-DTL) was released in 2012 with a paper on the algorithmics of DTL reconciliation ( Bansal  et al. , 2012 ), providing only rudimentary functionality. Despite its limited functionality, the preliminary version of RANGER-DTL has been frequently used for biological data analysis ( Dupont and Cox, 2017 ;  Heitlinger  et al. , 2014 ;  Heshiki  et al. , 2017 ;  Jeong  et al. , 2016 ;  Koczyk  et al. , 2015 ;  Ricci  et al. , 2015 ). Here, we release the first full version of RANGER-DTL with greatly extended and improved functionality, and featuring the new algorithms and techniques developed in  Bansal  et al.  (2013) ;  Kordi and Bansal (2016) ;  Kundu and Bansal (2018) . 2 Features RANGER-DTL 2.0 is designed to enable fast and rigorous analysis of gene families and provides several advanced features not available in any other reconciliation software. The software takes as input a gene tree (rooted or unrooted) and a rooted species tree and reconciles the two by postulating speciation, duplication, transfer and loss events. Advanced capabilities of RANGER-DTL 2.0 include (i) principled handling of unrooted gene trees by considering all possible optimal rootings, (ii) uniformly random sampling of the space of all optimal reconciliations, making it possible to compute multiple optimal reconciliations and account for the variability in optimal reconciliation scenarios, (iii) use of distance-dependent transfer costs to better model transfer dynamics, (iv) handling gene tree uncertainty by collapsing weakly supported gene tree edges and computing and considering all optimal resolutions of the gene tree and (v) computing support values for individual DTL event inferences and species mapping assignments while accounting for multiple optimal reconciliations, uncertainty in gene tree rooting, alternative event cost assignments and even gene tree topological uncertainty. Furthermore, RANGER-DTL 2.0 can efficiently analyze trees with thousands of taxa. While it can handle both undated and fully-dated species trees, the focus of RANGER-DTL 2.0 is on undated species trees, for which it offers the most options and functionality. The reason for focusing on undated species trees is explained in Section S1 in the  Supplementary Material . Several features of RANGER-DTL 2.0, including consideration of all optimal gene tree roots, all possible optimal resolutions of unresolved gene trees and distance-dependent transfer costs, are not available in any comparable software package. A detailed comparison of RANGER-DTL 2.0 with existing DTL reconciliation software appears in Section S2 of the  Supplementary Material . 3 Availability and requirements The software package consists of 10 related programs designed to work together to support various reconciliation analyses. These ten programs are organized into (i) three  core programs , which define the core functionality of RANGER-DTL 2.0, designed to be applied sequentially, (ii) five  Supplementary programs  that provide additional functionality and (iii) two  summary scripts . Further details on the implementation of RANGER-DTL 2.0 are given in Section S4 of the  Supplementary Material . RANGER-DTL 2.0 is available open-source under GNU General Public Licence v3. Pre-compiled executables for Linux, Mac, and Windows, source code and a detailed manual are freely available online. The eight core and Supplementary programs are written in C++ and can be compiled on any operating system with a C++ compiler supporting the ANSI C++ standard. These C++ programs use standard C++ libraries along with the freely available and widely used Boost C++ libraries ( http://www.boost.org/ ). The two summary scripts are written in Python and can be run on any operating system with the Python interpreter. RANGER-DTL is designed to be efficient in both time complexity and memory requirements, and all programs, except for the two that consider unresolved gene trees, are scalable to hundreds or thousands of genes and taxa on commodity hardware. For instance, computing an optimal reconciliation using the core  Ranger-DTL  program for species trees and gene trees with 200 leaves and 1000 leaves each requires approximately 5 s and 9 min, respectively, on a desktop computer with a 3.1 GHz Intel i5 processor and both instances require less than 1 GB of RAM. In fact, with the supplementary program  Ranger-DTL-Fast , reconciling the 1000-leaf trees takes less than a second. 4 Conclusion Accurate and efficient DTL reconciliation of gene trees and species trees is crucial to understanding microbial gene and species evolution and to inferring horizontal gene transfer and other evolutionary events. RANGER-DTL 2.0 makes it possible to perform fast and rigorous analysis of gene family evolution through DTL reconciliation and offers many important features, such as consideration of all optimal gene tree roots, all possible optimal resolutions of unresolved gene trees, and distance-dependent transfer costs, that are not available in any comparable reconciliation software. RANGER-DTL is also designed to be easy to use, with easily interpretable results. There are several additional features that we intend to add to RANGER-DTL to further improve its functionality and accuracy. These include fast heuristics for handling gene tree uncertainty and estimating its impact on the reconciliation, and consideration of transfers from unsampled or extinct lineages, e.g. ( Jacox  et al. , 2016 ). These and other new features will be extensively tested to assess their impact on DTL reconciliation accuracy, and those that result in an improvement will be added to RANGER-DTL. Funding This work was supported in part by U.S. National Science Foundation CAREER award IIS 1553421 and by U.S. National Science Foundation awards MCB 1616514 and IES 1615573 to MSB, and by a University of Connecticut Summer Undergraduate Research Fund award to SK. 
 Conflict of Interest : none declared. Supplementary Material Supplementary Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Proteome coverage prediction with infinite Markov models</Title>
    <Doi>10.1093/bioinformatics/btp233</Doi>
    <Authors>Claassen Manfred, Aebersold Ruedi, Buhmann Joachim M.</Authors>
    <Abstract>Motivation: Liquid chromatography tandem mass spectrometry (LC-MS/MS) is the predominant method to comprehensively characterize complex protein mixtures such as samples from prefractionated or complete proteomes. In order to maximize proteome coverage for the studied sample, i.e. identify as many traceable proteins as possible, LC-MS/MS experiments are typically repeated extensively and the results combined. Proteome coverage prediction is the task of estimating the number of peptide discoveries of future LC-MS/MS experiments. Proteome coverage prediction is important to enhance the design of efficient proteomics studies. To date, there does not exist any method to reliably estimate the increase of proteome coverage at an early stage.</Abstract>
    <Body>1 INTRODUCTION Over the last few years, mass spectrometry-based proteomics has emerged as the most powerful approach to comprehensively characterize a proteome. The experimental workflows for mass spectrometry-based proteomics have sufficiently advanced to enable extensive exploration of complex biological samples (Domon and Aebersold,  2006 ). While conceptional studies provided rough a priori insights about the scope of these workflows (Eriksson and Fenyo,  2007 ), there are still no means to dynamically infer the  a posteriori  potential, i.e. to predict the increase in proteome coverage for their real-world implementations. This work contributes the extended infinite Markov model DiriSim to predict proteome coverage (in terms of peptide discoveries) upon repetition of liquid chromatography tandem mass spectrometry (LC-MS/MS) experiments. By explicitly modeling false and true positive peptide identifications, DiriSim enables us to specify the maximally achievable proteome coverage for a specified quality constraint on the final set of peptide discoveries. The most successful strategy to achieve extensive proteome coverage is referred to as shotgun proteomics. In its simplest implementation, protein samples are extracted from their biological source, subjected to enzymatic digestion and the resulting peptide mixtures are finally analyzed by LC-MS/MS. More elaborate strategies essentially adopt the same workflow, additionally augmented by fractionation steps for proteins/peptides before LC-MS/MS analysis. Finally, peptide identities are inferred from the acquired fragment ion spectra and they are used to recover the protein composition of the initial biological sample. The complexity of the protein, and hence peptide mixtures, poses a formidable challenge to mass spectrometrical analysis. The reversed phase liquid chromatography step effectively reduces the complexity of the peptide mixture by selecting peptides for tandem mass spectrometry analysis according to their polarity. For the duration of the LC-MS/MS experiment, the mass spectrometer coupled to the liquid chromatography system constantly acquires tandem mass spectra from eluting peptides. The elution time of a particular peptide is defined by its polarity. Any time during the LC-MS/MS experiment, the mass spectrometer is thus exposed to a local peptide mixture that is less complex than the initial mixture ( Fig. 1 a). Nevertheless, these mixtures are typically still far too complex to allow the mass spectrometer to acquire tandem mass spectra for all peptides in a single LC-MS/MS experiment. Consequently, LC-MS/MS experiments are usually repeated extensively, in order to increase the number of peptides for which tandem mass spectra are acquired.
 Fig. 1. Illustration of an LC-MS/MS experiment. ( a ) Liquid chromatography fractionation generates a sequence of local peptide ensembles from the initial ensemble. Each of these ensembles is derived from the initial ensemble by pooling peptides of similar polarity. The sequence of ensembles features descending overall polarity in the course of the experiment. During the experiment peptides π t  are drawn from the sequence of ensembles and analyzed by the mass spectrometer coupled to the liquid chromatography system. ( b ) Graphical representation of the infinite Markov model. The initial ensemble is represented by its peptide distribution  G 0 .  G 0  is assumed to have a Dirichlet process prior with concentration parameter γ and uniform distribution  H  over the protein database 𝒟 as base probability measure. Local ensembles for which representative peptides have been detected are represented explicitly. Each of these ensembles is indexed by its representative peptide  i  and characterized by its peptide distribution  G i .  G i  is assumed to be sampled from a biased Dirichlet process with  G 0  as base probability measure. The peptide π t  following the series π 1 ,…, π t −1  =  i  of detected peptides is sampled from  G i . Each peptide π t  gives rise to an observable fragment ion spectrum  s t , defining the peptide-spectrum match ( s t , π t ). The error model for peptide-spectrum matches is omitted for clarity. See  Section 2.5  for details. Using one of a range of database search engines, tandem mass spectra are then assigned to peptide giving rise to a series of peptide-spectrum matches (Nesvizhskii  et al. ,  2007 ). Note that peptide-spectrum matches are typically highly redundant, i.e. the number of peptide discoveries covered by the peptide-spectrum matches is typically much smaller than the total number of peptide-spectrum matches. Not all peptide-spectrum matches are correct. Various approaches are available to estimate the reliability of peptide-spectrum matches (Elias and Gygi,  2007 ; Keller  et al. ,  2002 ). Target–decoy strategies have shown to be a generic and reliable strategy to estimate false discovery rates for peptide-spectrum matches, i.e. the expected fraction of false positive peptide assignments (Elias and Gygi,  2007 ). At this point, the preliminary result of a series of LC-MS/MS experiments reduces to a series of peptide-spectrum matches that is additionally characterized by some false discovery rate. Shotgun proteomics studies should ideally be designed such that proteome coverage, i.e. the number of discovered peptides increases efficiently with consecutive measurements. For a given series of already performed LC-MS/MS experiments, this requirement translates into the task of estimating the required number of additional experiments that have to be performed to achieve a reasonable increase in proteome coverage. If the estimated effort turns out to be too large, it might be more convenient to consider other experimental setups to analyze the underlying sample. Besides simply giving existing workflows a try, there have been approaches to rationally design promising setups according to statistical analysis of the already acquired peptide-spectrum matches (Brunner  et al. ,  2007 ). To the best of our knowledge, no method specifies the remaining potential of the currently performed experiments by predicting their proteome coverage progression. To close this gap, we present DiriSim, an extended infinite Markov model for LC-MS/MS experiments that yields a posterior prediction of the proteome coverage progression. DiriSim explicitly accounts for true and false positive peptide-spectrum matches by modeling a set of LC-MS/MS experiments as a mixture of an infinite Markov model (Beal  et al. ,  2002 ) and an error model distribution. The expected proteome coverage progression for additional experiments is estimated by sampling from the posterior predictive distribution. We have assessed this approach by cross validation on a set of 37 LC-MS/MS measurements of a complete proteome sample. We show that the extended infinite Markov model outperforms simple extrapolation methods and correctly predicts proteome coverage progression. Extrapolation of the proteome coverage progression further enabled us to specify the maximal coverage of the test set. 2 METHODS The data utilized by DiriSim consists of a list of LC-MS/MS experiments where peptide-spectrum matches have been generated by searching against a protein database 𝒟. Each peptide-spectrum match ( s , π) corresponds to a tandem mass spectrum  s  and its peptide assignment π ∈ 𝒟. Each LC-MS/MS experiment  R l  defines a series of  n l  peptide assignments π ( l ) =π 1 ( l ) ,…, π n l ( l ) . A fraction  q  of all peptide-spectrum matches is assumed to be erroneously assigned. The following sections describe how to predict the progression of proteome coverage conditioned on the given data. In summary, this estimate is achieved by sampling from the posterior predictive distribution given a series of LC-MS/MS experiments and counting the amount of newly discovered peptides. Section 2.1  briefly introduces Dirichlet processes and how these can be used to formally characterize peptide distributions arising in shotgun proteomics experiments.  Section 2.2  characterizes the distribution from which peptides are sampled during an ideal LC-MS/MS experiment without false positive peptide-spectrum matches.  Section 2.3  describes how to sample a series of peptides from such a distribution.  Section 2.4  first describes how to sample from this distribution conditioned on the given data and second how to predict the progression of proteome coverage from the  a posteriori  sampled trajectories.  Section 2.5  completes the framework description by introducing a component accounting for false positive peptide-spectrum matches. Unless otherwise noted, in the following π will denote a series of sampled peptides π t . Capital italic latin letters like  G ,  H  will denote distributions. 2.1 Dirichlet processes priors for peptide distributions In the course of a shotgun proteomics experiment, peptides are sampled from an unknown distribution and then identified by mass spectrometrical analysis. This distribution is defined by the biological sample contributing a characteristic set of proteins/peptides and by the experimental setup enriching/depleting particular types of proteins/peptides. The more samples we draw from this distribution, i.e. the more experiments we perform, the better we are able to characterize the distribution and thereby predict the future progression of peptide discoveries. The incremental estimation procedure is captured by a non-parametric Bayesian technique, denoted as  Chinese restaurant processes  (Blackwell and MacQueen,  1973 ). The Chinese restaurant process can be envisioned as a schematic task where  n  customers are to be seated in a restaurant with an infinite number of tables. At each table a particular dish is served that is denoted by its number in the menu. The first customer is seated at the first table and offered the corresponding dish π 1 . The  t -th subsequent customer is offered his dish π t  after having been seated either at an already populated table or at a new unpopulated table according to the following probabilities:
 (1) 
where  n i  corresponds to the number of customers already sitting at the table serving dish  i . In case a customer happens to be seated at a new table, the dish served at this table is drawn from the base probability measure  H . γ is referred to as the concentration parameter of the process. The larger γ, the higher the chances that a new customer is seated at a new table. The more customers have already been seated, the less likely it will open up a new table. Let us now assume that we do not know γ and have seated  n  customers. We want to estimate how many tables will be occupied, or equivalently how many different dishes will be served after  m  additional customers have been seated. In a first step, we characterize the seating distribution by fitting γ according to the observed seating arrangement, i.e. the more tables we find populated the larger we choose γ. We can now simulate  m  additional seating events using the γ estimate and thereby estimate the number of tables occupied afterwards. By identifying dishes with peptides and, respectively, customers with mass spectra, we obtain a simple model to sample peptide assignments, i.e. simulate experiments and in particular estimate the expected number of new peptide discoveries. Although being overly simple, this model captures an essential property of shotgun proteomics experiments. While always allowing to discover a novel peptide with non-zero probability, the overall progression of new discoveries slows down for a growing number of experiments. It turns out that a Chinese restaurant process with concentration parameter γ implements draws π t  from a discrete distribution  G  that itself is drawn from a prior distribution referred to as Dirichlet process DP with concentration parameter γ and base probability measure  H  (Antoniak,  1974 ; Ferguson,  1973 ):
 (2) 
Dirichlet processes have proven to be useful to formally express and deal with the uncertainty of an unknown discrete distribution, e.g. mixing distributions of mixture models. In this work, we assume Dirichlet process priors for distributions over peptides and sample from them by using the Chinese restaurant process construction. 2.2 Infinite Markov model for LC-MS/MS experiments During an LC-MS/MS experiment, peptides designated for tandem mass spectrometry are sampled from a multitude of unknown distributions ( Fig. 1 ). This section describes how to model these distributions with an infinite Markov model. The peptides in the initial ensemble are distributed according to an unknown discrete distribution  G 0 . We assume a Dirichlet process prior DP(γ,  H ) for  G 0  with base probability measure  H  and concentration parameter γ.  H  is assumed to be the uniform distribution over the peptides defined by the protein database 𝒟. Note that the prior DP(γ,  H ) does not necessarily identify  G 0  with  H , i.e. the uniform distribution over the protein database 𝒟. Peptides are not directly sampled from  G 0  in an LC-MS/MS experiment ( Fig. 1 ). In the course of liquid chromatography, the mass spectrometer is exposed to a subpopulation of the initial ensemble, confined to members within a time-dependent polarity range. Depending on the time point  t , peptides are thus sampled from a characteristic peptide distribution  G t  that is ‘related’ to  G 0 . The prior for  G t  has to capture the dependency on  G 0 . We particularly require the support of  G t  to be contained in the support of  G 0 . While retaining flexibility, this requirement is met by choosing the prior for  G t  to be a Dirichlet process with base probability measure  G 0  and concentration parameter β (Teh  et al. ,  2006 ). Due to technical difficulties to reproduce absolute time courses for a series of LC-MS/MS experiments, we abstain from explicitly modeling polarity and, thereby,  G t . Instead, we represent time or, respectively, ensemble polarity by peptide identities. We denote  G i  as the local peptide distribution at the time points where peptide  i  has been identified. Assume that we have sampled π t −1  =  i  in the course of an experiment. Since π t −1  =  i  is indicative for the current polarity, we assume the subsequent peptide π t  to be sampled from the local distribution  G i  ( Fig. 1 ). This representation induces a Markov chain whose states correspond to the identified peptides. We assume each state sequence π to begin at a distinguished start state π ∗ , i.e. we assume π 0 ∼δ π ∗ . Following (Beal  et al. ,  2002 ), we define the prior of  G i  to be a biased Dirichlet Process DP i  with base probability measure  G 0 , concentration parameter β and additional prior weight α on state  i . Thereby, α explicitly controls the rate of sampling self−transitions π t  = π t −1  =  i . Having a Dirichlet process prior on  G 0 , the number of sampled states is not fixed a priori and steadily grows with the number of sampled transitions. Due to the Dirichlet process prior on the local probability distributions  G i , the occurrence of transitions evolves in a similar fashion. We obtain the full characterization of the distribution that is sampled in the course of an LC-MS/MS experiment:
 (3) 2.3 Sampling sequences of peptide identifications In the following, we describe how to sample series of peptides from the distribution defined in the preceding section. Assume that α, β, γ,  H ,  q  are given and  m  series π = π (1) ,…, π ( m )  are to be sampled sequentially. We assume each series π ( k )  to begin at a distinguished start state π ∗ . π can be sampled in ascending order. To see this, assume that we already sampled the trajectory π 0 , π 1 ,…, π t −1 . In order to sample the subsequent peptide, we have to specify the distribution for π t  | π 0 , π 1 ,…, π t −1 , α, β, γ,  H . Starting from the hierarchy of Dirichlet processes ( 3 ) and after integrating out  G π t −1  and  G 0  we obtain a nested variant of the Chinese restaurant process construction ( 1 ) for the infinite Markov model:
 (4) 
 n ij ( t ) corresponds to the number of occurrences of observing the transition from peptide  i  to peptide  j  in the series π 0 ,…, π t −2 .  n o j ( t ) denotes how many times peptide  j  has been observed as a new transition target in the series π 0 ,…, π t −1 .  T i ( t ) is shorthand for (∑ j n ij ( t )+α+β) −1  and  T o ( t ) for (∑ j n o j +γ) −1 . The outcome ‘self’ denotes a self-transitions π t  = π t −1 . Accordingly, ‘non-self’ corresponds to already observed transitions π t  ≠ π t −1 . Note the distinguished role of self-transitions by the prior weight α. While the event ‘new target’ refers to the discovery of a new transition to a peptide already observed in another context, ‘new state’ denotes the discovery of a yet unobserved peptide. It is straight forward to sample the random variable π t  | π 0 , π 1 ,…, π t −1  =  i , α, β, γ,  H  since its distribution has a closed form and only depends on the given parameters and quantities defined by the series of preceding peptide assignments. 2.4 Posterior prediction of proteome coverage progression This section describes how to sample peptide series conditioned on already observed series. This task translates to sampling the posterior predictive distribution for π new  given the observed peptides π. Proteome coverage progression for future experiments is estimated by approximating the expected number  E [|𝒰(π new )| | π,  H ] of new peptide discoveries 𝒰(π new ) upon posterior predictive sampling. The posterior predictive distribution for π new  | π,  H  has no closed form. For sufficiently large series π, the posterior predictive distribution can be reasonably approximated by π new  | π, θ ML ,  H  where θ ML  corresponds to the maximum likelihood estimate for θ ≔(α, β, γ)
 (5) 
We predict the proteome coverage progression by approximating  E [|𝒰(π new )| | π,  H ] by averaging over a set of trajectories π 1 , π 2 ,… sampled from π new  | π, θ ML ,  H  as described in  Section 2.3 . 2.5 Proteome coverage progression with false identifications Sequences π of peptide assignments were assumed to be perfect in the preceding sections. Obviously this assumption does not hold in practice. This section describes an extension of the infinite Markov model by an error model that is able to deal with series of peptide assignments that are afflicted with a non-zero false discovery rate  q . We observe that false positive peptide assignments map to the decoy database in a non-redundant fashion, i.e. 83% of all decoy peptide discoveries of the test dataset (see  Section 3 ) are supported only by a single peptide assignment. Assuming that false positive peptide assignments distribute like decoy peptide assignments (Elias and Gygi,  2007 ), we approximate the distribution of false positive peptide assignments with  H , i.e. the uniform distribution over the protein database. In order to model the fraction  q  of false positive peptide assignments, we assume that peptide assignments are sampled from a mixture model with two components. The first component accounting for the true positive peptide assignments is given by the infinite Markov model as described in  Section 2.2 . The second component is given by the distribution of false positive peptide assignments, i.e.  H . Component weights are chosen according to the false discovery rate  q . Consequently, the first and second component are weighted 1− q  or  q , respectively. Series of peptide assignments are generated by sampling each peptide assignment π t  either from the infinite Markov model as described in  Section 2.3  or directly from  H , according to the components weights. Posterior sampling requires the estimate θ ML  from an already observed series π. Exact computation of θ ML  though involves an intractable sum over configurations of false positive peptide assignments. We approximate θ ML  by assuming that the number of false positive peptide assignments equals the expected value  n (1− q ) and that these distribute uniformly over π. This assumption allows us to approximate  P (π | θ,  H ,  q ) with adjusted transition counts, e.g.  .
 (6) 
Proteome coverage progression is then predicted as described in  Section 2.4 . 3 RESULTS In the following, we show results that first, demonstrate that prediction of proteome coverage progression is a non-trivial task that is not solved satisfactory by simple extrapolation methods and second, that the extended infinite Markov model can confidently predict proteome coverage progression from a small number of already performed experiments and third, that we can identify the putative number of LC-MS/MS experiments to be carried out until reaching maximal coverage. We conducted simulation studies to ensure that we can confidently estimate α, β, γ. Therefore, we generated a dataset by simulating peptide series with false discovery rate of 1% as described in  Section 2.5 . Parameters α, β, γ were chosen in a range also observed in the real-world test dataset that is introduced later. We assessed the estimates on 20 simulated series, each corresponding to multiple LC-MS/MS experiments. Each set of 20 series was chosen to be of length ranging from 1000 to 15 000 peptide assignments. For each of these series we estimated α, β, γ as described in  Sections 2.4  and  2.5  ( Fig. 2 ). It can be seen that α, β, γ can be reasonably recovered even from the smallest training series. The larger the series grows the more precise the estimates become. The approximations introduced in  Section 2.5  to account for false positive peptide assignments do not compromise the parameter estimates. Considering the equivalent of six or more LC-MS/MS experiments already yielded satisfactory estimates.
 Fig. 2. θ ML  estimate on simulated data. Performance is evaluated for different training set sizes, i.e. series of peptide assignments (psm) of length ranging from 1000 to 15 000. Performance is reported as log odds of predicted and true parameter value. Results are shown for parameters α, β, γ, respectively, governing the events of self-transitions ( a ), new transitions ( b ) and globally new discoveries ( c ). It can be seen that the parameters can be confidently estimated considering a training series of 10000 peptide assignments. We assessed DiriSim's ability to predict proteome coverage progression for real LC-MS/MS experiments. We consider proteome coverage to be the number of peptide discoveries, i.e. the number of different peptides represented in the series of peptide assignments. We were particularly interested to see how many LC-MS/MS experiments are needed to confidently extrapolate the progression of peptide discoveries. We expected that confident extrapolation is feasible after training DiriSim on a small training series of peptide assignments corresponding to a small number of LC-MS/MS experiments. To this end, we applied DiriSim to a test dataset covering 37 LC-MS/MS experiments of the complete  Drosophila melanogaster  proteome (Schmidt  et al. ,  2008 ). Peptide-spectrum matches were generated by searching against a target–decoy protein database (tryptic, ≤1 missed cleavage), for details see (Schmidt  et al. ,  2008 ). For our study, we selected top-scoring peptide-spectrum matches mapping to the target database at a false discovery rate of 1% as described in (Elias and Gygi,  2007 ). By this means, we finally considered 61 582 peptide-spectrum matches. We generated training series of varying size by subsampling the dataset, extrapolated the progression of peptide discoveries for each training series and compared to the observed progression of the complete dataset. In total, we subsampled 120 training series of peptide assignments. Note that the subsampling procedure has to preserve the peptide assignments order within the individual LC-MS/MS experiments. Therefore, we generated the training series by subsampling complete LC-MS/MS experiments. We subsampled 1, 2, 3, 4, 5 and 10 LC-MS/MS experiments, giving rise to 6 training series of peptide assignments. By repeating this step 20 times we generated a total of 120 training series. For instance, one of the training series comprised the series of 1139 peptide assignments defined by the two LC-MS/MS experiments with index 14 and 18 (out of all 37 experiments). The 120 training series varied in size, ranging from 596 to 20 277 peptide assignments, i.e. covering up to one-third of the complete dataset's peptide assignments. Note that two training series that were generated by subsampling the same number of LC-MS/MS experiments do not necessarily comprise the same number of peptide assignments. This is due to the heterogeneous number of peptide assignments contributed by the individual LC-MS/MS experiments. We extrapolated the progression of peptide discoveries for each training sequence and compared to the observed progression of the complete dataset. Therefore, we estimated α, β, γ and estimated the expected proteome coverage progression by averaging over 50 series sampled from the posterior predictive distribution of the extended infinite Markov model (see  Sections 2.4  and  2.5 ). Goodness of the prediction was evaluated as rmsd from the observed progression of the complete dataset. Training series in corresponding to six or more average LC-MS/MS experiments (approximately 1600 peptide assignments) yield good matches ( Fig. 3 a and b). These results demonstrate that first, the principles governing the yield of LC-MS/MS experiments seem to be well captured by the extended infinite Markov model and second, proteome coverage progression can be confidently predicted from a considerably small set of experiments.
 Fig. 3. Prediction of proteome coverage progression for a dataset comprising 37 LC-MS/MS experiments each giving rise to a series of peptide assignments (psm). We generated 120 training series of varying size (train psm) by subsampling complete LC-MS/MS experiments. We predicted the progression of proteome coverage (peptide discoveries) for each training series and compared to the progression observed for the series of the complete dataset. ( a ) Prediction accuracy for the 120 training series. Prediction accuracy is given as root mean square deviation (rmsd) from the observed progression of peptide discoveries. ( b ) Concatenated training and respective predicted progressions (black) from the largest three training series [corresponding items in (a) are encircled] compared to observed progression (red). Vertical lines denote the size of the training series. Vertical lines overlap due to similar sizes around 20 000. ( c ) Comparison of DiriSim with linear extrapolation of proteome coverage progression of last LC-MS/MS experiment in training series (linear) or respectively extrapolation of logarithmic regression of training series (log). Box plot of log odds of rmsd [log(rmsd DiriSim /rmsd compare )] for DiriSim and compared method (linear, log) on the 120 training series. Median log odds for comparison with the extrapolation methods linear and log are lower than 0, indicating weaker performance than DiriSim. 
 We compared DiriSim with other extrapolation methods. We chose two simple general purpose extrapolation methods since there do not exist specific methods for proteome coverage prediction. We first considered an extrapolation scheme that linearly extrapolated proteome coverage progression of the last LC-MS/MS experiment of a training series. Second, we considered the extrapolation of a logarithmic regression ( y = a log x  +  b ). We assessed prediction performance on the 120 training series as described above and observed that DiriSim clearly outperforms both extrapolation methods ( Fig. 3 c). These results indicate that proteome coverage prediction is a non-trivial task that is not solved satisfactory by  ad hoc  extrapolation methods. We further extrapolated the coverage progression 5-fold beyond the range covered by the test dataset ( Fig. 4 a). The progression of peptide discoveries for all peptide assignments shows a linear increase. Since DiriSim explicitly models true and false positive samples, we could exclusively monitor the series of true positive peptide assignments. We observe a pronounced divergence of the progression for all assignments and the exclusively true positive ones. We particularly see that the progression of true positive discoveries stagnates considerably. While the fraction of false positive peptide assignments is constantly held at 1%, the fraction of false positive peptide discoveries at the end of the predicted progression amounts to &gt;30%. The fraction of false positives among the novel discoveries beyond the range of the test set even surmounts 60%. Tolerating a limited amount of false positive peptide discoveries, bounds the maximal number of possible peptide discoveries as well as the number of experiments having to be performed ( Fig. 4 b). For instance, assume that we require that at most 15% of all peptide discoveries are false positive. This constraint restricts the maximally achievable coverage since we can discover at most 5000 distinct true positive peptides. According to  Fig. 4 a we will have reached this point after having acquired 90 000 peptide assignments.
 Fig. 4. The 5-fold extrapolation beyond the range of the test dataset. ( a ) Observed progression of the test dataset in red, predicted progression with standard deviations of all (black) and only true positive (green) peptide discoveries. The progression of true positive discoveries stagnates considerably. ( b ) Relates the absolute number of true positive (tp) peptide discoveries to the fraction of false positive discoveries (fdr peptide discoveries). The fraction of false positive peptide discoveries grows steadily with the total amount of peptide discoveries. Quality requirements on the final set of peptide discoveries limit the maximally achievable proteome coverage as well as the sensible number of LC-MS/MS experiments. 
 4 DISCUSSION To date, it is not clear beforehand how often to repeat an LC-MS/MS experiment on a single biological sample in order to efficiently achieve satisfactory proteome coverage. Furthermore, the maximally achievable proteome coverage with a particular method is not known. We address these issues by presenting DiriSim, a framework to predict the progression of proteome coverage for LC-MS/MS experiments. DiriSim models a series of LC-MS/MS experiments as an infinite Markov model, whose states correspond to peptides. We apply DiriSim to extrapolate the proteome coverage progression of a small number of already performed LC-MS/MS experiments. Note that this task is different to the  a posteriori  inference of the state sequence of these experiments. In contrast to previous applications (Beal  et al. ,  2002 ; Sohn and Xing,  2007 ), a posteriori inference of the state sequence is furthermore not necessary, since the states (peptides) are already assigned to the observable variables (tandem mass spectra) by means of the corresponding peptide-spectrum matches. Besides its application in proteome coverage prediction, the infinite Markov model could though serve as a prior in a Bayesian peptide identification setting and, in particular, prevent the accumulation of false positive peptide discoveries coming along with increasing dataset size. LC-MS/MS experiments are typically analyzed by database searching. The underlying protein databases are large but still of finite size and therefore define a finitely large set of possibly identified peptides.  De novo  sequencing approaches infer peptide identities without relying on protein databases and thereby implicitly support an infinite number of possible peptide identities. Using an appropriate base probability measure  H , the proposed infinite Markov model for LC-MS/MS experiments naturally lends itself to predict the proteome coverage in this context. We have shown that DiriSim correctly extrapolates proteome coverage progression from at most 10 LC-MS/MS experiments and outperforms  ad hoc  extrapolation methods. Proteome coverage prediction appears to be a non-trivial task due to the intricate dependency structure of an LC-MS/MS experiment. DiriSim provides a comprehensive non-parametric Bayesian characterization of an LC-MS/MS experiment that enabled us to confidently predict proteome coverage. Although capturing the dependencies of LC-MS/MS experiments, DiriSim remains a robust, non-complex model since it only needs three parameters that are to be learnt from data. By explicitly modeling false and true positive peptide assignments, DiriSim enables us to specify the maximally achievable proteome coverage with regards to true positive peptide discoveries. We have seen in the simulations that new peptide discoveries from extensive repetition of LC-MS/MS experiments mostly accumulate false positive discoveries. This observation reflects the difference between the distributions for true and false positive peptide assignments. While true positive peptide assignments concentrate over a small subset of the protein database, false positive peptide assignments distribute broadly over the protein database and therefore mostly contribute false positive peptide discoveries. Due to the exceedingly broad distribution of decoy matches, we do not expect that errors possibly introduced by the uniformity approximation compromise the observed accumulation of false positive peptide discoveries. We conclude that performing more and more experiments seeking for maximal coverage mainly deteriorates the overall quality of the complete peptide discovery set. Depending on the false discovery rate of the peptide assignments, a quality requirement on the set of peptide discoveries imposes an upper bound to the total number of experiments, which therefore, potentially limits the maximally achievable proteome coverage before the progression of true positive peptide discoveries is fully saturated. This limitation accrues from the occurrence of erroneous peptide-spectrum matches and their broad distribution over the protein database. As long as peptide-spectrum matches are afflicted with uncertainty, this reasoning holds for any proteome being studied. It will though be interesting to apply DiriSim to other datasets in order to study the quantitative impact of factors like proteome size and experimental setup on the maximally achievable proteome coverage. In summary, our results suggest that the design of large shotgun proteomics studies should focus on efficiency not only to save resources but, most importantly, to yield reliable peptide discoveries. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>A unifying framework for joint trait analysis under a non-infinitesimal model</Title>
    <Doi>10.1093/bioinformatics/bty254</Doi>
    <Authors>Johnson Ruth, Shi Huwenbo, Pasaniuc Bogdan, Sankararaman Sriram</Authors>
    <Abstract/>
    <Body>1 Introduction Genome-wide association studies (GWAS) have identified thousands of regions in the genome that contain variants that contribute to risk for many diseases. Many of these risk regions have been implicated in multiple phenotypes such as autism and schizophrenia (Autism Spectrum Disorders Working Group of The Psychiatric Genomics Consortium  et al. , 2017), multiple autoimmune diseases ( Cotsapas  et al. , 2011 ;  Ramos  et al. , 2011 ;  Richard-Miceli and Criswell, 2012 ), Crohn’s disease and psoriasis ( Ellinghaus  et al. , 2012 ), and many others. Understanding which causal variants are shared among diseases can provide novel etiological insight as well as provide evidence of potential shared causal mechanisms between complex traits. In addition, identifying which variants contribute to multiple traits can help decipher which molecular traits (e.g. gene expression) contribute to disease risk ( Giambartolomei  et al. , 2014 ;  Hormozdiari  et al. , 2016 ); genetic variants that causally alter gene expression as well as disease risk can link a particular gene to a given disease. Genetic overlap has been analyzed both at the genome-wide level and local level, where the latter refers to analysis done within a given genomic region.  Genetic correlation , a measure that quantifies the similarity in the genetic effects on pairs of traits, is commonly used for assessing the relationship between two traits and can be applied either genome-wide or to local data ( Bulik-Sullivan  et al. , 2015 ;  Shi  et al. , 2017 ). Many of the models for estimating genome-wide genetic correlation assume an  infinitesimal  genetic architecture where all SNPs, or single nucleotide polymorphisms, are assumed to have a very small effect on the trait. In contrast to genetic correlation,  colocalization  methods aim to estimate whether the GWAS association signals for two traits at the same region are due to the same causal variant across the traits or chance ( Giambartolomei  et al. , 2014 ;  Hormozdiari  et al. , 2016 ). The methods that relax the infinitesimal assumption either assume a single causal variant per region or limit the number of potential causal variants a priori, often due to computational considerations ( Giambartolomei  et al. , 2014 ;  Hormozdiari  et al. , 2016 ). Although, both genetic correlation and colocalization aim to describe the genetic sharing between traits, these methods have been utilized largely independently of each other. In this work, we present a unifying statistical model that ties together genetic correlation and colocalization. To accomplish this, we present a fully generative Bayesian statistical model that models the shared as well distinct genetic variants underlying a pair of traits. The model allows for sparse genetic architectures (where only a small fraction of variants are causally impacting the traits). The model is richly parametrized: allowing us to jointly model global parameters such as the proportion of variants that are causal for both as well for either trait, the trait heritability, the correlation of the effect sizes at the causal SNPs and local parameters such as the effect of a single SNP on each of the traits. A challenge of a non-infinitesimal genetic architecture is that it presents a computationally challenging inference problem. Performing inference under this model often involves explicitly enumerating all causal configurations of the SNPs. This exponential search space of  2 2 M , where  M  is the number of SNPs analyzed, proves intractable given the large genetic datasets now available. We propose Unifying Non-Infinitesimal Trait analYsis (UNITY) that relies on Markov Chain Monte Carlo (MCMC) to approximate the posterior probabilities of the model parameters. In this work, we focus on estimating the proportion of shared and trait-specific causal variants since parameters such as heritability and genetic correlation can be estimated using previous methods ( Bulik-Sullivan  et al. , 2015 ). Additionally, a key advantage of the method is that it only requires summary level association statistic data, which bypasses many of the privacy concerns associated with individual level data. With the widespread availability of GWAS summary statistics ( Pasaniuc and Price, 2017 ), we expect that a method operating only on summary statistics would prove most useful for the research community. Through comprehensive simulations and an analysis of height and body mass index (BMI), we show that our method can accurately estimate the proportion of shared causal SNPs between two complex traits. 2 Materials and methods 2.1 Generative model Here, we introduce a Bayesian framework for estimating the proportion of causal variants shared between a pair of complex traits. The input to our method is the vector of signed effect sizes at each SNP for each trait (we only analyze SNPs for which effect size estimates are available for both traits). We model the genetic as well as non-genetic variances in each trait, the genetic correlation among the traits, and the proportion of causal SNPs that are shared across traits as well as are unique to each. The proportion of causal SNPs shared between the traits is denoted by  p 11 , the proportion of causal SNPs specific to trait 1 and trait 2 as  p 10  and  p 01 , respectively, and the proportion of non-causal SNPs is denoted by  p 00 , where  p 00 + p 10 + p 01 + p 11 = 1 . For each trait  p ∈ { 1 , 2 } , we denote the genetic variance  σ p 2  (which is the same as its heritability as  h p 2  if the trait is standardized), the environmental noise as  σ e p 2 = 1 − h p 2 N p , where  N p  denotes the sample size for trait  p , and the genetic correlation between the two traits as ρ. Altogether, our model has the following parameters:  ( σ 1 2 , σ 2 2 , ρ , p 00 , p 10 , p 01 , p 11 ) . We assume that trait  p  ( p ∈ { 1 , 2 } ) measured in individual  i ,  y p , i  is a linear function of standardized genotypes  x = ( x i , 1 , … , x i , M )  measured across  M  SNPs with SNP effect sizes  β p = ( β p , 1 , … , β p , M )  and independent additive noise term  ϵ p , i . Further, we assume that there are no sample overlaps across the two studies.
 y 1 , i = ∑ m = 1 M β 1 , m x i , m + ϵ 1 , i     y 2 , i = ∑ m = 1 M β 2 , m x i , m + ϵ 2 , i i ∈ { 1 , … , N 1 }     i ∈ { 1 , … , N 2 } ϵ p , i ∼ iid N ( 0 , σ e p 2 ) A SNP  m  is causal for trait  p  if its true effect  β p , m ≠ 0  and it is not causal otherwise. We denote the probability of a SNP being causal for every combination of the two traits as:  p = ( p 00 , p 10 , p 01 , p 11 ) . Denoting the causal effect sizes for trait  p , p ∈ { 1 , 2 }  across all SNPs  γ p = ( γ p , 1 , … , γ p , M ) , we assume that the causal effect sizes for each SNP are independent, allowing us to model the effect sizes at SNP  m  for each of the two traits  ( γ 1 , m , γ 2 , m )  as a random vector drawn from a bi-variate normal distribution centered at zero with the following covariance matrix:
 ( γ 1 , m γ 2 , m )   |   ( σ 1 2 , σ 2 2 , ρ , p ) ∼ N ( ( 0 0 ) , ( σ 1 2 M ( p 11 + p 10 ) σ 1 σ 2 ρ M ( p 11 ) σ 1 σ 2 ρ M ( p 11 ) σ 2 2 M ( p 11 + p 01 ) ) ) 
 C p = ( C p , 1 , … , C p , M )  denotes the causal indicator vector for trait  p , where  C p , m = 1  if SNP  m  is causal for trait  p  and 0 otherwise.  ( C 1 , m , C 2 , m )  is a random vector drawn from a discrete distribution with parameters given by  p :
 P ( ( C 1 , m C 2 , m ) = ( a b )   |     p ) = p a b ,   a , b ∈ { 0 , 1 } The true effect sizes for each trait  p  at SNP  m ,  β p , m , conditioned on the causal status at a SNP is the element-wise product of the causal indicator vector and the true causal effect sizes.
 ( β 1 , m β 2 , m )   |     ( C 1 , m C 2 , m ) , ( γ 1 , m γ 2 , m ) = ( γ 1 , m C 1 , m γ 2 , m C 2 , m ) 
We can model the conditional distribution of the GWAS summary statistics given the true effect sizes, where  β ^ p , m  is the estimated marginal effect size of the  m th SNP for trait  p  ( Shi  et al. , 2017 ):
 ( β ^ 1 , 1 : M β ^ 2 , 1 : M ) |     ( ( β 1 , 1 : M β 2 , 1 : M ) , σ e 1 2 , σ e 2 2 ) ∼ N ( ( V β 1 , 1 : M V β 2 , 1 : M ) , Σ e ) Σ e = ( σ e 1 2 V 0 0 σ e 2 2 V ) 
 V  is the matrix of correlations among the SNPs, i.e. the linkage disequilibrium (LD) matrix.  V  can be estimated from a reference panel of genotypes collected from a population that is genetically similar to the populations for which summary statistics are available. Alternately, when performing inference at the genome-wide level, we can prune the list of SNPs such that they come from independent LD blocks. LD-pruning creates an approximately independent subset of SNPs in which case  V  can be approximated by the identity matrix,  I . In this work, we restrict our attention to the case where  V ≈ I . We impose a Dirichlet prior on  p :
 ρ | λ ∼ Dir ( λ ) Here  λ = ( λ 1 , λ 2 , λ 3 , λ 4 ) . In practice, we set  λ 1 = λ 2 = λ 3 = λ 4 = λ = 0.20 : In principle, we can also impose priors on the remaining parameters, i.e. the trait heritability  ( σ 1 2 , σ 2 2 )  and their genetic correlation ρ and estimate all of these parameters jointly with  p  in a fully Bayesian model. These parameters can be estimated using other methods ( Bulik-Sullivan  et al. , 2015 ) and, in this work, we fix the values of these parameters to their estimates and focus on estimating  p . Given the parameters ( σ 1 2 ,   σ 2 2 ,ρ,  λ ), the joint distribution of the probability of causal configurations  p , the causal indicator vectors  C 1 , C 2 , the causal effect sizes  γ 1 , γ 2 , and the estimated effect sizes  β ^ 1 , β ^ 2  is given by:
 P ( β ^ 1 , β ^ 2 , C 1 , C 2 , γ 1 , γ 2 , p | σ 1 2 , σ 2 2 , ρ , λ ) = P ( p | λ ) × ∏ m = 1 M { P ( ( C 1 , m C 2 , m ) | p ) P ( ( γ 1 , m γ 2 , m ) | ( σ 1 2 , σ 2 2 , ρ , p ) ) × P ( ( β ^ 1 , m β ^ 2 , m ) | ( ( γ 1 , m γ 2 , m ) , ( C 1 , m C 2 , m ) , σ e 1 2 , σ e 2 2 ) ) } Integrating over the hidden variables  C 1 , C 2 ,   γ 1 , γ 2 , we obtain:
 P ( β ^ 1 , β ^ 2 , p | σ 1 2 , σ 2 2 , ρ , λ ) ) = P ( p | λ ) × ∏ m = 1 M [ ∫ ∑ C 1 , m C 2 , m { P ( ( C 1 , m C 2 , m ) | p ) P ( ( γ 1 , m γ 2 , m ) | ( σ 1 2 , σ 2 2 , ρ , p ) ) × P ( ( β ^ 1 , m β ^ 2 , m ) | ( ( γ 1 , m γ 2 , m ) , ( C 1 , m C 2 , m ) , σ e 1 2 , σ e 2 2 ) ) } d γ 1 , m d γ 2 , m ] = Dir ( p ; λ ) × ∏ m = 1 M [ p 00 N ( ( β ^ 1 , m β ^ 2 , m ) ; ( 0 0 ) , ( σ e 1 2 0 0 σ e 2 2 ) ) + p 10 N ( ( β ^ 1 , m β ^ 2 , m ) ; ( 0 0 ) , ( σ 1 2 M ( p 11 + p 10 ) + σ e 1 2 0 0 σ e 2 2 ) ) + p 01 N ( ( β ^ 1 , m β ^ 2 , m ) ; ( 0 0 ) , ( σ e 1 2 0 0 σ 2 2 M ( p 11 + p 01 ) + σ e 2 2 ) ) + p 11 N ( ( β ^ 1 , m β ^ 2 , m ) ; ( 0 0 ) , ( σ 1 2 M ( p 11 + p 10 ) + σ e 1 2 σ 1 σ 2 M ( p 11 ) ρ σ 1 σ 2 M ( p 11 ) ρ σ 2 2 M ( p 11 + p 01 ) + σ e 2 2 ) ) ] 2.2 Parameter inference in our model Given the generative model described in the previous section, the inference problem lies in computing the posterior distribution of  p  given the estimated summary statistics
 P ( p | β ^ 1 , β ^ 2 , σ 1 2 , σ 2 2 , ρ , λ ) = P ( p , β ^ 1 , β ^ 2 | σ 1 2 , σ 2 2 , ρ , λ ) P ( β ^ 1 , β ^ 2 | σ 1 2 , σ 2 2 , ρ , λ ) The true joint posterior distribution is intractable. Thus, we use MCMC ( Brooks  et al. , 2011 ) to approximate the posterior distribution. MCMC approximates the target posterior distribution  
 P ( p   |   β ^ 1 , β ^ 2 , σ 1 2 , σ 2 2 , ρ )  by a sequence of random samples  ( p ( t ) ) t = 1 T  drawn from a Markov chain constructed so that the stationary distribution of the chain is the target posterior.
 P ( p   |   β ^ 1 , β ^ 2 , σ 1 2 , σ 2 2 , ρ , λ ) ≈ 1 T ∑ t = 1 T δ p ( t ) ( p ) In our setting, we use a random-walk Metropolis–Hastings algorithm ( Metropolis  et al. , 1953 ) that generates a sample  p ( t + 1 )  at iteration  t  + 1 given the sample  p ( t )  at the previous iteration using the following proposal distribution that generates a proposed sample  p *  which is then accepted or rejected depending on the Metropolis–Hastings ratio (which depends on the ratio of the posterior probability density at the proposed parameter to the previous parameter):
 p * ∼ Dir ( d ) d = λ + B p ( t ) 
Here,  B  is a constant that controls the variance of the proposal distribution. In practice, we found that  B  = 10 yields effective mixing. The final step in specifying the MCMC algorithm lies in computing the ratio of the posterior probability density at the proposed parameter to the original parameter. Computation of the ratio requires the evaluation of the posterior probability only up to a normalization constant:
 P ( p | β ^ 1 , β ^ 2 , σ 1 2 , σ 2 2 , ρ , λ ) ∝ P ( β ^ 1 , β ^ 2 , p | σ 1 2 , σ 2 2 , ρ , λ ) = Dir ( p ; λ ) × [ ∏ m = 1 M N ( ( β ^ 1 , m β ^ 2 , m ) ; ( 0 0 ) , ( σ e 1 2 0 0 σ e 2 2 ) ) · ( p 00 ) + N ( ( β ^ 1 , m β ^ 2 , m ) ; ( 0 0 ) , ( σ 1 2 M ( p 11 + p 10 ) + σ e 1 2 0 0 σ e 2 2 ) ) · ( p 10 ) + N ( ( β ^ 1 , m β ^ 2 , m ) ; ( 0 0 ) , ( σ e 1 2 0 0 σ 2 2 M ( p 11 + p 01 ) + σ e 2 2 ) ) · ( p 01 ) + N ( ( β ^ 1 , m β ^ 2 , m ) ; ( 0 0 ) , ( σ 1 2 M ( p 11 + p 10 ) + σ e 1 2 σ 1 σ 2 M ( p 11 ) ρ σ 1 σ 2 M ( p 11 ) ρ σ 2 2 M ( p 11 + p 01 ) + σ e 2 2 ) ) · ( p 11 ) ] 2.3 Efficient mixing of MCMC chains In any practical application of MCMC, the number of iterations, burn-in period, and initialization point are critical to ensuring convergence and accurate estimates. Slow mixing of the MCMC chains can occur if the starting point is at a region of low posterior density. As opposed to selecting a random starting point, we carefully select the initialization of each chain by choosing the set of parameters that yields the highest posterior density. We use the limited-memory Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm ( Byrd  et al. , 1994 ) to determine the maximum a posteriori estimates for  p 00 , p 10 , p 01 , p 11 . We repeat this 10 times, initializing the optimization algorithm with random starting values drawn from the prior. We compute the posterior density of all 10 candidate starting values and select the set that yields the highest density. This set of parameters is then used as the starting point for our MCMC chain. In addition, to diagnose convergence, we use 100 Markov chains all initialized using the scheme described above. Our final estimate is the mean of all samples drawn from the 100 chains. 2.4 Note on runtime We assessed the performance based on the number of seconds per iteration of the MCMC sampler. The main computation is calculating the likelihood at each iteration, which is directly dependent on the number of SNPs per trait. The complexity of the algorithm is  O ( m ) , where  m  is the number of SNPs. We empirically demonstrate that our method is linear in the number of SNPs through simulation ( Supplementary Fig. S1 ). In addition, the runtime is invariably connected to the number of iterations required for the MCMC to converge. We find that using the maximum a posteriori probability (MAP) estimate as an initialization value leads to fast convergence, requiring only 500 iterations in practice. 3 Results UNITY provides a novel generalized framework to jointly model GWAS summary statistics data of two complex traits, incorporating fundamental genetic parameters, such as heritability and genetic correlation, and makes minimal assumptions in inference procedures. Since UNITY assumes a non-infinitesimal model, it allows for very sparse genetic architectures, i.e. by setting  p 00 ≈ 1 . However, this non-infinitesimal model can also be generalized to the infinitesimal model by setting  p 00 ≈ 0 , p 10 ≈ 0 , p 01 ≈ 0 , p 11 ≈ 1 . 3.1 UNITY generalizes colocalization and genetic correlation We discuss a comparison of the parameters of UNITY with those obtained by other methods that perform cross-trait analysis and the underlying assumptions of each method. We first analyze the cross-trait LD score regression model ( Bulik-Sullivan  et al. , 2015 ), which estimates genome-wide genetic correlation based on the random-effect model, making the implicit assumption that every SNP has a non-zero effect. In contrast to cross-trait LD score regression, UNITY assumes a generalized non-infinitesimal model, explicitly modeling a sparse genetic architecture. We also compare UNITY with methods that do not make the infinitesimal model assumption. While models such as PleioPred explicitly model the proportion of trait-specific and shared causal variants  p 00 , p 10 , p 10 , p 11 , the main goal of this method is to perform genetic risk prediction ( Hu  et al. , 2017 a) rather than estimating these proportions. We compare UNITY with COLOC ( Giambartolomei  et al. , 2014 ) and eCAVIAR ( Hormozdiari  et al. , 2016 ), Bayesian methods to assess the evidence of colocalization, i.e. whether GWAS signals of two traits are driven the same underlying causal variants. Both methods explicitly model  p = ( p 00 , p 10 , p 10 , p 11 )  ( Giambartolomei  et al. , 2014 ;  Hormozdiari  et al. , 2016 ). However, COLOC makes the simplifying assumption that there is at most one-causal variant at a region ( Giambartolomei  et al. , 2014 ), allowing it to not explicitly model LD. And although eCAVIAR allows for multiple causal variants and explicitly models LD, it restricts the maximum number of causal variants at six per region for computational efficiency ( Hormozdiari  et al. , 2016 ). In comparison with these methods, UNITY allows for any number of causal variants while making the assumption that there is no LD between the SNPs. We outline a summary of the relationship between UNITY and all methods described in  Table 1 .
 Table 1. Displayed is a summary of current methods that perform joint trait analysis and the relationship to the parameters in UNITY Method h 2 ρ p Misc. UNITY * * * Cross-trait LD Score regression ( Bulik-Sullivan  et al. , 2015 ) * * p 11 ≈ 1 PleioPred ( Hu  et al. , 2017 a) * * * infers  p  to estimate effect sizes COLOC ( Giambartolomei  et al. , 2014 ) – – * max 1 causal eCAVIAR ( Hormozdiari  et al. , 2016 ) – – * max 6 causals Boxes with an (*) denote the values that a method models. Note that this summary is not exhaustive. To empirically demonstrate the benefit of the relaxed assumptions of UNITY as compared to current methods, we conduct a modest comparison against COLOC ( Giambartolomei  et al. , 2014 ). We simulated 100 regions of 500 SNPs with multiple causal variants. We perform colocalization analysis over all of the regions using COLOC. When there are causal variants independently associated with each trait and shared variants, COLOC estimates that the association within the region is driven only by two independent variants, where one is specific to trait 1 and the other is specific to trait 2. Because COLOC assumes at most one-causal variant per region, the method is unable to distinguish between a variant that independently drives only one trait versus a variant that is colocalized when both cases are present. For completeness, we also included a simulation that follows the assumption underlying COLOC of the one-causal setting. The full table listing these results in outlined in  Supplementary Table S2 . However, we are unable to directly compare estimates with COLOC because there is not a clear mapping between the estimates of COLOC and the estimated parameters of UNITY, thus any direct comparison would be an unfair comparison due to the mismatch in the models. 3.2 Simulations We generated summary statistics for 500 SNPs from two synthetic GWAS. The causal effect sizes for each SNP,  γ p , m , were drawn jointly from a multivariate normal distribution where  h 1 2 , h 2 2 , ρ  denote the heritability of each trait and the genetic correlation. We denote the number of SNPs as  M  and the proportion of causal variants for each trait as  p 10 ,  p 01  and the proportion of shared casuals as  p 11 :
 ( γ 1 , m γ 2 , m ) ∼ ( h 1 2 M ( p 11 + p 10 ) h 1 h 2 ρ M ( p 11 ) h 1 h 2 ρ M ( p 11 ) h 2 2 M ( p 11 + p 01 ) ) To simulate causal SNPs, we drew an  M × 4  matrix from a multinomial distribution parametrized by  p  where the  m th row of values denotes whether a SNP is causal for neither trait, only trait 1, only trait 2, or neither trait. Using this, we constructed two  M × 1  causal indicator vectors,  C 1 ,  C 2 , where  C 1 , m , C 2 , m = 1  if the  m th SNP was causal for both traits,  C 1 , m = 1 , C 2 , m = 0  if the SNP was only causal for trait 1,  C 1 , m = 0 , C 2 , m = 1  if it was only causal for trait 2, and  C 1 , m , C 2 , m = 0  if the SNP was non-causal. To get the true effect sizes, we multiplied element-wise  β 1 = C 1  ° γ 1  and  β 2 = C 2  ° γ 2  where we are essentially zeroing out any entry from the causal effect vector where a SNP is non-causal. To compute the estimated GWAS effect sizes,  β ^ p , we assumed  cov ( ϵ 1 , ϵ 2 ) = 0 , so random noise terms  ϵ 1 , ϵ 2  were drawn from two normal distributions  N ( 0 , 1 − h 1 2 N 1 )  and  N ( 0 , 1 − h 2 2 N 2 )  respectively. We assume that the SNPs being used at the genome-wide level will be LD-pruned such that there is very little or no correlation structure. Thus, we set the LD matrix  V  =  I M , where  I M  is an  M × M  identify matrix. We then draw the estimated effect sizes from a conditional distribution of the GWAS summary statistics, as described in Section 2. First, we confirm that our method accurately predicts the proportion of causal variants under varying sample sizes and heritability estimates. We tested a variety of simulation frameworks where we fixed the genetic correlation and heritabilities of the two traits. We ran each simulation for 500 iterations and used the first quarter of the iterations as burn-in. We vary the proportion of causal variants contributing to only trait 1 ( p 10 ), proportion of causal variants for only trait 2 ( p 01 ), and the proportion of casual variants contributing to both traits ( p 11 ). As shown in  Figure 1 , we can see that UNITY performs robustly across each scenario.
 Fig. 1. We estimate the proportion of causal variants under four simulation frameworks where we vary the sample size ( N 1 ,  N 2 ), heritability ( h 1 2 = h 2 2 ), and proportion of causal variants. First, we first simulated values where the total proportion of causal variants is low:  p 00 = 0.89 ,   p 10 = 0.05 , p 01 = 0.05 , p 11 = 0.01 , along with a low sample size and high heritability:  h 1 2 = 0.05 , h 2 2 = 0.05 , ρ = 0 , N 1 = 1000 , N 2 = 1000 , as shown in ( a ). Second, we tested the model with the same proportion of causal variants, but with a larger sample size and smaller heritability:  h 1 2 = 0.001 , h 2 2 = 0.001 , ρ = 0 , N 1 = 100   000 , N 2 = 100   000 , shown in ( b ). Third, we simulated data with a higher proportion of causal variants,  p 00 = 0.50 ,   p 10 = 0.20 ,   p 01 = 0.20 ,   p 11 = 0.10 . Using the same sets of heritabilities and sample sizes from the first two simulations, we tested the prediction accuracy of our model. ( c ) denotes the simulation with low sample size and high heritability, and ( d ) denotes the simulation with high sample size and low heritability. The dotted red lines denote the true proportion of causal SNPs in each simulation Next, to assess how UNITY performs with varying levels of heritability, we continued to fix ρ = 0, but varied the values of the heritability. Note that we used low heritability values due to the low number of simulated SNPs ( M  = 500). From  Figure 2 , we can see that the estimates reflect the prior distribution of  ( p 00 , p 10 , p 01 , p 11 )  when the heritability is very low. We also show in  Figure 3  that our estimates are invariant to the correlation between phenotypes.
 Fig. 2. We simulate the following proportion of causal variants  p 00 = 0.97 ,   p 10 = 0.01 ,   p 01 = 0.01 ,   p 11 = 0.01  and vary the heritability ( h 1 2 = h 2 2 ) while fixing  ρ , N 1 , N 2 , M . We vary the heritability from.01 to 5e−7 and plot the estimated proportion of non-causal variants ( a ), proportion of causal variants for trait 1 ( b ), proportion of causal variants for trait 2 ( c ) and proportion of shared causal variants ( d ). We note that as the heritability goes down, the data become less informative and the estimates reflect the prior Fig. 3. We simulate the following proportion of causal variants  p 00 = 0.97 ,   p 10 = 0.01 ,   p 01 = 0.01 ,   p 11 = 0.01  and vary the genetic correlation from 0 to 0.50 while fixing  h 1 2 , h 2 2 , N 1 , N 2 , M . We only show the estimate of  p 11 , since this would be the only estimate directly affected by the presence of genetic correlation To assess the role of sample size in our inference, we performed simulations where we varied the number of individuals from 1000 to 250 000. We find that the recommended sample size should be at least 50 000 individuals to yield precise results ( Supplementary Fig. S2 ). Additionally, to further assess the performance of the method, we also performed simulations where  h 1 2 ≠ h 2 2  and when  p 10 ≠ p 01 . Through simulation, we demonstrate that our method is robust to these scenarios, with detailed results provided in  Supplementary Figures S3 and S4 . Finally, through simulations, we empirically demonstrate that our method is well calibrated under the null hypothesis, defined as: (i)  p 10 = 0 , (ii)  p 01 = 0  and (iii)  p 11 = 0 . To demonstrate this, we simulated 100 000 SNPs with 100 000 individuals where  h 1 2 = 0.25 , h 2 2 = 0.25 , ρ = 0 . For each hypothesis, we set the parameter of interest exactly to 0 and then then simulated 2% causal variants between the remaining parameters. For example, for null hypothesis (1), the corresponding set of simulation parameters would be:  p 10 = 0 , p 01 = 0.01 , p 11 = 0.01 . Using UNITY, we estimated the null parameter and report the posterior mean and standard deviation in  Table 2 . Note that UNITY estimates the null parameter very close to zero, but not exactly zero. This is because there is a non-zero prior on the set of parameters, making it not possible to be exactly zero, but can instead be asymptotically close.
 Table 2. We present the posterior means and standard deviations estimated when the proportion of causal variants is set exactly to zero for trait 1 and trait 2, and when the shared proportion is exactly zero Hypothesis Null parameter Mean SD 1 p 10 0.0006 0.0023 2 p 01 0.0004 0.0005 3 p 11 0.0002 0.0003 3.2.1 LD-pruning to identify approximately independent SNPs To rigorously assess the role of LD in our model, we demonstrate a sufficient LD-pruning scheme through simulations. To model a realistic LD structure, we used SNPs from 1000 Genomes ( Consortium  et al. , 2012a ) to compute the LD for each of the approximately independent LD blocks identified in  Berisa and Pickrell (2016) . We filtered rare SNPs by minor allele frequency, MAF ≤ 0.05, and used 1 million SNPs sampled across the LD blocks. We chose only a subset of 1 million SNPs because this closely reflects the number of SNPs genotyped on SNP arrays. We simulated the GWAS effect sizes as outlined in Section 3.1, where the heritabilities for each of the each traits was set to  h 1 2 = 0.50  and  h 2 2 = 0.50  (which is similar to the estimated SNP heritability for height), and genetic correlation ρ = 0. To assess the role of LD-pruning, we divided the genome into K kilobase non-overlapping windows and selected a SNP from each window. We varied K to assess the minimal window size necessary to create a subset of approximately independent SNPs. In addition, we used cross-trait LD Score regression to estimate the heritabilities for both traits and the genetic correlation after pruning, which were subsequently used in the inference. Through simulations, we determined that a 5 KB window provides precise estimates ( Supplementary Table S1 ). 3.3 Empirical analysis of BMI and height We downloaded GWAS summary data for both height and BMI from the GIANT consortium ( Allen  et al. , 2010 ;  Speliotes  et al. , 2010 ) where each study has &gt;170 000 individuals. First, we overlapped each GWAS by rsid to get SNPs present in both studies. Then for each trait, we filtered out SNPs with a minor allele frequency  ≤ 0.05 . Additionally, we performed LD-pruning by taking a SNP from every 5 KB window. We used cross-trait LD Score to estimate the heritability and genetic correlation parameters:  h H 2 = 0.2390 , h B 2 = 0.1566 , ρ = − 0.0845 . Denoting height as the first trait and BMI as the second, we estimated the proportion of causal variants for each trait as,  p 00 = 0.9519 , p 10 = 0.0062 , p 01 = 0.01579 , p 11 = 0.0262 . We summarize the distribution of estimated causal SNPs in  Figure 4 .
 Fig. 4. We show the distribution of estimated non-causal and causal SNPs from the height and BMI analysis Our results are consistent with the known genetic makeup of BMI and height. Since BMI is a function of an individual’s height and weight, we expect all of the contributing variants for height to also contribute to BMI. UNITY predicts more BMI-only specific variants than height-only variants. We hypothesize that the BMI specific variants are those that contribute to weight, whereas the variants that contribute to height in the BMI dataset were already captured in the  p 11  estimate. In principal, we would expect  p 10  to be zero since SNPs contributing to height also contribute to BMI. We expect this could be due to the non-zero prior on  p 10 . Because of this, the estimate can never truly be zero but can be asymptotically close. 4 Discussion In this work, we introduce a statistical framework for quantifying the relationship between two complex traits. The key advantage of our method is that it makes very few assumptions about the data and few restrictions during inference. Rather than relying on assumptions about a trait’s genetic architecture, we let the data describe the underlying genetics. By using a Metropolis–Hastings sampling framework, we can calculate a variety of likelihoods without relying on any conjugate prior pairings. For example, although we choose to model the causal effect sizes through a multivariate normal, one could choose another distribution, and the sampling procedure would still hold even if the new distribution did not have a conjugate prior. Finally, by operating exclusively on GWAS summary statistic data, we aim to encourage future large-scale meta analyses, since obtaining individual level data are not always readily available. We conclude with several limitations and potential future directions of our framework. First, as the size of genetic datasets grow, subsampling methods such as MCMC may prove computationally intractable. Alternatives include using adaptive MCMC to accelerate mixing and convergence or variational methods that do not require subsampling. Additionally, we have yet to rigorously quantify the effects of LD in our model in practice for local inference. We leave rigorous comparison between UNITY and other relevant methods as future work. Additionally, recent integrative methods have shown that the incorporation of a variants functional genomic context can improve both power and accuracy in identifying potential causal variants ( Hu  et al. , 2017 b;  Kichaev  et al. , 2014 ;  Li and Kellis, 2016 ;  Pickrell, 2014 ). Large-scale initiatives such as the ENCODE ( Consortium  et al. , 2012b ) and ROADMAP ( Kundaje  et al ., 2015 ) projects have provided comprehensive databases of tissue-specific functional genomic annotations. Combining this rich atlas of functional data and the genetic information from GWAS will likely uncover novel insights into disease biology. We leave the incorporation of functional elements as a potential direction for future work. Supplementary Material Supplementary Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Improving disease gene prioritization using the semantic similarity of Gene Ontology terms</Title>
    <Doi>10.1093/bioinformatics/btq384</Doi>
    <Authors>Schlicker Andreas, Lengauer Thomas, Albrecht Mario</Authors>
    <Abstract>Motivation: Many hereditary human diseases are polygenic, resulting from sequence alterations in multiple genes. Genomic linkage and association studies are commonly performed for identifying disease-related genes. Such studies often yield lists of up to several hundred candidate genes, which have to be prioritized and validated further. Recent studies discovered that genes involved in phenotypically similar diseases are often functionally related on the molecular level.</Abstract>
    <Body>1 INTRODUCTION More than 1800 human hereditary disorders are known to be caused by mutations in a single gene (O'Connor and Crystal,  2006 ). However, most of these diseases are very rare. In contrast, many diseases of major importance to public health, like cancer, diabetes and cardiovascular disorders, are influenced by simultaneous alterations in several genes (Gibson,  2009 ). In order to identify genes involved in such multi-factorial diseases, genomic linkage and association studies are performed (Altshuler  et al. ,  2008 ; Cordell and Clayton,  2005 ; Teare and Barrett,  2005 ). The genomic regions resulting from these studies may comprise as many as several hundreds of candidate disease genes, most of them unrelated to the disease of interest. Experimental testing of the complete list of candidate genes is generally impractical because of the time and cost involved in such an extensive procedure. Therefore, several studies examined the specific properties of genes and their products known to be associated with human genetic disorders and explored networks linking diseases based on the involved genes (Feldman  et al. ,  2008 ; Goh  et al. ,  2007 ; Jimenez-Sanchez  et al. ,  2001 ; Lee  et al. ,  2008 ; van Driel  et al. ,  2006 ). In particular, the discovered relationships between properties of genes and gene products as well as their involvement in genetic disorders are exploited by a number of bioinformatics approaches for ranking and prioritizing disease gene candidates (Ala  et al. ,  2008 ; Ideker and Sharan,  2008 ; Kann,  2007 ,  2010 ; Navlakha and Kingsford,  2010 ; Oti and Brunner,  2007 ; Tranchevent  et al. ,  2010 ; Turner  et al. ,  2003 ; van Driel and Brunner,  2006 ; van Driel  et al. ,  2006 ; Yu  et al. ,  2008 ). Most computational approaches rely on the integration of several sources of heterogeneous data such as sequence features, gene expression data and protein–protein interactions (PPIs). For example, PROSPECTR is a sequence-based approach that uses decision trees trained on features such as the length of gene and protein sequences and the number of exons (Adie  et al. ,  2005 ). The subsequent method SUSPECTS by the same authors combines sequence features with gene expression, protein domains and Gene Ontology (GO) term similarity of candidates and known disease proteins (Adie  et al. ,  2006 ). Endeavour is another method that relies on the integration of biological evidence resulting from many different kinds of data, for instance, PPIs, pathways, gene expression and sequence similarity (Aerts  et al. ,  2006 ). The characteristics of known disease genes were extracted from each data source separately to rank candidate genes; the resultant ranking lists were then combined to a final overall ranking. Recently, several methods have been published (Chen  et al. ,  2009 ; Franke  et al. ,  2006 ; Ortutay and Vihinen,  2009 ; Ozgür  et al. ,  2008 ; Shriner  et al. ,  2008 ) that build on both interaction networks and GO annotations (Ashburner  et al. ,  2000 ). In particular, Chen  et al.  ( 2009 ) applied different algorithms originating from the analysis of social and web networks to disease gene prioritization. They concluded that methods using functional annotation are generally better than network-based methods, but that network data provide some valuable information. Ortutay and Vihinen ( 2009 ) integrated GO annotation and protein interactions for finding genes involved in immunodeficiencies. To this end, three different network topology parameters were computed pertaining to an interaction network of genes known to be related to the immune system. For each of these parameters, a set of genes was selected from the gene network and then subjected to GO enrichment analysis. Genes received higher priority if they were annotated with enriched terms and achieved some significant network parameter value. A number of methods for disease gene prioritization uses similarity measures for phenotypes, which leverage cross-references to structured vocabularies (Chen  et al. ,  2007 ; Freudenberg and Propping,  2002 ; Lage  et al. ,  2007 ; Oti and Brunner,  2007 ; Perez-Iratxeta  et al. ,  2002 ,  2007 ; Robinson  et al. ,  2008 ; Tiffin  et al. ,  2005 ; van Driel and Brunner,  2006 ; van Driel  et al. ,  2006 ; Wu  et al. ,  2008 ; Yilmaz  et al. ,  2009 ; Yu  et al. ,  2008 ). Different controlled vocabularies such as MeSH (Lowe and Barnett,  1994 ) and eVOC (Kelso  et al. ,  2003 ) have already been utilized, and the ACGR method by Yilmaz  et al.  ( 2009 ) is specifically based on manual annotation of diseases with GO terms. Candidate genes are selected based on the number of annotated GO terms shared with the disease in question. Subsequently, each candidate is assigned a similarity value based on the annotation similarity to the input disease. The authors did not perform a large-scale validation of their approach, but limited themselves to using three rare syndromes (AICARDI syndrome, CHARGE syndrome and focal dermal hypoplasia) as case studies. The required manual annotation of the disease with GO terms is a major hurdle for the large-scale application of the ACGR method. In the following, we present MedSim, a novel approach to disease gene prioritization that exploits the similarity between the functional annotations of diseases and candidate genes ( Fig. 1 ). This methodological advance is in contrast to other methods that consider only identical annotations or are based on GO enrichment computations. In particular, we automatically derive functional profiles consisting of GO terms for a certain disease phenotype based on the genes and proteins that are already known to be related to the phenotype.
 Fig. 1. Flow chart of the MedSim approach. First, the functional profiles of the disease of interest and the disease gene candidates are created using one of the annotation strategies. Afterwards, the functional profile of the disease is scored against each functional profile of a candidate, and the candidates are ranked according to this functional similarity score. Since the annotation of the human genome with GO terms is rather incomplete, we introduce and test several new strategies for automatically extending the available annotations of disease and candidate genes or proteins. The resulting functional profiles are compared with each other using GO and our sophisticated functional similarity measures (Schlicker  et al. ,  2007 ). Using different sets of proteins encoded by known disease genes, we demonstrate that our novel method allows for assigning known disease genes specifically to the correct phenotype. Most importantly, we show that MedSim is able to significantly outperform previous more complex methods that rely on more diverse and voluminous, and thus harder accessible data and we further explore the effect of different semantic similarity measures on prediction performance. MedSim also affords the distinction of disease phenotypes with a common functional basis from unrelated phenotypes. Finally, we implemented the best MedSim method in our FunSimMat web server ( http://www.funsimmat.de ), making it easily usable by biological and medical users (Schlicker and Albrecht,  2010 ). 2 METHODS 2.1 Data sources OMIM is a database of human genes and genetic disorders. OMIM entries describe either a single gene that is involved in some genetic disease or a phenotype with known or putative, but unknown, genetic basis. We extracted all phenotypes (entries starting with ‘#’ or ‘%’) from OMIM (downloaded on October 10, 2007). The mapping of proteins encoded by human disease genes to the OMIM phenotypes was obtained from UniProtKB release 12.3 (UniProt Consortium,  2009 ). Additionally, protein annotations with GO terms from all three ontologies, that is, biological process (BP), molecular function (MF) and cellular component (CC), were extracted from this UniProtKB release. We included annotations based on all GO evidence codes in our analysis; ∼62% of the human GO annotations in our dataset were derived by automatic methods (IEA). Our set of human PPIs was compiled from the Human Protein Reference Database (HPRD, version 7) (Prasad  et al. ,  2009 ), IntAct (downloaded on May 16, 2008) (Kerrien  et al. ,  2007 ), the Molecular Interactions Database (MINT, downloaded on April 7, 2008) (Chatr-Aryamontri  et al. ,  2007 ), the Database of Interacting Proteins (DIP, downloaded on February 14, 2008) (Salwinski  et al. ,  2004 ), protein complexes extracted from SIFTS (downloaded on March 4, 2008) (Velankar  et al. ,  2005 ) and the CORUM database (downloaded on May 19, 2008) (Ruepp  et al. ,  2008 ). All protein and gene identifiers used by these sources were mapped to UniProtKB accession numbers. Since members of the same protein complex possibly affect the same diseases, the matrix model (all possible pairs of interacting proteins in the complex) was chosen for decomposing protein complexes into pair-wise PPIs. A set of random PPIs was created by keeping one partner of an interaction fixed and randomly assigning a new partner from the interacting proteins. Mouse orthologs for human proteins were obtained from Inparanoid version 6.1 (Berglund  et al. ,  2008 ). Mouse and human proteins with an inparalog score of 1.0 were extracted as ortholog pairs from each Inparanoid cluster. An inparalog score of 1.0 indicates that the two proteins form the reciprocally best matching pair of orthologs. MGI (Blake  et al. ,  2009 ) and Ensembl (Hubbard  et al. ,  2009 ) accessions used by Inparanoid were converted to UniProtKB accessions using data from Ensembl BioMart (downloaded on May 14, 2008). Additionally, the chromosomal location of human genes and the cross-references to UniProtKB proteins were obtained via BioMart on October 21, 2008. 2.2 Functional profiles Human diseases are usually described using natural language and are annotated with genes or proteins known to be involved in the respective diseases. However, they are not directly annotated with structured vocabularies like GO. GO consists of the three ontologies BP, MF and CC, which are organized as directed acyclic graphs (Ashburner  et al. ,  2000 ). Biological concepts are represented as nodes in these graphs and relationships between concepts as edges. If a gene product is annotated with a GO term, the so-called ‘true path rule’ states that all of its parents are also valid annotations. For functional comparisons, we developed several new strategies for automatically annotating OMIM disease entries with GO terms ( Table 1 ). In the remainder of this article, we refer to the GO annotation of a disease phenotype or a candidate gene product as its functional profile. The first annotation strategy (AS-base) transfers all GO terms annotated to proteins encoded by known disease genes in UniProtKB to the corresponding OMIM entry. Genes and proteins are often annotated with terms from different levels of the GO hierarchy, which can lead to functional profiles that contain ancestral terms. Since annotation with a term implies annotation with all its predecessors, ancestral terms are redundant. Therefore, a term is removed if one of its descendants from the GO hierarchy is also contained in the functional profile.
 Table 1. Summary of the different annotation strategies used to create functional profiles of diseases Annotation strategy GO annotation source AS-base Known disease genes/proteins AS-ortho Known disease genes/proteins Orthologs of known disease genes/proteins AS-inter Known disease genes/proteins Interaction partners of known disease genes/proteins AS-sem Known disease genes/proteins Semantically similar terms The table lists sources of GO annotation used by the different annotation strategies. Term filtering can be applied to functional profiles created by any of these annotation strategies. 
 In case of AS-base, OMIM entries cannot be annotated if the known disease genes and proteins lack any GO annotation. Furthermore, the annotated disease genes and proteins may not cover all functions and processes involved in the respective disease. Therefore, we explored several possibilities to automatically extend the available annotation. The second annotation strategy (AS-ortho) adds GO terms from mouse orthologs of human disease proteins to the functional profile, and the third annotation strategy (AS-inter) augments the profile with GO terms from direct interaction partners of disease proteins ( Table 1 ). Both strategies involve the removal of redundant GO terms after adding the new terms to the profile. A fourth strategy for expanding the functional profiles (AS-sem) is based solely on GO. The simRel measure (see  Section 2.6  below) is used to identify terms that are highly related to at least one other term in the same profile. Two different simRel cut-offs, 0.90 and 0.95, are applied for selecting and adding related terms to a profile. Functional profiles of candidate disease genes and proteins are always generated by applying the same annotation strategy as used for the disease phenotype. If a protein has many interaction partners with diverse functions or the dataset contains false positive interactions, the described automatic strategies might lead to a diffuse functional profile containing diverging GO annotations for BPs, MFs and CCs. Therefore, we implemented a term filtering step for removing unrelated terms from the functional profiles. In this step, terms are retained only if they have a simRel score above a predefined threshold with at least one other term in the profile. For example, if we consider a functional profile consisting of four GO terms and two of these terms are similar to each other and the other two terms are not related to any term in the profile, the latter two are removed from the profile. In contrast, if the latter two terms are similar to each other as well, all four terms are retained in the profile. We tested the two simRel thresholds 0.60 and 0.80. The term filtering step was applied to all functional profiles consisting of at least three GO terms. If the functional profile of a disease contained no GO term pair with simRel exceeding the threshold, the respective disease was not included into the benchmark. 2.3 Benchmark set 1 Several prioritization methods assess the probability of a gene or protein to be generally associated with some disease, but are unspecific for the disease. In order to test whether MedSim allows for specifically assigning known disease gene products to the correct disease phenotype, we conducted leave-one-out cross-validation on a set of diseases and known disease-associated proteins. For this benchmark, we selected a preliminary set of 99 OMIM disease phenotypes, each of which is associated with at least three known disease proteins ( Supplementary Table S5 ). For each of these phenotypes, one disease protein was randomly selected and removed. Subsequently, the functional profiles of the 99 phenotypes were derived using annotation strategies AS-base, AS-ortho or AS-inter based on the remaining known disease proteins. Disease phenotypes were discarded if either the phenotype or the randomly selected protein was not annotated with terms from all three GO ontologies. This led to benchmark set 1 consisting of 78 phenotypes with 78 randomly selected known disease proteins. Five of these proteins are known to contribute to two diseases in the test set and were coincidentally chosen for both phenotypes.  Supplementary Table S1  summarizes the number of GO terms annotated to phenotypes and randomly selected proteins in benchmark set 1. 2.4 Benchmark set 2 Genomic loci found to be associated with a disease may contain up to several hundred candidate genes. The second benchmark simulates such a genomic experiment, which results in a quantitative trait locus (QTL) and the corresponding list of candidate disease genes. For each of the 519 disease gene-encoded proteins associated with one of the 99 phenotypes in benchmark set 1, leave-one-out cross validation was performed for classifying the protein according to its disease relatedness. After a protein  p  was removed from the list of known proteins for some disease, the functional profile of this disease was derived using the remaining associated proteins. An artificial QTL (aQTL) of size 10 Mbp was centered at the genomic start position of the gene encoding  p , and all proteins translated from any gene in this aQTL were added to the list of putative disease proteins. Benchmark set 2 contains 519 different aQTLs for 99 phenotypes. All four annotation strategies were applied to annotate benchmark set 2. Additionally, term filtering with both thresholds 0.60 and 0.80 was applied together with AS-base and AS-inter, as well as term filtering using threshold 0.80 with AS-sem. As control, random PPIs were used for AS-inter ( Section 2.1 ). 2.5 Benchmark set 3 Several approaches, for example, Endeavour (Aerts  et al. ,  2006 ), had been benchmarked using random artificial QTL (rQTLs) that contain one known disease gene and 99 random genes. To facilitate a performance comparison between MedSim and these methods, we created a third benchmark set. This set was compiled using the same set of phenotypes as benchmark set 2 but differs in the methodological details of creating the rQTLs. Here, each disease protein annotated with terms from all three ontologies was complemented with 99 proteins randomly drawn from the set of all human proteins annotated with terms from all three ontologies. Benchmark set 3 consists of 287 distinct rQTLs for 99 different phenotypes. To the phenotypes and rQTLs in this benchmark set, we applied AS-base without and with term filtering (threshold 0.80) as well as AS-sem (cut-off 0.95) with term filtering (threshold 0.80). 2.6 Functional similarity measures The similarity between functional profiles of diseases and candidate proteins was computed using the Functional Similarity Search Tool (FSST version 1.3.1) (Schlicker  et al. ,  2007 ). The computed functional similarity scores apply the best-match average approach, which determines whether a function contained in one profile is also contained in the second profile. The functional similarity scores are based on a semantic similarity measure for comparing two GO terms. The simRel score (Schlicker  et al. ,  2007 ), which assesses the differences and commonalities between GO terms, was used to determine the semantic similarity of GO terms. This score is affected by the level of detail of the annotated terms. In order to find out whether the performance of MedSim depends on the choice of the semantic similarity measure, Lin's ( 1998 ) measure was used as well. This similarity score measures the commonalities and differences between two GO terms, but is not affected by the degree of specificity of some term as given by the GO hierarchy. To compare two functional profiles, several similarity scores are evaluated: BPscore for BP, CCscore for CC, MFscore for MF, rfunSim combining BPscore and MFscore and rfunSimAll combining BPscore, CCscore and MFscore. A detailed description of all semantic and functional similarity scores can be found in the  Supplementary Data . 2.7 MedSim implementation We implemented the MedSim approach in our FunSimMat database and web service ( http://www.funsimmat.de ). FunSimMat contains precomputed functional similarity values for proteins and protein families, accessible through a web front-end as well as XML-RPC and RESTlike interfaces. The functional profiles for all OMIM entries and human proteins in UniProtKB were derived using strategy AS-base without and with term filtering (threshold 0.80), and all functional scores are pre-calculated. The FunSimMat web page offers a simple HTML form for prioritizing a list of candidates, which requires the user's input of the OMIM accession of a specific disease and the UniProtKB accessions of the corresponding candidate disease proteins. The results table contains the candidates ranked by the functional similarity score. An alternative for providing a candidate list is the possibility of scoring all human proteins against the disease of interest. Additionally, programmatic access to the data is possible through the XML-RPC and RESTlike interfaces. 3 RESULTS 3.1 Performance and coverage using different annotation strategies To measure the ability of MedSim to detect the correct protein for each disease, we applied receiver operating characteristic (ROC) analysis and determined the area under the ROC curve (AUC). Additionally, we calculated the sensitivity and specificity of the predictions. Sensitivity is the percentage of correctly identified disease proteins ranked above a preset rank or score cut-off. Specificity is the percentage of proteins not involved in the disease ranked below this cut-off. When stating sensitivity values, we will always refer to a specificity threshold of 90%. The performance values presented in the remainder of the text constitute conservative estimates due to the following two reasons. First, the ranking list of proteins may contain several proteins associated with a disorder, but solely the randomly left-out protein is considered a true positive. Second, proteins labeled as true negative might, in fact, be as yet unknown true positives. A detailed discussion of the results for benchmark set 1 can be found in the  Supplementary Data . Briefly, MedSim achieved an AUC of up to 0.81 on this set using strategy AS-ortho. This shows that MedSim effectively assigns top ranks to the correct protein in a list of known disease proteins. Benchmark set 2 was designed for simulating the most common application scenario for disease gene prioritization methods. The task is to rank a list of candidate disease genes or proteins such that the most likely candidates are on top of the list ( Fig. 1 ). Benchmark set 2 contains 519 aQTLs of size 10 Mbp, which encompass 312 proteins on average, including one known disease protein. FSST was used to calculate functional similarity between diseases and proteins in the corresponding aQTLs.  Supplementary Table S2  displays the number of annotated diseases and proteins in the aQTLs, and  Supplementary Table S3  contains the mean and median number of annotated terms for benchmark set 2. The results for benchmark set 2 using the different annotation strategies are listed in  Supplementary Table S7 . Regarding strategy AS-base, the best prediction AUC of 0.81 is achieved using the BPscore and the rfunSim score with a sensitivity of 0.51 and 0.50, respectively ( Fig. 2  and  Supplementary Fig. S4 ). Adding ortholog annotation leads to virtually identical AUC ( Supplementary Fig. S7 ) and sensitivity values. However, prediction performance using MFscore drops slightly, which also affects the results obtained with the rfunSim score. AS-inter performs worse, the best AUC being 0.71 for the rfunSimAll score ( Supplementary Fig. S8 ). Sensitivity, however, is only slightly decreased by adding protein interaction data. We also applied AS-sem to benchmark set 2 using two different simRel cut-offs, 0.90 or 0.95, for adding terms. In both cases, the AUC and sensitivity values are similar to the scores obtained with AS-base ( Supplementary Figs S15 and S17 ).
 Fig. 2. AUC values of MedSim on benchmark sets 2 and 3 using AS-base with term filtering (0.80) and without. When inspecting the availability of GO annotation ( Supplementary Table S3 ), it becomes evident that AS-ortho improves the coverage with functional annotation while preserving the performance. AS-inter increases coverage even more, but it negatively affects the prediction performance slightly. We carefully checked that this performance decrease is not due to an implementation error, and the application of AS-inter to a set of random PPIs yielded AUC values as expected for random prioritization (see  Supplementary Data  for details). By increasing the coverage, AS-ortho and AS-inter potentially allow for ranking candidate disease genes and proteins that are not amenable to analysis using AS-base due to the lack of direct GO annotation. Thus, we studied the results with the rfunSim and rfunSimAll scores for aQTLs to which we could not apply the strategy AS-base. For these cases, the sensitivity of MedSim using AS-ortho and AS-inter is 46% and 25%, respectively, with rfunSimAll. This indicates that both annotation strategies help ranking candidates if known human disease genes and proteins are not yet annotated with GO terms. 3.2 Improving prediction performance by filtering dissimilar terms The findings above indicate that prediction performance is negatively influenced by semantically unrelated terms. Thus, we applied a semantic similarity term filter to the functional profiles of benchmark set 2 created by AS-base and AS-inter. The term filter removes all terms that do not have a simRel score greater than a specific threshold (here, 0.60 or 0.80) to any other term in the profile. With respect to AUC, the results are inconclusive for AS-base ( Fig. 2  and  Supplementary Figs S5 and S6 ). The AUC drops slightly for BP and MF using both thresholds, but the AUC of CC and of the combined scores are larger than without term filtering. The best AUC is achieved with AS-base using the rfunSim score (AUC 0.85) and term filtering with the threshold 0.80. If the functional profiles are complemented by PPIs in AS-inter, term filtering improves the AUC in most cases ( Supplementary Figs S13 and S14 ). The rfunSimAll score has an AUC of 0.82 using AS-inter and term filtering (threshold 0.80), which is even better than the best performance of AS-base without term filtering. The sensitivity values show the same trend, and the maximum is reached at 65% using AS-base with term filtering (threshold 0.80). The annotation coverage of proteins in aQTLs is lower after applying the term filtering procedure, but when using AS-inter, it is about as high as with AS-base without term filtering. In case of the combined scores, rfunSim and rfunSimAll, however, the coverage is significantly lower using term filtering. Applying term filtering (threshold 0.80) before adding terms based on high-semantic similarity does not improve the results compared to term filtering alone. We have already shown above that adding terms from protein interaction partners helps ranking candidates in aQTLs that are not amenable to analysis with AS-base. When considering only cases in which the disease or the left-out protein could not be annotated using AS-base with term filtering (threshold 0.80), AS-inter with term filtering achieves a sensitivity of 31% with rfunSim and 36% with rfunSimAll. This further confirms that data from PPIs aids in identifying disease-related proteins if known human disease proteins are not annotated with GO terms. 3.3 Performance on rQTLs increased over aQTLs Benchmark set 3 was created in a fashion that is similar to previous publications for facilitating a comparison of the performance of MedSim and other prioritization methods. This benchmark set consists of 287 rQTLs, each containing 100 proteins annotated with BP, MF and CC. Functional profiles for diseases in benchmark set 3 were derived using AS-base without and with term filtering (threshold 0.80), and AS-sem (cut-off 0.95) with term filtering (threshold 0.80). The ranking results for benchmark set 3 are listed in  Supplementary Table S8 . Using AS-base ( Fig. 2  and  Supplementary Fig. S19 ), the best performance is achieved with the combined scores, rfunSim (AUC 0.85) and rfunSimAll (AUC 0.84). Using each combined score improves the sensitivity (57%) over the use of any other score (42–53%). Applying term filtering, deteriorates the AUC of the BPscore and the MFscore, but increases the sensitivity of the CCscore from 42% to 57% and of the MFscore from 47% to 51% ( Supplementary Fig. S20 ). In case of the combined scores, both performance measures improve if AS-base is applied with term filtering ( Fig. 2 ). The rfunSimAll score reaches a maximal AUC of 0.90 and a sensitivity of 73%. Virtually the same AUC and sensitivity are achieved when applying term filtering to AS-sem ( Supplementary Fig. S21 ). The impact on the coverage with GO annotation caused by the removal of unrelated GO terms from functional profiles was already described for benchmark set 2. For benchmark set 3, term filtering reduces the coverage to 36–59% in the cross-validations ( Supplementary Table S4 ). To calculate the combined scores, the functional profiles have to contain either both BP and MF terms for rfunSim or terms from all three ontologies for rfunSimAll. Therefore, term filtering has a much higher impact on the combined scores, reducing the coverage to ∼10% compared to ∼95% without term filtering. 3.4 Results for exemplary diseases Several inherited diseases involve cellular processes whose functional relationship on the molecular level is not clear yet. One such example is inflammatory bowel disease (OMIM #266600) (Schreiber  et al. ,  2005 ). UniProtKB currently maps five proteins reported by genome-wide association studies to this disease (Cho,  2008 ): the nucleotide-binding oligomerization domain-containing protein 2 (NOD2, Q9HC29), the solute carrier family 22 members 4 and 5 (SLC22A4, Q9H015; SLC22A5, O76082), interleukin 10 (IL10, P22301) and the interleukin 23-receptor (IL23R, Q5VWK5). In benchmark set 2, MedSim ranks all proteins except NOD2 in the top 22% when applying strategy AS-inter and the rfunSimAll score. Notably, SLC22A5 and SLC22A4 are ranked in the top 6% and top 11%, respectively. NOD2 is ranked in the top 11% using the rfunSim score and strategy AS-base. Further exemplary prioritization results for photosensitive trichothiodystrophy (OMIM #601675), susceptibility and resistance to human immunodeficiency virus type 1 (HIV-1) (#609423), Parkinson disease (OMIM #168600), prostate cancer (OMIM #176807) and familial hypertrophic cardiomyopathy (OMIM #192600) are described in the  Supplementary Data . 3.5 Comparison with other prioritization methods First of all, it is important to note that several aspects hamper a fully objective comparison between different disease gene prioritization methods. Many methods are not readily available, making it impossible to apply them on exactly the same benchmark set. Furthermore, the biological contents of the datasets used by different methods influences the prediction results, which limits any detailed comparison. Nevertheless, it is possible and necessary to conduct a general performance comparison by utilizing large-scale benchmark sets that are created in a methodologically similar way. To this end, the procedure applied for creating benchmark set 3 is very similar to previous publications (Aerts  et al. ,  2006 ; Chen  et al. ,  2007 ). Endeavour (Aerts  et al. ,  2006 ) is a state-of-the-art method based on the integration of multiple data sources. It can be used to prioritize genes based on single data sources or a combination of different sources. The authors validated their approach with a benchmark set of rQTLs that were constructed with a strategy similar to benchmark set 3. With GO annotation as the only data source, Endeavour achieved an AUC of slightly over 0.75. MedSim, on the other hand, reached an AUC value of up to 0.90 at a sensitivity of 73 % when relying only on GO annotation. In case of prioritization using all data sources, Endeavour was reported to achieve an AUC value of 0.87 and a sensitivity of 74% (at 90% specificity), which is comparable to the performance of the less complex MedSim approach using only GO annotations as data source. Recently, Chen  et al.  ( 2007 ) devised the ToppGene method that uses annotation with terms from the Mammalian Phenotype (MP) ontology (Smith  et al. ,  2005 ) among other data sources, for instance, biomedical literature and protein interactions. For comparing their tool to Endeavour, the authors used a benchmark similar to benchmark set 3. The reported AUC values are 0.91 and 0.89 with and without using MP annotation, respectively, and a sensitivity of 74% with MP annotation. This means that MedSim performs comparatively, while using a much simpler prediction approach based on GO annotation alone. Further comparisons to other methods that are based on GO annotations and PPI data are provided in the  Supplementary Data . 4 CONCLUSIONS We presented the new approach MedSim for disease gene prioritization that introduces several novel strategies for automatically annotating diseases with GO terms from known disease genes or proteins, and from their mouse orthologs or interacting human proteins. We also explored the possibility of increasing prediction performance by augmenting the functional profiles with semantically similar terms and filtering out dissimilar terms. The results obtained with several extensive benchmark experiments show that MedSim is able to specifically associate diseases with known proteins. Furthermore, despite its simplicity, MedSim achieves high AUC (up to 0.90) and sensitivity (up to 73%) values and is able to perform at least as well as more complex state-of-the-art methods like Endeavour (Aerts  et al. ,  2006 ) and ToppGene (Chen  et al. ,  2007 ). Moreover, we find that functional similarity can be used to distinguish diseases with a common functional basis from unrelated diseases, which enables further research on clustering diseases using functional criteria. In detail, the functional similarity scores BPscore, rfunSim and rfunSimAll perform best for differing benchmark sets and annotation strategies. The transfer of GO annotations from mouse orthologs to human proteins is particularly useful for increasing the coverage with GO annotation without lowering performance. Adding annotation from protein interaction partners greatly increases coverage (up to 41%), but can have a negative impact on the overall performance. Nevertheless, our results provide evidence for the fact that the use of GO annotations from orthologous mouse proteins or protein interaction partners aids in ranking candidate genes and proteins accurately if the latter do not already possess a suitable GO annotation. In particular, term filtering increases the performance and allows for finding a tradeoff between high coverage and high performance, especially when applied to functional profiles created with the help of protein interaction data. In general, our comparison of the prediction results from different benchmarks demonstrated that the assessed performance of a method depends on the actual construction of the benchmark set. The AUC and sensitivity for benchmark set 3 are generally higher than for benchmark set 2 using the same annotation strategy for both sets. This effect was also observed in our exemplary study of susceptibility to HIV-1. The effect is most likely due to the fact that the rQTLs in benchmark set 3 are of smaller size on average and that the unrelated proteins are randomly drawn from the whole proteome. Therefore, it is important to take into account how a benchmark set was constructed when comparing the performance of different prioritization approaches. All benchmarks used for validating the MedSim approach were compiled in such a way that every candidate list contains exactly one true positive. However, in real settings, it might happen that none of the candidates is related to the disease of interest. In such situations, the whole list might be rejected if no candidate scores significantly better than the rest of the candidates. If the functional similarity scores obtained for different disease are compared, it is important to normalize the absolute values because they are not directly comparable. In addition, we presented strategies for automatically extending the existing GO annotation of human genes and proteins using orthologs from model organisms or interaction partners. Our approach is not restricted to GO as functional annotation source. Since the semantic and functional similarity measures used are applicable to any vocabulary that is organized as a tree or directed acyclic graph, MedSim could also leverage annotations from other vocabularies like the Human Phenotype Ontology (Robinson  et al. ,  2008 ). The availability of functional annotations is generally expected to improve considerably in the near future because of comprehensive annotation efforts like the Reference Genome Annotation Project (Reference Genome Group of the Gene Ontology Consortium,  2009 ). The functional profile of a phenotype might also be used to predict functions for uncharacterized genes and proteins implicated in this phenotype. In particular, AS-ortho and AS-inter are useful for transferring GO annotations from functionally annotated orthologs from model organisms or interaction partners, respectively. It should be noted that the use of OMIM has some limitations. First, OMIM was initiated as database of Mendelian disorders and contains many entries describing single genes. These cannot be used for benchmarking methods that aim at the prioritization of candidates for polygenic diseases. Second, the information in OMIM is manually curated, which increases the quality but is labor-intensive. Therefore, OMIM does not contain all currently known genes affecting diseases as it became apparent in our exemplary study of susceptibility to HIV-1. Third, OMIM does not provide a hierarchical classification of phenotypes and contains free-text descriptions. This renders it difficult to automatically derive ontologies like the Human Phenotype Ontology and to use this information without further manual curation. Finally, the most promising MedSim annotation strategy, AS-base with term filtering (threshold 0.80), is available via our FunSimMat online service (Schlicker and Albrecht,  2010 ). In particular, FunSimMat contains functional profiles for all OMIM disease entries and human proteins derived by annotation strategy AS-base with and without term filtering (threshold 0.80). The pre-computation of functional similarity scores affords the fast ranking of genes in QTLs or even of the whole genome with respect to the disease of interest. Moreover, the MedSim approach can be easily incorporated into other disease gene prioritization methods. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>LAMPA, LArge Multidomain Protein Annotator, and its application to RNA virus polyproteins</Title>
    <Doi>10.1093/bioinformatics/btaa065</Doi>
    <Authors>Gulyaeva Anastasia A, Sigorskih Andrey I, Ocheredko Elena S, Samborskiy Dmitry V, Gorbalenya Alexander E, Ponty Yann</Authors>
    <Abstract/>
    <Body>1 Introduction Due to high-throughput next-generation sequencing, genomics is outpacing functional and structural characterization of proteins ( Brister  et al. , 2015 ). This gap is especially pronounced and fast growing for viruses, whose discovery and characterization in diverse habitats has been driven by metagenomics over the last 10 years ( Suttle, 2007 ;  Zhang  et al. , 2019 ). In genomics projects, conceptually translated open reading frames (ORFs) are functionally characterized by bioinformatics tools which use homology recognition for annotation. To improve accuracy of protein annotation, bioinformatics tools use iterative searches of databases of individual sequences [e.g. PSI-BLAST ( Altschul  et al. , 1997 ) versus GenBank ( Sayers  et al. , 2019 )], search profile databases [e.g. HMMER ( Finn  et al. , 2011 ) or HHsearch ( Remmert  et al. , 2012 ;  Söding, 2005 ) versus Pfam ( El-Gebali  et al. , 2019 ) or HHblits ( Remmert  et al. , 2012 ) versus Uniclust30 ( Mirdita  et al. , 2017 )], and may involve comparison of query and target secondary structure [e.g. HHsearch versus SCOP ( Fox  et al. , 2014 )]. Annotation pipelines favor selectivity over sensitivity by imposing stringent cutoffs on similarity between query and database entries. Scores of similarity are interpreted in statistical frameworks using either expectation values (default cutoff  E  = 0.001, BLAST, HMMER, HHsearch) or homology Probability (default cutoff  P  = 95%, HHsearch). To recognize distant homologs, popular HHsearch was fine-tuned based on a subset of SCOP 1.63 database with less than 20% pairwise sequence identity of structural domains ( Söding 2005 ), where mean sequence length is equal 178 aa ( Fox  et al. , 2014 ;  Fig. 1 ), typical of functional and structural domain ( Wheelan  et al. , 2000 ). Its hit statistical significance increases with score of similarity between query and target, and it depends on sizes and diversities of query and target ( Remmert, 2011 ). Specifically, large size increases likelihood of a hit score emerging by chance, while the opposite is true for small size. Notwithstanding HHsearch training on protein domains, it has been routinely used in analysis of proteins of unknown domain organization. For a single-domain protein, statistical significance of hit similarity must be applicable to its domain, since sizes of both are similar. On the other hand, for multidomain queries, statistical support of a hit associated with individual domain may be underestimated due to inflated search space that encompasses other domains of the query protein ( Altschul  et al. , 1997 ;  Söding, 2005 ). Fig. 1. Length distribution of proteins in datasets relevant to comparison of HHsearch and LAMPA. This plot depicts sizes of six protein datasets labeled from A to F and used or cited in this study. (A) 6271 SCOP domains used for HHsearch training (range: 21–1504 aa); (B) 2985 RefSeq virus polyproteins (range: 1001–8572 aa); (C) 431 RefSeq virus polyproteins which include 507 regions exclusively annotated by LAMPA (range: 1039–8572 aa); (D) 507 hit regions generated by LAMPA from 431 RefSeq polyproteins (range: 88–2172 aa); (E) 507 domains tentatively demarcated around LAMPA hits (range: 164–732 aa); and (F) 41 designed sizes of each of three proteins, 123 in total, tested in computational experiments (range: 10–100.000 aa) The query size issue could be of little practical consequence for proteins having closely related homologs in sequence databases. However, for identification of distant relationships, accurate estimation of statistical significance could be impactful. The above problem may be particularly acute for RNA viruses ( Baltimore, 1971 ), which typically encode large multidomain proteins (&gt;1000 aa) ( Das and Arnold, 2015 ). (Hereafter and for sake of simplicity, we will use polyprotein to refer to virus multidomain proteins). They are much larger than most proteins of cellular organisms, whose length distributions resemble lognormal, with a mean below 500 aa ( Zhang, 2000 ). Human immunodeficiency virus, Ebola virus, severe acute respiratory syndrome coronavirus and poliovirus, and very many other eukaryotic viruses encode polyproteins ( Dougherty and Semler, 1993 ;  Gorbalenya and Snijder, 1996 ). These polyproteins mediate replication/transcription and promote virus particle formation in either the synthesized form or after being proteolytically processed. Furthermore, the already known proteomes of RNA viruses are exceptionally diverse due to high mutation rate of RNA viruses ( Sanjuan  et al. , 2010 ), with many relationships in twilight and midnight zones of homology ( Habermann, 2016 ;  Kuchibhatla  et al. , 2014 ). In our recent HH-suit-mediated analysis of the largest known polyprotein of RNA virus (PSCNV, 13 556 aa) ( Saberi  et al. , 2018 ), we initially annotated only three regions by homology (polyprotein 7.1%). To check whether this result could be partially attributed to an underestimation of genuine statistical significance of the similarity between polyprotein domains and target protein profiles, we split the polyprotein using comparative genomics and, indeed, identified three other homologs with high confidence ( Saberi  et al. , 2018 ). The above positive experience led us to formalize this approach in R package, called LAMPA, LArge Multidomain Protein Annotator, that we describe in this article. Also, we present proof-of-the principle for LAMPA in study of homology between RNA virus polyproteins and pfamA_31.0 database. It was further supported and expanded by evaluation of dependences of HHsearch statistics for fixed similarity score from lengths and diversities of query and target in computational experiments. 2 Materials and methods 2.1 Databases and virus protein dataset We used pfamA_31.0 database ( El-Gebali  et al. , 2019 ), accompanying HH-suite ( Remmert  et al. , 2012 ), as  target  database to identify homology by profile searches and transfer annotation. We were interested in annotating virus proteins and selected a subset of NCBI Viral Genomes Resource database (RefSeq) ( Brister  et al. , 2015 ) to serve as  queries  in homology searches and the source of expert annotation ( Supplementary Text S1 .1). Only proteins of true RNA viruses that use RNA-dependent RNA polymerase (RdRp), positive and negative single-strand ed RNA viruses, (+)ssRNA and (−)ssRNA, respectively, and double-stranded RNA viruses, dsRNA, were included in the query protein dataset ( Supplementary Fig. S1 ). Protein sequences were obtained from ‘translation’ qualifiers of ‘CDS’ features in RefSeq genome entries. The query database included all 2985 protein sequences of RNA virus genomes listed in ‘Viral genome browser’ table on 26 July 2018 ( Supplementary Table S1 ), that were 1000 aa or longer (protein length ranged from 1001 aa to 8572 aa, median = 2081 aa;  Fig. 1 ). It was further grouped into 884 clusters using MMseqs2 ( Steinegger and Söding, 2017 ), following the authors recommendations for multidomain proteins and defining sequence identity rate (–cluster-mode 1 –min-seq-id 0.3 –alignment-mode 3) and local alignment coverage (–cov-mode 0 -c 0.8) (see  Supplementary Text S1 .2 and  Table S1 ). Most of these proteins are encoded in a single ORF ( Firth and Brierley, 2012 ). We parsed RefSeq entries corresponding to the analyzed proteins to extract region annotations from ‘Region’ features ( O’Leary  et al. , 2016 ). Other annotation features, such as ‘CDS’, ‘Protein’ and ‘Site’, which were not taken into analysis, may overlap with the ‘Region’ or include extra information. For further details about polyprotein query dataset see  Supplementary Text S1 .1. 2.2 Comparative sequence analysis Transmembrane (TM) helices in protein sequences were predicted by TMHMM 2.0c ( Sonnhammer  et al. , 1998 ). Secondary structures (SS) of query sequences, regardless of their length, were derived from the predictions made for the respective entire polyproteins by script addss.pl from HH-suite 3.0.0 (15 March 2015) ( Steinegger  et al. , 2019 ), which used PSIPRED 3.5 tool ( Jones, 1999 ). Query profiles were built and compared to a database by programs HHmake and HHsearch from HH-suite 2.0.16, respectively ( Söding, 2005 ;  http://wwwuser.gwdg.de/~compbiol/data/hhsuite/releases/all/ ). In all analyses, parameters of HH-suite programs were left at default values, with the exception of HHmake parameter ‘-M first’, indicating that columns with residue in the first sequence of the FASTA file are considered match states, and HHsearch three parameters: ‘-p 0’, allowing hits with Probability as low as zero; ‘-norealign’, blocking realignment of reported hits using maximum accuracy algorithm; ‘-alt 10’, enabling reporting up to 10 significant alternative alignments between a query and a target profile ( Söding, 2005 ) ( Supplementary Text S1 .3). To identify statistically significant hits and homologous regions, HHsearch hits were subjected to post-processing under three cutoffs: Probability &gt;95%,  E -value &lt;10 and hit length of &gt;50 aa of the query sequence. Hits satisfying these thresholds and overlapping on query were combined into a cluster, extreme N- and C-terminal residues of which defined boundaries of region in the query that was homologous to target(s). Statistics of the top-scoring hit in the cluster defined the entire cluster, and name of the top-scoring target profile in the cluster annotated the query region. Unless stated otherwise, all reported analyses used the hits post-processing. Also, we used HHblits v.3 ( Remmert  et al. , 2012 ) for analysis of selected polyproteins as detailed in  Supplementary Text S1 .4. Analysis and visualization were performed using R 3.3.0 ( R Core Team, 2018 ,  https://www.R-project.org/ ). 2.3 Statistics 
 P -value of Wilcoxon signed-rank test ( P W ) was calculated using function ‘wilcox.test’ from R package ‘stats’, with arguments ‘paired’ and ‘alternative’ set to values ‘TRUE’ and ‘greater’, respectively ( R Core Team, 2018 ,  https://www.R-project.org/ ). 2.4 Calculation of HHsearch P-value and Probability dependence from lengths and diversities of query-target pair for fixed hit score HHsearch uses extreme value distribution (EVD) model for estimating hit’s  P -value,  E -value, and Probability from query-target local alignment similarity score.  P -value for a given score is defined as:
 (1) P value ( score ) = 1 −   exp   ( − exp   ( − λ   *   ( score − μ ) ) ) , where λ and μ are the EVD parameters that optimally approximate the score distribution of false positives for a given pair of query and target profiles.  E -value is defined as  P value (score)*N DB , where N DB  is the number of searched target profiles in the database. For calculations of λ and μ, HHsearch uses ‘profile auto-calibration’ that employs two simple artificial neural networks ( Remmert, 2011 ). This default procedure makes use of dependence of λ and μ on four characteristics: profile lengths and sequence diversities of both query and target. The parameters of the neural networks were derived by training on a set of profiles based on 6271 sequences of SCOP20 v1.73 database (minimal, median and maximal protein lengths = 21 aa, 142 aa and 1504 aa, respectively; 5-to-95% range = 48-to-392 aa) ( Fig. 1 ). Estimation for Probability of detecting homologous relationship (true positives) is also based on the EVD distribution but involves correction by the SS alignment score. To learn how HHsearch performs on queries of our study with sizes close to or exceeding the largest protein in the training SCOP database, we conducted computational experiments using the HHsearch procedure that generates EVD parameters by adapting corresponding C++ source code into a Python Jupyter notebook (https://github.com/Gorbalenya-Lab/hh-suite-notebooks/tree/LAMPA). We approximated  P -value and Probability of hit for fixed local alignment similarity score (including also SS alignment score for Probability) in relation to lengths and/or diversities of the corresponding query and target profiles, one of which may have been set to vary in large range of values (see  Supplementary Text S1 .5). 3 Results 3.1 LAMPA, iterative approach for homology recognition and functional annotation of multidomain proteins LAMPA approach is aimed at improving detection of remote homology in large multidomain proteins (queries). Its multistage iterative procedure includes prediction of TM regions in query by TMHMM at the pre-iteration Stage #0 and comparisons of query and its regions with HH-suite profile database(s) (targets) using HHsearch for iterations at Stages #1–#3 ( Fig. 2 ). As query, intact protein is used for Stages #0 and #1, and various protein regions are used for Stages #2 and #3. Iteration is a single execution of a procedure involving protein regions demarcation and submission of regions to HHsearch-mediated homology searches to identify statistically significant hits (values of post-processing cutoffs, specified in Section 2.2, are default). The approach stages are detailed below: Fig. 2. LAMPA workflow and its application to RNA virus polyprotein. Presented is outline of the LAMPA approach (blue background) applied to polyprotein 1a (pp1a) of BPNV. Gray bars, regions of BPNV pp1a that served as TMHMM or HHsearch queries. Iterations of the procedure and programs used are depicted on the left; stages are indicated on the right. Clusters of TM helices are depicted in dark red, clusters of hits—in dark blue. Hit double digits refer to iteration and hit position on polyprotein from left to right, respectively, except for hits at Stage #0 which are labelled with the position only. Hits and annotations obtained on Stage #1 represent output of conventional HHsearch. Q-rich, region rich in glutamine residue; ZBD, zinc-binding domain; Pkinase, protein kinase; MTase, methyltransferase; 3CLpro, 3C-like protease. For other details see text. (Color version of this figure is available at  Bioinformatics  online.) Stage #0.  Detection of TM regions in original query.  TM region (domain) may include either single or few helices predicted by TMHMM. By default, more than one helix is included in a region if each helix is separated from its neighbor by &lt;100 aa. Region boundaries are defined by either helix boundaries (single-helix region) or opposite boundaries of two respective terminal helices (multiple-helix region). TM regions are used to split original query into smaller regions (see Stage #2). Stage #1.  Detection of homology regions in original query.  This is the first iteration of the annotation procedure that uses HHsearch-mediated homology search. Its input and output are the original query and hit-annotated regions, respectively. Stage #2.  Detection of homology regions in split query: query-protein-specific (QP-specific) iterations.  To initiate this stage, the procedure selects regions of the original query that are flanked by either of the following: N- or C-terminus of the original query, TM regions and hits clusters identified at the Stages #0 and #1, respectively. These regions are used as input to HHsearch-mediated homology searches. Obtained hits are used for annotation and to demarcate flanking smaller non-annotated regions. The latter are used to initiate a new iteration in the manner described above. The iterations are repeated until no hits satisfying the cutoffs are identified. Stage #3.  Detection of homology regions in split query: average-protein-size-specific (AP-specific) iterations.  Non-annotated regions after the Stage #2 are split into two overlapping sets of 300 aa queries (default). The most C-terminal queries of both sets are extended to include the remaining part of the respective region, if the remaining part is shorter than 300/2 = 150 aa (default) and if the extended query does not cover the entire region. The default 300 aa size is close to that of an average protein (AP), hence respective iterations are called AP-specific. Queries are defined starting from either the N-terminus (first AP-specific iteration) or 300/2 = 150 aa (default) downstream the N-terminus (second AP-specific iteration) of the non-annotated regions of Stage #2. They are run independently. During this stage, one and the same region of polyprotein may be found to have homolog and be annotated on both AP-specific iterations since two sets overlap. 3.2 LAMPA implementation The above approach was realized as LAMPA 1.0.0 R package (see also  Supplementary Text S1 .6) that includes a single command ‘LAMPA’ with 15 arguments that allow user to specify a single protein query sequence, target database(s), information required to run HH-suit and TMHMM, and parameters of the LAMPA procedure, which are detailed in the package manual (see https://github.com/Gorbalenya-Lab/LAMPA/blob/master/LAMPA_manual.pdf). LAMPA package employs two external R packages: seqinr ( Charif and Lobry, 2007 ) and IRanges ( Lawrence  et al. , 2013 ). Output of the command is a directory, name of which is identical to the name of the file with query sequence by default. This directory contains a plot (similar to  Fig. 2 ) and two tables summarizing TM predictions and homology annotations made for the query sequence (overlapping with  Supplementary Table S2 ), as well as files with detailed information about hits constituting each cluster, and a folder with raw data (see package manual for details). Analysis of 2985 virus polyproteins against pfamA_31.0, detailed below, required 2000 min on 16 CPUs for LAMPA to complete (with 0.3–2.5 min per query, and approximately extra 1000 min compared to HHsearch). A separate script, not included in the LAMPA package, was used to automate analysis of multiple queries in this study. 3.3 Evaluation of LAMPA performance relative to HHsearch in analysis of RNA virus polyproteins We evaluated LAMPA performance under default parameter values by querying pfamA_31.0 with 2985 RNA virus polyproteins (see  Section 2.1; Fig. 1 ). This analysis documents dependence of HHsearch statistics on query size: split protein fragments or regions (‘LAMPA’) relative to intact proteins (‘HHsearch’). Only the most N-terminal cluster of hits was considered in 26 cases of overlapping clusters from the LAMPA AP-specific stage. For annotation-related statistics, we did not consider TM domains (LAMPA Stage #0,  Fig. 2 ). The output of the LAMPA Stage #1 represented also output of the HHsearch run on intact proteins. Additionally, HHsearch was also used for further statistical analyses of the difference between outputs of two tools. For these analyses, HHsearch output was not subject to post-processing (see Section 2.2) that allowed to analyse hits with Probability ≤95%,  E -value ≥ 10 and size on query ≤50 aa (see below). This use of HHsearch was outside the LAMPA framework and required matching of hits obtained by LAMPA and HHsearch for evaluation. We restricted this matching to the top-scoring hits of LAMPA hit clusters and HHsearch that overlapped on query and targeted the same Pfam profile. 3.4 LAMPA outperforms HHsearch in recognizing homology and facilitating annotation of RNA virus polyproteins Neither LAMPA nor HHsearch found homology between 163 proteins (5.5% of the dataset) and pfamA_31.0. For 2391 proteins (80.1%), LAMPA and HHsearch hit the same homologous regions, from 1 to 18. For 420 proteins (14.1%), LAMPA annotated from 1 to 3 extra regions on top of 1 to 15 found also by HHsearch ( Fig. 3A ). For each of the remaining 11 proteins (0.4%), a single region was hit by LAMPA only. Increase in number of annotated regions per protein by LAMPA was statistically significant ( P W  = 9.5e−86). By design of the procedure, HHsearch outperformed LAMPA for none of the polyproteins. For the three virus genome classes (2273 proteins in total), share of proteins, for which gain in number of annotated regions by LAMPA was observed, varied five-fold: (−)ssRNA viruses (3.1%), dsRNA viruses (10.2%) and (+)ssRNA viruses (15.9%). Among the 712 proteins with unknown virus genome class, LAMPA outperformed HHsearch for 22.2% of polyproteins. Increase in the number of annotated regions ( Fig. 3D ) was accompanied by the increase in the polyprotein coverage by annotations, which ranged from 1.0% to 25.5% of polyprotein length ( Fig. 3B;  P W  = 1.18e−72). Fig. 3. Gain of homology recognition by LAMPA compared to HHsearch. Presented are four depictions of results of querying pfamA_31.0 with 2985 RNA virus proteins using LAMPA and HHsearch. ( A ) Number of regions (hit clusters) per query protein annotated by the two tools. Each protein is depicted by a transparent gray dot. Since multiple proteins may have the same or similar number of regions annotated by the two tools ( x  and  y  dot coordinates), dots may overlap. Gray density is proportional to the number of overlapping dots. Black line, diagonal. ( B ) Share of protein length (%) annotated by the two tools. For other details, see panel A. ( C ) Overlap between Pfam profiles that were linked to RNA virus proteins by the two tools. ( D ) Overlap between RNA virus polyprotein regions annotated by the two tools Also, we compared lists of Pfam profiles hit by LAMPA and HHsearch and were used for region annotation ( Fig. 3C ,  Supplementary Table S2 ). Both tools selected 173 profiles to annotate 5737 virus regions, and extra 67 profiles were used to annotate 5508 and 5947 virus regions by HHsearch and LAMPA, respectively. Also, additional 35 profiles were solely used by LAMPA to annotate 68 virus regions. Key enzymes of RNA viruses (RdRp, helicases, proteases and methyltransferases) dominated the shared part of the LAMPA and HHsearch Pfam profile lists ( Supplementary Fig. S2A ). In contrast, the LAMPA-restricted profiles did not include RdRp but included types of enzymes and non-enzymatic proteins not found in the shared list, e.g. seven kinase profiles ( Supplementary Fig. S2B  and  Table S2 ). Many protein regions exclusively annotated by LAMPA were from most divergent RNA viruses ( Shi  et al. , 2016 ). 3.5 Both QP- and AP-specific stages of LAMPA procedure contributed to gain of annotation Gain of annotation by LAMPA compared to HHsearch is fully attributed to QP- and AP-specific stages. The gain was observed for 431 polyproteins, with the share of regions exclusively annotated by LAMPA varying from 6.2% to 100.0% (mean = 27.2%) of all recognized regions. Mean percentage of regions annotated in these proteins during the Stages #1–#3 were 72.8%, 17.1% and 10.2%, respectively ( Fig. 4 ). During QP- and AP-specific stages, regions were identified in 322 proteins (10.8% of the whole dataset) and 126 proteins (4.2%), respectively. Fig. 4. Contribution of different stages of LAMPA procedure to protein annotation. Contribution of three LAMPA stages to annotation of 431 proteins, including regions exclusively annotated by LAMPA, was measured by percentage of regions annotated in each protein. Total number of regions annotated in each protein was considered 100%, regardless of their actual number and share in the protein. The box plots, lower and upper limits of the box delimit the first (25%) and third (75%) quartiles, midline limit of the box—median, whiskers extend to the most extreme data point which is no &gt;1.5 times the interquartile range from the box, data beyond that distance are represented by points 3.6 Increase of hit statistical significance by LAMPA compared to HHsearch is modest but common LAMPA identified 507 clusters of hits on 431 proteins, HHsearch counterparts of which were removed by post-processing under the used thresholds (see Section 2.2;  Fig. 3D ). We used the top-scoring hits in these clusters to estimate the gain of statistical significance (Probability and  E -value) by LAMPA compared to HHsearch and represent clusters in all analyses described below. We identified matching HHsearch hits for all 507 LAMPA hits ( Supplementary Table S2 ) with 437 hits (86.2%) having identical coordinates on query. In each pair of hits, LAMPA hit was characterized by higher Probability and lower  E -value ( Fig. 5A and B ). Probability increase by LAMPA compared to HHsearch was in the range from 0.5% to 37.6%, with mean 5.3% ( Fig. 5A ). Decimal logarithm of LAMPA to HHsearch  E -values ratio ranged from −3.4 to −0.2 with mean −1.5 ( Fig. 5B ). Positive correlation between Probability and −log E -value was accompanied by  E -value variation around two orders of magnitude for most Probabilities before and after they were elevated above the cutoff by LAMPA ( Supplementary Fig. S3 ). Likewise, for  E -values around 10 −1 , Probability varied approximately ±5%, illustrating that choice of statistic in addition to significance cutoff may affect output. Fig. 5. Gain of hit statistical significance by LAMPA compared to HHsearch. LAMPA hits to  region  queries, obtained during the QP-specific and AP-specific stages of LAMPA procedure, are compared with matching HHsearch hits to  polyprotein  queries, in respect to hit Probability ( A ) and  E -value ( B ); and with matching HHsearch hits to putative  domain  queries (operational definition, see text for details), in respect to hit Probability ( C ) and  E -value ( D ). Analyzed HHsearch hits were not subject to post-processing 3.7 LAMPA-demarcated regions may approximate authentic domains for purpose of homology detection The LAMPA region queries may still be (much) larger than the actual domains, natural borders of which remain unknown. Because of this uncertainty, we reasoned that the gain of statistical significance by LAMPA compared to HHsearch might provide only a lower estimate for the actual difference between Probabilities and  E -values of the respective hits obtained for the polyprotein and expected for its domains. To improve understanding about how close the obtained LAMPA Probabilities and  E -values for protein regions may be to those of the actual domains, we adopted an operational definition of polyprotein domain in relation to homology hit and used it to approximate borders of the actual domains; in total 507 hits on 431 polyproteins (see above) were considered for this purpose. Operational domain was demarcated as LAMPA hit that was extended by 100 aa to the N- and C-terminus; if distance to the polyprotein terminus was &lt;100 aa, extension was adjusted accordingly (which was used in 48 of 507 cases). The demarcated domain sizes ranged from 164 to 732 aa (mean = 315 aa) that was close to dominant domain size in public databases and narrower compared to the range of 88–2172 aa (mean = 479 aa) of region queries that produced the original LAMPA hits ( Fig. 1 ). For each of 507 hits, we then compared Probability and  E -value values, assigned by LAMPA, to those obtained by HHsearch for a matching hit in a separate analysis that used demarcated domains as queries and involved no hits post-processing (see Section 2.2;  Supplementary Table S2 ). We obtained data for all 507 hits, with 457 hits (90.1%) having identical coordinates on query in LAMPA and HHsearch analyses. The difference between the two Probability values ranged from −1.8% to 4.6% with mean and median close to zero (both were equal −0.2%); absolute value of the difference did not exceed 2% in 99.8% of cases ( Fig. 5C ). Decimal logarithm of the  E -values ratio ranged from −1.3 to 1.8, mean 0.2 ( Fig. 5D ). These differences were evenly distributed and much smaller than those observed in comparison of LAMPA hits to region queries and HHsearch hits to polyprotein queries ( Fig. 5A and B ). Based on these results, we concluded that sizes of queries used by LAMPA during iterative stages may be close to those of the respective authentic domains for the purpose of statistical evaluation of homology and annotation transfer under the employed cutoff. 3.8 Increase of statistical significance of hits by LAMPA compared to HHsearch is proportional to respective decrease of query length We then asked how LAMPA-based increase of statistical significance in 507 hits of 431 proteins in 504 pairs of polyprotein and Pfam profile depended on lengths of polyprotein (original query, varied between 1039 aa and 8572 aa) and its fragments (queries varied between 88 aa and 2172 aa at LAMPA Stages #2 and #3) ( Fig. 1 ). We observed steady but highly uneven increase of Probability gain for polyproteins in the size range between 1001 aa and ∼3000 aa which then leveled ( Fig. 6A ). That positive dependence was stronger and more common when Probability gain was plotted against relative length decrease in queries of LAMPA compared to HHsearch, which varied in the range from 1× to 45.3×, with 68.2% of the decreases of query length being in the 1–10× range ( Fig. 6B ). Accordingly, Probability gain fall steeply with increase of the LAMPA query length up to 2172 aa; it was below 10% and 5% for LAMPA queries including &gt;448 aa and 747 aa, respectively ( Fig. 6C ). Fig. 6. Relationship between Probability gain by LAMPA and query lengths. Difference between Probabilities of hit to region query (LAMPA Stages #2 or #3) versus polyprotein query (HHsearch without hits post-processing) (empty circle), is compared with difference between the respective approximated Probabilities for the matching hit in computational experiments (cross) at the  y  axis, for 507 hits in total. These values are plotted against values of three characteristics of respective queries at the  x  axis: ( A ) polyprotein length (Stage #1), ( B ) ratio of polyprotein to query region length (Stage #1 versus Stage #2/3) and ( C ) query region length (Stage #2/3) 3.9 Estimation of hits Probability by LAMPA may be approximated in computational experiment Non-uniform dependence of Probability gain from query length ( Fig. 6A and C ) implied other characteristics are involved. Indeed, besides query length, target length and diversities of query and target are used by HHsearch for the calculation of  λ  and  μ  that affect hit score  P -value (see Section 2.4). Accordingly, we analyzed the relationship between estimates of hit statistical significance and possible lengths of the corresponding query and target profiles systematically using computational experiments. They used local alignment similarity score of HHsearch hit of  full-length  query-target pair for approximating hit Probability on queries of  other observed and computationally generated sizes , assuming that hit score may not change with query size. This assumption proved to be accurate within a margin of error (see below). We used the HHsearch neural networks to generate EVD parameters, followed by calculation of Probability, as well as  P -value, of hit to polyprotein region from local alignment similarity score of this hit in every full-length query-target pair for which hit Probability gain was observed (in total 507 hits;  Figs 3D  and 6; for details see  https://github.com/Gorbalenya-Lab/hh-suite-notebooks/tree/LAMPA ). First, we noted good agreement between gains of Probabilities obtained in computational experiments and LAMPA runs ( Fig. 6 ). They are within of +0.7%/−0.4% deviation of Probability gain estimation by LAMPA for the 95 percentile of hit scores in the dataset ( Supplementary Fig. S4A ). The modest difference between the two values is explained by respective deviation of the underlying similarity score of the pairwise HHsearch hit alignment for polyprotein, which was fixed in computational experiments, from region-specific score that is calculated for actual query and target profiles by LAMPA. Thus, by default, the same hit alignment involving polyprotein and its part as queries might have slightly different scores and also coordinates, further contributing to difference between the respective Probabilities (and  P -values,  Supplementary Fig. S4B ) in computational experiments. 3.10  P -value and Probability of HHsearch hits depend non-linearly on the lengths and diversities of query and target profiles in computational experiments The increase of the hit Probability during QP- and AP-specific iterations ( Fig. 6 ) is likely explained by the use of query length in the auto-calibration procedure of HHsearch (see Section 2.4). We then conducted four computational experiments for three selected query-target pairs ( Supplementary Text S1 .5) that were characterized by the largest Probability gain of LAMPA hit at Stages #2 (37.6%) and #3 (25.8%), respectively, and associated with the largest decrease of query size (47 fold) ( Fig. 7 ,  Supplementary Fig. S5  and  Table S3 ). They also represent considerable ranges of hit scores (40.2, 41.1 and 67.2 for three pairs) and target diversities (6.7, 11.5 and 7.7). Forty-one computationally designed lengths of each of three queries were tested ( Fig. 1  and  Supplementary Text S1 .5). Fig. 7. Relationship between hit statistical significance and profile lengths in computational experiments. HHsearch hit  P -value ( A–C ) and Probability ( D–F ) were estimated for 41 designed lengths of  query  or  target , each of which was equidistant from its immediate neighbor on base 10 logarithmic scale (see  Supplementary Text S1 ). The 41 pairs of values were plotted to reveal relationship between two characteristics. These plots used hit score values of three query-target pairs, which are specified at the bottom of the figure and whose respective hit statistics values at the Stage #1 (HHsearch), and Stage #2 or #3 (LAMPA) are also depicted In the three query-target pairs, both  P -value and Probability showed strong non-linear dependence on designed sizes of query and target ( Fig. 7 ) (hereafter we use ‘designed’ to distinguish computational experiment from LAMPA). Specifically,  P -value changed steeply, with curves of designed queries and targets running in parallel relative to each other ( Fig. 7A–C ). In the designed length range from 100 aa to 10 000 aa, which encompasses most queries and targets of this study,  P -value increased by approximately four orders of magnitude for queries of three pairs. This increase was limited to two orders of magnitude for the three selected queries illustrating LAMPA gain versus HHsearch. In contrast, dependence of Probability on length of designed queries and targets followed inverted logistic curve and differed between target and query as well as between the three pairs ( Fig. 7D–F ). Dependence of Probability on designed query size was most noticeable only below the 95% threshold, where it followed growth phase of logistic. The selected LAMPA and HHsearch queries were at different places of this growth phase in two query-target pairs ( Fig. 7D and E ) and outside the growth phase in third pair ( Fig. 7F ) which explained different Probability gains of LAMPA hit in these pairs. Hit score and target diversity contributed to variable Probability gain in three pairs ( Supplementary Text S1 .5). 3.11 LAMPA can significantly expand RefSeq expert annotation of RNA virus polyproteins Finally, we compared annotations of the RNA virus polyproteins by LAMPA and HHsearch versus RefSeq experts ( Fig. 8 and Supplementary Fig. S6 ). Concerning the  number  of annotated regions per polyprotein, LAMPA and HHsearch were as good as RefSeq for 38.8 and 41.4% of polyproteins, respectively, while RefSeq expert or LAMPA/HHsearch outperformed the other for 23.3/27.0% and 37.9/31.6% of polyproteins, respectively  (Fig. 8A and Supplementary Fig. S6A ). Notably, LAMPA and HHsearch annotated regions in 298 and 291 out of 426 polyproteins with no RefSeq annotation and increased the number of annotated region(s) for further 833 and 652 polyproteins. Increase in the number of annotated regions per protein by LAMPA but not HHsearch was statistically significant ( P W  = 3.11e−08 and 0.752, respectively). LAMPA and HHsearch annotations covered larger share of polyprotein (mean region length was 312 aa, 321 aa and 265 aa for LAMPA, HHsearch and RefSeq annotation, respectively). This coverage increase was observed for 78.7% and 77.5% proteins, respectively, ( Fig. 8B and Supplementary Fig. S6B ) and was statistically significant ( P W  = 1.07e−291 and 3.81e−273). We note that the above numbers apply to annotation in the ‘Region’ fields of RefSeq entries. Other fields may record non-redundant annotation which is particularly likely for RefSeq entries with zero regions annotated in the ‘Region’ field. These entries are in minority in the dataset. In summary, LAMPA expands further HHsearch annotation that may already improve RefSeq annotation of RNA virus polyproteins. Fig. 8. Summary statistic of annotation coverage by LAMPA and RefSeq experts. Comparison of the number of regions per protein ( A ) or percentage of protein length (protein coverage) ( B ) annotated by LAMPA (Stages #1–3) and RefSeq experts, based on analysis 2985 RNA virus proteins. Each protein is represented by a transparent gray dot; dot density is proportional to the number of proteins with identical characteristics. Black line, diagonal 4 Discussion In this article, we present an iterative LAMPA pipeline for advanced homology detection in large multidomain proteins and proof-of-the-principle for LAMPA in its application to RNA virus polyproteins. Statistical apparatus of HHsearch, used in LAMPA, was trained on a dataset of structurally defined domains with the median size of 142 aa to ascertain high sensitivity and selectivity, although HHsearch is used for annotation of proteins, regardless of their domain composition and size. This expand ed application of HHsearch is due to two factors: (i) in contrast to sequence diversity of query (profile) (see HHblits), domain composition of query received relatively little attention in relation to HHsearch sensitivity; (ii) considerable complexity and uncertainty of domain delineation in protein sequences. We have addressed both aspects in this study and offer a practical solution to the detection of distant homology in multidomain proteins using conventional profile-based tools in the LAMPA pipeline, which could be particularly useful in the on-going exploration of the Virosphere ( Saberi  et al. , 2018 ;  Suttle, 2007 ;  Zhang  et al. , 2019 ). Length along with diversity are the two characteristics of query and target that determine hits Probability and  P -value in HHsearch profiles’ auto-calibration procedure ( Remmert, 2011 ). We employed this procedure in computational experiments of high accuracy to plot the dependence of hits Probability and P-value from designed query/target lengths of several query-target pairs over a large size range that was beyond those used for tuning the auto-calibration procedure (12–1504 aa) and this study (1001–8572 aa) ( Fig. 1 ). The produced plots revealed constrained statistic-specific shape of considerable variation for the two statistics characterizing a hit score in relation to query size ( Fig. 7 ). Due to training of the auto-calibration procedure on the  domain  dataset, this variation informs about hit score statistics in application to  single-domain  proteins. When applied to  multidomain  proteins, like those used in this study, it illustrates how statistical significance of hit scores may be underappreciated depending on difference of sizes of the intact protein and its domains. This underappreciation is realized regardless of multidomain protein size, although it may be consistently considerable only for large proteins. In line with the Formula 1 (see Section 2.4), the computational experiments revealed also complex dependencies of statistical significance of HHsearch hits on designed target length and profile diversities of query and target ( Fig. 7  and  Supplementary Fig. S5 ). These dependencies explained variable gains of hit statistical significance by LAMPA compared to HHsearch in different query-target pairs. They also provide theoretical foundation for further efforts of improving the homology recognition by LAMPA through enriching queries using HHblits and targeting several databases, as is discussed below. For queries including single domain or larger, false-positive rate of LAMPA may not be different from that of HHsearch ( Remmert  et al. , 2012 ;  Söding, 2005 ), which is used for calculation of hit statistical significance. Our results were obtained with Probability cutoff of 95%, which was chosen to ascertain homology detection and suppress false positives. The user may use  E -value instead of Probability or lower the cutoff that will trade confidence in homology detection for increasing polyprotein coverage. We expect LAMPA to outperform HHsearch at these lower cutoffs as well. Due to logistic dependence between Probability and query length ( Fig. 7D–F ), Probability gains with under 95% cutoffs could be bigger than reported here. We used TMHMM and HHsearch to functionally annotate polyproteins on structural grounds and by homology, respectively; they were used by LAMPA to delimit uncharacterized polyprotein regions that queried Pfam 31.0 further. (As discussed in  Supplementary Text S1 .3, the use of HHsearch in the LAMPA framework was adjusted for analysis of RNA virus polyproteins). Once this iterative query-specific characterization at the QP-stage was exhausted, we used average protein domain size to delimit the remaining non-annotated regions during further database searches. This AP-stage has elements of arbitrariness which were partially addressed  ad hoc  by using two alternative starting points for query delimitation. This aspect and the entire pipeline may be advanced further. At the Stage #0, other programs in addition to TMHMM may assist with functional annotation, e.g. mapping disordered regions, or regions anomalously enriched with certain amino acid residues, or cleavage sites for particular proteases like it was demonstrated in our recent study ( Saberi  et al. , 2018 ). In that study, HHsearch was used to scan several databases, and this provision is also available in the LAMPA 1.0.0 package. Also, iterative profile programs, e.g. PSI-BLAST or HHblits, could be incorporated in the LAMPA to enrich query and improve homology recognition by targeting proteins that are not part of curated profile databases. These improvements could increase relative share of the QP-stage in homology detection and region annotation. In theory, the LAMPA may identify all domains at the #1 and QP-stage, with the AP-stage generating no hits, either due to the lack of queries or homology. Notwithstand ing future advances, the current LAMPA version may already complement HHblits, the current top homology search tool. Indeed, under the 95% Probability cutoff HHblits failed to annotate 195 of 507 regions that LAMPA but not HHsearch annotated in 431 polyproteins of this study ( Supplementary Table S2  and Text S1.4). The reported gain of hit statistical significance by LAMPA compared to HHsearch was modest but sufficient to elevate many hits above the Probability 95% cutoff. It improved homology detection and hit coverage in 14.4% of polyproteins which were enriched with sequences that share not &gt;30% identity with others in the dataset. Thus, gain of hit statistical significance by LAMPA compared to HHsearch could be larger for viruses that prototype genera or higher rank taxa rather than species dominating our dataset (see  Supplementary Text S1 .2). LAMPA annotation was most frequent for (+)ssRNA viruses, which correlates with their abundance and expand ed diversity relative to dsRNA and (−)ssRNA viruses. Most newly detected homologs may already be known in other related viruses, which is evident from names and descriptions of hit Pfam profiles that often refer to viruses and their proteins ( Supplementary Table S2 ). However, they also include those not reported in literature, e.g. ZBD and MTase domains in pp1a (YP_009052476.1) of BPNV, python tobanivirus ( Fig. 2  and  Supplementary Table S2 ). The detection of the MTase domain, which is apparently conserved in the distantly related fish WBV (YP_803214.1) in this genome location, is particularly intriguing. These viruses and other nidoviruses with genomes &gt;20 kb are known to encode one or two MTases far downstream in the pp1b part of the pp1ab polyprotein ( Saberi  et al. , 2018 ;  Schutze  et al. , 2006 ;  Stenglein  et al. , 2014 ) that were implicated in the 5’-end mRNA cap formation ( Decroly  et al. , 2012 ). These and other functional assignments ( Supplementary Table S2 ) could be used to direct experimental research and in reconstruction of evolution of RNA viruses. LAMPA facilitates homology detection and may be used to improve annotation coverage by other tools and experts in genomic projects, as well as in curated databases, including RefSeq. However, other factors besides detection of homology may affect quality of annotation ( Punta and Ofran, 2008 ;  Radivojac  et al. , 2013 ) and they were outside the scope of this study. Supplementary Material btaa065_Supplementary_Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Development and application of an algorithm to compute weighted multiple glycan alignments</Title>
    <Doi>10.1093/bioinformatics/btw827</Doi>
    <Authors>Hosoda Masae, Akune Yukie, Aoki-Kinoshita Kiyoko F, Hancock John</Authors>
    <Abstract/>
    <Body>1 Introduction The aim of our research is to elucidate the glycan recognition patterns of glycan-binding proteins (GBPs). Glycans are molecules that consist of monosaccharides and glycosidic bonds and have branched structures which are more complicated than amino acid sequences. They are synthesized by glycosyltransferases which act on glycans traveling through the endoplasmic reticulum and golgi, and they eventually reach the cell surface, where they contribute to protein binding and function. Glycan binding is known to play significant roles in cell adhesion, virus infection and other biological functions ( Varki  et al. , 2009 ). GBPs and their recognition are also involved in intracellular signaling. Thus the roles of glycans are important in cellular biology. One of the main features of glycans in physiological phenomena is their recognition by GBPs. Lectins are a particular type of GBP, which usually recognize and bind to the non-reducing end of glycans. However, it has been suggested that other internal monosaccharides may also involved in recognition ( Ohtsubo and Marth, 2006 ;  Varki  et al. , 2009 ). In terms of glycomics research, experimental technologies such as mass spectrometry, glycan arrays, lectin arrays, etc, are conducted to understand glycan structure and their mechanisms. The glycan array was developed for understanding glycan recognition mechanisms ( Alvarez and Blixt, 2006 ;  Fukui  et al. , 2002 ). This experimental technique can detect binding reactions by detecting fluorescent labels of GBPs that have bound to various glycans attached onto a chip. The glycan-binding affinity of proteins, glycan-binding viruses, antibodies and even cells can be measured. Various databases such as CFG (Consortium of Functional Glycomics) ( Raman  et al. , 2006 ) and JCGGDB ( Maeda  et al. , 2015 ) have accumulated such experimental data for glycans. However, informatics techniques for analyzing large data sets for elucidation of glycan function from experimental data is needed. The GlycoPattern web resource was recently developed to aid users in analyzing and mining glycan array data, especially from the CFG ( Agravat  et al. , 2014 ). However, there are still many methods that could be applied to such data, which are not readily available for glycobiologists to use. For example, there is an algorithmic approach that uses support vector machine technology to classify glycans and detect glycan motifs, but it is not available to glycobiologists as a web tool ( Yamanishi  et al. , 2007 ). Moreover, GNAT is MATLAB software for simulating glycan structure biosynthesis pathways. To use this software, basic knowledge of programming is necessary, and so glycobiologists can not easily use it ( Liu  et al. , 2013 ). Many bioinformatics approaches for protein analysis such as BLAST ( Altschul  et al. , 1990 ) and ClustalW ( Thompson  et al. , 1994 ) are developed and published on the Web. However, these algorithms cannot be applied directly to glycan structures. Because of the branched nature of glycans, the development of a tool that can analyze complex glycan structures has been difficult. We have reported the tool of multiple tree alignment, called MCAW (Multiple Carbohydrate Alignment with Weights) for glycan structures previously ( Hosoda  et al. , 2012 ), but we did not explain details of the scoring and backtracking algorithm. Here, we describe the details for calculating the monosaccharide and bond score as well as the algorithm flow in more detail. Furthermore, we present analysis of multiple alignment of known motifs from customized data sets. In this work, we also demonstrate the effectiveness of MCAW to efficiently align multiple glycans recognized by Galectin-3 to extract biologically meaningful glycan patterns. 2 Background 2.1 Definition of glycan structure We first describe the vocabulary used in this manuscript for readers to understand our algorithm. Glycans are classified on the basis of subtree patterns in what is called the core section, which include the subtree containing specific monosaccharides and generally containing the root node.  Figure 1  is an N-glycan structure which are usually found on an asparagine residue of proteins. These glycans have on average 10–15 monosaccharides in mammalian species. The glycan structure in  Figure 1  is represented as an unordered tree that has monosaccharide residues as nodes and glycosidic bonds as edges. The root of the tree is drawn on the right and the leaves are drawn on the left. Adjacent nodes have parent–child relationships, with the node on the root side being the parent. In glycobiology, the root is the reducing end of the glycan, and children are on the non-reducing end. Glycosidic bonds carry three types of information: the anomer (α or β configuration of the child node), the non-reducing side carbon number (usually 1 or 2), and the reducing side carbon number (usually 2, 3, 4 or 6). Fig. 1. Description of the core N-glycan structure. Glycan structures are expressed as graphs using symbols that are defined by the CFG. Each monosaccharide is signified as a node and each glycosidic bond is indicated as an edge. The reducing terminal is the root which binds to proteins and lipids, and the non-reducing terminal is the opposite side, known as leaves. Adjacent nodes have parent–child relationships, with the node on the root side being the parent. Children having the same parent are defined as siblings 2.2 PKCF We presented ‘PKCF (ProfileKCF)’ in  Hosoda  et al.  (2012)  as the text format for storing glycan profiles, based on the input data which was formatted in KEGG Chemical Function (KCF) format ( Aoki-Kinoshita, 2009 ).  Figure 2  illustrates an alignment of three glycans and its corresponding PKCF. The locations where nodes are aligned are called positions. In this example, there are eight positions in this profile (indicated by the number following the NODE line). PKCF includes information indicating the glycans that were aligned, alignment position and the node content of each position, which may include gaps or monosaccharides. This format can also represent a single glycan structure as a profile by simply storing a single glycan, with a single monosaccharide aligned 100% (by itself) at each position. Therefore multiple alignment were possible with PKCF because the format could treat not only a single glycan structure but also multiple aligned glycans as a profile. Fig. 2. ProfileKCF format example. The alignment of Glycans G1–G3 (top right) is illustrated below it. The corresponding PKCF is listed on the left 2.3 KCaM There is pairwise alignment algorithm for glycan structures called KCaM ( Aoki  et al. , 2004 ), which is a combination of the maximum common subtree and Smith-Waterman local protein alignment algorithms. This algorithm thus incorporated tree edit distance ( Bille, 2005 ) and pairwise protein sequence alignment algorithms ( Smith and Waterman, 1981 ). The global dynamic programming algorithm of KCaM is given in  Figure 3 . Fig. 3. Dynamic programming global alignment algorithm of KCaM for two glycan tree structures  T 1  and  T 2 .  u  and  v  refer to a particular node  u  in one tree and node  v  in the other, and  Q [ u , v ]  computes the alignment score of the subtrees rooted at  u  and  v .  sons ( x ) refer to the children of node  x ,  d ( x ) refers to the gap penalty of deleting node  x ,  M ( u ,  v ) refers to the mapping of  sons ( u ) with  sons ( v ), and  w ( u ,  v ) refers to the score of matching nodes  u  and  v . Further details are described in the text In this algorithm,  Q [ u , v ]  of two tree structures  T 1  and  T 2  computes the alignment score of the subtrees rooted at nodes  u  and  v  of the two trees being aligned, respectively.  sons ( x ) refer to the children of node  x ,  d ( x ) refers to the gap penalty of deleting node  x ,  M ( u ,  v ) refers to the mapping of  sons ( u ) with  sons ( v ), and  w ( u ,  v ) refers to the score of matching nodes  u  and  v .  w ( u ,  v ) is defined below.  m ,  a ,  n ,  r  are parameters of the match score for monosaccharide, anomer, carbon number on the non-reducing side monosaccharide, and carbon number on the reducing side monosaccharide, respectively.  mono ( u ) is monosaccharide name of node  u ,  anomer ( u ) is the anomeric configuration (α or β) of the glycosidic linkage between  u  and  p ( u ), where  p ( u ) is the parent node of node  u .  nonred  is the carbon number on the non-reducing side monosaccharide, and  red  is the carbon number on the reducing side monosaccharide. δ computes the difference between its two arguments, returning 0 if the same and 1 if different.
 w ( u , v ) = max ⁡ { 0 , m ( 1 − δ ( mono ( u ) , mono ( v ) ) ) + a ( 1 − δ ( anomer ( p ( u ) , u ) , anomer ( p ( v ) , v ) ) ) + n ( 1 − δ ( nonred ( p ( u ) , u ) , nonred ( p ( v ) , v ) ) ) + r ( 1 − δ ( red ( p ( u ) , u ) , red ( p ( v ) , v ) ) ) } 3 Materials and methods Here, we describe the details of the MCAW algorithm, which is based on the progressive alignment algorithm of ClustalW. In this work, we chose a progressive algorithm over an iterative one because the sizes of glycans are small and the resultant error is expected to be minimal. 3.1 MCAW algorithm In order to avoid too many gaps in the multiple alignment, each glycan is added to the multiple alignment in order of similarity. We also compute weights for each glycan based on the guide tree constructed from the distance matrix which is computed from the similarity scores between all pairs of glycans used as input. The overall MCAW procedure is as follows:
 Make a distance matrix for all pairs of input glycans by using the global alignment algorithm of KCaM. Since the similarity score computed by KCaM is at most the number of monosaccharides (of the larger glycan) times 100, the distance of two glycans can be computed by subtracting the similarity score from (100 times the number of monosaccharides of the larger glycan being compared). The parameters of  m ,  a ,  n ,  r  of KCaM are set by default to 70, 10, 10, 10, respectively, totaling 100 for each set of monosaccharide and glycosidic linkage. Create a guide tree of the glycans based on this distance matrix using the Fitch-Margoliash method ( Fitch  et al. , 1967 ). Calculate the weights of each glycan based on the guide tree; the distance from the root is used as the weight for each glycan. The scores for aligning similar structures will thus be given a small weight so that they have a less significant effect on the alignment, and conversely, the scores for less similar structures will be given a larger weight so that they have a greater influence on the alignment. This is in accordance with the ClustalW algorithm. According to the guide tree, align pairs of glycans (profiles) in order of similarity. Use the MCAW dynamic programming algorithm to align monosaccharides (positions) from the leaves toward the root. The maximum score computed among all pairs of monosaccharides (positions) represents the rootmost monosaccharides (positions) that could best align the glycans (profiles) and thus determines the backtracking point. From this pair, the glycans (profiles) can be aligned. Align the unaligned monosaccharides by inserting Ends where necessary. Repeat with the remaining glycans in the guide tree, in descending order of similarity. MCAW compares two glycan profiles containing positions that groups monosaccharides and linkages. For simplicity, we can assume that a single glycan is a simple profile of one structure. Based on this, we formulated the dynamic programming algorithm of MCAW to align glycan profiles as follows. This algorithm is based on the local alignment algorithm of KCaM and ClustalW.
 Q [ u , v ] = max ⁡ { 0 , max ⁡ v i ϵ s o n s ( v ) { Q [ u , v i ] + d ( v ) } , max ⁡ u i ϵ s o n s ( u ) { Q [ u i , v ] + d ( u ) } , 1 | A | | B | { ∑ n = 1 | A | ∑ m = 1 | B | w ( u n , v m ) a n b m } +                     max ⁡ ψ ϵ M ( u , v ) { ∑ u i ϵ s o n s ( u ) Q [ u i , ψ ( u i ) ] } } Here,  u  and  v  refer to a particular position  u  in one profile and position  v  in the other, and  Q [ u , v ]  computes the alignment score of the subtrees rooted at  u  and  v .  sons ( x ) refer to the children of  x ,  d ( x ) refers to the gap penalty of deleting node  x .  w ( u n , v m )  is the same as that used by KCaM and calculates the match score of the monosaccharides, anomers, non-reducing side carbon numbers and reducing side carbon numbers for the monosaccharides at positions  u  and  v  of glycans  A n  and  B m , respectively.  a n  (respectively  b m ) signifies the weight of the  n th glycan in profile  A  (respectively  m th glycan in profile  B ).  M ( u ,  v ) refers to the mapping of  sons ( u ) with  sons ( v ) and  ψ ( u i )  represents the positions mapped with  s o n s ( u i )  ( Hosoda  et al. , 2012 ). 3.2 Implementation We implemented steps 1–3 of the MCAW algorithm using Perl, and step 4 was implemented in Java. The Perl program calls the external KCaM program on every pair of input glycans and stores the results of the guide tree as a text file, containing weights calculated for each glycan structure. The Java program reads this file and progressively builds the multiple alignment. The results of the alignment is output in PKCF format. A web form has also been developed to take KCF-formatted glycans as input, compute the PCKF results and display the output graphically on the web. 3.3 Experimental data First, we prepared a test dataset to confirm MCAW tool performance to align an arbitrary set of glycans containing a predefined motif, the well known sialyl-Lewis X structure composed of the tetrasaccharide of sialic acidα2-3, galactoseβ1-4, N-acetyl-glucosamine and α1-3 fucose (Neu5Ac(a2-3)Gal(b1-4)[Fuc(a1-3)]GlcNAc in IUPAC format). We randomly selected six glycans containing this motif from the RINGS database. Additionally, we prepared three test datasets containing the sialyl Lewis X motif in different locations. This data has been provided in  Supplementary Figure S1–S3 . First, we randomly selected ten glycans containing more than six saccharides and containing this motif. Second, we modified two of the structures in this first data set so that the terminal sialyl-Lewis X structure has an additional mannose on its sialic acid. This was to test whether MCAW could find motifs that are located internally. Third, we added structures having no sialyl-Lewis X to the first dataset. We also prepared several analysis datasets to test our MCAW tool using data from the CFG, which are available to the public on the Web. They provide glycan array experiment data as Excel files that have measured the fluorescence intensity of glycan-protein binding affinity. There are experimental data of various lectins, and this database makes it possible to search these data by GBP (analyte) type, including C-type lectins, galectins, viruses etc. We prepared experimental data of galectin-3 which is a lectin that binds to galactose. This lectin is reported to function in eosinophil recruitment and allergic inflammation in airways  in vivo  ( Rao  et al. , 2007 ). The CFG provides several glycan array analysis data that has varied the concentration of galectin-3 (2, 5 10 µg). These array experiments were carried out on CFG array version 5 and the primary screen ids of 2, 5 and 10 µg are 6004, 6005 and 6006, respectively. To select the glycans to analyze from these arrays, we selected the high-affinity glycan structures having rank &gt; 75 and a %CV &lt; 20 ( Heimburg-Molinaro  et al. , 2011 ) from all three datasets. Rank was calculated by taking each average RFUs (relative fluorescence units) value and dividing it by the highest RFU. That is Rank = 100 × (RFUaverage/highest averageRFU) and %CV = (averageRFU/StDev) × 100. For each glycan, we divided its fluorescence intensity value by 10 000, rounded to the nearest unit and used this number as the number of times to duplicate the glycan in the data set. This method of weighting according to RFU is based on the method in ( Hosoda  et al. , 2012 ), where those glycans with higher affinities were made to be more prevalent than those with lower affinities. This allows us to accurately reflect the binding affinity results from the glycan array experiments. Consequently, the number of glycans that satisfied the criteria rank &gt; 75 and %CV &lt; 20, were as follows: the 2 µg dataset consisted of 12 types of glycans, weighted to total 53 structures; the 5 µg dataset consisted of 19 types of glycans, weighted to total 88 structures; and the 10 µg dataset consisted of 17 types of glycans, weighted to total 88 structures. We provide the actual data and binding affinity RFU values in  Supplementary Table S1  Sheet1–3. 4 Result 4.1 Multiple alignment algorithm To describe the multiple alignment results, we give an example of aligning three glycans G1–G3 in  Figure 2 . The KCaM similarity scores for each pair of glycans are: for G1–G2 = 56.9, G2–G3 = 67.0 and G1–G3 = 36.3. From this we find that G2–G3 are the most similar. Next, the weights of each glycan were calculated from the guide tree computed by the Fitch-Margoliash method, resulting in weights 376.7, 327.53 and 337.83 for G1, G2 and G3, respectively. Thus the alignment order, the guide tree, has been determined. First, G2–G3 are aligned, followed by the alignment of G1 and the aligned G2–G3 profile. We set the parameters of gap = −10 and ( w ( u ,  v )) to  m =  60,  a =  30,  n =  30 and  r =  30. As a result, the final alignment score was 545.75, and the final alignment is illustrated on the bottom right of  Figure 2 . 4.2 Web tool We have developed MCAW as web tool on RINGS, which can visualize multiple glycan alignments by entering multiple glycan structures. The input glycan structures must be specified in KCF format. RINGS provides tools to convert from a variety of formats into KCF. The glycans can be entered into the text field or specified as a file. The MCAW tool is available from  http://www.rings.t.soka.ac.jp . Users can modify the score parameters for comparing structures used in the dynamic programming algorithm, including the scores for matching anomers, monosaccharides, reducing and non-reducing side carbon numbers, and gap penalty by entering values under the advanced weighting options. Default values are preset as follows: gap = −10, matching monosaccharides = 60, matching anomers = 30, matching carbon number on reducing end side and non-reducing end = 30 each. The MCAW tool can then be executed by pressing the Submit button. The alignment result can be viewed as a profile ( Fig. 4 ), and it can also be obtained in PKCF format. The percentage of each node at each position is listed graphically. Fig. 4. The glycan profile produced by MCAW as a result of aligning a data set of arbitrary glycans containing the sialyl-Lewis X motif. The result shows that the sialyl-Lewis X structure is aligned 100% in positions 5–8 of the resulting profile 4.3 Data experiment 4.3.1 Sialyl-lewis X motif We analyzed the dataset of glycans containing the sialyl-Lewis X motif by inputting it into MCAW with the default settings for the advanced weighting options.  Figure 4  is the result showing that the motif structure aligned 100% in positions 5 through 8 of the resulting profile. Thus, it was able to successfully align this biological motif without any prior knowledge of the data set. Note that positions 9, 10, 15 and 17 also contain a similar motif, but with varying glycosidic linkages; our tool could visually express such patterns that were unexpected when constructing this data set. We further analyzed the three customized datasets containing ten glycans having the same motif, but in various positions in the glycans. The results are provided in  Supplementary Figure S4 . For the dataset of ten glycans containing this motif, we were again able to align it 100%. For the dataset containing terminal sialyl-Lewis X with an additional mannose on the non-reducing terminal, the sialyl-Lewis X motif is aligned internally 100%. Finally, for the dataset containing glycans having no sialyl-Lewis X, the results showed that the motif structure was aligned 90.9% along with Ends. If the Ends were ignored, it would be aligned 100%. 4.3.2 Galectin-3 We also analyzed the three galectin-3 datasets of varying concentrations.  Figure 5  shows the resulting profiles for 2, 5, 10 µg from top down. We provide a high-resolution version of  Figure 5  in  Supplementary Figure S5 . In these results, the N-glycan core structure and two repeated Galβ1-4GlcNAc (lactosamine) structures are highly aligned in each of the three concentrations. The datasets of lower concentrations (2 and 5 µg) additionally aligned two repeated lactosamine structures 100%, and in the 10 µg result, it was aligned slightly lower because lactosamine was modified by a fucose. Fig. 5. Result of analyzing the three datasets of varying concentrations of galectin-3 using MCAW. Our tool shows that the disaccharide Galβ1-4GlcNAc on the  N -glycan core is highly aligned. It is known that Galectin-3 interacts with lactosamine structures, and our results reflected this 5 Discussion 5.1 MCAW algorithm In this work, we provided additional details regarding the MCAW algorithm in terms of the monosaccharide and glycosidic linkage scoring and the backtracking step. Our algorithm is developed for unordered trees so that it can take as input IUPAC and KCF, which do not have strict rules for describing the input glycans. These formats may describe the same glycan but order the children differently (even randomly). In MCAW, these structures can be correctly aligned; however, because it takes the glycosidic linkage information into consideration. Thus it in effect takes into account the order of the children while being flexible to handle unordered trees. In terms of execution time, since we use local alignment dynamic programming, the computation time is  O ( T n 2 )  where  T  is the number of trees, and  n  is the number of monosaccharides in the largest glycan being compared. In other words, this algorithm is loosely bounded by the largest glycans being compared times the total number of glycans. However, because glycan structures are not as large as proteins, the results on average are computed very quickly, and on our local computers they can be obtained in about 2–3 min on average. 5.2 Alignment experiments The multiple alignment of glycans containing the sialyl-Lewis X structure showed that the motif could be aligned 100%, regardless of the location or presence of the motif across the glycans. In the execution of the MCAW tool using experimental data that measures glycan-protein interaction data from the CFG, polylactosamine was aligned at a high ratio at all the different concentrations of galectin-3, verifying knowledge in the literature ( Fukumori  et al. , 2007 ). Note here that this same disaccharide motif can also be seen all along towards the non-reducing end, but that they are aligned with Ends. At the highest concentration of Galectin-3 analyzed on the array, an additional fucose was found to be involved in binding, and in fact, recognition of Fuc α 1-2Gal by Galectin-3 has been suggested in the literature ( Ideo  et al. , 2002 ). It is known that Galectin-3 recognizes lactosamine even when carbon 2 or 3 of the galactose is substituted by fucose, sialic acid, GalNAc or sulfate. We searched the structures on the array to see if other glycans containing these modifications were arrayed. The lactosamine structure with sialic acid attached was found on the array in various configurations: one with a maximum of three lactosamines and other structures with terminal sialo-lactosamines on multiple branches. However, all of these structures had low-binding affinity, so we could not see the effect of sialic acid or these other modifications on this array. The proportion of the nodes aligned on the leaf side is low because the dataset of glycans interacting with galectin-3 contained long and short structures. When omitting the aligned monosaccharides taking up &lt;10% of a position, all concentrations showed the same alignment. Nodes below 10% can be considered as noise, so they can be ignored. Therefore this result shows that the same profile pattern for galectin-3 can be seen across all concentrations. Even if the high-affinity glycans change due to the change in concentration of the GBP, a common glycan pattern was found regardless. 5.3 Comparison with other tools To compare the results of MCAW with a similar tool, we ran the Glycoviewer tool ( Joshi  et al. , 2010 ) with the same input that we used for MCAW. This input data and the Glycoviewer results are provided in  Supplementary Figure S6 . The alignment results for 2 and 5 µg show similar alignment diagrams, but different results were obtained with 10 µg. In particular, it was different in the position of fucose and how branched fucose and galactose were expressed. In Glycoviewer, fucose is expressed on two consecutive galactoses on two antennas of GlcNAc β1-2 Man (small red dots in the center of the yellow circles at four different locations). However, in MCAW, each fucose was found once on the two antennae of GlcNAc β1- 2 Man. This can be explained by the fact that MCAW can calculate and align with gaps, whereas Glycoviewer will align without considering gaps. By arranging the gaps, MCAW can extract profiles without scattering monosaccharides. 5.4 Future work Our analysis was performed using the default parameters for  w ( u ,  v ) in the MCAW dynamic programming algorithm, which we set to gap = −10,  m  = 60,  a =  30,  n =  30,  r =  30. We have found that these parameter values are most suitable for the glycan data we have surveyed so far (data not shown). In the future, we will analyze the effects of modifying these parameters. Moreover, we will perform analysis of other GBPs to obtain more patterns of glycan structure recognition. Future work will focus on finding relationships between these patterns and protein sequence/structure. We will also consider ways to improve the result view similar to Glycoviewer which displays glycan profiles more visually with colors. This could be made an option for the user to select for their results. As for future prospects of multiple glycan alignment, score matrices of glycans will now be possible to develop, as it greatly depends on multiple glycan alignment ( Aoki  et al. , 2005 ). Glycan score matrices represent the similarity of monosaccharides bound by a glycosidic bond. By using score matrices for glycan structure comparison, a gradient can be used to compare monosaccharides as opposed to simply matching the same moosaccharides as zero or one. MCAW can also be applied to probabilistic models such as ProfilePSTMM ( Aoki-Kinoshita  et al. , 2006 ) to determine the state model used for modeling glycan recognition profiles. Therefore, the development and availability of an effective multiple glycan alignment tool opens possibilities for many other glycoinformatics analysis, making this work a big step towards furthering glycomics analysis. Supplementary Material Supplementary Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>PriorsEditor: a tool for the creation and use of positional priors in motif discovery</Title>
    <Doi>10.1093/bioinformatics/btq357</Doi>
    <Authors>Klepper Kjetil, Drabløs Finn</Authors>
    <Abstract>Summary: Computational methods designed to discover transcription factor binding sites in DNA sequences often have a tendency to make a lot of false predictions. One way to improve accuracy in motif discovery is to rely on positional priors to focus the search to parts of a sequence that are considered more likely to contain functional binding sites. We present here a program called PriorsEditor that can be used to create such positional priors tracks based on a combination of several features, including phylogenetic conservation, nucleosome occupancy, histone modifications, physical properties of the DNA helix and many more.</Abstract>
    <Body>1 INTRODUCTION Computational discovery of transcription factor binding sites in DNA sequences is a challenging problem that has attracted a lot of research in the bioinformatics community. So far more than a hundred methods have been proposed to target this problem (Sandve and Drabløs,  2006 ) and the number of publications on the topic is steadily increasing. There are two general approaches for discovering potential transcription factor binding sites with computational tools. One is to examine regulatory regions associated with a group of genes that are believed to be regulated by the same factors and search for patterns that occur in all or most of these sequences. This approach, often referred to as  de novo  motif discovery, can be used when we have no prior expectations as to what the binding motifs might look like. One concern with this approach, however, is that it might be necessary to consider rather long sequence regions to ensure that the target sites are indeed covered. Since binding motifs for transcription factors are usually short and often allow for some degeneracy, the resulting signal-to-noise ratio can be quite low, making it difficult to properly discriminate motifs from background. Another problematic issue is that DNA sequences inherently contain a lot of repeating patterns, such as tandem repeats and transposable elements, which can draw focus away from the target binding motifs when searching for similarities between sequences. The other general motif discovery approach, called  motif scanning , searches for sequence matches to previously defined models of binding motifs, for instance in the form of position weight matrices (PWMs; Stormo,  2000 ). The main drawback with motif scanning is that it tends to result in an overwhelming number of false positive predictions. According to the ‘futility theorem’ put forward by Wasserman and Sandelin ( 2004 ), a genome-wide scan with a typical PWM could incur in the order of 1000 false hits per functional binding site, which would make such an approach practically infeasible for accurate determination of binding sites. The problem here lies not so much in the predicted binding patterns themselves, since many of these would readily be bound by transcription factors  in vitro .  In vivo , however, most such binding sites would be non-functional, perhaps because the chromatin conformation around the sites precludes access to the DNA (Segal  et al. ,  2006 ) or because the target factors require the cooperative binding of additional factors nearby to properly exert their regulatory function (Ravasi  et al. ,  2010 ). One way to improve accuracy in motif discovery is to try to narrow down the sequence search space as much as possible beforehand, for instance, by masking out portions of the sequences that resemble known repeats or considering only sequence regions that are conserved between related species (Duret and Bucher,  1997 ). Kolbe  et al.  ( 2004 ) introduced a measure they called ‘Regulatory Potential’ which combines phylogenetic conservation with distinctive hexamer frequency profiles to identify possible regulatory regions. This measure calculates a score for each position along the sequence, and regions receiving higher scores are deemed more likely to have a regulatory role. Regulatory Potential can be considered as an example of a ‘positional prior’ since each position is associated with an a priori probability of possessing some specific property. Positional priors can be used as an aid in motif discovery by assigning high prior values to regions that we consider more likely to contain functional binding sites and then focus the search on these regions. Besides conservation and oligonucleotide frequencies, other features that can be relevant for assigning prior values include: localized physical properties of the DNA double helix, distance from transcription start site or other binding sites, ChIP-chip and ChIP-seq data, and potentially tissue-specific epigenetic factors such as the presence of nucleosomes and associated histone modifications. Many of the aforementioned features have previously been applied and shown to improve the performance of motif discovery by themselves (see e.g. Bellora  et al. ,  2007 ; Segal  et al. ,  2006 ; Whitington  et al. ,  2009 ), and it has also been demonstrated that further gain can be achieved by integrating information about multiple features (see e.g. Ernst  et al. ,  2010 ; Lähdesmäki  et al. ,  2008 ). We present here a program called PriorsEditor, which allows users to easily construct positional priors tracks by combining various types of information and utilize these priors to potentially improve the motif discovery process ( Fig. 1 ).
 Fig. 1. The top left panel in this screenshot shows examples of some of the features that can be used as a basis to create positional priors. These features are visualized as data tracks in the main panel for a selected set of sequences. The bottom-most track contains predicted matches to TRANSFAC and JASPAR motifs in regions with non-zero RegulatoryPotential7X scores. 2 SOFTWARE DESCRIPTION The first step in constructing a priors track with PriorsEditor is to specify the genomic coordinates for a set of sequences one wishes to analyze. Next, data for various features can be imported to annotate these genomic segments. PriorsEditor supports three types of feature data. The first type,  numeric data , associates a numeric value with each position in the sequence and can be used to represent features such as phylogenetic conservation scores, DNA melting temperatures and nucleosome-positioning preferences. Numeric data tracks are also used to hold the final positional priors. The second feature type,  region data , can be used to refer to continuous stretches of the DNA sequence that share some unifying properties which distinguish them from the surrounding sequence. Different regions are allowed to overlap, and regions can also be assigned values for various attributes, including type designations, score values and strand orientations. Features best represented as regions include genes, exons, repeat regions, CpG-islands and transcription factor binding sites. The last feature type,  DNA sequence data , represents the DNA sequence itself in single-letter code. DNA sequence data can be passed on to motif discovery programs for further analysis, and it can also be used to estimate various physical properties of the DNA double helix, such as GC content, bendability and duplex-free energy. Additional feature data can be obtained from web servers such as the UCSC Genome Browser (Rhead  et al. ,  2010 ) or be loaded from local files. Once the data for the desired features have been loaded, the data tracks can be manipulated, compared and combined to create a priors track using a selection of available operations. These include operations to extend regions by a number of bases upstream and/or downstream, merge overlapping regions or regions within close proximity, filter out regions, normalize data tracks, smooth numeric data with sliding window functions, interpolate sparsely sampled data, weight numeric data tracks by a constant value or position-wise by another track, combine several numeric tracks into one using either the sum or the minimum or maximum value of all the tracks at each position and several more. It is also possible to specify conditions for the operations so that they are only applied to positions or regions that satisfy the condition. For example, to design a priors track that will focus the search toward conserved regions within close proximity of other binding sites, one could start off with a phylogenetic conservation track, then load a track containing previously verified binding sites from the ORegAnno database (Griffith  et al. ,  2008 ), extend these sites by a number of bases on either side and lower the prior values outside these extended sites. After a priors track has been constructed, there are several ways to make use of this new data. The most straightforward way is to provide it as input to a motif discovery program that supports such additional information, for instance, PRIORITY (Narlikar  et al. ,  2006 ) or MEME version 4.2+ (Bailey  et al. ,  2010 ). Unfortunately, not many motif discovery programs are able to incorporate priors directly, so an alternative is to mask sequence regions that have low priors by replacing the original base letters with Xs or Ns since most motif discovery tools will simply ignore positions containing unknown bases when searching for motifs. Apart from being used to narrow down the sequence search space, priors information can also be applied to post-process results after motif discovery has been carried out, for instance, by filtering out predicted binding sites that lie in areas with low priors or adjusting the prediction scores of these sites based on the priors they overlap. Positional priors tracks and masked sequences can be exported for use with external tools, but it is also possible to perform motif discovery from within PriorsEditor itself by using operations to launch locally installed programs. To facilitate motif scanning, PWM collections from TRANSFAC Public (Matys  et al. ,  2006 ) and JASPAR (Portales-Casamar  et al. ,  2010 ) have been included, and users can also import their own PWMs or define new collections based on subsets of the available PWMs. Constructing priors tracks and performing motif discovery analyses can be tedious, especially when it involves many datasets and requires several steps to complete. If a user discovers a good combination of features to use for priors, it may be desirable to repeat the same procedure to analyze other sequence sets as well. PriorsEditor allows such repetitive tasks to be automatized through the use of protocol scripts. Protocol scripts describe a list of operations to be performed along with any specific parameter settings that apply for these operations. They can be programmed manually in a simple command language or be constructed using a ‘macro recording’ function which logs all operations the user carries out while in recording mode. With protocol scripts these same series of operations can be automatically applied to new sequence sets simply by the click of a button. These scripts can also be set up so that users can provide values for certain settings during the course of an execution, enabling users to select for instance a different background model or PWM threshold value to use in the new analysis. By providing a protocol script describing the operations to be performed along with a file specifying the target sequences, it is possible to run PriorsEditor from a command-line interface instead of starting up the normal graphical interface. This allows the construction and use of positional priors to be incorporated into a batch-processing pipeline. Funding:  The National Programme for Research in Functional Genomics in Norway (FUGE) in The Research Council of Norway. Conflict of Interest : none declared. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>PINE-SPARKY: graphical interface for evaluating automated probabilistic peak assignments in protein NMR spectroscopy</Title>
    <Doi>10.1093/bioinformatics/btp345</Doi>
    <Authors>Lee Woonghee, Westler William M., Bahrami Arash, Eghbalnia Hamid R., Markley John L.</Authors>
    <Abstract>Summary: PINE-SPARKY supports the rapid, user-friendly and efficient visualization of probabilistic assignments of NMR chemical shifts to specific atoms in the covalent structure of a protein in the context of experimental NMR spectra. PINE-SPARKY is based on the very popular SPARKY package for visualizing multidimensional NMR spectra (T. D. Goddard and D. G. Kneller, SPARKY 3, University of California, San Francisco). PINE-SPARKY consists of a converter (PINE2SPARKY), which takes the output from an automated PINE-NMR analysis and transforms it into SPARKY input, plus a number of SPARKY extensions. Assignments and their probabilities obtained in the PINE-NMR step are visualized as labels in SPARKY's spectrum view. Three SPARKY extensions (PINE Assigner, PINE Graph Assigner, and Assign the Best by PINE) serve to manipulate the labels that signify the assignments and their probabilities. PINE Assigner lists all possible assignments for a peak selected in the dialog box and enables the user to choose among these. A window in PINE Graph Assigner shows all atoms in a selected residue along with all atoms in its adjacent residues; in addition, it displays a ranked list of PINE-derived connectivity assignments to any selected atom. Assign the Best-by-PINE allows the user to choose a probability threshold and to automatically accept as “fixed” all assignments above that threshold; following this operation, only the less certain assignments need to be examined visually. Once assignments are fixed, the output files generated by PINE-SPARKY can be used as input to PINE-NMR for further refinements.</Abstract>
    <Body>1 INTRODUCTION Despite rapid progress toward automating many facets of research in structural biology, visualization and expert verification of computational results continue to be required. PINE-NMR (Bahrami  et al. ,  2009 ) is an automated protein NMR assignment package that accepts, as input, the amino acid sequence of a protein and peak lists associated with defined NMR experiments and provides, as output, probabilistic backbone and side chain assignments and an analysis of the secondary structure. PINE-NMR can accommodate prior information about assignments or stable isotope labeling schemes. PINE-NMR achieves robust and consistent results that have been shown to be effective in subsequent steps of NMR structure determination. In cases where the input data do not support unequivocal assignments (because of weak signals or too many missing signals) PINE-NMR provides multiple ranked possibilities that need to be evaluated. The PINE-SPARKY software package described here provides a graphical interface for reviewing possible assignments in the context of their experimental basis (peaks in multidimensional NMR spectra) and for choosing among them. The software enables the expert to inject additional knowledge into the assignment process in an efficient and straightforward manner. The functionality of PINE-SPARKY is different from the SPARKY spin graphs extension, which shows connectivities between assigned peaks, but will not handle PINE-NMR results. 2 IMPLEMENTATION We selected SPARKY as the viewing and verification tool, because currently it is the most popular NMR visualization and assignment program according to software citations in BMRB (Ulrich  et al. ,  2007 ). Another benefit is that SPARKY enables programmers to utilize its internal classes to write Python extensions. PINE-SPARKY consists of two parts: (i) PINE2SPARKY, which converts PINE-NMR assignments and their associated probabilities to SPARKY inputs and (ii) PINE. SPARKY extensions, which support intuitive interfaces that enable various visualization and assignment tasks. PINE2SPARKY converter:  Multiple assignments and their probabilities (output from PINE-NMR) are converted into labeled objects, and these objects are incorporated into SPARKY save files by the PINE2SPARKY converter ( Fig. 1 A). After the user chooses which assignment is correct, the incorrect labels can be removed. Colors of the labels are associated with the level probability. These can be configured by the user, but the default spectrum is blue for the highest probability and red for the lowest. We developed PINE2SPARKY under Lazarus, an IDE of Free Pascal, and the software is compatible with multiple operating systems (MS Windows, MacOSX and Linux).
 Fig. 1. PINE-SPARKY user interface and its use in 3D structures determination. ( A )  PINE2SPARKY  converter and SPARKY labels. ( B )  PINE Assigner  box. ( C )  PINE Graph Assigner  box. ( D )  Assign the Best by PINE  box. ( E ) Three-dimensional structure of ubiquitin determined from PINE-SPARKY assignments. PINE Assigner  is a dialog box. The peak to be analyzed is selected prior to opening the dialog box. The dialog box lists all possible assignments for that peak ( Fig. 1 B) and contains buttons that simplify the assignment selection process. Each button is labeled with its function (Update, Assign, Best probability, Unassign, Floating labels, Graph, Stop, Close). PINE Graph Assigner  is a graphical window consisting of four parts: the covalent structural representation of a tripeptide, a list of spectra associated with different NMR experiments that PINE-NMR used for the assignment ( Fig. 1 C), buttons with defined functions (Previous residue, Next residue, Update, Assign, Unassign, Close), and list of labels. When the user chooses a residue from the protein sequence, the graphical window displays all the atoms in that residue as well as the atoms in the residues sequentially to either side. Atoms with assignments are color coded (yellow for  1 H, red for  13 C, blue for  15 N); gray denotes atoms that PINE-NMR was unable to assign. Chemical shifts and their standard deviations associated with the assignments are displayed below and to the right of each assigned atom. When the user clicks on an individual atom and a spectrum, PINE Graph Assigner displays a ranked list of PINE-derived assignment connectivities to that atom from that spectrum. By going to the spectrum view, the user sees a list of available peak labels associated with the chosen atom. One can assign or unassign peaks with a few mouse clicks. The list of spectra includes only those currently loaded into PINE-SPARKY. Assign the Best by PINE  enables the user to bypass the manual steps needed to fix assignments. The user can choose a threshold, such as 90%, and Assign the Best by PINE will fix all assignments with probabilities greater than or equal to this value ( Fig. 1 D). 3 RESULTS AND CONCLUSION We used NMR data from the 76-residue protein, human ubiquitin, to illustrate the use of PINE-SPARKY in a structure determination project.  1 H– 15 N HSQC,  1  H– 13 C HSQC, CBCA(CO)NH, HNCACB and HBHA(CO)NH data sets were collected to support backbone assignments, and (H)CC(CO)NH, H(CC)(CO)NH and HCCH-TOCSY data sets were collected to support sidechain assignments.  15 N-edited NOESY  13 C-edited NOESY data sets were used in a subsequent structure determination. NMRpipe (Delaglio  et al. , 1999) was used to process all NMR spectra, and NMRdraw (Delaglio  et al. , 1999) was used to pick peaks in all but the NOESY data sets. ATNOS (Herrmann  et al. ,  2002 ) was used to pick NOESY peaks. We generated a SPARKY project and save files with the processed spectra. PINE-NMR was used to generate probabilistic assignments, and these were uploaded via the PINE2SPARKY converter. Tolerances for  13 C  15  N were set at 0.4 ppm, and that for  1 H was set to 0.03 ppm. Assign the Best by PINE was performed with a threshold of 0.9 (90%) with all (non NOESY) NMR spectra. Peaks that remained unassigned after that process were assigned with PINE Graph Assigner and PINE Assigner. Assign the Best by PINE with 0.9 threshold assigned more than 90% of the peaks automatically. After this the procedure, it was possible to quickly assign the remaining peaks with small number of clicks using PINE Graph Assigner. TALOS (Cornilescu  et al. ,  1999 ) was used to determine torsion angle constraints from the assigned chemical shifts: 106 torsion angles involving 53 residues were judged to be ‘good’ by TALOS, and these were used constraints along with the NOESY data in 3D structure calculations by CYANA (Güntert,  2004 ). In the resulting 20 best structures, the root mean standard deviation was 0.46 Å for backbone atoms and 1.22 Å for all heavy atoms in the structured regions ( Fig. 1 E). The following is an analysis of the time required to determine the structure following initial data collection: PINE-NMR run (∼1 h), PINE-SPARKY analysis (30 min), TALOS analysis (20 min), CYANA structure determination (7 min) with 16 CPUs. Funding :  National Institutes of Health  [grant numbers  P41 RR02301  and  1U54 GM074901 ]. Conflict of Interest : none declared. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Author Index</Title>
    <Doi>10.1093/bioinformatics/btq505</Doi>
    <Authors/>
    <Abstract/>
    <Body>Abeel, T. i554 Ahn, T. i414 Akutsu, T. i460 Ala, U. i618 Albrecht, M. i561 Alessio, M. i531 Anwar, S. i547 Asai, K. i460 Askenazi, M. i482 Attwood, T.K. i568 Bailey, J. i524 Balzer, S. i420 Baral, C. i547 Bartaševičiūtė, E. i540 Bartenschlager, R. i653 Batt, G. i603 Beißbarth, T. i596 Bender, C. i596 Berezovsky, I.N. i497 Blanchet, C. i540 Bonnet, E. i638 Bopardikar, A.S. i414 Bryne, J.C. i540 Cai, J. i547 Cannistraci, C.V. i531 Cantone, I. i603 Carbonell, J.G. i645 Chen, K. i489 Chen, K.-B. i504 Colak, R. i625 Csaba, G. i474 Dao, P. i625 Daran, J.-M. i433 Davicioni, E. i625 de Ridder, D. i433 de Jong, H. i603 Deane, C.M. i611 Di Cunto, F. i618 Disfani, F.M. i489 Eils, R. i653 Erhard, F. i426 Ester, M. i625 Fages, F. i575 Fröhlich, H. i596 Gay, S. i575 Goessler, G. i603 Goncearenco, A. i497 Höner zu Siederdissen, C. i453 Hamada, M. i460 Haviv, I. i524 Henjes, F. i596 Heringa, J. i412 Higuchi, T. i589 Hofacker, I.L. i453 Hristov, B.H. i446 Huang, H. i467, i659 Ideker, T. i531 Ison, J. i540 Jonassen, I. i420, i540 Joseph, A. i540 König, R. i653 Kaderali, L. i653 Kahn, C.L. i446 Kaipa, K.K. i414 Kalaš, M. i540 Kato, Y. i460 Kedarisetti, K.D. i489 Kell, B.D. i568 Klein-Seetharaman, J. i645 Korf, U. i596 Kowalczyk, A. i524 Kumar, A. i653 Kurgan, L. i489 Lanzén, A. i420 Lee, K. i414 Lengauer, T. i561 Liang, S. i547 Linial, M. i482 Maciel, C.D. i632 Macintyre, G. i524 Malde, K. i420 Marsh, J. i568 Matula, P. i653 McDermott, P. i568 Michailidis, G. i517 Michoel, T. i638 Mizianty, M.J. i489 Molineris, I. i618 Monteiro, P. i603 Montevecchi, F.M. i531 Moreau, Y. i412 Moser, F. i625 Naamati, G. i482 Nagao, H. i589 Nam, D. i511 Narayanan, R. i414 Nguyen, N. i467, i659 Nijkamp, J. i433 Nikoloski, Z. i582 Oraintara, S. i659 Page, M. i603 Pettifer, S. i540 Pettifer, S.R. i568 Piro, R.M. i618 Provero, P. i618 Puntervoll, P. i540 Qi, Y. i645 Rapacki, K. i540 Raphael, B.J. i446 Ravasi, T. i531 Ravid-Amir, O. i440 Rebhan. I. i653 Reinders, M. i433 Rito, T. i611 Rohr, K. i653 Rosset, S. i440 Saeys, Y. i554 Saito, M.M. i589 Sajitz-Hermstein, M. i582 Salari, R. i625 Sato, K. i460 Schönhuth, A. i625 Schlicker, A. i561 Sharma, A. i420 Shojaie, A. i517 Soliman, S. i575 Srikantha, A. i414 Stach, W. i489 Suratanee, A. i653 Töpfer, A. i540 Tari, L. i547 Tastan, O. i645 Thorne, D. i568 Van de Peer, Y. i554, i638 Van Landeghem, S. i554 Venkataraman, Parthasarathy i414 Villanueva, E. i632 Vo, A. i467, i659 Wörz, I. i653 Wang, Z. i611 Watanabe, Y. i460 Weston, J. i645 Wiemann, S. i596 Winterbach, W. i433 Yoshida, R. i589 Zhang, Y. i504 Zimmer, R. i426, i474 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Understanding the limits of animal models as predictors of human biology: lessons learned from the sbv IMPROVER Species Translation Challenge</Title>
    <Doi>10.1093/bioinformatics/btu611</Doi>
    <Authors>Rhrissorrakrai Kahn, Belcastro Vincenzo, Bilal Erhan, Norel Raquel, Poussin Carine, Mathis Carole, Dulize Rémi H. J., Ivanov Nikolai V., Alexopoulos Leonidas, Jeremy Rice J., Peitsch Manuel C., Stolovitzky Gustavo, Meyer Pablo, Hoeng Julia</Authors>
    <Abstract>Motivation: Inferring how humans respond to external cues such as drugs, chemicals, viruses or hormones is an essential question in biomedicine. Very often, however, this question cannot be addressed because it is not possible to perform experiments in humans. A reasonable alternative consists of generating responses in animal models and ‘translating’ those results to humans. The limitations of such translation, however, are far from clear, and systematic assessments of its actual potential are urgently needed. sbv IMPROVER (systems biology verification for Industrial Methodology for PROcess VErification in Research) was designed as a series of challenges to address translatability between humans and rodents. This collaborative crowd-sourcing initiative invited scientists from around the world to apply their own computational methodologies on a multilayer systems biology dataset composed of phosphoproteomics, transcriptomics and cytokine data derived from normal human and rat bronchial epithelial cells exposed in parallel to 52 different stimuli under identical conditions. Our aim was to understand the limits of species-to-species translatability at different levels of biological organization: signaling, transcriptional and release of secreted factors (such as cytokines). Participating teams submitted 49 different solutions across the sub-challenges, two-thirds of which were statistically significantly better than random. Additionally, similar computational methods were found to range widely in their performance within the same challenge, and no single method emerged as a clear winner across all sub-challenges. Finally, computational methods were able to effectively translate some specific stimuli and biological processes in the lung epithelial system, such as DNA synthesis, cytoskeleton and extracellular matrix, translation, immune/inflammation and growth factor/proliferation pathways, better than the expected response similarity between species.</Abstract>
    <Body>1 INTRODUCTION From basic biology to translational medicine and clinical trials, animal models have been an invaluable tool for inferring human biological responses. Yet, in spite of the advances these models have facilitated, numerous findings have also been unsuccessfully translated to humans, as evidenced by the failure of many clinical trials. These failures could derive from species-specific differences in response to perturbations or stimuli that would preclude naively translating information learned in one animal model directly to another. Systems biology offers the means for understanding the limits of translatability of animal models in different settings, from clinical trials to toxicological assessments to basic cell biology. This approach can provide a more comprehensive predictive model because it considers changes at different levels of the entire system. This is achieved through the development of systematic studies and integration of data over multiple experiments and data-generation platforms ( Barabasi and Oltvai, 2004 ;  Consortium, 2004 ,  2010 ;  Gerstein  et al. , 2010 ;  Goh  et al. , 2007 ;  Meyer  et al. , 2012 ;  Papin  et al. , 2005 ;  Tarca  et al. , 2013 ). These more complete models will aid our understanding of at what regulatory levels and to what degree responses to different perturbations are translatable between species. When developing models for species translation, orthologous genes are commonly thought to share the same or similar function. This assumption does not always hold, as several reports show that even among closely related species this is not necessarily the case ( Gharib and Robinson-Rechavi, 2011 ). Such divergence goes beyond differences in function and can be seen in changes in essentiality; among 120 mouse orthologs of human essential genes, 27 (22.5%) were found to be non-essential ( Liao and Zhang, 2008 ). In contrast, while paralogs may be expected to diverge more often than orthologs, it has been observed that changes in paralog function are observed with the same frequency as in orthologs ( Studer and Robinson-Rechavi, 2009 ). Certainly, changes in the essentiality and functional role of a gene product are not solely driven by differences in gene sequence but also other factors (i.e. spatiotemporal expression of genes) must be considered when investigating species translation. Gene expression, being at the core of biological function, is commonly used to evaluate changes between species and their response to perturbations. The conservation of promoters and transcription factor (TF) binding sites are important predictors of gene expression similarity, and there is a correlation between conservation of TF binding events and conservation of the target gene expression ( Hemberg and Kreiman, 2011 ). TF promoter binding sites are conserved in liver cells for ∼ 30% of the cases when comparing human and mice ( Odom  et al. , 2007 ), and most conserved non-coding DNA regions in vertebrates correspond to regulatory elements ( Hemberg  et al. , 2012 ). Existing species translation methods rely heavily on the concept of pathways for organization and prediction ( Alleyne  et al. , 2009 ). Indeed, it seems that pathways may be better conserved than its individual components (i.e. genes and proteins;  McGary  et al. , 2010 ;  Subramanian  et al. , 2005 ), as groups of orthologous genes may continue to operate together between species. In such cases, pathway analysis provides important organizational information on the potential action of sets of genes. The two main approaches for deriving pathways or sets of functionally coherent genes are topology-driven and data-driven ( Melas  et al. , 2011 ). The sbv IMPROVER Species Translation Challenge (STC), using a systems biology approach, provided participants with both training and test datasets designed to assess the ability of methods to predict responses in normal human bronchial epithelial (NHBE) primary cells coming from two different donors from the responses observed in normal rat bronchial epithelial (NRBE) primary cells coming from an inbred laboratory strain. These cells were exposed to 52 different stimuli. Stimuli were chosen to ensure a broad spectrum of perturbations in the cellular system, and for each stimulus, samples were collected at different time points to generate phosphoproteomics, gene expression and secreted cytokines data. These data were used by 29 teams to make 49 predictions across four different sub-challenges that were each evaluated using multiple scoring metrics. The STC was centrally focused on two questions: (i) can the phosphoproteomic responses in human cells be predicted given responses generated by the same stimuli in rat cells? If so, does the accuracy of this prediction depend on the nature of the applied perturbation? (ii) Which gene expression regulatory processes (biological pathways/functions) are predictable across species? 2 METHODS 2.1 Data preparation A complete description of the experimental design, data set generation and processing can be found in ( Poussin  et al. , 2014 ). In brief, 19 phosphoproteins, 22 cytokines and genome-wide mRNA levels were measured under 52 different stimuli or Dulbecco’s Modified Eagle’s Medium (DME) control treatments (in triplicate),  Table 1 . The experiment was performed in two parts: 40 stimuli in the first experiment and 12 in the second. In each part, primary NHBE and NRBE cells were grown and exposed to the indicated number of stimuli. Cells were collected and lysed at different time points: 5 and 25 min. For phosphoprotein measurements, 6 h for gene expression measurements and 24 h for cytokine measurements. All cells were exposed to stimuli in triplicate, and DME controls were performed in 4-, 5- or 6-plicate.
 Table 1. STC datasets Dataset Condition Number of replicates Number of measurements Time point(s) Total size Phospho-proteomics 52 stimuli 3 biological replicates 18 phosphoproteins 5 min 10 000+ data points 25 min mRNA expression 20 000 human genes 6 h 330+ CEL files 19 000 rat genes Cytokine level 22 cytokines 24 h 7000+ data points mRNA samples from the first experiment were processed in three batches. Each batch included human and rat mRNA for a subset of stimuli. DME control mRNA samples (four replicates) were measured for each batch. For the second experiment, all mRNA samples were processed together, including DME control mRNA samples (five replicates). Low-quality chips were excluded following quality control (QC) analysis. All remaining expression data including two to three replicates per stimulus were normalized using GC robust multiarray averaging within species. Probesets were mapped to gene symbols using Affymetrix annotations: HG-U133 Plus 2 (na33) and Rodent 230 2.0 (na32), for human and rat, respectively. Probesets mapping to multiple genes were excluded. In cases of multiple probesets mapping to the same gene, the probeset with the highest average expression over all experimental conditions was selected as representative. These high-quality normalized gene expression data in the gene symbol namespace were provided to the participants. Protein phosphorylation status was measured independently for each experiment part in cell lysates collected at 5 and 25 min (in triplicates) using Luminex xMap ( Dunbar, 2006 ). Experiment parts 1 and 2 have 6 and 5 DME controls, respectively. After QC, 16 phosphoproteins were kept for the challenge. Data were normalized using a robust regression, and normalized values were provided as the ratio of residuals to the root mean squared error of the fit. Cytokine data were similarly processed, though normalization was carried out by taking the  Z -score of each cytokine across all stimuli within an experimental batch, including DME controls. All data were divided into two equal groups, subsets A and B, by stimulus treatment to be used for training and testing of methods. To ensure similar distributions of signals in both data subsets, stimuli were separated through a data-driven approach that clustered stimuli according to phosphorylation level, gene set activation, gene expression (GEx) batch and differential gene expression. For each cluster, stimuli were randomly assigned to subset A or B. Orthologs were identified using the HGNC Comparison of Orthology Predictions (downloaded December 19, 2012). Only gene symbol mappings between human and rat were used. A total of 12 458 orthologs were common between human and rat Affymetrix arrays after mapping of probesets to gene symbols. Gene sets were based on the C2CP (Canonical Pathways) collection from MSigDB v3.1 of the Broad Institute ( Subramanian  et al. , 2005 ). This collection was filtered to remove highly redundant gene sets, i.e. overlapping gene sets with many shared members, ensuring that remaining gene sets cover as many pathways/biological functions as possible. The resulting 246 gene sets were used for the STC. Gene set enrichment analysis (GSEA) was performed to assess co-regulation of genes representative of pathways/biological functions. For the analysis, genes were ranked based on calculated LIMMA t-values comparing respective DME control versus stimulus conditions ( Smyth, 2004 ). LIMMA was performed using the  lmFit  and  eBayes  functions from the  limma  R package for the R Statistical Language with default parameters. The design matrix was constructed to compare the batch-specific DME control with each stimulus individually. Computed NES and associated significance values for each gene set were indicative of the activation/perturbation (increase or decrease) of pathways/biological functions by each stimulus in NHBE and NRBE cells ( Subramanian  et al. , 2005 ). GSEA size parameters were  min  = 15 and  max  = 500. GSEA NES and FDR q-values were provided to participants. 2.2 Scoring Sub-challenges 1 (SC1), 2 (SC2) and 3 (SC3) were scored as binary classification problems. Starting with the postulate that no single metric will capture all the attributes of a prediction, we used an aggregate of three metrics for evaluation. The metrics were proposed by IBM team members, and an independent panel of experts comprising the External Scoring Panel (ESP) decided on the final scoring approach. Participant identities were kept anonymous to the IBM team scoring the submissions. Five other metrics were considered but rejected as being redundant to the chosen three. The details of these metrics were not disclosed to the participants until the end of the challenge to avoid influencing method development toward optimizing for the scoring function rather than solving the scientific question posed. This practice is in keeping of other prediction evaluation challenges, like CASP, DREAM and a previous iteration of sbv IMPROVER. We used non-redundant metrics that highlight three different qualities of a prediction: threshold versus non-threshold, order-based versus confidence-based and different ways of rewarding correct versus incorrect predictions. The chosen metrics were also selected to avoid rewarding pathological predictions, e.g. predicting all items to be of one class. Further complicating metric selection, the quantities of both classes (active and inactive) were imbalanced in the STC with active cases accounting for only ∼ 10% of all cases. Participants were required to give confidence values for their predictions of either protein phosphorylation status or gene set activation (increase or decrease) to a given stimulus, depending on the sub-challenge. Confidence values could range between 0 and 1, where 1 represents the full confidence of an element being activated (either up- or downregulated) and 0 for full confidence of inactivation. A binarized gold standard (GS) was developed for protein phosphorylation status and gene set activation. For the phosphoprotein GS, normalized expression levels, which is akin to the standard deviation of a normal distribution, with an absolute value ≥3 were considered active, as agreed on by the ESP. Similarly for gene set activation, GSEA FDR q-values ≤0.25 were designated active, as recommended by GSEA. The submitted matrix of predictions (stimuli versus protein or gene set response) could have been scored column-by-column or row-by-row and then aggregated together. However, given the sparseness of the GS for both protein phosphorylation status and gene set activation, we decided (in agreement with the ESP) to transform the matrix into a vector for scoring, i.e. columns of the matrix were joined to obtain single vector. 2.2.1 Metric descriptions Area Under the Precision – Recall Curve  (AUPR) is a well-known measure of classifier power. A list of items is ordered by descending confidence value (used only for ranking and not directly in the metric). The list is traversed corresponding to increasingly permissive confidence thresholds, and precision (fraction of ‘active’ predictions that are correct) is plotted versus recall (fraction of true ‘active’ class members correctly predicted). The area under this precision–recall curve is the AUPR score and is represented by a single number that summarizes the tradeoff between both measures. Balanced Accuracy  (BAC) avoids magnifying performance estimates of imbalanced datasets. It is computed as the average accuracy of either class.
 (1) B A C = 1 2 ( T P P + T N N ) 
 where  TP  is the number of true positives,  P  is total number of positives,  TN  is the number of true negatives and  N  is the total number of negatives. For the STC, we used a confidence threshold of 0.5 to binarize the predictions as either positive (≥0.5) or negative (&lt;0.5). Pearson Correlation Coefficient  (PCC) describes the linear dependence between two variables. In the STC, it was computed as the correlation between the predictions and the binarized GS, where 1 indicates an item is active and 0 inactive. PCC normally ranges from −1 to 1, but to be consistent with the AUPR and BAC measures, which range from 0 to 1, we used a normalized PCC:
 (2) P C C n o r m a l i z e d = 1 2 ( P C C + 1 ) 
 For simplicity, we will refer to PCC normalized  as PCC when in reference to the challenge scoring metric. 2.2.2 Metrics aggregation A rank-sum scheme to aggregate scoring metrics was proposed by the IBM team, along with one alternative, and was selected by the ESP because it equally weights each metric to produce an overall ranking. This rank-sum scheme was composed of ranking all teams within each respective metric. A team’s aggregate rank was then calculated by summing their rank across these three metrics. This rank sum was used for the final ordering of participants, with best performers achieving the lowest rank sum. To determine the robustness of these rankings, bootstrapping was performed to ensure that best performers were not highly sensitive to the exact configuration of GS. GS was sampled without replacement 1000 times, and the rankings recomputed each time. Given the imbalanced nature of GS, the bootstrapping was constrained to maintain the same proportion of active versus inactive items as observed in the entire GS. 2.3 Statistical significance of metrics The null distribution for each metric in SC1-3 was generated by scoring 10 6  random predictions. To generate the confidences of a random submission, a uniform random number  r  ( 0  ≤  r  ≤  1 ) was generated for each ‘item’. For SC1-3, the null hypothesis simulation was used to compute  Z -scores. The mean ( μ ) and standard deviation (σ )  of the scores obtained by the simulated prediction was computed and combined with an individual team’s score ( x ) to calculate the  Z -score.
 (3) Z s c o r e = x − μ σ 
 FDRs were computed for each metric using the R ( Computing, 2013 ) function  p.adjust  with the  method  = ‘fdr’, which computes the  Benjamini and Hochberg (1995)  correction. To compute a score’s  P -value for each of the metrics, we counted the number of random predictions that were better than or equal to the observed score and divided it by the number of simulated predictions. FDR correction ( Benjamini and Hochberg, 1995 ) was applied to the  P -value, and a value of ≤0.05 was considered to be statistically significant. The measure  S  represents the overall response similarity between human  H  and rat  R  GS, and is a Matthews correlation coefficient (MCC). The MCC represents a Pearson correlation between two binary vectors. A high  S  value would indicate a putatively conserved response and a signal that is expected to be translatable. Similarity measures can also be calculated per stimulus  S s  = MCC( R s ,  H s ), where  R s  and  H s  are binary vectors of phosphoprotein or gene set responses to stimulus  s ; per phosphoprotein  S p  = MCC( R p ,  H p ), where  R p  and  H p  are binary vectors of responses to stimuli for phosphoprotein p; and per gene set  S g  = MCC( R g ,  H g ), where  R g  and  H g  are binary vectors of responses to stimuli for gene set  g . Predictability  Pr  represents the overall similarity or agreement between the GS and a team’s or aggregate of teams’ predictions  T , and is a MCC. A high  Pr  value would indicate good prediction performance and that the response was predictable. Like  S ,  Pr  can be calculated per stimulus Pr s  = MCC( GS s ,  T s ), where  GS s  and  T s  are binary vectors of predicted phosphoprotein or gene set responses to stimulus  s ; per phosphoprotein Pr p  = MCC( GS p ,  T p ), where  GS p  and  T p  are binary vectors of predicted responses to stimuli for phosphoprotein  p ; and per gene set  Pr g  = MCC( GS g ,  T g ), where  GS g  and  T g  are binary vectors of predicted responses to stimuli for gene set  g . The empirical  P -values for the presence of genes in overlapping gene sets were calculated by sampling 10 5  times choosing a group of 25 gene sets of 246 gene sets. The frequency a gene is a member of the 25 randomly selected gene sets is recorded. The  P -value is obtained by dividing the frequency a gene was found in at least  x  gene sets by 10 5 . 3 RESULTS The STC consisted of four sub-challenges, each addressing a different aspect of translatability: the intra-species protein phosphorylation prediction (SC1), the inter-species protein phosphorylation prediction (SC2), the inter-species pathway perturbation prediction (SC3) and the species-specific network inference (SC4). We explored the translatability of signals between different layers of transduction pathways by asking whether gene expression measurements are sufficient to predict upstream changes in protein phosphorylation. Furthermore, we examined across-species similarity in pathway activation by testing whether it was possible to predict the gene set activation and phosphorylation status of different pathways and important signaling proteins, respectively, in human lung epithelial cells given expression data in rat. These questions could reveal to what extent mathematical models are capable of recapitulating perturbed cellular functions from different data types within the same cell type and its across-species cell counterpart. While the primary aim of SC2-4 was species translation, SC1 focused on assessing the informative power of transcriptional changes in response to different stimuli to infer phosphorylation responses. Transcriptional changes are typically the result of upstream signaling events driven by phosphorylation cascades. SC1 sought to address whether changes in gene expression are sufficiently informative to infer the molecular modifications observed upstream, in particular, the phosphorylation status of effector proteins. Furthermore, insights derived from this challenge could be informative for teams in the remaining sub-challenges. When making across-species predictions, it may be important to understand to what extent transcriptional data should be weighted when inferring phosphoproteomic responses. Hence for SC1, participants were provided with GEx, protein phosphorylation (P) and secreted cytokine (Cy) data from stimuli subset A as training data ( Fig. 1 A). For testing, participants were asked to predict which proteins showed changes in their phosphorylation status (up- or downregulation is hereafter considered as an activation also stated as a response) for each stimulus in subset B. These predictions were to be reported as confidence values between 0 and 1, where 1 indicated the highest confidence of activation and 0 the lowest confidence. Phosphorylation levels were measured by the Luminex xMAP technology—a bead-based assay where microspheres are coated with antibodies designed to bind specifically to phosphorylated proteins—in primary NRBE cells under growing conditions (see methods).
 Fig. 1. Overview of the STC: ( A ) Schematic of predictions to be made for each sub-challenge. Each sub-challenge required the prediction of the different sets of responses, indicated in red. ( B ) Schematic of SC4 to indicate utilization of a provided reference network with species-specific information from the training dataset to generate species-specific networks through the addition and removal of edges. Though cytokine measurements were made available to participants, they were not used in scoring, and for simplicity, were not included in this overview figure As SC1 dealt with inferring protein phosphorylation status from downstream gene expression response, SC2 extended that aim to assess the across-species translatability of that phosphorylation status over the same set of proteins and stimuli. This sub-challenge required the prediction of human phosphoprotein activation in subset B based on equivalent data from homologous phosphoproteins in rat. The participants were provided with P, GEx and Cy data from subsets A and B in rat and subset A in human ( Fig. 1 A). Predictions could be based on translating signals directly from rat phosphoproteins to human phosphoproteins. They could also be made using GEx data to generate across-species inferences of gene expression that would then be used to predict human phosphoprotein status leveraging computational approaches developed for SC1. Similar to SC2, SC3 sought to explore the across-species translatability of molecular changes in the signaling response pathway, here focused on transcriptomic responses. Though orthologous genes by sequence conservation do not necessarily share the same pattern of expression changes, functionally coherent sets of genes representing biological pathways may often have a more conserved response between species or continue to operate as a group. It may also be the case that similar pathways are activated between species, but each uses different sets of genes from the same gene families. As such, SC3 asked for a prediction of the activation status of a broad range of gene sets in subset B of human cells ( Fig. 1 A), provided similar data as in SC2, along with gene set enrichment scores with associated significance values for subsets A and B in rat and subset A in human. From this sub-challenge, we hoped to identify which biological processes/pathways are similarly or differently perturbed between species, enabling the identification of conserved or divergent responses between biological systems. The goal of SC4 was to infer human and rat networks given P, GEx and Cy data, as well as an  a priori  reference network ( Fig. 1 B). Participants were asked to use network inference methodologies to add or remove edges from the reference network to produce rat-specific and human-specific networks (see Bilal  et al.  in this issue). This sub-challenge differed from the others in that there is no obvious GS, but instead looked to leverage the wisdom of crowds to develop a consensus network that describes the conservation and divergence of biological pathways and interactions in response to the stimuli in subset A. 3.1 Challenge results SC1-3 was scored in three different ways using different criteria and measured by the PCC, AUPR and BAC between the submitted confidence values and binarized GS. The ranks of the participants for each of these metrics were combined to obtain a final ranking (see methods). The robustness of these ranks was evaluated by subsampling 10% of the GS 1000×, while preserving the proportion of active/inactive calls, and calculating  P -values ( Supplementary Fig. S2 ). As shown in  Supplementary Figures S1A  and  S2A  and  Supplementary Table S1 , from among 21 participating teams in SC1, the top three teams—teams 49, 50 and 75—could not be distinguished robustly between one another, and all were declared best performers (see Dayarian  et al.  in this issue). We compared these results to a series of aggregated ‘teams’ formed by averaging the prediction confidences of the best N teams to ascertain whether information could be gained by leveraging the wisdom of crowds. We found that the score for the aggregate of all teams ranks fourth overall and is better than the best performers in two out of three metrics (AUPR and PCC,  Supplementary Fig. S3A ). From among 13 participating teams in SC2, team 50 was clearly the best performer, followed by Team 111 [see ( Biehl  et al. , 2014 ) in this issue]. In this sub-challenge, averaging the predictions of all teams did not fare better than the best performer, but ranked fifth overall and was better than the second best performer in two of three metrics (AUPR and PCC,  Supplementary Fig. S3B ). Finally in SC3, of 7 participating teams, team 50 was again the best performer, followed by Teams 49 and 111, which tied for second (see Hormoz  et al.  and Hafemeister  et al.  in this issue). As in SC2, averaging the prediction confidences of all teams did not fare better than the best performer, but the aggregate of all teams ranked fourth overall and was better than the second best performer in two of three metrics (AUPR and PCC,  Supplementary Fig. S3C ). A known risk in classification problems is that some algorithms correctly separate the classes but label them incorrectly. Having seen such mislabeling in previous challenges, we attempted to identify similar occurrences in this challenge. Though reversing class labels may be less likely when datasets are highly imbalanced, as the STC was with only an ∼ 10% GS activation level in SC1-3, several teams from across different sub-challenges would have improved their rank if their class labels were reversed. It is important to note that if a prediction is close to random, then evaluating the reversed labels can give a small increase of performance. However, our aim was to look for predictions with large differences in their scores and where the prediction with reversed labels scores much higher. In SC1 four teams received a slightly better score when their prediction labels were reversed, and two teams achieved slightly better scores in SC2. SC3 stood out with one team, Team 111, having clearly reversed its labels, and its revised score would have positioned them as the best performer (see Hafemeister  et al.  in this issue). The overall success of participants in a sub-challenge can be measured by the median  Z -score of the scoring metrics, which may be used to quantify the amount of predictive signal available in the provided data for a given classification problem.  Z -scores offer a useful cross-challenge measure, as it takes into account size differences in the universe of predictions; important, since participants had to predict the activity of 16 × 26 phosphoprotein–stimulus pairs (SC1-2) and 246 × 26 gene set–stimulus pairs (SC3). Comparisons of the  Z -scores for the three different metrics in  Figures 2 A–C suggest that protein phosphorylation was easier to translate across species (SC2) than solely within species from GEx (SC1), as reflected by higher  Z -scores for AUPR and PCC. Inter-species protein phosphorylation also appeared easier to translate than inter-species pathway activation (SC3), as supported by the lower AUPR and PCC  Z -scores for SC3 compared with SC2. The  Z -scores for all three sub-challenges were tied for BAC ( Fig. 2 A–C).
 Fig. 2. Scores and computational methods used for solving the STC. The null hypothesis simulation was used to compute and plot team  Z -scores of AUPR curve, balance accuracy (BAC) and PCC for SC1 ( A ), SC2 ( B ) and SC3 ( C ).  Z -scores are used to compare the apparent difficulty of each of the sub-challenges. Panels ( C–G ) reflect actual performance differences—as measured by overall rank of three metrics—for different methodological approaches. Teams’ rank distributions are plotted separately by the type of approach for SC1 (D), SC2 (E) and SC3 (F). (G) In SC2, teams’ rank distribution is separated by usage of solely protein phosphorylation data or in combination with gene expression data. SVM: support vector machines, Trees: random forest and other tree-based methods, NN: neural networks, GA: genetic algorithm The diversity of algorithms that participants deployed when solving the STC and broad rank distribution of similar approaches indicates these results were independent of the method used. Indeed, 7 teams used support vector machines (SVM), 14 teams used regression-based methods, 8 teams used decision trees or random forest, 4 teams used neural networks and 3 teams used a Bayesian approach. When the teams’ rank distribution was separated by the type of approach used for each sub-challenge, no clear tendency arose as the rankings of similar methods varied widely ( Fig. 2 D–F). Teams tried different combinations of feature selection approaches and classification algorithms. Although the sub-challenges shared similarities and a single team was best performer, no single combination of methods was universally advantageous across all sub-challenges. Consider that for SC2, 8 of 13 participants did not use GEx to infer phosphorylation activation in human and restricted their analysis to rat protein phosphorylation data. This seemed to be advantageous, as 5 of the 6 top-ranked submissions did not use gene expression, but no statistically significant difference was found between those who did and did not use GEx ( P -value = 0.35,  Fig. 2 G). Nevertheless, there were some promising approaches arising from the STC. Neural network approaches ranked 1 and 2 for SC2, and it would have ranked 1 in SC3 had the class labels been reversed. The analysis of methods also suggested that a promising combination for the task of feature selection and classification is to select a subset of genes and use Linear Discriminant Analysis, an approach taken by half of the top 3 performing methods used for SC1 and SC2. 3.2 Analysis of stimulus prediction through gene sets and phosphoproteins To assess how the accuracy of the participants’ predictions depended on the nature of the stimulus applied, we defined the species similarity  S  and the predictability or teams’ prediction performance  Pr . Briefly,  S  is the MCC between rat and human GS, and  Pr  is the MCC between a team’s submission and the human GS. A high  S  value would indicate a putatively conserved response between rat and human; a high  Pr  suggests the signal is well translated by participants.  S  and  Pr  could be defined for stimuli based on gene set or protein phosphorylation activation.  S  and  Pr  could also be defined for gene set and phosphorylation activation based on response to stimuli (see methods for details).  Figure 3  shows the mean  Pr s  for all participants plotted against  S s  based on the activation of gene sets ( Fig. 3 A) and protein phosphorylation ( Fig. 3 B).
 Fig. 3. Predictability versus species similarity for stimuli. ( A ) The y-axis indicates for each stimulus the mean predictability  Pr s  of all team predictions when considering gene set activation in SC3. The x-axis is species similarity  S s  of gene set activation. In red are stimuli where  Pr s  &gt;  S s  &gt; 0. ( B ) The y-axis indicates for each stimulus the mean predictability  Pr s  of all team predictions when considering protein phosphorylation activation in SC2. The x-axis is  S p  of phosphoprotein activation. In red are stimuli where  Pr s  &gt;  S s  &gt; 0. ( C ,  D ) Plots showing the percentage of teams where  Pr s  &gt;  S s  for each stimulus when predicting gene set activation (C) or phosphoprotein activation (D). Stimuli are ordered by percentage of teams and the number of activated gene sets or phosphorylated proteins is indicated on top of each stimulus. The number of active calls per gene set is shown on the top of the graph. Nineteen stimuli are not shown in (B) and (D) because no proteins were measured as phosphorylated Based on both gene set and phosphoprotein activation, clomipramine and IL1B were better predicted than expected by  S s  ( Pr s  &gt;  S s  &gt; 0). In addition, formaldehyde, taurocholic acid, cisapride and activation, and insulin were better predicted based on protein phosphorylation. The correlation between  Pr s  and  S s  was higher for protein phosphorylation activation (PCC = 0.6,  P -value &lt; 0.013) than for gene set activation (PCC = 0.326,  P -value &lt; 0.051), perhaps reflecting not only a higher predictability for the protein phosphorylation data but also its smaller prediction space. Overall a higher percentage of teams performed better than  S s  when predicting gene set activation in response to stimuli versus predicting phosphorylation status.  Figure 3  shows that in 12 stimuli at least 50% of teams achieved a  Pr s  &gt;  S s  when predicting gene set activation ( Fig. 3 C), but only in one stimulus, HBEGF, when predicting phosphorylation status ( Fig. 3 D). The individual team values  Pr s  for protein phosphorylation and gene set activation are displayed in  Supplementary Figure S4 , and it shows that the translation of epigallocatechin and dimethyloxalyglycine was particularly difficult for both data types. Finally although aggregating the results of all teams did not yield a better overall prediction of stimuli effects when predicting protein phosphorylation, the aggregate of the five best teams performed better than individual predictions for insulin, clomipramine, IL1B, dimethyloxalyglycine, NaCl and epigallocatechin ( Supplementary Fig. S4B ). 3.3 Analysis of pathway predictions through gene sets and phosphoproteins We also set out to assess the accuracy of the participants’ predictions regarding different biological pathways and to test which gene expression regulatory processes (biological pathways/functions) were translatable and therefore predictable across species. To do so, we defined the species similarity for protein and similar measures of predictability, or teams’ prediction performance,  Pr p  and  Pr g  (see methods). Figure 4 A and B show the mean  Pr p  and  Pr g  for all participants plotted against  S p  and  S g ,  respectively, based on activation in stimuli. A total of 49 of 246 gene sets were predicted better than expected by  S g  ( Pr g  &gt;  S g  &gt; 0,  Fig. 4 A). Prediction performance per phosphoprotein  Pr p  showed a ribosomal protein S6 kinase (KS6A1) and mitogen-activated protein kinases (MK09 and MP2K6) were predicted better than expected by  S p  ( Fig. 4 B). Although aggregating all teams’ results did not yield a better overall prediction for protein phosphorylation activity, the aggregate of the five best teams performed better than individual predictions ( Supplementary Fig. S5B ). The high correlation between  Pr p  and  S p  (PCC = 0.71,  P -value &lt; 0.0087) reveals that most of the pathways defined by the protein phosphorylation activation were predicted with an accuracy expected by species similarity. We observed a similar situation for gene set activation prediction, with a lower but still significant correlation (PCC = 0.38,  P -value &lt; 1e-6). These results again suggested a slightly higher predictability in the protein phosphorylation data, though the prediction space was smaller. The individual team values for  Pr p  and found that participants’ predictions were well translated for 71 of 176 active gene sets and for 8 of 16 phosphorylated proteins ( Fig. 4 A and B). Overall a higher percentage of teams performed better than species similarity when predicting protein phosphorylation activation (55%) versus predicting gene set activation (41%; see  Fig. 4 C and D). Nevertheless, when looking specifically at the set of active gene set and stimulus pairs (n = 560), 30% were correctly predicted by at least three teams ( Fig. 5 A), and in contrast to phosphorylation activation, six of seven teams in SC3 were better at globally translating the effects of stimuli than gene set activity ( Fig. 5 B).
 Fig. 4. Predictability versus species similarity for gene sets and phosphoproteins. ( A ) The y-axis indicates for each gene set the mean  Pr g  of all team predictions when considering response to 26 stimuli in SC3. The x-axis is  S g  of gene set activation. In red are stimuli where  Pr g  &gt;  S g  &gt; 0. ( B ) The y-axis indicates for each protein the mean  Pr p  of all team predictions when considering response to 26 stimuli in SC2. The x-axis is  S p  for phosphoprotein activation. ( C  and  D ) Plots showing the percentage of teams where  Pr g  &gt;  S g  (C) and  Pr p  &gt;  S p  gene sets and phosphoproteins are ordered by number of active calls, indicated on top of each black dot 
 Fig. 5. Best translated gene sets representative of different pathways. ( A ) Histogram of the percentage of active gene set/stimulus pairs [560 pairs from 6396 (246 gene sets × 26 stimuli)] correctly predicted by N teams. Blue line represents the cumulative of the histogram values. ( B ) Distribution of teams’  Pr g  (blue) and  Pr s  (red) values. ( C  and  D ) Best predicted gene sets as measured by  Pr g . (C) Barplot of 25 gene sets having a  Pr g  Z-score ≥ 1.9. Blue star indicates a  S g  Z-score ≥ 1.5. All gene sets are originally derived from Reactome unless otherwise indicated, according to MSigDB. (D) Hierarchical clustering of gene sets and genes that are present in at least 4 of the top 25 best predicted gene sets. Each cell is valued according to gene set membership and frequency the gene is found as part of that gene set’s GSEA CORE enrichment set. Gene/gene set pairs are assigned a 0 if the gene is not a member, 1 if only a member or 1 +  C , where  C  is the number of stimuli under which the gene is found to be part of the CORE enrichment. Cells have a theoretical maximum value of 27. Cells are represented by a blue scale ranging from dark blue for 0 to white for the maximum value reached, here 7. Significantly overrepresented genes among these gene sets are labeled red ( P -value &lt; 0.01) or yellow ( P- value &lt; 0.05) The 25 best-predicted gene sets showed some concordance in the biological processes they represent; in particular, translation and protein folding, apoptosis, metabolism, immune response (TCR, cytokine), growth signaling pathways (insulin, NGF, MET, TGFB), kinase signaling (ERK, PI3K), cell cytoskeleton and adhesion [extracellular matrix (ECM), integrin, actin, L1CAM] were well predicted ( Fig. 5 C). It was possible that specific genes were especially important for reaching high levels of predictability. To identify such biological drivers, the gene membership of the top 25 best-predicted gene sets (Z-score ≥ 1.9) was reviewed to identify genes that were consistently present. Moreover, from GSEA, genes identified as part of the  CORE enrichment  may be considered as the most biologically relevant as they contributed significantly to the enrichment score and were part of the ‘leading edge’ subset ( Subramanian  et al. , 2005 ).  Figure 5 D reflects a hierarchical clustering of genes that were present in at least 4 of the top 25 best-predicted gene sets (49 genes among 19 gene sets), as well as the frequency they were found as part of the CORE enrichment for that gene set. The TF CREB1, the elongation factor eIF4EBP1 and kinases like AKT1, PIK3, PDPK1 and MAPK3 were in many of these gene sets and were also part of the CORE enrichment set for those gene sets, though their presence was not statistically significant. Notably, CREB1 and AKT1 phosphorylation activity was also well predicted by participants in SC2 ( Fig. 4 B). Yet, MAPK3 activity was not, showing some but not total coherence between the drivers of predictability in the two different data types, gene set and protein phosphorylation activation. Finally we performed a similar analysis looking for the most biologically relevant genes when considering the gene sets that were better predicted than expected by species similarity ( Supplementary Fig. S6 ). Unexpectedly, we found that nuclear pore genes and replication factors were significantly enriched, as was a  paxilin  gene related to the FAK1 kinase, whose phosphorylation status was not very well translated by the participants ( Fig. 4 B). 4 DISCUSSION We organized the STC as part of the sbv IMPROVER initiative and provided participants with experimental data describing multiple layers of different signaling pathways. The goal was to assess the ability of computational methods to predict biological responses in primary NHBE cells based on responses observed in primary NRBE cells. Several of our observations support the conclusion that changes in phosphorylation status and gene set activation induced by cellular response to 52 different perturbations in human cells can be predicted to some extent given responses generated in rat cells. Overall, stimuli caused more activation in rat than in human cells for most gene sets and for all phosphoproteins, except for phosphoproteins KS6A1 and HSPB1 ( Supplementary Fig. S7 ). The differences in stimulus-induced activity could be due to a more homogenous biological sample in rats than in humans, or simply to higher sensitivity and faster signaling of NRBE cells compared with NHBE cells. Interestingly, differences between the kinetics of activation of homolog phosphoproteins at the 5 and 25 min time points were minimal [14 pairs for the whole dataset; see  Figure 3  in ( Biehl  et al. , 2014 ) in this issue]. Although not statistically significant, the average performance over all participants tended to be higher in SC2 than in SC3. This was seen in the higher  Pr  values when predicting stimuli activity across all phosphoproteins or gene sets and also when predicting the challenges’ respective signaling layer, gene set activation or phosphorylation responses across all stimuli. This observation holds when considering the performance of the best performer in both sub-challenges, where Team AMG’s prediction  Pr  values were higher for SC2 versus SC3. When we considered cases where the majority of participants performed better than species similarity, i.e. a naïve, direct translation, 12 stimuli and 71 (of 176) active gene sets were well predicted in SC3, and 8 phosphoproteins and 1 stimulus in SC2 ( Figs 3 C and D and  4 C and D). The greater number of well-predicted stimuli by gene set activation may have to do with the relatively lower levels of similarity  S g  as compared with  S p , although, importantly teams were able to find informative biological signal in spite of these lower levels of response conservation. Finally, 10 teams in SC2 and 5 teams in SC3 submitted predictions that were statistically significantly different from random in two of the three metrics used ( Supplementary Tables S1–S3 ). Overall for SC1-3, about two-thirds of the submissions were statistically significant (26 of 41), this indicates that in SC1, GEx data were sufficiently informative to infer upstream phosphorylation responses and that overall across-species predictions were achievable for specific stimuli, phosphoproteins and gene sets. While many teams achieved a statistically significant result when considering random submissions as a null hypothesis, most teams in SC2 found their methods were unable to outperform a completely naïve approach to the challenge. If a team had submitted the rat’s subset B protein phosphorylation status in SC2 and the gene set activations in SC3 as their predictions, they would have ranked second and fifth, respectively. However, best-performer teams used approaches that did significantly better than the naïve approach, suggesting that their computational methods could capture additional informative biological signal in rat data [see in this issue ( Biehl  et al. , 2014 ), Hafemeister  et al.  and Hormoz  et al. ]. Interestingly, there was also not a statistically significant difference between the five teams that used both P and GEx data and all others. The SC2 second place team, Team IGB, went further to test multiple variations of their Neural Network method to include GEx data and found it fared worse than methods using phosphorylation data alone [see ( Biehl  et al. , 2014 ) in this issue]. This may be owed to the smaller difference in relative standard deviations (RSD) between human and rat phosphorylation response data versus GEx data ( Supplementary Fig. S8 ). The similarity in phosphorylation response across species may have been difficult to detect due to the low number of replicate samples—only 3. These observations suggesting that it was relatively easier to predict a response of the phosphoproteomic layer are also reflected in the median Z-scores of the challenge metrics, where SC2  Z -scores were higher than SC3 for AUPR and PCC ( Fig. 2 A–C). These results likely reflect both the larger universe of predictions for SC3, which were an order of magnitude greater than for SC2 (246 × 26 = 6396 gene sets/stimuli pairs versus 16 × 26 = 416 phosphoproteins/stimuli pairs) and also a higher conservation between rat and human for protein phosphorylation activation ( S p  = 0.71) compared with the gene set activation ( S g  = 0.38). It is also possible that the higher similarity in phosphorylation response and smaller difference in the RSD between human and rat in the protein phosphorylation data with respect to the GEx data enabled more accurate predictions ( Supplementary Fig. S8 ). This is likely due to the greater heterogeneity of human samples coming from two different donors compared with rat samples coming from an inbred laboratory strain but also given the increased complexity of human signaling. Transcriptional responses are a relatively downstream event from phosphorylation in signaling cascades. Hence, it is possible that when the signal finally propagates to the transcriptional layer, many other species-specific factors may amplify the differences of response between both species. An alternative explanation could be related to platform-specific biases due to inherent differences in mRNAs. However, we paid special attention to experimental design and execution as well as sample and data processing to minimize, as much as possible, experimental biases and avoid confounding effects within and between species ( Poussin  et al. , 2014 ). Additionally, phosphorylation response predictions benefited from more targeted experiments, which looked at 16 phosphoproteins enriched for active signals ( Poussin  et al. , 2014 ) while GEx data was genome-wide and would only be expected to have a small percentage of genes differentially expressed. An interesting follow-up experiment would be to look at targeted gene expression for the pathway components using more specific measurements, like qRT-PCR or deep-sequencing. More sampled time points for GEx could provide greater granularity and may reveal patterns difficult to observe with a single 6 h exposure time point, and also confirm whether this choice left transcriptional responses to particular stimuli undetected. The STC’s results indicate that GEx data were potentially noisier and the numbers of active gene set–stimulus pairs were low, however participants in SC3 made better predictions per stimulus than per gene sets ( Fig. 5 B). This shows that given sufficiently large datasets, it was still possible to extract a biologically relevant signal. Stimuli such as the chemical formaldehyde, cholesterol-derived taurocholic acid and the serotonin 5-HT 4  receptor agonist cisapride were predicted better than the conservation of the response between both species. Similarly, the hydroxylase inhibitor dimethyloxalyglycine, the cytokine CCL3, the TLR activator PolyIC ( Fortier  et al. , 2004 ) and the nerve growth factor NT3 were effectively translated and predictable at a level comparable with the observed conservation of response  S  ( Fig. 3 A) when based only on activation of protein phosphorylation. When we considered the overall prediction performance for phosphorylation response and gene set activation by all teams in SC2 and SC3, the antidepressant clomipramine and cytokine IL1B were better predicted than the levels of response conservation. This observation indicates that teams were able to identify human-specific signals that were not significantly present in the rat data. We also found that predictions based on the two data-generating platforms were not always in agreement, as in the case of insulin, which was well predicted based on protein phosphorylation but not based on gene set activation, and vice versa for NaCl ( Fig. 3 A and B). Regarding the inference of biological processes, the gene sets predicted better than species similarity and/or best-predicted were related to DNA synthesis, cytoskeleton and ECM, translation, immune/inflammation and growth factor/proliferation ( Fig. 5  and  Supplementary Fig. S6 ). The gene sets associated with growth factor/proliferation processes shared a subset of CORE driver genes belonging to the following signaling pathways PI3K, RAS, RAF, MAPK, ERK and CREB. When considering only gene sets that were among the top 25 best predicted and had at least 1 member shared with at least 3 other best predicted gene sets, both CREB1 and AKT1 genes were present in, respectively, 4 and 6 of 19 gene sets ( Fig. 5 D). To note, the empirical  P -values calculated for their presence across gene sets was not significant, indicating that these genes are frequently present in gene sets (see  Fig. 5 D). This may be due to a central role of these genes in many biological functions leading to a broad representation of those genes through C2CP gene sets. At the protein level, the activity of CREB1, AKT1 and MAP kinases such as MAPK9 and MP2K6 was well translated based on protein phosphorylation activation, showing a consistency in similar pathway perturbation prediction at different layers of the cellular system. It is interesting to note that the species similarity of MAPK9 and MP2K6 activation profile across stimuli was low, whereas the activation of both proteins was well predicted. The best predicted protein activity was that of KS6B (p70S6K). This protein is activated upstream by PI3K/AKT/mTOR protein kinases and regulates downstream phosphorylation of p70S6 protein, which is directly involved in the translation process found to be among the top best-predicted functions at the gene expression level. Interestingly, EIF4EBP1 and EIF4G1 factors involved in rate-limiting steps during the initiation phase of protein synthesis ( Franke, 2008 ) were identified as CORE driver genes of translation-related gene sets, and the  P -value associated to their presence across best-predicted gene sets was significant, indicating that those genes were rarely shared with other genes sets and therefore were specific to this biological function. Importantly, EIF4G1 gene also belonged to CORE genes contributing to the enrichment of translation-related gene sets in rat cells, suggesting a conserved role for this gene between human and rat species. Other proteins, such as TF65 and IKBA, associated to the NFkB signaling pathway showed good predictability in their activity in human cells. This result was coherent with the observation of good predictability for immune/inflammatory-related gene sets. Except for some proteins such as MAPK9 and MP2K6 as mentioned above, many proteins of our measured panel have a level of species similarity that positively correlated with the level of predictability. This suggests that conserved responses at the protein levels possibly drive the translatability between both species in this cellular context. Agreement in the biological processes that are similar between rat and human extends to the results of SC4, for the insulin, IL1R, MAPK, CREB1 and NFKB pathways (see  Figs 2  and  5  in Bilal  et al.  in this issue). Participants of SC4 also found that regulation of  RPS6KA1 , active in human, and  WNK1 , in rat, differed between the two species (see Bilal  et al.  in this issue). Finally, we observed that gene sets related to metabolism, which were generally expected to be conserved between species—such as insulin secretion, the CREB pathway, amino acid and fatty acid metabolism—were indeed well translated, although oxidative phosphorylation and gluconeogenesis were less well translated than expected by conservation (see  Supplementary Table S6 ). The 49 different submissions to the STC used a diverse set of approaches including SVM, regression-based methods, tree-based methods like random forest, neural networks, Bayesian analyses and a genetic algorithm. The diversity of approaches might explain why the aggregate of all participant results performed better for two of three metrics in SC1 and SC2 ( Supplementary Fig. S3 ). A lower participation level in SC3 may also explain why aggregation of all seven teams’ predictions did not perform better than the best performers. Interestingly, similar computational methods could have a wide range of performance within the same challenge, and no single method emerged as the clear winner ( Fig. 2 ). Yet, methods that selected a subset of genes and used Linear Discriminant Analysis ranked among the top 3 performing approaches for SC1 and SC2. For SC2, one would have naively expected that using more data would benefit a prediction, and while not statistically significant, participants that used only protein phosphorylation data tended to rank higher than participants using protein phosphorylation data in conjunction with gene expression data ( Fig. 2 G). A notable challenge in the STC was the imbalance in data, as only ∼ 10% of stimuli/phosphoprotein and stimuli/gene set pairs were active. Such a strong bias toward the inactive class complicates the training of models, although usage of ensemble methods that repeatedly sample the data, either over- or under-sampling, to converge on stable predictions could help overcome this imbalance. Generally, teams did not explicitly compensate for these imbalances, though methods like random forest inherently addressed such concerns. Yet as seen in the challenge results, random forests were not uniformly superior to all other methods and so there is potential for improved approaches that more explicitly account for class imbalance. For their predictions, participants exclusively used data-driven approaches, and no computational method included  a priori  biological knowledge, as would be the case for topological approaches ( Anvar  et al. , 2011 ;  Melas  et al. , 2011 ). This rendered the interpretation of results with respect to biology more difficult. The construction and usage of  a priori  knowledge is characteristic of topology-based approaches. For example, Melas  et al.  used such approaches with data similar to the STCs to reconstruct pathways from input stimuli to the output cytokine release with phosphoprotein levels as intermediate signals. The initial construction of canonical pathways was based on gathering information from different databases (e.g. KEGG, Biocarta, etc.) combined with manual curation from the literature (e.g. reviews). Later, the Boolean networks were refined with a data-driven method using multi-linear regression on the phosphoprotein and cytokine data. Alternative methods, beyond traditional machine learning approaches that assume training and test sets from the same dataset/domain, would be necessary to generalize predictions and enhance biological conclusions. Transfer learning and domain adaptation would be worth examining specially in problems that aim to integrate multiple layers of information—such as distances between stimuli based on the similarity of their chemical structure or their distance similarity in a protein interaction/transcriptional response network ( Blitzer, 2006 ;  Iorio  et al. , 2010 ;  Napolitano  et al. , 2013 ;  Pan  et al. , 2011 ). The current work constitutes a proof of principle that predictability of responses in an  in vitro  system from one species is feasible to some extent given responses from another species. The results of this challenge provide insights on the predictability/accuracy in the context of diverse data types generated at various layers of the biological system studied; on the importance of time resolution to gain in prediction accuracy for species-specific sequential molecular events; on the different performance of similar computational methods due to variations in data preprocessing, feature selection and the classification algorithm (Tarca  et al. ); and on the different degrees of predictability of pathways/processes depending on the stimulus-induced perturbation in human and rat bronchial epithelial cells. Processes such as DNA synthesis, cytoskeleton and ECM, translation, immune/inflammation and growth factor/proliferation were better translated. It will be important to test whether results and methods discussed here can be extended to more complex systems such as tissue, organ and whole organisms, the ultimate objective of translation between species. A better understanding of the range of applicability of the translation concept will impact the predictability of signaling responses, mode of action and efficacy of drugs in the field of systems pharmacology as well as increase the confidence in the estimation of human risk from rodent data for toxicological risk assessment. 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>MitoTrack, a user-friendly semi-automatic software for lineage tracking in living embryos</Title>
    <Doi>10.1093/bioinformatics/btz717</Doi>
    <Authors>Trullo A, Dufourt J, Lagha M, Berger Bonnie</Authors>
    <Abstract/>
    <Body>1 Introduction Live imaging of developing organisms provides the unprecedented opportunity to access to nuclei/cellular genealogy. The combination of lineage information and transcriptional history permits to decipher the extent of transcriptional memory, i.e. the influence of the transcriptional status of mother nuclei on that of their daughters ( Ferraro  et al. , 2016 ). Here we present MitoTrack, a user-friendly and open source software, able to automatically track nuclei and their transcriptional activity across multiple cell divisions. The tool has been primarily developed for images of living Drosophila embryos, in which labeling nascent transcription has become popular ( Garcia  et al. , 2013 ;  Lucas  et al. , 2013 ;  Pichon  et al. , 2018 ). However, MitoTrack could also be used for genealogy and memory tracking in other organisms, such as zebrafish embryos, mouse early embryos or even in organoids. 2 Software description The typical data processed with MitoTrack are images from early Drosophila embryos where nuclei are labeled with a histone-RFP transgene and reporter mRNA detected upon binding of the MCP-GFP to MS2 containing nascent mRNAs (sample data can be downloaded following this link) ( Fernandez and Lagha, 2019 ). Most of MitoTrack algorithms are not novel and several more sophisticated programs are similarly able to perform lineage tracking in living organisms ( Amat  et al. , 2014 ;  Tinevez  et al. , 2017 ;  Tran  et al. , 2018 ;  Wolff  et al. , 2018 ). However, MitoTrack was developed to fulfill the specific task of extracting lineage information and the timing of transcriptional activation directly from the raw data. All the developed algorithms work in a single package, embedded into a user-friendly graphical user interface (GUI) ( Fig. 1a ), released as open source (a tutorial with walk-through examples is provided as a  Supplementary Material ). The main objective of MitoTrack is to analyze transcriptional activation in a nucleus-by-nucleus basis, across multiple nuclear divisions to quantify the extent of transcriptional memory. The general tasks are (i) to segment and track nuclei across mitosis (keeping lineage information), (ii) to segment spots (transcription sites) and (iii) to merge these pieces of information and obtain the activation timing for each mother nucleus and its daughters. The analysis is organized into four steps: ‘before mitosis’, ‘during mitosis’, ‘after mitosis’ and ‘merging of partial analysis’.
 Fig. 1. MitoTrack GUI to track transcriptional activation and nuclei lineage. ( a ) Main window composed of four distinct panels, among which, two are shown here, with raw nuclei channel (upper left) and detected nuclei (upper right). MitoTrack typical tools and edit boxes are shown on the left. ( b ) ‘Modifier Mitosis Tool’ showing all its commands and parameters. Three typical images of nuclei at a time t, t + 1 and their overlap are shown to illustrate the basis of the algorithm for automatic nuclei tracking during mitosis. ( c ) Interface of the ‘Modify Segmentation Tool’ with its main commands visible. Exemplary results of manual corrections of nuclei segmentation 3 Features Input data are 4D stack (.czi or .lsm) with two channels, one for nuclei and the other for transcription sites (spots). When loaded, data are automatically maximum intensity projected. The ‘before mitosis’ and ‘after mitosis’ steps of the analysis work similarly: nuclei are segmented and tracked while transcriptional sites are detected and each of them is associated to the closest -if not overlapping- nucleus. Nuclei are pre-smoothed and then thresholded with the classical Otsu thresholding algorithm ( Otsu, 1979 ); the black and white resulting images are labeled and segmented using a watershed segmentation algorithm ( van der Walt  et al. , 2014 ) and finally tracked in consecutive frames following a minimum distance criterion of their centroids positions. Spots are segmented with a blob detection algorithm, namely a Laplacian of Gaussian. The resulting images are filtered and each spot is then associated to the closest nucleus. Potential segmentation errors can be corrected with a pop-up tool that allows the user to easily modify nuclei segmentation by merging or splitting nuclei ( Fig. 1c ). The ‘during mitosis’ step consists in a similar nuclei segmentation but differs in the tracking procedure, where mitotic duplication of chromosomes must be accounted for. MitoTrack follows nuclei across mitosis under an overlapping criterion, by projecting the mask of each nucleus at frame t onto the following frame. In this way, two nuclei (daughters) can be associated to a single nucleus (mother), thereby creating a lineage genealogy. A pop-up tool ( Fig. 1b ) allows the user to easily correct both segmentation and tracking during this particular step. The last part of MitoTrack consists in merging the three aforementioned sections in order to update nuclei labels for the entirety of the analyzed movie encompassing mitosis. Moreover, in this step, the software attributes sub-labels to properly distinguish the two daughters from each other. In addition, MitoTrack allows for a spatial analysis and hence records the timing of transcriptional activation within user-defined domains. All the extracted quantitative data are organized into an .xls file. 4 Results and perspectives The described program represents a robust analysis platform to quantify the timing of transcriptional activation prior and after mitosis while keeping the genealogy information. Using MitoTrack software, we can now quantify the extent of transcriptional memory in hundreds of nuclei from various genotypes, as recently performed in ( Dufourt  et al. , 2018 ). One of the main advantages of MitoTrack resides in the various tools it provides for manual correction of nuclei tracking during mitosis ( Fig. 1c ). However, mainly depending on nuclei crowding and temporal resolution, MitoTrack is able to automatically track all nuclei during mitosis with near 100% accuracy (for example in mitosis from nc12 to nc13 with a temporal resolution of 20 s/frame, or in mitosis from nc13 to nc14 with images every 4 s, as the sample data accompanying the provided tutorial). By comparing the timing of activation of neighboring nuclei, we can now directly evaluate the extent of temporal coordination [referred to as synchrony ( Lagha  et al. , 2013 )]. Thanks to MitoTrack spatial analysis tool, it is possible to measure the timing of activation in distinct spatial domains and thus quantify the impact of gradients of morphogens (for example, the Dorsal activator) on transcriptional dynamics. Funding We are grateful to CNRS and FRM for supporting this work. A.T was sponsored by the SyncDev ERC grant. This work was supported by the ERC_SynDev Grant # 679792 to M.L. and HFSP-CDA grant to M.L. 
 Conflict of Interest : none declared. Supplementary Material btz717_Supplementary_Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>scRNABatchQC: multi-samples quality control for single cell RNA-seq data</Title>
    <Doi>10.1093/bioinformatics/btz601</Doi>
    <Authors>Liu Qi, Sheng Quanhu, Ping Jie, Ramirez Marisol Adelina, Lau Ken S, Coffey Robert J, Shyr Yu, Robinson Mark</Authors>
    <Abstract/>
    <Body>1 Introduction Single cell RNA-sequencing (scRNA-seq) is a powerful technique of whole-transcriptome profiling at the resolution of individual cells. It has been successfully used to discover rare and heterogeneous cell populations, and reconstruct developmental trajectories ( Carter  et al. , 2018 ;  Karaayvaz  et al. , 2018 ). One major challenge in scRNA-seq analysis is to detect technical artifacts and remove poor quality cells. Previous studies have employed different strategies to detect technical artifacts ( Ilicic  et al. , 2016 ;  Jiang  et al. , 2016 ;  Lun  et al. , 2016 ;  McCarthy  et al. , 2017 ;  Tian  et al. , 2018 ). They generally use features such as overall gene expression patterns, number of genes detected and housekeeping genes or spike-in RNA. For example, SinQC integrated both gene expression patterns and sample sequencing library qualities, such as total number of mapped reads, mapping rate and library complexity to detect technical artifacts ( Jiang  et al. , 2016 ). Ilicic  et al.  used biological and technical features, including number of genes detected, number of mapped reads and percentage of mitochondrial genes to train a SVM model to distinguish low from high quality cells ( Ilicic  et al. , 2016 ). The Scater and scPipe packages provided methods to compute a variety of QC metrics and visualization to diagnose potential issues ( McCarthy  et al. , 2017 ;  Tian  et al. , 2018 ). These strategies successfully identify compromised cells within a single dataset. For integrated or comparative analysis of large collections of scRNA-seq experiments, quality assessment across datasets is crucial to detect outliers, potential batch effects, or systematic biases since they will mask underlying biology and result in misleading conclusions. Here, we present scRNABatchQC, an R package designed to assess the similarity/difference across scRNA-seq datasets over numerous technical factors, biological features, expression profiles and related pathways. By comparing technical and biological metrics across datasets, scRNABatchQC enables the detection of systematic errors, batch effects or outlier samples. It will greatly improve quality control and reproducibility analysis for single-cell RNA sequencing. 2 Implementation scRNABatchQC is written in R. It is easy to implement even for users with limited programming experience. There is only one required input, gene-by-cell count matrices, which can be supplied by any delimited files or compressed files (ending gz or .bz2), or read from 10X, SingleCellExperiment or Seurat v3 object. Besides, there are optional arguments that users can specify or adjust, such as the organism, the number of highly variable genes (HVGs), the number of principle components (PCs), scale factor, etc.scRNABatchQC summarizes QC report in one html file, which includes six sections: Overview, QC summary, Technical View, Biological View, Expression Similarity and Pairwise Difference ( Fig. 1 ). The Overview gives a brief introduction of the software. QC summary provides a table listing a variety of QC metrics, such as the number of total counts/cells/genes, the cutoff for filtering cells, the number of cells removed due to low quality in each sample. Technical View presents diagnostic graphics on 11 technical features, such as the distribution of total counts and the variance explained by total counts in each sample, mean-variance trend, etc. Biological View compares the HVGs, the genes related to one specific principal component (PC-related genes), and their enriched pathways if the organism is supported by WebGestalt ( Zhang  et al. , 2005 ). Expression Similarity provides the global expression correlation across datasets, and two low-dimensional embedding of all cells, principal component analysis and t-distributed stochastic nearest-neighbor embedding. Pairwise Difference identifies differentially expressed genes between all pairs of samples and pathways associated with these genes (Methods Description in the  Supplementary File S1 ). In addition to the html file, scRNABatchQC stores the gene-by-cell count matrices and QC metadata in SingleCellExperiment objects, which ensures the output of scRNABatchQC compatible with other Bio-conductor workflows.
 Fig. 1. The outline of scRNABatchQC  3 Application For demonstration, we used scRNA-seq data of mouse retinal bipolar cells, which includes a total of 44 994 cells in 6 replicates prepared from 2 experimental batches ( Shekhar  et al. , 2016 ). Batch 1 has 4 replicates (S1–S4), while batch 2 consists of 2 replicates (S5 and S6).  Supplementary File S2  is the output generated by scRNABatchQC. The report summarizes that batch 1 has ∼4500 cells/replicate, and batch 2 has ∼13 000 cells/replicate. Compared to batch 1, batch 2 has more cells with high percentage of mitochondrial RNA genes (the maximum percentage is 70%∼80%), which were removed from downstream analysis due to low quality ( Supplementary File S2 : QC Summary). Batches 1 and 2 present different distributions on all 11 technical features ( Supplementary File S2 :  Figs S1–11 ), suggesting the existence of batch effects. For example, batch 2 shows a more rapid increase in the expression cumulative plot, suggesting a library with lower complexity ( Supplementary File S2 :  Fig. S5 ). Biological View illustrates that six samples are very similar in their HVGs, all enriched for processes related to photo-transduction. These results suggest that HVGs mainly capture the biological variations across cell types, and six samples share similar biological heterogeneity and cellular compositions. Although being very similar, samples are still clustered by their batch, indicating that batch effects have some minor effect on variations within samples ( Supplementary File S2 :  Figs S12 and 13 ). Pairwise Difference identifies differentially expressed genes between two batches, such as Xist, Hopx, mt-Rnr1 and mt-Rnr2 ( Supplementary File S2 :  Fig. S19 ). No enriched pathways are detected for these differential genes, therefore  Supplementary Figure S20  is not generated. Xist, Hopx are sex-related genes, while mt-Rnr1 and mt-Rnr2 are mitochondrial RNA genes. These results suggest that differences in the sex populations and library preparation between the two batches are likely to contribute to batch effects, which is consistent with the original paper ( Shekhar  et al. , 2016 ). 4 Conclusion Large-scale projects, such as Human Cell Atlas, are now generating comprehensive collections of scRNA-seq datasets. Understanding the existence and the sources of experimental noise is very important to integration and interpretation of the data. scRNABatchQC, a quality assessment tool over numerous technical and biological features, not only provides a global overview of all experiments, but also enables the examination of technical or biological origin of discrepancies between experiments and detect possible outliers and batch effects. scRNABatchQC can also be applied to other single cell experiments, such as single nuclei RNA-seq. Funding This work was supported by the National Cancer Institute grants (U2C CA233291 and U54 CA217450). 
 Conflict of Interest : none declared. Supplementary Material btz601_Supplementary_Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Batch-normalization of cerebellar and medulloblastoma gene expression datasets utilizing empirically defined negative control genes</Title>
    <Doi>10.1093/bioinformatics/btz066</Doi>
    <Authors>Weishaupt Holger, Johansson Patrik, Sundström Anders, Lubovac-Pilav Zelmina, Olsson Björn, Nelander Sven, Swartling Fredrik J, Kelso Janet</Authors>
    <Abstract/>
    <Body>1 Introduction Medulloblastoma (MB) is a brain tumor arising in the cerebellar vermis predominantly in pediatric patients. It is currently treated by surgery, radiotherapy and chemotherapy, achieving 5-year overall survival rates of about 70%. However, survivors often suffer from permanent neurocognitive sequelae. It is now commonly accepted that MB harbors four distinct molecular subgroups, referred to as Wingless/Integrated (WNT), Sonic hedgehog (SHH), Group 3 (G3) and Group 4 (G4) ( Taylor  et al. , 2012 ). These subgroups have been shown to exhibit distinctive molecular landscapes ( Northcott  et al. , 2017 ) and are associated with different clinical risk groups ( Kool  et al. , 2012 ). As a consequence, personalized therapy of MB patients appears now almost within reach ( Gopalakrishnan  et al. , 2015 ;  Sengupta  et al. , 2017 ). However, to fully endorse such treatment options, more research is required to understand the origin and development of MB and how model organisms for drug testing can be derived. The establishment of such knowledge is heavily dependent on molecular profiling data. For instance, the possibility of further delineating MB subgroups into subsets has been hypothesized earlier ( Taylor  et al. , 2012 ), but it was not until larger cohorts had been gathered that such subsets have been reported in detail ( Schwalbe  et al . 2017 ;  Cavalli  et al. , 2017 ;  Northcott  et al. , 2017 ). Furthermore, while the discovery of genomic events and potential driver genes has also been greatly advanced ( Cavalli  et al. , 2017 ;  Northcott  et al. , 2017 ), it is yet to be fully revealed how these genes are linked to the phenotype at a system wide level. Related investigations would likely require more integrative methods such as molecular networks ( Barabási  et al ., 2011 ;  Vidal  et al. , 2011 ), the reverse engineering of which in turn is also heavily dependent on large-scale datasets. Gene expression is one of the most profiled types of high-throuput data in the MB field. However, the establishment of large MB patient cohorts has been hampered by the low incidence rate of only ∼1.8 new cases per year per million people ( Louis  et al. , 2016 ). Consequently, a multitude of transcription datasets of varying compositions and numbers of samples has been released to the public. Additionally, while healthy controls play a crucial role in many types of analyses, publicly available MB transcription datasets typically lack any normal cerebellar samples. Yet, several independent transcription datasets with cerebellar samples exist in the literature. Thus, while the data generated by the community reflects an unprecedented collection of MB and cerebellar transcriptional profiles, it is not clear how such a resource can be fully exploited, considering that it is distributed across different studies and platforms. While meta-analyses represent a possible avenue to deal with scattered datasets ( Kool  et al. , 2012 ;  Morgan  et al. , 2014 ), a more generic use for such data would instead require them to be merged into one integrated resource, a task that is greatly hampered by batch effects, i.e. study related systematic biases caused for instance by variations in sample handling or profiling platforms ( Lazar  et al. , 2013 ). To combine transcription data from different batches, several MB related studies have made use of methods to remove or minimize batch effects ( Margol  et al. , 2015 ;  Natarajan  et al. , 2012 ;  Northcott  et al. , 2014 ;  Pöschl  et al. , 2014 ). However, these efforts were typically conducted on only a few batches each or lacked a thoroughly documented evaluation of batch effects and their removal. In this study we report the first effort of establishing a large scale resource of MB and cerebellar gene expression data through merging of a majority of the related publicly available datasets and careful inspection and removal of batch effects using the Removal of Unwanted Variation (RUV) algorithm ( Gagnon-Bartsch and Speed, 2012 ;  Jacob  et al. , 2016 ). Numerous other tools for the removal of batch-effect have been proposed (e.g.  Giordan, 2014 ;  Heider and Alt, 2013 ;  Huang  et al. , 2012 ;  Johnson  et al. , 2007 ;  Leek  et al. , 2012 ). Among those, ComBat ( Johnson  et al. , 2007 ) and sva ( Leek  et al. , 2012 ) represent two of the most widely used approaches for batch correcting. Specifically, in a comparison of different batch-effect removal algorithms, not including RUV, ComBat has been demonstrated to be generally preferrable ( Chen  et al. , 2011 ). However, algorithms that utilize batches as covariates, such as ComBat and sva, might face problems in distinguishing between batch effects and biological differences, or might artificially increase differences between phenotypes ( Nygaard  et al. , 2016 ;  Parker  et al. , 2014 ). In the present study such difficulties can be expected to be particularly distinct, since tumor samples and normal controls are largely separated into individual batches. Rather than relying on batch covariates, the RUV method ( Gagnon-Bartsch and Speed, 2012 ) can correct for batch effects via Negative Control Genes (NCGs). While likely more suitable for the batch-normalization of the outlined datasets, this technique requires additional knowledge about NCGs, which are expected to exhibit almost constant expression between any of the investigated conditions. House keeping genes were suggested as one potential source of controls ( Gagnon-Bartsch and Speed, 2012 ). However, given that such genes are typically identified as genes with high expression across adult tissues under normal conditions ( Eisenberg and Levanon, 2013 ), they are not generically applicable to MB. Here, we empirically estimated NCGs from the available MB and cerebellar transcription datasets and we show that the selection of NCGs has a substantial impact on the batch effect removal. Utilizing the RUV method in conjunction with such controls and a thorough strategy for evaluating batch-removal performance, we were able to merge 1641 MB and cerebellar gene expression samples, identify the dominant sources of batch effects and produce a batch-corrected dataset. 2 Implementation and Results Detailed methods are described in the  Supplementary Methods . 2.1 Collection of gene expression datasets Following an extensive screening of the literature, 1796 MB and normal brain (cerebellar) transcription samples were selected, which were distributed across four platforms ( Fig. 1 A) and 23 datasets ( Fig. 1 B). For MB we considered only primary tumor samples, while for the normal controls only samples clearly annotated as cerebellum, cerebellar cortex or upper rhombic lip were included. Fig. 1. Collection of MB and cerebellar gene expression datasets. ( A ) Numbers of samples per platform initially selected for merging. ( B ) Distribution of unique MB and cerebellar patients and duplicate samples across the included studies. The numbers of samples included from each study are shown on top of each bar. ( C ) The final numbers of unique MB samples per subgroup. ( D ) Distributions of phenotypes with available information over four age groups based on age in years ( y ): embryonic ( y     ≤     0 ), infants ( 0 &lt; y     ≤     3 ), children ( 3 &lt; y &lt; 18 ) and adults ( 18     ≤     y ). The number of samples in each respective category is stated on top of each bar Accounting for duplicate samples of the same patient in any study ( Fig. 1 B), the final collection comprised a total of 1641 putatively unique patients ( Supplementary Table S1 ), including 1213 MB cases with available subgroup affiliations and 137 MB cases without ( Fig. 1 C), and 291 normal brain samples. Patient ages were available for a large portion of the samples and showed the expected distributions for the MB cases ( Fig. 1 D) ( Kool  et al. , 2012 ). The age distribution of normal brain samples was not perfectly matched to the MB age range, with many samples from patients in late adulthood, but the datasets also included a number of embryonic and pediatric samples ( Fig. 1 D). Upon merging of the datasets (see  Supplementary Methods ), a single gene expression dataset was obtained, which spanned 14 883 unique genes. 2.2 Subgroup classification of MB samples Given the number of datasets with diverse sources and the number of samples with lacking subgroup affiliations in the merged data, a classification procedure was implemented in order to investigate supplied class labels and if possible assign class labels to yet unclassified samples. Specifically, two different classifiers were established, using either the  Prediction Analysis for Microarrays  (PAM) method available through the R package  pamr  or an Elastic Net classifier implemented in the  glmnet  package. The former classifier was applied to a set of 100 genes comprising 25 signature genes for each MB subgroup, which were empirically derived through differential gene expression analyses ( Supplementary Fig. S1 A,  Supplementary Table S2 ). The latter classifier was applied on all genes, enabling the method to independently derive the classification coefficients ( Supplementary Table S3 ). Upon application to all 1213 samples with supplied MB subgroup affiliation, using leave-one-out classifiers, the PAM classifier correctly classified 1172 samples ( ∼ 0.966 % ), while the ElasticNet classifier performed slightly better with 1183 ( ∼ 0.975 % ) correctly predicted samples ( Fig. 2 A–B). By considering an existing class label to be reliable, if it was reproduced by both classifiers, a total of 1158 samples were classified correctly, while 55 class labels could not be reproduced and the corresponding samples were relabeled as having an unknown subgroup affiliation ( Fig. 2 C;  Supplementary Fig. S2 A). The percentage of correctly predicted class labels by the PAM classifier was highly robust over a large range of numbers of signature genes chosen per subgroup ( Supplementary Fig. S1 B). Indeed, as few as four signature genes appeared sufficient to correctly classify over 85% of the samples in every subgroup ( Supplementary Fig. S1 B), which is in line with a previous classification effort that reported a classifier comprising a total of 22 genes ( Northcott  et al. , 2012a ). Fig. 2. Reclassification of MB samples. ( A–B ) Confusion matrices depicting the number of correctly and incorrectly predicted class labels by the PAM classifier (A) and the ElasticNet classifier (B). ( C ) Results of the re-classification of MB samples with supplied subgroup affiliation. The heatmap shows the expression of 100 MB signature genes. ( D ) Heatmap showing the classification results for samples with previously unknown MB subtype affiliation. ( E ) Pie chart showing the number of MB subgroup cases after reclassification After removing the MB subgroup label from samples with unreproducible subgroup affiliation, new PAM and ElasticNet classifiers were trained on the MB samples with retained subgroup labels and applied to the 137 samples (∼10% of all MB samples), for which no subgroup label was originally supplied. 128 of these samples received matching class predictions from both classifiers and were labeled with the corresponding subgroup, while 9 samples could not be robustly classified and were retained without subgroup label ( Fig. 2 D;  Supplementary Fig. S2 B). As a result of the relabeling of provided subgroup affiliations and classification of unlabeled samples, the dataset finally comprised a total of 1286 (∼95%) MB samples with subgroup labels and 64 (∼5%) MB samples without ( Fig. 2 E). Thus, by collecting and comparing the individual datasets, it was possible to re-evaluate existing subgroup affiliations, and the outlined classification strategy was highly successful in predicting robust class labels for samples without available subgroup affiliations. The majority of incorrect classifications affected G3 and G4 samples, for which some degree of intermixing was expected ( Supplementary Fig. S2 A–B). In addition, the expression profiles of samples that could not be robustly classified appeared in many cases to correlate with multiple subgroups or lack a high correlation with any particular subgroup, and in some cases also showed an artificially high correlation with normal cerebellar samples ( Supplementary Fig. S2 C). Many of such samples were interpreted as borderline cases falling in between two or more subgroups ( Supplementary Fig. S2 D and E). 2.3 Visualization of batch effects To inspect the existence of batch effects in the merged data, we started with plotting the relative log expression (RLE), which revealed substantial differences between expression distributions ( Fig. 3 A). Furthermore, an inspection of the multi-dimensional scaling (MDS;  Fig. 3 B) and hierarchical clustering (HC;  Fig. 3 C) plots of the data demonstrated that samples clustered on the top level due to platforms, suggesting that differences between platforms presented the major contribution to batch effects observed in the dataset. Within platforms however, samples appeared to cluster predominantly due to phenotype rather than study. Thus, in order to merge the listed datasets, batch-removal would have to predominantly adjust for platform related differences in gene expression profiles. Fig. 3. Visualization of batch effects in raw, merged dataset. ( A ) Modified RLE plot showing the median, interquartile region (IQR), and non-outlier ranges of each sample’s RLE distribution. ( B ) Scatter plot showing the result of a two-dimensional MDS analysis utilizing the top 1200 most variable genes. ( C ) Hierarchical clustering of MB samples and the 1200 most variable genes 2.4 Empirical negative control genes NCGs for normalizing the presented data should exhibit stable expression within each phenotype, between MB subgroups and between MB and cerebellum. To our knowledge, a documented list of such genes does not yet exist. Instead potential NCGs were empirically determined from the collected datasets. Specifically, to estimate the extent of gene expression variation across the three comparisons, three scores were computed for each gene ( Supplementary Fig. S3 ), based either on the relative mean absolute deviation (RMD) of expression values among samples from the same phenotype, or one-way ANOVA tests to estimate mean expression differences between MB and normal brain or between MB subgroups, respectively. The three scores were ultimately integrated using the mean rank, and the 2.5% (n = 372) top ranking genes were selected as NCGs. The mean gene expression values of NCGs were distributed across the largest part of the range of observed mean expression values in the merged data ( Fig. 4 A). Additionally, these genes also spanned a wide range of expression dispersion across all samples as measured by the mean absolute deviation (MAD) ( Fig. 4 A). Due to the way in which these genes were selected, it was not surprising that there was no significant overlap with published house keeping genes ( Fig. 4 B). However, a further investigation of the empirically selected NCGs confirmed that they exhibited similar expression levels between phenotypes within individual studies, with generally increased variation between studies and the largest variations observed between platforms ( Fig. 4 C and D). Together, these results confirmed the previous findings, suggesting that platforms presented the largest contribution to batch effects. Additionally, these results demonstrated the suitability of the selected NCGs for normalizing the data, due to their ability to capture such differences while exhibiting more robust expression levels between phenotypes within platforms. Fig. 4. Empirical selection of NCGs. ( A ) Scatter plot of the mean and mean absolute deviation (MAD) of expression values across all samples for NCGs or all other genes in the dataset. ( B ) Venn diagram illustrating the overlap of the empirically defined NCGs with published house keeping genes ( Eisenberg and Levanon, 2003 ,  2013 ). ( C ) Strip chart showing the gene expression of the NCGs with highest MAD ( UBL4B , top panel) and second highest MAD ( ATF2 , bottom panel). Gene expression levels for samples are shown according to subgroup, study and platform. ( D ) Strip chart depicting the variation of expression values between phenotypes, between studies, and between platforms for the empirically defined NCGs (one dot per gene and category). For each gene, the variation between phenotypes was calculated within each study as the RMD across phenotype means and the maximum RMD across studies was utilized as the final value. Similarly, the variation between studies was calculated on study means within each platform and the maximum across platforms was recorded. The variation between platforms was calculated as the RMD across platform mean expression values. ***:  P  &lt; 0.001 (Wilcoxon signed-rank test) To evaluate the importance of the NCGs for the batch effect correction, we further considered three reference sets of NCGs, i.e. (i) 314 house keeping genes proposed by  Eisenberg and Levanon (2003)  and retained in the merged data ( HKG ), (ii) the 372 genes with the lowest expression RMD values calculated across all samples ( Ctrl1 ) and (iii) 372 genes chosen randomly ( Ctrl2 ). 2.5 Batch effect removal Batch effect correction was performed via the  naiveRandRUV  method ( Jacob  et al. , 2016 ), employing the NCGs and a range of combinations of regularization parameters. Selection of a suitable configuration then required the use of a panel of metrics by which the performance of the batch-correction could be evaluated. A multitude of related methods has been proposed during the last years, reviewed for instance in ( Lazar  et al. , 2013 ). Given the availability of phenotypic affiliations in the present data, particular focus was placed on metrics that evaluate normalization performance based on various aspects of phenotype relationships. Specifically, beyond the use of the three visual tools described above, six quantitative metrics were utilized as follows. The standard deviation of median RLE values ( σ m R L E ) was used as a quantification of RLE plots. To evaluate the clustering of samples, (i) a k-means clustering was performed, followed by the computation of the Adjusted Rand Index (ARI) in order to estimate the agreement with the optimal clustering, and (ii) the entropy of the order of platform labels in the HC was calculated. A more general quantification of similarities between samples was estimated by calculating ratios of mean Intra- to Inter-Group Distances (IIGD). The mean classification accuracies of classifiers established from merged data was computed on the basis of a support vector machine (SVM) framework. Finally, a differential gene expression analysis was conducted to determine the mean Overlap with Positive control Genes (OPG). Initial inspection of the metrics between raw and RUV-normalized data suggested an overall improvement of data integration following batch effect removal based on five measures,  σ m R L E , IIGD, ARI, Entropy and SVM ( Fig. 5 A–E). The OPG measure was found less informative for evaluating the batch-correction, as it produced close to maximum scores already in the raw data, with batch normalization only leading to slight absolute improvements ( Fig. 5 F). Importantly, a comparison between RUV normalizations performed with the empirically defined control genes or performed on the three types of controls revealed a general superiority of the empirical NCGs, as demonstrated by significant improvements for at least four of the measures (IIGD, ARI, SVM, OPG) as compared to each of the other sets of control genes ( Fig. 5 B, C, E and F). Fig. 5. Evaluation of batch effect removal. ( A–F ) Box plots depicting the distribution of  σ m R L E  (A), IIGD (B), ARI (C), Entropy (D), SVM (E) and OPG (F) scores obtained from the raw expression data or after batch normalization over a range of regularization parameters and using either empirically defined NCGs (RUV) or three reference sets of control genes (HKG, Ctrl1, Ctrl2). ( G–J ) Heat maps illustrating the dependence of the  σ m R L E  (G), IIGD (H), ARI (I) and Entropy (J) scores on the choice of RUV regularization parameters. ( K ) Scatter plot showing the ranking of promising RUV regularization parameters based on both ARI and Entropy scores. ( L ) Scatter plot depicting the  σ m R L E  and IIGD scores for the top 30 RUV settings from (K) and the selection of the final batch-normalized dataset. ***:  P  &lt; 0.001; *:  P  &lt; 0.05 (Wilcoxon rank sum test) A closer inspection of the four evaluation metrics,  σ m R L E , IIGD, ARI and Entropy, revealed clear dependencies between the respective scores and the RUV regularization parameters, i.e. the normalization strength ( nu.coeff ) and number of independent sources of variation ( k ) ( Fig. 5 G–J). To select a putatively best performing batch-corrected dataset, we first ranked RUV setups based on the mean rank of ARI and Entropy measures ( Fig. 5 K). The 30 top-ranking settings were further evaluated visually and in terms of  σ m R L E  and  I I G D  ( Fig. 5 L). The RUV run with  k = 10  and  nu.coeff = 3 · 10 − 5  was considered to produce the best batch-corrected data, which was subsequently chosen as the final normalized dataset. A visual inspection of the respective dataset demonstrated a clear removal of the majority of batch effects observed in the raw data ( Fig. 6 ). Specifically, the normalization diminished the heterogeneous RLE distributions between samples ( Fig. 6 A). At the same time, two-dimensional or three-dimensional MDS plots ( Fig. 6 B;  Supplementary Fig. S4 ), PCA plots ( Supplementary Fig. S5 ), and a HC plot ( Fig. 6 C) revealed a substantial clustering of samples by phenotype rather than platform, with the HC recapitulating the expected organization ( Northcott  et al. , 2011 ). Fig. 6. Visualization of batch effects in RUV-normalized dataset. ( A ) Modified RLE plot showing the median, interquartile region (IQR), and non-outlier ranges of each sample’s RLE distribution. ( B ) Results of a two-dimensional MDS analysis utilizing the top 1200 most variable genes. ( C ) Hierarchical clustering of MB samples and the 1200 most variable genes 2.6 Validation of the overall strategy on independent training and test datasets The hitherto described normalization scheme was based on NCGs derived from the same dataset, to which they were then applied in the RUV normalization. Thus, we sought to validate the proposed batch-removal approach also on two independent datasets, i.e. a training dataset used only for NCG extraction and an independent testing dataset, which was then normalized using the identified NCGs. Accordingly, the entire merged dataset (excluding samples with lacking MB subgroup labels) was split into two separate datasets, one comprising 7 studies with 958 samples and one containing 16 studies with 619 samples ( Supplementary Table S5 ), used for training and testing, respectively. A detailed description of the validation experiment is described in the  supplementary methods  and illustrated in  Supplementary Figures S6 – S9 . Briefly, the training dataset was used to identify NCGs, which were then used to RUV-normalize the testing data, resulting in a successful batch-correction of the dataset and thus confirming the validity of the proposed strategy ( Supplementary Fig. S8 – S9 ). 3 Discussion A large range of omics profiling techniques have become available for the study of MBs, which beyond transcriptional assays also enable investigations of methylation ( Danielsson  et al. , 2015 ;  Schwalbe  et al.  2013 ;  Schwalbe  et al.  2017 ;  Cavalli  et al. , 2017 ;  Hovestadt  et al. , 2014 ;  Northcott  et al. , 2017 ), histone modifications ( Dubuc  et al. , 2013 ;  Northcott  et al. , 2014 ), copy number aberrations ( Northcott  et al. , 2012b ) or mutations ( Robinson  et al. , 2012 ). These studies can with benefit be combined to obtain a more complete view on the biological foundation of MB, or be utilized individually to address more specific questions. Among the different techniques, transcriptional profiling remains one of the most widely employed methods for functional studies. Yet, expression datasets of MB and normal brain are largely scattered across various studies and platforms. Thus, it is of crucial importance to understand how these data can be integrated into one comparable resource. By merging a large number of publicly available transcription datasets, we showed that batch-effects, especially between technical platforms, present a major factor interfering with the integration of such data. Given the distribution of phenotypes across batches, we argued that a batch effect removal strategy based on NCGs was the most feasible way to normalize the dataset. A bottleneck with this technique was the lack of a set of golden standard NCGs. To overcome this problem, we have implemented a novel approach for the empirical estimation of NCGs. Comparing the performances of batch effect removal attempts utilizing either the empirically defined NCGs or other sets of controls, we were able to show that the choice of NCGs has a pronounced effect on normalization and that our empirically defined controls performed generally superior. Another major obstacle is the question of how to estimate the existence of batch-effects in the data and evaluate the performance of associated normalization approaches. To address this issue, we have investigated a broad panel of visual and quantitative criteria, reflecting either previously described metrics or bespoke methods. Our findings exemplify the notion that the choice of evaluation metrics is perhaps just as crucial as the choice of negative control genes, and that multiple methods need to be combined to address various aspects of data quality affected by batch effects. 4 Conclusions In summary, we have established the largest publicly available normalized dataset of microarray gene expression covering both MB and normal brain samples. We anticipate that this resource will greatly aid the research community due to the increased sample size and inclusion of normal controls. Furthermore, this study also presents a proof of principle for the presented batch effect normalization strategy. We hope that the outlined approach will provide a useful reference for future normalization efforts in the field of MB or other diseases. Specifically, while the recent years have seen a gradual replacement of gene expression microarrays by RNA-seq profiling, future studies will also be hampered by the low incidence rate of MB, likely leading to the generation of multiple RNA-seq datasets, which once again require sophisticated batch effect normalization strategies. Funding This work was generously sponsored by the European Research Council under Horizon 2020 (Project No. 640275, Medulloblastoma—ERC-2014-StG), the Swedish Childhood Cancer Foundation, the Swedish Cancer Society, the Swedish Research Council and the Ragnar Söderberg’s Foundation. 
 Conflict of Interest : none declared. Supplementary Material btz066_Supplementary_Methods Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Enhancing the drug discovery process: Bayesian inference for the analysis and comparison of dose–response experiments</Title>
    <Doi>10.1093/bioinformatics/btz335</Doi>
    <Authors>Labelle Caroline, Marinier Anne, Lemieux Sébastien</Authors>
    <Abstract/>
    <Body>1 Introduction Drug discovery is a highly multidisciplinary process that encompasses the domains of biology, chemistry, computer science and mathematics ( Rudin, 2006 ). A relevant therapeutic target is first identified, then different experiments are set up to analyze its activity under various conditions ( Szymański  et al. , 2011 ). Such an approach makes it possible to deploy research efforts in a relevant and precise way, as well as in a context where there is a demand and a need for novel therapies. The drug discovery process generates a very large amount of data, which often makes it difficult to manage and analyze experiment results. Analyses are thus often limited and omit a large amount of information.  Intuition  hence plays an important role when interpreting the results which can easily lead to biased conclusions. This work aims at developing a methodology that addresses these important issues in the specific context of dose–response experiments. 1.1 Dose–response experiments The technological and biomedical advancements made in recent years have helped to accelerate the drug discovery process. For a specific assay, various chemical compounds are tested in order to identify those capable of generating a satisfactory response. The studied response is specific to the assay setup and can represent inhibition of cell growth, proliferation of cells etc. High-throughput screening (HTS) allows to quantitatively characterize a very large number of compounds (several thousands per day) in an  in vitro  or  in vivo  setting. HTS also allows the rapid elimination of unfit compounds in the context of a specific study ( Szymański  et al. , 2011 ). Screen assays are often used to assess the effectiveness of a chemical compound: it evaluates the biological response for a given dose of the compound of interest. It is possible to study single-dose responses as well as a set of responses for a dose gradient (dose–response screen). Assays can also be designed to study the effect of a combination of chemical compounds (synergistic screen). The proposed methodology described in this article is primarily applicable to dose–response screens, but its application could be widen to the other types of assay mentioned. Dose–response screens are what we could refer to an idealized HTS experiment. It is quite typical that the effectiveness of a set of  hits  identified through a single-dose assay is validated by a dose–response screen (Editorial, 2007). For a gradient of concentrations, a compound of interest is added to well containing cells (cell lines, patient-derived cells etc.). The set of responses obtained (one for each concentration times the number of replicates) is then used to model a dose–response curve from which efficacy metrics are derived ( Pabst  et al. , 2014 ) ( Fig. 1 ).
 Fig. 1. Dose–response curve and efficacy metrics. Example of dose–response curve modeled by non-linear regression. The four commonly reported efficacy metrics are identified in red From a dose–response curve, four efficacy metrics can be derived: 
    IC 50 : the dose needed to generate a mean response equidistant from minimal and maximal responses (low-dose response ( LDR ) and high-dose response ( HDR )]; HDR–LDR: the asymptotic responses generated for very low and very high doses of the compound, also referred to as the plateaus of the curve; and      S : the steepness of response transition between the two plateaus. 
 These metrics can be embedded into a mathematical model, the log-logistic ( Equation 1 ), in which a response  f ( x ) is modeled in terms of a dose  x . Although there are different models ( Brain and Cousens, 1989 ;  Calabrese, 2002 ) that can be used for dose–response analysis, the log-logistic is by far the most commonly used ( Ritz, 2010 ).
 (1) f ( x ) = LDR + HDR − LDR 1 + 10 S · (   log   10 I C 50 −   log   10 x ) We often seek to identify the compound with the lowest  IC 50  ( Pabst  et al. , 2014 ), that is the compound capable of generating a maximal response for the lowest dose. 1.2 Marquardt–Levenberg The process by which the metrics (or the model’s parameters) are normally estimated is called non-linear regression. The experimental data are used to adjust the parameters of the model such that the difference between the experimental data and the dose–response curve is minimized. The regression can be identified with algorithms such as gradient descent, Gauss-Newton and Marquardt–Levenberg ( Levenberg, 1944 ), the latter being the most widely implemented. Various software tools are available to estimate a dose–response curve and its associated metrics ( Gadagkar and Call, 2015 ;  Naqa  et al. , 2006 ;  Veroli  et al. , 2015 ). Other tools include GraphPad, ActivityBase, the R environment and multiple Python libraries. The vast majority of these tools are not accessible to everyone, either because they are costly or because of they are complex to use. None of them allows for the comparison of two curves which limits the comparative analyzes to a qualitative numerical comparison of parameter estimates. The non-linear regression approach, as implemented by the Marquardt–Levenberg algorithm, greatly limits the conclusions that can be made: it does not take into account the  uncertainty  of the estimated efficacy metrics. The  certainty  of the adjusted parameters and of the dose–response curve in regards to the experimental data is generally evaluated on the basis of  intuition , based on visual inspection of the model fit. Complementary methodologies to the non-linear regression are sometimes used to compute confidence intervals. Bootstrap re-sampling ( Efron, 1992 ) and Monte-Carlo simulation are among the most popular. There is a significant need for a methodology that explicitly quantifies the reliability of the efficacy metrics taking into account the noise over the data, while adjusting the log-logistic model. 1.3 Bayesian inference Bayesian inference refers to the process of fitting a probabilistic model to a specific dataset and to represent the fitted parameters by probability distributions. The results obtained are both representative of observed and unobserved data  Gelman  et al. , 2014 ). Bayesian inference aims to infer the  posterior  probability of a hypothesis  H  given a dataset of evidence  E  and previous knowledge about  H . As more elements of  E  are presented to the model, the posterior of  H  is updated. The final results are a  posterior  distribution of the probability of  H  as described by Bayes Theorem ( Equation 2 ).
 (2) P ( H  |  E ) = P ( E  |  H ) · P ( H ) P ( E ) The probability of  H  given  E  is directly proportional to the likelihood  P ( E  |  H )  and to the  prior  distribution  P ( E ). The latter represents our intuition regarding the value of  H . The  prior  is often defined by anterior evidence and observations, as well as theoretical knowledge. The likelihood evaluates the probability of obtaining  E  given  H  ( Bernardo and Smith, 2001 ). Given a parametric model of data  y ∼ f ( x  |  θ ) , it is assumed that  θ  is a random variable which uncertainty can be described by a distribution, hence the  prior . Defining the  prior  is not a trivial task and using a suboptimal  prior  can be detrimental to the analysis. In the context of dose–response,  y  represents an experimental response to dose  x , and  θ  = { IC 50 ,  HDR ,  S ,  LDR } defines the log-logistic model. Various works have already been published on the application of Bayesian inference to the analysis of dose–response experiments ( Collis  et al. , 2017 ;  Cummings  et al. , 2003 ;  Johnstone  et al. , 2016 ;  Messner  et al. , 2001 ;  Smith and Marshall, 2006 ). Although they span a wide range of experimental contexts and their applications are well demonstrated, most methodology lacks flexibility in the type of data it can analyze. To our knowledge, no work has been done on Bayesian comparative methodology which could be beneficial to dose–response analysis. From a software development perspective, there currently exists various platforms to facilitate the implementation and execution of probabilistic analyses. Among the most frequently cited are Stan ( Carpenter  et al. , 2017 ) and PyMC3 ( Salvatier  et al. , 2016 ). 1.4 Objectives The methodologies currently in place limit the analysis of dose–response screens. To overcome these limitations, a significant weight is given to the  intuition  of the experimenter which can easily results in incomplete, biased and difficult to reproduce conclusions. These methodologies do not exploit the experimental data to their full informational potential and thereby impede the drug discovery process. We aim at developing and implementing a Bayesian model for the analysis and comparison of dose–response datasets. The model incorporates the notion of  intuition  through  prior  distributions and computes the  most probable value distribution  for each of the efficacy metrics that define the log-logistic model. The comparison approach computes the  most probable value distribution  for differences between the metrics of two experiments. Finally, we want to redefine the way experimenters, such as medicinal chemists, analyze and interpret dose–response experiments by including uncertainty in their reasoning and providing them a simple and visual approach to do so. 2 Materials and methods We separated our work in three main axes: (i) the probabilistic analysis of a single dose–response dataset, (ii) the comparative analysis of two dose–response datasets and (iii) the development of a web interface. The latter encapsulated the methodologies developed in the two first axes. 2.1 Inferring a dose–response curve We used a hierarchical Bayesian model ( Equation 3 ) to infer the parameters of the log-logistic model ( Equation 1 ) given a dataset  y  of dose–response data.
 (3) P ( θ  |  y ) = P ( y  |  θ ) · P ( θ ) P ( y ) For each component of  θ  we define a  prior  distribution  P ( θ ) . We assume that the dose–response data are normally distribution around  f ( x ; θ )  and for some shared value of  σ  ( Equation 4 ). The value of  σ  is also inferred but without  prior . Its  posterior  is representative of the noise in the dataset.
 (4) P ( y  |  θ ) ∼ N ( f ( x ; θ ) , σ 2 ) To obtained the  posterior  distribution  P ( θ  |  y )  we use the Markov chain Monte Carlo approach with the No-U-Turn sampler ( Carpenter  et al. , 2017 ). Summarily, we define a chain as an ensemble of values that approximate  P ( θ  |  y ) . For every  i  iterations of  I , a set of values  θ  is proposed. It is obtained by sampling a multivariate normal distribution centered at  θ i . The  posterior P ( θ  |  y )  is calculated and the values of Θ are appended to the chain with a probability given by the ratio of likelihood of  θ  and  θ i . If Θ is accepted,  i  becomes  i  + 1 and  θ i + 1 = θ ; if  θ  is not accepted we say that the iteration as resulted in a divergence and  θ i + 1 = θ i . Once  i  as reach  I , a number  w  of the first iterations is discarded as they are  warm-up  iterations. Multiple chains  C  can be run in parallel and their results concatenated to generate the final  posterior  distribution, which is thus composed of  C × ( I − w )  values. Once  P ( θ  |  y )  is obtained, we compute  N ( f ( x ; θ ) , σ 2 )  for a wide continuous range of hypothetical  x . This allow use to derive an inferred dose–response curve, which is really the sequence of median responses for hypothetical and very close to each other doses  x . In the same fashion, we are also able to derive a confidence interval around the curve by aligning the  100 − α th 2  percentiles of every  N ( f ( x ; θ ) , σ 2 )  for the lower bound, and the  100 − 100 − α th 2  percentiles for the upper bound. By doing so, we are capable to analyze what the responses might be for untested experimental doses while characterizing their uncertainty. The data can also be analyzed by plotting the histograms of the  posterior  distributions of  θ . Confidence interval and median values can easily be derived from these distributions. We tested our Bayesian model for various setups and multiple contexts (see Section 3). As demonstrated in the following section, an important aspect of Bayesian inference is the definition of the  prior  distributions. The current paper only presents analyzes done on inhibition rate (%) responses (see Section 2.4), that is responses that range from more or less 0 to 100, and increase as the doses increase. Our general  intuition  regarding the values of the efficacy metrics is as follow:
 We would expect the  IC 50  to be around the median experimental dose (assuming an appropriate range of doses has been tested); We are assuming its value could span a very large range of hypothetical doses while above the  absence of compound  dose; The  LDR  should have a positive value and should more or less have a maximal value of 100%; We do not assume that its value is caped at nor will reach 100%; We would expect the slope ( S ) to be positive (inhibition rate response); We do not restrict it to have a positive value; The  LDR  should be somewhere around the 0% mark. Following these elements of  intuition , we tested different  prior  distribution in order to assess their effects on the inferred  posterior  distributions. Our model could easily be applicable to other type of responses (e.g. survival rate) by adjusting the  prior . 2.2 Comparing two dose–response curves To further our analysis approach and to propose a novel methodology, we adapted our Bayesian model so that we can infer the probability that two curves have significantly different components of  θ . Given two dose–response datasets  D 1  and  D 2 , we are asking  What is the probability that θ k  of D 1  will be greater than that of D 2 ?  In order to answer this question, we evaluate the  posterior  of differences between  θ 1 k  and  θ 2 k  ( Equation 5 )
 (5) P ( Δ θ  |  θ 1 , θ 2 ) 
 Posterior  distributions are inferred for  D 1  and  D 2  in parallel. For every accepted Θ appended to the chain,  Δ θ = θ 2 − θ 1  is computed and stored. In the end, the  w  first elements are discarded, just as for the other  posterior . We can evaluate the probability that each data has the largest value for  θ k  by calculating the ratios of positive ( D 1 ) and negative ( D 2 )  posterior  values. To facilitate the interpretation, we plot the histogram of the differences  posterior  with a contrasted vertical segment marking the median difference. It is also easy to calculate confidence interval and evaluate the reliability of the comparison. This comparative methodology takes into account the  uncertainty  of  θ  which is currently ignored when comparing two dose–response curves. We tested our approach on both synthetic and experimental results, and the results proved to be more informative than the simple qualitative comparison. 2.3 Implementation Our Bayesian model is implemented in the modeling language Stan ( Carpenter  et al. , 2017 ). We use 4 chains of 2000 iterations and 1000 warm-ups to compute the  posterior . We use the PyStan interface (v2.18.0.0) to work with Stan in the Python (v3.0.0) environment. Our plots are generated with Matplotlib (v3.0.2). When comparing our model to the Marquardt–Levenberg algorithm, we used the  optimize  package of Scipy (v1.2.0) with default settings to implement the non-linear regression. For our web interface, we use Flask (v1.0.2) and Python on the server side. On the client side, standard HTML5 and JavaScript is used as well as Jinja and Bootstrap (v3.3.7). Interactivity is mainly provided by the use of jQuery (v2.1.1). 2.4 Dose–response data We use various datasets to test and demonstrate the efficacy of our proposed approach. We use both synthetic and experimental datasets. Using synthetic data allow us to evaluate the efficacy of the various approaches tested in a controlled environment. These data are generated from the log-logistic model ( Equation 1 ). For a given set of 10 hypothetical doses  x  and defined θ = { IC 50 ,  HDR ,  S ,  LDR }, we compute the associated  f ( x ; θ )  responses. Noise is added to dataset by sampling from  N ( y j , σ 2 )  for each response  y j . We used multiple  σ  to test how well our methodology dealt with noise. The various synthetic datasets used in Section 3 are described in  Table 1 . When referencing a synthetic dataset, we use the label of  Table 1  to which we add the  σ  value in subscript. For instance,  A 0  would describe a dataset with an  IC 50  of 2.15, a  HDR  of 60 and a Gaussian noise of  σ = 0.1 . Table 1. Synthetic datasets Label 
 HDR 
 
 IC 
 50 
 
 σ 
 
 A 
 60 2.15 {0.1, 10 } 
 B 
 60 2.0 {0.1, 5, 10 } 
 C 
 90 2.15 {0.1, 5, 10 } 
 S  = 0.8 and  LDR  = 0.0 We also used real experimental data to demonstrate the application of our proposed methodology. The datasets  E 1 ,  E 2  and  E 3  are from a single assay and represents different compounds. The compounds were tested at eight concentrations against patient-derived leukemic cell. The response measured is representative of cell growth inhibition rate (%). The experimental data were obtained through the Leucegene project. Our proposed Bayesian model is unaffected by the number of replicates  R  (number of measured responses for each concentration).  R  varies from one experimental setting to another: to demonstrate the flexibility of our approach, we generated synthetic datasets with  R = { 1 , 3 }  and used experimental datasets with  R  = 2. 3 Results and discussion Results presented in this section are obtained by analyzing both synthetic and experimental datasets (Section 2.4). Most of the figures adaptation from our web interface (Section 3.3). 3.1 Bayesian inference on dose–response data We evaluated the efficacy and limits of our Bayesian model in various experimental contexts. We first compared its results to those obtained by non-linear regression (Marquardt–Levenberg algorithm). We then assessed the effects of various  prior  in order to define the most appropriate. Last, we discussed the inferred  σ posterior  distributions for multiple datasets. 3.1.1 Marquardt–Levenberg versus Bayesian inference We did not optimized the Bayesian  prior  but they were chosen wisely. As for the Marquardt–Levenberg algorithm, we tested it for both the four-parameters (4P) log-logistic model ( Equation 1 ) and a two-parameters model (2P). In the 2P model, only the  IC 50  and slope ( S ) parameters are estimated: the  HDR  and  LDR  are fixed to constant values, 100 and 0, respectively. These three approaches are applied to two synthetic datasets with varying noise ( A 0  and  A 10 ) and on an experimental dataset ( E 1 ). The results are reported in  Figure 2 .
 Fig. 2. Marquardt–Levenberg versus Bayesian inference. Both synthetic datasets  A 0  (black) and  A 10  (orange) have triplicate ( R  = 3). The experimental dataset  E 1  (blue) displays no response in the range of doses tested. Our Bayesian model is used to estimate dose–response curves with a 95% CI. Marquardt–Levenberg estimates the parameters of the 4P log-logistic model ( Equation 1 ) and of the 2P model (only the  IC 50  and  S  parameters are estimated: the  HDR  and  LDR  are fixed to constant values, 100 and 0, respectively).  HDR  and  LDR  (median values for Bayesian Inference) are represented by horizontal black segments;  IC 50  (median value for Bayesian Inference). Root mean square error (RMSE) value is identified for each curve. For Bayesian Inference, we used the median curve to compute the residuals Both Bayesian inference and Marquardt–Levenberg 4P generate the expected values when the data has very little noise ( A 0 , black dataset). The median  HDR  and adjusted  HDR  are the same (60.1) and contrary to the Marquardt–Levenberg 2P, they stay around the 60% mark. As expected, Marquardt–Levenberg 2P generates a dose–response curve that is not representative of the dataset as its  HDR  is fixed at 100%. This forces the  IC 50  to shift to the right (3.62) and the slope to flatten (0.286). When the data are noisier ( A 10 , orange dataset), Marquardt–Levenberg 4P generates curves that resemble the expected model the most. Its curve is steeper (1.36), which can be explained by its high  LDR  (11.1). The estimated  HDR  is as expected (58.9) and there is a small shift in the  IC 50  (2.00). The Marquardt–Levenberg 2P curve is mostly the same as for the  A 0  dataset. Interestingly, the Bayesian inference results differ from the previous ones. First, the CI (95%) surrounding the curve is significantly larger. Second, the median  HDR  now reaches well above 60% (86.5), creating a shift in the  IC 50  to the right (2.60) and a flatter response (slope of 0.290). Even though the curve  seems  to represent well the data, the median parameters do not approximate those expected, with the exception of the  LDR  (−3.17). When compared with the two datasets presented above,  E 1  (blue dataset) completely breaks both Marquardt–Levenberg 4P and 2P. The latter is simply unable to converge (when using Scipy’s implementation, see Section 2.3). As for the former, it returns a very low  HDR  (15.2) and an unrealistically steep slope (18.9). Confusingly, the  IC 50  estimated could lead to erroneously conclude that the compound is active ( IC 50  = 2.58). The Bayesian inference curve better models the absence of response over the range of doses tested. The curve’s inflexion, or  IC 50 , is largely out of the experimental range and reaches a median value of 6.23. The confidence interval surrounding the right side of the curve (outside of the experimental doses range) is extremely wide: its bound span from ∼120 to 0%. The dataset  E 1  is not sufficient to infer precisely efficacy metrics, but sufficient to clearly indicate the lack of response for this compound over the range of doses tested. Finally, since the Marquardt–Levenberg 4P directly minimizes the RMSE, it is not surprising that it achieves overall lower values. It is interesting to interpret the results of  Figure 2  by comparing how each methodology handles the concept of  intuition . Marquardt–Levenberg 4P has no implemented consideration for it: only the data are considered when computing the estimates for the parameters of the model. This greatly limits the analysis to the range of experimental doses, as the approach assumes that the lower and upper response plateaus have been experimentally observed. This explained the unrealistic dose–response curve obtained for  E 1  (absence of the high dose plateau). If we were to analyze this dataset only by looking at its  IC 50 , which is common practice, we would conclude that the tested compound is somewhat active. If we were to further our analysis to the other parameters, we would be puzzled by the very low  HDR . The dataset would most likely be discarded because of the small distance between the  LDR  and  HDR  estimates and/or because of the unusual shape of the curve. The decision to discard  E 1  is entirely based on  intuition  from the experimenter. The Marquardt–Levenberg 2P does take into account the notion of  intuition  in its implementation, but in an extreme way. By fixing the  HDR  and  LDR  to constant values, we imply that our  intuition  is rather a  certitude . Again, this methodology is highly limiting, since our  intuition  prevails over the data. This is exactly what happened during the analysis of  A 0 . In the case where the data do not fit our  intuition  ( E 1 ), the algorithm simply does not converge and the dataset is discarded. Neither of the Marquardt–Levenberg methodologies are capable of considering both the data and our  intuition  in a complementary fashion: it is one or the other. As demonstrated in  Figure 2  this can highly bias our conclusions. Our proposed Bayesian inference methodology is a good alternative to the problematic Marquardt–Levenberg. The use of  prior  allows us to incorporate the notion  intuition  into the computation in a less drastic way than Marquardt–Levenberg 2P. Thus, the resulting dose–response curve can be expanded to doses that were not tested experimentally. This approach also allows for the quantification of  uncertainty , which neither of the Marquardt–Levenberg approaches do. For instance, we can conclude with certainty that  E 1  does not support an  IC 50  within the range of doses tested and the compound can be eliminated from further studies. Even though Bayesian inference is better suited for the analysis of dose–response data than the Marquardt–Levenberg algorithm, it still presents some limitations as demonstrated by the analysis of  A 10 : inappropriate  prior  combined with high noise can skew the results ( Fig. 2 ). The following section discusses this topic in more details. 3.1.2 Defining  prior  distributions We must think of  prior  as safety nets: when the data are insufficient, the inference gradually falls back on the  prior  distributions. It is thus important to use appropriate  prior  that best represent the experimental context. The process of defining the most suitable  prior  for  θ  is referenced as  prior elicitation . It can either be based on consensus notions regarding  θ  ( Chen  et al. , 1999 ), or on beliefs ( Albert  et al. , 2012 ). The latter corresponds to our aim of mathematically implementing the notion of  intuition . We describe  prior  in terms of their  informativeness  which refers to the information that they provide. More informative  prior  are not necessarily better: they may be too restrictive which can be highly detrimental if they do no complement the data. A  prior  should be a representation of  intuition  rather than  certitude  of what the unobserved data would be. To demonstrate the effects  prior  informativeness, we tested two sets of  prior  for the analysis of the synthetic dataset  A 0  with  R  = 1. All  prior  are normally distributed and for a given parameter, centered around the same value. ‘Informative’  prior  have very narrow distributions, while ‘Less Informative’  prior  have wider distributions with  σ  five times bigger than that of the ‘Informative’ ( Fig. 3 ).
 Fig. 3. 
 Prior  informativeness. Informativeness can be described by the wideness of the distribution. ‘Informative’ (narrow)  prior  prevail on the data and the inference is biased. ‘Less Informative’ (wide)  prior  do not overshadow the data and the curve is inferred with high certainty. We used the  A 0  dataset with  R  = 1. The median  HDR  and  LDR  are represented by black horizontal segments, and the median  IC 50  vertical segments. RMSE value is identified for each curve. We used the median curve to compute the residuals When comparing both Bayesian inferences, it is clear that the ‘Informative’  prior  are not suited to the data ( Fig. 3 ). Even though both  HDR prior  are centered at 100%, the ‘Less Informative’  prior  does not prevail over the data and parameters can be inferred as expected (2.14, 60.1, 0.801, 0.032 for the  IC 50 ,  HDR ,  S  and  LDR , respectively). The second curve is reminiscent of the one obtained when using Marquardt–Levenberg 2P ( Fig. 2 ). In such case, the  prior  are highly restrictive and do not complement the data, causing the inferred curve to mainly be representative of the  prior  themselves. 
 Figure 3  illustrates the effect of  prior  informativeness on 10 data points ( R  = 1). The undesirable effects of ‘Informative’  prior  can be counterbalanced by giving more data points to the Bayesian model. For example,  A 0  dataset with  R  = 5 prevails on the ‘Informative’ setup. In the context of dose–response analysis, it is not always possible to generate large dataset due to cost and material limitations.  Prior  should thus be defined by less informative distributions. We tested various setups of  prior  distributions ( Table 2 ) in order to establish the ones that can generalized to multiple experiments with similar contexts. Again, we used the synthetic dataset  A 10  with  R  = 1. The dose–response curves and  posterior  distributions are presented in  Figure 4 . Table 2. Distributions parameters Description 
 IC 
 50 
 
 HDR 
 
 S 
 
 LDR 
 More Informative Normal Dist. 
 N ( 2.5 , 5 ) 
 
 N ( 100 , 10 ) 
 
 N ( 1 , 5 ) 
 
 N ( 0 , 10 ) 
 Less Informative Uniform Dist. 
 U ( − 15 , 45 ) 
 
 U ( 0 , 150 ) 
 
 U ( − 10 , 10 ) 
 
 U ( − 50 , 50 ) 
 Less Informative Normal Dist. 
 N ( x ^ , 10 ) 
 
 N ( 100 , 20 ) 
 
 N ( 0.5 , 10 ) 
 
 N ( 0 , 20 ) 
 
 Note :  x ^ , median of experimental doses. Fig. 4. Effects of various  prior . Three different  prior  settings ( Table 2 ) are tested on the  A 10  synthetic dataset with  R  = 1. Dose–response curves are plotted against the data and with a 95% CI. The  posterior  of each parameter is represented by an histogram. The colored vertical segments represent the median value (continuous) and its 95% CI (hashed). The numerical median value is indicated in the legend. The expected values (used to generate the synthetic data) are identified by a red arrow on the x-axis. The light gray segment superimposed on the histogram illustrates the contour of the  prior  distribution The ‘More Informative Normal Dist.’  prior  resulted in a higher than expected  HDR  (96.0) which generates a shift to the right in the  IC 50  (3.40). The slope is also flattened by this high  HDR  and its value diverges greatly from the expected one. Interestingly, the  HDR posterior  is highly similar to the  prior . Similarly, the  LDR posterior  is also matching its  prior . When looking at the data, we notice that there are no clearly define upper and lower plateaus: the inference must thus rely mainly on the  prior  to define these regions of the dose–response curve. Even though the  prior  distributions are not highly informative, they are still too informative and force the  HDR  to reach the theoretical optimal 100.0% even though it is not directly supported by the data. The ‘Less Informative Uniform Dist.’  prior  is the less informative out of the three settings. Only the median  HDR  is approaching the expected values (59.1) but its CI (95%) is quite large. The other inferred parameters do not resemble those expected, which is not surprising considering the noise present in the data. When comparing the  posterior  distributions to the  prior , we noticed that they were bound by very similar limits with the exception of the slope, which has a lower bound of 0. The ‘Less Informative Normal Dist.’  prior  seems to be a good compromise between the two previously described settings. The median values are not as expected but this can be explained by the noise in the data, mainly in the low dose region. The median  HDR  is however not too far from the 60% mark. It is interesting to notice the shift between the  posterior  and the  prior  of that parameter, which is not observed in the other two settings. Overall, normally distributed  prior  ( N ( μ , σ 2 ) ) appear more appropriate. The uniform distributions  prior  ( U ( α , β ) ) are too uninformative: when data are insufficient, the distribution values suggested by the  prior  are all equally probable which has the same effect as adding a large amount of new noisy data. This could explain the very large confidence intervals when using uniform  prior , with the exceptions of the slope. In addition to the lack of informativeness in regards to the most probable value, uniform distributions are constrained by their  α  and  β  parameters. For instance, the slope  posterior  abruptly stops at 10 which is incidentally the defined  β  we selected for the slope uniform  prior . Comparatively, the normal distribution is not bound and each distribution values as its own probability. We also adjusted our intuition of  μ  for both the  IC 50  and slope  prior  ( Table 2 ). Assuming the experimental doses are sufficient and range on a 2-fold scale, we could expected the  IC 50  to be near the median experimental dose. We will be using the ‘Less Informative Normal Dist.’  prior  as default settings for now on. 3.1.3 Unresponsive data So far, we mainly used synthetic datasets to explore the application of our Bayesian model to the analysis of dose–response data. To assess the extend of the applicability of our approach, we applied it to the analysis of a seemingly unresponsive experimental dataset,  E 1  ( Fig. 5 ). This type of response is frequent during the drug discovery process and it is of the utmost importance that the analysis approach applied can confidently assess that this compound has an  IC 50  value above the range of doses tested and must be discarded.
 Fig. 5. Bayesian inference applied to seemingly unresponsive experimental data. Results obtained for the experimental data  E 1  using our default  prior  settings. Parameters  posterior  are represented by histograms. Their median values are identified by the colored full vertical segments and the values are reported in the legend. The colored dashed vertical segments mark the 95% CI bounds. The light gray segment superimposed on the histogram illustrates the contour of the  prior  distributions On this specific dataset, the responses never reach &gt;30% and there is no clear tendency. The inferred dose–response curve is mainly flat for the entire experimental doses range. The median  IC 50  is high (9.37). As we expected it, the  HDR posterior  is highly reminiscent of the  prior : the data did not give any indication regarding the response at very high doses. All the parameters’ confidence intervals are quite large. We are not able to determine with certainty the efficacy metrics of the tested compound. We can however conclude with certainty its  IC 50  is bigger than 5    log   10 nM , which is enough to discard this compound as ineffective. Such a high certainty conclusion cannot be made on seemingly unresponsive dataset with commonly used Marquardt–Levenberg algorithm methodology, without resorting to  ad hoc  rules. 3.1.4 Inferring noise Our Bayesian model also infers a  posterior  distribution for  σ  ( Equation 4 ), which describe the amount of noise in the dataset. For synthetic datasets ( A 0  and  A 10 ), the median  σ  is close to the actual  σ  used to generate the data ( Fig. 6 ). It is true that we used a Gaussian noise when generating the data, and that our Bayesian model assumes that the responses are from independent identical normal distributions. That being said, the median  σ  for the experimental dataset  E 1  is very close to the standard deviation of responses for this dataset ( Fig. 6 ), which corresponds to interpreting the dose–response as flat and corresponding to the  LDR  plateau.
 Fig. 6. A  posterior  distribution of  σ .  posterior  distributions obtained by applying our Bayesian model to the synthetic datasets  A 0  and  A 10  (black and orange respectively), and to experimental dataset  E 1  (blue). The median  σ  are represented by the full segments and their values are reported in the legend. The dashed segments mark the bounds of a 95% CI. For the two synthetic datasets, the real value of  σ  is identified by a red arrow on the x-axis. For the experimental dataset, the standard deviation of the responses is represented by the red full segment and its value is reported in the legend 3.2 Comparison of two dose–response datasets To further the analysis of dose–response data, we proposed a novel comparison methodology. As mentioned in Section 2.2, the comparison is done by inferring the  posterior  of the difference between two values of an efficiency metric. From these  posterior , we can derive the probability that a dataset has the largest value for a given efficiency metric. The uncertainty identified through the individual dose–response inference is carried to our comparison analysis, which allows to characterize the uncertainty of the difference. When comparing the synthetic datasets  B 0  and  C 0  ( Fig. 7 ), we can conclude with great certainty that the  C 0 IC 50  is larger than that of  B 0 , even though the difference between the value is quite small (∼0.15). We can also conclude with great certainty that  C 0  has a higher  HDR  than  B 0 . The precision of both datasets ( σ = 0.1 ) allows us to draw these conclusions without doubt. In contrast, when comparing  B 5  to  C 5  ( Fig. 7 ) we cannot make such conclusion. These two datasets share the same parameter values as  B 0  and  C 0 , respectively, but they were generated with increased noise ( σ  = 5). The inferred  IC 50  are more uncertain and their  posterior  distributions overlap. When comparing their respective median  IC 50 , one could easily concludes that the  B 5  dataset has a larger  IC 50  than the  C 5  dataset (2.29 &gt; 2.15) and that the  B 5  compound is thus less effective than that of  C 5 . This conclusion is highly biased: the uncertainty of the inferred  IC 50  does not allow for the identification of significantly greater value as demonstrated by the Δ IC 50 posterior . If we take a look at the comparison of  HDR  values, we notice that the uncertainty does not affect the comparison: the values are different enough that the two  posterior  do not overlaps. We thus can conclude with certainty the  C 5 HDR  is greater than the  B 5 HDR  even when considering their respective uncertainty. Similar results have been observed when comparing the two highly noisy ( σ  = 10) that are  B 10  and  C 10  ( Fig. 7 ). The difference in median  IC 50  is even greater but the Δ  IC 50 posterior  tends more toward the expected conclusion. The  HDR  comparison is still highly convincing despite an higher level of uncertainty in the individual  HDR posterior . To highlight the informative gain of our comparative approach, we compared and analyzed two experimental datasets ( E 2  and  E 3 ) using two methodologies: (i) the commonly used numerical comparison of  IC 50  and (ii) our differences  posterior  approach. 3.2.1 Numerical comparison When comparing the  IC 50  median values, we notice they differ by 0.11 log 10  nM  (Fig. 8A)  which is equivalent to ∼32 nM. We would conclude that the  E 3  dataset has a larger  IC 50  than that of the  E 2  dataset. The  E 2  compound is thus seemingly more effective than the  E 3  compound.
 Fig. 7. Comparison of synthetic datasets. Three pairs of synthetic datasets with  R  = 1 are compared:  B 0  to  C 0 ,  B 5  to  C 5  and  B 10  to  C 10 . Each pair of datasets differs in their  IC 50  and  HDR  values. The 95% CI of each median curve is represented by the colored shaded regions. For both  IC 50  and  HDR , the stacked individual  posterior  are represented by the colored histograms. Their median are marked by black segments. The numerical values are reported in the legend. The Δ  IC 50  and Δ  HDR posterior  are represented by gray histograms. The true difference is identified by a red arrow on the x-axis (0.15 for the  IC 50  and 30 for the  HDR ). The median values of the differences  posterior  are identified by the red hashed segments, and the numerical values are reported in the legend. The probability (in %) that a dataset has the largest value for a given parameter is identified on the graph in the color corresponding to the dataset 3.2.2 Differences  posterior When we first look at the Δ  IC 50 posterior  we cannot conclude that one of the  IC 50  is greater than the other. The  IC 50  were not inferred with enough certainty, because of the noise present in the data, for us to conclude that their values are significantly different. The  HDR  are however significantly different, despite the great uncertainty of  E 2 HDR (Fig. 8A) . The Δ  HDR posterior  identify the  E 3  dataset as the one with the overall largest  HDR . It is also interesting to note that the difference between the two  HDR  is quite large, with median difference of almost 23%  (Fig. 8A) . The  E 3  also have the overall largest  S (Fig. 8A) . When combining all of these information, we can conclude that the  E 3  compound is more effective at generating a maximal response than the compound of  E 2 . The two conclusions greatly differ and the one drawn from the numerical comparison is biased. The numerical comparison methodology is highly limited as it only considers one efficacy metric and does not consider the uncertainty associated to its values. It is preferable to consider all four metrics to get a more complete characterization of the efficacy of a compound. We must also evaluate the probabilities of certainty on the metrics as well as on the comparison itself to ensure our conclusions are as precise as is appropriate. Last, interpreting pairwise plots of the  posterior  distributions can also help to draw informed conclusions. This sort of representation can identify inter-parameter dependencies which should be considered when analyzing  posterior  distributions. We can observe in  Figure 8B  that both datasets are distinguishable by pairing their  HDR  and  S , which was not observable from the analysis of the histograms of  Figure 8A . 3.3 BiDRA: an online tool The two previous sections demonstrated how well and how much more information can be gathered when using our proposed Bayesian methodology for the analysis and comparison of dose–response data. The conclusions drawn from such analyses are less prone to bias compared with other commonly used methodologies. We are aware that the implementation and subsequent application of our Bayesian approach is not within everyone’s reach. We thus decided to develop an easy-to-use web interface,  BiDRA  (Bayesian inference for dose–response analysis). The interface proposes both the analysis of a single dataset (Sections 2.1 and 3.1) and the comparison of two datasets (Sections 2.2 and 3.2). For both analyses, the user simply uploads the dataset(s) in a comma-separated values (CSV) format with the first column corresponding to the doses and the second representing the associated responses. It is important that the doses be log-transformed since we are using the log-logistical model ( Equation 1 ). The data type must then be specified:  Inhibition  if the response increases with the dosage;  Activity  is the response decreases as the dosage increases. The  HDR  and  LDR prior  are adjusted according to the response type. We suggest default  prior  distributions (Section 3.1.2), assuming the data represent some sort of rate (%). The user can however easily specify his own  μ  and  σ  for each parameter. The results are returned in both a figure and in a table. For the single dataset inference, the median dose–response curve as well as the  posterior  of all efficacy metrics and the  σ  are plotted. The returned results are similar to  Figure 5 . For the two datasets analysis, the individual inference plots are returned as well a figure describing the comparison. The latter includes the stacked individual  posterior  as well as the differences  posterior . As an example,  Figure 8A  was obtained from BiDRA. For every computed  posterior , we return its median and the bounds for 10, 5 and 1% CIs in a table. The interface is accessible ( https://bidra.bioinfo.iric.ca/ ) and does not require any authentication. The interface is not connected to any database and the analysis is not saved. We plan on adjusting the interface as our work progresses (see Section 4). 4 Implications We propose in this article a Bayesian inference methodology for the analysis of dose–response data. This approach is then extended to directly infer differences in efficacy metrics between two dose–response experiments. Our approach addresses two limitations of the commonly used Marquardt–Levenberg algorithm: first, it yields a single point estimate for each efficacy metrics, with no assessment of the uncertainty for these values. The experimenter is then left to decide on whether to accept or reject a given fit based on its  intuition . This process is typically manual leading to possible biases and difficulty to reproduce analysis results. The second limitation is that the Marquardt–Levenberg algorithm relies entirely on the experimental data to estimate the efficacy metrics. In cases where the data are insufficient to determine one of the efficacy metrics, this algorithm will settle for the mostly likely value without consideration for experimentally sound boundaries. These limitations are compounded by the fact that there exists no methodology to support direct comparison of dose–response curves besides numerically comparing the efficacy metrics. The Bayesian inference approach we describe here allows us to incorporate in the analysis of dose–response the notion of experimental  intuition  to guide the identification of plausible ranges for each of the efficacy metrics. This reduces the necessity for careful inspection of curve fitting and provides a sound statistical framework to communicate the reliability of estimates to the experimenter. Our approach shares similarities to the ones presented in  Collis  et al.  (2017) ,  Cummings  et al.  (2003 ),  Johnstone  et al.  (2016) ,  Messner  et al.  (2001 ) and  Smith and Marshall (2006 ); as it implements a simple hierarchical Bayesian model. We consider as part of our analysis all efficacy metrics of the log-logistic model ( Equation 1 ). We also propose a novel and informative approach to compare two dose–response curves, again unambiguously conveying estimates uncertainty as  posterior  distributions of the differences for efficacy metrics of interest. In practice, these distributions are either communicated as a probability that one value is larger for one experiment than in the other, or as a confidence interval on the difference. As mentioned by  Johnstone  et al.  (2016) , the Bayesian inference still have some limitations even though it provides numerous advantages when compared with the usual non-linear regression approach. As for Marquardt–Levenberg, computation time increases with the number of data points under consideration: the analysis of  A 10  ( R  = 3) ( Fig. 2 ) took ∼0.8 s, while the analysis of  A 10  ( R  = 1) ( Fig. 4 ) took ∼0.5 s (Intel, i9-7920X). Comparatively, the comparison of  B 10  and  C 10  (both  R  = 1) ( Fig. 7 ) took ∼3 s. In most practical settings, the computational time necessary for these analyses is insignificant to the time required for actually performing the experiments being analyzed. A more important limitation to consider is the difficulty to clearly express the relative weight of the  prior  in the analysis. As shown in Section 3.1.2, an inappropriate  prior  can greatly alter the  posterior  distributions. This effect is mostly seen for the  HDR  and  LDR  as they often depend on extrapolation of the experimental data. As a general rule of thumb, the  prior  informativeness should not outweigh the data information and least informative  prior  should be favored in most situations.
 Fig. 8. Comparison of experimental datasets. Datasets  E 2  and  E 3  are compared. ( A ) The 95% CI of each median curve is represented by the colored shaded regions. The individual  posterior  are represented by the stacked colored histograms. The median values are indicated by black segments and the numerical values are reported in the legend. The Δ  posterior  are shown as gray histograms with their median values represented by red hashed segments. The probability (in %) that a dataset has the largest value for a given efficiency metric is identified on the graph in the color corresponding to the dataset. ( B ) Pairwise comparison of the individual  posterior . Each dot a single value of the  posterior  for a given efficiency metric. Datasets are identified by their colors That being said, we do think our approach to directly compare two dose–response will provide a useful tool to support the drug discovery process, either at the stage of secondary validation following a primary screen or during compound optimization. Considering distributions of  probable values  instead of single point estimates brings more depth to interpretation efficacy metrics and supports better informed decision from the experimenters. These benefits are also attained through a method that better support automated analysis as we greatly reduced the necessity for manual inspection of each fit. Finally, we would also like to emphasis the flexibility of the proposed framework. We are currently exploring the use of this approach in the context of primary screens based on high-throughput, single-dose assays or to the more complex context of two-compounds synergistic dose–response assays. There are currently no established methodologies for the analysis of these types of assay. We think that Bayesian inference would be highly beneficial and could help to more reliably identify compound  hits  as well as better quantification of compounds interactions. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>BamView: viewing mapped read alignment data in the context of the reference sequence</Title>
    <Doi>10.1093/bioinformatics/btq010</Doi>
    <Authors>Carver Tim, Böhme Ulrike, Otto Thomas D., Parkhill Julian, Berriman Matthew</Authors>
    <Abstract>Summary: BamView is an interactive Java application for visualizing the large amounts of data stored for sequence reads which are aligned against a reference genome sequence. It supports the BAM (Binary Alignment/Map) format. It can be used in a number of contexts including SNP calling and structural annotation. BamView has also been integrated into Artemis so that the reads can be viewed in the context of the nucleotide sequence and genomic features.</Abstract>
    <Body>1 INTRODUCTION Second-generation sequencing produces large volumes of short-read sequence data. In common applications of the technology, such as resequencing or transcriptome sequencing, reads are mapped against a reference genome. In many cases, bases in a reference are covered with alignment depths varying by orders of magnitude and therefore present a challenge for visualization. SAM (Sequence Alignment/Map) and BAM (Binary Alignment/Map) formats are emerging as a standard representation for read alignments. It is therefore important to have visualization software for this format. BAM files contain the same information as SAM. As BAM format is compressed it provides an efficient means to store the data and enables fast retrieval of regions and so this format has been adopted here. SAMTools (Li  et al. ,  2009 ) includes a very simple text alignment viewer using the GNU ncurses library giving a detailed view at the nucleotide resolution level. Lookseq (Manske and Kwiatkowski,  2009 ) is a perl-cgi application used to display, in a web browser, reads mapped against a reference. It can read the data either directly from BAM files or from a relational database to display reads and plots paired read positions against their inferred size. Alignment tools can be used to produce BAM format files. For instance, SSAHA (Ning  et al. ,  2001 ) now supports SAM format (Long  et al. ,  2009 ) and Maq output (Li  et al. ,  2008 ) can be converted to SAM/BAM using SAMTools. BamView can be used as a stand-alone Java application or displayed in Artemis (Carver  et al. ,  2008 ; Rutherford  et al. ,  2000 ,  Fig. 1 ) in conjunction with the reference sequence and annotation. Artemis is a freely available genome browser and annotation tool. BamView in Artemis provides the annotator with an extra level of information that can inform them about structural annotation.
 Fig. 1. ( a ) Showing the BAM stack view in the Artemis genome browser. This is a region in chromosome 13 of  Plasmodium falciparum  3D7. The BAM view is displaying RNA-Seq Solexa reads from the early ring stage in the life cycle of  P.falciparum . ( b ) This is a zoomed in view showing the boundaries of adjacent exons. These boundaries are confirmed by the reads in BamView. A paired read is selected and marked by a red rectangle. SNPs are coloured red. The black reads are single reads. 
 BamView not only displays the Lookseq type of view (plotting against the paired reads inferred size) but also has other types of views that are described below. It does not require a web server and reads the data from an indexed BAM file. 2 IMPLEMENTATION The BAM file needs to be sorted and indexed using the SAMTools command tool. This creates the BAM index file that the viewer uses to access the region to display in a fast way. BamView uses picard (picard.sourceforge.net), which is a Java API to read from the BAM file the reads in the region of sequence being displayed. Picard requires Java 1.6+ and so this is the minimum requirement for BamView. The alignment data can be displayed at different levels of resolution ( Fig. 1 ). Zooming out displays more of the read alignments across the reference sequence. The stand-alone BamView has ‘+’ and ‘−’ buttons to zoom in and out. In Artemis, the resolution of BamView matches the top feature display ( Fig. 1 ). The resolution is controlled using the Artemis zoom scrollbar (on the right-hand side of the feature display). All other BamView options and functionality are in both the standalone and integrated versions. Right clicking on the window gives a pop-up menu with a ‘View’ menu. There are four views provided by BAMView. The first is similar to one devised within Lookseq, whereby paired reads are plotted against their inferred insert size. An option displays single read alignments. There is an option to draw this against a log scale of the inferred size. In situations where the distribution of inferred sizes is broad this has the advantage of showing them closer together. The second is a ‘stack view’ where reads in a region are displayed piled against the reference to reduce the alignment depth. A variation is the ‘paired stack view’, where lines join paired reads within the stacks. When forward and reverse reads are described in the BAM file, the ‘strand stack view’ can be used to display reads above and below a line representing the reference. The reads are colour coded so that paired reads are blue and those with an inversion are red. Reads that do not have a mapped mate are black and are optionally shown in the inferred insert size view. In the stack view, duplicated reads that span the same region are collapsed into one green line. In addition to these views, when zoomed in the reads are displayed at the nucleotide level to shows how they are aligned to the reference sequence. Bases can be coloured by their mapping quality score (blue &lt;10; green &lt;20; orange &lt;30; black ≥30) and there is an option to display insertions. Bases that disagree with the reference consensus (sequencing errors or polymorphism) can also be displayed by selecting an option in the pop-up menu. These are red vertical lines on the reads and displayed as red nucleotides when zoomed in. From the pop-up menu, BamView has the option to filter the reads displayed. This enables the user to display reads that are of significance and it also has the advantage of reducing the memory required by the programme. Reads can be filtered based on their mapping quality (i.e. the MAPQ field in the BAM file). Read alignments can also be filtered using the FLAG field. This field is used in the SAM/BAM specification to describe predefined properties of each read alignment (e.g. proper pair, mate unmapped, first of pair). When the user points the mouse cursor over a read, a tool-tip will display information about the read (name, coordinates, insert size and reference sequence name). The reads can be selected which is useful to track the read when zooming in and out. When fully zoomed in, the selected reads are highlighted by a red rectangle. Right clicking on a reads shows a pop-up menu that includes an option to go to the other read of a mate pair. 3 DISCUSSION Using the different views in BamView and as an integrated window in the Artemis tool means that it has a range of uses. For example, BamView can be used to inspect the confidence of a deep alignment of short reads, by highlighting base discrepancies. As it is integrated into Artemis, the underlying reference consensus sequence can be edited directly based on the aligned evidence. More commonly, the view is used in-house to assess the evidence behind a SNP or other polymorphism call. The alignment depth and base quality can both be easily browsed and inspected. Perhaps the greatest strength of BamView, when embedded in Artemis, is that it allows annotation to be changed based on manually inspecting data from transcriptome sequencing experiments (RNAseq, Otto  et al. ,  2009 ; Wang  et al. ,  2009 ). An annotator can zoom into intron–exon boundaries, identified from coverage plots, and see the quality of evidence supporting a prediction or manually adjust exons coordinates to fit the evidence. Simply viewing individually aligned reads cannot resolve alternate splicing patterns, but by clicking through read-pairs, an annotator can reconstruct the phase of exons in different isoforms. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Fuzzy modeling and global optimization to predict novel therapeutic targets in cancer cells</Title>
    <Doi>10.1093/bioinformatics/btz868</Doi>
    <Authors>Nobile Marco S, Votta Giuseppina, Palorini Roberta, Spolaor Simone, De Vitto Humberto, Cazzaniga Paolo, Ricciardiello Francesca, Mauri Giancarlo, Alberghina Lilia, Chiaradonna Ferdinando, Besozzi Daniela, Valencia Alfonso</Authors>
    <Abstract/>
    <Body>1 Introduction Cells are complex heterogeneous systems, whose functioning is governed by a finely regulated interplay between various types of molecules involved in gene expression, signal transduction and metabolic pathways, altogether resulting in different cellular phenotypes. Dysfunctional processes caused by events occurring at the molecular level can induce a cascade of local and global damages in cells, tissues, organs and, possibly, in the whole organism. Therefore, understanding molecular regulations at a mechanistic level is indispensable either to prevent or control the onset of many diseases. In this context, the integration between experimental data and computational methods facilitate the definition of predictive mathematical models, whose simulations can elucidate the emergent properties of the biological system in physiological and pathological conditions, reveal possible counter-intuitive mechanisms and envisage new hypotheses that can be tested in the laboratory ( Faeder and Morel, 2016 ;  Kitano, 2002 ). The mathematical description of biological systems can be realized with different approaches, such as mechanism-based ( Wilkinson, 2009 ) or logic-based modeling ( Le Novère, 2015 ). Mechanism-based models provide a detailed description of the underlying biochemical reactions ( Chylek  et al. , 2015 ;  Szallasi  et al. , 2006 ). However, these models require the knowledge or the inference ( Chou and Voit, 2009 ) of quantitative parameters (e.g. kinetic rates, molecular amounts) that are often difficult to be measured, especially  in vivo  and for large-scale systems, therefore hampering the effectiveness of many computational analyses ( Somogyi  et al. , 2015 ;  Tangherloni  et al. , 2017 ). Moreover, biological systems and their components are often described in a qualitative way, by using natural language terms, such as ‘moderately active’ or ‘highly expressed’, which highlight biological uncertainty and experimental measurement limitations. Given these issues, logic-based models may be used as a reliable alternative to study cellular systems ( Morris  et al. , 2010 ;  Samaga and Klamt, 2013 ;  Wynn  et al. , 2012 ), since they are more suitable than mechanistic models when only qualitative data are available ( Flobak  et al. , 2015 ;  Fumiã and Martins, 2013 ;  Zañudo and Albert, 2015 ). In particular, fuzzy logic represents a powerful extension of Boolean logic to model complex systems, since it generalizes the binary formalization of the variables states to deal with any uncertainty related to the system ( Yen and Langari, 1999 ). Fuzzy logic-based models are defined by: (i) a directed graph representing the set of system components (i.e. linguistic variables) and their mutual positive or negative regulations; (ii) a set of linguistic terms for each variable, necessary to give a qualitative description of intracellular concentrations or functional activities. Noteworthy, linguistic terms bypass the necessity of a precise parameterization but, at the same time, they allow to provide a quantitative representation of the variables states thanks to the so-called membership functions; (iii) a set of fuzzy logic rules that specify the state that each linguistic variable will assume over time, according to the states of the variables by which it is regulated. So far, fuzzy logic has been applied to different strategies in the field of cellular biology, for instance, to model and simulate signaling pathways ( Aldridge  et al. , 2009 ) or gene regulatory networks ( Küffner  et al. , 2010 ), overcome the lack of kinetic parameters ( Bordon  et al. , 2015 ;  Liu  et al. , 2016a ), automatically infer network models ( Keller  et al. , 2016 ;  Liu  et al. , 2016b ;  Morris  et al. , 2011 ;  2016 ) or implement regression models ( Schmidt-Heck  et al. , 2015 ). In this work, we propose a novel and general-purpose computational method, based on fuzzy logic, designed to facilitate the modeling and analysis of heterogeneous systems. In particular, our method was designed to provide a mathematical description of complex biological systems whose components range from single molecules to whole cellular processes and observable cell phenotypes, together with their mutual regulations. One of the main advantage of this method is that, although it does not require the availability of quantitative (kinetic) parameters and exact values of the state or the abundance of cellular components, it is a  dynamical  modeling approach that allows to simulate and predict the temporal evolution of the system in both unperturbed and perturbed conditions. We also show that our fuzzy modeling approach, coupled with an optimization algorithm, automatically identifies a potential (minimal) set of system components whose perturbation can maximize, or minimize, a desired system response. This automatic identification represents one of the main novelties of our computational approach, which can largely facilitate the design of new laboratory experiments by yielding putative perturbations able to drive the behavior of an arbitrary complex system. Although several fuzzy logic tools and libraries are available in the literature, none of them was specifically designed to support the modeling, the dynamical simulation and the optimization of the heterogeneous systems that we aim to investigate. For this reason, our methodology was implemented from scratch and, in particular, we developed a novel user-friendly software named FUMOSO (FUzzy MOdel SimulatOr), which supports the definition, editing, export and simulation of heterogeneous fuzzy models of complex dynamical systems. To show the potentiality of this novel computational method, we investigated a complex, heterogeneous system consisting of oncogenic K-ras cancer cells—characterized by the so-called ‘Warburg effect’—grown in a progressive limiting amount of glucose, in order to understand the glucose-dependent mechanisms driving cancer cells to death or survival. The Warburg effect, or aerobic glycolysis, is a metabolic hallmark of malignancy. Accordingly, numerous cancer cells, grown either in low glucose availability or in free glucose, are strongly susceptible to cell death as compared to normal cells. However, it has also been observed that not all cancer cells undergo cell death upon a really harsh environment, such as in glucose starvation, since some of them might acquire the ability to survive in this new environmental condition by activating compensatory signaling pathways ( Huang  et al. , 2019 ;  Palorini  et al. , 2016 ) and alternative metabolic routes ( Ye  et al. , 2015 ;  Zaugg  et al. , 2011 ). Worthy of note, metabolic rewired cancer cells, which often are more aggressive ( Endo  et al. , 2018 ), can be selected by chemotherapy, by therapies exploiting synergism between chemotherapeutic treatments and anti-metabolic drugs or by genetic and pharmacological ablation of oncogenic pathways ( Elgendy  et al. , 2019 ;  Viale  et al. , 2014 ;  Zhao  et al. , 2013 ). In addition, strategies to directly inhibit glycolysis in cancer patients have been partially dumped since they might also likely damage normal healthy tissues (i.e. smooth and skeletal muscle, and normal viscera). In this scenario, the combination of therapies targeting aerobic glycolysis, adaptive mechanisms (i.e. increased autophagy) and well-established cancer-specific targets (i.e. tyrosine kinase signaling pathways) represent a potential approach to be explored in cancer cure. Here we present a fuzzy logic model of programmed cell death and survival under progressive glucose depletion, defined on the basis of an extensive prior knowledge of the main components involved in K-ras-transformed cells grown in this perturbed condition. In particular, we considered cancer cell death occurring upon glucose starvation along two major pathways: (i) a pathway centered on mitochondria (reactive oxygen species (ROS), adenosine triphosphate (ATP) depletion, calcium (Ca 2 + ) overloading) ( Elmore, 2007 ;  Taylor  et al. , 2008 ) and (ii) an endoplasmic reticulum (ER)-stress pathway associated with reduction of  N -glycosylation and cell attachment, and a consequent activation of the unfolded protein response (UPR) leading to cell death ( Hetz, 2012 ;  Hetz and Papa, 2018 ). In contrast, we indicated two major mechanisms as survival routes: (i) mitochondrial activity rewiring and (ii) autophagy. The model was validated against data obtained from mouse fibroblasts transformed by oncogenic K-ras expression (NIH3T3 K-ras cells) and a human K-ras-mutated breast cancer cell line (MDA-MB-231), both grown in unperturbed and different perturbed conditions. The optimization analysis allowed to automatically search for and detect the combination of perturbations that maximize pro-apoptotic processes in cancer cells for the purpose of guiding the development of novel therapeutic treatments. 2 Materials and methods 2.1 Dynamic fuzzy rules-based modeling A  dynamic fuzzy rules-based model  (DFM) is a computational paradigm to describe and analyze the emergent behavior of heterogeneous complex systems characterized by uncertainty. In DFMs, a linguistic variable and a set of linguistic terms (e.g. Low, Medium and High) are associated with each component of the system to provide a qualitative description of all the possible states that component can assume in time ( Aldridge  et al. , 2009 ;  Yen and Langari, 1999 ). A DFM handles the intrinsic uncertainty of the state of the variables by means of the membership functions associated with each linguistic term. Linguistic variables and terms are used to define a set of fuzzy rules, which provide a qualitative description of the mechanisms (e.g. feedback regulation) driving the overall behavior of the system. Fuzzy rules are conditional statements generally written in the form ‘IF  x  IS  a  THEN  y  IS  b ’. The antecedent of a fuzzy rule is a predicate involving variables (i.e. a system component  x ) and their associated linguistic terms (i.e.  a ). The consequent of a fuzzy rule can either be a fuzzy set, a constant or a function (i.e.  b ) that is assigned to an output variable (i.e.  y ), according to the specific fuzzy inference engine employed. The variables belonging to a DFM can be partitioned into two sets:  outer  and  inner  variables. The set of outer variables contains  input  and  output  variables, which can only appear as antecedents and consequents of fuzzy rules, respectively. Namely, input variables correspond to the components that trigger the dynamic evolution of the system, while output variables represent the components of interest for the analysis of the system (i.e. some experimentally measurable component). On the contrary, inner variables can appear on both sides of fuzzy rules, and they are used to represent mutual regulations among the system components. The state of input variables is set (or perturbed) over time using appropriate user-defined functions, which induce the evolution in time of the whole system. On the contrary, the state of all other variables change as a result of the synchronous application of the fuzzy rules. The inference engine here exploited for the variables’ state update is the zero-order Sugeno method ( Sugeno, 1985 ), in which the outputs of the rules are constant crisp values. To evaluate the next state of a variable over time, this method performs an aggregation of the output values produced by the rules, weighed according to the membership degrees of the antecedents of each rule. Additional information about the Sugeno inference method can be found in  Supplementary Section S2 . After the application of the Sugeno inference to all inner and output variables, their states are updated and the dynamic simulation of the DFM proceeds to the following time step. The process iterates until the simulation time reaches the maximum time step  t max  set by the user. Given a set of linguistic variables, a perturbation  π  can be applied to the DFM as follows. For each variable belonging to  π , a list of time intervals is specified. During each time interval, the Sugeno inference is disabled for that variable and the state update is performed by using a time-dependent arbitrary function. 2.2 FUMOSO: a general-purpose simulator of DFMs FUMOSO is a novel open source and cross-platform software specifically designed and implemented for the definition, simulation and analysis of DFMs (see implementation details in  Supplementary Section S1 ). FUMOSO is provided with an intuitive Graphical User Interface, realized to guide the users through the steps required for the creation of a DFM, that is, the definition of linguistic variables, linguistic terms, output crisp values, membership functions and fuzzy rules (see  Supplementary Fig. S1 ). FUMOSO allows the user to enter all additional information required to simulate the dynamics of a DFM: (i) the simulation interval  [ t 0 , t max ] ; (ii) the functions that drive the dynamics of input variables; (iii) the initial state of all variables; (iv) the perturbation functions (if any). Optionally, for each variable involved in a perturbation  π , the user can specify a list of time intervals in which that perturbation becomes active over that variable. Given a set  O  of linguistic variables chosen as observed targets for the perturbation  π , the effect of  π  on each variable  o ∈ O  is calculated as the difference between the state of  o  at the end and at the beginning of any perturbation interval. The overall effect of  π  on the set of all observed targets in  O , during all time intervals that characterize  π , can be calculated as a user-defined function  F ( π ) . Once the simulation is completed, FUMOSO plots the dynamics of any chosen system component, along with the shape of the membership functions and the weights of the firing rules involved in the update of that component at any arbitrary time step, according to the Sugeno inference method. The analysis of the simulation outcome is facilitated thanks to the possibility of creating groups of components, that is, subsets of variables whose dynamics are shown in the same plot. 2.3 Global optimization of DFMs Given a DFM, FUMOSO allows to automatically explore the emergent behavior of the system in different scenarios, where the state of one or more linguistic variables is varied to simulate the effect of perturbations (e.g. drugs) in obtaining a desired system behavior. To this aim, a perturbation  π  can be defined by the user within a chosen time interval  [ t b , t e ] ⊆ [ t 0 , t max ] , by setting the state of (a subset of) inner variables to a specific value, which either belongs to the term set of each perturbed variable or is equal to the ‘unperturbed’ value. The latter corresponds to the state of each perturbed variable at the current time step, as evaluated by the Sugeno method. Given a set  O  of linguistic variables chosen as observed targets, a user-defined function  F ( π )  can be defined to quantitatively assess the effectiveness of  π  in obtaining the desired system behavior. A sampling time instant  Δ &gt; 0 , such that  t b + Δ ≤ t e , can be set by the user to evaluate  F ( π )  in between  t b + Δ  and  t b .  F ( π )  is used in FUMOSO as the fitness function for the global optimization of the DFM. Global optimization allows to realize an effective and efficient exploration of the huge search space of possible perturbations of a DFM, whose dimension grows exponentially with the number of perturbed variables. To automatically investigate the search space and find out the optimal perturbation, FUMOSO is coupled with Simulated Annealing (SA) ( Kirkpatrick  et al. , 1983 ). The SA procedure integrated in FUMOSO starts from an initial perturbation  π 0 —where all the variables are set to the ‘unperturbed’ state—and explores the neighborhood of potential perturbations. During the  i th iteration of SA, a new putative perturbation  π i ′ = η p ( π i )  is calculated, where  η p  is a neighborhood function that randomly modifies the current perturbation by changing at most  p  variables. To be more precise, the perturbation  π  is modified by randomly changing the state of a selected variable taking a new linguistic term from the set of perturbable values of that variable. In the case of maximization problems, if  F ( π ′ i ) &gt; F ( π i )  then  π ′ i  is accepted as the new perturbation  π i + 1  and the process iterates; otherwise,  π ′ i  is accepted if  exp ( − ( F ( π ′ i ) − F ( π i ) ) / T ) &gt; rnd , where rnd is a random number sampled from the uniform distribution in  [ 0 , 1 ] , and  T  is a parameter that starts from an initial value  T 0  and linearly decreases to 0 during the iterations, progressively reducing the exploration capabilities of SA. The output produced by SA is the perturbation  π  characterized by the best fitness value  F ( π ) . In this work, we considered  T 0 = 0.1  and  p  =   2. 2.4 Experimental protocol Mouse K-ras-transformed NIH3T3-derived cell line 226.4.1 and human breast cancer MDA-MB-231 (obtained from ATCC, Manassas, VA, USA) were cultured in DMEM containing 4 mM  l -glutamine, 100 U/ml penicillin and 100 mg/ml streptomycin and pyruvate free (complete medium), supplemented with 10% newborn calf serum (mouse cells) or 5% fetal bovine serum (human cells). Cells were grown and maintained according to standard cell culture protocols. All reagents for media were purchased from Thermo Fisher Scientific. For analysis, cells were plated at a density of 3000 cells/cm 2  in complete medium. After 18 h cells were washed with phosphate buffer saline (PBS) and incubated in growth medium (time 0) supplemented with 25 or 5 mM glucose (high glucose, HG) or 1 mM glucose (low glucose, LG). Cells were then treated and collected for analysis as described in the figure legends. To measure cell proliferation, harvested cells were counted using the Burker chamber. Where indicated, cell viable count was performed using Trypan Blue Stain 0.4%.  Supplementary Section S3  contains detailed information about the used compounds, flow cytometric analyses, confocal microscopy, adhesion assays, Western blot analysis, transcriptome analysis and statistical analysis of the experimental data. 3 Results 3.1 Fuzzy logic model The DFM defined in this work describes the cellular components that govern death and survival in K-ras cancer cells under progressive glucose depletion. This growth condition, influenced by highly interconnected processes with multiple levels of regulations (i.e. protein–protein interaction and modification, positive and negative feedback, etc.), and able to promote opposite effects on cancer cells (i.e. sensitivity versus resistance to chemotherapy), here is regarded as a complex and heterogeneous system to which apply our DFM approach, to the aim of efficiently identifying novel therapeutic treatments that maximize apoptosis over survival in cancer cells in such growth condition. The model can be depicted as a graph consisting of 25 nodes corresponding to heterogeneous components—e.g. proteins, small molecules and metabolites, biochemical pathways, cellular processes, output phenotypes—while edges between nodes indicate the known positive or negative regulations among these components ( Fig. 1 ). To describe cell death processes as a result of glucose starvation in cancer cells, we considered: Fig. 1. Interaction network of the model of cell death and survival. Yellow circles represent metabolites and ions, green rectangles represent proteins, red rectangles represent pathways or cellular processes, light blue hexagons represent the system phenotypes related to cell death. Positive and negative regulations are pictured as arrows and blunt-ended arrows, respectively. Glucose, Ras-GTP and PKA are the input variables; survival, autophagy, apoptosis and necrosis are the output variables, while the remaining are inner variables. (Color version of this figure is available at  Bioinformatics  online.) the main cellular processes and components involved in energy production, such as glucose, glycolysis, ATP, mitochondria and autophagy; different mitochondrial processes and components, such as mitochondrial potential variation, ROS generation, mitochondrial complex I (CI) activity and B-cell lymphoma 2 (Bcl2) expression and activity; some processes and components related to the ER, such as UPR, Ca 2+ , C/EBP-homologous protein (CHOP) and c-Jun N-terminal kinase (JNK); processes and proteins involved in cellular adhesion, such as hexosamine biosynthesis pathway (HBP),  N -glycosylation, attachment and Src; proteins and processes involved in the regulation of cell death and survival mechanisms, such as Ras, extracellular-signal-regulated kinase (ERK), death-associated protein kinase 1 (DAPK), Bcl2, Beclin1 (BCN1), caspase 3 (Casp3), protein kinase A (PKA) and the phenotypes related to apoptosis, necrosis and survival of cells; the protein Ras-GTP to mimic the hyperactivation of K-ras inside tumor cells displaying both the Warburg effect and PKA as a key node involved in cancer cells survival to glucose starvation. Since not all of these components (e.g. apoptosis, survival, UPR, HBP, etc.) can be formally represented by a quantitative variable, and some interactions cannot be specified by means of kinetic reactions, we relied on fuzzy logic modeling to handle this heterogeneity and the lack of parameters by defining suitable linguistic variables that represent general concepts like concentration, activation or presence of a component. For each model component, we defined a linguistic variable and an associated set of linguistic terms to describe all the possible states of the variable (see, e.g.  Supplementary Fig. S2 , left side, for the ROS component shown in  Fig. 1 ). To formalize the interactions existing among the components considered in the model, we exploited literature and expert knowledge. The complete list of linguistic variables and the corresponding output crisp values of their associated linguistic terms are available in  Supplementary Table S1  and  Table S2 , respectively. A set of fuzzy logic rules was defined for each linguistic variable, for a total of 252 rules (see Supplementary Material). As an example,  Supplementary Figure S2 , right side, shows the fuzzy rules that describe the regulations acting on the ROS component given on the left side, according to the linguistic terms (e.g. Low, Medium and High) defined for the three components CI, DeltaPsi and ROS. Three variables in the DFM of cell death and survival in glucose depletion represent the input of the system, namely, glucose, Ras-GTP and PKA: (i) Glucose is formally regulated by a custom update function that simulates glucose consumption; (ii) Ras-GTP is constantly kept to the High state to mimic the oncogenic hyperactivation of K-ras in cancer; (iii) PKA is either kept to the High or Low states to mimic the ability of cancer cells either to survive or die under glucose starvation (see  Supplementary Section S4 ). Apoptosis, necrosis and survival represent the observable output variables of the DFM. 3.2 Model validation The DFM of death and survival of K-ras cancer cell grown on progressive glucose depletion, defined on the basis of empirical data and the manual curation of literature, was experimentally validated against data obtained from cell cultures grown in different glucose availability, and in presence or absence of different protein and process modulating molecules (see  Supplementary Section S5 ). Experimental data were obtained on NIH3T3 K-ras cells and a human model of breast cancer, the MDA-MB-231 cell line, which carries the oncogenic K-ras gene. The two cell lines were used in an exchangeable manner, since previously published results indicated that they have a comparable behavior as regard to numerous parameters associated with growth in 25 or 5 mM glucose (HG growth), and in 1 mM glucose (LG growth) ( Gaglio  et al. , 2011 ; Palorini  et al. ,  2013a, b ,  2016 ). 3.3 Perturbation analysis A perturbation analysis of the DFM of programmed cell death and survival was performed in order to identify potential stimuli leading to both a reduced survival and an increased death by apoptosis in cancer cells. The total number of possible perturbations for the DFM defined in this work is  3 6 · 4 9 = 191 , 102 , 976 , where 6 and 9 are the numbers of perturbable variables that can assume 3 and 4 possible states, respectively (see  Supplementary Table S3 ). Since an exhaustive search of this huge space of perturbations would not be feasible, we exploited SA to automatically infer the minimal set of perturbations that allows to maximize apoptotic death in K-ras cancer cells. The optimization analysis was carried out both in the case of low and hyperactivated states of PKA, during different glucose availability. Among the solutions identified by SA, a set of promising single/double perturbations was selected and tested in the laboratory to assess their effectiveness in inducing death by apoptosis in MDA-MB-231 cells. A list of perturbations found by SA—validated either in this work or confirmed by previous experimental evidences—is given in  Supplementary Tables S4 and S5 , ranked by their fitness values. Details about the fitness function exploited to evaluate the effectiveness of the perturbations are given in  Supplementary Section S6 . 
 Figure 2  and  Supplementary Figure S10  show the comparison between the simulated dynamics and the experimental data for  single  perturbations predicted by SA. In particular, we assessed the effects of UPR activation ( Fig. 2 ) achieved by using 10 nM thapsigargin (thap), or CI inhibition ( Supplementary Fig. S10 ) achieved by using 10 nM rotenone (rot) or 20 nM piericidin (pier) in high and low glucose availability, respectively (the experimental scheme for both perturbations is shown in  Supplementary Fig. S9c ). Both perturbations were chosen taking into account their rank and the existence of previous data indicating that the pharmacological over-activation of UPR pathway, as well as CI inhibition, can lead to cancer cell death ( Cubillos-Ruiz  et al. , 2017 ;  Galluzzi  et al. , 2013 ). For both perturbations, high or low activation of PKA was also experimentally analyzed. In particular, to avoid the endogenous activation of PKA under our experimental conditions, low PKA was achieved by cell treatment with the known PKA inhibitor H89 ( Chijiwa  et al. , 1990 ) (experimental details are available in  Supplementary Fig. S9 ).
 Fig. 2. Assessment of the effects of perturbations (UPR activation) predicted by the global optimization algorithm. ( a ,  b ) Simulation outcome of the three main model output (apoptosis, necrosis and survival) upon UPR activation, either in (a) PKA Low state or (b) PKA High state. The perturbation was applied from time  t b  = 0 to the end of the simulation, and evaluated after  Δ = 0.13  a.u. (shaded area, see also  Supplementary Section S6 ). ( c ) MDA-MB-231 cells, grown in HG, were daily treated with 10 μM FSK mimicking the PKA High state, or 5 μM H89 mimicking the PKA Low state and, upon 24 h, also with 10 nM thap (single treatment). Samples were evaluated for cell death at 48 and 72 h post-treatment by using trypan blue exclusion method. The experimental scheme is shown in  Supplementary Figure S9c . All data represent the average of at least three independent experiments (±SD); * P  &lt; 0.05 (Student’s  t -test). (Color version of this figure is available at  Bioinformatics  online.) It is interesting to notice that the model properly predicted the experimental data. Indeed, an enhanced cell death was observed in the simulations (orange line for apoptosis and magenta line for necrosis in  Fig. 2a and b ) as well as in the experimental data upon both treatments ( Fig. 2c ). This increase is evident when moving from a high availability of glucose (left side of the simulation plots) to a situation of glucose starvation (right side of the simulation plots), and especially in the PKA Low state with respect to the PKA High state, suggesting an important role of PKA in cancer cell survival in acute UPR activation or in glucose starvation. These computational results are consistent with the experimental measurements, as represented by the graph bars in  Figure 2c , which show a higher level of cell death in glucose starvation (72 h), with the fold change being higher when PKA is not activated. Similar considerations can be done for the condition of chronic CI inhibition, as shown in  Supplementary Figure S10 . Analogously,  Figure 3  shows the comparison between the simulated dynamics and the experimental data for the  double  perturbations predicted by SA. In particular, it shows the effects of UPR activation coupled with autophagy inhibition ( Fig. 3a–c ),  N -glycosylation and HBP inhibition ( Fig. 3d–f ),  N -glycosylation and autophagy inhibition ( Fig. 3g–i ). Both the simulation outcomes and the experimental data show an increase in cell death, especially when moving towards a state of glucose starvation, and the protective role of PKA. This result is evident by comparing, for example, the final states reached in each condition by apoptosis (orange line) and necrosis (magenta line) in  Figure 3a, d  and  g , with the states reached by the same variables in  Figure 3b ,  e  and  h . These computational results are consistent with the experimental measurements shown by the graph bars in  Figure 3c ,  f  and  i .
 Fig. 3 Assessment of the effects of perturbations (UPR activation and autophagy inhibition) predicted by global optimization. ( a ,  b ) Simulation outcome of the three main model output (apoptosis, necrosis and survival) upon UPR activation and autophagy inhibition, either in (a) PKA Low state or (b) PKA High state. The perturbation was applied from time  t b  = 0 to the end of the simulation, and evaluated after  Δ = 0.13  a.u. (shaded area, see also  Supplementary Section S6 ). ( c ) MDA-MB-231 cells, grown in HG, were daily treated with 10 μM FSK mimicking the PKA High state and, upon 24 h, also with 10 nM thap and 20 μM chloroquine (CQ) (single treatment of both). Samples were evaluated for cell death at 48 and 72 h post-treatment by using trypan blue exclusion method. The experimental scheme is shown in  Supplementary Figure 9d . ( d ,  e ) Simulation outcome of the three main model output (apoptosis, necrosis and survival) upon HBP and  N -glycosylation inhibition, either in (d) PKA Low state or (e) PKA High state. ( f ) MDA-MB-231 cells, grown in HG, were daily treated with 10 μM FSK mimicking the PKA High state and, upon 24 h, also with 1 μM aza and 50 ng/ml tuni (single treatment of both). Samples were evaluated for cell death at 48 and 72 h post-treatment by using trypan blue exclusion method. The experimental scheme is shown in  Supplementary Figure 9d . ( g ,  h ) Simulation outcome of the three main model output (apoptosis, necrosis and survival) upon  N -glycosylation and autophagy inhibition, either in (g) PKA Low state or (h) PKA High state. ( i ) MDA-MB-231 cells, grown in HG, were daily treated with 10 μM FSK mimicking the PKA High state and, upon 24 h, also with 50 nM tuni and 10 μM CQ (single treatment of both). Samples were evaluated for cell death at 48 and 72 h post-treatment by using trypan blue exclusion method. The experimental scheme is shown in  Supplementary Figure 9d . All data represent the average of at least three independent experiments (±SD); * P  &lt; 0.05, ** P  &lt; 0.01 (Student’s  t -test). (Color version of this figure is available at  Bioinformatics  online.) To analyze the combinatory effect on cell survival we used, when possible, sub-toxic concentrations of the compounds, previously determined experimentally on this cell model (see, e.g. tuni and aza). In addition, in these sets of validation experiments, we evaluated also whether the model could predict the endogenous PKA behavior, since no H89 inhibitor was used (experimental scheme in  Supplementary Fig. S9d ). Combination effects of selected compound pairs exceeded the effects of single compounds for the majority of combinations, and well fitted with the model outcome also in the absence of exogenous PKA inhibition. Thus, these results corroborated the capability of the DFM, coupled with the optimization algorithm, in predicting the system’s response in perturbed conditions. 4 Conclusion Complex biological systems are characterized by emergent, non-linear dynamic behaviors that arise from negative and positive feedback regulations among a huge number of different molecules and processes in cells. The elucidation of the mechanistic interactions that govern the correct functioning of cells or that, in contrast, can induce the onset of a disease, is an extremely challenging task from both the experimental and the computational perspectives. This is strikingly true when the biological system under investigation is poorly understood, or it cannot be easily subject to accurate experimental measurements, or the interplay between its intrinsic negative and positive control mechanisms is so complex that the system can be analyzed only by considering a subset of (usually, homogeneous) components, such as metabolic traits, gene expression, etc. In this context, we presented a novel computational method able to address two open issues in the field of biochemical system modeling, that is, taking into account the heterogeneous nature of all biological systems, and dealing with the lack of quantitative parameters that generally prevents the precise characterization of most cellular components and processes. This modeling approach may be adopted to incorporate data generated from different ‘platforms’ using knowledge-based rules (e.g. transcriptional and proteomic data, microscopy imaging), and can integrate multiple data types together, i.e. signaling protein activation/inhibition, post-translational modifications, small molecule variation, end-process activation or inhibition, etc. As such, our computational method not only solves some relevant issues related to the modeling and dynamical simulation of heterogeneous systems, but it is also able to provide valuable predictions that facilitate our understanding in controlling cellular systems. Our method, based on the coupling between fuzzy logic modeling and a global optimization algorithm, allows for the definition of models in a readable and simple format, through the use of linguistic variables. Here, in particular, we showed that our approach was able to predict the behavior of K-ras-transformed cells grown either under progressive glucose depletion or in different perturbed conditions, as well as to identify possible novel cancer therapeutic treatments. The model simulations showed that targeting the HBP, its downstream route controlling protein  N -glycosylation, the ER processes or the mitochondrial CI, alone or in combination with PKA inhibition, cause a significant increase in cancer cell death. Noteworthy, all computational results were experimentally substantiated here, or matched literature results. For instance, it has been recently reported that HBP inhibition in HG growth induces cell death in breast cancer cells and xenograft mice ( Ricciardiello  et al. , 2018 ), as also shown in this work by computational and experimental data. Moreover, we demonstrated that inhibition of  N -glycosylation has a profound effect on breast cancer cell survival, validating other recent observations ( Chiaradonna  et al. , 2018 ;  Serrano-Negrón  et al. , 2018 ;  Wang  et al. , 2018 ;  You  et al. , 2018 ). In both cases, the model correctly predicted the outcomes of new experiments that were not used in model construction, thus confirming the reliability of our computational method. It is noteworthy that the relevance and the validity of our method is supported by its performance in predicting not only recognized cell death-inducing stimuli, but also unrecognized stimuli equally leading to cell death. For instance, a sub-toxic amount of tuni, coupled with both a sub-toxic amount of the HBP inhibitor aza or the autophagic inhibitor chloroquine, enhances cancer cell death. Remarkably, both treatments are strongly attenuated by exogenous PKA stimulation, implying the involvement of this pathway in ER stress response, at least in our experimental conditions. Confirming results were shown also about the protective role of PKA upon ER stress induction by thapsigargin that, in combination with PKA inhibition, induces cancer cell death, an effect that is strongly impaired by exogenous activation of PKA. Previous data indicated that PKA activation protects cancer cells from death induced by glucose starvation ( Palorini  et al. , 2013b ,  2016 ). Here, we revealed a protective role of PKA in cancer cells under acute ER stress. While this protective mechanism has been shown to be active in mouse embryonic fibroblasts ( Aguileta  et al. , 2016 ) and hepatocytes ( Li  et al. , 2015 ), to the best of our knowledge it has never been described in cancer cells, further supporting the predictive value of our method. Interestingly, also simultaneous CI and PKA inhibition induces massive cancer cell death, an effect that is prevented by PKA activation. Therefore, the model also predicted that PKA activation is involved in mitochondrial CI function and, more in general, in OXPHOS activity, corroborating previous results ( García-Bermúdez  et al. , 2015 ;  Lark  et al. , 2015 ;  Ould Amer and Hebert-Chatelain, 2018 ;  Papa  et al. , 2012 ). Altogether, our results provide the first evidence of a protective role for PKA against several treatments mimicking cellular stress conditions, such as ER stress,  N -glycosylation inhibition, mitochondrial CI inhibition and glucose starvation in cancer cells. These findings will hopefully pave the way for the use of new and more specific PKA inhibitors in cancer therapy. We further underline that the coupling of fuzzy logic modeling with optimization algorithms is a promising tool to uncover new potential therapeutic targets by assessing, in an automatic way, the response of the system to an extensive number of perturbations, therefore both reducing the costs and facilitating the design of laboratory experiments. The dynamic nature and predictive power of DFMs could prove useful in assessing the effects of different types of perturbations, such as drugs or mutations, on the behavior of the system under examination in many application fields (e.g. medicine, pharmacology, etc.). In the future, this novel computational method could be exploited to preliminary assess the effects of FDA-approved drugs, especially in combination with other metabolic drugs, on the survival of resistant cancer cells. The great flexibility of fuzzy logic could also be exploited to integrate different model formalisms to define complex hybrid models, able to both represent different layers of biological complexity (at the functional, temporal, or phenomenological level) and leverage precise kinetic information when available ( Spolaor  et al. , 2019 ). Namely, with this novel computational method we aim at opening the way to filling the gap between quantitative (mechanism-based) models and qualitative (logic-based) models, in order to simultaneously exploit the peculiar advantages provided by each modeling approach. Funding This work was supported by grants from the Italian Ministry of University and Research (MIUR) [SYSBIONET-Italian ROADMAP ESFRI Infrastructures to L.A. and F.C., PRIN2008 to F.C.] and A.I.R.C. IG2014 [Id.15364 to F.C.]; SYSBIONET-Italian ROADMAP ESFRI Infrastructures to G.V. and H.D.V. (fellowships); Italian Ministry of University and Research to R.P. and F.R. (fellowships); CAPES 9281-13-4 to H.D.V. (fellowship); and A.I.R.C. IG2014 [Id.15364 to G.V. (fellowship)]. Financial support from MIUR through grant ‘Dipartimenti di Eccellenza-2017’ to University of Milano-Bicocca, Department of Biotechnology and Biosciences is also acknowledged. 
 Conflict of Interest : none declared. Supplementary Material btz868_Supplementary_Data Click here for additional data file. </Body>
  </Article>
</Articles>
