<Articles>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>TREAT: a bioinformatics tool for variant annotations and visualizations in targeted and exome sequencing data</Title>
    <Doi>10.1093/bioinformatics/btr612</Doi>
    <Authors>Asmann Yan W., Middha Sumit, Hossain Asif, Baheti Saurabh, Li Ying, Chai High-Seng, Sun Zhifu, Duffy Patrick H., Hadad Ahmed A., Nair Asha, Liu Xiaoyu, Zhang Yuji, Klee Eric W., Kalari Krishna R., Kocher Jean-Pierre A.</Authors>
    <Abstract>Summary: TREAT (Targeted RE-sequencing Annotation Tool) is a tool for facile navigation and mining of the variants from both targeted resequencing and whole exome sequencing. It provides a rich integration of publicly available as well as in-house developed annotations and visualizations for variants, variant-hosting genes and host-gene pathways.</Abstract>
    <Body>1 INTRODUCTION Next-generation sequencing offers the promise of scientific discovery with the challenge of results interpretation ( Schuster, 2008 ). One experiment such as exome sequencing can generate tens of thousands of single nucleotide variants (SNVs) and small insertions or deletions (INDELs), which must be elucidated in the search for disease associated mutations ( Ansorge, 2009 ;  Metzker, 2010 ). Whole exome sequencing is an application of NGS that has been successfully used to identify disease-associated variants in several monogenic disorders ( Gilissen  et al. , 2010 ;  Lupski  et al. , 2010 ;  Ng  et al. , 2009 ,  2010 ) and complex diseases ( Bonnefond  et al. , 2010 ;  Harbour  et al. , 2010 ). While these studies demonstrated the power of NGS, they also highlighted the challenge of efficiently sifting through thousands of variants to identify a subset that is potentially clinically relevant. Bioinformatics solutions are beginning to be released that address this challenge and facilitate filtering and interpretation of human sequence variation data ( Nix  et al. , 2010 ;  Sana  et al. , 2011 ;  Shetty  et al. , 2010 ;  Wang  et al. , 2010 ). We developed TREAT to extend the functionality of these tools and directly integrated structured and sortable formats with embedded hyperlinks to sequence alignment, gene specificity and gene pathway visualizations. In addition, to enable broad accessibility, we have fully deployed TREAT to the Amazon Cloud. TREAT is optionally offered as part of a complete workflow for exome or targeted sequencing, providing users with a convenient method for integrated sequence alignment, mutation detection and results interpretation. We believe this tool offers investigators with an accessible and convenient method for annotating and visualizing sequencing data and a means of efficiently identifying variants of interest. 2 METHODS AND RESULTS 2.1 Variant annotation TREAT provides four categories of variant annotations ( Supplementary Figure S1 ): (i) the general variant annotations which provide the physical locations, and the dbSNP IDs and allele frequencies of known variants from HapMap and 1000 Genome Pilot Project in Caucasian (CEU), Yoruban (YRI) and East Asian (CHB/JPT) populations; (ii) sample-specific read depths supporting A, C, G, T bases at each variant position, and the quality scores for base calls and read mappings. These annotations are only available when the users choose to use TREAT for read alignment and variant calling; (iii) publically available annotations from SIFT ( Kumar  et al. , 2009 ) and SeattleSeq ( http://gvs.gs.washington.edu/SeattleSeqAnnotation/ ) that include variant classifications (synonymous, missense, non-sense, frame-shift, etc.) and the predictions of the functional impact of the variants from SIFT and PolyPhen2 and (iv) in-house developed novel annotations including the tissue expression specificity measures for variant-hosting genes (detailed in  Supplementary Data  S2), and the identification of variants adjacent to exon–intron boundaries that potentially disrupt known splice-sites. An additional novel function of TREAT is the hyperlinks of each variant hosting gene to its associated KEGG pathway(s) ( http://www.genome.jp/kegg ) and Gene Ontology terms ( http://www.geneontology.org/ ). 2.2 Reporting and visualization TREAT automatically creates output in one easy-to-navigate HTML page, which provides the project description, QC reports, target coverage and sequencing depth information, descriptions of the annotations provided by TREAT, and links to the SNV and INDEL reports. The Microsoft Excel formatted SNV and INDEL reports provide row-based synopses of per-variant annotation. Each variant is hyperlinked to Integrative Genomics Viewer (IGV) ( Robinson  et al. , 2011 ) for the visualization of read alignments and variant calling information at the variant position. The functions of the variant hosting genes are illustrated via hyperlinks to the KEGG pathways and Gene Ontology terms, and the tissue expression specificity graph. 2.3 Access TREAT is deployed in two formats, a standalone annotation application and an integrated version for an end-to-end analysis of exome or targeted sequencing data. The standalone annotation tool takes the list of called variants as input files and allows users the flexibility of generating the variants using alignment and variant calling tools of their own choosing. The integrated version accepts either FASTQ or BAM files as input files and carries out sequence alignment using BWA ( Li and Durbin, 2009 ) or Bowie ( Langmead  et al. , 2009 ), local sequence re-alignment (GATK;  McKenna  et al. , 2010 ) and variant calling (GATK or SNVMix;  Goya  et al. , 2010 ), which provides users with a convenient solution to their informatics needs. Both TREAT versions can be downloaded for local runs, or can be launched on the Amazon Elastic Compute Cloud (EC2) ( http://en.wikipedia.org/wiki/Amazon_Elastic_Compute_Cloud ) using Amazon Machine Images provided at our Website. The Machine Images are loaded with all the open-source tools and necessary annotation files for the direct execution of TREAT. The run time and cost estimate of TREAT Cloud version are provided in the  Supplementary Data . 3 DISCUSSIONS We have developed a bioinformatics tool, TREAT, which addresses the current challenges in analyzing and interpreting targeted and whole exome sequencing data. The annotations provided by TREAT have been carefully evaluated and selected from a pool of available open source tools and databases, and complimented by additional in-house developed annotations (details at the TREAT website). The variant reports in Excel format integrate the visualizations of the sequence alignment at variant positions, pathways and expression specificity of the variant hosting genes via clickable hyperlinks for each reported INDELs and SNVs. In addition, the summary of the targeted resequencing results is stored in a centralized HTML report with links to the TREAT website, the targeted region coverage report and the read QC report, the description of the TREAT workflow, and links to the website of the annotation tools and databases. For maximum flexibility, two versions of TREAT were implemented: an annotation only version, and a version integrating read alignment, variant calling and annotations. Both versions can be downloaded as local installations or as Amazon Cloud images which makes TREAT available for users with no access to local bioinformatics infrastructures. By targeting all user groups and enabling rapid integration of emerging analytic methods, we believe that TREAT provides a sustainable NGS analytic workflow with wide applicability to the research community. We plan to continue adding new functionality and features to TREAT to make it a comprehensive tool for targeted and exome analysis. These include the development of an in-house variant database that collects all variants detected from hundreds of individuals with various types of diseases using exome and whole genome sequencing. This database will provide critical annotations whether the observed variants are truly ‘novel’ or disease specific. In addition, we are in the process of making TREAT applicable to whole genome sequencing data analysis, this would require adding annotation tracks for non-coding regions such as the conservations and regulatory domains. In summary, the rich set of annotations provided by TREAT, the easy to use, centralized HTML summary report, and the Excel-formatted variant reports with hyperlinked visualization utilities enable the filtering of detected variants based on their functional characteristics, and allow the researchers to navigate, filter and elucidate tens of thousands of variants to focus on potential disease-associated variant(s). 
          Supplementary Material 
          
             
                Supplementary Data 
             
             
             
          
       </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Gene expression models based on transcription factor binding events confer insight into functional cis-regulatory variants</Title>
    <Doi>10.1093/bioinformatics/bty992</Doi>
    <Authors>Shi Wenqiang, Fornes Oriol, Wasserman Wyeth W, Valencia Alfonso</Authors>
    <Abstract/>
    <Body>1 Introduction Understanding the functional roles of genetic variants in human disease is a fundamental challenge in medical genetics. Whole genome sequencing (WGS) enables clinicians to systematically seek variants that contribute to disease phenotype. Current clinical approaches focus primarily on the ∼2% of the genome coding for proteins, yet up to 88% of disease-related variants in genome-wide association studies are located within noncoding regions ( Hindorff  et al. , 2009 ). However, predicting the functional impact of noncoding variants remains a challenge. With the rapid accumulation of WGS data, there is a recognized need for bioinformatics methods that provide mechanistic insights into noncoding variants. Gene expression is a key intermediate phenotype for genetic studies. Substantial progress has been made on detecting statistical relationships between variants (single-nucleotide variants and short indels) and gene expression levels. These expression quantitative trait loci (eQTL) are enriched in regulatory regions, including promoters, enhancers and transcription factor (TF) bound regions, revealing the potential functional mechanisms of these variants ( Lappalainen  et al. , 2013 ;  The Encode Project Consortium, 2012 ). Partially based on the success of eQTL analysis, regression-based models trained on common variants (minor allele frequency, MAF ≥ 0.05) proximal to genes have been developed to predict gene expression levels ( Gamazon  et al. , 2015 ;  Manor and Segal, 2013 ). Such correlative approaches are useful, yet they lack the resolution to direct researchers to specific functional variants for two reasons. First, functional variants are hard to infer in association studies due to linkage disequilibrium (LD) between variants ( Farh  et al. , 2014 ). Second, uncommon variants (minor allele frequency, MAF &lt; 0.05) are excluded from most association studies, yet, rare variants (MAF &lt; 0.01) are often causal for familial genetic disorders ( Gibson, 2012 ;  Lappalainen, 2015 ) and have been hypothesized to contribute to human complex traits ( Bomba  et al. , 2017 ). Both reasons can be considered from the perspective of model feature engineering (i.e. how to incorporate model features). Current models use genotypes as predictors and defer the annotation of variant function until a model is constructed. To focus upon function, an alternative choice is to introduce model features emphasizing regulatory regions, which should increase the biological insights of gene expression models. It has been hypothesized that altered TF-binding events are a central mechanism by which  cis -regulatory variants impact gene expression ( Pai  et al. , 2015 ). TFs bind to specific locations in the genome, which can be identified experimentally by methods such as chromatin immunoprecipitation combined with sequencing (ChIP-seq). Machine learning approaches coupled to extensive TF ChIP-seq data have enabled predictions of TF-bound regions across the genome ( Zhou and Troyanskaya, 2015 ). Altered or disrupted TF-binding events have been associated with various diseases, including osteoarthritis ( Dodd  et al. , 2013 ), type-2 diabetes ( Claussnitzer  et al. , 2015 ) and colorectal cancer ( Wang  et al. , 2016 ). Recently, the compilation of altered TF-binding events has increased, and computational models have emerged to predict such events ( Chen  et al. , 2016 ;  Shi  et al. , 2016 ). However, bioinformatics approaches that quantify the relationship between altered TF-binding events and personalized gene expression levels remain elusive. To bridge this gap, we have developed TF2Exp models to infer relationships between personalized gene expression and altered TF-binding events caused by  cis -regulatory variants. We have explored the utility of TF2Exp in answering four important questions: (i) are alterations of TF-binding events predictive of personalized gene expression levels?; (ii) what are the characteristics of the functional altered TF-binding events?; (iii) do TF2Exp models perform as well as the state-of-the-art variant-based models?; and (iv) are TF2Exp models able to infer functional variants in LD and uncommon variants? Our results show that TF2Exp models successfully predict the alteration of gene expression for over three thousand genes, with an average performance comparable to that of models based on variants. Our framework systematically reveals the mechanism by which  cis -regulatory variants impact gene expression, providing unique interpretive capacity for future human genetic studies. 2 Materials and methods 2.1 Quantifying gene expression from RNA-seq data Lymphoblastoid cell line (LCL) RNA-seq and variant-calling data for 358 individuals from European populations were downloaded from the GEUVADIS project ( Lappalainen  et al. , 2013 ) and the 1000 Genomes Project ( 1000 Genomes Project Consortium  et al. , 2015 ) ( Supplementary Note s). Individuals covered 4 populations, including 89 Utah residents with Northern and Western European ancestry (CEU), 92 Finns (FIN), 86 British (GBR) and 91 Toscani (TSI). For each population, we built sex-specific transcriptomes in which SNP positions with MAF ≥ 0.05 were replaced by N (representing any of the four nucleotides A, C, G, T) using scripts from ( Grubert  et al. , 2015 ). RNA-seq data were processed using Sailfish (version 0.6.3) ( Patro  et al. , 2014 ), and expression levels of each gene were quantified as transcripts per million reads. The resulting expression data were normalized via multiple steps, including standardization, variation stabilization, quantile normalization and batch effects removal (i.e. population and gender, and 22 hidden covariates) by PEER ( Stegle  et al. , 2012 ) ( Supplementary Fig. S1 ). Any gene that was either on the sex chromosomes or showed near-zero variance in expression levels was removed, leaving 16 354 genes for model training. 2.2 Associating TF-binding events to genes using Hi-C data We obtained Hi-C proximity scores measuring physical interactions between DNA regions (Hi-C fragments) in GM12878 cells (an LCL) from ( Grubert  et al. , 2015 ). The average size of Hi-C fragments was 3.7 kb ( Grubert  et al. , 2015 ). For each gene, the proximal region was defined as the ±2 kb region centered at the start position of that gene [outermost transcript start position annotated by Ensembl ( Aken  et al. , 2016 ) in genome assembly GRCh37]. Proximal regions were extended to include any overlapping Hi-C fragments, and extended proximal regulatory regions have a median length of 11.3kb ( Supplementary Fig. S2 ). Within 1 Mb of gene body (as delimited by the outermost transcript start and end), distal regulatory regions were defined as Hi-C fragments interacting with (proximity score &gt; 0.4), but not overlapping, the proximal region of that gene. The median distance between distal regulatory regions and TSSs is 300.0kb ( Supplementary Fig. S3 ). Uniformly processed GM12878 DNase I hypersensitivity sites (DHSs) and ChIP-seq peaks for 77 TFs were downloaded from the ENCODE project ( The Encode Project Consortium, 2012 ). As DHS is a general indicator of TF binding ( Neph  et al. , 2012 ), DHSs are referred to as part of the set of TF ChIP-seq peaks within this manuscript for editorial convenience. A TF-binding event was associated to a gene if the ChIP-seq peak overlapped the proximal or a distal regulatory region of the gene. The resulting associations between genes and TF-binding events derived from GM12878 cells were used as the reference for all studied individuals. 2.3 Predicting sequence variation impact on TF-binding events Variant-calling data of each individual was downloaded from the 1000 Genomes Project (release 20130502) ( 1000 Genomes Project Consortium  et al. , 2015 ). We only considered single nucleotide variants and small indels (&lt;100bp). For each individual, the impact of a variant within a TF-binding event was evaluated as the binding score difference between the altered and reference alleles, as determined by the corresponding DeepSEA (v0.93) TF-binding model trained on GM12878 data ( Zhou and Troyanskaya, 2015 ). DeepSEA is a deep learning-based tool that accurately predicts the binding probability of a TF to any DNA sequence in certain cell types. To allow for the analysis of multiple variants within a TF-binding event, we modified DeepSEA to calculate the binding score of each allele using the 1100 bp region centered at the ChIP-seq peak-max position (the original code would center the 1100 bp region at each variant). Score differences of multiple variants within the same TF-binding event were aggregated to represent the overall alteration of that event. TF ChIP-seq peaks with multiple peak-max positions and overlapped peaks from the same experiment were split at the center of each pair of neighboring peak-max positions. At heterozygous positions, the binding score difference was divided by two. Lastly, we calculated the LD between variants across studies individuals using plink2 ( Chang  et al. , 2015 ). 2.4 Quantitative models of gene expression 
 LASSO regression on gene expression:  We developed a regression model to predict the expression level of a gene using altered TF-binding events associated with that gene based on the following equation:
 (1) Y i ∼ ∑ k = 1 n β k Δ T F i , k + ϵ 
where  Y i  is the expression levels of gene  i  across the studied individuals,  n  is the number of TF-binding events associated with gene  i ,  Δ T F i , k  is the alteration of TF-binding event  k  across the studied individuals and β k  is the effect size of TF-binding event k. In  equation (1) ,  Y i  is the response and  Δ T F i , k  is the input feature for the LASSO regression model, which was trained using the R glmnet package ( Friedman  et al. , 2010 ) on a training set of 358 LCLs. Features with near zero variance were filtered out before model training using the caret package ( Kuhn, 2015 ). Model performance was evaluated by 10-fold nested cross-validation, in which internal folds identified the optimal hyper-parameter lambda, and outer layers tested the model performance. Model performance was measured as the square of the correlation between predicted and observed expression levels (R 2 ). The trained models would select a subset of TF-binding events as key features of which effect sizes were not zero. When Hi-C proximity scores were used as the prior to select features, the prior (penalty.factor in the glmnet function) was set to ‘1 – proximity score’. 
 Defining TF-TF interactions:  For TFs known to interact in the BioGrid database ( Chatr-Aryamontri  et al. , 2015 ), we created interaction terms between pairs of TF-binding events (one from each TF) if they satisfied one of the following conditions: 1) two binding events overlapped by at least 200 bp; or 2) their regulatory regions were reported to interact in the Hi-C data. 
 Variant-based models:  For each gene, we trained regression models based on multiple variants to predict the expression level of that gene following the procedure as in the work of Gamazon  et al.  ( Gamazon  et al. , 2015 ). We only considered common variants (single-nucleotide variants and short indels with MAF ≥ 0.05) within 1 Mb of gene body regions. The regression formula for variant-based models is as follows:
 Y i ∼ ∑ k = 1 n β k X i , k + ϵ 
where  Y i  is the expression levels of gene  i  across studied individuals,  n  is the number of variants and  X i , k  is the number of minor alleles of  v a r i a n t i , k . 2.5 External validation with expression data For external validation of TF2Exp models on microarray data, we relied on expression levels of 15 997 Ensembl genes for LCLs of 80 CEU, 87 Chinese (CHB) and 89 Japanese (JPT) individuals ( Stranger  et al. , 2012 ). For these individuals, variant data was retrieved from the 1000 Genomes Project. We applied TF2Exp models to predict gene expression levels from potentially altered TF-binding events based on the variant data, and compared these predictions with the gene expression levels reported from the microarray. To test TF2Exp models on GTEx data ( GTEx Consortium, 2017 ), called genotype variants and expression levels derived from GTEx project were obtained from dbGAP (release phs000424.v7.p2). For each tissue, expression data were normalized similarly to LCLs (see section ‘Quantifying gene expression from RNA-seq data’): standardization of the data, followed by quantile normalization and batch effects removal (i.e. gender and 20 hidden covariates) by PEER ( Stegle  et al. , 2012 ). GTEx eQTLs data (version 6) were downloaded from  https://gtexportal.org/home/datasets . 3 Results 3.1 TF2Exp: regression models to predict the impact of altered TF binding on gene expression levels We developed TF2Exp, a gene-based computational framework to assess the impact of altered TF-binding events on gene expression levels ( Fig. 1 ). As detailed in Section 2, variant-calling data (single nucleotide variants and small indels) and gene expression data for 358 lymphoblastoid cell lines (LCLs) were obtained from the 1000 Genomes ( 1000 Genomes Project Consortium  et al. , 2015 ) and GEUVADIS projects ( Lappalainen  et al. , 2013 ). Moreover, TF-bound regions for 77 distinct TFs and DNase I hypersensitivity sites (DHSs) were obtained from the ENCODE project for the GM12878 LCL ( The Encode Project Consortium, 2012 ). TF-binding events (inclusive of DHSs) were associated to a gene if they overlapped either the proximal or distal regulatory region of that gene (see Section 2). The impact of each single variant within a TF-binding event was scored using DeepSEA ( Zhou and Troyanskaya, 2015 ), and multiple variants within the same TF-binding event were aggregated to generate an overall alteration score of that TF-binding event in each individual. On average, each gene had 420.0 altered TF-binding events within 36.6 regulatory regions (both proximal and distal) across the 358 individuals. Based on the computed alteration scores of TF-binding events in each individual, a regression model was trained by LASSO ( Friedman  et al. , 2010 ) for each gene to predict expression levels and to identify the key contributing TF-binding events. For users seeking to apply the trained models, note that TF2Exp models only require genotype data as input and will output the predicted expression changes for the corresponding genes. The TF2Exp framework is publicly available at  https://github.com/wqshi/TF2Exp .
 Fig. 1. Overview of the TF2Exp framework. ( A ) Infer regulatory regions and TF-binding events of each gene based on the reference cell line (GM12878). Distal regulatory regions are associated to a gene according to Hi-C data. TF-binding events on the proximal or distal regulatory regions of a gene are assigned to that gene. ( B ) Score the alteration of TF-binding events based on the overlapped variants for each individual. ( C ) Train regression models for each gene across the collected individuals 3.2 TF2Exp predicts the expression levels for a subset of genes We successfully trained TF2Exp models for 15 887 genes. Average model performance (R 2 ) by 10-fold cross-validation was 0.049, with most models having low predictive power ( Fig. 2 ). To assess the impact of random noise in the model training process, we set up control models in which gene expression levels were shuffled across individuals while preserving TF binding features. Control models achieved an average R 2  of only 3.6 × 10 −5  ( Fig. 2 ), indicating that the signal captured by TF2Exp models is not random. Repeating the randomization process 20 times for the genes on chr1 showed mean performance of 4.0 × 10 −5  and maximum performance of 0.048, respectively. To focus on predictive models, we applied an R 2  threshold of 0.05 as in ( Manor and Segal, 2013 ), resulting in models for 20.1% of genes (hereinafter referred to as predictable genes). As in the work of Manor  et al.  ( Manor and Segal, 2013 ), we observed a significant correlation between model performance and variance of expression levels for the predictable genes (Spearman’s correlation = 0.21,  P -value &lt; 2.2 × 10 −16 ;  Supplementary Fig. S4 ). We performed gene ontology (GO) enrichment analysis using GREAT ( McLean  et al. , 2010 ). The top 10% predictable genes were enriched in pathways including graft-versus-host disease and allograft rejection, terms which are relevant to the roles of B cells (i.e. the cell type before transforming to LCL) in the immune system. In contrast, we did not observe any enriched GO term for the top 10% of genes with the highest expression variance, highlighting the ability of TF2Exp to capture expression levels relevant to the sample of interest.
 Fig. 2. Performance comparison of alternative TF2Exp models. For each type of TF2Exp model, performances (R 2 ) of investigated genes (y axis) are plotted in ascending order with respect to the cumulative percentage of genes (x axis). The horizontal dashed line indicates the defined performance threshold of 0.05 for predictable genes We next assessed whether prior knowledge, such as Hi-C proximity scores and known TF-TF physical interactions, could improve TF2Exp models. We introduced the proximity scores of Hi-C interactions to guide model fitting, so that TF-binding events on highly-interacting regions would be less regularized by LASSO (Section 2). We observed that adding Hi-C proximity scores resulted in a slight R 2  improvement of 1.6 × 10 −3  (Wilcoxon signed-rank test,  P -value = 1.5 × 10 −41 ), suggesting that the original TF2Exp models had captured most of the signal from the Hi-C data. We also tested models including interaction terms for known TF-TF physical interactions (Section 2). Adding TF-TF interactions significantly reduced model performance by 2.2 × 10 −3  (Wilcoxon signed-rank test,  P -value = 4.6 × 10 −152 ,  Fig. 2 ), potentially due to our incomplete knowledge of TF-TF interactions and/or limited training samples. Taken together, models incorporating prior knowledge achieved similar performance to the original ones. Thus, we focused on the original (and simpler) TF2Exp models in the next stages of the analysis.  3.3 Alterations of DHS, CTCF and tissue-specific TF binding are the most frequently selected features We next sought to identify TFs for which binding events were more frequently selected in TF2Exp models. For the predictable genes, models selected an average of 4.2 key features (where a feature was the alteration score of a single TF-binding event). Frequently selected TFs had more binding events across the genome (Pearson correlation 0.97,  P -value &lt; 2.2 × 10 −16 ). The top 5 selected TF features were DHS, RUNX3, CTCF, EBF1 and PU.1, accounting for 33.6% of the selected features ( Fig. 3 ). Particularly, 42.0% of the predictable genes had at least one DHS feature, which is in agreement with the well-known relationship between chromatin accessibility and gene expression ( Natarajan  et al. , 2012 ). CTCF has diverse roles in gene regulation across multiple tissues ( Ong and Corces, 2014 ), and the remaining three TFs perform important roles in LCL tissue-specific regulation: RUNX3 in immunity and inflammation ( Lotem  et al. , 2015 ), EBF1 in B lymphocyte transcriptional network expression ( Hagman  et al. , 2012 ) and PU.1 in lymphoid development ( Iwafuchi-Doi and Zaret, 2014 ). Lastly, we observed that RAD21 and SMC3, despite being among the top 10 TFs with the highest number of peaks in the training set ( Fig. 3 ), were selected less frequently than the other 8 TFs (&lt;0.65%), in accordance with their nature of non-sequence specific DNA-binding.
 Fig. 3. Top 10 TFs with the highest number of binding events and their selection frequency in predictable genes. Red bars indicate the total number of TF-binding events selected by TF2Exp models. Blue bars indicate the total number of genes that selected binding events of the indicated TF as key features. The percentage on top of each red bar indicates the ratio between the number of selected events in TF2Exp models and the total number of peaks for that TF 3.4 Selected TF-binding events correlate with gene expression levels in vivo We next sought to assess whether  in vivo  TF binding of selected features correlated with gene expression levels. We obtained CTCF and PU.1 ChIP-seq LCL data for two independent sets of 45 originally-training individuals (see  Supplementary Note s). TF-binding signals were extracted from the reference GM12878 TF-binding events (i.e. the ChIP-seq features used in the TF2Exp for model construction). In predictable genes, 83 CTCF and 72 PU.1 binding events were selected for testing based on their high variance of binding score change (see Section 2). Eight CTCF (9.7%) and seven PU.1 (9.6%) of the tested  in vivo  binding events significantly correlated with gene expression levels (Pearson correlation, FDR &lt;0.05), and their correlation coefficients were consistent with the correlation estimated between the TF sequence alteration score and gene expression ( P -value= 1.4 × 10 −4 , coefficient = 0.81). Due to limited size of test samples (n = 45), we did not have sufficient statistical power to detect weakly correlated TF-gene relationships (e.g. coefficient &lt; 0.29, see  Supplementary Note s), which accounted for most (89.7%) of the tested  in vivo  binding events. In summary, we observed that 9.7% of TF-binding events selected by TF2Exp displayed detectable correlation (correlation coefficient &gt; 0.29) between  in vivo  binding and gene expression levels. 3.5 Selected TF-binding events within proximal regions have greater effect sizes We next examined the locations and effect sizes of selected features. In proximal regions, selected features were mostly within 10 kb from gene start positions, while, in distal regulatory regions, they were distributed within ∼500 kb. We observed significant depletion of selected features in distal regulatory regions compared with proximal regions (Fisher’s exact test, odds ratio = 0.11,  P -value &lt; 2.2 × 10 −16 ). Effect sizes of TF-binding events decreased rapidly with respect to the distance from gene start positions ( Supplementary Fig. S5A ). Such a trend has been reported for effect sizes of eQTLs ( Battle  et al. , 2014 ). Selected features in proximal regions also exhibited significantly larger absolute effect sizes (Wilcoxon rank-sum test,  P -value = 7.3 × 10 −68 ,  Supplementary Fig. S5B ) and more positive effects (Wilcoxon rank-sum test,  P -value = 4.22 × 10 −5 ) than features in distal regulatory regions. Nevertheless, the selected distal features of a gene were significantly enriched in the enhancer regions associated to that gene, as specified in the FANTOM5 project ( Andersson  et al. , 2014 ) (Fisher's exact test, odds ratio = 1.3,  P -value = 0.002, see  Supplementary Note s), supporting a functional role of the selected distal TF-binding events. Thus, TF2Exp models are identifying  cis -regulatory sequence variants that bring functional insights into the mechanisms underlying gene expression levels. 3.6 Uncommon variants increase the number of predictable genes As TF2Exp models can distinguish the impact of variants in TF-binding events, we investigated the contribution of uncommon (MAF &lt; 0.05) variants to model performance. TF2Exp models trained only on uncommon variants achieved higher average performance (R 2  = 0.004) than control models, indicating that the contribution of uncommon variants was not random. To further explore the predictive potential of uncommon variants, we divided TF-binding events into two classes: (i) events altered only by uncommon variants (i.e. do not include any common variants); and (ii) the rest of events overlapping at least one common variant. Note that, by definition, class-2 events can still contain uncommon variants. After adding class-1 events on top of class-2 events, there was a mean performance improvement of 3.8 × 10 −4 , and the number of predictable genes increased to 3177 compared with 3139 genes for models trained only on class-2 events. To test whether this observation was due to random effects, we shuffled class-1 events across individuals. For shuffled models, the number of predictable genes decreased to 3076, suggesting that the benefit of using TF-binding events altered only by uncommon variants was not random. Moreover, in pairwise comparison between the two models for the same gene (i.e. adding class-1 events or not), 31.4% of shared predictable genes gained performance increase (6.7×10 −3  on average) after adding, while 37.2% of genes showed performance decrease but with smaller alteration (4.7×10 −3  on average). Furthermore, the newly selected features in the improved models were enriched at proximal regulatory regions compared with those models with decreased performance (Fisher's exact test,  P -value = 3.2 × 10 −5 , odd ratio = 1.8). To avoid noisy uncommon variants, we next focused on TF-binding events altered only by uncommon variants (class-1 events) within proximal regions. After adding these events, the number of predictable genes increased to 3179. Although the two types of models performed similarly for most cases (Wilcoxon rank sum test,  P -value = 0.51;  Fig. 4 ), there was a significant mean performance improvement of 5.9 × 10 −4  after adding class-1 events (Welch's t-test,  P -value = 4 × 10 −4 ). While 95% of the genes showed small absolute performance alteration (1.6 × 10 −3  of R 2  on average), for the remaining ∼5% of genes, adding class-1 events enabled significantly larger performance improvement ( Fig. 4 ; Wilcoxon rank sum test,  P -value = 4.6 × 10 −5 , estimated difference = 1.5 × 10 −2 ).
 Fig. 4. Performance comparison after adding uncommon-variant only events in proximal regulatory regions. Each dot represents an evaluated gene-model. Coordinates ( x  and  y ) indicate the cross-validation performances before or after adding TF-binding events altered only by uncommon variants in proximal regulatory regions, respectively. The dot shape indicates the magnitude of absolute performance alteration, solid for small alteration (&lt;1.6 × 10 −3 , 95% quantile of the absolute performance alteration) and circle for the rest genes with larger alteration 3.7 Alteration of TF-binding events improve the performance of variant-based models We compared our TF2Exp models with state-of-the-art models that predict alteration of gene expression levels based on proximal variants ( Gamazon  et al. , 2015 ;  Manor and Segal, 2013 ) (see Section 2). First, we trained TF2Exp and variant-based models on the same set of common variants (variants within TF-binding events, MAF ≥ 0.05) for each gene. Both models showed comparable performance across the shared predictable genes (Wilcoxon signed-rank test,  P -value = 0.15;  Supplementary Fig. S6 ). In addition, the default variant-based models using all the proximal common variants within 1 Mb of the gene body showed better performance than TF2Exp models trained on common variants (Wilcoxon signed-rank test,  P -value = 0.06), potentially due to variants in unknown TF-binding events. As uncommon variants are informative for a subset of TF2Exp models, we trained models on combined features of the default variant-based model and the default TF2Exp model for each gene. Combined models achieved better performance than variant-based models (Wilcoxon signed-rank test,  P -value = 0.02, estimated median difference = 1.8 × 10 −4 ), in agreement with the positive contribution of proximal uncommon variants observed in the previous section. We further explored whether introducing binding events of new TFs would improve model performance. We collected and added TF-binding events of 92 new TFs from other cell types (175 ENCODE ChIP-seq datasets) to TF2Exp models, and tested the model performance of genes in 5 chromosomes (chr1-2 and chr20-22). High performance genes (R 2  &gt; 0.25) in TF2Exp models gained significant improvement (Wilcoxon signed-rank test,  P -value = 0.03, estimated median difference = 5.3 × 10 − 4 ) after the addition of binding events for new TFs, while the rest of the genes were better represented with the original models (Wilcoxon signed-rank test,  P -value = 7.7 × 10 − 5 , estimated median difference = 1.6 × 10 − 3 ). These findings suggest that additional TF sets are informative, but model performance is limited by the size of training samples.  3.8 TF2Exp models distinguish variants in LD compared with variant-based models Unlike variant-based models, TF2Exp models are able to infer the functional roles of variants in linkage disequilibrium (LD) based on the predicted impact of variants on TF-bound regions. Comparing TF2Exp and variant models on the same set of common variants (variants within TF-binding events), most selected binding events in TF2Exp models (62.7%,  n  = 12 663) overlapped selected variants (59.8%,  n  = 9386) in variant-based models for the same gene. Of the total of overlapped variants, 18.4% were in high LD (r 2  &gt; 0.9) with other variants in the same TF-bound regions, hindering the inference of the causal variants by variant-based models. Using TF2Exp models, we found that 36.8% of the linked variants showed at least a two-fold impact on the overlapped TF-bound region compared with the selected variants ( Supplementary Fig. S7 ), suggesting a more dominant contribution of the linked variants. In addition, a subset of selected variants (20.1%) overlapped with more than one selected TF-binding event, indicating that individual variant could alter multiple mechanisms of gene regulation. Overall, TF2Exp models provide a quantitative way to evaluate the impact of variants in LD, suggesting a broader utility for genomic studies than variant-based models. 3.9 TF2Exp models exhibit robust performance in external validation datasets We finally sought to evaluate TF2Exp models of predictable genes on external datasets. We obtained microarray expression data from LCLs of 256 individuals ( Stranger  et al. , 2012 ), including 80 Utah residents with Northern and Western European ancestry (CEU), 87 Chinese (CHB) and 89 Japanese (JPT) (Section 2). As 79 of the CEU individuals overlapped with the training individuals of TF2Exp models, we first evaluated the agreement between the microarray and RNA-seq data on these individuals. Relative expression levels across all genes within each individual were concordant between microarray and RNA-seq experiments (average Spearman’s correlation = 0.76), supporting an overall consistency between the two datasets. However, when we considered a single gene across the 79 individuals, the correlation between the two platforms was low (average Spearman correlation = 0.19). Therefore, we expected models trained on RNA-seq data to have an upper limit performance when applied to microarray data. We used TF2Exp models (trained on CEU individuals) to predict gene expression levels on the CHB and JPT individuals. Predictable TF2Exp models achieved an average correlation of 0.16 for both populations. Similarly, predictable variant-based models achieved an average correlation of 0.17 for both populations. An example of a high performing gene (FAM105A) in the external validation is illustrated in  Figure 5 . FAM105A is associated with pancreatic islet function and type 2 diabetes ( Pedersen  et al. , 2017 ;  Taneera  et al. , 2015 ). For this gene, TF2Exp identified 4 contributing TF-binding events ( Fig. 5 ), of which two of them had greater weights: DHS (chr22: 45711760-45711910, effect size: −0.325) and MEF2A (chr22: 45771822-45772122, effect size: 0.334). Alterations of these key events largely explained the changes of gene expression across the different individuals. For example, NA18640 had the lowest observed expression level in CHB individuals, as variant rs104664 of this individual was predicted by TF2Exp to increase the score of DHS; while rs5765304 in NA18573 increased MEF2A binding scores, resulting in the highest predicted expression across all individuals.
 Fig. 5. Key features of TF2Exp for FAM105A gene in the external validation set. The top panel illustrates the key TF-binding events learned from the training datasets. The figure legend is the same as for  Figure 1 . The middle and bottom panel show the variants within the key TF -binding events and their inferred roles on gene expression for the two individuals To test model performance on tissues other than LCLs, we applied TF2Exp models of predictable genes on chromosome 1 to ten tissues with the largest sample sizes from the GTEx project ( GTEx Consortium, 2017 ). Average Pearson correlations between predicted and observed expression levels ranged from 0.11 to 0.08 across the 10 tissues ( Supplementary Table S1 ). Despite the expected performance loss in non-LCL tissues, TF2Exp models displayed robust performance, highlighting their potential application to other (non LCL) cells or tissues. Moreover, for eQTLs identified by GTEx project in these tissues on chromosome 1, TF2Exp correctly predicts the direction of expression change for 79% of the variants within key TF-bound regions and, akin to the assessment used in the recently published ExPecto paper ( Zhou  et al. , 2018 ), for 91% of top 500 variants with strongest predicted expression impact overlapping key TF-bound regions within 20 kb of the TSS. 4 Discussion Deciphering the functional roles of regulatory variants is a critical challenge in the post-sequencing era. To address this challenge, we have introduced a novel framework, TF2Exp, which uses alterations of TF binding as key features to elucidate the functional impact of regulatory variants and predict personalized gene expression levels. TF2Exp models based on lymphoblastoid cell line data showed predictive capacity for 3196 genes, incorporating an average of 4.2 altered TF-binding events per gene model. The most frequently selected TF-binding events included both general properties (e.g. alterations within DNase I hypersensitive regions) and tissue-specific properties (e.g. alterations in TF-bound regions for TFs relevant to the studied lymphoblastoid samples). TF2Exp models could incorporate uncommon variants to improve model performance, and provide mechanistic insights into  cis -regulatory variants. TF2Exp models have the potential to address two challenges left unresolved by variant-based models and classical eQTL studies. For these approaches, it is difficult to: (i) infer variant function (the studied variants can be in high linkage disequilibrium with many others); and (ii) evaluate the impact of uncommon variants (which are excluded from such analyses). By treating TF-binding events as functional units, TF2Exp models can evaluate the relative impact of any variant (single nucleotide variants or small indels) within a TF-bound region. As in the example presented in  Figure 5 , for individual variants, the derived impact within the model is independent of linkage disequilibrium or allele frequency. Moreover, even though the inclusion of uncommon (and rare) variants only improved model performance for a small portion of genes, the resulting TF2Exp models offer a unique advantage for the inference of functional  cis- regulatory variants, compared with previous variant-based methods ( Gamazon  et al. , 2015 ;  Manor and Segal, 2013 ). Similarly to variant-based methods, the predictive performance of TF2Exp models is limited, showing utility only for a subset of genes (20.1%), and even within these genes, model performance was modest (R 2  = 0.21). Such a limited performance is likely attributable to multiple causes. First, variance of gene expression due to common variants is quite low [e.g. 15.3% as estimated by  Gamazon  et al.  (2015 )], suggesting that models restricted to DNA sequence features alone can only account for a portion of the observed variance in gene expression levels. Second, TF2Exp models are limited by the availability of ChIP-seq data (78 TFs in LCLs), while transcriptome studies have revealed that human cells express an average of 430 TFs ( The Fantom Consortium, 2014 ). Though we anticipate an increasing amount of available ChIP-seq data in the future, a practical and more immediate solution would be to computationally predict TF-bound regions. To improve TF-binding prediction, multiple tools combine DNA sequence and chromatin properties (e.g. ATAC-seq, DHS or histone modifications) of the target tissue ( Pique-Regi  et al. , 2011 ); these types of data are currently available for &gt;100 primary tissues and cells ( Roadmap Epigenomics Consortium  et al. , 2015 ). Third, TF2Exp models focus on TF-binding events potentially involved in transcriptional regulation, but other regulatory mechanisms (e.g. post-transcriptional regulation) or genomic features (e.g. DNA methylation or sequence conservation) might explain an additional portion of the observed variance of gene expression. Fourth, TF2Exp models are likely constrained by the small number of available training samples, as including additional features (e.g. TF-TF interactions and uncommon variants) decreased model performance. We expect that the expansion of reference transcriptome datasets will provide more samples for exploring more complex relationships between genes and TF-binding events, thereby improving model performance. During the review process of this manuscript, a new related tool named ExPecto was published ( Zhou  et al. , 2018 ). Though both TF2Exp and ExPecto take DNA sequences as input to predict gene expression levels, the tools differ significantly from each other as they focus on distinct types of variance of gene expression. ExPecto uses a single regression model per tissue to predict the variance of gene expression levels across all the genes (i.e. ‘intra-individual’ variance). In contrast, TF2Exp uses one model per gene per tissue to predict the variance of gene expression levels of that gene across the different individuals (i.e. ‘inter-individual’ variance). Past studies have shown that intra-individual variance is easier to predict than inter-individual variance based on regulatory features. For instance, H3K27ac levels correlate well with broad gene expression in CD4+ T cells (intra-individual variance; Pearson’s correlation coefficient = 0.72) ( Karlic  et al. , 2010 ), while in lymphoblastoid cells from 47 individuals, they only correlate with the expression levels of 22% of genes (inter-individual variance) ( Waszak  et al. , 2015 ). While the reported model performance measures for TF2Exp and ExPecto are not directly comparable, emerging independent datasets should enable benchmarking of the two tools in the future. It should also be noted that while the performance of ExPecto did not increase with the inclusion of distal regions (more than 20 kb distal from the TSS), the inclusion of distal features in TF2Exp was beneficial (as shown in  Supplementary Fig. S5 ). We suspect that such difference lies in the TF2Exp focus on candidate distal regulatory regions supported by Hi-C and TF binding data. In conclusion, identifying the impact of  cis -regulatory variants on gene expression is a critical step towards understanding the genetic mechanisms contributing to diseases. TF2Exp models are able to predict the impact of TF-binding on gene expression levels and provide mechanistic insights into the roles of selected TF-binding events and  cis -regulatory variants. We anticipate that future enlarged omics data, in LCLs and other cell types, will greatly expand the application scope of TF2Exp models. Supplementary Material bty992_Supplementary_Material Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Identification of causal genes for complex traits</Title>
    <Doi>10.1093/bioinformatics/btv240</Doi>
    <Authors>Hormozdiari Farhad, Kichaev Gleb, Yang Wen-Yun, Pasaniuc Bogdan, Eskin Eleazar</Authors>
    <Abstract>Motivation: Although genome-wide association studies (GWAS) have identified thousands of variants associated with common diseases and complex traits, only a handful of these variants are validated to be causal. We consider ‘causal variants’ as variants which are responsible for the association signal at a locus. As opposed to association studies that benefit from linkage disequilibrium (LD), the main challenge in identifying causal variants at associated loci lies in distinguishing among the many closely correlated variants due to LD. This is particularly important for model organisms such as inbred mice, where LD extends much further than in human populations, resulting in large stretches of the genome with significantly associated variants. Furthermore, these model organisms are highly structured and require correction for population structure to remove potential spurious associations.</Abstract>
    <Body>1 Introduction Genome-wide association studies (GWAS) have been extremely successful in reproducibly identifying variants associated with various complex traits and diseases ( Altshuler  et al. , 2008 ;  Hakonarson  et al. , 2007 ;  International Multiple Sclerosis Genetics Consortium  et al. , 2013 ;  Kottgen  et al. , 2013 ;  Ripke  et al. , 2013 ). The most common type of genetic variants comes in the form of single nucleotide polymorphisms (SNPs), which we make the focus of this study. Because of the correlation structure in the genome, a phenomenon referred to as linkage disequilibrium (LD) ( Pritchard and Przeworski, 2001 ;  Reich  et al. , 2001 ), each GWAS-associated variant will typically have hundreds to thousands of other variants which are also significantly associated with the trait. Identifying the variants responsible for the observed effect on a trait is referred to as fine mapping ( Hormozdiari  et al. , 2014 ;  Kichaev  et al. , 2014 ;  Maller  et al. , 2012 ;  Yang  et al. , 2012 ). In the context of association studies, the genetic variants which are responsible for the association signal at a locus are referred to in the genetics literature as the ‘causal variants’. Causal variants have biological effect on the phenotype. Generally, variants can be categorized into three main groups. The first group is the causal variants which have a biological effect on the phenotype and are responsible for the association signal. The second group is the variants which are statistically associated with the phenotype due to LD with a causal variant. Even though association tests for these variants may be statistically significant, under our definition, they are not causal variants. The third group is the variants which are not statistically associated with the phenotype and are not causal. We note that this usage of the term causal has little to do with the concept of causal inference as described in the computer science and statistics literatures ( Pearl, 2000 ;  Spirtes  et al. , 2000 ). Fine-mapping methods take as input the full set of association signals in a region and attempt to identify a minimum set of variants that explains the association signals. A common approach is to calculate marginal association statistics for each variant and, depending on the study budget, select the top  K  ranked variants for follow-up studies. However, the local correlation structure at a fine-mapping locus will induce similar association statistics at neighboring, non-causals variants, thereby making this approach suboptimal in this context. Furthermore, it fails to provide a guarantee that the true causal variant is selected. A recent work ( Maller  et al. , 2012 ) addressed this issue by estimating the probabilities for variants to be causal under the simplifying assumption that each fine-mapping locus contains a single causal variant. Ranking variants based on association strength (similar to top  k ) and this probabilistic approach ( Maller  et al. , 2012 ) assuming a single causal variant give identical relative rankings. However, the probabilistic approach provides the added benefit that we can now select enough variants to guarantee that we have captured the true causal variants with ρ level of confidence. Unfortunately, the key underlying assumption that a fine-mapping locus contains a single causal variant is likely to be invalidated at many risk loci ( Hormozdiari  et al. , 2014 ;  Kichaev  et al. , 2014 ). For regions that putatively harbor multiple independent signals, a common strategy is to use iterative conditioning to tease out secondary signals ( Yang  et al. , 2012 ). This process is analogous to forward stepwise regression, where at each iteration, the variant with the strongest association is selected to enter the model and then marginal statistical scores are re-computed for the remaining variants condition on the ones that have been selected. This process is repeated until there are no remaining variants that are statistically significant. However, it has been shown that this approach is highly sub-optimal ( Hormozdiari  et al. , 2014 ;  Kichaev  et al. , 2014 ) due to lack of LD consideration. To address these issues, we recently proposed probabilistic fine-mapping methods ( Hormozdiari  et al. , 2014 ;  Kichaev  et al. , 2014 ) that build on the concept of a standard confidence interval by providing a well-calibrated, minimally sized confidence set of variants using principled, LD-aware modeling of multiple causal variants. In these methods, we assign probability to each variant to be causal and subsequently select the smallest number of variants that achieve the desired posterior probability. Many accurate fine-mapping methods have been designed for human studies where there are a relatively small number of associated variants in a region. In model organism studies, however, pervasive LD patterns result in GWAS-associated loci that may span several megabases and contain thousands of variants and dozens of genes. For example, in a widely utilized design for mouse studies, the Hybrid Mouse Diversity Panel (HMDP) ( Bennett  et al. , 2010 )—the typical associated region—is approximately 1–2 megabases. Identifying which genes underlie an associated locus in model organism studies is a major, labor-intensive process involving generating gene knockouts. Therefore, it is often the case that identifying the causal genes at an associated locus requires a larger effort than the initial GWAS ( Flint and Eskin, 2012 ). In addition to large LD blocks, fine-mapping studies in model organisms are complicated by population structure (i.e. the complex genetic relationship between different individuals in the study;  Flint and Eskin, 2012 ;  Kang  et al. , 2008 ;  Price  et al. , 2006 ) that invalidate commonly used association statistics that assume the individuals in the study are independent. Model organisms such as mice have a high level of population structure, typically larger than what is observed in human populations; therefore, correcting for the population structure for mouse GWAS is imperative to mitigate the chance of false positive signals of association ( Flint and Eskin, 2012 ;  Kang  et al. , 2008 ;  Price  et al. , 2006 ). In this article, we propose CAVIAR-Gene (CAusal Variants Identification in Associated Regions), a statistical method for fine mapping that addresses two main limitations of existing methods. First, as opposed to existing approaches that focus on individual variants, we propose to search only over the space of gene combinations that explain the statistical association signal, and thus drastically reduce runtime. Second, CAVIAR-Gene extends existing framework for fine mapping to account for population structure. The output of our approach is a minimal set of genes that will contain the true casual gene at a pre-specified significance level. This gene set together with its individual gene probability of causality provides a natural way of prioritizing genes for functional testing (e.g. knockout strategies) in model organisms. Through extensive simulations, we demonstrate that CAVIAR-Gene is superior to existing methodologies, requiring the smallest set of genes to follow-up in order to capture the true causal gene(s). To validate our approach, we applied CAVIAR-Gene to real mouse data and found that we can successfully recover  Apoa2 , a known causal gene for high-density lipoprotein (HDL) ( Flint and Eskin, 2012 ;  van Nas  et al. , 2009 ), for the HDL phenotype in the HMDP. 2 Methods 2.1 Overview of CAVIAR-Gene CAVIAR-Gene takes as input the marginal statistics for each variant at a locus, an LD matrix consisting of pairwise Pearson correlations computed between the genotypes of a pair of genetic variants, a partitioning of the set of variants in a locus into genes, and the kinship matrix which indicates the genetic similarity between each pair of individuals. Marginal statistics are computed using methods that correct for population structure ( Kang  et al. , 2008 ;  Lippert  et al. , 2011 ;  Listgarten  et al. , 2012 ;  Zhou and Stephens, 2012 ). We consider a variant to be causal when the variant is responsible for the association signal at a locus and aim to discriminate these variants from ones that are correlated due to LD. Our previous proposed method CAVIAR, is a statistical framework that provides a ‘ρ causal set’ that is defined as the set of variants that contain all the causal variants with probability of at least ρ. The intuition is that due to LD structure, it is impossible to identify exactly the causal variants, but it is possible to identify a set which contains these causal variants. CAVIAR was designed to work on human GWAS where we deal with regions that have at most 100 variants in a locus and we consider all possible causal combinations of at most 6 causal variants to detect the ρ causal set. However, in model organisms, the large stretches of LD regions result in a large number of variants associated in each region, thus making CAVIAR computationally infeasible. CAVIAR-Gene mitigates this problem by associating each variant to a proximal gene, and instead, operating on the gene level, thus reducing the computational burden by an order of magnitude while facilitating interpreting of GWAS results. Similarly, CAVIAR-Gene detects a ‘ρ causal gene set’ which is a set of genes in the locus that will contain the actual causal genes with probability of at least ρ. Note that not all the genes selected in the ρ causal gene set will be causal. A trivial solution to this problem would be to output all the genes as the ρ causal gene set. However, because this provides no additional information, we are interested in detecting the ρ causal gene set which has the minimum number of genes. We demonstrate that CAVIAR-Gene is well-calibrated as it fails to detect the actual causal gene 1−ρ fraction of the time. 2.2 Standard GWAS Consider a GWAS on a quantitative trait where we collect phenotypic values for  n  individuals and genotype all the individuals on  m  variants. Let  y i  indicate the phenotypic value of the  i th individual and  g i k ∈ { 0 , 1 , 2 }  indicate the minor allele count of the  i th individual for the  k th variant. We use  Y  to denote the  ( n × 1 )  vector of phenotypic values and  X k  to denote the  ( n × 1 )  vector of normalized genotype values for the  k th variant for all the  n  individuals in the study. Without loss of generality, we assume that genotype values for each variant have been standardized to have mean 0 and variance 1 yielding the following relationships:  1 T  X k  = 0 and  X k T X k = n , where  1  denotes the  ( n × 1 )  vector of ones. We assume that the data generating model follows a linear additive model, and for simplicity the variant  c  is the only variant associated (causal) with the phenotype. Each variant is categorized into one of the three groups. The first group is variants which are associated with the phenotype and are considered causal. The second group is variants which are statistically associated with the phenotype due to LD with a causal variant—these variants are considered not causal. The third group is variants which are not associated with the phenotype and are considered not causal. Standard GWAS analysis for the  c th variant is performed utilizing the following model equation:
 (1) Y = μ 1 + β c X c + e 
where  μ  is the mean of the phenotypic values,  β c  is the effect size of the  c th variant, and  e  is the residual noise. In this model, the residual error is the  ( n × 1 )  vector of i.i.d and normally distributed error. Let  e ∼ N ( 0 , σ e 2 I ) , where  I  is the ( n  ×  n ) identity matrix and  σ e  is a covariance scalar. The estimates of  β c , which are indicated by  β c ^ , are obtained by maximizing the likelihood,
 β ^ c = X c T Y X c T X c , β ^ c ∼ N ( β c , σ e 2 ( X c T X c ) ) 
and the statistics is computed as follows:
 S c = β ^ c σ e ^ ( X c T X c ) ∼ N ( λ c , 1 ) . 
where  λ c  is the non-centrality parameter (NCP) and is equal to  β c σ e n . We obtain the estimated value for  μ ,  e , and  σ e  as follows:  μ ^ = 1 T X c n ,   e ^ = Y − 1 μ ^ − β ^ c X c , and  σ ^ e = e ^ T e ^ n − 2 . 2.3 The effect of LD in GWAS In the previous section, we consider that there is only one variant (variant c), and this variant is causal. Now, we extend the previous case and for simplicity we assume there are two variants,  c  and  k . Similar to the previous section, the variant  c  is causal and variant  k  is correlated to  c  through LD but has no phenotypic effect. The correlation between the two variants is  r  which is approximated by  1 n X k T X c . Thus, the estimate for the effect size for the variant  k  is as follows:
 β ^ k = X k T Y X k T X k , β ^ k ∼ N ( r β c , σ e 2 ( X k T X k ) ) 
and the statistics is computed as follows:
 S k = β ^ k σ e ^ ( X k T X k ) ∼ N ( r λ c , 1 ) . 
 We compute the covariance between the estimated effect size of the two variants as follows:
 Cov ( S c , S i ) = Cov ( ( X c T X c ) β ^ c σ e , ( X i T X i ) β ^ k σ e ) = 1 σ e 2 Cov ( X c T Y X c T X c , X i T Y X i T X i )   = X i T X c X i T X i X c T X c = r . 
 Thus, the joint distribution of the marginal association statistics for the two variants given their NCPs follows a multivariate normal distribution (MVN),
 ( [ S i S j ] | [ λ i λ j ] ) ∼ N ( [ λ i λ j ] , [ 1 r i j r i j 1 ] ) ,   
where  r ij  is the genotype correlation between the  i th and  j th variants. In the case that both variants are not causal, we have  λ i = λ j = 0 . In the case that the  j th variant is causal and the  i th variant is not causal, we have  λ i = r λ j . In the case that  j th variant is not causal and the  i th variant is causal, we have  λ j = r λ i . This result is known from previous studies ( Han  et al. , 2009 ;  Hormozdiari  et al. , 2014 ;  Kichaev  et al. , 2014 ;  Zaitlen  et al. , 2010 ). 2.4 Computing the likelihood of causal SNP status from GWAS data Given a set of  m  variants, the pair-wise correlations denoted by Σ, we use the  ( m × 1 )  vector  S = [ S 1 … S m ] T  to denote the marginal association statistics. We extend the joint distribution mentioned above for  m  variants. The joint distribution follows an MVN distribution,
 (2) ( S | Λ ) ∼ N ( Σ Λ , Σ ) , 
where Λ is the  ( m × 1 )  vector of normalized true effect sizes and Σ is a  ( m × m )  matrix of pair-wise genotype correlations between different SNPs. Let  X = [ X 1 , X 2 ⋯ X m ]  be a  n  ×  m  matrix of genotype. We can approximate Σ using genotype data as follows:  Σ = 1 n X T X . In CAVIAR ( Hormozdiari  et al. , 2014 ), we introduce a new parameter  C , which is a  ( m × 1 )  binary indicator vector used to represent causal status of  m  SNPs in a region (i.e.  c ( i )  is 1 if the  i th SNP is causal and 0 otherwise). We define a prior probability on the vector of Λ for a given causal status using an MVN distribution,
 (3) ( Λ | C ) ∼ N ( 0 , Σ c ) , 
where Σ c  is a diagonal ( m  ×  m ) matrix. The diagonal elements of Σ c  are set to  σ e 2  or  ϵ  where  ϵ  is a very small constant to make sure the matrix Σ c  is full rank. The  i th element on the diagonal is set to  σ e 2  if the  i th variant is causal and set to  ϵ  if the  i th variant is non-causal. We know that the LD between two variants is symmetric ( Σ T = Σ ). We combine  Equations (2)  and  (3)  to compute the joint marginal association statistics of all the variants. The joint distribution follows an MVN distribution,
 (4) ( S | C ) ∼ N ( 0 , Σ + Σ Σ c Σ ) . 
 2.5 Computing the posterior probability of causal SNP status from GWAS data Given the observed marginal association statistics,  S = [ S 1 , ⋯ S m ] T , we can compute the posterior probability of the causal SNP status  P ( C * | S )  as,
 (5) P ( C * | S ) = 1 Z P ( S | C * ) P ( C * ) = P ( S | C * ) P ( C * ) ∑ C ∈ C P ( S | C ) P ( C ) . 
where  C  is the set of all possible causal SNPs. Thus, the size of  C  is  2 m . Furthermore,  P ( C * )  is the prior probability for a particular causal SNP status,  C * . We use  Z  to indicate the normalization factor. In CAVIAR, we use a simple prior for a causal SNP status. We assume that the probability of an SNP to be causal is independent from other SNPs and the probability of an SNP to be causal is  γ . Thus, we compute the prior probability as  P ( C * ) = ∏ i = 1 m γ | c i | ( 1 − γ ) 1 − | c i | . In our work, we set  γ  to 0.01 ( Darnell  et al. , 2012 ;  Eskin, 2008 ;  Jul and Eskin, 2011 ). It is worth mentioning that although we use a simple prior for our model, CAVIAR can incorporate external information such as functional data or knowledge from previous studies. As a result, we can have SNP-specific prior where  γ i  indicates the prior probability for the  i th SNP to be causal. Thus, we can extend the prior probability to a more general case,  P ( C * | γ = [ γ 1 , γ 2 , ⋯ γ ℓ ] = ∏ i = 1 ℓ γ i | c i | ( 1 − γ i ) 1 − | c i | . To compute the posterior probability for each causal SNP status, we need to consider all the possible causal SNP status which is the denominator of  Equation (5) . To ease the computational burden, we assume we have at most six causal SNP in each region. Assuming we have an upper bound on the number of causal variants is a common procedure in fine-mapping methods ( Hormozdiari  et al. , 2014 ;  Kichaev  et al. , 2014 ). We show the upper bound of six causal variants have small effect on the results ( Hormozdiari  et al. , 2014 ). This assumption reduces the size of  C  from  2 m  to  m 6  which is computationally feasible. 2.6 ρ causal SNP set Give a set of SNPs  K , we define a causal SNP configuration as all the possible causal SNP status which excludes any SNP as causal outside the set  K . Note, our definition of causal SNP configuration includes the causal SNP status where no SNP is considered as causal. We use  C K  to denote the causal SNP configuration for the  K . We compute the posterior probability of set  K  capturing all the true causal genes,
 P ( C K | S ) = ∑ C ∈ C K P ( C | S ) . 
 Let ρ denote the value of the posterior probability, where  ρ = P ( C K | S ) , and we refer to it as the confidence level of  K  capturing the actual causal SNPs. We refer to  K  as the ‘ρ confidence set’. Given a confidence threshold  ρ * , there may exist many confidence sets that have a confidence level greater than the threshold. However, among all the possible  ρ *  confidence sets, the sets which have the minimum number of SNPs are more informative or have higher resolution to detect the actual causal SNPs. Thus, we are interested in finding the  ρ *  confident set with the minimum size (with minimum number of selected SNPs),  P ( C K * | S ) ≥ ρ * , where  K *  has the minimum size. 2.7 ρ causal gene set Unfortunately, the ρ causal SNP sets for mice can select many variants due to the high LD. Instead, we would like to find a set of genes that harbors causal variants. We define a ρ causal gene set as a set of genes which captures all the genes which harbor the causal variants with probability at least ρ. One of the benefits of detecting the ρ causal gene set requires less computation than detecting the ρ causal SNP set. For simplicity, we use genes as a way to group the SNP to detect the causal SNPs. Thus, SNPs are partition to sets and this partition of the SNPs is done based on the genes. As a result, when a gene is selected in the ρ causal gene set, we can consider all the SNPs which are assigned to that gene which are selected in the ρ causal SNP set in the CAVIAR model. We use a simple way to assign SNPs to a gene—we assign an SNP to the closest gene. We would like to emphasize that CAVAIR-Gene can incorporate more complicated SNP to gene assignment. Let  G  be a set of genes and  K ( G )  indicate all the SNPs assigned to the genes in the set  G . Then, we formally define the ρ causal gene set as a  G *  set where the total posterior probability of all the SNPs in  K ( G * )  that captures all the causal SNPs is ρ. Among all the ρ causal gene set, we are interested in the set which has the minimum number of genes selected.
 P ( C K ( G * ) | S ) ≥ ρ . 
Thus, to detect the ρ causal gene set, we need to search over all the possible sets of genes. Given  ℓ  genes in loci, we have  2 ℓ  possible causal gene set which is much smaller than all the possible sets of SNP, which are  2 m . 2.8 Greedy algorithm to detect the ρ causal gene set We would like to emphasize that ρ causal gene set should capture all the causal genes; however, not all the genes selected in the ρ causal gene set are causal. Thus, even if we set an upper bound of six on the number of causal genes, the size of the ρ causal gene set can be larger than six genes. For example, if we have one causal variant and all the variants in that region have perfect LD, just utilizing the marginal statistics is impossible to distinguish which gene is the actual causal gene. Thus, in order to have 95% causal gene set, we have to select all the genes in the region. This is similar to what we observe in the variant level from previous studies ( Hormozdiari  et al. , 2014 ;  Kichaev  et al. , 2014 ). Instead of considering all the possible causal gene set to find the ρ causal gene set, we propose the following greedy algorithm to ease the computational burden. For each gene, we define a weight that indicates the amount that each gene contributes toward the posterior probability of the ρ causal gene set. Genes which have higher weights will have higher probability of being selected in the ρ causal gene set. Thus, we pick the top set of genes for which the summation of their weights is at least ρ fraction of total weights of all genes in the region. We use  W = [ w 1 , w 2 , ⋯ w ℓ ]  as a  ( ℓ × 1 )  vector for the weights of all the genes, where  w i  is the weight of the  i th gene and we compute the weight for the  i th gene as follow:
 (6) w i = ∑ C ∈ C : c ( i ) = 1 P ( C | S ) = ∑ C ∈ C P ( S   |   C ) P ( C ) c ( i ) ∑ C ∈ C P ( S   |   C ) P ( C ) . 
 We compute the weight for the  i th gene by summing over all the causal gene statuses where the  i th gene is selected as causal. We show in Section 3 that the proposed greedy and the brute force algorithm which consider all possible causal gene status tend to have similar results. 2.9 Handling marginal statistics corrected for population structure The linear model which is used in the standard GWAS assumes only one causal SNP as shown in  Equation (1) . Moreover, in this linear model, we assume that the phenotypic value of each individual is independent from the phenotypic value of another individual. This assumption is not true in general for GWAS, especially in model organisms such as inbred mice. The model that accounts for this dependency is as follows:
 (7) Y = μ 1 + ∑ i = 1 m β i X i + e 
 Unfortunately, in a typical GWAS, the number of individuals in a study is much smaller than the number of SNPs ( n &lt; &lt; m ). Thus, estimating the effect size of all the SNPs is not possible. We test each SNP one at a time,  Y = μ 1 + β c X c + u + e , where  u = ∑ i ≠ c β i X i  models the random effects. In this model, we assume that each SNP has an effect and the effect of each SNP is distributed normally as  β i ∼ N ( 0 , σ g m ) . The total genetic variance is defined as  σ g 2  and we use  σ ^ g 2  as the estimated genetic variance. We compute the variance of the random effect as  V a r ( u ) = σ g 2 K , where  K = X X T / m  is referred to as the kinship matrix. The kinship matrix defines pair-wise genetic relatedness which is computed from the genotype data. Let  V  be the total variance of phenotype  Y , which is computed as  V = σ e 2 I + σ g 2 K . Let  σ ^ e  be the estimated environment and measurement error variance. Thus, the total estimated variance is  V ^ = σ ^ e 2 I + σ ^ g 2 K . We assume that the collected phenotype has an MVN distribution as follows:  Y ∼ N ( μ 1 + β c X c , σ e 2 I + σ g 2 K ) . Similar to linear regression, we compute the estimate of the effect size of the causal SNP  β ^ c  by maximizing the likelihood. Moreover, we can estimate the effect size of the SNP  β ^ i  which is indirectly associated to the causal SNP,
 β ^ c = X c T V ^ − 1 Y X c T V ^ − 1 X c , β ^ c ∼ N ( β c , ( X c T V ^ − 1 X c ) − 1 ) 
and the statistics is computed as follows:
 S c = β ^ c X c T V ^ − 1 X c ∼ N ( λ c , 1 ) 
 We would like to emphasize all the existing methods ( Kang  et al. , 2008 ;  Lippert  et al. , 2011 ;  Listgarten  et al. , 2012 ;  Zhou and Stephens, 2012 ) which correct for population structure computes the marginal statics for each variant. However, corrected marginal statistics cannot be used by existing fine-mapping methods ( Hormozdiari  et al. , 2014 ;  Kichaev  et al. , 2014 ). As in these methods, we assume that the correlation between the computed marginal statistics is equal to the correlation between the two corresponding variants. As shown in our experiment below, the correlation between the marginal statistics which are corrected for population structure is not equal to the correlation of genotypes corresponding to the two variants. We compute the covariance between the observed statistics for a causal SNP (variant) and an SNP (variant) which is indirectly associated with the causal SNP as follows:
 Cov ( S i , S c ) = Cov ( X i T V ^ − 1 Y X i T V ^ − 1 X i , X c T V ^ − 1 Y X c T V ^ − 1 X c )   = X i T V ^ − 1 X c X i T V ^ − 1 X i X c T V ^ − 1 X c 
Let matrix  L  be the Cholesky decomposition of matrix  V ^ − 1 ,   V ^ − 1 = L T L . Let  X c ′ = L X c  and  X i ′ = L X i . We assume that  L X c ,   L X i , and  L Y  are normalized to mean 0 and variance 1. Thus, we can re-write the covariance between the computed statistics for two SNPs as follow:
 Cov ( S i , S c ) = Cov ( X i ′ , X c ′ ) = Cov ( L X i , L X c ) 
 This indicates that the covariance between the two marginal statistics corrected for population structure follows an MVN where the correlation between the two statistics is the correlation between the transformed genotype for both SNPs. Thus, we re-write  Equation (2)  for the case the marginal statistics is corrected for population structure as follows:  ( S | Λ ) ∼ N ( Σ ′ Λ , Σ ′ ) , where  Σ ′  is the pair-wise correlation matrix which is computed by transforming the genotyped data and then computing the pair-wise correlation of transformed genotypes. In principle, this result could also be applied to other problems such as imputing the missing variants that utilize the summary statistics ( Lee  et al. , 2013 ;  Pasaniuc  et al. , 2014 ). 3 Results 3.1 CAVIAR-Gene is computationally efficient CAVIAR and CAVIAR-Gene at high level can consider all possible causal combinations for variants and genes, respectively. However, considering all possible causal combinations is intractable. In CAVIAR, we make an assumption that in each locus we have at most six causal variants. However, in CAVIAR, in order to detect the ρ causal variants, we consider all possible causal sets which can be very slow depending on the number of variants selected in the ρ causal variant set. In the worst case, the running time of CAVIAR can be  O ( 2 m ) , where  m  is the total number of variants in a region. In CAVIAR-Gene, we use the proposed greedy method which is mentioned in Section 2.8. This greedy algorithm reduces the complexity of CAVIAR from  O ( 2 m )  to  O ( m 6 ) . Applying CAVIAR on loci with 100 of variants will take around 30 h. However, it will take 2 h for CAVIAR-Gene to finish on the same loci and 3 h for CAVIAR-Gene to finish on loci with 200 variants.  Figure 1  indicates the running time compression between CAVIAR and CAVIAR-Gene for different number of variants in a region.
 Fig. 1. CAVIAR-Gene is computationally more efficient than CAVIAR. Running time comparison between CAVIAR and CAVIAR-Gene. The experiments are run on a 64 bit Intel(R) Xeon(R) 2 G with 5 GB RAM 
 3.2 CAVIAR-Gene-estimated causal gene sets are well-calibrated To assess the performance of our method, we conducted a series of simulations. To make our simulations more realistic, we utilize real genotypes from three different datasets: outbred dataset ( Zhang  et al. , 2012 ), F2 dataset ( van Nas  et al. , 2009 ), and HMDP dataset ( Bennett  et al. , 2010 ). After obtaining the real genotype for each dataset, we partition the genome into segments containing 200 genes. For each segment, we implant one, two, or three causal genes in the region where a gene is considered causal if it harbors at least one causal variant. We then generate simulated phenotypes for each segment using a linear mixed model as in the previous studies ( Han  et al. , 2009 ;  Zaitlen  et al. , 2010 ). We extend the existing methods, which are designed to detect the causal variants, to detect the causal genes. For these methods, we consider a gene to be causal if any of the variants in that gene are selected as causal. We run TopK-Gene, conditional method (CM-Gene) ( Yang  et al. , 2012 ), 1Post-Gene ( Maller  et al. , 2012 ), and CAVIAR-Gene. Among these methods, CAVIAR-Gene is the only method that is well-calibrated to detect causal genes as shown in  Table 1 . We consider a method to be well-calibrated if it accurately captures the causal genes in ρ fraction of the time. It is worth mentioning that 1Post-Gene is well-calibrated when we only have one true causal gene; however, 1Post-Gene is mis-calibrated when there are more than one causal gene in the locus as shown in  Table1 .
 Table 1. CAVIAR-Gene estimated causal gene-sets are well-calibrated Causal gene Recall rate (%) Causal gene size 1Post-Gene CM-Gene CAVIAR-Gene 1Post-Gene CM-Gene CAVIAR-Gene 1 0.995 0.941 0.990 2.59 1.16 2.10 2 0.790 0.526 0.964 3.93 2.28 3.17 3 0.760 0.610 0.951 3.23 3.28 6.65 a Note:  We implanted one, two, or three causal genes in a region. 1Post-Gene is well-calibrated to detect the causal genes in regions where we have only one true causal gene. CAVIAR-Gene is well-calibrated in all our experiments. We consider a method to be well-calibrated when the recall rate is at least 95%. We compute the recall rate of a method as a percentage of the total simulations where all the true causal variants are detected. a Although we allow for only six causal genes in a region, we can have more than six causal genes in the ρ causal gene set (see Section 2.8). 3.3 CAVIAR-Gene provides better ranking of the causal genes To compare the performance of each method, we compare the recall rate and the number of causal genes selected by each method. We calculate the recall rate as a percentage of the total simulations where all the true causal variants are detected. Unfortunately, each method selects a different number of genes as causal. Thus, to make the comparison fair, we compute the recall rate for each method as a function of the number of genes each method selects. The results for all the methods across all three datasets are shown in  Figure 2 . In this figure, the  X -axis is the number of genes selected by each method and the  Y -axis is the recall rate for each method.  Figure 2 c and e indicates the recall rate for Outbred, F2, and HMDP datasets where we have implanted one causal gene. Although the difference between the TopK-Gene and CAVIAR-Gene in the case of one causal gene is negligible, we observe a 10% higher recall rate when there are multiple causal genes in a region ( Fig. 2 b, d, and f).
 Fig. 2. CAVIAR-Gene provides better ranking of the causal genes for Outbred, F2, and HMDP datasets. Panels  a  and  b  illustrate the results for Outbred genotypes for case where we have one causal and two causal genes, respectively. Panels  c  and  d  illustrate the results for F2 genotypes for case where we have one causal and two causal genes, respectively. Panels  e  and  f  illustrate the results for Outbred genotypes for case where we have one causal and two causal genes, respectively 
 Although in  Figure 2  we only compare recall rate of different methods as we vary the number of causal genes selected by each method, these figures are similar to receiver operating characteristic (ROC) curves which are used as a measure to compare results for different methods in statistics and machine learning. In ROC curves, the  y -axis is the true positive rate which is equivalent to the recall rate in our result, and the  x -axis is the false positive rate which indicates the fraction of simulations where the non-causal genes are selected as causal. Because of the fact that all methods are forced to pick the same number of causal genes, the false positive rate is the same for all the methods. Moreover, similar to ROC curves in our results, as we increase the false positive rate, the recall rate increases and as we reach false positive rate of 1, which means if we select all the genes as causal, we have a recall rate of 1. 3.4 Greedy algorithm and brute force algorithm have similar results We proposed a greedy algorithm in Section 2.8 to detect the ρ causal gene set in order to speed up the process. In this section, we show that the results obtained from the greedy algorithm and the brute force algorithm are very close. The brute force algorithm considers all the possible  2 ℓ  different causal gene sets in order to compute the ρ causal gene set. We consider a region with 20 genes and then we simulated data similar to the previous sections. We implant one, two, or three causal genes in the region. We ran both methods and computed the recall rate as well as the size of the ρ causal gene set selected by each method.  Table 2  shows the results. We calculate the recall rate as a percentage of the total simulations where all the true causal variants are detected.
 Table 2. Greedy algorithm and brute force algorithm have similar results Causal gene Recall rate (%) Causal gene size Greedy Brute force Greedy Brute force 1 0.999 0.999 1.72 1.67 2 0.983 0.990 3.84 3.30 3 0.956 0.976 4.82 4.73 Note:  We implanted one, two, or three causal genes in a region. We run both the greedy and brute force algorithm on the simulated data sets. This result indicates that the differences between these two methods are negligible. 3.5 CAVIAR-Gene adjusts for population structure It is known that in the case where there exists no population structure, the correlation between the marginal statistics of two variants is the same as the correlation between the genotypes from which the statistics were computed. CAVIAR utilizes this fact to compute the likelihood for each possible causal combination. However, when population structure is present and corrected for, this may not hold. We demonstrate in our experiments that the correlation between the marginal statistics for any two variants which are corrected for population structure is the same as the correlation of a transformed version of genotype for the same two variants. We provide the description of this transformation in Section 2. CAVIAR-Gene utilizes this transformation to adjust for the population structure to compute the correct likelihood. We use an HMDP dataset ( Bennett  et al. , 2010 ) which we determine to have population structure. We generate phenotypes with population structure and compute the marginal statistics for each variant both corrected and not corrected for population structure. We then compute the correlation between each pair of marginal statistics and the correlation between each pair of variants for the original genotype and the transformed genotype. We calculate the difference between the correlation computed from the marginal statistics for each pair of variants and the correlation of the genotype of the same variants. The boxplot of these differences are shown in  Figure 3 .
 Fig. 3. CAVIAR-Gene adjusts for population structure. Panel  a  illustrates the case where the data have population structure and the statistics is not corrected for the population structure. Panels  b  and  c  illustrate the cases where we have corrected the statistics for the population structure. However, in Panel b, we compute the correlation between the original genotypes and in Panel c the correlation is computed from the transformed genotypes. Then, we calculate the difference between the correlation computed from the marginal statistics for each pair of variants and the correlation of the genotype of the same variants. The difference between the correlation of the marginal statistics and the correlation of the transformed genotype shown in Panel c is close to zero and their variance is much smaller than other cases as shown in Panels a and b. To compare the results, we plot the residual difference between −0.4 and 0.4, as a result some points for Panel b are not shown 
 As expected, the difference between the correlation of the marginal statistics and the correlation of the transformed genotype is close to zero and their variance is much smaller than other cases. Thus, the correlation between the marginal statistics when population structure is corrected is closer to the correlation between the genotype which is transformed using the right transformation matrix. 3.6 CAVIAR-Gene identifies  Apoa2  as causal gene in HDL To illustrate an application of our method in real data, we use an HDL dataset which was collected for three different mouse strains: outbred dataset ( Zhang  et al. , 2012 ), F2 dataset ( van Nas  et al. , 2009 ), and HMDP dataset ( Bennett  et al. , 2010 ). We ran CAVIAR-Gene on a region ∼80 megabases in length containing 595 genes (chr1: 120,000,000–197,195,432). This region harbors  Apoa2 , a gene previously established to influence HDL levels ( Flint and Eskin, 2012 ;  van Nas  et al. , 2009 ). We applied CAVIAR-Gene on the HMDP dataset considering all the genes in this region which yielded a 95% ρ causal set of 130 genes. Next, we conducted a more refined experiment, using domain-specific knowledge of the phenotype, to create a list of 53 potential candidate genes. CAVIAR-Gene selected a 23 gene subset of this list as the ρ causal gene set. Running CAVIAR-Gene on the Outbred dataset for all 595 genes resulted in a 95% gene set of only 13 genes. Because of the fact that the Outbred mice have a smaller degree of population structure than the HDMP, it is expected that the gene set resolution should be greater in this data. Most importantly, across all the datasets, CAVIAR-Gene includes  Apoa2  in the gene set.  Figure 4  illustrates the genes which are selected by CAVIAR-Gene for each datasets. The five genes which are common between all the datasets are  Nr1i3 ,  Tomm40l ,  Apoa2 ,  Fcer1g , and Ndufs2 . All these genes are known to be highly associated with the HDL. This suggests that CAVIAR-Gene not only recovers the actual causal gene, but simultaneously reduced the number of genes that need to undergo functional validation.
 Fig. 4. Venn diagram of the genes selected by CAVIAR-Gene on each of the dataset. HMPD ALL is the results of CAVIAR-Gene on HMDP when we utilize all the genes. HMDP CG is the result of CAVIAR-Gene on HMDP when we utilize candidate genes 
 4 Discussion In this article, we propose a novel method, CAVIAR-Gene, for performing fine mapping on the gene level. CAVIAR-Gene computes the probability of each set of genes capturing the true causal genes. Then, CAVIAR-Gene selects the set which has the minimum number of genes selected as causal and the probability of the set capturing the true causal gene is higher than a user-defined threshold (e.g. typically 95% or higher). We note that the usage of the term causal has little to do with the concept of causal inference as described in the computer science and statistics literature ( Pearl, 2000 ;  Spirtes  et al. , 2000 ). In the context of association studies, we consider a variant to be causal if the variant is responsible for the association signal in the locus. CAVIAR-Gene can incorporate marginal statistics which is corrected for population structure. This property makes CAVIAR-Gene suitable for performing fine mapping on the model organism such as inbred mice. We show using simulated data that CAVIAR-Gene has higher recall rate compared with the existing methods for fine mapping on the variants level, while the size of the causal set selected by CAVIAR-Gene is smaller than these methods. CAVIAR-Gene incorporates external information such as functional data as a prior to improve the results. Funding This work was supported by the  National Science Foundation  ( 0513612 ,  0731455 ,  0729049 ,  0916676 ,  1065276 , 1302448 , and  1320589  to F.H., W.Y., and E.E.) and the  National Institutes of Health  ( K25-HL080079 ,  U01- DA024417 ,  P01-HL30568 ,  P01-HL28481 ,  R01-GM083198 ,  R01-MH101782 , and  R01-ES022282  to F.H., W.Y., and E.E.). E.E. is supported in part by the NIH BD2K award, U54EB020403. We acknowledge the support of the  National Institute of Neurological Disorders and Stroke Informatics Center for Neurogenetics and Neurogenomics  ( P30 NS062691  and  T32 NS048004-09 ). G.K. and B.P. are supported in part by the  National Institutes of Health  ( R01 GM053275 ). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. Conflict of Interest : none declared. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Inferring parental genomic ancestries using pooled semi-Markov processes</Title>
    <Doi>10.1093/bioinformatics/btv239</Doi>
    <Authors>Zou James Y., Halperin Eran, Burchard Esteban, Sankararaman Sriram</Authors>
    <Abstract>Motivation: A basic problem of broad public and scientific interest is to use the DNA of an individual to infer the genomic ancestries of the parents. In particular, we are often interested in the fraction of each parent’s genome that comes from specific ancestries (e.g. European, African, Native American, etc). This has many applications ranging from understanding the inheritance of ancestry-related risks and traits to quantifying human assortative mating patterns.</Abstract>
    <Body>1 Introduction Recent developments in DNA technology bring personal genomics to reality. This opens up unprecedented possibilities for individuals to learn about their genomic history (e.g. ancestry, family history) as well as their genomic future (e.g. disease risk). A particular aspect of personal genomics that has garnered significant public and medical interest is the ability to precisely quantify the ancestry composition of one’s genome ( Royal and Kittles, 2004 ;  Royal  et al. , 2010 ). Consider a Mexican individual as an example. Her genome consists of alternating blocks of DNA sequences, where each block has African, European or Native American ancestry. The length and frequency distributions of blocks from different ancestries reflect the patterns of admixtures over the last several centuries. A substantial fraction of humans today are offsprings of historical mixing between distinct populations and their genomes are such mosaics of ancestry blocks ( Hellenthal  et al. , 2014 ). The ability to quantify genomic ancestries has important bio-medical implications. For example, African ancestry is a risk factor for asthma. This partially explains the high prevalence of asthma in African American as well as Puerto Ricans with larger African genomic ancestry ( Vergara  et al. , 2013 ). In addition, genomic ancestry gives insights into many social science questions, and expands the common notions of ethnicity and race ( Bryc  et al. , 2015 ;  Hochschild and Sen, 2015 ). Given the genome of an individual, recent machine learning methods can accurately determine the fraction of this person’s genome that originates from each ancestry ( Alexander  et al. , 2009 ;  Pritchard  et al. , 2000 ). However, for many applications in biomedical and social science research, it is important to go beyond the individual’s ancestry and to infer the genomic ancestries of the parents (since most genetic datasets do not have genotype information from the parents). In studies of ancestry linked risk factors, genomic ancestry information of parents can be used to investigate how risks propagate through generations. In social science applications, parental genomic ancestry can be used to understand genetic basis of human mate selection, a subject of substantial recent interest. Latino parents, e.g. were shown to have significant correlations in their genomic ancestries ( Risch  et al. , 2009 ). However, current methods cannot be used to infer the genomic ancestry of each of the two parents of an individual given only the DNA of the individual. Inferring parental genomic ancestry is challenging since the observed DNA are unordered pools of the DNA from the two parents. We show that this problem of parental genomic ancestry inference can be well modeled as a pooled semi-Markov process. To the best of our knowledge, this is the first method that can accurately infer the parameters of the parental ancestries in admixed populations. We applied the efficient algorithms we have developed for pooled semi-Markov process to infer parental ancestry. On experimental data from 231 Mexican families and 258 Puerto Rican families for whom we know the true genomic ancestry of each parent, we show that our method provides accurate estimates of parental genomic ancestry. Our method applies to any common genotyping data from an individual; importantly, no family or phasing information is needed, and hence it is broadly applicable to existing genetic data. Although in this article, we focus on the application of inferring genomic ancestry, we believe that many other settings can also be modeled as pooled semi-Markov process. For example in a tumor sample, there are many clonal subpopulations of cells, each with its own copy number aberrations which can be modeled as a semi-Markov process ( Wu  et al. , 2014 ). When we sequence the tumor in clinics, we typically obtain a pooled collection of reads from the various subpopulations. 1.1 Contributions Our main contributions are:
 We set up the mathematical framework of pooled semi-Markov processes and construct efficient, scalable inference algorithms. Using this framework of pooled semi-Markov processes, we develop a method to infer the parameters of the parental ancestries in admixed populations. This is important because it allows for a better understanding of how certain disease risks are associated with ancestries. We demonstrate the accuracy of our method on a real genotype dataset of 489 families where we can measure the true genomic ancestry of each parent. We further validate it using simulations. 
 1.2 Related work Semi-Markov models have been well studied in literature and have many applications ranging from economics to biology ( Ross, 1999 ). A related class of models for sequential data is factorial HMMs (FHMMs) ( Ghahramani and Jordan, 1997 ). FHMMs model outputs that are function of several hidden states where each hidden state evolves according to an independent Markov model. Because exact inference in FHMMs is intractable, a number of approximate inference procedures have been developed. The pooled semi-Markov process differs from FHMM in significant ways. First, the holding time in each HMM state is geometrically distributed, while we allow for arbitrary distributions. Second, the pooling model introduces hard combinatorial constraints that make standard variational inference inapplicable. There is a large body of work on the inference of local ancestry in admixed populations, in which the ancestry of each position in the genome is inferred. These methods typically use hidden Markov chain models, (e.g.  Price  et al. , 2009 ;  Pritchard  et al. , 2000 ;  Sankararaman  et al. , 2008a ;  Tang  et al. , 2006 ) variants such as switch HMMs ( Sankararaman  et al. , 2008b ) and factorial HMMs ( Baran  et al. , 2012 ). Principal component analysis (PCA) has been shown to correlate well with global ancestry, and variants of PCA have been proposed ( Yang  et al. , 2012 ). In the case of African-Americans, these models have been applied to show that African-Americans today are an admixture of African and European ancestries in the ratio 0.8:0.2 over the last 6–10 generations ( Smith  et al. , 2004 ). Further, it has been shown that local ancestry can be accurately inferred in African-Americans. A limitation of these approaches is that they do not distinguish between the maternal and the paternal contributions to the genetic ancestry. Methods, such as Hapmix ( Price  et al. , 2009 ), estimate the unordered pair of local ancestry states at each position but do not assign the local ancestry to each parental haplotype and hence do not tell us the genomic ancestries of each parent. 2 Methods 2.1 Pooled semi-Markov processes A semi-Markov process generalizes continuous time Markov process to settings where the holding time in a state may not be exponentially distributed. We recall the generative procedure for sampling from a semi-Markov process of  K  states.
 Definition Let  f  denote the probability density function of a random variable parametrized by  λ  ( λ  could represent either a scalar or a vector depending on the form of the density function). To sample from a  K -state semi-Markov process parametrized by  { λ k , α k } k = 1 K ,   ∑ α k = 1 , we do the following:
 i ← 1 . Sample the state of the first block,  ϕ 1 ∼ Discrete ( { α k } ) . Sample the length of the first block,  L 1 ∼ f ( λ ϕ 1 ) . Repeat while  ∑ j = 1 i L i &lt; L , where  L  is a specified length:
 i ← i + 1 Sample the state of block  i ,  ϕ i ∼ Discrete ( { α ^ k } ϕ i − 1 ) , where  α ^ k = α k 1 − α ϕ i − 1  if  k ≠ ϕ i − 1  and  α ^ ϕ i − 1 = 0 . Sample the length of block  i ,  L i ∼ f ( λ ϕ i ) . 
 
We call  f ( λ k )  the  holding distribution  of state  k  and a  jump  is a transition between two consecutive blocks. For our applications it is sufficient to work with this parametrization of the transitions using  α k ’s. All results can be extended to general semi-Markov process. The output sample is a chain of length  L  composed of blocks of distinct states. For the last block, we cut it off so that it stops at  L . For  x ∈ [ 0 , L ] , we denote by  ϕ ( x )  the state of the block that  x  belongs to, i.e.  ϕ ( x ) = ϕ i  if  x  is in block  i . 
 If  f  is the exponential distribution, then the corresponding semi-Markov process is equivalent to a continuous time Markov chain. For state  k , the holding time is the time spent in that state and is an exponentially distributed random variable with rate  λ k . If we observe the states  ϕ ( x )  for an individual semi-Markov process and  L  is sufficiently long, then it is straightforward to perform maximum likelihood inference of the parameters  { λ k , α k } . In genetics and other applications however, we do not observe each individual process but a  pool  of multiple semi-Markov processes where the identity of which process a given state is from is lost.
 Definition Suppose we have  M  independent semi-Markov processes, each of length  L . The  j -th process is parametrized by  { λ k j , α k j } k = 1 K  and the state of the  j -th process in position  x  is denoted by  ϕ j ( x ) . The  pooled semi-Markov process  (abbreviated as PSMP) is obtained by the assignment of each  x ∈ [ 0 , L ]  to the  K -dimensional vector of counts,  Φ ( x ) , such that the  k -th entry is number of elements in  { ϕ j ( x ) } j = 1 M  that equals to  k . We call  Φ ( x ) ,   x ∈ [ 0 , L ] , the observations of the pooled semi-Markov process. The model is parametrized by  Θ ≡ { λ k j , α k j } k = 1 , ... K j = 1 , ... , M . 
 We focus on continuous holding distributions  f  such that with probability 1 each process has a finite number of jumps in  [ 0 , L ] . Let  N  denote the sum of the number of jumps across all  M  processes. Then the continuous observations of the pooled semi-Markov process can be concisely described by the finite set  X = { Φ i , L i } i = 1 N , where  Φ i  is the counts vector observed at the  i -th block across the  M  processes, and  L i  is the length of this block. Note that for continuous distributions, the probability that two blocks of two different semi-Markov chains end at the same point  x  is zero, thus with probability one  Φ i  and  Φ i + 1  differ by one transition. When it is clear from context, we also use the equivalent representation  Φ ( x ) = { ϕ j ( x ) } j = 1 M . Under a pooled semi-Markov process, the likelihood of the parameters Θ is 
 P ( X | Θ ) = ∫ 1 [ ∑ j = 1 M e ( ϕ j ( x ) ) = Φ ( x ) , ∀ x ∈ [ 0 , L ] ] ∏ j = 1 M d P ( ϕ j | { λ k j , α k j } k = 1 , … , K ) 
where  e ( ϕ j ( x ) )  is the unit vector with 1 in the  ϕ j ( x ) th  entry and  d P ( ϕ j )  is the measure induced by the  j th semi-Markov process. In the above integral, the set of hard constraints  1 [ ∑ j = 1 M e ( ϕ j ( x ) ) = Φ ( x ) , ∀ x ∈ [ 0 , L ] ]  are finite (there being a single constraint for each of the  N  blocks). Nevertheless, these constraints make it intractable to exactly compute the likelihood in general. We develop efficient approximations below.
 Example An application of the pooled semi-Markov process naturally arises in the field of population genetics. The diploid genome of an individual consists of one transmitted genome from each parent (the two segments in  Fig. 1 ). The transmitted genome of each parent is a sequence of intervals, where each interval has a different genomic ancestry–(E)uropean, (A)frican, (N)ative American, etc. For example, if the mother is African American, then the genome that she passes on to the offspring is a mosaic of blocks of state A of some length distribution and state E of a possibly a different length distribution, and similarly for the father. Hence the genome passed from the mother to the offspring is well-modeled by a semi-Markov process ( Donnelly, 1983 ;  Gravel, 2012 ). When we genotype the offspring, say the one in  Figure 1 , we can infer that first region has ancestry AA and the second region has ancestry EA (using method described in the next section); however, we do not know whether the E part come from parent 1 or parent 2, and thus the information about the parents’ ancestry is lost. Given the pooled observations of the ancestries (e.g. AA, AE, EE) at every point in the genome, the goal is to infer the parameters  { λ k j , α k j } k = 1 K  for both parents. The  λ ’s parametrizes the length distribution of each ancestry state in a parent and the  α ’s capture the frequency of the ancestry states. With estimates for these parameters, we can infer the global ancestry of each parent, i.e. the fraction of the individual’s genome that is European, Native American or African, as well as the number of generations since the admixture. 
 Fig. 1. Illustration of pooled semi-Markov process. A and E are the states and have different length distributions in the two semi-Markov processes 
 2.2 Algorithms for inference We first consider the parameter estimation problem for pooled semi-Markov processes with exponential holding distributions (which are equivalent to continuous time Markov processes). Exponential holding distribution captures many of the genetic datasets of interests and can often be used as a reasonable approximation to other more complex distributions. We treat this problem in a Bayesian setting in which we place a prior on the parameter vector  Θ ≡ { λ k j , α k j } k = 1 , ... K j = 1 , ... , M  and for a given observation  X ≡ { Φ i , L i } i = 1 N , we compute the posterior probability  Pr ⁡ ( Θ | X ) . We approximate the posterior using a Markov chain Monte Carlo (MCMC). To compute the likelihood as part of the MCMC, we developed a dynamic programming algorithm. In the next section, we show that this method gives accurate posterior estimates of the parameters of the model on genotype data from Mexican and Puerto Rican families. For the non-exponential case, the dynamic programming algorithm needs to keep track of all the possible lengths for which a block of ancestry extends. This becomes expensive when the number of blocks,  N , is large. Hence, we develop a more general stochastic- Expectation-Maximization (EM) algorithm to perform maximum likelihood inference on general pooled semi-Markov chains, as described in the next subsection. Note that there is an inherent symmetry in terms of which process we label as 1, 2, etc. that is not identifiable. Here we assume that the processes are labeled according to some arbitrary but fixed order, and the goal is to recover the parameters up to permutation of labels. 2.3 MCMC inference algorithm for pooled Markov process For given observations  X ≡ { Φ i , L i } i = 1 N  and parameters  Θ ≡ { λ k j , α k j } k = 1 , ... , K j = 1 , ... , M , we use dynamic programming to compute the exact likelihood of the parameters,  P ( X | Θ ) . For each  i = 1 , ... , N , we keep track of all the distinct  ordered  states that are permutations of the observation  Φ i = { ϕ i j } j = 1 M . For example, if  Φ i = { A , E } , then the consistent ordered states are ( A ,  E ), where state  A  is generated by process 1, and ( E ,  A ), where state  A  is generated by process 2. Note that we denote ordered states by  ( )  and unordered states by  { } . For a given unordered set  Φ i , we denote by  { π ( Φ i ) | π ∈ S i }  all the distinct ordered tuples that are consistent with  Φ i . Here  S i  is the permutations of  [ 1 , ... , M ]  that give rise to distinct tuples. For each  π ( Φ i ) , let  P i ( π ( Φ i ) | Θ )  be the probability of observing  { Φ 1 , ... , Φ i − 1 , π ( Φ i ) }  in a pooled Markov chain parametrized by Θ. In other words, this is the probability of seeing the unordered observation up to  i  − 1 and then observing the ordered tuple  π ( Φ i ) . There are in the worst case  K M  such  P i ( π ( Φ i ) | Θ )  for each  i .
 For  i  = 1, if the ordered state is  π ( Φ 1 ) = ( ϕ 1 1 , ... , ϕ 1 M ) , then  P 1 ( π ( Φ 1 ) | Θ ) = ∏ j = 1 M α ϕ 1 j j λ ϕ 1 j j exp ⁡ ( − λ ϕ 1 j j L 1 ) . Given all the probabilities  P i − 1 ( π ( Φ i − 1 ) | Θ ) , we have  P i ( π ( Φ i ) | Θ ) ) = ∑ π ^ ∈ S i − 1 ( π ) P i − 1 ( π ^ ( Φ i − 1 ) | Θ ) P ( π ^ ( Φ i − 1 ) → π ( Φ i ) | Θ ) 
 where  S i − 1 ( π )  are all the tuples that are one edit distance from  π  (since we know exactly one jump in one process has occurred between block  i  − 1 and  i  almost surely) and consistent with  Φ i − 1 , and  p ( π ^ ( Φ i − 1 ) → π ( Φ i ) | Θ )  is the probability for transitioning from  π ^ ( Φ i − 1 )  to  π ( Φ i ) , which can be computed analytically as a product of exponentials and  α k j ’s. For example, suppose the states are  E  and  A , and the observation at  i  is { E ,  E }. In this case there is just one tuple, ( E ,  E ), consistent with it, and
 P i ( ( E , E ) | Θ ) = P i − 1 ( ( E , A ) | Θ ) e − λ E 1 L i λ E 2 e − λ E 2 L i + P i − 1 ( ( A , E ) | Θ ) λ E 1 e − λ E 1 L i e − λ E 2 L i . 
In the first term of the right hand side,  e − λ E 1 L i  is the contribution from continuing  E  with  E  in the first chain, and  λ E 2 e − λ E 2 L i  comes from continuing  A  with  E  in the second chain. And similarly for the second term of the right-hand side. Because there are two states in each chain,  α ’s do not appear. Given this method for computing the likelihood of any observed data  X  for parameters  Θ = { λ k j , α k j } k = 1 K , we use adaptive MCMC to compute the posterior distribution over Θ. The advantage of this approach is that we obtain full posterior distributions of Θ, and for several human populations, it gives accurate estimations (next section). A drawback is that computing the exact likelihood is expensive when the state space is large or when there are many chains—the run time is  O ( K M ) . 2.4 Stochastic EM inference algorithm for general pooled semi-Markov process For general semi-Markov processes with non-exponential holding times, the dynamic programing would have to keep track of the last ordered state as well as its length, making it prohibitively expensive to compute the likelihood. We therefore propose a stochastic EM algorithm to perform parameter inference in general pooled semi-Markov processes. For each block  i , the observation is the unordered set of states  { ϕ j i } j = 1 M . Let  Z  be an  M -by- N  matrix where  Z [ j , i ] ∈ [ 1 , ... , M ]  denote the process that generated the state  ϕ j i  at block  i .  Z  is the matrix of the latent variables. 2.4.1 E-step Given the current values of the parameters Θ and observations  X , it is in general intractable to compute the posterior  p ( Z | X , Θ ) . However we can generate samples  { Z s }  from  p ( Z | X , Θ )  using an efficient sequential procedure using the expansion
 P ( Z | X , Θ ) = P ( Z [ : , 1 ] | X , Θ ) P ( Z [ : , 2 ] | Z [ : , 1 ] , X , Θ ) ... P ( Z [ : , N ] | Z [ : , N − 1 ] , X , Θ ) . 
For the base case, let  Φ 1 = { ϕ 1 1 , ... , ϕ 1 M } , then
 p ( Z [ : , 1 ] | X , Θ ) = p ( Z [ : , 1 ] | Φ 1 , Θ ) ∝ ∏ j = 1 M α ϕ 1 j Z [ j , 1 ] 
subject to the contraint that  Z [ : , 1 ]  is a permutation of  [ 1 , ... , M ] . This can be sampled efficiently using rejection sampling. The vector  Z [ : , i ]  is one edit distance from  Z [ : , i − 1 ] , so that given  Z [ : , i − 1 ]  there are at most  KM -feasible values for  Z [ : , i ] . If vector  W  differs from  Z [ : , i − 1 ]  in index  j , then  p ( W | Z [ : , i − 1 ] , X , Θ )  can be computed as a function of the length of the current state for the  j -th semi-Markov process and the  α k j ’s. Therefore we can explicitly compute the conditional probability of  Z [ : , i ]  given  Z [ : , i − 1 ]  for all values of  Z [ : , i ] . To sample  Z [ : , i ]  we just sample from these conditional probabilities. 2.4.2 M-step Given samples  Z s  of the latent variables, we compute the maximum likelihood Θ by maximizing  ∏ s p ( X , Z s | Θ ) . Given  Z , the parameters  λ k j  and  α k j  are independent for different  j ∈ [ 1 , ... , M ] . For that  r -th semi-Markov process, the optimization problem is  arg max ∏ s p ( { ϕ i j s . t . Z s [ j , i ] = r } , { L i } | λ r , α r ) . For standard distributions, this optimization can be solved analytically. We iterate the E and M steps until convergence. 3 Results 3.1 Mexican and Puerto Rican trios We used 231 Mexican mother-father-child trios and 258 Puerto Rican trios from the Genetics of Asthma in Latino Americans (GALA) study ( Risch  et al. , 2009 ) For each trio, we have the genotypes of the two parents and the offspring across the entire genome. The trios were genotyped using the Affymetrix 6.0 GeneChip Array, which provides measurements of the genome at over 900 000 positions, called single nucleotide polymorphisms. Subjects were filtered based on call rates &gt;95%, consistency between reported and genetic sex, and the absence of any unexpected identity by descent (IBD) or by state. Familial relationships were confirmed using measures of IBD and Mendelian inconsistencies. We used LAMP-LD, a commonly used method, to infer the local ancestry state at each position in the genome in each individual ( Baran  et al. , 2012 ). LAMP-LD uses a generative model in which the genome is divided into non-overlapping windows. An admixed genome is generated as an emission within each window from a HMM with  ( K 2 )  states, where  K  is the number of ancestral populations. Transitions between the hidden states occur between adjacent windows. LAMP-LD computes a Viterbi decoding of the pairs of local ancestries along the genome. Since Puerto-Ricans and Mexicans have mixed ancestry with (E)uropean, (A)frican, and (N)ative American ancestries, LAMP-LD assigns to each position in the genome one of 6 states: EE, NN, AA, EA, EN and NA, depending on the ancestry of that position (e.g. NA corresponds to the case where one copy originated in Africa and the other in America). In these datasets, we observed that the genomes of each of the parents are well approximated by exponential length distribution and hence by a Markov process. The genome of the child can then be modeled as a pooled Markov process, with  M  = 2 and  K  = 3. Note that in general, the genomes of the parents themselves cannot be modeled as a Markov process but as a semi-Markov process ( Gravel, 2012 ). However in these data the exponential distribution proved to be a good approximation, likely because admixture occurred many generations ago in these samples and have been continuing ever since. For the validation experiment, we take as input the observed local ancestry blocks of each offspring, and use the MCMC algorithm described earlier, with uniform priors, to infer the posterior distribution over the parameters of the model. In these data, the MCMC estimates are more accurate than estimates from the stochastic EM (not shown). There are six  λ  parameters and four independent  α  parameters. The global European (or African, Native American) genomic ancestry of an individual is defined to be the proportion of the total genome that is identified to be of European (or African, Native American) descent. For each set of parameters, we infer the global ancestry proportion of the corresponding parent by running a Markov chain with these parameters to equilibrium and computing the fractions. Then we compare the inferred global genomic ancestry of each parent with the true genomic ancestry of the parent computed explicitly by running LAMP-LD. Genomes of Mexican samples contain primarily European (average of 43%) and Native American (49%) ancestries, and a small amount of African ancestry. In contrast, Puerto Ricans genomes contain mostly European (62%) and African (23%) ancestries, with a minor component of Native American. Moreover the two populations have distinct demographic histories leading to different statistical properties of their ancestries, corresponding to different distributions of  λ ′ s  and  α ′ s  ( Bryc  et al. , 2010 ). Hence these two datasets are complementary in exploring the performance of our approach under different conditions. Table 1  contains the  r 2  between our estimated genomic ancestries using PSMP and the true genomic ancestries in the 462 Mexican (MX) parents and 516 Puerto Rican (PR) parents. We report the  r 2  for each of the ancestry states: European (E), Native American (N), African (A). In Mexican trios, our estimated proportions of European and Native American ancestries agree very well with the ground truth (coefficient of determination  r 2  = 0.84 for both). It performs worse in estimating the African proportion, likely because African blocks are only observed a few times in most samples. In Puerto Rican trios, our estimates for the European and African ancestries closely match the ground truth. It performs worse for the less frequent Native American ancestry ( Figs. 2  and  3 ).
 Fig. 2. Comparisons of the estimated genomic ancestry of each parent with the ground truth. The top row is for Mexican samples, each dot corresponding to one parent: European proportions (left), Native American proportions (middle) and African proportions (right). The bottom row is for Puerto Rican samples: European proportions (left), Native American proportions (middle) and African proportions (right) 
 Fig. 3. Comparisons of the estimated and true average block length of each ancestry type. The top row is for Mexican samples, each dot is one parent: average European block length (left), average Native American block length (middle) and average African block length (right). The bottom row is the average block length in Puerto Ricans for European (left), Native American (middle) and African (right) ancestries 
 Table 1. Ancestry estimation accuracy  r 2 MX E MX N MX A PR E PR N PR A PSMP 0.84 0.84 0.35 0.72 0.5 0.75 Offspring 0.76 0.75 0.33 0.63 0.43 0.66 Independent 0.83 0.82 0.27 0.58 0.18 0.41 The first three columns correspond to the Euroean (E), Native American (N) and African (A) ancestries of the Mexican individuals. The last three columns correspond to the European, Native American and African ancestries of the Puerto Rican individuals. In addition to accurately estimating the global genomic ancestries of each parent, our method also infers finer grained information. In particular, since the holding distributions are exponential,  1 / λ  gives the average block length of each ancestry type in a parent. From standard coalescent models of population genetics, these length scales inform us the number of generations since the interbreeding of these populations in the family history of that individual. We compare the inferred length scales for each parent and ancestry type with the ground truth measured on the transmitted allele. In Mexicans, there’s strong correlation between length estimates from our method and the ground truth for European and Native American ancestries ( r 2  of 0.73 and 0.75, respectively). The estimate is less accurate for the less frequent African block lengths ( r 2  = 0.25). For Puerto Ricans, we find the strongest agreement in the block lengths of Africans ( r 2  = 0.75), followed by Europeans ( r 2  = 0.54) and Native Americans ( r 2  = 0.45). 3.1.1 Scalability Our algorithms treat the samples independently and can be run in parallel on all the samples. For each human sample, it required ≤5 min on a standard desktop. 3.2 Comparison to benchmarks In practice, it is often assumed that the genomic ancestry of the offspring is a good approximation of the ancestry of the parents. This only works if the genomic ancestries of the two parents are very similar, since the offspring’s ancestry essentially averages the parents’. This assumption is especially problematic in admixed populations (Latinos, African Americans, etc.) where the two parents may have very different ancestries. We tested this assumption in our trios, where we use the empirically measured genomic ancestry of the offspring as estimations of the parents’ ancestries. The correlation with the true genomic ancestries is reported in the second row of  Table 1 , and it is significantly worse than the results of the pooled semi-Markov process. For more heterogeneous populations, we expect the offspring to be even worse estimators of the parent’s ancestry. The pooled semi-Markov process explicitly models the spatial correlation of nearby states. A simpler algorithm is to assume that all the observations are independent. The accuracy of this simpler model is reported in the third row of  Table 1 . It performs worse than the pooled semi-Markov process in all the categories and is especially poor in Puerto Ricans. 3.3 Simulations 3.3.1 Random trios As an additional validation, we tested our algorithm on simulated Mexican male–female–offspring trios. In the actual trios, using the genotype of the three individuals, we inferred the transmitted allele from each parent to the offspring. To generate a random trio, we then randomly selected a male and a female parent and computationally combined their transmitted alleles to form a new offspring. This creates realistic offspring genotypes while preserving the complex demography encoded in the parents’ transmitted alleles. Using this process, we simulated 100 new trios for which we knew the true genomic ancestry of each individual. As before, we applied our method to the offspring data to infer the ancestries of the parents. Comparison of the inferred ancestries with the ground truth showed very good agreement ( Fig. 4 ). For the European, Native American and African ancestries, we achieved  r 2  of 0.9, 0.89 and 0.77, respectively.
 Fig. 4. On simulated Mexican trios, comparisons of the estimated genomic ancestry of each parent with the ground truth. Each dot corresponds to one parent. The  x -value shows the actual ancestry of the parent and the  y -value shows the inferred ancestry. European, Native American and African ancestries are shown in the left, middle and right panels, respectively 
 3.3.2 Non-exponential chains We also investigated how well we can do inference on pooled semi-Markov processes where the distributions are very different from exponential, as these could be relevant for other demographic models and applications. We consider the particular case where the block lengths of each state are Gaussian distributed. We use the more general stochastic EM algorithm given above to perform inference. In the experiments, we varied  K = 2 , ... , 6  and  M = 2 , ... , 5 . For each combination of  K  and  M , we simulate 50 pools of semi-Markov processes. We consider unit variant Gaussians with mean  λ k . For each process, we sampled  α  uniformly from the  K -dim simplex and sample  λ k  uniformly from [5, 10]. Different processes in the same pool have different  α ’s and  λ ’s. Each observed dataset is created by pooling  M  different Gaussian semi-Markov processes. To better match the quantity and noise of realistic genomic data, we use only the first  N  = 500 blocks of the pooled semi-Markov process as observations. This is the input into our stochastic EM algorithm. To evaluate the estimation, we compute the  r 2  between the estimated  λ ’s and the true  λ ’s and between the estimated and true  α ’s, across all pools and all processes. The results are summarized in  Figure 4 . For  M  = 2, 3, we obtain accurate estimations for even large numbers of states, with  r 2  &gt; 0.8. The accuracy of inference declines as the number of processes in a pool increases. In these more complex models, we can improve our accuracy by collecting larger number of observations ( N ) from each pool ( Fig. 5 ).
 Fig. 5. Experimental validation of inference for  λ  (left) and  α  (right) for pooled Gaussian semi-Markov processes. The X axis corresponds to the number of states  K  and different line colors correspond to different number of processes  M . For the  α  estimations,  K  = 2 is trivial since all the  α ’s are 0.5, and is omitted 
 4 Discussion We developed an efficient method to infer the genomic ancestry of the parents from the genotype of an offspring. We applied our method to genotype data of 231 Mexican and 258 Puerto Rican individuals to infer the parents’ ancestries. We showed that the method is highly accurate by comparing the inferred ancestries with each parent’s true genomic ancestries. We further validated the method on simulated trios. For pooled Markov processes, we showed how to compute likelihood exactly using dynamic programming. For general pooled semi-Markov processes, we developed a stochastic EM algorithm to infer the model parameters. We additionally validated accuracy of our inference algorithm in settings where the semi-Markov length distributions are Gaussians. We tested our algorithm on Latino trios, but it can be applied to other admixed populations and can be used to infer ancestries other than European, Native American and African. The method can be used on general genotype datasets of unrelated, unphased individuals, for which large cohorts exist, to infer the genomic ancestries of the parents. This has immediate applications in investigating assortative mating in human populations. The current approach assumes that the local ancestry of the offspring has been computed from his/her genotype. This is reasonable for large admixed populations such as Latinos and African Americans, where existing algorithms (e.g. LAMP-LD) can accurate infer the local ancestries. For other admixed populations, an interesting direction of future work is to jointly infer the local ancestry of the offspring and the global ancestries of the parents Funding EH is a faculty fellow of the Edmond J. Safra Center for Bioinformatics at Tel Aviv University. EH was partially supported by the  Israeli Science Foundation  (grant  1425/13 ), by the  National Science Foundation  grant  III-1217615 , and by the  United States-Israel Binational Science Foundation  (grant  2012304 ). Conflict of Interest : none declared. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>EthSEQ: ethnicity annotation from whole exome sequencing data</Title>
    <Doi>10.1093/bioinformatics/btx165</Doi>
    <Authors>Romanel Alessandro, Zhang Tuo, Elemento Olivier, Demichelis Francesca, Stegle Oliver</Authors>
    <Abstract/>
    <Body>1 Introduction Interrogation of the entire coding genome for germline and somatic variations through Whole Exome Sequencing (WES) is rapidly becoming a preferred approach for the exploration of large cohorts (such as The Cancer Genome Atlas initiative) especially in the context of precision medicine programs ( Beltran  et al. , 2015 ). In this setting, the estimation of individual’s ethnical background is fundamental for the correct interpretation of variant association studies and of personal genomic variations importance ( Petrovski and Goldstein, 2016 ;  Price  et al. , 2006 ;  Spratt  et al. , 2016 ;  Zhang  et al. , 2016 ). To enable effective annotation of individual’s ethnicity and improve downstream analysis and interpretation of germline and somatic variations, we developed EthSEQ, a tool that implements a rapid and reliable pipeline for ethnicity annotation from WES data. The tool can be used to annotate ethnicity of individuals with germline WES data available and can be integrated in any WES-based processing pipeline. EthSEQ also exploits multi-core technologies when available. 2 Approach EthSEQ provides an automated pipeline, implemented as R package, to annotate the ethnicity of individuals from WES data inspecting differential SNPs genotype profiles while exploiting variants covered by the specific assay. As input the tool requires genotype data at SNPs positions for a set of individuals with known ethnicity (the  reference model ) and either a list of BAM files or genotype data of individuals with unknown ethnicity. EthSEQ then annotates the ethnicity of each individual using an automated procedure ( Supplementary Fig. S1a ) and returns detailed information about individual’s inferred ethnicity, including aggregated visual reports. The  reference model  builds on genotype data of individuals with known ethnicity; 1000 Genome Project individuals data is here used to construct platform-specific reference models relying on the most conserved ethnic groups EUR (Caucasian), AFR (African), EAS (East Asian) and SAS (South Asian) for multiple WES designs: Agilent HaloPlex, Agilent SureSelect and Roche Nimblegen ( Supplementary Methods ). More generally, given a set of genomic regions and genotype data of a set of individuals annotated for ethnicity, a procedure to automatically generate a reference model is also provided by EthSEQ. The  target model  is created either from the input list of individual’s germline BAM files that are genotyped at all reference model’s positions using the genotyping module of ASEQ ( Romanel  et al. , 2015 ) (depth of coverage ≥ 10X and read/base mapping qualities ≥ 20 here required by default to guarantee confident genotype calls) or from genotypes provided as input to EthSEQ in VCF format. Principal component analysis (PCA) is next performed by means of  SNPRelate  R package ( Zheng  et al. , 2012 ) on aggregated target and reference models genotype data; only SNPs that satisfy user-defined call rate are retained. The space defined by the first two PCA components is then automatically inspected to first generate the smallest convex sets identifying the ethnic groups described in the  reference model  and next to annotate the ethnicity of the individuals of interest ( Supplementary Fig. S1b ). Individuals positioned inside an ethnic group set (or intersecting group sets) are annotated with the corresponding ethnicity and labeled with INSIDE. For individuals positioned outside all ethnic group sets, the relative contribution of each group is computed through the distances from the centroids using the procedure described in  Supplementary Figure S2 , and top ranked contributing groups are reported (labeled CLOSEST). To better discern ethnicity annotations across ancestrally close groups within a study cohort (for instance Ashkenazi and Caucasians), a multi-step inference procedure is provided. Given a tree of ethnic group sets such that sibling nodes have non-intersecting ethnic groups and child nodes have ethnic groups included in the parent node ethnic groups, ethnicity of individuals is inferred by a pre-order traversal of the tree. At each node with ethnic groups  S , annotations resulted from the analysis of the parent node is refined by reducing both  reference  and  target  models on individuals with annotations in  S  only. Global annotation of all individuals is updated throughout the tree traversal. 3 Performances and results Performances of EthSEQ ethnicity inference method were tested for precision, computational time and dependence on SNP set size on two main datasets, 1000 Genomes Project genotype data and germline samples TCGA data. Initial precision tests utilized 1000 Genomes Project data; we randomly divided 2096 individuals into reference and target model groups while preserving the ethnic groups proportions, and ran EthSEQ relying only on SNPs in WES platform-specific captured regions ( Supplementary Methods ). Analyses were performed using reference models either built considering major ethnic groups annotations (EUR, AFR, EAS and SAS) or considering annotations for all the corresponding 21 populations reported in the 1000 Genome Project. In the first case, individuals’ ethnicities were all correctly classified (100% precision and more than 97% of the individuals annotated with the INSIDE label) ( Supplementary Fig. S3, Table S1 ). When the fine-grained annotation was used, ethnicity inference reached a precision of 92.2% with the multi-step refinement analysis. For instance, when considering European individuals only that includes 5 populations, precision reached 94% ( Supplementary Methods  for details). Finally, EthSEQ performances were compared to LASER 2.0  trace  module ( Wang  et al. , 2015 ) performances on the same data. Results in the PCA space were highly concordant ( Supplementary Figs S4 and S5 ), but EthSEQ was up-to 10X faster using a single core and up-to 18X faster when exploiting parallel computation ( Supplementary Fig. S6 ) on multi-individual analyses ( Supplementary Methods  for details). Further, EthSEQ was ran on germline WES data from 604 TCGA (cancergenome.nih.gov) individuals with reported interview-based race classification (as per TCGA nomenclature, race is annotated as 513  White , 42  Black or African American  and 49  Asian ). 505  White  individuals were annotated by EthSEQ as EUR, 37  Black or African American  individuals as AFR and 48  Asian  individuals as EAS or SAS for an overall precision of 97.7%. EthSEQ results were compared to results from fastSTRUCTURE tool ( Raj  et al. , 2014 ) fed with genotype data generated by EthSEQ pre-processing module. For 594 individuals (98.3%) the two analyses inferred the same major ethnic contribution. Both tools inferred 5 individuals originally annotated as  Black or African American  in TCGA as admixed with a major Caucasian contribution, one originally annotated as  Asian  as non-admixed Caucasian, and two originally annotated as  White  as major African contribution (see  Supplementary Methods and Table S1 ). In terms of tool specific results, 4.6% of individuals were inferred as admixed by fastSTRUCTURE that explained the TCGA dataset population structure with 3 clusters achieving a precision of 98%; 7.9% of individuals were inferred as CLOSEST by EthSEQ with the majority with SAS main contributions, not captured by fastSTRUCTURE, and secondary African contribution correctly detected by EthSEQ when above 15%. EthSEQ analysis resulted 3.2X faster. The effectiveness of the multi-step refinement analysis was recently proven in a precision medicine setting study ( Zhang  et al. , 2016 ) were ethnicity based stratification was key to interpret the relevance of germline cancer-associated variants. Specifically, our analysis ruled out the possibility that the high fraction of germline cancer-associated variants observed in the clinical cohort of 343 patients with metastatic tumors ( Beltran  et al. , 2015 ) was due to the presence of Ashkenazi inheritance ( Carmi  et al. , 2014 ) shown to carry high percentage of cancer-associated variants. Provided an Agilent HaloPlex reference model including Ashkenazi genome data ( Carmi  et al. , 2014 ) the identification of Ashkenazi individuals required the multi-step analysis to precisely discern them from the ancestrally close European individuals; 29.7% of Ashkenazi individuals were identified confirming the anticipated fraction of about 30% based on an internal cancer registry. To measure the impact of the number of available SNPs on EthSEQ precision, we extended the performance analyses by randomly down-sampling the number of SNPs both in the 1000 Genome Project and in the TCGA dataset ( Supplementary Methods ).  Supplementary Figure S7  shows that using the multi-step refinement analysis, 2000 SNPs are sufficient to reach more than 98% precision. Overall, this data indicates that EthSEQ is also amenable to targeted sequencing NGS data. 4 Conclusions We presented EthSEQ, a rapid, reliable and easy to use R package to annotate individuals ethnicity from WES data. EthSEQ can be used to process single sample or multi-sample datasets, provides a large variety of pre-computed platform-specific reference models, a simple and transparent mode to generate ethnicity annotations starting from a list of BAM files and can be easily integrated into any WES based processing pipeline also exploiting multi-core capabilities. Funding This work has been supported by the Prostate Cancer Foundation Challenge Award 2014 (F.D., A.R.), the Caryl and Israel Englander Institute for Precision Medicine, New York and the European Research Council ERCCoG648670 (F.D.). 
 Conflict of Interest : none declared. Supplementary Material Supplementary Data Click here for additional data file. Supplementary Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Inference of historical migration rates via haplotype sharing</Title>
    <Doi>10.1093/bioinformatics/btt239</Doi>
    <Authors>Palamara Pier Francesco, Pe’er Itsik</Authors>
    <Abstract>Summary: Pairs of individuals from a study cohort will often share long-range haplotypes identical-by-descent. Such haplotypes are transmitted from common ancestors that lived tens to hundreds of generations in the past, and they can now be efficiently detected in high-resolution genomic datasets, providing a novel source of information in several domains of genetic analysis. Recently, haplotype sharing distributions were studied in the context of demographic inference, and they were used to reconstruct recent demographic events in several populations. We here extend the framework to handle demographic models that contain multiple demes interacting through migration. We extensively test our formulation in several demographic scenarios, compare our approach with methods based on ancestry deconvolution and use this method to analyze Masai samples from the HapMap 3 dataset.</Abstract>
    <Body>1 INTRODUCTION Recent advances in high-throughput genomic technologies enable population-wide surveys of genetic variation. Although exacerbating challenges associated with data handling, this increase in volume and resolution had the effect of exposing new genomic features, creating the need for new models and computational tools. Among these new genomic features, identical-by-descent (IBD) haplotypes have recently emerged as a new source of information in several genetic applications, ranging from genotype–phenotype association studies ( Browning and Thompson, 2012 ;  Gusev  et al. , 2011 ) to the reconstruction of recent familial relationships ( Huff  et al. , 2011 ;  Kirkpatrick  et al. , 2011 ), the inference of recent demographic events ( Gusev  et al. , 2012 ;  Palamara  et al. , 2012 ) or the study of natural selection ( Albrechtsen  et al. , 2010 ;  Han and Abney, 2012 ). IBD segments are co-inherited from recent common ancestors by pairs of individuals and are delimited by historical recombination events. Such recombination events can now be detected in cohorts that have been densely genotyped (although not requiring the availability of full sequences), and several methods have now been developed for efficient IBD detection in large datasets ( Browning and Browning, 2011 ;  Gusev  et al. , 2009 ). Although shared haplotypes are found to be common even across populations that diverged hundreds of generations ago ( Gusev  et al. , 2012 ), the average detectable IBD segment is transmitted from shared ancestors that lived tens to a few hundreds of generations before present. Haplotype sharing analysis is, therefore, suitable to reveal the signature of the relatively recent demographic events that followed the agricultural revolution, where most classical methods provide limited insight. Leveraging this property of IBD, several recent surveys relied on shared haplotypes to analyze population demographics ( Atzmon  et al. , 2010 ;  Henn  et al. , 2012 ;  Lawson and Falush, 2012 ). In a recent work ( Palamara  et al. , 2012 ), we studied several theoretical quantities of IBD haplotypes, as a function of the demographic history of a population. We used the derived framework to infer the demographic history of two populations with different characteristics: (i) a population that underwent substantial recent isolation (Ashkenazi Jews) and (ii) a cohort that deviates from a single population model, with migration across small demes likely playing an important role in shaping recent genomic diversity (Kenian Masai). The analytical models we previously described are limited by the assumption that all the analyzed samples belong to a single population. Although such models can be used to provide insights in cases of extreme historical isolation, fine-scale interactions across populations were frequent in recent history, and the reconstruction of these events is of great interest for genetic-driven investigation of historical events ( Henn  et al. , 2010 ) and genetic analysis at large. In this article, we propose an extension of the analytical framework described in  Palamara  et al.  (2012) , allowing to explicitly model the presence of multiple populations that interact through migration rates. We test our approach on several synthetic populations with known population size changes and migration rates, finding that our model accurately matches the empirical distributions and provides a novel tool for the inference of recent demographic events that involve multiple interacting demes. We compare our method with existing approaches based on the distribution of migrant tracts obtained through ancestry deconvolution, and we revisit the analysis of the Masai population using the presented formulation. 2 METHODS 2.1 Haplotype sharing and demographic history Here, we provide a brief summary of the formulation developed in  Palamara  et al.  (2012) , and we invite the reader to refer to that article for additional details. At a chosen genomic site, a pair of modern day individuals from a population will have a common ancestor that lived a number of generations in the past. Such common ancestor transmits several adjacent sites along with the one being considered, in a region that is delimited by any recombination event happening along the lineage between the two individuals on either side of the locus ( Fig. 1 ), and by chromosome boundaries. We define such non-recombinant region as IBD. Recombination shortens IBD segments during meiotic transmission, and the genetic length of shared haplotypes is probabilistically linked to the number of generations separating two individuals from their most recent common ancestor. In addition, standard assumptions of coalescent theory ( Kingman, 1982 ) postulate that when tracing the ancestry of a pair of individuals back in time, the chance of randomly finding their common ancestor is inversely proportional to the effective size of the analyzed population, with a smaller effective population size resulting on average on earlier common ancestors. Combining these principles, the length of IBD segments detected in a cohort of studied individuals can be used to gain insight into the distribution of coalescent times, which in turn can be used to gain insight into the effective population size within and across populations at different time scales.
 Fig. 1. An IBD segment (blue) is co-inherited by two present day individuals from a common ancestor that lived four generations in the past. Recombination shortens the IBD segment, as meiotic events occur along the lineage between the two individuals In the remainder of this article, a population’s effective population size in a coalescent model will simply be referred to as population size. We represent the demographic history of the studied population via the vector  , which may hold just one parameter in the simplest case of a constant (Wright–Fisher) population, or several parameters in more complex cases (e.g. current population size, ancestral population size and duration of an exponential expansion). The probability of the considered genomic site to be spanned by a shared IBD haplotype of genetic length comprised in the range   can be expressed as
 (1) 
where  , in the reminder simply written  , represents the probability of finding the common ancestor for the considered site at (continuous) time  t  in the past, measured in generations;  , later indicated as  , represents the probability of a segment spanning the site to have length  l  after being transmitted for  t  generations. In this model, population size is allowed to arbitrarily change in time. In the simple case of a Wright–Fisher population of constant size  N e , the coalescence probability is simply  . Recombination events may happen on either side of the considered locus at an exponential rate that depends on the number of meiotic events in the lineage to a common ancestor.  , therefore, assumes the form of a sum of two exponential random variables or Erlang-2 distribution:   (note that length here is expressed in Morgans). Combining these into  Equation (1)  and integrating, we obtain
 (2) 
or   for the particular case of  . As shown in  Palamara  et al.  (2012) ,  Equation (2)  can be used to obtain a closed form estimator of recent effective population size. Taking the limit of such estimator for  , it assumes the form
 (3) 
where   is the average observed fraction of genome shared through segments longer than a length threshold  u  (here in morgans). The computation of   allows us to derive several additional theoretical quantities of IBD sharing. Because of the linearity of the expectation operator, the average fraction of genome shared through IBD segments in the length range  R  is simply  . The distribution of the length of a randomly sampled segment shared by the pair of individuals is obtained as  , and it can be used to compute the expected length of a randomly sampled shared segment in the chosen length range,  s R . For a region of length γ, a pair of individuals is expected to share   segments, and the distribution for the number of shared segments can be modeled as a Poisson random variable with the aforementioned expectation. Using this information, an expression for the variance of the fraction of genome shared in an interval  R  can be computed. Finally, the full probability distribution for the fraction of genome shared by a pair of individuals through segments in the length range  R  can also be computed using the previously described quantities. The quantity   is, therefore, central in this formulation, as it allows expressing a number of different measures of IBD sharing as a function of demographic history. Furthermore,   only depends on   through  , the probability of a coalescence event in the demographic scenario  . If the goal is to infer the demographic parameters in a model comprising multiple populations, we, therefore, need to express the coalescence probability  , where   now includes historical size variation for multiple populations and migration rates across them. 2.2 IBD distributions in the presence of migration We begin discussing the case of multiple populations referring to a simple scenario, where two populations of constant size  N e  exchange individuals at a fixed rate  m  per individual, per generation (see model in  Fig. 2 a). We encode this migration rate using the matrix
 
 Fig. 2. Two demographic models that involve two populations and migration between them. In model ( a ), the populations have the same constant size  N e , and exchange individuals at the same rate  m . In model ( b ), a population of constant ancestral size  N atot  splits  G  generations in the past, resulting in two populations whose sizes independently fluctuate from   and   individuals to   and   individuals during  G  generations. During this period, the populations interact with asymmetric migration rates  m 12  and  m 21 We consider two individuals,  i  and  j , each sampled from either population. We trace the ancestors of these individuals at one genomic site and encode their state (in terms of population their ancestors belong to), using a vector of dimensionality 2. If individual  i  is sampled from population 1 and individual  j  from population 2, for example, the state at generation 0 is known and we write it as  . If both are sampled from population  . After  t  generations (measured in continuous time), the probability that the ancestor of individual  i  at this genomic location belongs to either population is given by
 (4) 
if individual  i  was sampled from population 1, or, symmetrically
 (5) 
if it was sampled from population 2. We are interested in expressing the probability of individuals  i  and  j  to coalesce at time  t . This requires both individuals to be in the same population, in which case coalescence happens at rate  . Formally  , which in this setting becomes
 (6) 
if  , and
 (7) 
otherwise. To compute  , we plug the coalescence probability in  Equation (1) . Also, for simplicity, we take  , obtaining
 (8) 
if  , and
 (9) 
otherwise. Recall that  , which is the expected fraction of genome shared through segments of length between  u  and  v  by an individual pair. To infer   and  , we, therefore, consider the observed average fraction of genome shared through IBD segments longer than a threshold  u , for all pairs of individuals sampled from the same population or from different populations (which we call   and  , respectively, now omitting the dependence on the length range). We then solve the system obtained by equating   and   to the quantities in ( 9 ) and ( 10 ), to obtain the estimators
 
 (10) 
 A simple generalization of the aforementioned scenario consists in allowing the two considered populations to differ in their effective population sizes,   and  . In this scenario, it is still possible to obtain a closed form expression for  , and a closed form estimator for  , which are reported in the Appendix. 2.3 The general case Although the previously discussed case of constant population sizes and migration rates has a simple formulation and can be used to gain initial insight into the recent demography of a study cohort, such population dynamics are oversimplified and generally unrealistic. Luckily, given a few reasonable assumptions, population sizes and migration rates can be allowed to arbitrarily fluctuate in time, still permitting a closed form computation of  . Consider two populations whose sizes at generation  g  are expressed as   and  . The rate at which these two populations exchange individuals can be encoded in a discrete migration matrix
 (11) 
where   represents the probability of an individual migrating from population 1 to population 2 at generation  g  (backwards in time). After  g  generations, the probability that the ancestor of individual  i  at a genomic location belongs to either population is given by the vector  . Define the matrix   to be diagonal with   and   as its diagonal elements. The probability of coalescence from generation  g  − 1 to generation  g  is then
 (12) 
and the probability of the two individuals to coalesce  g  generations before present is
 (13) 
 Equation (13)  can be used in  Equation (1) , in its discrete version, to compute
 (14) 
 Note that  Equation (14)  is general, and we can allow additional demographic changes to take place. For instance, by setting   and   for all  , we encode a population split that occurred  G  generations ago. In practice, a pair of populations will have split a number of generations back in time, and it is, therefore, convenient to consider models of the kind depicted in  Figure 2 b. In this model, a population of constant size  N atot  splits  G  generations in the past, forming two populations of size   and  . The size of these two groups then fluctuates in time, to reach a present size of   and  . During their separation, the populations exchange individuals at a rate of  m 12  and  m 21  per generation, per individual. Of course, other models can be defined, allowing variable migration rates, and different population size dynamics. For mathematical convenience, it is safe to assume the ancestral population size becomes constant a number of generations in the past. Models where the ancestral population size ( N atot  in  Fig. 2 b) is constant from generation  G  to infinity allow for a closed form computation of  Equation (14) , no matter which demographic dynamics take place from generation 0 to  G  [see  Palamara  et al.  (2012)  for this expression]. Furthermore, extremely remote demographic events have negligible impact on shared haplotypes of currently detectable lengths (e.g.   cM). 2.4 Simulations, ancestry deconvolution and real data We tested our framework using extensive simulation of realistic chromosomes under several demographic models, using the GENOME coalescent simulator ( Liang  et al. , 2007 ). For computational convenience, we set the size of the simulator’s non-recombinant segments between 0.01 and 0.025 cM, as specified in  Section 3 , always using a recombination rate of 1cM/Mb. A modified version of the simulator was used to extract ground truth IBD haplotypes from the simulated genealogies, defined as non-recombinant segments co-inherited by pairs of individuals from their most recent common ancestor. For some of the simulations, we inferred shared haplotypes using the GERMLINE software package ( Gusev  et al. , 2009 ) on phased genotype data, which were obtained setting GENOME’s mutation rate to   per base pair ( Roach  et al. , 2010 ). Genotypes were post-processed to mimic the information content of array data. To this extent, we computed the allele frequency spectrum of European individuals from the HapMap 3 dataset ( Frazer  et al. , 2007 ), using frequency bins of 2%. We then randomly selected the same proportion of alleles from the simulated genotypes. We obtained an average density of   single nucleotide polymorphisms/Mb. To compare the proposed IBD-based approach for migration inference to the approach of  Gravel (2012) , which is based on ancestry deconvolution, we simulated synthetic datasets under several demographic models and extracted genotype data as previously described. We then ran the PCAdmix software ( Brisbin  et al. , 2012 ) with windows of size 0.3cM and the genetic map used in the simulations. The output of PCAdmix was used to infer migration rates via the Tracts software package ( Gravel, 2012 ). IBD information was computed in the same datasets running the GERMLINE software, and the output was used to infer migration rates using the DoRIS software package, which implements the proposed framework. Perfectly phased haplotypes were used in input for both PCAdmix and GERMLINE. Only migration rates were inferred, whereas all other demographic parameters were set to the true simulated values for both Tracts and DoRIS. To demonstrate the use of the DoRIS framework on real data, we analyzed 56 trio-phased samples from the HapMap 3 dataset. Phased genotypes were downloaded from the HapMap 3 webpage at  http://hapmap.ncbi.nlm.nih.gov . IBD haplotypes were extracted using GERMLINE, as previously described in  Palamara  et al.  (2012) . 3 RESULTS 3.1 Constant size and symmetric migration rates To test the accuracy of demographic inference based on the proposed model, we initially simulated a number of populations of constant size  N e , which exchange individuals at a constant, symmetric migration rate  m , as depicted in the model of  Figure 2 a. We simulated 15 possible sizes of synthetic populations, ranging from 2000 to 30 000 haploid individuals, with increments of 2000. For each population size, we simulated 11 possible migration values, uniformly chosen between   and  . For a total of 165 datasets, we simulated a chromosome of 300 cM for 500 haploid individuals from each subpopulation and computed IBD sharing within and across populations. The simulations used non-recombining blocks of 0.02 cM. This resolution may introduce small biases in the analysis, which we found to be negligible in our previous work. We then used  Equation (10)  to estimate   and  , with results shown in  Figure 3 . To test the model’s accuracy, for this analysis, we only considered ground-truth IBD segments extracted from the synthetic genealogies (see  Section 2 ).
 Fig. 3. True versus inferred parameters for the model in  Figure 2 a. Estimates were obtained using  Equation (10) We obtained a good correspondence between the true population size and the size inferred via the estimator of  Equation (10) , with almost perfect correlation shown in  Figure 3 a. Inferred migration rates were also close to the simulated rates, although a moderate upward bias and higher estimation variance for large migration rates was observed in this case ( Fig. 3 b). In addition to using the effective population size estimator of  Equation (10) , we used the estimator previously computed in  Palamara  et al.  (2012)  for the case of constant population with no migration, reported in  Equation (3) . As expected, the inferred recent effective population size was in this case inflated by the presence of migration, as shown in  Figure 4 . When migration rates are increased, the inferred population size quickly approaches the total population size (in this case  ).
 Fig. 4. Inference of recent effective population size using  Equation (3) , which neglects migration. The ratio between inferred and true population size ( y -axis) increases as the migration rate ( x -axis) is increased, approaching the sum of population sizes for both populations (twice the true size) 3.2 Dynamic size and asymmetric migration rates We then tested our model’s performance in the more complex demographic scenario depicted in  Figure 2 b, where a population splits into two subpopulations that grow at different exponential rates, interacting with asymmetric migration rates. We simulated a chromosome of   cM for 500 haploid individuals per subpopulation. Simulated non-recombinant blocks had size 0.025 cM. In all simulated scenarios, we kept  N atot  fixed to 10 000 haploid individuals, whereas   and   were kept fixed at 5000 individuals. For   and  , we simulated all possible combinations of sizes between 5000 and 205 000 haploid individuals, with increments of 15 000 (excluding cases where  ). Note that on average, the simulated values of   were smaller, resulting in higher inference accuracy compared with  . For each pair of population sizes, we simulated values of  m 12  and  m 21  using all combinations of the migration rates 0.0001, 0.0167, 0.0334 and 0.5. A total of 540 synthetic populations were tested. For each synthetic population, we extracted the average fraction of genome shared through haplotypes of different length intervals by pairs of individuals within each population or across populations. As in our previous work, we used a combination of intervals of uniform length and length intervals corresponding to quantiles of the Erlang-2 distribution, which is used in  . Inference performance was tested via minimization of the root-mean-squared deviation between observed and predicted average fraction of shared genome. Note that a likelihood-based approach (e.g. considering the number of shared segments) could be used based on the quantities derived in  Section 2.1 . We scanned several possible values for one parameter at a time, performing a line search while fixing the remaining model parameters to the true simulated value. The results of this analysis are reported in  Figure 5 .
 Fig. 5. Results of the evaluation of our method on synthetic populations with demographic history depicted in the model of  Figure 2 b. Higher variance in the method’s accuracy is observed because of limited sample sizes and increased population sizes. Higher migration rates further decrease the rate of coalescent events in the recent generations ( Fig. 5 b), resulting in additional uncertainty. However, no significant bias is observed in the inference As expected, because of the large recent effective population sizes we simulated, the variance of the inference accuracy was higher in this scenario, suggesting that more than a single chromosome for 500 diploid individuals may be required for the analysis of these demographics. A single chromosome of   cM sampled in 500 diploid individuals is in fact equivalent for the purpose of this inference to the analysis of all the autosomal chromosomes for   diploid samples (see  Palamara  et al. , 2012 ). Larger population sizes result in lower signal-to-noise ratio for the estimation of the expected fraction of genome shared via IBD segments, and increasing sample size or analyzing additional chromosomes is expected to reduce the variance in the inference performance. Lower accuracy was observed in the inference of   since, as previously mentioned, this simulated subpopulation was on average larger. Inferred population sizes were more accurate in the presence of low-migration rates (represented by colors in  Fig. 5 a and b), as high migration further reduces the chance of early coalescent events, exacerbating the effects of large population sizes. Overall, no significant bias was observed in the recovered parameter values, suggesting our model provides a good match for the empirical distributions. 3.3 Applicability of the model to genotype data Although the previous analysis was mainly concerned with testing the model’s accuracy, and it relied on ground-truth IBD sharing extracted from the simulated genealogies, it is interesting to ask whether this approach can be used on genotype data. To this extent, we simulated genotypes for the demographic model of  Figure 2 a. We set the population sizes to 4000 or 12 000 diploid individuals per population, and extracted 300 diploid sampled from each group. The migration rate was symmetric and set to 0.04 per individual, per generation. Chromosomes of 150 cM were simulated using non-recombinant blocks of size 0.01 cM, and the synthetic genotypes were post-processed to reproduce the density and allele frequency spectrum of realistic SNP array data (see  Section 2 ). In addition to extracting the ground truth IBD information as previously described, we inferred IBD haplotypes from the simulated genotypes using the GERMLINE software. The results suggest that when accurate phase information is available (e.g. for the X Chromosome, or for trio-phased samples), GERMLINE is able to recover the IBD sharing distribution across any pair of samples with high fidelity ( Fig. 6 ). However, when the samples were computationally phased using the Beagle software ( Browning and Browning, 2007 ), GERMLINE had an inconsistent performance, accurately recovering the IBD sharing in the case of  N  = 4000, whereas poorly inferring long haplotypes in the case of  N  = 12 000. This suggests that additional care must be taken when analyzing computationally phased data, particularly when analyzing cross-population IBD spectra, were the quality of the inferred IBD haplotypes will likely vary from population to population, as a result of different underlying demographic histories.
 Fig. 6. We simulated a chromosome of 150 cM for 600 individuals using the model in  Figure 2 a, setting population sizes to 4000 and 12 000 diploid individuals, with a migration rate of 0.04. IBD sharing was extracted directly from the simulated genealogy (diamonds), or inferred trough GERMLINE using perfectly phased (circles) or computationally phased (triangles) chromosomes 3.4 Real data analysis To demonstrate the applicability of our method to real data, we analyzed the HapMap 3 Masai dataset, which was already studied in our previous work using a simulation-based approach. We here revisit this analysis, using the described analytical framework. Cryptic relatedness across individuals in this dataset is extremely common, and it does not appear to be because of the presence of occasional outliers among the samples. Demographic reports are not supportive of recent population bottlenecks in this group, which is, though, to be slowly but steadily expanding ( Coast, 2001 ). The Masai are a semi-nomadic people, and individuals often reside in small communities ( Manyatta ) of tens to few hundreds of members. To study their demography, we, therefore, use a model where  V  villages of constant size  N  exchange individuals at a constant and symmetric rate  m . This model is similar to the one depicted in  Figure 2 a, with symmetric migration rates across several populations. We assumed that all samples were extracted from the same village and used the model described in  Section 2.3  for the analysis. We performed a grid search testing migration rates from 0.01 to 0.4, with intervals of 0.01, village sizes from 50 to 4000 with steps of 10 and number of villages from 3 to 150 with increments of 1. We also obtained 95% confidence intervals for the inferred values using a bootstrap approach, by creating 400 re-samples randomly selecting individuals with replacement, then re-computing the optimal parameters using a gradient-driven procedure, which was initialized using the parameters inferred using the original samples (note, however, that small correlations exist for IBD sharing across individual pairs, and this method may provide optimistic intervals). Using this approach, we obtained the following estimates:  V  = 58 (95% CI: 46–75),  N  = 400 (95% CI 360–470) and   (95% CI 0.09–0.12). 3.5 Comparison with existing methods The structure of long-range haplotypes is known to carry relevant information about recent population dynamics, but this genomic feature has only recently become observable thanks to the development of modern high-throughput genomic technologies. As a consequence, methods that rely on a population’s haplotypic structure to reconstruct demographic events have only recently arose. A model proposed in  Pool and Nielsen (2009) , and recently expanded in  Gravel (2012) , provides a way to analyze the distribution of migrant tracts and infer the timing and intensity of recent migration events. To analyze the distribution of migrant haplotypes, however, ancestry deconvolution needs to be accurately performed. This typically requires the availability of two suitable reference populations, which are required to be sufficiently diverged from each other. The amount of required divergence depends on the specific method used for the deconvolution, but in general, this poses significant constraints in terms of the demographic scenarios that can be analyzed using these methods. To compare our IBD-based approach with methods based on ancestry deconvolution, we simulated the demographic scenario of  Figure 7 , where two populations split  G s  generations in the past, and  G a  generations in the past contribute a fraction of genomes to the creation of a group of admixed individuals, with probability  m  and  , through a unique pulse of migration. All three population sizes were fixed to either  N  = 5000 or  N  = 10 000, m was set to 0.2 and  G a  was 25 in all simulations. We varied  G s  from 40 to 600, with increments of 20, and extracted genotype data on a single 400 cM chromosome for 250 diploid samples in each of the three extant populations (see  Section 2 ). We used the output of the PCAdmix software as input for the Tracts program ( Gravel, 2012 ), and the IBD segments retrieved by GERMLINE as input for the DoRIS software. Note that for the IBD analysis, we only used the 250 admixed samples and the 250 samples from the population contributing   haplotypes at generation  G a , whereas the samples from the third population were ignored. In both cases, we inferred the value of  m , setting all other parameters to the true simulated values, with results shown in  Figure 8 .
 Fig. 7. The model used to simulate admixed populations 
 Fig. 8. We created several simulation genotype datasets using the model in  Figure 7 , varying  G s  while keeping  , and using constant populations of size 5000 or 10 000 diploid individuals. We inferred the value of  m  using PCAdmix + Tracts, or GERMLINE + DoRIS, here reported as a function of  G s DoRIS performed better on average (mean inferred  , std 0.025), although providing slightly noisy results, suggesting the need for a larger sample size and/or the analysis of additional chromosomes. The migration rate inferred by Tracts (mean  , std 0.0233) was strongly biased. We note that in this setting, Tracts is essentially used to only report the proportion of ancestry inferred by the deconvolution method, which is the actual source of inaccuracy. Even for populations that diverged 600 generations in the past (∼15 000 years before present assuming a generation of 25 years), the recovered rate was substantially lower than the simulated rate. The case of  N  = 5000 yielded better estimates because of the higher drift found in smaller populations, which improved the power of PCAdmix to call migrant tracts. We additionally run the PCAdmix + Tracts analysis on longer time scales, simulating values of  G s  from 200 to 6000, with intervals of 200 generations, using  N  = 10 000. Even for several thousand generations since the split of the reference populations, a small bias was observed ( Fig. 9 ).
 Fig. 9. We created several datasets using the model in  Figure 7 , varying  G s  from 200 to 6000, and using   with population sizes of 10 000 diploid individuals. We inferred the value of  m  using PCAdmix + Tracts from phased genotype data This analysis suggests that although the methods that rely on ancestry deconvolution are a useful tool for the specific case of recently admixed groups arising from strongly diverged populations, they may not be suitable for the analysis of fine-scale migration events, such as those that occurred across populations that split few tens to hundreds of generations in the past. It is, however, possible that adjusting some of the parameters used for the GENOME simulations and for the PCAdmix software, or using other deconvolution methods, the obtained accuracy may be increased. Furthermore, the development of methods for ancestry deconvolution in sequence data, where rare variants are observable, is expected to substantially increase the power of this analysis, although the effects of limited population divergence are likely to still affect the accuracy of methods that do not explicitly take this aspect into account. An additional difference to be noted between the two considered approaches is that Tracts does not model population size changes in the populations, focusing on relative migration rates, whereas DoRIS allows recovering both population size fluctuations and migration rates, thus providing insights into the magnitude of migration events. This increased flexibility, however, may complicate the inference, also in light of our observation that large sample sizes are required for the IBD analysis. 4 DISCUSSION In this article, we have extended our previous work on the relationship between long-range haplotypes that are shared IBD across individuals from a study cohort and the demographic history of the individuals’ populations of origin. Specifically, the described framework removes the limiting requirement that all sampled individuals belong to a single population and allows for explicitly modeling and inferring demographic interactions across multiple demes. The evaluation we performed on   synthetic populations confirms the accuracy of the derived IBD model and suggests that haplotype sharing can be used to gain insight into fine-scale demographic dynamics for the past tens to few hundreds of generations, provided enough samples are collected. Our analysis of the HapMap 3 Masai samples, as well as our previously reported analysis of an Askenazi cohort, suggests that this method can be applied to currently available datasets, provided that the quality of haplotype phasing and IBD detection is carefully considered. Among available methods for demographic inference, another approach that explicitly models the effects of recombination (the Pairwise Sequentially Markovian Coalescent model, PSMC) was recently proposed in  Li and Durbin (2011) . This model relies on a Markovian approximation of the coalescent with recombination ( McVean and Cardin, 2005 ) and is able to simultaneously consider the effects of mutation and recombination. The PSMC, however, differs from the proposed IBD-based model for its applicability, as it requires full sequence information and is currently focused on the analysis of remote demographic events using single individuals, or pairs of phased chromosomes. Because of the scarcity of coalescent events in the recent history, the simultaneous analysis of multiple samples is needed to infer recent demographics. An extension of the PSMC to handle the analysis of multiple samples, however, is computationally challenging, and efficient approximations are being developed ( Sheehan  et al. , 2013 ). In addition to these whole-sequence–based methods, independent current work ( Ralph and Coop, 2013 ) infers historical demographic changes from length distributions of IBD segments, taking a complementary, less parametric approach, thereby allowing increased flexibility during inference of plausible coalescent time distributions, but without providing explicit modeling of migration and population size changes. Among other methods aimed at inferring migration, our approach is conceptually related to those that rely on the frequency and length of migrant tracts. These methods, however, do not model population size fluctuations and are dependent on the possibility of reliably performing ancestry deconvolution to assign chromosomal tracts to a set of reference populations. These populations may not be available and, more importantly, need to be substantially divergent to attain high-quality deconvolution, as shown in our analysis. Although whole-sequence datasets and methodological developments may improve the performance of deconvolution methods, this limitation may prevent methods based on migrant tracts from being effectively used in the reconstruction of fine-scale migration patterns of the recent millennia. Methods based on ancestry deconvolution, however, may in some scenarios be used in concert with methods based on IBD sharing. Knowing whether an IBD tract was co-inherited from a specific population, in fact, may provide information on the directionality of migration, and also offer further insight into deeper time scales, as shown in  Campbell  et al.  (2012)  and  Velez  et al.  (2012) . This direction may be further explored in light of the recently developed analytical model for migrant tracts and the presented model for IBD. The proposed IBD framework will further be enhanced by accurate whole-genome sequence information, as the presence of mutations on IBD segments will improve the timing of common ancestors and IBD detection of shorter segments. Finally, our model still relies on the assumption of selective neutrality. Natural selection has been shown to have an impact on long-range haplotype sharing ( Albrechtsen  et al. , 2010 ;  Gusev  et al. , 2012 ). Although selective forces are mostly visible at local scales, demography affects the entire genome. This framework could, therefore, be used to test local deviations from neutrality, and the presented extension, which handles the case of multiple population models, may further assist the analysis of cross-population IBD sharing in this context. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>smCounter2: an accurate low-frequency variant caller for targeted sequencing data with unique molecular identifiers</Title>
    <Doi>10.1093/bioinformatics/bty790</Doi>
    <Authors>Xu Chang, Gu Xiujing, Padmanabhan Raghavendra, Wu Zhong, Peng Quan, DiCarlo John, Wang Yexun, Birol Inanc</Authors>
    <Abstract/>
    <Body>1 Introduction Detection of low-frequency variants is important for early cancer diagnosis and is a very active area of research. Targeted DNA sequencing generates very high coverage over a specific genomic region, therefore allowing low-frequency variants to be observed from a reasonable number of reads. However, distinguishing the observed variants from experimental artifacts is very difficult when the variants’ allele frequencies are near or below the noise level. Providing an error-correction mechanism, unique molecular identifiers (UMIs) have been implemented in several proof-of-concept studies ( Jabara  et al. , 2011 ;  Kennedy  et al. , 2014 ;  Kukita  et al. , 2015 ;  Newman  et al. , 2016 ;  Peng  et al. , 2015 ;  Schmitt  et al. , 2012 ) and used in translational medical research ( Acuna-Hidalgo  et al. , 2017 ;  Bar  et al. , 2017 ;  Young  et al. , 2016 ). In these protocols, UMIs (short oligonulceotide sequences) are attached to endogenous DNA fragments by ligation or primer extension, carried along through amplification and sequencing and finally identified from the reads. Sequencing errors can be corrected by majority vote within a UMI family, because reads sharing a common UMI and random fragmentation site should be identical except for rare collision events ( Liang  et al. , 2014 ) or errors within the UMI sequences. DNA polymerase errors occurring during DNA end repair and early PCR cycles (particularly the first cycle), however, cannot be corrected because all reads in the UMI would presumably carry the error. Although PCR error rates are low ( 10 − 4 − 10 − 6 , depending on the enzyme and types of substitution), they impose fundamental limits to UMI-based variant calling. A two-step UMI-based variant calling approach that first constructs a consensus read with tools like fgbio ( https://github.com/fulcrumgenomics/fgbio ) and then applies one of the conventional low-frequency variant callers ( Xu, 2018 ) to the consensus reads has been implemented in  Peng  et al.  (2015)  and  Blumenstiel  et al.  (2017) . In addition to the two-stage method, three UMI-based variant callers, DeepSNVMiner ( Andrews  et al. , 2016 ), smCounter ( Xu  et al. , 2017 ) and MAGERI ( Shugay  et al. , 2017 ), are publicly available. DeepSNVMiner relies on heuristic thresholds to draw consensus and call variants. By default, a UMI is defined as ‘supermutant’ if 40% of its reads support a variant and two supermutants are required to confirm the variant. smCounter was released in 2016 by our group and reported above 90% sensitivity at fewer than 20 false positives per megabase for 1% variants in coding regions. smCounter’s core algorithm consists of a joint probabilistic modeling of PCR and sequencing errors. MAGERI is a collection of tools for UMI-handling, read alignment, and variant calling. The core algorithm estimates the first-cycle PCR errors as a baseline and calls variants whose allele frequencies are higher than the baseline level. MAGERI reported 93% area under curve (AUC) on variants with about 0.1% allele frequencies. In this article, we present smCounter2, a single nucleotide variant (SNV) and short indel caller for UMI-based targeted sequencing data. smCounter2 offers significant upgrades from its predecessor (smCounter) in terms of algorithm, performance and usability. smCounter2 adopts the widely popular Beta distribution to model the background error rates and Beta-binomial distribution to model the number of non-reference UMIs. An important feature of smCounter2 is that the model parameters are dynamically adjusted for each input read set. In addition, smCounter2 uses a regression-based filter to reject artifacts in repetitive regions while retaining most of the real variants. The algorithm improvements help to push the detection limit down to 0.5% from the previously reported 1% and increase the sensitivity and specificity compared to other UMI-based methods (two-step consensus-read approach and smCounter), as shown in Section 3. For ease of use, smCounter2 has been released with a Docker container image that includes the complete read processing (using reads from a QIAGEN QIAseq DNA targeted enrichment kit as an example) and variant calling pipeline as well as all the supporting packages and dependencies. 2 Materials and methods 2.1 smCounter2 workflow smCounter2’s workflow ( Fig. 1 ) begins with read-processing steps that (i) remove the exogenous sequences such as PCR and sequencing adapters and UMI, (ii) identify the UMI sequence and append it to the read identifier for downstream analyses and (iii) remove short reads that lack enough endogenous sequence for mapping to the reference genome. The trimmed reads are mapped to the reference genome with BWA-MEM, followed by filtering of poorly mapped reads and soft-clipping of gene-specific primers. A UMI with much smaller read count is combined with a much larger read family if their UMIs are within edit distance of 1 and the corresponding 5' positions of aligned R2 reads are within 5 bp (i.e. at the random fragmentation site). After UMI clustering, the aligned reads (BAM format) are sent for variant calling.
 Fig. 1. smCounter2 workflow. Rectangular boxes represent the data files and elliptical boxes represent steps of the pipeline. Users can choose to run the whole pipeline from FASTQ to VCF or run the variant calling part only from BAM to VCF Like many variant callers, smCounter2 walks through the region of interest and processes each position independently. At each position, the covering reads go through several quality filters and the remaining high-quality reads are grouped by putative input molecule (as determined by both the clustered UMI sequence and the random fragmentation site). A consensus base call (including indels) is drawn within a UMI if  ≥ 80 %  of its reads agree. The core variant calling algorithm is built on the estimation of background error rates, i.e. the baseline noise level for the data. A potential variant is identified only if the signal is well above that level (Sections 2.2 and 2.3). The potential variants are subject to post-filters, including both traditional filters such as strand bias and novel model-based, UMI-specific repetitive region filters (Section 2.4). Finally, the variants are annotated with SnpEff ( Cingolani  et al. , 2012a ) and SnpSift ( Cingolani  et al. , 2012b ) and output in VCF format. For better flexibility, users can choose to run the variant calling part only. smCounter2 accepts both raw UMI-tagged BAM file and consensused BAM file (e.g. generated by fgbio) as input. In addition, smCounter2 can be used to verify a list of pre-called variants if a VCF file is provided. 2.2 Estimation of background error rates Estimating the background error rates is one of the commonly used strategies in somatic variant calling. EBCall ( Shiraishi  et al. , 2013 ) and shearwater ( Gerstung  et al. , 2014 ) assume that each site has a distinctive error rate (predominantly sequencing errors) that follows a Beta distribution. LoLoPicker ( Carrot-Zhang and Majewski, 2017 ) estimates site-specific sequencing error rates as fixed values. For UMI-tagged data, background errors can come from base mis-incorporation by DNA polymerase during end repair and the first-cycle PCR reaction, oxidation damage to DNA bases during sonication shearing and probe hybridization ( Newman  et al. , 2016 ;  Park  et al. , 2017 ), UMI mis-assignment, misalignment, and polymerase slippage (often in repetitive sequences), etc. iDES ( Newman  et al. , 2016 ) characterizes the site-specific background error rates in duplex-sequencing data using Normal or Weibull distributions. The limitation of these algorithms is the requirement of many control samples for the site-specific error modeling. As an alternative, MAGERI ( Shugay  et al. , 2017 ) assumes a universal Beta distribution for all sites, which may result in lower accuracy compared to site-specific error modeling, but as a trade-off requires only one control sample, if the UMI coverage is high enough to observe the background errors and enough sites are covered to reveal the full distribution of error rates. smCounter2 takes similar experimental and modeling approaches as MAGERI with important modifications. To obtain high-depth data for error profiling, we sequenced 300 ng of NA12878 DNA within a 17 kbp region using a custom QIAseq DNA panel. After excluding the known SNPs [Genome in a Bottle Consortium ( Zook  et al. , 2014 )], we calculated the error rates by base substitution at each site assuming any non-reference UMIs are background errors. The calculation process is explained in  Supplementary Material , Section 2. We observed notable variation across different base substitutions and that transitions were more error-prone than transversions ( Fig. 2a ). We used the Beta distribution to fit the observed error rates [ R fitdistrplus  ( Delignette-Muller and Dutang, 2015 ),  Fig. 2b ]. The quantile plot indicates good fit in general and under-estimation of the tail, possibly due to outliers ( Fig. 2c ). We prepared two versions of error models, one excluding singletons (UMIs with only one read pair) and the other including singletons, to accommodate deep and shallow sequencing depths. For read sets with mean read pair per UMI (rpu)  ≥ 3 , smCounter2 drops singletons to reduce errors and uses the error model without singletons. For read sets with rpu &lt; 3, smCounter2 keeps some or all singletons ( Supplementary Material , Section 4) to avoid losing too many UMIs, and uses the error model with singletons.
 Fig. 2. Underlying model of smCounter2. ( a ) Background error rates for each type of base change, averaged across the panel of M0466. ( b ) Modeling of the background error rates using the Beta distribution. The histogram shows the frequency of observed  G &gt; A  error rates in M0466. The dashed curve is the density of the fitted Beta distribution. ( c ) Quantile plot to check the goodness-of-fit of the  G &gt; A  error rate modeling. The observed and fitted quantiles form a 45° line in most places, indicating perfect fit. The tail skews towards ‘observed’, indicating under-estimation of the extremely high error rates. This may simply be explained by outliers, or suggests that a distribution (or mixed distributions) with heavier tail is needed. ( d ) A real example of parameter adjustment. The dashed curve is the originally fitted Beta distribution. The dotdashed curve with higher peak is the adjusted error model with the mean of the input data (N13532) and the original variance. ( e ) Illustration of the variant calling  P -value. The density curve is a hypothesized Beta-binomial distribution. The vertical line indicates the observed non-reference UMI counts. The area of the shaded region is the  P -value. ( f ) Detection limit prediction and confirmation. The top and bottom curves are the predicted site-wise detection limit for Ti and Tv/indels, respectively. The dots are the true variants in N13532 (outliers with extremely low UMI depth or high allele frequency excluded). For the dots, the  y -axis represents the observed allele frequencies. Round dots are the variants detected and triangle dots are the ones not detected, concentrated in the low enrichment regions As a distinctive feature of smCounter2, the Beta distribution parameters are adjusted for each dataset to account for the run-to-run variation. Because the true variants are unknown in the application dataset, we conservatively assumed that all non-reference alleles with VAF below 0.01 are background errors. The low DNA input in most applications impose another challenge in that few of the applications generate enough site-wise UMI coverage for any meaningful update of the error rate distribution. Fortunately, sufficient UMIs can usually be obtained by aggregating the target sites to accurately estimate the mean. Therefore, we only adjust the mean of the Beta distribution to equate the panel-wise mean and leave the dispersion unchanged ( Fig. 2d ). In specific, the adjusted Beta parameters are
 (1) a * = μ * ( μ * ( 1 − μ * ) σ 2 − 1 ) (2) b * = ( 1 − μ * ) ( μ * ( 1 − μ * ) σ 2 − 1 ) , 
where  μ *  is the mean error rate of the current data and  σ 2  is the variance of the error rate from our control sample. The adjusted distribution  B e t a ( a * , b * )  has a mean of  μ *  and variance of  σ 2 . Background errors are sensitive to enrichment chemistry and DNA polymerase. The error pattern we observed in QIAseq DNA panels agrees with that in other PCR enrichment studies ( Potapov and Ong, 2017 ;  Shagin  et al. , 2017 ) but differs from hybridization capture studies ( Newman  et al. , 2016 ;  Park  et al. , 2017 ) where  A  &gt;  C  and  G  &gt;  T  errors are dominant. Also, certain high-fidelity DNA polymerases have been shown to generate tens- or hundreds-fold lower error rates ( Potapov and Ong, 2017 ). Therefore, we did not attempt to build a universal error model by pooling data from multiple experiments with different polymerases as MAGERI did, but instead suggest users who run hybridization capture protocols or use non-QIAseq enrichment chemistry to build their own error profile. This can be done using a script provided in the Github repository. Limited by sequencing resources, we were unable to obtain adequate site-wise UMI depth to model base substitutions with low error rates, including all transversions and some transitions. This deficit had several impacts on our modeling procedure. First, we had to assume that all transitions followed the distribution of  G  &gt;  A  (second highest) and all transversions followed the distribution of  C  &gt;  T  (higher than all transversions). This conservative configuration ensured that the error rates were not under-estimated, but also prevented us from reaching the theoretical detection limit. Second, we were unable to model the indel error rates because (i) indel polymerase errors occur more frequently in repetitive regions, and our panel did not include enough such regions, (ii) there are countless types of indels and we cannot model the errors by each type and (iii) indel polymerase error rates are on average lower than base substitution and we lacked the UMI depth to observe enough of them. Again, we conservatively assumed that indel error rates followed the distribution of  G  &gt;  A . Third, because the error rates are very low, zero non-reference UMIs were observed at some sites, especially in low enrichment regions. Depending on the percentage of such sites, we either imputed the zeros with small values or used a zero-inflated Beta distribution (a mixture of Beta distribution and a spike of zeros) instead of Beta. 2.3 Statistical model for variant calling and detection limit prediction We treated variant calling as a hypothesis testing problem, where the null hypothesis ( H 0 ) is that all non-reference UMIs are from background errors and the alternative hypothesis ( H a ) is that the non-reference UMIs are from the real variant. We assume that there are  n  UMIs covering a site and  k  of them have the same non-reference allele. Under  H 0 ,  k  follows a Binomial distribution  Bin ( n ,  p ) where  p  is the background error rate. If  p  follows the Beta distribution with the adjusted parameter  B e t a ( a * , b * ) , the marginal distribution of  k  given  n , a * , b *  is Beta-binomial. If a zero-inflated Beta distribution is used,  k  has a non-standard marginal distribution. To compute the  P -value, we first simulated random samples of  { p i , i = 1 , … , I }  according to the distribution being used. Then for each  p i  we computed  P B i n ( K ≥ k | n , p i )  based on the Binomial distribution. The  P -value represents the probability of observing  ≥ k  non-reference UMIs at a wild-type site ( Fig. 2e ) and is approximated by
 (4) P = I − 1 ∑ i = 1 I P B i n ( K ≥ k | n , p i ) ≈ ∫ 0 1 P B i n ( K ≥ k | n , p ) f ( p | a * , b * ) d p (3) = P B e t a − b i n ( K ≥ k | n , a * , b * ) . To avoid extremely small fractions, smCounter2 reports  Q = min ( 200 , −   log   10 P )  as the variant quality score. The choice of variant calling threshold depends on the tolerance of false positive rate because if the model fits perfectly, the specificity would equal to 1 minus the  P -value threshold. By default, smCounter2 aims for  ≤ 1  false positives per megabase, which is equivalent to a threshold of  P ≤ 10 − 6  or  Q ≥ 6 . We will show in Section 3 and  Supplementary Material  that this threshold works well for datasets with deep and shallow UMI coverage and for variants with a range of VAFs (0.5, 1, 5% and germlines). The only exception is that, if 0.5–1% indels are of interest, we recommend lowering the Q-threshold to 2.5 to account for the overestimation of indel error rates. Under this framework, the site-specific detection limit (sDL, the minimum allele frequency to exceed the  P -value threshold) is a decreasing function of the UMI depth. It also depends on the type of variant because transitions have higher background error rates than transversions and indels. We estimate that the sDL of transitions is higher than transversions and indels on by about 0.001, or 0.1% in allele frequency. We denote  P ( n , k , t )  as the  P -value given UMI depth  n , non-reference UMI count  k  and the type of variant  ∈ { Ti ,   Tv + indel } .  P ( n , k , t )  can be computed by  Equation (3) . The sDL is denoted as arg min  k { P ( n ,  k ,  t ) &lt; threshold}∕ n  and can be computed numerically. Importantly, the predicted sDL is the  observed  allele frequency that often deviates from the true allele frequency in the sample due to random enrichment bias. If we loosely define the overall detection limit as the minimum  true  allele frequency that the variant caller can detect with good sensitivity and specificity, the overall detection limit is usually higher than sDL. Based on our calculation, the theoretical detection limit of a QIAseq DNA panel is around 0.5% when UMI depth is between 2000 and 4000. This detection limit was confirmed experimentally by sequencing a sample with known 0.5% variants ( Fig. 2f ). 2.4 Repetitive region filters based on UMI efficiency Repetitive regions such as homopolymers and microsatellites are enriched in non-coding regions where variants can have important functions from regulating gene expression to promoting diseases ( Khurana  et al. , 2016 ). Unfortunately, these regions are a major source of false variant calls due to increased polymerase and mapping errors. For instance, polymerase slippage (one or more bases of the template are skipped over during base extension) occurs more frequently at homopolymers and results in false deletion calls. Reads may be incorrectly mapped to similar regions or mis-aligned if they do not span the whole repetitive sequence, both causing false variant calls. Conventional variant callers apply heuristic filters to remove false calls. For example, Strelka ( Saunders  et al. , 2012 ) rejects somatic indels at homopolymers with  ≥ 8 nt or di-nucleotide repeats with  ≥ 16 nt. Recent haplotype-based variant callers such as GATK HaplotypeCaller ( DePristo  et al. , 2011 ) perform local  de novo  assembly to avoid mapping/alignment errors in repetitive regions. However, these methods were developed for non-UMI data.smCounter2 includes a set of repetitive region filters that are specifically designed for UMI data. The filters were inspired by the observations that (i) UMIs of the false variants tend to have lower read counts and more heterogeneous reads compared to UMIs of real variants, and (ii) reads of the false variants are more likely to contradict with their UMIs’ consensus allele (usually wild-type), whereas reads of the real variants are likely to agree with their UMIs. We used the term ‘UMI efficiency’ to describe these distinctions ( Fig. 3a ) and quantified the UMI efficiency with four variables: (i)  vafToVmfRatio , the ratio of allele frequencies based on reads and UMIs; (ii)  umiEff , the proportion of reads that are concordant with their respective UMI consensus; (iii)  rpuDiff , difference of read counts between variant UMIs and wild-type UMIs, adjusted by the standard deviations and (iv)  varRpu , mean read fragments per variant UMI.
 Fig. 3. Training and testing of the homopolymer indel filter. ( a ) Illustration of UMI efficiency. The UMI on the left has perfect efficiency because all reads contributed to the consensus. The UMI on the right has low efficiency because two reads in red disagree with the majority and thus are wasted. smCounter2 requires 80% agreement to reach a consensus, so the entire UMI would be dropped and the other three reads would be wasted as well. ( b ) Relative importance of each predictor ranked by the explained variation minus the degree of freedom. The read pairs per variant UMI ( varRpu ) and the ratio between allele frequencies by read and by UMI ( vafToVmfRatio ) are the two variables with the most predictive power. The plot is generated with R rms package. ( c ) ROC curves of the logistic regression classifier. The black curve is for the training data that combined all true and false homopolymer indels in N0030, N0015, N11582 and N0164. The blue and red curves are for two test datasets N13532 and N0261, respectively. The dots represent the actual sensitivity and specificity at the cutoff, which is consistent in all three datasets We trained and validated a logistic regression model to distinguish real homopolymers indels from artifacts. We focused our resources on this repetitive region subtype because during development, we observed that homopolymer indels were the main contributor of false positives. We combined data from several UMI-based sequencing experiments to assemble a training set with 255 GIAB high-confidence homopolymer indels with allele frequencies from 1 to 100% and 386 false positives that would otherwise be called without the filters. In addition to the UMI efficiency variables, we included  sVMF  (VAF based on UMI) and  hpLen8  (binary variable indicating whether the repeat length  ≥ 8 ) as predictors. We found that  varRpb  and  vafToVmfRatio  were the two most important predictors in terms of explained log-likelihood ( Fig. 3b ). We chose the cutoff on the linear predictors to target on the highest sensitivity while maintaining 99% specificity using the R package OptimalCutpoints ( López-Ratón  et al. , 2014 ). The model and cutoff were applied to two independent datasets N13532 and N0261, both containing 0.5% variants. N13532 had 41 real homopolymer indels and 122 false positives with  Q ≥ 2.5 . The predictive model achieved 39.0% sensitivity, 96.7% specificity, and 0.868 AUC. N0261 had 39 real homopolymer indels and 42 false positives with  Q ≥ 2.5 . The predictive model achieved 71.8% sensitivity, 95.2% specificity, and 0.910 AUC ( Fig. 3c ). For other subtypes of variants and repetitive regions, we used heuristic thresholds as filters due to lack of training data. The model parameters and default thresholds are presented in the  Supplementary Material . 3 Results 3.1 Training and validation datasets To develop the statistical model and fine-tune the parameters, we did multiple sequencing runs using reference materials NA12878 and NA24385, both of which have high-confidence variants released by GIAB (v3.3.2 used for this study). We mixed small amounts of NA12878 DNA into NA24385 based on the amount of amplifiable DNA measured by QIAseq DNA QuantiMIZE assay to simulate low-frequency variants. The modeling of background error rates was based on M0466, a high-input, deep-sequencing run that reached over 45 000 UMI coverage per site. The selection of variant calling threshold and refinement of filter parameters were based on four datasets: N0030, N0015, N11582 and N0164. After development, we tested smCounter2 on three independent datasets: N13532, N0261 and M0253 without any modification to the algorithm and parameters. The datasets involved in this study are summarized in  Table 1 . A more detailed description of these datasets is provided in the  Supplementary Material .
 Table 1. Key statistics of the datasets used for training and testing of smCounter2 Dataset Purpose Sample Target region (bp) Mean UMI depth Mean read pairs per UMI VAF (%) SNVs Indels M0466 Training 0.2% NA12878 17 859 45 335 3.2 0.1 87 0 N0030 Training 2% NA12878 1 032 301 3612 8.6 1 363 56 N0015 Training 10% NA12878 406 846 4825 8.5 5 4412 369 N11582 Training 100% NA24385 1 094 204 479 2.6 50 or 100 729 49 N0164 Training 1–20% NA12878 66 661 3692 11.5 0.5–10 237 177 N13532 Test 1% NA12878 928 315 4040 7.6 0.5 293 164 N0261 Test 1% NA12878 45 299 3384 13.8 0.5 5 269 M0253 Test 50% HDx Tru-Q 7 38 370 4980 13.0 ≥ 0.5 36 (with MNPs) 1 3.2 Benchmarking 0.5% variant calling performance using mixed GIAB samples We benchmarked smCounter2 against six state-of-the-art UMI variant calling algorithms (fgbio+MuTect, fgbio+MuTect2, fgbio+VarDict, MAGERI, DeepSNVMiner and smCounter) on N13532, which contained 0.5% NA12878 variants. The first three algorithms represent the two-step approach discussed in Section 1. We first constructed consensus reads from the aligned reads (BAM file) using fgbio’s  CallMolecularConsensusReads and FilterConsensusReads  functions and then applied three popular low-frequency variant callers, MuTect, MuTect2 ( Cibulskis  et al. , 2013 , and VarDict ( Lai  et al. , 2016 ), on the consensus reads. MAGERI, DeepSNVMiner and smCounter are representative UMI-aware variant callers. The results ( Fig. 4 ), stratified by type of variant (SNV and indel) and genomic region (all, coding and non-coding), were measured by sensitivity and false positives per megabase (FP/Mbp, or  10 6 ( 1 − specificity ) ) at several thresholds. smCounter2 outperformed the other methods in all categories. In coding regions, smCounter2 achieved 92.4% sensitivity at 12 FP/Mbp for SNVs and 84.4% sensitivity at 7 FP/Mbp for indels ( Table 2 ). In non-coding regions, smCounter2 was able to maintain comparable accuracy for SNVs (83.3% sensitivity at 4 FP/Mbp), but produced lower sensitivity (56.8%) and higher false positive rate (42 FP/Mbp) for indels. In the indel-enriched dataset N0261, smCounter2 produced consistent sensitivity (81.4% in coding and 61.3% in non-coding) and seemingly higher FP/Mbp (0 in coding and 114 in non-coding). However, FP/Mbp in N0261 was based on a very small target region (45 kbp) and therefore provides a less accurate specificity estimate.
 Table 2. smCounter2 performance in detecting 0.5, 1, 5 and 50–100% variants, stratified by type of variant (SNV and indel) and genomic region (coding and non-coding) Dataset Region Type TP FP FN TPR (%) FP/Mbp PPV (%) HC size (bp) N13532 Coding SNV 171 7 14 92.4 12 96.1 591 154 (0.5%, test) Indel 38 4 7 84.4 7 90.5 591 154 Non-coding SNV 90 1 18 83.3 4 98.9 259 162 Indel 67 11 51 56.8 42 85.9 259 162 N0261 Coding Indel 35 0 8 81.4 0 100.0 6119 (0.5%, test) Non-coding Indel 138 4 87 61.3 114 97.2 35 172 M0253 All SNV/MNV 32 — 4 88.9 — — 38 370 (0.5–30%, test) Indel 0 — 1 0.0 — — 38 370 N0030 Coding SNV 214 5 4 98.2 7 97.7 694 189 (1%, training) Indel 36 1 3 92.3 1 97.3 694 189 Non-coding SNV 137 3 8 94.5 13 97.9 236 687 Indel 12 3 5 70.6 13 80.0 236 687 N0015 Coding SNV 528 0 4 99.2 0 100.0 35 718 (5%, training) Indel 9 0 1 90.0 0 100.0 35 718 Non-coding SNV 3851 7 29 99.3 24 99.8 297 805 Indel 285 13 74 79.4 44 95.6 297 805 N11582 Coding SNV 421 2 0 100.0 3 99.5 682 483 (50–100%, Indel 4 0 0 100.0 0 100.0 682 483 training) Non-coding SNV 301 1 7 97.7 4 99.7 269 761 Indel 34 1 11 75.6 4 97.1 269 761 Notes : The metrics were generated with the default thresholds ( Q ≥ 2.5  for indels in N13532, N0261, M0253 and  Q ≥ 6  for all other cases). The allele frequency and the purpose of the dataset are displayed under the dataset name. All performance metrics are measured on GIAB high-confidence regions only, the sizes of which are presented in the last column. Fig. 4. Benchmarking smCounter2, smCounter, fgbio+MuTect, fgbio+VarDict and fgbio+MuTect2 on 0.5% variants in N13532. The performance is measured by false positives per megabase ( x -axis) and sensitivity ( y -axis), stratified by type of variant (SNV and indel) and region (coding, non-coding, and all). The ROC curves are generated by varying the threshold for each method:  Q -score for smCounter2, prediction index for smCounter, likelihood ratio for MuTect and MuTect2 and minimum allele frequency for VarDict. MuTect does not detect indels so is not included in the indel comparison We did not show DeepSNVMiner and MAGERI’s performance in  Figure 4 . DeepSNVMiner generated 7654 FP/Mbp to achieve 86% sensitivity for SNVs at the default setting. Similar or worse performance was achieved at other settings that we tested. Because this level of false positive rate is much higher than other methods (&lt;200FP/Mbp at similar sensitivity), it would be hard to put the ROC curves in the same figure. For MAGERI, it is unfair to compare its performance with smCounter2 using QIAseq data. MAGERI’s error model is based only on primer extension assays from a mix of DNA polymerases including several high-fidelity enzymes ( Shagin  et al. , 2017 ), while smCounter2’s error model is specific to the entire QIAseq targeted DNA panel workflow, including DNA fragmentation, end repair and PCR enrichment steps. Because the MAGERI error model does not include errors introduced at the typical DNA fragmentation and end repair process (their assays do not have those steps), MAGERI’s background error rates are lower than those in smCounter2. For example, the mean error rate of  A  &gt;  G  and  T  &gt;  C  used by MAGERI is  6.3 × 10 − 5  per base ( https://github.com/mikessh/mageri-paper/blob/master/error_model/basic_error_model.pdf ) and about  3 × 10 − 4  per base for smCounter2 ( Fig. 2a ). Therefore, with QIAseq data, MAGERI will produce more false positives due to under-estimation of the error rate. We included MAGERI’s ROC curve in the  Supplementary Figure S2  to illustrate the point that the error models are specific to each NGS workflow and need to be empirically established for different workflows. We applied smCounter2 on the same fgbio consensus reads that were used with MuTect/MuTect2 and VarDict. As expected, fgbio+smCounter2_consensus achieved lower sensitivity and specificity than smCounter2 on the raw reads ( Supplementary Fig. S2 ). One reason is that many smCounter2-specific filters cannot be used in this case because the UMI efficiency metrics are not computed by fgbio and therefore lost after consensus. We had to use smCounter’s filters for the fgbio consensus reads. However, despite having the same filters and a better statistical model, fgbio+smCounter2_consensus was still outperformed by smCounter. This can possibly be changed by further fine-tuning the parameters of fgbio and smCounter2. But on the other hand, it illustrates the challenge of the two-stage approaches for UMI-based variant calling, which is harmonizing the consensus and variant calling algorithms, as pointed out in  Xu  et al.  (2017)  and  Shugay  et al.  (2017) . We used the default setting for smCounter and adjusted the parameters of fgbio, MuTect and VarDict based on our experience of working with them. However, given the infinite parameter space, we cannot claim that the results reported here reflect their optimal performance. Several variant calling thresholds were used to investigate the sensitivity-specificity trade-off and draw the ROC curves. For fgbio+MuTect/MuTect2, we used MuTect and MuTect2’s likelihood ratio score as threshold. For fgbio+VarDict, we set VarDict’s minimum allele frequency (− f ). For MAGERI, we did not use the seemingly obvious threshold ‘Q-score’ because they were not allowed to exceed 100 for computational reasons, and even a  Q -score of 100 was overly sensitive and generated too many false calls. Instead, we held  Q -score constant at 100 and varied the number of reads in a UMI  (-defaultOverseq ). The parameters and thresholds used in this study are listed in the  Supplementary Material , Section 4. 3.3 Detecting  ≥ 1 %  variants in (possibly) shallow sequencing runs smCounter2 achieved good sensitivity on 1, 5, 50 and 100% variants as well ( Table 2 , datasets N0030, N0015, N11582). The biggest advantage for smCounter2 was in non-coding regions due to the repetitive region filters. Compared to smCounter, for 1% non-coding variants, smCounter2’s sensitivity increased from 75.2 to 94.5% for SNVs and from 23.5 to 70.6% for indels ( Supplementary Fig. S3 ). For 5% non-coding variants, smCounter2’s sensitivity increased from 95.1 to 99.3% for SNVs and from 58.2 to 79.4% for indels ( Supplementary Fig. S4 ). For 50 and 100% non-coding variants, smCounter2’s sensitivity increased from 89.0 to 97.7% for SNVs and from 42.2 to 75.6% for indels ( Supplementary Fig. S5 ). Both smCounter2 and smCounter outperformed fgbio+MuTect and fgbio+VarDict on 1 and 5% variants in all categories. For germline variants, however, smCounter2 had lower sensitivity for non-coding indels compared to fgbio+HaplotypeCaller (75.6% versus 88.9%). This demonstrated the advantage of a haplotype-based strategy in difficult regions. Other than for non-coding indels, the two methods achieved comparable accuracy in other categories. To test smCounter2’s robustness under low sequencing capacity, we  in silico  downsampled N0030 to 80, 60, 40, 20 and 10% of reads to mimic a range of sequencing and UMI depths. smCounter2 outperformed other methods in all sub-samples (Supplementary Figs S6–S10). The downsample series also demonstrated that smCounter2’s constant threshold can maintain consistently low false positive rates for SNV across a range of UMI depths ( Fig. 5 ). In contrast, smCounter’s default threshold must move linearly with the UMI depth to maintain a certain level of false positive rate. Similarly, MuTect’s threshold based on the likelihood ratio needs to be adjusted for datasets with varying read depth. smCounter2’s invariant threshold allows users to apply the default setting to a wide range of sequencing and sample input conditions.
 Fig. 5. Default thresholds of smCounter and smCounter2 at different UMI depths and associated false positive rates based on the downsample series of N0030. smCounter’s threshold moves linearly with the UMI depth and is determined using an empirical formula  y = 14 + 0.012 x . smCounter2’s threshold is constant at 6. The false positive rates for SNV are well controlled (between 5 and 13 FP/Mbp, represented by the point size) using both methods It is important to note that the results described in Sections 3.2 and 3.3 are measured over GIAB high-confidence region. smCounter2’s performance in GIAB-difficult regions is unknown, both absolutely and in comparison to other variant callers. We also note that the results in Section 3.3 are based on training datasets only. We have not tested smCounter2 on independent 1% or above variants. 3.4 Detecting complex cancer mutations using horizon Tru-Q samples The performance data described so far were based on diluted NA12878 or pure NA24385, all of which contained germline variants. To test smCounter2 on low-frequency cancer mutations, we sequenced the Tru-Q 7 reference standard (Horizon Dx) that contained verified 1.0% (and above) onco-specific mutations. The sample was diluted 1:1 in Tru-Q 0 (wild-type, Horizon Dx) to simulate 0.5% variants. For this dataset (M0253), smCounter2 detected 32 out of 36 SNV/MNVs (88.9%) and narrowly missed the only deletion ( Q  = 2.49 for threshold of 2.5). Because not all variants in the Tru-Q samples are known, we cannot evaluate specificity using this dataset. The list of variants in this dataset, along with the observed VAF and smCounter2 results, can be found in Supplementary File  M0253.HDx.Q7.vcf . The Tru-Q sample contains some complex multi-allelic variants that are challenging for variant callers that are not haplotype-aware. For example, there are four variants  A  &gt;  C ,  A  &gt;  T , AC &gt; CT and AC &gt; TT at one position (chr7: 140453136, GRCh37) and a  C  &gt;  T  point mutation at the next position. smCounter2 detected the three SNVs but failed to recognize the two MNVs. 4 Discussion 4.1 Improvement over smCounter In this paper, we described smCounter2, the next version of our UMI-based variant caller. Compared to the previous version of smCounter, smCounter2 features lower detection limit, higher accuracy, consistent threshold and better usability. smCounter2 pushed the detection limit of QIAseq targeted DNA panels from smCounter’s 1% down to 0.5%. smCounter2 achieved a lower detection limit because the background error rates were accurately estimated for specific base incorporation errors. The statistical model allows smCounter2 to quantify the deviation from real variants to the background errors using  P -values. Therefore, the ambiguous variants whose allele frequencies are close to the background error rates can be called by smCounter2 with reasonable confidence. Importantly, 0.5% is a not an algorithm limit, but rather a chemistry limit. We believe that smCounter2 can achieve even lower detection limits for other chemistry with lower background error rate. smCounter2 has higher accuracy than its predecessor for both SNVs and indels, in both coding and non-coding regions, for both deep and shallow sequencing runs, and for both low-frequency ( ≥ 0.5 % ) and germline variants ( Fig. 4  and Supplementary Figs S2–S10). In particular, for 0.5% coding region mutations, smCounter2 achieved over 92% sensitivity for SNVs and 84% for indels in coding regions at the cost of about 10 false positives per megabase, a significant improvement compared to smCounter’s 82% sensitivity for SNVs and 27% for indels at similar false positive rate. The accuracy improvement is due to the modeling of background error rates and, particularly in non-coding regions, UMI-based repetitive region filters. The filters catch false positives in the repetitive regions that pass the  P -value threshold but have low ‘UMI efficiency’, a novel concept that we have proved to be useful in distinguishing real variants from artifacts. Particularly for indels in homopolymers, smCounter2 employs a logistic regression classifier that was trained and validated with separate datasets. smCounter2 has a more consistent variant calling threshold ( Q ≥ 2.5  for 0.5–1% indels and  Q ≥ 6  for other cases) that is independent from the UMI depth, unlike smCounter or MuTect whose optimal threshold must move with the UMI or read depth. This is because smCounter evaluates potential variants by the  number  of non-reference UMIs, while smCounter2 evaluates potential variants by the  proportion  of non-reference UMIs. Moreover, because smCounter2 performs a statistical test at each site, UMI depth has already been accounted for in the  P -value. A higher UMI depth will result in better power of detection without raising the threshold. The consistent threshold makes it easier to benchmark smCounter2 with independent datasets. As pointed out by  Xu (2018) , benchmarking studies face the challenge of tuning the variant callers for different datasets. smCounter2 is also easier to use than smCounter. The read-processing code has been released together with the variant caller, making smCounter2 a complete pipeline from FASTQ to VCF. Some users may prefer to use their own read-processing script because read structures may differ from protocol to protocol. These users can run the variant caller only with the BAM file as input, if UMIs are properly tagged in the BAM. In addition, smCounter2 accepts UMI-consensused BAM files or pre-called variants in VCF format as input. Last but not least, smCounter2 is released as a Docker container image so that users do not need to install the dependencies manually. 4.2 Comparison with other UMI-based variant callers smCounter2 achieved better accuracy over other UMI-based variant callers in most of our benchmarking datasets ( Fig. 4  and Supplementary Figs S2–S4, S6–S10) except for non-coding germline indels where smCounter2 was outperformed by fgbio+HaplotypeCaller ( Supplementary Fig. S5 ). Compared to the two-stage approach, smCounter2 requires less tuning and achieves better detection accuracy with low-frequency variants. In contrast to MAGERI’s strategy of pooling data from several polymerases, smCounter2’s error model is developed using a single dataset with very deep coverage. Library preparation method and DNA polymerase have a large impact on the background error rates. Therefore we believe that profiling the errors per individual polymerase and protocol is a better approach. Furthermore, smCounter2 adjusts the error model for each individual dataset, making it a Bayesian-like procedure where the final error model is determined by both the prior knowledge and the data. 4.3 Limitations smCounter2 has several limitations. First, the error model is specific to the QIAseq targeted panel sequencing protocol, which uses integrated DNA fragmentation plus end repair process and single primer PCR enrichment. Without further tests, we are less certain if the error model holds for other types of library preparation and enrichment protocols. We are more certain, however, that our error model would not fit the data generated by hybridization capture enrichment due to distinct base errors from hybridization chemistry. We have released the modeling code and encourage users, who want to use smCounter2 on non-QIAseq panel data, to re-estimate the background error rates if datasets with sufficient UMI depth are available. Second, limited by resources, we were not able to generate data with enough UMI depth to accurately estimate the transversion and indel error rates. This deficit prevented the variant caller from reaching the assay’s theoretical detection limit. However, as we continue to generate data, we will update the error models with more precise parameters. Third, the germline indel calling accuracy, especially in non-coding regions, is lower than the two-step approach of fgbio+HaplotypeCaller. Although smCounter2 has very efficient repetitive region filters, it still adopts a base-by-base variant calling strategy and relies on the mapping, which is error-prone in repetitive regions. Haplotype-aware variant callers such as HaplotypeCaller are more effective in repetitive and variant-dense regions because they perform local assembly and no longer rely on the local reference genome alignment information. Fourth, smCounter2 has difficulty in handling very complex variants. For example, it failed to report all minor alleles of the complex, multi-allelic variant in Section 3.4. This can potentially be solved by including haplotype-aware features. We have not tested smCounter2’s reliability in detecting variants with three or more minor alleles, partly because these variants are not observed frequently. By default, smCounter2 reports bi- and tri-allelic variants only. Fifth, the benchmarking study was based on reference standards. We have not demonstrated smCounter2’s performance using real tumor samples and therefore cannot claim clinical utility. We hope smCounter2 will be used in both translational and clinical studies and look forward to feedback from users. Additional files and availability of data The high-confidence heterozygous NA12878-not-NA24385 variants (GIAB v3.3.2) in N13532, N0261, N0030, N0015, high-confidence NA24385 variants in N11582 and verified Tru-Q 7 variants in M0253 are available in VCF format. N0015 and N0030 reads have been published in  Xu  et al.  (2017)  and are available in Sequence Read Archive (SRA) under accession number SRX1742693. M0253, N13532, N0261 and N11582 are available in SRA under study number SRP153933. Supplementary Material Supplementary Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>ASAP: a web-based platform for the analysis and interactive visualization of single-cell RNA-seq data</Title>
    <Doi>10.1093/bioinformatics/btx337</Doi>
    <Authors>Gardeux Vincent, David Fabrice P A, Shajkofci Adrian, Schwalie Petra C, Deplancke Bart, Stegle Oliver</Authors>
    <Abstract/>
    <Body>1 Introduction Several bioinformatic platforms have been developed that aim to lower the entry point to ‘-omic’ type of analyses ( Afgan  et al. , 2016 ;  Reich  et al. , 2006 ). The latter include pipelines dedicated to single-cell analyses such as SINCERA ( Guo  et al. , 2015 ), SEURAT ( Satija  et al. , 2015 ), MAST ( Finak  et al. , 2015 ), PAGODA ( Fan  et al. , 2016 ) or SC3 ( Kiselev  et al. , 2016 ). However, these pipelines are embedded in R which makes them still computationally complex. For example, SC3 has preprocessing abilities using the scater package ( McCarthy  et al. , 2017 ) but has an interactive component that focuses mainly on the clustering part. Indeed, most of the other available pipelines lack an interactive visualization component as well as integration of a broad range of available single-cell data processing algorithms. In response, several valuable platforms have recently been developed that integrate graphics components. These include SCell ( Diaz  et al. , 2016 ), Sincell ( Julia  et al. , 2015 ), Fastproject ( DeTomaso and Yosef, 2016 ), or START ( Nelson  et al. , 2016 ). These tools are embedded in stand-alone applications and cover more comprehensively the whole RNA-seq analysis pipeline, yet, they still lack key features. For example, FastProject performs filtering and visualization but no further analysis. SCell implements RUVg normalization only, and visualization is limited to PCA. Moreover, SCell lacks marker gene identification (based on differential gene expression analysis) or functional gene set enrichment capacities. Finally, these pipelines require local installation of the software, which can be time-consuming or even daunting. To alleviate these constraints, we developed ASAP, a fully integrated, web-based pipeline aimed at the complete analysis of scRNA-seq data post genome alignment. Our choice of rendering ASAP completely web-based was motivated by the fact that fewer users are inclined to install and update manually their tools, which is no longer required with web 2.0 software. ASAP allows the user to easily select and compare common, as well as single-cell specific algorithms, and provides an interactive visualization of the results. ASAP supports users in the data interpretation process by its fast speed, running the whole analysis pipeline in minutes, and by providing on-the-go visualization, clustering, differential gene expression analysis, and enrichment functionality. ASAP, to our knowledge, is currently the only tool that combines in-depth analysis features and sophisticated visualization for single-cell data in one unique platform. 2 Materials and methods ASAP is a web-based application written in Ruby on Rails. The core structure is completely independent from any currently hosted web application (which are mostly coded in R/Shiny). This effectively makes the platform autonomous and allows the implementation of any tool independent of its source language. Currently, the server runs codes in R, Python and Java, and this process is invisible for the user, who only requires a web browser without prior installation of any development tool. The current list of methods that is included in ASAP is shown in  Figure 1  and detailed in  Supplementary Table S1 . Current and past versions are visible in the ‘Help’ page of the website (ASAP is versioned according to tool versions).
 Fig. 1 ASAP pipeline. The figure depicts the complete pipeline, including tools, that is implemented in ASAP. The user starts by uploading a count matrix (or a normalized matrix) of gene expression after which either the default pipeline or different filtering algorithms can be selected. After the normalization step, the user can apply different dimensionality reduction methods to visualize the data in 2D or 3D. The user can interactively select samples, or run clustering algorithms to perform differential gene expression analysis. Finally, the selected gene list can be analyzed for enrichment in biological modules or pathways such as the Gene Ontology or KEGG. All tools are referenced in  Supplementary Table S1 The current implementation of ASAP relies on the  delayed::job  framework which automatically creates and queues jobs when the user asks to run a particular method. This allows the application to be perfectly scalable to any IT architecture and prevents major slowdown of the website. Of course, the job execution time scales with the number of users and the host’s computational power. But this will be mainly dependent on the available cores/RAM on the server that hosts ASAP. ASAP has also full compatibility with the last versions of Chrome, Mozilla and Safari. The uploaded user data is protected by an anonymous registration system which keeps the user data private. A sandbox also allows any user to analyze the example project or upload his own data without prior registration. However, the data is destroyed when the user’s session ends. 3 Results As a proof-of-concept, we re-analyzed data from ( Dueck  et al. , 2015 ), in which scRNA-seq was used to study gene expression variation across five mouse cell types, involving 91 cells. We demonstrate that ASAP is capable of replicating the main findings of this study in minutes in straightforward fashion (Supplementary Figs S1–S14). We also made these data available as a demo study on the ASAP front page, which is available without registration. It is important to note that, despite the fact that ASAP is primarily dedicated to single-cell analysis, most of the tools can be employed for bulk RNA-seq analysis as well, which makes the pipeline more versatile and universal. ASAP will be further developed as we commit to adding more functionalities and species handling on a continuous basis. We also plan to add an automatic report generation functionality, aiming to summarize the employed methods together with figures, version, citation and parameters. The database for functional enrichment analysis will remain automatically updated through a CRON job, and more databases will be added to cover links to oncogenes, drugs, as well as additional species. Funding This work has been supported by funds from the Swiss National Science Foundation (#31003A_162735 and #IZLIZ3_156815) and by Institutional support from the EPFL and Human Frontier Science Program LT001032/2013 (to PCS). 
 Conflict of Interest : none declared. Supplementary Material Supplementary Tables Click here for additional data file. Supplementary Figures Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>BioBloom tools: fast, accurate and memory-efficient host species sequence screening using bloom filters</Title>
    <Doi>10.1093/bioinformatics/btu558</Doi>
    <Authors>Chu Justin, Sadeghi Sara, Raymond Anthony, Jackman Shaun D., Nip Ka Ming, Mar Richard, Mohamadi Hamid, Butterfield Yaron S., Robertson A. Gordon, Birol Inanç</Authors>
    <Abstract>Large datasets can be screened for sequences from a specific organism, quickly and with low memory requirements, by a data structure that supports time- and memory-efficient set membership queries. Bloom filters offer such queries but require that false positives be controlled. We present BioBloom Tools, a Bloom filter-based sequence-screening tool that is faster than BWA, Bowtie 2 (popular alignment algorithms) and FACS (a membership query algorithm). It delivers accuracies comparable with these tools, controls false positives and has low memory requirements.</Abstract>
    <Body>1 INTRODUCTION Pipelines that detect pathogens and contamination screen for host sequences so they do not interfere with downstream analysis ( Castellarin  et al. , 2012 ;  Kostic  et al. , 2011 ;  Tang  et al. , 2013 ;  Xu  et al. , 2014 ). The alignment-based algorithms that these pipelines use provide mapping locations that are irrelevant for classification, and thus perform more computation than is needed. To address this, we have developed BioBloom Tools (BBT). BBT uses Bloom filters—probabilistic, constant time access data structures that identify whether elements belong to a set ( Bloom, 1970 ). Bloom filters are similar to hash tables but do not store the elements themselves; instead, they store a fixed number of bits for every element into a common bit array. Thus, they use less memory, but queries to the filter may return false membership (hits) because of hash collisions in the common bit array. The false-positive rate (FPR) resulting from these false hits can be managed by increasing the size of the filter ( Supplementary Material ). Using Bloom filters for sequence categorization was pioneered by the program FACS ( Stranneheim  et al. , 2010 ). Here, we describe a Bloom filter implementation that includes heuristics to control false positives and increase speed. 2 METHODS We first build filters from a set of reference sequences by dividing the sequences into all possible  k -mers (substrings of length  k ). We compare the forward and reverse complement of every  k -mer, and include the alphanumerically smaller sequence in the filter. We calculate the bit signature of a  k -mer by mapping the sequence to a set of integer values using a fixed number of hash functions ( Supplementary Materials ) ( Broder and Mitzenmacher, 2004 ). The bitwise union of the signatures of all the  k -mers constitutes a Bloom filter for the corresponding reference sequences. To test whether a query sequence of length  l  is present in the target reference(s), we use a sliding window of  k -mers. Starting at one end of the query sequence, and shifting one base pair at a time along this sequence, we check each  k -mer against each reference’s Bloom filter. When a  k -mer matches a filter, we incrementally calculate a score:
 s = ∑ i = 1 c ∑ j = 1 a i 1 − 1 ( j + 1 ) l − k 
where  c  is the number of contiguous stretches of adjacent filter-matching  k -mers until the current position in the query, and  a i  is the length of the  i- th stretch. This heuristic penalizes likely false-positive hits. We evaluate  k -mers this way until we reach either a specified score threshold ( s* ) or the end of the query sequence. If at any point we reach  s* , we categorize the query as belonging to the reference, and terminate the process for that query. Further, we use a jumping  k -mer heuristic that skips  k k -mers when a miss is detected after a long series of adjacent hits. This efficiently handles cases in which the query has a single (or a few) base mismatch(es) with the target. 3 BENCHMARKING We compared BBT against two widely used Burrows–Wheeler transform-based alignment tools that have low memory usage and high accuracy—BWA ( Li and Durbin, 2003 ) and Bowtie 2 (BT2; Langmead and Salzberg, 2012)—and against the C++ implementation of FACS ( https://github.com/SciLifeLab/facs ). Tool versions and other details are provided in the  Supplementary Materials . 3.1 Benchmarking on simulated data We used dwgsim ( https://github.com/nh13/DWGSIM ) to generate simulated Illumina reads from human, mouse and  Escherichia coli  reference genomes. For each genome, we generated 1 million 2 × 150 bp paired-end (PE) reads and 1 million 100 bp single-end (SE) reads. We used  E. coli  because it is a common contaminant and is genetically distant from human. With mouse, which is commonly used in xenograft studies, we tested categorization accuracy for species that are closely related genetically. Because FACS does not support PE reads, we used the 100 bp SE reads to compare the false- and true-positive rates (FPR and TPR, respectively) of BBT and FACS. We tested a range of scoring thresholds for both tools. Using a  k -mer size of 25 bp, BBT generally matched or outperformed FACS ( Fig. 1 A and B). We note that, for shorter  k -mers, performance of BBT and FACS algorithms would deteriorate, especially in distinguishing sequences from closely related references. For both tools, longer  k -mers gave lower FPR but also lower maximum TPR ( Supplementary Figs S1 and S2 ), with BBT performing increasingly better than FACS for longer  k -mers.
 Fig. 1. Performance comparisons of BBT against FACS, BWA and BT2. Receiver operator characteristic curves of BBT and FACS using simulated 100 bp SE reads from  Homo sapiens  mixed with ( A )  E.coli  and ( B )  Mus musculus  filtered against an  H.sapiens  Bloom filter using a  k -mer size of 25 bp; ( C ) CPU time benchmark comparing BT2 (for a range of built-in settings), BWA (using aln and mem settings), FACS and BBT, on one lane of human 2 × 150 bp PE Illumina HiSeq 2500 reads To compare BBT and FACS to BWA and BT2, we used 2 × 150 bp PE reads. In our tests, overall, BBT performed comparably with the aligners and outperformed ‘fast’ and ‘very fast’ settings of BT2 in both false-negative rate (FNR) and false-discovery rate (FDR;  Table 1 ).
 Table 1. Benchmarking results using simulated paired end 2 × 150 bp reads Tool and Settings FNR FDR FDR ( H.sapiens) ( M.musculus ) ( E.coli) BT2 very sensitive 1.40 × 10 −5 2.03 × 10 −2 0 BT2 sensitive 7.52 × 10 −4 9.08 × 10 −3 0 BT2 fast 1.26 × 10 −2 5.90 × 10 −3 0 BT2 very fast 1.34 × 10 −2 5.65 × 10 −3 0 BWA aln 3.26 × 10 −3 8.14 × 10 −4 0 BWA mem 0 1.92 × 10 −1 1.00 × 10 −4 FACS 1.22 × 10 −1 9.88 × 10 −3 0 BBT ( s * = 0.1) 8.42 × 10 −3 3.78 × 10 −3 0 Note : All reads were treated as SE reads for FACS. 3.2 Benchmarking on experimental data We used a single lane of 2 × 150 bp PE human DNA reads ( https://basespace.illumina.com/run/716717/2x150-HiSeq-2500-demo-NA12878 ) generated with an Illumina HiSeq 2500 sequencer to benchmark computational performance. For a controlled comparison, we ran at least eight replicates for each tool, and we measured CPU time, with all applications using a single thread. We ran BBT with  s * = 0.1 and compared it with FACS, BWA and BT2, using a range of run modes for the latter two tools. BBT was faster than the fastest aligner/settings combination (BT2 very fast) by at least an order of magnitude ( Fig. 1 C). The mapping rates (categorization rates for BBT and FACS) of each tool were comparable, at 96.69 (BT2 very sensitive), 96.57 (BT2 sensitive), 96.18 (BT2 fast), 95.97 (BT2 very fast), 99.76 (BWA mem), 95.12 (BWA aln), 95.81 (FACS) and 97.27% (BBT). 3.3 Memory usage For categorization, using the human reference and simulated reads, the peak memory usage (GB) for each tool was 3.8 (BBT), 4.8 (FACS), 3.1 (BWA aln), 5.2 (BWA mem) and 3.4 (BT2). These figures are for categorization only and do not include the memory usage for creating the FM-indexes or Bloom filters. Unless slower disk-based methods are used, creating an FM-index takes at least O( n log( n )) bits of memory, where  n  is the size of the reference sequence ( Ferragina  et al. , 2012 ). In contrast, Bloom filter memory usage is the same for the creation and categorization stages, and takes O(- n log( f )) bits of memory, where  f  is the FPR and  n  is the number of input sequences. We created filters using 3.2 GB of memory for both FACS and BBT. Assuming optimal numbers of hash functions are used, filters with the same size should have similar FPRs. However, in practice, we had to use different FPR settings in creating these filters (FPR of 0.5% for FACS and 0.75% for BBT). We note that the tools would differ from theoretical estimates because of implementation-specific calculation differences. Finally, to demonstrate the scalability of BBT, we built a filter for 5182 bacterial sequences (representing 6 × 10 10  unique 25-mers), using 6.8 GB of memory, corresponding to an FPR of 0.75%. 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Modelling haplotypes with respect to reference cohort variation graphs</Title>
    <Doi>10.1093/bioinformatics/btx236</Doi>
    <Authors>Rosen Yohei, Eizenga Jordan, Paten Benedict</Authors>
    <Abstract/>
    <Body>1 Background Statistical modelling of individual haplotypes within population distributions of genetic variation dates back to the  Kingman (1982) n-coalescent . In general, the coalescent and other models describe haplotypes as generated from some structured state space via recombination and mutation events. Although coalescent models are powerful generative tools, their computational complexity is unsuited to inference on chromosome length haplotypes. Therefore, the dominant haplotype likelihood model used for statistical inference is the  Li and Stephens (2003)  model (LS) and its various modifications. LS closely approximates the more exact coalescent models but admits implementations with rapid runtime. Orthogonal to statistical models, another important frontier in genomics is the development of the variation graph, as described in  Paten  et al.  (2014) . This is a structure which encodes the wide variety of variation found in the population, including many types of variation which cannot be represented by conventional models. Variation graphs are a natural structure to represent reference cohorts of haplotypes since they encode haplotypes in a canonical manner: as node sequences embedded in the graph (see Novak  et al. , 2016). 
 Dilthey  et al.  (2015)  demonstrate the benefit of incorporating a graph representation of population information into a model for genome inference. However, their model does not account for haplotype phasing. In this paper, we present the first statistical model for haplotype modelling with respect to graph-embedded populations. We also describe an efficient algorithm for calculating haplotype likelihoods with respect to large reference panels. The algorithm makes significant use of the graph positional Burrows-Wheeler transform (gPBWT) index of haplotypes described by Novak  et al.  (2016). 2 Materials and methods 2.1 Encoding the full set of human variation Haplotypes in the  Kingman (1982) n -coalescent and  Li and Stephens (2003)  models are represented as sequences of values at linearly ordered, non-overlapping binary loci. Some authors model multiallelic loci (for example, single base positions taking on values of  A, C, T, G or gap ) as in  Lunter (2016) , but all assume that the entirety of genetic variation can be expressed by values at linearly ordered loci. However, many types of genetic variation cannot be represented in this manner. Copy number variations, inversions or transpositions of sequence create cyclic paths which cannot be totally ordered. Large population cohorts such as the 1000 Genomes Project Consortium  et al.  (2015) project data contain simple insertions, deletions and substitution at a sufficient density that these variants sometimes overlap or nest into structures not representable by linearly ordered sites. Two examples of this phenomenon from 1000 Genomes data [Phase 3 Variant Call Format file (VCF)] for chromosome 22 are pictured in  Figure 1 . Fig. 1 Two examples of non-linearly orderable loci in a graph of  1000 Genomes  variation data for chromosome 22 which form overlapping or nested sites In order to represent these more challenging types of variation, we use a  variation graph . This is a type of  sequence graph —a mathematical graph in which nodes represent elements of sequence, augmented with 5′ and 3′sides, and edges are drawn between sides if the adjacency of sequence is observed in the population cohort (see  Paten  et al. , 2017 ). Haplotypes are embedded as paths through oriented nodes in the graph. We are able to represent novel recombinations, deletions, copy number variations or other structural events by adding paths with new edges to the graph, and novel inserted sequence by paths through new nodes. 2.2 Adapting the recombination component of LS to graphs The  Li and Stephens (2003)  model (LS) can be described by an HMM with a state space consisting of previously observed haplotypes and observations consisting of the haplotypes’ alleles at loci. Recombinations correspond to transitions between states and mutations are modelled within the emission probabilities. Since variation graphs encode full nucleic acid sequences rather than lists of sites we extend the model to allow recombinations at base-pair resolution rather than just between loci. Let  G  denote a variation graph. Let  S ( G )  be the set of all possible finite paths visiting oriented nodes of  G . A path  h  in  S ( G )  encodes a potential  haplotype . A variation graph posesses an embedded  population reference cohort H  which is a multiset of haplotypes  p ∈ S ( G ) . Given a pair  ( G , H ) , we seek the likelihood  P ( h | G , H )  that  h  arose from haplotypes in  H  via recombinations. Recall that every oriented node of  G  is labelled with a nucleic acid sequence. Therefore, every path  h ∈ S  corresponds to a nucleic acid sequence  s e q ( h )  formed by concatenation of its node labels. We represent recombinations between haplotypes by assembling subsequences of these sequences  s e q ( h )  for  h ∈ H . We call a concatenation of such subsequences a  recombination mosaic . This is pictured in  Figure 2 . Fig. 2 The labelled path shows the recombination mosaic  x  superimposed on the embedded haplotypes  H  in our  1000 Genomes project  chr 22 graph; below,  x  is mapped onto its nucleic acid sequence Fig. 3 A sketch of the flow of information in the likelihood calculation algorithm described. Blue arrows a represent the  rectangular decomposition ,  R · ( · )  are prefix likelihoods We can assign a likelihood to a mosaic  x  by analogy with the recombination model from LS. Assume that nucleotide in  x  has precisely one successor in each  p ∈ H  to which it could recombine. Then, between each base pair, we assign a probability  π r  of recombining to a given other  p ∈ H , and therefore a probability  ( 1 − ( | H | − 1 ) π r )  of not recombining. Write  π c  for  ( 1 − ( | H | − 1 ) π r ) . By the same argument underlying the LS recombination model, we then we have a probability of a given mosaic having arisen from  ( G , H )  through recombinations:
 (1) P ( x | G , H ) = π r R ( x ) π c | x | − R ( x ) 
where  | x |  is the length of  x  in base pairs and  R ( x )  the number of recombinations in  x . We will use this to determine the probability  P ( h | G , H )  for a given  h ∈ S ( G ) , noting that multiple mosaics  x  can correspond to the same node path  h ∈ S ( G ) . Given a haplotype  h ∈ S ( G ) , let  χ ( h )  be the set of all mosaics involving the same path through the graph as  h . The law of total probability gives
 (2) P ( h | G , H ) = ∑ x ∈ χ ( h ) P ( x | G , H ) (3) = ∑ x ∈ χ ( h ) π c | h | − R ( x ) π r R ( x ) = π c | h | ∑ x ∈ χ ( h ) ( π r π c ) R ( x ) 
Let  ρ : = π r π c ; then  P ( h | G , H )  is proportional to a  ρ R ( x ) -weighted enumeration of  x ∈ χ ( h ) . We can extend this model by allowing recombination rate  π ( n )  and effective population size  | H | eff ( n )  to vary across the genome according to node  n ∈ G  in the graph. Varying the effective population size allows the model to remain sensible in regions traversed multiple times by cycle-containing haplotypes. In our basic implementation we will assume that  π ( n )  is constant and  | H | eff ( n ) = | H | ; however varying these parameters does not add to the computational complexity of the model. 2.3 A linear-time dynamic programming for likelihood calculation We wish to calculate the sum  ∑ x ∈ χ ( h ) ρ R ( x )  efficiently. (See (3) above) We will achieve this by traversing the node sequence  h  left-to-right, computing the sum for all prefixes of  h . Write  h b  for the prefix of  h  ending with node  b . 
 Definition  1.  A  subinterval s  of a haplotype  h  is a contiguous subpath of  h . Two subintervals  s 1 , s 2  of haplotypes  h 1 , h 2  are  consistent  if  s 1 = s 2  as paths, however we distinguish them as separate objects. 
 Definition  2.  Given a indices  a , b  of nodes of a haplotype  h ,  S b a  is the set of subintervals  s *  of  p ∈ H  such that there exists a subinterval  s  of  h  which begins with  a , ends with  b  and is consistent with  s * there exists no such subinterval of  p  which begins with  a − 1 , the node before  a  in  h (left-maximality) 
 Definition  3.  For a given prefix  h b  of  h  and a subinterval  s *  of a haplotype  p ∈ H , define the subset  χ ( h ) s * ⊆ χ ( h )  as the set of all mosaics whose rightmost segment arose as a subsequence of  s * . The following result is key to being able to efficiently enumerate mosaics: 
 Claim  1.  If  s 1 , s 2 ∈ S b a  for some  a , then there exists a recombination-count preserving bijection between  χ ( h b ) s 1  and  χ ( h b ) s 2 . 
 Proof . See  Supplementary Material . 
 Corollary  1.  If we define
 (4) R b ( s i ) : = ∑ x ∈ χ ( h b ) s i ρ R ( x ) 
then  R b ( s 1 ) = R b ( s 2 )  if  s 1 , s 2 ∈ S b a  for some  a . Call this shared value  R b ( a ) . 
 Definition  4. 
 A b  is the set of all nodes  a ∈ G  such that  S b a  is nonempty. Using these results, the likelihood  P ( h b | G , H )  of the prefix  h b  ending at index  b  can be written as
 (5) P ( h b | G , H ) = π c | h b | ∑ s i R b ( s i ) = π c | h b | ∑ a ∈ A b | S b a R b ( a ) Let  b − 1  represent the node preceding  b  in  h ; we wish to show that if we know  R b − 1 ( a )  for all  a ∈ A b − 1 , we can calculate  R b ( a )  for all  a ∈ A b  in constant time with respect to  | h | . This can be recognized by inspection of the following linear transformation:
 R b ( a ) = ρ f s ( w , ℓ ) ( A + B ) + 1 a ≠ b ( 1 − ρ ) ( f t ( ℓ ) R b − 1 ( a ) + (6) f s ( w , ℓ ) + f t ( ℓ ) w A ) 
where  w = ∑ a | S b a | ,  f s ( w , ℓ ) : = ( 1 + ( w − 1 ) ρ ) ℓ − 1 ,  f t ( ℓ ) : = ( 1 − ρ ) ℓ − 1 , and  A , B  are the  | A b − 1 | -element sums
 (7) A : = ∑ a ∈ A b − 1 | S b a | R b − 1 ( a ) , (8) B   : = ∑ a ∈ A b − 1 [ | S b − 1 a | − | S b a | ] R b − 1 ( a ) 
Proof that (6) computes  R b ( · )  from  R b − 1 ( · )  is straightforward but lengthy and therefore deferred to the  Supplementary Material . If we assume memoization of the polynomials  f s ( h , ℓ ) , f t ( ℓ ) , and knowledge of  w , ℓ  and all  | S b a | ’s, then all  R b ( a ) ’ s can be calculated together in two shared  | A b − 1 | -element sums (to calculate  A  and  A + B ) followed by a single sum per  R b ( a ) . Therefore, by computing increasing prefixes  h b  of  h , we can compute  P ( h | G , H )  in time complexity which is  O ( n · m )  in  n = | h | , and  m = max b | A b | . The latter quantity is bounded by  | H |  in the worst theoretical case; we will show experimentally that runtime is asymptotically sublinear in  | H | . 2.4 Using the gPBWT to enumerate equivalence classes in linear time The gPBWT index described by Novak  et al.  (2016) is a succinct data structure which allows for linear-time subpath search in a variation graph. This is graph analogue of the positional Burrows Wheeler transform by  Durbin (2014)  which is used in the  Lunter (2016)  fast implementation of the Viterbi algorithm in the LS model. Like other Burrows-Wheeler transform variants, the gPBWT possesses a subsequence search function which returns intervals in a sorted path index. 
 Novak  et al.  (2016)  prove that the gPBWT allows  O ( n )  query of the number of subintervals from a set of graph-embedded paths containing a sequence of length  n . Therefore, for any indices  a , b  in a path  h  we can compute the following quantity in  O ( b − a )  time. 
 Definition  5 .  J b a : =  the number of subpaths in  H  matching  h  between nodes  a  and  b . Since we can cache the search interval used to compute  J b a  from the gPBWT, we can also calculate  J b a  in  O ( 1 )  time given that we have already computed  J b − 1 a . This is important because 
 Claim  2 .  | S b a | = J b a − J b a − 1 Proof. By straightforward manipulation of definitions 2 and 5. And therefore, if we have already calculated  { | S b − 1 a | : a ∈ A b − 1 } , then in order to compute  { | S b a | : a ∈ A b } , we need only perform  | A b − 1 | O ( 1 )  extensions of the gPBWT search intervals used to compute the  | S b − 1 a | ’s and one additional  O ( 1 )  query to compute  | S b b | . Therefore, we can compute all nonzero values  | S b a | , for indices  a ≤ b  of  h , using  | A b − 1 | + 1 O ( 1 )  gPBWT search interval extensions for each node  b ∈ h . This makes the calculation of all such nonzero  | S b a | ’s calculable in  O ( n · m )  time overall, where  n = | h |  and  m = max b | A b | . This result, combined with the results of Section 2.3, show that we can calculate  P ( h | G , H )  in  O ( n · m )  time, for  n = | h |  and  m = max b | A b | . 2.5 Modelling mutations We can assign to two haplotypes  h , h ′  the probability  P m ( h | h ′ )  that  h  arose from  h ′  through a mutation event. As in LS model, we can assume conditional independence properties such that
 (9) P tot ( h | G , H ) = ∑ h ′ ∈ s e q ( G ) P m ( h | h ′ ) P r ( h ′ | G , H ) 
It is reasonable to make the simplifying assumption that  P m ( h | h ′ ) = 0  unless  h ′  differs from  h  exclusively at short, non-overlapping substitutions, indels and cycles since more dramatic mutation events are vanishingly rare. This assumption is implicitly contained in the  n -coalescent and LS models by their inability to model more complex mutations. Detection of all simple sites in the graph traversed by  h  can be achieved in linear time with respect to the length of  h . The number of such paths remains exponential in the number of simple sites. However, our model allows us to perform branch-and-bound type approaches to exploring these paths. This is possible since we can calculate upper bounds for likelihood from either a prefix, or from interval censored haplotypes where we do not specify variants within encapsulated regions in the middle of the path. Furthermore, it is evident from our algorithm that if two paths share the same prefix, then we can reuse the calculation over this prefix. If two paths share the same suffix, in general we only need to recompute the  | S b a |  values for a small number of nodes. This is demonstrated in Section 4.2. 3 Implementation We implemented the algorithms described in C ++, building on the variation graph toolkit  vg  by  Garrison (2016) . This is found in the ‘ haplotypes ’ branch at  https://github.com/yoheirosen/vg . No comparable graph-based haplotype models exist, so we could not provide comparative performance data; absolute performance on a single machine is presented instead. 4 Results 4.1 Runtime for individual haplotype queries We assessed time complexity of our likelihood algorithm using the implementation described above. Tests were run on single threads of an Intel Xeon X7560 running at 2.27 GHz. To assess for time dependence on haplotype length, we measured runtime for queries against a 5008 haplotype graph of human chromosome 22 built from the 1000 Genomes Phase 3 VCF on the hg19 assembly created using  vg  and 1000 Genomes Project Consortium  et al.  (2015) project data. Starting nodes and haplotypes at these nodes were randomly selected, then walked out to specific lengths. In our graph, 1 million nodes correspond, on average, to 16.6 million base pairs. Reported runtimes are for performing both the rectangular decomposition and likelihood calculation steps ( Fig. 4 ). The observed relationship (see  Fig. 4 ) of runtime to haplotype length is consistent with  O ( n )  time complexity with respect to  n = | h | . Fig. 4 Runtime (s) versus haplotype length (nodes) for Chr 22 1000 Genomes data. Line with slope  1.01  and  R 2 = 0.972  was fitted to samples with length &gt;50 000 nodes in the log-log plot. This supports a  O ( n )  time complexity with respect to haplotype length We also assessed the effect of reference cohort size on runtime. Random subsets of the  1000 Genomes  data were made using  vcftools  ( Danecek  et al. , 2011 ) and our graph-building process was repeated. Five replicate subset graphs were made per population size with the exception of the full population graph of 2504 individuals. We observe (see  Fig. 5 ) an asymptotically sublinear relationship between runtime and reference cohort size. Fig. 5 Runtime (s) versus reference cohort size (diploid individuals) for chromosome 22 1000 Genomes data. Line with slope  0.27  and  R 2 = 0.888  was fitted to samples with population size &gt;300 individuals in the log-log plot. This supports an asymptotically sublinear time complexity with respect to reference cohort size 4.2 Time needed to compute the rectangular decomposition of a haplotype formed by a recombination of two previously queried haplotypes The assessments described above are for computing the likelihood of a single haplotype in isolation. However, haplotypes are generally similar along most of their length. It is straightforward to generate rectangular decompositions for all haplotypes  h ∈ H  in the population reference cohort by a branching process, where rectangular decompositions for shared prefixes are calculated only once. This will capture all variants observed in the reference cohort. Haplotypes not in the reference cohort can then be generated through recombinations between the  h ∈ H . If this produces another haplotype also in  H , it suffices to recognize this fact. If not, then given that  h  is formed by a recombination of  h 1  and  h 2 , then  h  must contain some sequence of nodes  c → j  contained in neither  h 1  nor  h 2 . We only need to recalculate  S b a  for  a ≤ j ≤ b . We have implemented methods to recognize these nodes and perform the necessary gPBWT queries to build the rectangular decomposition for  h . The distribution of time taken (in milliseconds) to generate this new rectangular decomposition for randomly chosen  h 1 , h 2  and recombination point is shown in  Figure 6 . Fig. 6 Distribution of times (in milliseconds) required to recompute the rectangular decomposition of a haplotye given that it was formed by recombination of two haplotypes for which rectangular decompositions have been constructed. This graph omits 0.6% of observations which are outliers beyond 1 s of time Mean time is 141 ms, median time 34 ms, first quartile time 12 ms and third quartile time 99 ms. To compute a rectangular decomposition from scratch mean time is 71 160 ms, first quartile time 68 690 ms and third quartile time 73 590 ms. This rapid calculation of rectangular decompositions formed by recombinations of already-queried haplotypes is promising for the feasibility of a mutation model or of sampling the likelihoods of large numbers of haplotypes. Similar methods for the likelihood computation using this rectangular decomposition are a subject of our current research. 4.3 Qualitative assessment of the likelihood function’s ability to reflect rare-in-reference features in reads We used vg to map the 1000 Genomes low coverage read set for individual NA12878 on chromosome 22 against the variation graph described previously. 1 476 977 reads were mapped. Read likelihoods were computed by treating each read as a short haplotype. These likelihoods were normalized to ‘relative log-likelihoods’ by computing their log-ratio against the maximum theoretical likelihood of a sequence of the same length. An arbitrary value of  10 − 9  was used for  π recomb . We define a read to contain  n  ‘novel recombinations’ if it is a subsequence of no haplotype in the reference, but it could be made into one using a minimum of  n  recombination events. We define the prevalence of the rarest variant of a read to be the lowest percentage of haplotypes in the index which pass through any node in the read’s sequence. We segregated our set of mapped reads according to these features. We make three qualitative observations, which can be observed in ( Fig. 7 ). First, the likelihood of a read containing a novel recombination is lower than one without any novel recombinations. Second, this likelihood decreases as novel recombinations increase. Third, the likelihood of a read decreases with decreasing prevalence of its rarest variant. Fig. 7 Left: density plot of relative log-likelihood of reads not containing variants below 5% prevalence or novel recombinations (black line) versus reads containing novel recombinations. Right, density plot of relative log-likelihood of reads not containing variants below 5% prevalence or novel recombinations (black line) versus reads containing variants present at under 5% prevalence and under 1% prevalence A further comparison ( Fig. 8 ) of these same mapped reads against reads which were randomly simulated without regard to haplotype structure shows that the majority of mapped reads from NA12878 score are assigned higher relative log-likelihoods than the majority of randomly simulated reads. Fig. 8 Density plot of relative log-likelihood of mapped reads versus randomly generated simulated haplotypes 5 Conclusions We have introduced a method of describing a haplotype with respect to the sequence it shares with a variation graph-encoded reference cohort. We have extended this into an efficient algorithm for haplotype likelihood calculation based on the gPBWT described by Novak  et al.  (2016). We applied this method to a full-chromosome graph consisting of 5008 haplotypes from the 1000 Genomes data set to show that this algorithm can efficiently model recombination with respect to both long sequences and large reference cohorts. This is an important proof of concept for translating haplotype modelling to the breadth of genetic variant types and structures representable on variation graphs. Our basic algorithm does not directly model mutation, however we describe an extension which does. Making this extension computationally tractable will depend on being able to very rapidly compute likelihoods of sets of similar haplotypes. We demonstrate that our algorithm can be modified to compute rectangular decompositions for haplotypes related by a recombination event in millisecond-range times. We have also devised mathematical methods for recomputing likelihoods of similar haplotypes which take advantage of analogous redundancy properties; however, they have yet to be implemented and tested. However, we anticipate that we will be able to compute likelihoods of large sets of related haplotypes on a time scale which makes modelling mutation feasible. Supplementary Material Supplementary Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Powerful fusion: PSI-BLAST and consensus sequences</Title>
    <Doi>10.1093/bioinformatics/btn384</Doi>
    <Authors>Przybylski Dariusz, Rost Burkhard</Authors>
    <Abstract>Motivation: A typical PSI-BLAST search consists of iterative scanning and alignment of a large sequence database during which a scoring profile is progressively built and refined. Such a profile can also be stored and used to search against a different database of sequences. Using it to search against a database of consensus rather than native sequences is a simple add-on that boosts performance surprisingly well. The improvement comes at a price: we hypothesized that random alignment score statistics would differ between native and consensus sequences. Thus PSI-BLAST-based profile searches against consensus sequences might incorrectly estimate statistical significance of alignment scores. In addition, iterative searches against consensus databases may fail. Here, we addressed these challenges in an attempt to harness the full power of the combination of PSI-BLAST and consensus sequences.</Abstract>
    <Body>1 INTRODUCTION PSI-BLAST achieves a remarkable compromise between speed and quality . Ideally, an alignment method should accurately identify related sequences in today's rapidly growing databases within the shortest possible time. While we want to simultaneously optimize speed and reliability, in practice there is a tradeoff: very accurate alignment methods are relatively slow (e.g. profile–profile alignment algorithms), while very fast methods are far less sensitive than we might wish (e.g. BLAST; Altschul  et al. ,  1990 ). PSI-BLAST (Altschul  et al. ,  1997 ) strikes an excellent compromise between speed and sensitivity. Consensus sequences improve PSI-BLAST performance . Consensus sequences were used early on to improve alignments (Patthy,  1987 ). The initial approaches mimicked profile-sequence alignments (Henikoff and Henikoff,  1997 ; Sonnhammer and Kahn,  1994 ). Many improvements followed (Finn  et al. ,  2006 ; Kahsay  et al. ,  2005 ; Letunic  et al. ,  2006 ; Marchler-Bauer  et al. ,  2002 ; Merkeev and Mironov,  2006 ; Schaffer  et al. ,  1999 ; Schultz  et al. ,  1998 ; Servant  et al. ,  2002 ; Thelen  et al. ,  1999 ). However, none of those methods approached the success of PSI-BLAST. We have recently proposed a simple add-on to PSI-BLAST that substantially improves its performance (Przybylski and Rost,  2007 ). The add-on did not require any code change in PSI-BLAST. It consisted of adding a final step of ‘freezing’ the profile after the standard, iterative search against native sequences and then using it to search a database with the native sequences replaced by their consensus counterparts. This simple add-on improves the performance throughout the entire sensitivity curve. However, it is not clear how the underlying residue composition of database sequences affects the statistics of alignment scores. This is an important issue because users rely on the estimates of statistical significance to judge retrieved alignments. In addition, incorrect scoring might invalidate iterative searches against consensus sequences; a single false alignment in one of the intermediate searches might pollute a scoring profile and thereby all subsequent searches. This study was motivated by the following three assumptions: (1) For a given residue substitution scoring matrix, the statistical significance of alignment scores depends on the residue compositions of aligned sequences. Assume that a particular scoring matrix highly rewards the alignment of tryptophan. This implies that sequences rich in tryptophan will likely generate higher alignment scores than those with average tryptophan content. (2) In general, the composition of consensus sequences differs from that of native sequences. Therefore, the distribution of alignment scores is likely different for consensus and native sequences, at least when using the same scoring matrix for both [such as BLOSUM62 (Henikoff and Henikoff,  1992 ) or the corresponding position-specific scoring matrices]. (3) PSI-BLAST is very popular, well-maintained, and has a great impact on the community of scientists that use sequence alignments. Therefore, it is desirable to improve PSI-BLAST performance without changing its alignment parameters (including scoring matrices and gap scores) with which the community is already familiar. In order to accomplish this, we have asked the following questions: how much do the parameters of alignment score distribution change for various types of consensus sequences? Can PSI-BLAST compensate for compositional variations through its internal composition-based adjustments (Schaffer  et al. ,  2001 )? Or, can we build consensus sequences in a way that renders statistical significance reported by PSI-BLAST as valid? Finally, can we apply PSI-BLAST to iteratively search consensus sequence databases? 2 METHODS 2.1 Generation of consensus sequences We derived the consensus sequences from position-specific scoring matrices (PSSM, also known as scoring profiles) generated by iterative PSI-BLAST (‘blastpgp’) (Altschul  et al. ,  1997 ) searches of the redundancy-reduced UniProt (Apweiler  et al. ,  2004 ) database containing about 1.5 million sequences. The sequence redundancy was reduced with CD-HIT (Li  et al. ,  2001 ) such that pairs of sequences had &lt;80% identical residues (globally). We allowed up to five PSI-BLAST iterations, i.e. the  frozen  profile was computed based on the fourth iteration or the next to the last one for early converging queries. The  E -value threshold for inclusion in PSSMs was set to 0.001 and we increased the maximum number of aligned sequences to 2000 [blastpgp options ‘-j 5 -h 0.001 –v 2000 –b 2000 -Q PSSM(ASCII)’]. Other options were left unchanged, including the default compositional adjustment of alignment score statistics and gap scores of −(11+ k ) for gaps of length  k . The determination of consensus sequences was based on ASCII PSSMs. For a given sequence and a residue position, we looked at the corresponding column of its PSSM and/or the frequency profile also present in the PSI-BLAST output. We explored three alternative ways for computing consensus residues at a given position  i  of a sequence: (1)  MF :  maximal frequency —the consensus residue  j  had the highest occurrence frequency  f ij  in the profile column, (2)  MET: maximal relative entropy term —we chose the residue  j  with the highest relative entropy term  f ij ln ( f ij / b j ) with respect to the background frequency  b j , (3)  MR: maximal ratio of frequencies —we chose the residue with the highest frequency ratio  f ij / b j . In addition, we studied full ( MF-full ,  MET-full ,  MR-full ) and partial ( MF-partial ,  MET-partial ,  MR-partial ) versions of consensus sequences. For the ( 1 ) full consensus sequences, we computed the consensus residue at each sequence position, and for the (2) partial consensus, we computed the consensus in a constrained way, e.g. only for the more  informative  positions. The more informative positions were those having profile frequency columns with the relative entropy equal or above 0.6 (as reported in the PSI-BLAST output). 2.2 Alignments All of the alignments (except those used to estimate the asymptotic values of the alignment score distribution parameters) were generated using PSI-BLAST version 2.2.15. The  frozen  scoring profiles (PSSMs) for the non-iterative profile-sequence alignments were generated in the same way as those used for generation of consensus sequences, except that a file containing the binary version of a PSSM was also stored [blastpgp option ‘-C PSSM(binary)’]. Those binary PSSMs were used for a final (non-iterative) PSI-BLAST search against the appropriate consensus or native sequence databases [blastpgp options: ‘-j 1 -R PSSM(binary)’]. For the non-profile-based sequence–sequence alignments the default BLOSUM62 (Henikoff and Henikoff,  1992 ) scoring matrix was used (blastpgp options: ‘-j 1’). When studying iterative searches against consensus sequence databases, we compared the performance for various number of iterations. The consensus version of the redundancy-reduced UniProt database used in iterative consensus searches was computed over a period of a few months using spare CPUs of a large computing cluster. 2.3 Evaluation of similarity search capability We evaluated the ability to identify remotely related proteins using SCOP (Murzin  et al. ,  1995 ) (release 1.69). We used the usual, descending hierarchy levels of ‘fold’, ‘superfamily’ and ‘family’ to define true and false relationships. Our positives consisted of pairs of protein domains from the same SCOP superfamily, but different SCOP families (i.e. the relatively easy pairs from the same family were not counted). However, for the more sensitive iterative searches against consensus sequences, we also counted pairs from the same SCOP-fold as positives. The negatives belonged to different SCOP-folds. We removed domains with: discontinuous sequences, missing coordinates in their three-dimensional structures, NMR and low-resolution structures (&gt;2.5 Å), and the short ones (&lt;50 residues). Next, we reduced the sequence redundancy of the set so that no pair of sequences could be aligned by BLAST with  E -values better than 10 −3  (when computed on UniProt database of ∼2 000 000 sequences), or at levels of sequence identity and alignment length that corresponded to homology-derived structures of proteins (HSSP)-values above 0 (Rost,  1999 ; Sander and Schneider,  1991 ) (whichever of the two criteria applied). This yielded a dataset of 2476 sequences for which we applied the all-against-all test. 2.4 Score statistics PSI-BLAST provides statistical significance of alignment scores in terms of expectation values ( E -values) that are given by:
 (1) 
where  m  and  n  are the effective lengths (Altschul and Gish,  1996 ) of aligned sequences (query and database),  score  is a raw alignment score (as given by the values in scoring matrix and gap penalties), and  K  and λ are the parameters of the score distribution that depend on a scoring system and the residue composition of aligned sequences. Note that the computation of the  E -value primarily depends upon a proper estimate of λ and much less so on that for  K . 2.5 Determining parameters of alignment score distributions The problem of estimating the statistical significance of alignment scores has been studied extensively (Altschul and Gish,  1996 ; Karlin and Altschul,  1990 ; Mott,  1992 ; Waterman and Vingron,  1994 ). We computed λ and  K  parameters [Equation ( 1 )] with our implementation of the ‘island’ approach (Altschul  et al. ,  2001 ; Olsen  et al. ,  1999 ) for a case of scoring profiles. This approach is appropriate as the primary methods studied in this article rely on searching databases of consensus sequences with precomputed PSSMs. We have also estimated the score distribution parameters for profile-based searches against native sequences to relate our results to the earlier studies. First, we obtained the initial PSSMs for hundreds of thousands of randomly selected UniProt sequences. Most of them were too short to study the score distribution in the asymptotic limit of very long sequences. Therefore, we concatenated them in random order and then cut them into final long PSSMs, each composed of 7000 columns. We ended up with 75 000 of such long PSSMs. To generate corresponding long random consensus sequences, we first computed consensus sequences from each of the long PSSMs and used them to compute consensus residue background frequencies. Those background frequencies were used to generate random sequences used for studying asymptotic alignment score distribution parameters. For partial consensus sequences, we computed two separate sets of backgrounds—inside and outside of consensus regions and used them accordingly for generation of random partial consensus sequences (with the informative positions indicated by the original PSSM relative entropy values at each sequence position). 2.6 Studying the compositional adjustment of alignment score statistic in PSI-BLAST The newer versions of PSI-BLAST can adjust alignment score statistics based on varying residue compositions of query and database sequences (‘-t’ option in PSI-BLAST). In particular, we looked at the performance of the default adjustment implemented in the 2.2.15 version of the software. We have generated random sequence databases based on the native and consensus background residue frequencies. The numbers of random sequences and their sizes were the same as those found in the non-redundant UniProt database. We queried those databases with about 20 000 randomly chosen native sequences and the corresponding PSI-BLAST profiles (PSSMs). We recorded the average cumulative numbers of alignments per query that had  E -values better than a given threshold value. 3 RESULTS AND DISCUSSION 3.1 Alignment score parameters depended on consensus type The variation of λ with the alignment score [Equation ( 1 )] for gapped alignments has been described before (Altschul  et al. ,  2001 ). Low-scoring alignments usually have fewer gaps and their score distribution differs from those obtained for high-scoring alignments with gaps. Here, we have focused mostly on asymptotic values of λ for high scores because they correspond to statistically significant alignments originating from searches of large sequence databases. In particular, we looked at λ for PSSMs generated with five iterations of PSI-BLAST. We observed that λ depended on the sequence types ( Fig. 1 ). Computing consensus residues for the full sequence produced largest changes in λ (open symbols in  Fig. 1 , i.e.  MR-full ,  MET-full  and  MF-full ). For each one of them, the asymptotic value of λ was less than 0.2 (more data points would be needed to establish a precise limit). The value of λ for the profile-sequence alignments of the native sequences was about 0.255 ( Fig. 1 ; green squares). This is rather close to a value of 0.267 previously established for the sequence-sequence alignments with the BLOSUM62 scoring matrix (Altschul  et al. ,  2001 ). For the partial consensus sequences, λ appeared to follow the value obtained for the native sequences (filled symbols in  Fig. 1 , i.e.  MF-partial  and  MET-partial ). To some extent this result is not surprising because partial consensus substitutions are more restricted than the full ones, i.e. change fewer residues ( Table 1 ). As a result, we established that one could use PSI-BLAST without any modifications to perform profile-based search against partial consensus sequence databases and maintain proper estimates of  E -values.
 Fig. 1. Estimating λ. Score distribution parameter λ [Equation ( 1 ),  y -axis] varies with alignment scores ( x -axis). In practice, we are interested in the asymptotic value of λ for higher scores. Full consensus sequences affected λ significantly (open symbols) when compared to native sequences (green squares). In contrast, partial consensus did not significantly affect λ (filled black and blue symbols). Red error bars estimate the SD (for clarity only shown for native sequences). Note that high alignment scores were attained by few alignments. 
 Table 1. Pairwise residue identities of native and consensus sequences Native Full consensus Partial consensus native MR MF MET MR MF MET Native native 100 Full consensus MR 65 100 MF 54 76 100 MET 51 80 90 100 Partial consensus MR 86 79 64 63 100 MF 83 72 71 67 93 100 MET 82 73 69 69 94 98 100 Shown are average percentages of pairwise residue identities between different types of sequences of a test set. 
 We have also estimated the location parameter  K  used for computing  E -values [Equation ( 1 )]. For example, we found it to be ∼0.015 for the full consensus sequences ( MF-full ), 0.030 for the partial consensus sequences  (MF-partial ) and 0.032 for the native sequences. 3.2 Search performance similar for all consensus types Do some types of consensus sequences retrieve related sequences from a database better than others? For each type of consensus, we ordered all query alignments by PSI-BLAST  E -values. Next, we computed the cumulative numbers of true positive relations (same SCOP superfamily but different family) for increasing cumulative numbers of false positive pairs (different SCOP-folds). For any number of false positives (i.e. at any error rate), the profile-sequence searches against the databases of full consensus sequences yielded most true positives ( Fig 2 ; top three curves:  MET-full ,  MF-full ,  MR-full ). Interestingly, it did not matter much how we compiled the full consensus (three top lines with open symbols in  Fig. 2  are almost indistinguishable). The profile-based searches against partial consensus sequences (only most informative positions replaced by consensus) were somewhat less efficient, especially when more false hits were allowed ( Fig 2 ;  MET-partial ). Nevertheless, they were significantly better than standard profile-sequence searches of PSI-BLAST ( Fig 2 ;  native ). For comparison, we also included the performance of sequence–sequence searches with pairwise BLAST against the native and consensus sequences ( Fig. 2 ;  MET-full-1, MET-partial-1, native-1 ). As expected, pairwise searches fared much worse than profile-sequence searches. The relative performance difference between the full and partial consensus sequences appeared larger for the sequence–sequence ( Fig. 2 ;  MET-full-1, MET-partial-1 ) than for profile-sequence searches.
 Fig. 2. Comparison of search performance. All-against-all alignments of the test set sequences were ordered by their PSI-BLAST  E -values. The cumulative numbers of non-trivial true relations (same SCOP superfamily but different SCOP family) were plotted against the cumulative numbers of false positives (different SCOP-folds). The profile-sequence searches against the full consensus sequences performed best (top three curves:  MET-full ,  MF-full, MR-full ). Profile-sequence searches against partial consensus sequences were slightly less efficient ( MET-partial ), but they were still significantly better than standard profile-sequence ( native ). Sequence–sequence searches (one cycle of PSI-BLAST with BLOSUM62 matrix) were clearly inferior ( MET-full-1, MET-partial-1, native-1 ). 3.3 Composition of consensus sequences varied The search performance appeared not to differ between various types of full consensus sequences, although their average residue compositions were quite different ( Fig. 3 A). The consensus based on the maximum ratio of target and background frequencies ( MR-full ) weighed more heavily rare residues such as tryptophane (W). The consensus based on the most frequent residue ( MF-full ) weighed more heavily the more ubiquitous ones such as leucine (L). Finally, the consensus based on relative entropy ( MET-full ) produced the composition that appeared to be more balanced ( Fig. 3 A, blue bars). The average percent differences in residue identity (and SDs) between native and full consensus sequences were: 65 (±14) for  MR-full , 54 (±16) for  MF-full  and 51 (±17) for  MET-full  consensus sequences. The partial consensus calculations resulted in average compositions that were much closer to the native ones ( Fig. 3 B). The corresponding residue identities with respect to native sequences were: 86 (±7)%, 83 (±8)% and 82 (±8)%. Thus, the consensus calculation (MR) that changed sequences the least in terms of the average residue identity has changed the score distribution parameters the most. Other pairwise residue identities are given in  Table 1 . All calculations were performed on our non-redundant SCOP test set.
 Fig. 3. Comparison of residue compositions. We computed the background residue compositions for consensus and native sequences in our test set. Full consensus sequences ( A ) differed more from native than partial consensus sequences ( B ). Choosing the consensus residue corresponding to the highest relative entropy term (blue bars) resulted, on average in smaller deviations from the native composition. 3.4 PSI-BLAST compositional adjustments were partially successful When compositions of aligned sequences differ from a standard one, PSI-BLAST can attempt to correct the estimates of statistical significance accordingly (Schaffer  et al. ,  2001 ; Yu and Altschul,  2005 ). We studied how well the default adjustments perform on consensus sequences (non-default adjustments are not available for profile-based searches). Using PSI-BLAST profiles we searched against the consensus and native sequence databases ( Section 2 ). For the comparison, we also searched with the BLOSUM62 substitution matrix (standard, non-profile BLAST search). In the latter case, the estimates of statistical significance were not very sensitive to compositional differences and the statistic adjustments worked well ( Table 2 , observed and expected counts similar; adjustments were conservative). However, for the profile-based searches the compositional differences played a significant role, particularly for the full consensus sequences (especially pronounced for  MR-full ,  Table 3 ). The compositional adjustment of scores attempted by PSI-BLAST (-t option set to 1) failed to satisfactorily correct for the differences. In contrast, the  E -value estimates were good for partial consensus sequences. For both native and partial consensus sequences, the compositional score adjustment sometimes resulted in slightly increased numbers of random alignments with significant  E -values.
 Table 2. Accuracy of BLAST  E -values a Observed Native Full consensus Partial consensus Expected native native-adj. MR MR-adj. MF MF-adj. MET MET-adj. MR MR-adj. MF MF-adj. MET MET-adj. 0.001 0.0014 0.0010 0.0010 0.0002 0.0007 0.0006 0.0009 0.0003 0.0014 0.0008 0.0018 0.0008 0.0012 0.0009 0.01 0.010 0.007 0.006 0.004 0.006 0.005 0.008 0.004 0.011 0.006 0.012 0.005 0.011 0.006 0.1 0.09 0.07 0.07 0.04 0.03 0.06 0.08 0.06 0.09 0.07 0.11 0.07 0.10 0.07 1 0.9 0.7 0.7 0.5 0.2 0.7 0.9 0.6 0.9 0.7 1.1 0.7 1.0 0.7 10 9 7 7 6 18 7 9 7 9 8 11 8 10 8 a Shown are the expected and observed numbers of random alignment scores per query for ∼20 000 sequence queries on randomly generated databases (of UniProt size) of native and consensus sequences. Appendix ‘-adj.’ indicates results obtained with the use of compositional adjustment of  E -values with BLAST option ‘-t’ set to 1. 
 Table 3. Accuracy of PSI-BLAST a   E -values b Observed Native Full consensus Partial consensus Expected native native-adj. MR MR-adj. MF MF-adj. MET MET-adj. MR MR-adj. MF MF-adj. MET MET-adj. 0.001 0.0013 0.0033 66.4912 1.6637 0.0024 0.0008 0.0215 0.0093 0.0053 0.0040 0.0014 0.0028 0.0013 0.0038 0.01 0.008 0.020 98.880 4.3162 0.018 0.028 0.086 0.036 0.022 0.026 0.010 0.021 0.009 0.022 0.1 0.08 0.18 159.83 12.83 0.170 0.22 0.51 0.26 0.17 0.17 0.10 0.20 0.10 0.20 1 0.8 1.6 259.1 34.9 1.6 1.8 3.2 2.1 1.4 1.4 1.0 1.7 1.0 1.7 10 8 13 405 102 14 14 22 16 12 12 9 13 10 14 a Shown are the expected and observed numbers of random alignment scores per query for a set of about 20 000 profile (PSSM) queries on randomly generated databases (of UniProt size) of native and consensus sequences. Appendix ‘-adj.’ indicates results obtained with a use of compositional adjustment of E-values with PSI-BLAST option ‘-t’ set to 1. b PSI-BLAST search was restarted from a stored profile. 
 3.5 Little additional CPU needed for add-on In this study, we used separate databases for the iterative derivation of PSSMs (non-redundant UniProt) and for the final search and alignment against consensus sequences. On average, the entire iterative PSI-BLAST search took about 10 min per query (about 2 min per iteration on a single 3.2 GHz CPU with 2 GB of RAM using query sequences with average length of 415 residues). The additional time consumed by the add-on to search against a consensus sequence database of the same size depended on the sequence types. It took about 7 min to search  MR-full  and 4.5 min for  MF-full  consensus sequence databases. In the case of partial consensus it took about 2.5 min to search the  MR-partial  and about 2.2 min for  MF-partial  (compared to about 2 min needed to search native one with PSI-BLAST profile). 3.6 Iterative searches against consensus sequences yielded further improvements We made the first attempt at analyzing iterative PSI-BLAST searches against consensus sequence databases. For this analysis, we pushed the envelope by running up to 20 iterations. We counted hits belonging to the same SCOP-fold but to different families as positives to reach deeper into remote protein–domain relationships. The iterative PSI-BLAST searches against the native sequence database resulted in near saturation of performance at about 10 iterations. Only a small improvement was observed in the subsequent 10 iterations ( Fig. 4 ; top two green lines). The iterative searches against consensus sequences ( MF-full ) produced significantly more true hits with just three iterations. Five consensus iterations produced almost twice as many true hits as the native PSI-BLAST search produced with 20. For comparison, we showed the results of the profile-sequence search (profile obtained from 10 iterations of PSI-BLAST on a native database) against a final database of consensus sequences ( Fig. 4 ; blue line, mixed). These results remain to be compared to the performance of profile–profile methods (Bujnicki  et al. ,  2001 ; Fischer  et al. ,  2003 ).
 Fig. 4. Iterative PSI-BLAST searches against native and consensus sequences. Iterative PSI-BLAST searches and PSSM refinements on native sequence database (green lines) resulted in near saturation of performance at about 10 iterations (top two green lines). The corresponding searches on the database of consensus sequences (black lines) found significantly more true hits (same SCOP-fold but different family) with just three iterations (black triangles), while five iterations (black circles) retrieved almost twice as many true hits as the maximum for the native PSI-BLAST. For comparison, a result of the  frozen  profile-based search against a final database of consensus sequences ( MF-full ) is presented (blue line). 4 CONCLUSIONS PSI-BLAST is an excellent, well-known, well-maintained and trusted resource for searching and aligning sequence databases. A simple add-on consisting of searching with a PSI-BLAST generated scoring profile against a database of consensus sequences significantly improved the performance in finding related sequences. Here, we specified in detail how different strategies of compiling consensus residues affected the estimates of statistical significance and performance. Profile-based PSI-BLAST searches against full consensus sequences improved the most over searches against native sequences. However, they sometimes suffered from problems in the estimates of statistical significance. The partial consensus sequences improved significantly over native sequences without sacrificing estimates of statistical significance. Our initial results for iterative searches against consensus sequences were very promising: a lower number of iterations used less CPU overall and yielded about twice as many correct hits at the same error rates as standard PSI-BLAST searches did. Hence, the fusion of PSI-BLAST and consensus sequences promises another leap in database searches. </Body>
  </Article>
</Articles>
