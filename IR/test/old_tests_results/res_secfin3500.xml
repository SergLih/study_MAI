<Articles>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>MutCombinator: identification of mutated peptides allowing combinatorial mutations using nucleotide-based graph search</Title>
    <Doi>10.1093/bioinformatics/btaa504</Doi>
    <Authors>Choi Seunghyuk, Paek Eunok</Authors>
    <Abstract/>
    <Body>1 Introduction With the advances in genomics and proteomics technologies such as next-generation sequencing and tandem mass spectrometry, we can better identify sample-specific and/or novel peptides. Construction of protein sequence database plays an important role in identifying novel peptides because reliable peptide identification depends heavily on the database being searched. In terms of mutated peptide identification, many methods have been proposed to construct a proteogenomic database, such as CustomizedDB ( Park  et al. , 2014 ), CustomProDB ( Wang and Zhang, 2013 ), CanProVar ( Zhang  et al. , 2017 ) and a variant graph ( Woo  et al. , 2014a ). CustomizedDB and CustomProDB assumed that all mutations occur simultaneously in a gene. On the other hand, CanProVar assumed that all digested peptides can have no more than a single mutation. These approaches reduce the search time by avoiding the exhaustive search for all possibilities, but naturally preclude covering all possible mutated peptides at the same time. Woo and colleagues proposed a ‘variant graph’, which represents a given transcriptome model as a direct acyclic graph, where each node is a nucleotide sequence representing a part of an exon or a variant call and edges connect neighboring exons indicating splice sites or point mutation occurrences. They also provided a ‘variant graph to FASTA’ enumeration package because most database search engines only take a FASTA formatted database as an input. The enumerated variant graph can represent almost all possible combinations of mutated peptides depending on a user specific parameter. However, the parameter is not intuitive because it is an internal parameter that controls the algorithm behavior and does not directly describe the desired output. The parameter value has to do with the density of mutation calls, which may vary widely depending on genes, making it very difficult to set the value properly. Furthermore, the variant graph method does not allow a user to set the translation frame: a user may want only in-frame translation, or all three-frame translations. To overcome such limitations, we developed MutCombinator, which enables us to identify combinatorially mutated peptides by searching a variant graph directly ( Fig. 1 ) without enumerating them into amino acid sequences off-line. A variant graph is built by taking reference genome sequences in FASTA, transcriptome model in GTF and variant calls in VCF format as input. MutCombinator can identify peptides with combinations of maximum  n  mutations ( n  is a user-specified parameter) in a peptide. The possible combinations of mutations grow exponentially as  n  increases; therefore, the search time can grow exponentially. To keep the search time under control, we adopted the two existing techniques: (i) extract short amino acids sequence tags (of length 3) from a spectrum, and search paths containing at least one tag to avoid traversing the whole variant graph [this approach was suggested elsewhere ( Mann and Wilm, 1994 ), but they used FASTA database instead of a variant graph] and (ii) indexing variant graph using three amino acids long sequences to directly access nucleotide sequences in the graph. For convenience of a user, MutCombinator supports (i) multi-threading, which enables parallel processing of spectra and (ii) both in-frame coding region search as well as three-frame search that encompasses non-coding regions as well.  Fig. 1. Overview of MutCombinator. A variant graph is built from the reference genome sequences, a transcriptome model and variant calls. Positions of each nine nucleotide sequence in the variant graph are indexed by corresponding three amino acids and stored in a pre-compiled index table. Positions of sequence tags (e.g. PRE and TTY) deduced from a spectrum are directly recognized by looking up the index table. PSM is processed by traversing the flanking paths of tag positions. The black box represents an exon model in the given transcriptome model. The gray box represents a variant model annotated in the given variant calls We designed a multistage search ( Madar  et al. , 2018 ) with MutCombinator to effectively identify mutated peptides using proteogenomics data from a previous study ( Mun  et al. , 2019 ). First, we used unidentified tandem mass (MS/MS) spectra from the previous study as an input for the second stage search using MutCombinator under the conditions: (i) use of 12 688 mutations from sample-specific variant calls and 83 873 mutations from COSMIC database (a total of 96 287 mutations), (ii) 28 843 expressed protein coding transcripts (supported by FPKM &gt; 1) and (iii) allowing up to three mutations per a peptide. As a result, we additionally identified 80 mutated peptides than the previous report. From this result, we could find 10 additional KEGG-pathways and 70 combinations of mutations. Furthermore, we also identified four mutated peptides harboring exclusively expressed mutations. At the third stage, we further searched unidentified MS/MS spectra from the first and second stage search against the same database with the same search conditions as the second stage except for one thing: use of 12 516 expressed non-coding transcripts (also supported by FPKM &gt; 1). We identified 14 aberrantly translated peptides—five pseudogenes, four frameshifts, two exon extensions, two 5′ UTRs, and one 3′ UTR peptides. 2 Materials and methods Peptide identification in MutCombinator consists of four parts: (i) construction of a variant graph with frame information, (ii) indexing the variant graph to directly access nucleotide sequences in the graph, (iii) candidate peptides generation by traversing the variant graph from indexed positions and (iv) scoring peptide spectrum matches (PSMs). 2.1 Construction of variant graph with frame-awareness Originally, a variant graph was not designed to limit the search only to in-frame translation because it assumed that the graph would be built directly from RNA-Seq results. This assumption was made for discovery of potential novel coding regions; however, it is not suitable for identifying mutations in known protein coding regions, because it triples the search space, resulting in increased false positives and execution time. Recent proteogenomic research have focused more on identifying mutated peptides in the coding region because their relation with the disease can be significant ( Mertins  et al. , 2016 ;  Mun  et al. , 2019 ;  Zhang  et al. , 2014 ). To facilitate comprehensive mutation identification in a proteogenomic search, we augmented variant graphs with frame information in each node whenever it is available. When constructing a variant graph with frame-awareness, each transcript model is initially represented as a linear graph structure (list) where nodes represent nucleotide sequences of exons and edges represent junction sites between two exons ( Fig. 2a ). When multiple transcripts share a common region (the same genome positions and the same nucleotide sequences, shown in gray in  Fig. 2a ), it is represented as a single node in the merged transcript as shown in  Figure 2b  while the remaining parts of the original transcript, i.e. distinct parts, are split from the original node and the split sites are connected by an edge. When splitting nodes, each node inherits its frame information from the original transcript. The frame information is recorded as binary vectors where each row represents each of the three frames and each column represents a transcript of a given gene. For example, if a gene has two transcripts, then the first transcript is represented with a bit value 0b00000001 and second one 0b00000010. Thus, only one of the three rows in the binary vectors has non-zero value in the original transcript, if the transcript model represents a coding sequence ( Fig. 2a ). We used only seven bits because the sign-bit (the most significant bit) is not suitable for index value. When a gene has more than 7 transcripts, PABPC1 has 18 protein coding transcripts for example and the column size of the binary vectors is determined as {1 + quotient of dividing the number of transcripts by 7} bytes.  Fig. 2. Construction of a variant graph augmented with frame information. Nodes represent nucleotide sequences and edges connect neighboring nodes. Edges generated by splitting nodes in each step are represented as dotted lines. The letter boxes indicate coding sequences. ( a ) Each transcript is represented as a linear graph structure. Frame information is recorded in each node as binary vectors. Gray nodes represent common regions among multiple transcripts of the same gene. ( b ) The common regions are merged into a single node, and their original frame information is coalesced by bit-OR operation. Each gene is represented as a single directed acyclic graph after this step. ( c ) SNVs and insertions are added, and frames are re-calculated To compact the transcripts into a single merged transcript, nodes in the transcripts are split into common and distinct parts based on both genomic positions and nucleotide sequences. When nodes are split into two or three nodes, each frame information of the split nodes is recalculated in order to keep track of the transcript structures. In the example shown in  Figure 2b  (focusing on how the frame information of the second transcript changes) frames of the first node do not change. Frames of the remaining nodes are calculated based on their predecessor nodes in a topological order. If the nucleotide sequence length of a predecessor node is a multiple of three, then the current node is assigned the same frame of the predecessor node. Otherwise, the frames of the current node are set as the union of up-rotating each predecessor’s frame by the remainder of dividing its nucleotide sequence length by 3. Union operation is actually performed by bit-OR operation. This way, all the transcript models of a gene can be merged into a single variant graph. In case there are SNVs and insertions given as input (in VCF format), a node containing mutation site is split based on its mutation position. A new node representing the mutation is created and added to the graph, and their frames must be recalculated. The recalculation method is the same as above. In the example shown in  Figure 2c , there are two mutations: insertion ‘G &gt; GTT’ and SNV ‘G &gt; T’. While there is a single predecessor node for the last node SNV ‘G &gt; T’ (the split node and new node), the last node ‘GCA’, which is caused by the insertion, has two predecessors such as node ‘G’ and node ‘GTT’. In this case, we coalesce both up-rotated frame information by bit-OR operation, and apply the coalesced frame information as that of the last node ‘GCA’. In the case of deletion, we simply make a new transcript model in the gene before the merge step. As an concrete example, the first node in  Figure 2b , representing a nucleotide sequence ‘ATGGCA’, is split into three nodes such as ‘AT’, ‘G’ and ‘GCA’ shown in  Figure 2c . Node ‘AT’ inherited the same frame as the original node ‘ATGGCA’ because it is the first node among the three. The successor of node ‘AT’ is node ‘G’; therefore, the frames of node ‘G’ is set as up-rotating that of node ‘AT’ twice. Similarly, the frame information of the last node ‘GCA’ is set as bit-OR operation of both up-rotating that of node ‘G’ once and up-rotating that of node ‘GTT’ three times (thus no rotation operation). Notice that node ‘GTT’ as the insertion ‘G &gt; GTT’ just copies the same frame information with node ‘G’ because they have the same predecessor node ‘AT’. 2.2 Indexing variant graph In a typical database search approach to peptide identification, each spectrum is compared with candidate peptide sequences in a sequence database. There have been three major methods to avoid searching the whole database: (i) limit candidate peptides only to those that match the precursor mass of a given spectrum, (ii) select candidate peptides using fragment ion matches ( Kong  et al. , 2017 ) and (iii) select candidate peptides that contain sequence tags derived by applying de novo sequencing to a given spectrum ( Mann and Wilm, 1994 ;  Na  et al. , 2012 ;  Tabb  et al. , 2003 ). The first and second approaches enumerate all possible enzymatic peptides and find the best match for each spectrum. Enumerating all possible peptides of a given variant graph is not practical once we start to consider combinatorial mutation. MutCombinator adopted the third approach: extracting three amino acids sequence tag from an input spectrum and directly accessing the tag positions in variant graph by pre-compiled index. Each index is generated by traversing the whole variant graph and recording the following information: (i) a start node, (ii) an end node, (it must be noted that a sequence tag may span over multiple nodes), (iii) a tag start position within the start node, (iv) a tag end position within the end node, (v) a gene id, (vi) the number of mutation sites included in the nine nucleotide sequence of a tag and (vii) the frame information of a tag obtained by bit-AND operation of all the nodes in a path spanning the nine nucleotide sequence. During index generation, there are two cases when an index of tag should be discarded: (i) the number of mutation occurrences exceeds the maximum allowable mutations  n , specified as a user parameter, or (ii) all three frame information is 0b00000000, meaning that there is no proper path denoted by the tag. 2.3 Generating candidate peptides Sequence tags are inferred from a spectrum by  de novo  sequencing. The positions of these tag occurrences in a variant graph can be retrieved by looking up the pre-compiled index table. Candidate peptides, the masses of which match to a precursor mass of the spectrum, are generated by traversing the flanking nodes neighboring the tag positions. When traversing the flanking nodes, nucleotide sequences in a node is virtually translated to amino acids for all the valid frames and the peptide mass is calculated while extending the tag sequence into the neighboring nodes until the peptide mass just exceeds the precursor mass given the tolerance. The frame information in the tag is updated during the traversal by bit-AND operation among the visited nodes to confirm the validity of a path, i.e. a variant graph merged all the transcript models of a gene into a single graph, thus an integrity check is necessary to confirm that a path actually corresponds to some transcript model ( Supplementary Fig. S1 ). The traversal stops extending into the neighboring nodes whenever the frame information becomes 0b00000000 (i.e. there is no valid transcript that matches the nucleotide sequence of the current path), or when a path contains more mutations than the number of maximum allowable mutation  n . 2.4 Evaluating PSM quality MODa ( Na  et al. , 2012 ) evaluated PSMs using a logistic regression of four component scores such as: (i) prefix residue mass (PRM) score, (ii) mass error of matched fragment ions, (iii) the fractions of b and y ions found and (iv) the propensity to a particular ion type. We used the same scoring method to evaluate PSMs. Briefly, an experimental spectrum is first converted into a PRM spectrum, and the PRM spectrum is used to match against the candidate peptides using an alignment based on dynamic programming. 3 Results 3.1 Multistage search to further identify mutated peptides A huge number of disease-related mutations could be obtained by several resources such as ClinVar ( Landrum  et al. , 2018 ) and COSMIC databases ( Tate  et al. , 2019 ). To use these resources for proteogenomic study, we usually make a mutated peptide sequence database by considering all possible combinations of mutations because we cannot be sure which mutations might be observed in our samples. Under the assumption that some of these mutated peptides, derived from the public mutation resources, could also be observed by MS/MS spectra for the sample of our interest, we designed multistage search ( Madar  et al. , 2018 ) using MutCombinator ( Fig. 3 ). We chose N33T34 dataset, obtained from a tissue sample of a microsatellite instability (MSI) high cancer patient from a previous proteogenomics study on EOGC (early onset gastric cancer) ( Mun  et al. , 2019 ). N33T34 dataset included three types of data—4 215 882 MS/MS spectra [the spectra were processed by PE-MMR ( Shin  et al. , 2008 )] labeled with 4-plex iTRAQ, mRNA-seq and whole exome-seq. We used a preprocessed dataset provided by Mun and colleagues. There were a total of 41 359 expressed transcripts annotated in Ensembl transcriptome model v71. Among them, 28 843 and 12 516 transcripts were protein coding and non-coding, respectively. As for mutations, there were a total of 12 688 mutations matched to the expressed transcripts.  Fig. 3. Multistage search using MutCombinator. Unidentified MS/MS spectra from the previous result (EOGC second-stage dataset) are subjected to MutCombinator as an input. MutCombinator identifies mutated peptides considering combinations of mutations of both sample-specific and COSMIC mutations in the expressed coding transcripts. Unidentified MS/MS spectra from the expressed coding transcripts are subjected to identification of aberrantly translated peptides in the expressed non-coding transcripts. The identified PSMs are filtered out if there is the same sequence in UniProt proteome or contaminants. Note that the result of conventional search was provided by Mun and colleagues Among 12 688 sample-specific mutations, only 254 mutations (247 SNVs and 7 insertions) were found among 83 873 stomach cancer-related mutations of COSMIC database (version 87) with the following conditions: (i) available genomic positions and mutated nucleotide sequences, (ii) categorized as SNV, insertion, or deletion and (iii) matched to the expressed transcripts. Assuming that the two mutation sources can be complementary to each other, we constructed CnSSVG (Cosmic and sample-specific variant graph) using 96 307 unified mutations (83 873 stomach cancer-related mutations as well as 12 688 sample-specific mutations). EOGC group identified 588 483 PSMs by searching N33T34 spectra against CustomizedDB ( Park  et al. , 2014 ) using MS-GF+ search ( Kim and Pevzner, 2014 ) and we denoted this search strategy as a conventional search in  Figure 3 . To further identify mutated peptides considering combinations of mutations of both sample-specific and COSMIC mutations, we used 2 792 596 unidentified MS/MS spectra. Note that PE-MMR generates multiple spectra per scan by correcting precursor m/z and charge state; therefore, we filtered out 1 423 286 MS/MS spectra corresponding to 588 483 identified MS/MS scans in the previous result. We searched 2 792 596 MS/MS spectra using MutCombinator against CnSSVG. The search parameters were set as follows: 10 ppm for precursor tolerance, 0.025 Da for fragment tolerance, three fixed modifications (carbamidomethylation at cysteine and iTRAQ label at peptide N-terminal and lysine), semi-tryptic for enzyme specificity allowing up to two miscleavages and eight for minimum peptide length. We also set  n  to three, allowing up to three mutations per peptide. After the search, the same scans could appear more than one time in the PSM list because the spectra were processed by PE-MMR; therefore, we selected a PSM having the highest score among PSMs with the same scan number. And then, we applied separate false discovery rate (FDR) strategy ( Woo  et al. , 2014b ) so that mutated peptides and wild-type peptides could fairly compete with each other. We divided the search results into two: (i) PSMs with wild-type peptide match including both target or decoy and (ii) PSMs with mutated peptide match including both target or decoy. If a sequence of mutated peptide is equivalent to a sequence of wild-type peptide, we assigned the PSM as a wild-type PSM. The result was estimated at 1% local-FDR at PSM level. We identified 8778 wild-type PSMs and 231 mutated PSMs. From 231 mutated PSMs, we filtered out those sequences of which were found in UniProt proteome (release 2019-11) or contaminants. Repeating a similar workflow, we further identified aberrantly translated peptides from 12 516 non-coding transcripts using non-coding search mode in MutCombinator as the last stage. We also compared MutCombinator with a conventional search (MS-GF+ applied against CustomizedDB including mutation) when executed in a single stage search mode ( Supplementary Fig. S2 ). It must be noted that the search space of the two can be tremendously different. 3.2 Mutated peptides in coding regions With a multistage search using MutCombinator, we further identified 211 mutated PSMs in the coding regions ( Fig. 4a ). This result amounts to additional identification of 80 mutated peptides, 52 genes and 70 combinations of mutations. We compared two KEGG-pathways ( P -value &lt; 0.05) from (i) genes from the conventional search and (ii) genes from the conventional search together with MutCombinator, using DAVID ( Huang da  et al. , 2009 ) to see whether the additional gain in peptide identification could lead to different interpretation in terms of pathways ( Fig. 4b ). In the original conventional search results, two pathways were strongly enriched in ECM-receptor interaction and focal adhesion, showing significantly negative mRNA-survival correlation ( Mun  et al. , 2019 ). Our approach resulted in 10 additional significantly enriched pathways. To make sure that such additional enriched pathways are not random, possibly due to high proportions of such genes in CnSSVG, we further calculated  P -values using Fisher exact test. We used all genes harboring mutations in CnSSVG, as a background population and then calculated  P -value of each pathway using the genes found by MutCombinator only. All of the pathways showed  P -value below 0.05, showing that the pathways are significantly enriched in the search results (details in  Supplementary Table S1 ). Proteoglycans in cancer, one of the additional pathways, also showed significant negative mRNA-survival correlation in the previous report. Intriguingly, inflammation related pathways such as phagosome, leukocyte transendothelial migration, bacterial invasion of epithelial cells and viral myocarditis were enriched and this result is consistent with the already known relationship between inflammation and cancers ( Coussens and Werb, 2002 ).  Fig. 4. The identification of mutated peptides in coding regions. ( a ) Peptides, genes and combinations of mutations corresponding to a total of 211 mutated PSMs are described. ( b ) KEGG-pathways of two gene groups––results of conventional search with/without MutCombinator analysis—show different patterns. Pathways with  P -value &lt; 0.05 are used. ( c ) Combinations of mutations observed in MS/MS are categorized into three groups––conventional search, MutCombinator and commonly observed by both. The combinations of mutations in MutCombinator group are further classified into sample-specific and COSMIC mutations. ( d ) Mutated peptides harboring exclusively expressed mutations in LSP1 protein. Identified peptides and corresponding gene model are shown, and amino acid changes are indicated by red underline Owing to MutCombinator, we can further identify 70 combinations of mutations including 60 SNVs, 6 INDELs, 3 double SNVs and 1 double INDEL ( Fig. 4c ). Nine SNVs were derived from the sample-specific mutations, and the rest were derived from the COSMIC mutations. On the other hand, MutCombinator enables considering combinations of mutations allowing up to three mutations per peptide. We could identify four mutated peptides harboring exclusively expressed mutations in RHOA, RRBP1, HIST1H3H and LSP1. For example, we identified two mutated peptides resided in a genomic region from 1 902 744 to 1 902 800 in chromosome 11 ( Fig. 4d ). One of them had Alanine changed into Threonine at position 9 because of SNV (G &gt; A) at locus chr11:1 902 768. The other peptide had Glutamine changed into Leucine at position 17 because of SNV (A &gt; T) at locus chr11:1 902 793. Although these mutations originated from the same sample-specific mutations, they were expressed exclusively at the protein level. Next-generation sequencing analyses bulk of cells simultaneously, thus the actual combinations of mutations are not distinguishable at the genomic level. Certain conventional proteogenomic analyses could have missed identifying these two exclusively mutated peptides, but our approach could successfully resolve mutational ambiguities at the protein level by considering mutations combinatorially during the second stage search. 3.3 Aberrantly translated peptides in non-coding regions Proteogenomic approach can be useful in identifying peptides deduced not only from mutations but also from aberrant expression of non-coding RNAs and pseudogenes ( Kim  et al. , 2014 ;  Nesvizhskii, 2014 ). Protein sequence database built from three frame translation of genes of interest such as non-coding RNAs and/or pseudogenes is used to identify aberrantly expressed peptides from MS/MS spectra. Such an approach could be useful to correct gene annotation or, perhaps, analyze disease-specific patterns ( Stewart  et al. , 2019 ). We added three frame translation mode for non-coding RNAs and pseudogenes to MutCombinator so that a user can easily identify aberrantly expressed peptides with/without combinatorial mutations. We applied third stage search to identify aberrantly expressed peptides, after the two-stage search in the coding regions. We searched 2 773 632 unidentified spectra against 12 516 expressed non-coding/pseudogene transcripts, considering sample-specific and COSMIC mutations. We estimated at 1% local FDR and identified 122 PSMs. We removed 86 PSMs, peptide sequences of which exactly match UniProt (release 2019-11) or common contaminant sequences. We obtained genomic loci of 27 peptides corresponding to the remaining 36 PSMs by applying ACTG tool ( Choi  et al. , 2017 ). Thirteen mutated peptides could be matched to coding regions in Ensembl v71 so we further discarded the mutated PSMs in the identifications of aberrantly translated peptides. The summary of non-coding search result is described in  Figure 5 .  Fig. 5. Aberrantly translated peptides supported by MS/MS assay. ( a ) Mutated and wild peptides are categorized into variant types such as (1) protein coding, (2) frameshift, (3) UTRs, (4) exon extension and (5) pseudogene. ( b ) Details of novel peptides supported by MS/MS assay. The first peptide in the list is matched to two genes 4 Discussion  Proteogenomics has improved understandings of biology, via integration of genomics and proteomics. The baseline results of the integration depend on identifications of expressed and mutated peptides; however, there is no practically available software tool to identify mutated peptides considering all possible combinations of mutations in coding regions. We designed MutCombinator so that it can be applied to identify mutated peptides allowing combinatorial mutations using a reasonable amount of computational resources. A total of 2 792 596 spectra were processed using 75 GB of memory and 16 threads, taking 42 h on workstation computers. We demonstrated the usefulness of MutCombinator in two aspects: (i) identifications of mutated peptides with combinatorial mutations and (ii) incorporation of large-scale mutation database such as COSMIC. By considering combinations of mutations, MutCombinator facilitates identifying mutated peptides regardless of where the mutations really come from. In other words, we now can decode the combinations of mutations even when mutations from different sources are aggregated in a single database. Funding This work was supported by the National Research Foundation of Korea [NRF-2017M3C9A5031597, NRF-2017R1E1A1A01077412 and NRF-2019M3E5D3073568]; and the BK21 plus program through the National Research Foundation funded by the Ministry of Education of Korea. 
 Conflict of Interest : none declared.  Supplementary Material btaa504_Supplementary_Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>OMSim: a simulator for optical map data</Title>
    <Doi>10.1093/bioinformatics/btx293</Doi>
    <Authors>Miclotte Giles, Plaisance Stéphane, Rombauts Stephane, Van de Peer Yves, Audenaert Pieter, Fostier Jan, Hancock John</Authors>
    <Abstract/>
    <Body>1 Introduction The Bionano Genomics platform is able to visualize occurrences of specific, short sequence motifs (e.g. 7 bp) along very long stretches of linearized DNA molecules (up to 2.5 Mbp), thus forming a unique, sequence-specific pattern per molecule, sometimes referred to as a ‘barcode’. By using those signature patterns, the molecules can be assembled in a complete consensus genome map. This view of the genome can be used to validate or improve de novo genome assembly, by providing a scaffold on which the contigs can be anchored ( Shi  et al. , 2016 ), or to detect large-scale structural variation in genomes ( Mak  et al. , 2015 ). Bionano Genomics optical map data is generated in several steps. First, DNA molecules of up to 2.5 Mbp are labeled using nicking restriction endonucleases, cutting one strand of the DNA near specific recognition nucleotide sequences. At these nicking sites fluorescent nucleotides are introduced into the DNA to highlight the position where the DNA motif occurs. The labeled DNA is then linearized using nanochannel arrays and imaged, such that the fluorescent labels along each molecule can be detected. For each DNA molecule, its size as well as the positions of the labels on the molecule are estimated and stored in BNX format. These data can be visualized as beads on a cord, and assembling these into optical consensus maps involves the alignment of the molecules such that the label positions match. The Irys Software System ( http://bionanogenomics.com/wp-content/uploads/2015/01/datasheet-web.pdf ) and the Irys-scaffolding scripts ( Shelton  et al. , 2015 ) can be used to automate this procedure. We developed OMSim to generate synthetic optical map data. This serves two purposes. First, OMSim can assist in the development and benchmarking of tools that operate on Bionano Genomics optical map data such as alignment and assembly software. Simulated optical map data were used to this end in ( Muggli  et al. , 2014 ,  2015 ;  Li  et al. , 2016 ;  Leung  et al. , 2017 ), but the simulation tools were not publicly available and only took into account a limited subset of the noise factors present in real data. Second, OMSim can assist in designing the optimal experimental setup: given a genome of interest, OMSim-generated data can help to select the right nicking enzyme or combinations thereof, to identify local label-depleted areas with low information content, to evaluate the distribution of nicking sites, to identify fragile sites due to nearby occurring labels, etc. This information can then be used to optimize the parameters of an Irys run, to ultimately generate an optimal amount of useful real data. A concrete example of this assistance in experimental design is the simulation of data corresponding to a structurally altered genome and evaluating the ability of the Bionano platform to identify the structural variations. The use of simulated data for this second application significantly improves upon the use of nicker software, e.g. BioNano Genomics Knickers, which provide overal statistics on label density based on a reference genome analysis. These global statistics provide only limited insights in the problem at hand, while simulated data allows to actually test the performance of the assembly or variant detection. OMSim simulates the Bionano Genomics process using statistical models for which the parameters were derived from real data (see  Supplementary Material  data S1 for the parameter description), and generates output in BNX format. It is implemented in Python, and relies on the Scipy library to sample from the required distributions. A graphical user interface has been developed to facilitate the setup of the simulation process. OMSim requires a reference assembly as the ground truth for the simulation. Each map is simulated from a single contig, hence the contiguity of the reference assembly limits the lengths of the simulated optical maps, i.e. it is impossible to simulate an optical map that is longer than the contig from which it is simulated. 2 Methods and results OMSim was designed to accurately mimic all sources of variation that occur in the Bionano Genomics data. First, false positive and false negative labels are taken into account, where labels are either erroneously placed or not placed. Second, there is the occurrence of fragile sites, where labels that occur very close to each other cause systematic breaks in the molecules. Third, each molecule has a stretch factor, which quantifies how the migration of the DNA molecule through a nanochannel causes the molecule to stretch or shrink. Fourth, there is some additional variability in the position of the labels due to local stretching. Fifth, due to the limited optical resolution, nearby labels may appear as one label in the image. Finally, also due to the optical resolution, there is the possibility of chimeric maps, which occur when distinct molecules are close together in a nanochannel such that they appear as a single molecule in the image. The OMSim process consists of two steps. First, the locations of the sequence recognition sites in the genome are indexed using the computationally efficient Knuth-Morris-Pratt algorithm ( Knuth  et al. , 1977 ). This index can be reused for future runs. Second, using this index, OMSim simulates the actual optical map data. Molecule lengths are generated from a negative binomial distribution and for each molecule a start location is uniformly chosen on the provided reference genome. Then, labels and noise are introduced in each molecule. False positive (resp. negative) labels are uniformly distributed along the molecules (resp. labels). The molecules are broken at fragile sites, based on the proximity of neighbouring labels. Stretch factor variations are normally distributed. Labels that occur close together are collapsed into a single label. After simulating the molecules, chimeras are introduced by concatenating molecules. Optical map data was simulated from the human genome reference Hg19, and anchored using the Bionano Genomics RefAligner. The resulting alignments were compared to the alignments of real data from NA24385 (Ashkenazim Trio son, public data from  http://bionanogenomics.com/science/public-datasets/ ). A portion of these alignments and the coverage and the size distribution of both simulated maps and real maps are shown in  Figure 1 . This figure shows that the simulated data can be aligned to the reference, that similarly as in real data missing labels are present due to false positives or collapsing labels, and that the size distributions of the simulated and real data are nearly identical. A peak memory usage of 478 MB was measured while indexing the human reference Hg19 and simulating optical map data from this index. Peak memory usage depends on the number of nicking sites in the reference. The indexing run time is linear in the size of the reference, while the simulation run time is linear in the size of the output. In our tests for genomes with sizes ranging from 4 Mbp up to 30 Gbp, this corresponds to a throughput of 30 Mbp per minute for indexing and 12.5 Gbp per minute for the actual simulation. Loading the index in subsequent runs took less than 30 seconds for all data sets. From these results we conclude that OMSim efficiently simulates data that resemble the real Bionano Genomics data.
 Fig. 1 ( A ) Alignments of real and simulated data on a section of chromosome 10 of the human genome. The alignments were obtained with the Bionano Genomics RefAligner. The tracks from top to bottom are: (1) the consensus map, (2) real optical maps from data set NA24385 and (3) optical maps simulated with OMSim. Markers on each track correspond to the anchored labels. Only a fraction of the actual coverage is shown. ( B ) Comparison of the label coverage in 100 bins along chromosome 10 in Hg19, for both simulated and real data. ( C ) Comparison of the size distribution of simulated and real data over the entire genome. Molecules shorter than 150 kbp were filtered out  Funding This work was supported by The Research Foundation–Flanders (FWO) [G0C3914N]. 
 Conflict of Interest : none declared. Supplementary Material Supplementary Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Seed-based IntaRNA prediction combined with GFP-reporter system identifies mRNA targets of the small RNA Yfr1</Title>
    <Doi>10.1093/bioinformatics/btp609</Doi>
    <Authors>Richter Andreas S., Schleberger Christian, Backofen Rolf, Steglich Claudia</Authors>
    <Abstract>Motivation: Prochlorococcus possesses the smallest genome of all sequenced photoautotrophs. Although the number of regulatory proteins in the genome is very small, the relative number of small regulatory RNAs is comparable with that of other bacteria. The compact genome size of Prochlorococcus offers an ideal system to search for targets of small RNAs (sRNAs) and to refine existing target prediction algorithms.</Abstract>
    <Body>1 INTRODUCTION Bacterial small RNAs (sRNAs) are regulatory RNAs that often act as post-transcriptional regulators by base pairing to  trans -encoded target mRNAs. The sRNA–mRNA interaction can result in translational repression and/or mRNA degradation, as well as translational activation, mostly in response to changing environmental conditions (Waters and Storz,  2009 ). The few sRNA–mRNA interactions experimentally characterized so far have been particularly studied in the two model organisms  Escherichia coli  ( E.coli ) and  Salmonella typhimurium  LT2 ( Salmonella ) (Gottesman,  2005 ; Vogel,  2009 ). However, sRNA regulators are not restricted to model bacteria, but occur ubiquitously in bacteria. In this study, we investigated the ecologically important cyanobacterium  Prochlorococcus . This photoautotrophically dwelling organism often accounts for up to 50% of the organic biomass in the oligotrophic areas of the open oceans, and is thus a crucial component of the food web (Goericke and Welschmeyer,  1993 ; Vaulot  et al. ,  1995 ). A recent systematic survey of sRNAs in  Prochlorococcus  MED4 revealed a large number of potential regulatory RNAs comparable with those found in other bacteria (Steglich  et al. ,  2008 ). This finding was very surprising, as  Prochlorococcus  has experienced an evolutionary streamlining of its genome, leading to very compact genomes between 1.64 and 2.68 Mb, which notably results in a small number of regulatory proteins (Kettler  et al. ,  2007 ). The identification of sRNA targets in  Prochlorococcus  constitutes a big challenge, since common experimental approaches such as knockouts of these sRNAs cannot be applied. Instead, the only possible approach is a combination of  in silico  target prediction, followed by  in vivo  experimental validation (in a heterologous expression system). An interesting sRNA candidate to study is Yfr1, which is an abundant RNA with ubiquitous appearance in all lineages of cyanobacteria except for two  Prochlorococcus  strains (Voss  et al. ,  2007 ). Recent studies have shown that Yfr1 is constitutively expressed and accumulates up to 18 000 copies per cell in  Synechococcus elangatus  PCC6301 (Nakamura  et al. ,  2007 ). The high copy numbers of Yfr1 raise the question of whether this RNA acts as a  trans -encoded sRNA through base pairing with its targets, or whether it modulates protein activity. An example of such modulation activity is the 6S RNA, which downregulates mRNA transcription by mimicking an open promoter complex (Wassarman,  2007 ). However, a prominent feature of Yfr1 is the ultraconserved 11 nt long sequence motif located in an unpaired sequence stretch flanked by two stem–loops ( Fig. 1 A). Similar to Yfr1, the two  Salmonella  sRNAs GcvB and RybB show a conserved single-stranded region. In both the GcvB and RybB sRNAs, these regions are involved in the binding of multiple targets, which results in reduced translation of the targets (Vogel,  2009 ). To verify whether Yfr1 analogously regulates  trans -encoded mRNAs via base pairing, we predicted putative interaction partners of Yfr1 in the cyanobacterium  Prochlorococcus  MED4 and experimentally validated these candidates by a reporter system based on green fluorescent protein (GFP).
 Fig. 1. ( A ) Secondary structure of  Prochlorococcus  MED4 Yfr1, as predicted by RNA fold  (Hofacker  et al. ,  1994 ). The ultraconserved region is set in bold. The arrow indicates the introduced mutation M2 (dark grey). ( B ) Secondary structure resulting from mutation M1 (substituted positions highlighted in light grey). 2 METHODS 2.1 Computational prediction of Yfr1 targets For the target prediction, a 400 nt subsequence including 250 nt upstream and 150 nt downstream of the start codon was extracted for all annotated genes of the  Prochlorococcus  MED4 genome [GenBank accession number BX548174 (Rocap  et al. ,  2003 ) using the updated annotation by Kettler  et al.  ( 2007 )]. In total, we obtained 1964 sequences covering the full 5′ untranslated region (5′ UTR) (if not &gt;250 nt) and the beginning of the coding sequence of each gene to search for interactions with Yfr1. Putative interactions with Yfr1 were predicted with I nta RNA based on hybridization energy and accessibility of the interaction sites (Busch  et al. ,  2008 ). The I nta RNA approach also incorporates interaction seeds, i.e. short regions of (nearly) perfect sequence complementarity. Accessibility is defined as the energy required to unfold the region of interaction in each molecule. In the calculation of these unfolding energies, we assumed global folding of Yfr1. In contrast, the mRNA does not fold globally due to helicase activity of the ribosome (Takyar  et al. ,  2005 ). Hence, the mRNA subsequence was locally folded in a 200 nt window with a maximal base pair distance of 100 nt. For each gene, the optimal interaction and up to five suboptimal interactions were computed. In  Prochlorococcus  MED4, the ultraconserved motif 5′-ACUCCUCACAC−3′ covers positions 17–27 of Yfr1 RNA ( Fig. 1 A). This motif was predicted to be single-stranded in the consensus secondary structure of Yfr1 orthologs from 31 cyanobacteria (Voss  et al. ,  2007 ). In order to search for interactions with this motif as seed region, we extended the I nta RNA program by adding optional constraints that allow to fix the seed position to a given interval of the sRNA sequence. For the target search, we defined an interaction seed of eight paired bases and at most one unpaired base within the aforementioned conserved Yfr1 motif (I nta RNA parameters -p 8 -u 1 -f 17,27). To investigate the influence of interaction seeds, the target prediction was additionally conducted without requiring a seed region (I nta RNA parameter -p 2 for at least 2 bp). We also tested a modified energy score that weights the accessibility against the hybridization energy with factor α:
 
where  E hybrid  denotes the hybridization energy of the interaction and ED x  denotes the energy required to make the interaction site accessible in sequence  x . The original I nta RNA scoring does not weight the unfolding energy of the interaction sites, i.e. α=1. In addition to the I nta RNA energy score, the location of the interaction in the mRNA is used as a further criterion to evaluate the quality of prediction. The majority of characterized  trans- encoded sRNAs downregulate their targets by base pairing to the 5′ UTR in the vicinity of the ribosome binding site (RBS) (reviewed in Aiba,  2007 ). Therefore, the predicted target candidates were filtered for interactions that involve the mRNA region from −39 to +19 relative to the start codon, which is the maximal region covered by ribosomes (Hüttenhofer and Noller,  1994 ). The Yfr1-target interactions predicted with fixed seed and full accessibility scoring are provided in  Supplementary Material 1 . Target candidates resulting from each parameter setting are listed in  Supplementary Table 1 . 2.2 Experimental validation of Yfr1 targets 2.2.1 E.coli growth conditions and plasmid constructions E.coli  strain Top10F′ was used for cloning of all target- gfp  fusions in plasmid pXG-10 or of Yfr1 gene in plasmid pZE12- luc . All interaction studies were carried out in  E.coli  strain Top10.  E.coli  cells were grown in Luria–Bertani broth at 37°C in the presence of 100 μg/ml ampicillin and/or 25 μg/ml chloramphenicol. Plasmids used in this work were obtained from Dr Jörg Vogel (MPI, Berlin). Plasmid constructions of the respective 5′ UTRs and of Yfr1 are described in detail in Urban and Vogel ( 2007 ). In brief, full-length 5′ UTRs and the first coding residues of the targets of interest were ligated in pXG-10 plasmid using two complementary oligonucleotides with an Mph1103I restriction site at the 5′ terminus and an NheI restriction site at the 3′ terminus, which were annealed to each other prior to ligation. In the case of the 5′ UTR of PMM0494, a PCR-generated fragment (containing an Mph1103I and an NheI restriction site) was digested and ligated into Mph1103I- and NheI-digested pXG-10 plasmid. The Yfr1 gene was amplified by PCR containing an XbaI restriction site and ligated in pZE12- luc  plasmid containing an XbaI restriction site for insertion. Yfr1 mutants (Yfr1 M1: CC at positions 20 and 21 substituted by GG leading to the formation of a stem–loop structure in the normally unpaired region, Yfr1 M2: UCCU at positions 19–22 substituted by AAAA without changing the structure, see  Figure 1 ) were generated by annealing two complementary oligonucleotides containing an XbaI restriction site. The complete list of oligonucleotides used for cloning is provided in  Supplementary Table 2 . 2.2.2 Analysis of Yfr1-mediated target regulation We tested potential interactions of Yfr1 sRNA with the 5′ UTRs of the putative targets PMM0050 ( argJ , bifunctional ornithine acetyltransferase/ N -acetylglutamate synthase), PMM0494 ( ppa , putative inorganic pyrophosphatase), PMM0538 (unknown function), PMM1119 ( som , outer membrane protein), PMM1121 ( som , outer membrane protein) or PMM1697 (type II alternative σ factor). For fluorescence measurement, overnight cultures were grown in 96-well plates (Nunc, Roskilde, Denmark) at 37°C with gentle agitation in an air humidity saturated environment to prevent evaporation. Cells were diluted 1:100, fixed in 1% Histofix (Roth, Karlsruhe, Germany) and kept in darkness until measurements were conducted. Single cell fluorescence was determined by flow cytometry with the flow cytometer LSR II (BD Bioscience, New Jersey, USA). Cell fluorescence was measured with an excitation wavelength of 488nm and the emission was detected at 513/17nm. Target- gfp  fusions as well as control plasmids pXG-0 (negative control) and pXG-1 (positive control) were tested in the presence of a nonsense RNA and Yfr1 sRNA, respectively. The mean fluorescence per plasmid combination was calculated from 10 000 events (cells) of six individual clones. 3 RESULTS AND DISCUSSION 3.1 Experimental validation of predicted Yfr1 targets Table 1  lists the 10 highest scoring candidates of the Yfr1 target prediction. Out of these, we experimentally tested the six monocistronic target candidates with known transcriptional start sites and interaction sites predicted in the 5′ UTR or at the start codon. The predicted interactions for targets with a GFP fluorescence signal above background (indicating measurable expression) are shown in  Figure 2 . Two of the six tested target candidates are translationally repressed by Yfr1, as shown by a reduced GFP fluorescence signal ( Fig. 3 ). The first clusters of the bar chart in  Figure 3  constitute the negative controls ( E.coli  strain Top10 without plasmid or with plasmid pXG-0 devoid of  gfp , respectively) and the positive control ( E.coli  strain Top10 with plasmid pXG-1 carrying  gfp ). The remaining clusters represent the 5′ UTR- gfp  fusions for the targets of interest. Each  gfp  fusion plasmid was tested in the presence of a second plasmid containing a nonsense RNA (white bars), Yfr1 sRNA (red bars) and the two mutated Yfr1 sRNAs M1 and M2 (light and dark blue bars) ( Fig. 3 ).
 Table 1. Highest scoring Yfr1 target candidates and their ranks under different I nta RNA parameter settings Target Fixed seed No seed α 1 0.5 0 1 0.5 0 PMM1119 ( som ) 1 4 22 1 11 173 PMM0494 ( ppa ) 2 3 25 2 10 195 PMM1121 ( som ) 3 6 32 4 32 184 PMM1697 4 7 12 12 19 10 PMED4_09051 5 1 8 52 2 46 PMM0538 6 16 14 26 84 91 PMM0130 7 13 72 13 60 238 PMM1021 8 22 69 5 51 830 PMM1405 9 12 26 37 61 115 PMM0050 ( argJ ) 10 11 7 40 8 11 Only interactions at RBS [−39 to +19 relative to start codon, see Hüttenhofer and Noller ( 1994 )] were considered. All ranks are given according to I nta RNA energy score. α is a weighting factor for the accessibility in the energy score. 
 Fig. 2. Interactions between Yfr1 and target mRNA 5′ UTRs predicted by I nta RNA. Additionally, a putative interaction between Yfr1 and the positive control pXG-1 is presented. The 5′ ends of the mRNAs were experimentally mapped by deep sequencing (C.Steglich, unpublished data). Yfr1 RNA and coding sequences of the mRNAs are set in bold. Start codons are underlined. Shine-Dalgarno (SD) sequences are marked with a box. Asterisks denote start codons that are presumably misannotated in the  Prochlorococcus  MED4 genome sequence. The arrows indicate mutations M1 (light grey) and M2 (dark grey) introduced in Yfr1. 
 Fig. 3. Experimental validation of Yfr1 target predictions. The relative decrease in GFP fluorescence as determined by flow cytometry indicates the strength of Yfr1-mediated regulation. The dashed line indicates background fluorescence (i.e. cellular autofluorescence), determined as the mean GFP signal of the negative controls. Fold changes of reduced GFP signal for PMM1119 (3.0-fold), PMM1121 (2.7-fold) and pXG1 (1.5-fold) were calculated after background subtraction from absolute fluorescence values (Urban and Vogel,  2007 ). In the presence of the nonsense RNA, no regulation of the 5′ UTR- gfp  fusions by an interaction is expected ( Fig. 3 , white bars), and the fluorescence measured here represents the 5′ UTR-specific translation efficiency. The different GFP fluorescence intensities can be explained by differences in the affinities of the ribosomes for the translation initiation region. The strongest inhibition by Yfr1 was detected for the 5′ UTRs of the two  som  genes PMM1119 and PMM1121 (3.0- and 2.7-fold reduced GFP signal, red bars in  Fig. 3 ). No change in GFP fluorescence was observed for PMM1697 and PMM0538 5′ UTRs in the presence of Yfr1. For PMM0494 and PMM0050, no fluorescence above the background level (dashed line in  Fig. 3 ) could be detected for any tested plasmid combination. Translation inhibition of the two  som s was abolished by the introduction of a mutation in the conserved Yfr1 motif exchanging CC by GG (Yfr1 M1, light blue bars in  Fig. 3 ). These two substitutions involve the region predicted to base pair with the RBS of the two  som  mRNAs. Furthermore, mutation M1 led to a structural change by introducing a stem–loop in the single-stranded region of wild-type Yfr1 ( Fig. 1 B). Thus, mutation M1 results in both a sequential and structural change at the interaction site. To test whether the destruction of the antisense complementarity alone (without structural change) abolishes regulation by Yfr1, we constructed another Yfr1 mutant. In the Yfr1 mutant M2, nucleotides UCCU were substituted by AAAA without changing the secondary structure of wild-type Yfr1 ( Fig. 1 A). Again, translation of PMM1119 and PMM1121 was restored ( Fig. 3 , dark blue bars). These results indicate that Yfr1 inhibits translation of the two  som  mRNAs by direct base pairing at the RBS. Furthermore, the results strongly indicate that both sequence and structure are important for Yfr1 regulation. Surprisingly, we also observed a 1.5-fold reduction in GFP fluorescence for the positive control pXG-1 in the presence of Yfr1 and restored translation under the control of Yfr1 M1 and M2. However, the strong RBS in the 5′ UTR of  gfp  in pXG-1 (Urban and Vogel,  2007 ) shows a perfect complementarity to part of the conserved Yfr1 motif. Thus, Yfr1 can form a perfect 6 nt duplex with the 5′ UTR ( Fig. 2 ), which can explain the observation of a reduction in translation. 3.2 Influence of seed requirement and accessibility on Yfr1 target prediction The prediction of sRNA targets with I nta RNA is based on two assumptions: (i) a seed region is required to initiate the interaction [in analogy to the 5′ seed region of miRNAs (Bartel,  2009 )] and (ii) the accessibility of the interaction sites is important for target recognition. A previous study on a dataset of 18 different sRNA–mRNA interactions presented evidence that the incorporation of these two requirements improves the prediction quality of I nta RNA (Busch  et al. ,  2008 ). Here, we investigated the importance of accessibility and of a seed region in a practical application, namely the identification of new targets for the Yfr1 sRNA. Therefore, we computed lists of putative targets without enforcing a seed region and with enforcing a seed at the conserved Yfr1 motif. When requiring the fixed seed position, we obtained a short list of only 29 target candidates with the experimentally validated Yfr1 targets PMM1119 and PMM1121 ranked at positions 1 and 3, respectively ( Table 1 ). Without the seed requirement, 1418 target candidates were obtained with the two true positives ranked at positions 1 and 4. Even without using a seed constraint, the interactions predicted for the true positives include the conserved single-stranded region of Yfr1. Thus, the combination of complementarity and accessibility alone resulted in interactions with an implicit seed. In addition to the effect of a seed requirement, we studied the influence of accessibility on the Yfr1 target prediction. In the original I nta RNA scoring, hybridization energy and interaction site accessibilities contribute equally to the energy score. Here, we tested a modified energy score, where the interaction site accessibility of both sequences was weighted by factor α with the values 0, 0.5 and 1. For both seed requirements studied, the true positives PMM1119 and PMM1121 were ranked best with the original scoring ( Table 1 ). One interesting observation was that in the case of Yfr1, a full weighting of the interaction site accessibility, i.e. α=1, was required for a correct target site prediction. When both the seed region and accessibility were neglected, the two verified Yfr1 targets were not found within the top 150 predictions. When the seed position was fixed to the conserved region but accessibility was not included in the scoring, the validated targets were ranked at positions 22 and 32. However, in this case, predicted interactions involved almost the entire Yfr1 sequence (data not shown). This observation is consistent with the findings of Tjaden  et al.  ( 2006 ) and Busch  et al.  ( 2008 ), who showed that an energy model based solely on hybridization energy tends to maximize the length of hybridization, resulting in a small fraction of correctly predicted base pairs (i.e. low positive predictive value). 4 CONCLUSIONS In this study, we show that Yfr1 sRNA modulates the translation of two high-scoring predicted targets by an antisense interaction. Both target mRNAs code for outer membrane proteins (Hansel  et al. ,  1998 ). This class of proteins constitutes a major functional class that is regulated by bacterial sRNAs in  E.coli  and  Salmonella  (Waters and Storz,  2009 ). The result was surprising as, until now, no highly abundant sRNAs have been shown to act via base pair interaction. However, both mRNA targets identified herein are also highly abundant [among the 10 most expressed mRNAs and with long half-lives of about 30 min (C.Steglich, unpublished data)], which may require a high copy number of Yfr1 for efficient regulation. Furthermore, an mRNA with a long half-life can be regulated more efficiently by translational control than by transcriptional control. Additionally, we assessed the influence of seed regions and interaction site accessibility on the prediction quality of Yfr1 targets. As with the  Salmonella  sRNAs GcvB and RybB, Yfr1 contains a conserved single-stranded region, which seems to constitute a perfect interaction seed. When requiring this region as seed for the target prediction, the number of putative Yfr1 targets was remarkably smaller without seed requirement (29 versus 1418 candidates), although the two true positives were under the highest ranking candidates in both settings. When neglecting both accessibility and a seed region, the true Yfr1 targets could not be found amongst the top 150 predictions. In conclusion, the combination of computational and experimental methods, as presented in this study, proved to be an appropriate approach for the identification of sRNA targets in organisms where genetic manipulation constitutes a great challenge. Funding : German Research Foundation DFG Priority Program SPP1258 Sensory and Regulatory RNAs in Prokaryotes (grant number BA 2168/2-1 to R.B., Ste 1119/2-1 to C.S.]; German Federal Ministry of Education and Research FRISYS - Freiburg Initiative for Systems Biology (grant number 0313921 to R.B.). Conflict of Interest : none declared. 
          Supplementary Material 
          
             
                [Supplementary Data] 
             
             
             
             
             
          
       </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Optimization strategies for fast detection of positive selection on phylogenetic trees</Title>
    <Doi>10.1093/bioinformatics/btt760</Doi>
    <Authors>Valle Mario, Schabauer Hannes, Pacher Christoph, Stockinger Heinz, Stamatakis Alexandros, Robinson-Rechavi Marc, Salamin Nicolas</Authors>
    <Abstract>Motivation: The detection of positive selection is widely used to study gene and genome evolution, but its application remains limited by the high computational cost of existing implementations. We present a series of computational optimizations for more efficient estimation of the likelihood function on large-scale phylogenetic problems. We illustrate our approach using the branch-site model of codon evolution.</Abstract>
    <Body>1 INTRODUCTION The development of evolutionary models has a long tradition in phylogenetics, and recent advances have enhanced our understanding of the molecular mechanisms involved. At the heart of these advances is the democratization of the use of the likelihood framework, which was made possible by algorithmic developments ( Felsenstein, 1981 ) and the wide availability of powerful computing platforms. The surge of genomic data is, however, pushing the limits of current implementations [e.g. ( Rannala and Yang, 2008 )] and demands for the developments of better and more efficient ways to compute the phylogenetic likelihood function (PLF). The development of codon models is a good example to illustrate these current challenges and the benefits that can be reached by improving the efficiency of current likelihood calculations ( Gil  et al. , 2013 ). There are clear advantages to use codon models in phylogenetics ( Seo and Kishino, 2008 ), but these are currently not widely used because of the large computational burdens involved ( Anisimova and Kosiol, 2009 ). Further, the detection of positive selection has been facilitated by the development of new codon models. However, their application to genome-scale data comprising a large number of species, or individuals in the case of population genomic studies, remains challenging. Thus, there exists an urgent need for improved implementations and novel optimization techniques to analyse emerging genomic datasets ( Lemey  et al. , 2012 ;  Murrell  et al. , 2012 ;  Schabauer  et al. , 2012 ). The prevalent approach for detecting positive selection in protein-coding genes is to use Markov models of codon substitution to estimate the ratio of non-synonymous to synonymous changes along the branches of a phylogenetic tree ( Yang, 2006 ). The branch-site model (BSM) [ Yang, 2006  (Section 8.4);  Zhang  et al. , 2005 ] allows to detect positive selection that affects a subset of codon sites for a subset of branches in a phylogenetic tree. This model is particularly useful to perform interspecific comparisons and is probably the most widely used approach for this specific purpose. The test compares a model that assumes positive selection on one branch or on a set of a priori specified branches (hypothesis  H 1 ) with a null model that does not incorporate positive selection (hypothesis  H 0 ). If the test is significant, the Bayes Empirical Bayes (BEB) method is used to compute the posterior probability of each particular codon to evolve under positive selection along the specified branches ( Yang  et al. , 2005 ). In CodeML, the test is usually applied iteratively and independently to each branch of a given phylogenetic tree ( Anisimova and Yang, 2007 ;  Studer  et al. , 2008 ). This approach is compute bound, and although alternatives have recently been proposed, the limiting factor of such analyses still lies with the repeated calls to compute the PLF. For example, the estimation of positive selection on a large genomic vertebrate dataset ( Proux  et al. , 2009 ) shows the enormous computational requirements of such analyses [approx. 100 CPU years for  each  release of the Selectome database ( Kraut  et al. , 2010 )]. As a consequence, large gene trees, comprising more than 100 sequences, are usually excluded and faster implementations of the BSM are urgently needed. This clearly illustrates the need to further optimize current software and to develop more efficient computational approaches for maximum likelihood inference on phylogenetic trees. Several recent studies introduced techniques for efficiently computing positive selection on the branches of a phylogenetic tree. One idea is to use stochastic mapping to count substitutions along the branches of a tree and thereby derive dN/dS ratios ( Dutheil  et al. , 2012 ;  Lemey  et al. , 2012 ). While this approach is fast, it is computationally distinct. Alternatively, new models have been proposed to avoid the likelihood ratio test (LRT) estimation of positive selection for all branches of the tree. Instead, branch assignments are considered as a random effect within a mixed effect framework ( Murrell  et al. , 2012 ). Their model notably differs from the BSM ( Zhang  et al. , 2005 ) in that putative positive selection is not optimized on a priori defined branches, but over a subset of branches which are determined by the software. This technique reduces the computational cost of the test, but the accuracy and robustness of this new model is not yet fully characterized. Moreover, the authors introduced solutions for parallelizing BSM computations, but the parallel approach is not discussed in their article. The bottleneck in efficiency of phylogenetic software is commonly the PLF, as the majority of runtime is spent here. In ( Stamatakis, 2011 , p.2), the PLF is reported to consume &gt;95% of total execution time in maximum likelihood and Bayesian tools for phylogenetic tree reconstruction. Although this was estimated when searching for the best tree topology, which is a key component of phylogenetic computations but not the focus of this article, the PLF is still the core element in  all  phylogenetic applications using maximum likelihood. All these areas would therefore benefit from an optimized PLF. Recent discussions have proposed to use data augmentation strategies to speed up the likelihood calculations by using heuristics to simplify the estimation of the conditional vectors at each node ( Rodrigue and Aris-Brosou, 2011 ). However, there are still opportunities for improving the PLF with respect to sequential efficiency and parallelization techniques. Our main objective is therefore to propose methodological and algorithmic improvements and parallelization strategies to compute the PLF  without  modifying the underlying evolutionary model. Our optimizations and parallelizations yield substantial speedups in the likelihood computations. Hence, we can apply the BSM to large trees of several hundreds of sequences and obtain results in feasible times. These computational optimizations are thus of broad applicability to further likelihood-based phylogenetic software, including but not limited to nucleotide- and amino acid-based phylogenetic analyses in both the maximum likelihood and Bayesian frameworks ( Nielsen, 2005 ). 1.1 Number of elementary tree operations In the BSM framework, four site classes 0, 1, 2a and 2b are applied to model combinations of purifying selection, neutral evolution, and positive selection on foreground and background branches. When computing hypotheses  H 0  and  H 1 , each site class has its distinct proportion according to its contribution to the overall likelihood (cf. the  supplementary material  for an introduction to the BSM). These proportions only depend on the two parameters  p 0  and  p 1 ; each site class has a specific ω value for its selective pressure in the foreground and in the background.   is in the interval (0,1),   and  either 
  (foreground for  H 1 )  or 
  (foreground for  H 0 ).   corresponds to  , respectively. Computing the likelihood requires computing the transition probabilities for a given branch length  t  by computing the matrix exponential  , where  Q  is the instantaneous substitution rate matrix,  S  is the symmetric codon substitution matrix and Π is the diagonal matrix of codon frequencies. The resulting probability matrix  P t  is used to update the corresponding conditional probability vector (CPV)  w , that is,  . Each CPV models the site-wise transition between 61 codon states (universal genetic code) along each branch of the phylogenetic tree. This operation is applied to all sites of the multiple sequence alignment (MSA) and to all nodes of the tree by means of a post-order tree traversal. The CPU-intensive computation of the CPV entails the following three computational kernels that operate on real dense matrices (similar to SlimCodeML, see  Section 2.1.2 ): (i) eigendecomposition of a symmetric matrix [see, e.g. ( Bai  et al. , 2000 )], (ii) multiplication of a matrix by its transpose (resulting in a symmetric matrix) and (iii) multiplication of a symmetric matrix by a vector. 1.1.1 How many decompositions? To compute  e Qt  we need to decompose  Q  for each distinct combination of parameters κ (transition to transversion rate),   and ω. The   are constant over site classes and parameter optimization steps; κ may change at each parameter optimization step (but is constant over site classes); ω varies among optimization steps  and  site classes. For each distinct value of ω,  Q  is distinct and therefore needs to be decomposed separately. There are three distinct ω values over all site classes; hence, we need to decompose three  Q  matrices in the first parameter optimization step. For subsequent steps,   remains constant, but  Q 1  may change because of a new κ value. The total number of  Q  decompositions does not depend on the number of branches in the tree nor on the number of sites in the MSA. In the general case, the number of  Q  matrices depends on the number of unique substitution matrices in the model, which can be large in mixture models [e.g. ( Lartillot and Philippe, 2004 ;  Venditti  et al. , 2008 )]. With respect to other evolutionary models, similar optimizations may be applicable. 1.1.2 How many matrix–matrix multiplications? P t  has to be computed for each combination of  Q  and  t . For our case of binary trees, the number of branches in the phylogeny equals   where  n  is the number of extant taxa. For each distinct  Q , branches have to be computed separately. The BSM applies  Q 0  and  Q 1  to each branch, but  Q 2  only to foreground branches. In other words,  P t  has to be computed for all branches using  Q 0  and  Q 1  (site classes 0 and 1),  and  in addition on the foreground branch(es) by using  Q 2  (site classes 2a and 2b). Therefore, we need to compute  P t 
  times for  m  branches in the phylogeny and  l  foreground branches; this yields   branches when using a single foreground branch. Overall, we need to compute 17 distinct  P  matrices in our example 1. This matrix–matrix multiplication is also applied in further evolutionary models based on substitution matrices. 1.1.3 How many matrix–vector computations? In a straightforward approach, each CPV is computed along each branch for all sites and all site classes. In our example this makes   CPV computations. If a CPV connected to a leaf is computed on ‘clean’ data [no ambiguity symbols in MSA ( Comnish-Bowden, 1985 )], the CPV at the leaf only contains a single 1 (0 elsewhere). In this case, computing the resulting CPV simplifies to selecting the corresponding column of the P matrix. In the general case, an upper limit of the number of involved matrix–vector multiplications per site class is the number of branches in the phylogeny × the number of sites in the MSA. Certainly, this number can be decreased depending on similarities in the codons as discussed in  Section 2.1.1  (‘subtrees reuse’). Likewise, this step is important to all other evolutionary models based on substitution matrices. Further computational savings are possible. In this context, we refer to a ‘subtree’ as a connected part of the phylogeny where at least one node is a leaf. Whenever a particular branch of a single site applies the same  P  and all other CPVs of its subtree match, the particular CPV has a ‘twin’ in another site class and needs to be computed only once. In  Figure 1 , such matching CPVs are identified by matching indexes. For example, CPV23 appears in site class 1 and in site class 2b, as also CPV20 and CPV21 have twins, and they pairwise apply matching  P  matrices (here, all based on  Q 1 ). These redundancies are caused by matching   values for site classes 0 and 2a and by matching   values for site classes 1 and 2b. In our example, this means that only 40 out of 64 (62.5%) CPVs have distinct values and will hence have to be computed. CPVs are computed recursively via a postorder traversal propagating from the leaves towards the root ( Felsenstein, 1981 ). Hence, for the BSM in general, the number of distinct CPVs depends on the location of the foreground branch in the tree (the closer to the root, the less CPV computations are required).
 Fig. 1. Analysis on how many elementary subtree computations are necessary in the branch-site model; CPV m  correspond to  m  distinct conditional probability vectors, where matching  m  need to be computed only once; Q  identify three distinct Q matrices for distinct   values 2 IMPROVEMENTS Here we discuss optimization techniques that we propose. Note that we have not added any heuristics, and each of the following improvements is supposed to be beneficial independent of the number of species and independent of the number of alignment sites. Specific implementation issues are described along with each optimization technique. 2.1 Sequential improvements 2.1.1 Subtrees reuse The per-site likelihoods for a MSA are independent of each other and can thus be computed in an arbitrary order. If two or more sites of the MSA are identical, it suffices to only compute the logarithmic likelihood (lnL) on one site and multiply it by the number of identical sites to obtain the total lnL. This technique is used in most likelihood-based software, but there are further redundant computations caused by re-occurring patterns in the MSA. In each subtree, there is a potential to economize CPV computations for different sites of the MSA. If the same state appears at two or more sites of a sequence, all occurrences yield identical CPVs at the particular leaf. If the patterns of the sub-alignment induced by a subtree match are identical for two or more sites, the corresponding CPVs for the two sites are also identical. However, identical patterns in the sub-alignments induced by a subtree need to be identified first. The identification of such identical patterns in sub-alignments can be done, e.g. by searching (i) sequentially or (ii) using a symbol table ( Sedgewick and Wayne, 2011 , p.361). In the latter case, the  key  is the index of the CPV within the tree, and the  value  associated with the key is its CPV. In the straightforward approach (i), there are no costs on storing values, but up to  m  – 1 lookups for a matching subpattern, where  m  is the length of the MSA. For huge MSAs, it may be advantageous to implement the second approach, where the additional cost for storing or linking site patterns is compensated by a faster lookup. In FastCodeML, we identify reusable subtree patterns in a preprocessing step and tag each node with the codon sequence identified by the subtree rooted in this node. Subsequently, a lookup of these tags for all sites with identical subtrees is done. Once identified, the CPV that can be re-used is linked via a pointer in the reusing tree, that is, this saves the costs of computing this particular CPV. The unused subtree can be freed to reduce memory consumption. In the example of  Figure 2 , computing the two CPVs incident to two leaves in box ① and the CPV at  ②  are redundant, because both codon sites feature an identical subtree: all involved CPVs match. Thus three CPV computations can be saved.
 Fig. 2. Subtrees reuse strategy depicted for two (not necessarily neighboring) sites in the MSA; in ( a ) subtree (1) contains identical codons for both sites; consequently, in ( b ) the CPVs for both sites are identical and need to be computed only once (dotted line) Related techniques for extending pattern detection and re-use in the MSA to the subtree level have already been proposed ( Izquierdo-Carrasco  et al. , 2011 ;  Stamatakis  et al. , 2002 ;  Sumner and Charleston, 2010 ). However, they focus on detecting patterns and avoiding redundant likelihood computations on trees whose topologies change in the course of ML tree search. For dynamically changing trees, a trade-off between the pattern detection and memory storage costs and the amount of saved computations needs to be achieved. To reduce the cost of pattern detection, the initial implementation of the Subtree Equality Vector (SEV) technique ( Stamatakis  et al. , 2002 ) only considered subtree patterns that contained a single identical character. The book keeping was subsequently further simplified to sites consisting entirely of gaps ( Izquierdo-Carrasco  et al. , 2011 ). In  Kosakovsky Pond and Muse (2004) , the authors suggest to sort nucleotide-based MSAs by site similarity to avoid redundant computations. This approach minimizes memory consumption, as only a subset of sites needs to be kept in memory. However, this incurs additional costs for rearranging the sites in order to maximize the number of lookups from neighboring sites. The memory consumption for our application scenario (Selectome database updates) does not represent a limiting factor. Hence, all CPVs can be kept in memory, avoiding the expensive reordering of sites. However, especially for memory-intensive approaches, it may be more effective to keep only a subset of all CPVs in memory and consider site sorting. 2.1.2 New matrix exponential and CPV computation In  Schabauer  et al.  (2012) , we transformed the problem of computing the matrix exponential of non-symmetric  Qt  into a symmetric problem as follows: we define the symmetric matrix   and compute its eigendecomposition  . By introducing  , the matrix exponential of  Qt  becomes  . An additional modification transforms the final asymmetric matrix–vector multiplication   into a symmetric matrix–vector product:
 (1) 
 (2) 
 Note that   is by construction a symmetric matrix, whereas   is generally asymmetric. The advantage of this modification is that the symmetry reduces the number of necessary matrix memory accesses by approx. 50% ( Golub and Van Loan, 2013 , p.18). This technique has been implemented in FastCodeML. 2.1.3 LRT optimization When optimizing parameter values for H 0  and H 1  one after the other, one can save on parameter optimization steps. Each step in the parameter optimization procedure improves the associated lnL of the tree until convergence has been reached. In this discussion, the optimizer may modify all parameter values at each single step. One can either (i) optimize H 0  first with high accuracy and iteratively improve H 1  afterwards: once   becomes larger than  , the parameter optimization for H 1  can be stopped because the LRT is already significant. This potentially saves optimization steps for H 1 . Or we can (ii) optimize H 1  first, then proceed analogously: the parameters of H 0  are optimized until   becomes smaller than  . In general, a significant LRT (i.e. detecting positive selection) is a relatively rare event ( Kosiol  et al. , 2008 ;  Studer  et al. , 2008 ). Strategy (i) saves optimization steps if positive selection occurs; strategy (ii) saves optimization steps if not. Consequently, without prior knowledge of the frequency of occurrence of positive selection in the MSA at hand, strategy (ii) (implemented in FastCodeML) will yield larger savings. If the LRT is significant, a BEB is applied to identify the sites under positive selection. Otherwise, FastCodeML does not execute the BEB, in contrast to CodeML. In the general case, this optimization is applicable if different models are compared, where each of them is optimized iteratively. 2.2 Parallelization While the parallelization of ML-based nucleotide- protein- and codon models has already been addressed ( Stamatakis, 2011 ) (e.g. RAxML, IQPNNI, HyPhy), it has mostly been in the context of tree topology optimization, and not for the likelihood itself. The main challenge in parallelizing ML-based phylogeny computations comes from the tree structure that leads to an irregular domain decomposition ( Tomko, 1995 ). An efficient parallelization of the BSM is even more challenging due to its site classes and dependencies in between. Our implementation optimizes simultaneously all the parameters. The maximizer acts as an impenetrable boundary for parallelization, and we distinguish parallelization ‘above’ (coarse-grain) and ‘within’ (fine-grain) this boundary (cf.  supplementary material ,  Fig. 1 ). 2.2.1 Coarse-grain parallelization: Gene-wise  parallelization . Because distinct genes typically have different evolutionary histories with distinct branch lengths and evolutionary parameters, phylogenies for genes are commonly estimated independently for each gene. Consequently, single genes cannot be concatenated into multi-gene alignments to attain high scalability by means of a fine-grain parallelization of the likelihood function [see, e.g. ( Stamatakis and Ott, 2009 )]. Here we test for selection independently (gene-wise), these analyses can be carried out in an embarrassingly parallel way [see, e.g. ( Foster, 1995 , p.21)]. Foreground branch  parallelization .  A further BSM parallelization option is the simultaneous analysis of distinct foreground branches. This is possible because we want to test for positive selection on each branch of a given phylogeny. Thus, the   tests for positive selection, where  n  is the number of taxa, can be conducted in parallel by duplicating the tree data structure and CPVs. Under this parallelization strategy, a dedicated master process broadcasts all model parameters, tree topologies and branch lengths to all worker nodes. The workers then conduct the tests independently of each other on different foreground branches of the same tree. Afterwards, the worker nodes return the estimated parameter values and the lnL scores to the master process. We implemented this approach using MPI ( Message Passing Interface Forum, 1994 ). The foreground-branch based parallelization can be combined with a site-wise fine-grain parallelization of the per-tree likelihood computations ( Section 2.2.2 ) into a  hybrid  parallelization scheme. Hypotheses  parallelization .  Note that for each foreground branch, hypotheses  H 0  and  H 1  can be computed independently and simultaneously, thus increasing the degree of parallelism. However, the simultaneous computation of  H 0  and  H 1  prevents us from using the aforementioned LRT optimization ( Section 2.1.3 ). Although the LRT and the subsequent BEB must be computed after  H 0  and  H 1 , they can be parallelized between different foreground branch computations. This parallelization strategy can be applied whenever two evolutionary models are compared. It is implemented in FastCodeML via the same master-worker scheme. 2.2.2 Fine-grain parallelization: Site-wise  parallelization . A common way to parallelize likelihood computations on shared memory architectures is by parallelizing over the sites of the MSA. This site-wise parallelization can be implemented using OpenMP or POSIX Threads. MPI-based implementations exist but focus on large MSAs that are outside the scope of this article. However, while our subtree patterns re-use scheme ( Section 2.1.1 ) reduces the number of computations along the branches, it poses a load balance challenge: (i) a particular CPV for a site can only be computed after the site whose results it reuses (i.e. data dependency) has been computed and (ii) a site that reuses a previously computed CPV exhibits a smaller workload which leads to load imbalance. The load balancing strategy we use in FastCodeML subdivides the alignment sites into groups such that each group exclusively reuses subtrees from the previous groups ( Fig. 3 ). Each group is assigned a rank value starting from zero. CPVs from groups with lower rank values can potentially be reused. The first group does not reuse any subtree. All subtrees of a group can be parallelized, because they are independent of each other. The groups are then computed sequentially in order of rank. To balance the load for each group, subtrees can be moved to higher ranked groups. To increase parallelism, the trees of each group are replicated for each site class that should be computed until no lower rank group depends on it. The parallelization inside each group has been implemented using OpenMP.
 Fig. 3. Load balancing strategy: the sites of the tree are grouped so that each group depends only on groups at its left (continuous lines). A tree can be moved to a group to its right (dashed line) only if it has no dependencies from other trees in intermediate groups This site-wise parallelization strategy including load balancing can likewise be applied to nucleotide- or protein-based MSAs. The parallel performance may vary due to different computational load per site. 2.3 Implementation FastCodeML has been implemented from scratch (except for the BEB that was largely taken from the CodeML codebase) in ISO C++ 2003 utilizing BLAS and LAPACK for linear algebra operations, and Spirit ( http://www.boost.org/doc/libs/release/libs/spirit/ ) for tree parsing. We use the parameter optimization codebase of CodeML. 3 EVALUATION We measure median runtimes of 10 individual runs for each evaluation (three on the large scale analysis in  Section 3.5 ). Speedup values are determined by  , where  T 1  is the runtime (elapsed time, wall-clock time) of the reference execution and  T 2  the runtime of the execution to be evaluated on the same dataset; for a  relative  speedup  T 1  and  T 2  denominate runtimes of the same executable, while for the  absolute  speedup  T 1  is strictly the original CodeML. Initial branch lengths were read from file, while model parameters are initialized randomly. Memory consumption of CodeML, SlimCodeML and FastCodeML for these datasets is not a limiting factor and therefore not performance critical. Although a single executable can be used for all subsequent evaluations, we built sequential, OpenMP parallelized, MPI parallelized and hybrid executables separately. A summary of the platforms used can be found in the  supplementary material . 3.1 Datasets Table 1  contains the six datasets we used for evaluation. With respect to the Selectome database, these empirical datasets are representative for the cases: (D1) small number of species/medium sequence length; (D2) small number of species/large sequence length; (D3) medium number of species/small sequence length; (D4) large number of species/short sequence length; (D5) a simulated dataset with positive selection based on dataset D1 (using PAML’s evolver choosing ‘evolverNSbranchsites’ for the BSM with  ). Finally, we analyse in D6 a very large rbcL dataset ( Grass Phylogeny Working Group II, 2012 ) which cannot be processed in a feasible time by CodeML.
 Table 1. Test datasets of our analyses; remaining branches is the percentage of non-redundant branches for the given data over all sites of the alignment; dataset D5 is generated based on ENSGT00390000016702.Primates.1 with  Abbr. Full name No. of species No. of branches Remaining branches [%] Length (codons) D1 ENSGT00390000016702.Primates.1 7 12 37.74 299 D2 ENSGT00530000063518.Primates.1 95 188 75.49 39 D3 ENSGT00550000073950.Euteleostomi.7 25 48 56.31 67 D4 ENSGT00580000081590.Primates.1 6 10 20.92 5004 D5 Generated by evolver (PAML) 7 12 38.04 282 D6 Grass_rbcL 506 1242 19.54 414 3.2 Accuracy In  Table 2  we analyse the accuracy of FastCodeml with respect to lnLs and LRT scores. We use SlimCodeML as a proxy for good accuracy, as it gives very similar results as CodeML ( Schabauer  et al. , 2012 ), which is the established gold standard. We note that the accuracy of computed lnLs is much higher than typically required to discriminate between significant and insignificant LRTs.
 Table 2. Accuracy of SlimCodeML and FastCodeml on Macpro;   is the absolute difference of lnLs comparing either SlimCodeML or FastCodeML with CodeML on  H 0  ( H 1 ), respectively Dataset LRT pos. selection SlimCode versus CodeML D1 no (✓) D2 no (✓) D3 no (✓) D4 no (✓) D5 10.4 site 239 (✓) FastCodeML versus CodeML D1 no (✓) D2 no (✓) D3 no (✓) D4 no (✓) D5 10.4 site 239 (✓) Note : ‘✓’ indicates agreement of the computed result with CodeML. 3.3 Sequential runtimes Sequential speedups of FastCodeML (single-threaded) versus CodeML and SlimCodeML for five datasets ( H 0  and  H 1 , respectively) on platform Macpro (cf.  supplementary material ) are depicted in  Figure 4 ; here, FastCodeML includes the following improvements: faster matrix exponentiation ( Section 2.1.2 ) and subtrees reuse ( Section 2.1.1 ). LRT optimization ( Section 2.1.3 ) is not considered, as either  H 0  or  H 1  is computed per run. We observe speedups of FastCodeML versus CodeML ranging from 2.6 to 5.8. The sequential FastCodeML is significantly faster than both CodeML and SlimCodeML on all five datasets.
 Fig. 4. Sequential speedups of FastCodeML in comparison with CodeML and SlimCodeML on Macpro for  H 0  and  H 1 , respectively 3.4 Parallel runtimes 3.4.1 Site-wise parallelization Figure 5  shows the scaling of FastCodeML on a site-wise (OpenMP based) parallelization strategy for dataset D2 on 1–12 CPU cores (one thread per core); we observe relative speedups comparing FastCodeML in   versus 1 threads, reaching 11.1 for 12 cores without subtrees reuse, and speedups up to 7.6 for 12 cores with subtrees reuse. These relative speedups correspond to absolute speedups versus CodeML of up to 23.4 without subtrees reuse, and speedups up to 19.9 with subtrees reuse. While scaling of subtrees reuse is slightly worse than without subtrees reuse, absolute runtimes on this particular platform and dataset suggest to enable subtrees reuse on 1–11 cores but not on 12. The worse scaling of subtrees reuse is presumably caused by load imbalance. Due to differences in the sequential performance of subtrees reuse, we also expect the performance of parallel subtrees reuse to vary with different datasets. In general, the effectiveness of parallel subtrees reuse is a trade-off between the number of redundant branches versus the data dependencies introduced.
 Fig. 5. Parallel site-wise relative ( top ) and absolute ( bottom ) speedups of FastCodeML on Castor on dataset D2 for  H 1 3.4.2 Foreground branch-based parallelization Figure 6  depicts the relative scaling of FastCodeML on a foreground-branch based parallelization strategy. The evaluation has been done for dataset D3 on 1–7 worker nodes (single thread per node). Due to the master–worker scheme used, performance gains are observed for two or more worker nodes. The analysis is done for all possible 22 foreground branches, where the runtime for CodeML is measured only on a single foreground branch but multiplied by 22; running CodeML on all foreground branches is expected to consume more than a day. We observe relative speedups of up to 5.9 on 7 worker nodes, which corresponds to absolute speedups from 3.3 to 19.4. In general, the relative speedup for foreground branch-based parallelizations benefits from a high ratio of foreground branches to available nodes, as the workload can more easily be divided into balanced parts.
 Fig. 6. Parallel foreground branch (MPI based) relative speedups of FastCodeML for dataset D3 on Castor for  H 1 ; only a  single  CPU core per node was used 3.4.3 Hybrid parallelization Figure 7  depicts absolute scaling of FastCodeML on a hybrid (foreground branch and site-wise) parallelization strategy implemented using OpenMP and MPI on 1–7 worker nodes, where all 12 CPU cores are used. Corresponding runtimes, relative and absolute speedup values are summarized in  Table 3 . We observe relative speedups up to 6.3 on 7 worker nodes, which corresponds to absolute speedups up to 170.9.
 Fig. 7. Parallel hybrid (OpenMP and MPI based) scaling of FastCodeML for dataset D3 on Castor for  H 1 
 Table 3. Overall parallel performance of FastCodeml versus CodeML on Castor for dataset D3 on all possible foreground branches for  H 1 ; CodeML runtime for absolute speedups is extrapolated from computing a single foreground branch Worker nodes (cores) FastCodeML runtime [s] Rel. speedup Abs. speedup 1 (12) 429 1 27.6 2 (24) 218 2 54.2 3 (36) 151 2.9 78.4 4 (48) 114 3.8 103.7 5 (60) 93 4.7 126.5 6 (72) 81 5.4 145.3 7 (84) 69 6.3 170.9 3.5 Large scale analysis A large scale analysis has been conducted to prove the use of FastCodeML beyond the capabilities of CodeML. In initial tests, we verified that dataset D6 achieves its best runtime performance on platform Castor (cf.  supplementary material ) by using all 12 available cores per node and by reusing subtrees ( Section 2.1.1 ). We analysed D6 for  H 0  and  H 1  running FastCodeML (multi-threading) on 12 CPU cores and determined average runtimes of three test runs. The average runtime of FastCodeML on dataset D6 is 21.9 h for  H 0  and 31.9 h for  H 1 . Due to time restrictions, we evaluated only a single iteration of CodeML for D6 which took 2.2 h on  H 0  (367 iteration steps) and 2.3 h on  H 1  (426 iteration steps) on the same platform. As we apply the same parameter optimization codes, we use the average number of optimization steps of FastCodeML on dataset D6 for the following speedup metric: we extrapolate that CodeML would have finished executing in approximately   h (i.e. ca. 33.6 days) for  H 0  and   h (i.e. ca. 40.8 days) for  H 1 . The estimated speedups comparing the single threaded CodeML with FastCodeML running in 12 threads is thus 36.9 for  H 0  and 30.7 for  H 1 . In this example, the LRT optimization saves 268 optimization steps for  H 1  (63%). 4 CONCLUSIONS We introduced here three sequential code optimizations: an improved matrix exponential, subtrees reuse and LRT optimization. We observed significant speedups versus both CodeML and our previous version SlimCodeML, and the first two optimizations can be used in various likelihood computations in phylogenetics. Moreover, we present a parallelization strategy that uses a fine-grain and a coarse-grain approach. Overall, our improvements allow for testing selection on phylogenetic trees which exceed the possibilities of the original CodeML software; this is crucial to tackle the genomic data avalanche. The discussed improvements are motivated by the branch-site model but can, due to the likelihood framework, be extended to nucleotide- and amino acid-based MSAs as well as Bayesian approaches. We briefly identified such opportunities where applicable, but an extensive discussion is subject to future work. The optimization of the likelihood surface for phylogenetics problems is complex and we have started experimenting with the alternative parameter optimizers available in NLopt ( http://ab-initio.mit.edu/wiki/index.php/NLopt ). It may be interesting to compare different implementations of the Broyden–Fletcher–Goldfard–Shanno (BFGS) optimization method, but a deeper investigation of the global and derivative-free optimizers is needed to better understand the potential solutions to find the maximum likelihood estimator for complex evolutionary models. In a future version the dependencies between nodes could be modelled as a directed acyclic graph and the parallelism be based on a dataflow model ( YarKhan  et al. , 2011 ) to study and potentially further improve parallel performance. Moreover, the site classes could be included into the dependency graph. This way a more fine-grained parallelism could be achieved. Increasing the parallel performance becomes crucial with the trend of more parallelism in future computer platforms ( Dongarra, 2012 ). 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Identification of OBO nonalignments and its implications for OBO enrichment</Title>
    <Doi>10.1093/bioinformatics/btn194</Doi>
    <Authors>Bada Michael, Hunter Lawrence</Authors>
    <Abstract>Motivation: Existing projects that focus on the semiautomatic addition of links between existing terms in the Open Biomedical Ontologies can take advantage of reasoners that can make new inferences between terms that are based on the added formal definitions and that reflect nonalignments between the linked terms. However, these projects require that these definitions be necessary and sufficient, a strong requirement that often does not hold. If such definitions cannot be added, the reasoners cannot point to the nonalignments through the suggestion of new inferences.</Abstract>
    <Body>1 INTRODUCTION Several efforts in recent years have focused on the semiautomatic addition of links between existing terms in the Open Biomedical Ontologies (OBOs) through the creation of formal definitions of these terms using more atomic terms, a process to which we refer as  ontology enrichment . Of note, the Gene Ontology Next Generation (GONG) project first used the description-logic-based language DAML+OIL to formally define 250 Gene Ontology (GO) metabolism terms using MeSH terms (Wroe  et al. ,  2003 ), and later OWL to formally define a much larger number of GO metabolism, binding and transport terms again using MeSH terms (Aranguren,  2004 ); this project has since evolved into the more general Biological Ontology Next Generation (BONG), which currently exists as a plugin to the Protege ontology editor. The Obol effort uses a series of Prolog production rules that can be used to decompose a given matching GO term into an Aristotelean genus (category) and one or more differentiae (necessary and sufficient conditions that differentiate the term from other terms of the same genus); the Gene Ontology Consortium is currently using Obol to generate Aristotelean definitions of OBO terms that refer to other OBO terms (Mungall,  2004 ). In our frame-based Protege ontology-enrichment effort, we have created over 9600 assertions linking terms in the GO (The Gene Ontology Consortium,  2000 ), Chemical Entities of Biological Interest (ChEBI) ontology (Degtyarenko,  2003 ), and the Cell Type Ontology (CL) (Bard  et al. ,  2005 ); these base assertions have been integrated into this set of ontologies such that each assertion is consistent with all assertions made at more general levels (Bada and Hunter,  2007 ). Both GONG and Obol have been able to take advantage of associated reasoners; for the former, an OWL reasoner can be used, while for the latter, the Aristotelean definitions can be imported into OBO-Edit ( www.oboedit.org ), the primary tool in which OBOs are developed, and its associated reasoner invoked. A great advantage of using such a reasoner is its ability to make new inferences derived from the added formal term definitions. For example, in the second published GONG study, using the newly added formal defintions for the GO molecular function (MF) terms  neurotransmitter binding  and  glutamate binding  (which use the MeSH terms  Neurotransmitters  and  Glutamates , respectively), the OWL reasoner inferred that  neurotransmitter binding  subsumes  glutamate binding , a link absent at that point in GO. However, both GONG/BONG and Obol/OBO-Edit require that these definitions use necessary and sufficient conditions in order for these inferences to be made. This is a strong requirement that does not hold bidirectionally in many, if not most cases: it is necessary and sufficient that catecholamine transport is a transport that results in the directed movement of a catecholamine. However, the semantics of OWL or OBO say that, for an existential restriction expressed for a subject class A linking it to an object class B via property p, each instance of A must have at least one value from B for p. Since we cannot say that every catecholamine takes part in a catecholamine-transport process, it is not even possible to make this a necessary assertion. Consequently, using terms from these two terminologies that have been linked, these new subsumptive inferences can only be made between subject terms for which necessary and sufficient definitions can be created (e.g. substance-transport terms) and not with the object terms (e.g. the substances that are being transported) used in these definitions. The inferences that are made by these reasoners point to what we call  nonalignments— subsets of terms that are linked (other than via  is_a ), but that are not aligned in that the terms of one side of the links are linked by subsumption while the terms of the other side are not. (The nonalignments we identify all consist of subject terms that are subsumptively linked and object terms that are not subsumptively linked.) For example, as can be seen in  Figure 1 , we have linked the ChEBI term  chlorohydrocarbons  to the GO term  chlorinated hydrocarbon metabolism  and also the ChEBI term  1,3-dichloro-2-propanol  to the GO term  1,3-dichloro-2-propanol metabolism . These pairs of terms are not aligned in that  1,3-dichloro-2-propanol  is subsumed by  chlorohydrocarbons  in ChEBI, but  1,3-dichloro-2-propanol metabolism  is not subsumed by  chlorinated hydrocarbon metabolism  in GO. We expect the two sides to be aligned in that if 1,3-dichloro-2-propanol is indeed a kind of chlorohydrocarbon (as represented in ChEBI), then it should be metabolized in a kind of chlorinated-hydrocarbon metabolism—but 1,3-dichloro-2-propanol metabolism is not a kind of chlorinated-hydrocarbon metabolism (as represented in GO). In the nonalignments we identify, if the more specific subject entity (e.g. 1,3-dichloro-2-propanol) is indeed a kind of the more general subject entity (e.g. chlorohydrocarbons), then the assertion made for the more specific subject entity (e.g. that 1,3-dichloro-2-propanol can be metabolized in a 1,3-dichloro-2-propanol-metabolism process) should be subsumed by the assertion made for the more general subject entity (e.g. that a chlorohydrocarbon can be metabolized in a chlorinated-hydrocarbon-metabolism process).
 Fig. 1. The relationships between a pair of terms from ChEBI and another pair of terms from the GO BP ontology, the analysis of which an ontology nonalignment has been identified. Specifically,  1,3-dichloro-2-propanol  is subsumed by  chlorohydrocarbons  in the former, but  1,3-dichloro-2-propanol metabolism  is not subsumed by  chlorinated hydrocarbon metabolism  in the latter. This nonalignment was identified by analyzing the respective object classes of  is metabolized in  at the levels of  1,3-dichloro-2-propanol  and of  chlorohydrocarbons . In this example, with necessary and sufficient definitions of  chlorinated hydrocarbon metabolism  and  1,3-dichloro-2-propanol metabolism  in terms of  chlorohydrocarbons  and  1,3-dichloro-2-propanol , respectively, these reasoners would point to this nonalignment through the suggestion of an  is_a  link from  1,3-dichloro-2-propanol metabolism  to  chlorinated hydrocarbon metabolism . However, if instead  1,3-dichloro-2-propanol  was not subsumed by  chlorohydrocarbons  and  1,3-dichloro-2-propanol metabolism  was subsumed by  chlorinated hydrocarbon metabolism , these reasoners would not be able to suggest an  is_a  link from  1,3-dichloro-2-propanol  to  chlorohydrocarbons , because the required necessary and sufficient definitions of  1,3-dichloro-2-propanol  and c hlorohydrocarbons  in terms of 1, 3-dichloro-2-propanol metabolism  and c hlorinated hydrocarbon metabolism , respectively, could not be created using these terms in an ontologically valid way. This is not a fault of OWL or of Aristotelean formalism; these representational systems have strict semantics, to which ontologists should adhere when making assertions. It is just that reasoners relying solely on necessary and sufficient definitions will likely miss many of these nonalignments because ontologically valid definitions cannot be created, and it is desirable that as many of these nonalignments as possible be rectified. We have implemented our ontology-enrichment project in Protege-Frames (mainly because this is part of a larger frame-based effort). There is no associated reasoner to Protege-Frames, so we implemented a simple reasoning system to ensure the global consistency of the added assertions in our set of integrated ontologies. It is this same reasoning system we use here to discover nonalignments in the constituent ontologies through structural analysis of the assertions we added in our previous work (Bada and Hunter,  2007 ). Reasoning over these assertions, we were able to discover nearly 1700 instances of nonredundant nonalignments, 39.8% of which likely could not be identified via suggested inferences by OWL or OBO-Edit reasoners due to the fact that the required necessary and sufficient definitions could not be created in an ontologically valid way using these terms of the linked ontologies. We propose that those nonalignments for which such inferences cannot be made by these reasoners also be examined to increase consistency among the linked ontologies. 2 METHODS The method by which we ensure the global consistency of the set of assertions to the ontologies is through an analysis of the object classes of the properties of the classes. Specifically, this analysis relies on the fact that the object expression (here, an object class or union of object classes) of a property at a given class level must be subsumed by the object expression of the property at higher (i.e. more general) class levels. Furthermore, the object expression of a given property must be subsumed by the object expression at higher property levels. Put more simply, object expressions should monotonically narrow as one descends to more specific classes and slots. In order for each assertion to be consistent with each assertion made at more general levels, any object class of a property at a given class level that was not subsumed by an object class at a higher class and/or property level such that these conditions were satisfied was appropriately propagated up the class and/or slot hierarchies. The full details of this procedure can be read in the initial publication of our OBO-enrichment work (Bada and Hunter,  2007 ). Our methodology for discovering ontology nonalignments follows from this global consistency enforcement. For each base assertion (represented as a triple of a subject class, property and object class), each of the class's direct superclasses is checked to see if it is within the domain of the property. If so, it is checked if at least one of the object classes of the property of the superclass subsumes the object class of the property of the base assertion. If there is no such subsuming class, this is a nonalignment between the subject and object classes of the two assertions. If there is such a subsuming class at the level of this direct superclass, the same examination is performed for each of its direct superclasses. This continues recursively until either all direct superclasses are outside of the domain of the given property or a root of the ontology is reached. This can be made clearer with a simple but real example. Consider the base assertion  1,3-dichloro-2-propanol is metabolized in 1,3-dichloro-2-propanol metabolism , which states that 1,3-dichloro-2-propanol can be metabolized in a 1,3-dichloro-2-propanol-metabolism process. The sole direct superclass of  1,3-dichloro-2-propanol-chlorohydrocarbons  is obtained. It is checked that  chlorohydrocarbons  is within the domain of the slot i s metabolized in , which is the case. The set of allowed classes of  is metabolized in  at the level of  chlorohydrocarbons  is then obtained, which is the single class  chlorinated hydrocarbon metabolism  (which indicates that a chlorohydrocarbon can be metabolized in a chlorinated-hydrocarbon-metabolism process). The set of allowed classes at the superclass level (the one-member set  chlorinated hydrocarbon metabolism ) should subsume the set of allowed classes at the base-assertion level (the one-member set  1,3-dichloro-2-propanol metabolism ). However, it does not; this is thus a nonalignment.  Figure 1  illustrates this example. For each discovered nonalignment, we extracted four entities into which the nonalignment can be distilled: the subject class of the base assertion, the superclass of this subject class at the level of which the nonalignment was found, the object class of the base assertion (i.e. the allowed class of the assertion), and the set of object classes at the level of the superclass (i.e. the set of allowed classes for the slot at the level of the superclass). There is only one object class for each base assertion, while there can be more than one object class at the level of the superclass, since monotonicity as one travels down the class hierarchy is preserved as long as an object class of a property of a class is subsumed by at least one object class of the property of the superclass.  Figure 2  illustrates another real example where the set of allowed classes at the level of the supeclass has more than one member. In this example, the set of object classes for  results in binding of  at the level of  protein binding  was assigned the set [ proteins, protein polypeptide chains, protein complex ]. Such a multiply membered set of object classes is represented as a union of classes, so this assertion indicates that a protein-binding process can result in the binding of either a protein, a protein polypeptide chain, or a protein complex. (This was done because the definition of protein binding is ‘interacting selectively with a protein or protein complex’.) However, relatively few terms so far have been assigned multiple allowed classes as in this example, so this is currently an exceptional case.
 Fig. 2. The relationships between terms from the GO BP ontology and ChEBI and the GO CC ontology, the analysis of which an ontology nonalignment has been identified. Specifically,  histone binding  is subsumed by  protein binding  in the former, but  histones  is not subsumed by  proteins, protein polypeptide chains  or  protein complex  in the latter. This nonalignment was identified by analyzing the respective object classes of  results in binding of  at the levels of  histone binding  and of  protein binding . Each stored nonalignment represented by the four summarizing entities was written out to a text file in the following format:
 subject class of base assertion -&gt; superclass of subject class object class of base assertion !-&gt; object-class set at level of superclass 
 This neatly summarizes the nonalignment by stating that the subject class of the base assertion is subsumed by the superclass, but the object class of the base assertion is not subsumed by any of the object classes at the level of the superclass. Thus, the nonalignment illustrated in  Figure 1  is represented as:
 1,3-dichloro-2-propanol -&gt; chlorohydrocarbons 1,3-dichloro-2-propanol metabolism !-&gt; chlorinated hydrocarbon metabolism 
 Such a representation makes clear the essence of the nonalignment—that  1,3-dichloro-2-propanol  is subsumed by  chlorohydrocarbons  (in ChEBI), but  1,3-dichloro-2-propanol metabolism  is not subsumed by  chlorinated hydrocarbon metabolism  (in the GO biological process (BP) ontology). Due to the extensive multiple inheritance of the component ontologies, it is possible to discover redundant nonalignments or even the same nonalignment more than once. Only nonredundant nonalignments were stored and exported, as examining redundant nonalignments to assess whether there are true semantic discrepancies entails additional, unnecessary effort and biases statistics. Two nonalignments are redundant if the resolution of the one also results in the resolution of the other. Consider the following two nonalignments:
 benzoate -&gt; anions benzoate transport !-&gt; anion transport benzoate -&gt; ions benzoate transport !-&gt; ion transport 
 These two nonalignments are redundant with respect to one another. If the first nonalignment was resolved by adding an  is_a  link from  benzoate transport  to  anion transport , the second nonalignment would also be resolved since this link addition would result in the implication that  benzoate transport  is a type of  ion transport ; thus, the second nonalignment would also be resolved. In cases of redundancy, we have kept the more specific nonalignment; thus, for the example above, only the first nonalignment was stored. The relevant relationships between the terms of these two nonalignments are illustrated in  Figure 3 .
 Fig. 3. The relationships between terms from ChEBI and the GO BP ontology, the analysis of which two redundant ontology nonalignments were identified. Specifically,  benzoate  is subsumed by  anions  in the former, but  benzoate transport  is not subsumed by  anion transport  in the latter. Also,  benzoate  is subsumed by  ions  in the former, but  benzoate transport  is not subsumed by  ion transport  in the latter. The March 6, 2008 versions of GO, ChEBI and CL were used for this study. These base ontologies were previously enriched with 10 270 additional assertions linking the component terms using 50 specific relationships detailed in the initial publication of our OBO-enrichment work. It is important to note that although this study relies upon the links we created in our previously published ontology-enrichment work, our methodology for nonalignment identification is not limited by the specific relationships we chose to use. (The quality of the nonalignments, however, is dependent on the quality of the links that the methodology analyzes.) In fact, we have recently generated nonalignments based on links created by members of the OBO Consortium and have begun a discussion of ways of managing these nonalignments. 3 RESULTS Using this methodology resulted in a total of 1938 nonredundant nonalignments within the set of GO, ChEBI and CL; this set of nonalignments can be examined at  http://compbio.uchsc.edu/Hunter_lab/Bada/nonalignments_2008_03_06.html . To better characterize their distribution, we clustered the nonalignments according to the ontologies that were the sources of the subject and object terms of the nonalignments. For example, the nonalignment illustrated in  Figure 1  is a ChEBI-to-BP nonalignment, since the subject terms ( 1,3-dichloro-2-propanol  and  chlorohydrocarbons ) are from ChEBI and the object terms ( 1,3-dichloro-2-propanol metabolism  and  chlorinated hydrocarbon metabolism ) are from the GO BP ontology. There is a slight complication in that the two sets of object terms of a nonalignment may be from different ontologies, but this is rare. In such a case, the object term of the base assertion is used for the classification of the nonalignment. Table 1  lists the number of assertions and nonredundant nonalignments for each directed pairwise combination of ontologies for which there is at least one corresponding assertion. For example, there are 2710 total added assertions from a GO BP term to another GO BP term, and 94 nonredundant nonalignments were identified from these assertions. The numbers of nonalignments are largely symmetric. The biggest discrepancy is that between the 598 nonalignments identified from the BP-to-ChEBI assertions and the 1022 nonalignments identified from the ChEBI-to-BP assertions.
 Table 1. Numbers of assertions and nonredundant alignments for each directed combination of ontologies for which there is at least one added assertion Ontology to ontology Assertions Nonalignments GO BP to GO BP 2710 94 GO BP to GO CC 156 17 GO BP to ChEBI 3022 598 GO BP to CL 117 5 GO BP to GO MF 65 3 GO CC to GO BP 156 19 GO CC to GO CC 154 10 GO CC to GO MF 32 3 ChEBI to GO BP 3022 1022 ChEBI to GO MF 242 79 CL to GO BP 117 10 GO MF to GO BP 65 0 GO MF to GO CC 32 9 GO MF to ChEBI 242 69 Table 2  lists the numbers of assertions and nonredundant nonalignments and the ratio of nonalignments to assertions for each undirected pairwise combination of ontologies for which there is at least one corresponding assertion. The lowest ratios of nonalignments to assertions are those between BP terms and MF terms (0.02), between BP terms and BP terms (0.034), between BP terms and CL terms (0.064) and between cellular component (CC) terms and CC terms (0.065). This suggests that terms within these pairs of ontologies are relatively well-aligned. The highest ratios of nonalignments to assertions are those between ChEBI terms and MF terms (0.306), between BP terms and ChEBI terms (0.2680) and between CC terms and MF terms (0.19). This suggests that these pairs of ontologies are relatively not aligned well, which agrees with our empirical observations in our ontology-enrichment work that ChEBI is relatively not aligned well with GO.
 Table 2. Numbers of assertions and nonredundant alignments and the ratio of nonalignments to assertions for each undirected pairwise combination of ontologies for which there is at least one added assertion Ontology - ontology Assertions Nonalignments Ratio GO BP - GO BP 2798 94 0.034 GO BP - GO CC 312 36 0.12 GO BP - ChEBI 6044 1620 0.2680 GO BP - CL 234 15 0.064 GO BP - GO MF 130 3 0.02 GO CC - GO CC 154 10 0.065 GO CC - GO MF 64 12 0.19 ChEBI - GO MF 484 148 0.306 Another way to characterize the nonalignments is whether the subject terms of the nonalignments are the more complex terms or the more atomic terms. For example, in the example illustrated in  Figure 1 , the subject terms ( 1,3-dichloro-2-propanol  and  chlorohydrocarbons ) are more atomic than the object terms in that the latter are built up from the former. Conversely, in the example illustrated in  Figure 2 , the subject terms ( protein binding  and  histone binding ) are more complex than the object terms. As will be explained more fully in the next section, this characterization has important implications in that the new inferences made by the GONG/BONG and Obol projects correspond to the first type of nonalignment, in which the subject classes are more atomic, since ontologically valid necessary and sufficient definitions, which are required for these projects, can more easily be constructed in these cases. The second type of nonalignment includes all of the BP-to-CC, BP-to-ChEBI, BP-to-CL, BP-to-MF, MF-to-CC and MF-to-ChEBI nonalignments, while the BP-to-BP and CC-to-CC sets of nonalignments have mixtures of the two types of nonalignments. We have found that 772 (39.8%) of the 1938 nonredundant nonalignments are of the second type, thus showing that our methodology can identify a large number of nonalignments that may be missed by the reasoning methods of the other projects. 4 DISCUSSION 4.1 Evaluation and management of nonalignments In this study, we have used the term nonalignment to refer to two analogous sets of entities such that one entity is subsumed by the other in the first pair while one entity is not subsumed by the other in the second pair. Upon examination of a given nonalignment, if it is determined that the pairs of entities should be aligned, we term this a  discrepancy . Not all nonalignments are discrepancies;  Figure 4  illustrates such an example. Here,  laminin-1 binding  is subsumed by  extracellular matrix binding  in the GO MF ontology, but  laminin-1 complex  is not subsumed by  extracellular matrix  in the GO CC ontology. Even though it is a nonalignment, we believe that this is not a discrepancy in that these pairs of terms should not be aligned; that is, laminin-1 binding is a type of extracellular-matrix binding, but the laminin-1 complex is not a type of extracellular matrix (but rather a component of the extracellular matrix). Nevertheless, we assert that a large majority of the nonalignments we have identified are indeed discrepancies.
 Fig. 4. The relationships between a pair of terms from the GO MF ontology and a pair of terms from the GO cellular-component ontology, the analysis of which an ontology nonalignment has been identified. We assert this is an example of nonalignment that is not a discrepancy in that the subsumption relationship between the subject terms and the lack of a subsumption relationship between the object terms appear to be valid. If a given nonalignment is assessed to be a discrepancy, there are two ways to resolve it. The first is to add an  is_a  link from the object term of the base assertion to the object term at the superclass level (or, in the case of multiple object terms at the superclass level, to at least one of the object terms). For example, we assert the nonalignment illustrated in  Figure 1  is a discrepancy: according to this model, a chlorohydrocarbon can only be metabolized in a chlorinated-hydrocarbon-metabolism process, but a molecule of 1,3-dichloro-2-propanol, which is a kind of chlorohydrocarbon (according to ChEBI), can only be metabolized in a 1,3-dichloro-2-propanol-metabolism process, which is not a kind of chlorinated-hydrocarbon-metabolism process (according to GO BP). One way to resolve this discrepancy is the addition of an  is_a  link from  1,3-dichloro-2-propanol metabolism  to  chlorinated hydrocarbon metabolism . With this addition, a molecule of 1,3-dichloro-2-propanol can be metabolized in a 1,3-dichloro-2-propanol-metabolism process, which is now a more specific kind of chlorinated-hydrocarbon-metabolism process. The second way to resolve a discrepancy is the removal of the  is_a  link from the subject term of the base assertion to the subject term at the superclass level. In  Figure 1 , this corresponds to the removal of the is_a link from  1,3-dichloro-2-propanol  to  chlorohydrocarbons . With the removal of this link, 1,3-dichloro-2-propanol is no longer a more specific kind of chlorohydrocarbon, which aligns with the fact that a 1,3-dichoro-2-propanol-metabolism process is not a kind of a chlorinated-hydrocarbon-metabolism process. In the case of a nonalignment that is not a discrepancy, there is still a logical inconsistency, and action should be taken to rectify the inconsistency. A general, automatic solution to such an inconsistency is the propagation of the object class of the base assertion up to the superclass level; this is the type of upward propagation we previously extensively employed in our ontology-enrichment work so as to ensure the global consistency of the ontologies when adding enriching assertions. For example, in  Figure 4 , we assert that neither of the two steps described in the previous paragraphs should be performed; however, there is still a logical inconsistency in that an extracellular-matrix-binding process results in the binding of an extracellular matrix, but a laminin-1-binding process, which is a kind of extracellular-matrix-binding process (according to GO MF), results in the binding of a laminin-1 complex, which is not an extracellular matrix (according to GO CC). (According to GO CC,  laminin-1 complex  is transitively  part_of extracellular matrix .) The rectification we describe here consists of adding  laminin-1 complex  as an object class of  results in binding of  at the level of  extracellular matrix binding ; this is illustrated in  Figure 5 . The semantics of this new model are that an extracellular-matrix-binding process results in the binding of an extracellular matrix or a laminin-1 complex, while a laminin-1-binding process further restricts this to a laminin-1 complex.
 Fig. 5. The relationships between terms from the GO MF ontology and cellular-component ontologies in which the nonalignment identified in  Figure 4  has been rectified by the propagation of  laminin-1 complex . Specifically,  laminin-1 complex  has been added as an object class of  results in binding of  at the level of  extracellular matrix binding . A more elegant solution in this example is to instead add the GO CC term  extracellular matrix part  as an allowed class of  results in binding of  at the level of  extracellular matrix binding ; the semantics of this are that an extracellular-matrix-binding-process results in the binding of an extracellular matrix or an extracellular-matrix part, which seems to be a valid definition for  extracellular matrix binding . The original nonalignment would be resolved in that  laminin-1 complex  at the level of  laminin-1 binding  would be subsumed by  extracellular matrix part  at the level of  extracellular matrix binding . Though this is semantically closer to the definition of  extracellular matrix binding , it is also more manual and thus more labor-intensive (which is not to say that it should not be done). Our methodology could be used to either automatically upwardly propagate the specific classes so as to make the ontologies consistent, as described in the previous paragraph, or it could be used to automatically make suggestions to the ontology curators, who would decide to add either the specific terms or more general terms (such as  extracellular matrix part ). Of total of 1938, 100 nonredundant nonalignments were randomly selected for an evaluation. Out of these 100, 96 were assessed to be discrepancies; that is, we assert that they should be similarly aligned through the addition or removal of an  is_a  link, corresponding to the first two types of resolution. The remaining four nonalignments are analogous to the example seen in  Figure 4 , in which the subject and object terms should not be aligned; rather, the third type of resolution should be undertaken, in which an object term should be added to the higher-level assertion such that the lower-level assertion is subsumed, as seen in  Figure 5 . 4.2 Comparison to other projects Both the GONG/BONG and Obol projects have been focusing on creating formal defintions of OBO terms using more atomic OBO terms in necessary and sufficient conditions. These definitions can then be reasoned over (by an OWL reasoner for the former and by the Obol reasoner or the OBO-Edit reasoner for the latter), which can make new inferences using the definitions. However, the reasoner can only make new inferences using the linked terms if ontologically valid necessary and sufficient definitions can be constructed. The type of inferences that can be made largely corresponds to the absent subsumptions in the type of nonalignments in which the subject terms are more atomic than the object terms.  Figure 1  is such an example. Necessary and sufficient definitions could be produced for  1,3-dichloro-2-propanol metabolism  (as a subclass of  metabolism  with a  results in metabolism of 1,3-dichloro-2-propanol  condition) and for  chlorinated hydrocarbon metabolism  (as a subclass of  metabolism  with a  results in metabolism of chlorohydrocarbons  condition). If the associated reasoner reasons over ChEBI and GO (including these added definitions), given that  1,3-dichloro-2-propanol  is subsumed by  chlorohydrocarbons  as in  Figure 1 , it can infer an  is_a  link from  1,3-dichloro-2-propanol metabolism  to  chlorinated hydrocarbon metabolism . This is the same link that is the absent subsumption between the object terms (i.e. that  1,3-dichloro-2-propanol metabolism  is not subsumed by  chlorinated hydrocarbon metabolism ) of the nonalignment described for this example. Thus, these projects could predict analogous inferences for all of our nonalignments in which the subject terms are more atomic than the object terms, so long as ontologically valid necessary and sufficient definitions could be constructed, as was done in this example. Our methodology does not automatically suggest that all object pairs in each identified nonalignment be linked via  is_a , as this may not be the correct action to take; it allows the curator to resolve the nonalignment with any of the four methods described in the previous section. However, these projects likely could not predict new inferences for many if not all of the nonalignments in which the object terms are more atomic than the subject terms presented here, because the required necessary and sufficient definitions likely could not be made in an ontologically valid manner.  Figure 6  illustrates such an example. The nonalignment identified here is that  aldonate transport  is subsumed by  hexose transport  in GO BP, but  aldonates  is not subsumed by  hexoses  in ChEBI. Given necessary and sufficient definitions of  hexose transport  in terms of  hexoses  and  aldonate transport  in terms of  aldonates  and the fact that  aldonate transport  is subsumed by  hexose transport , a reasoner from one of these projects cannot infer that  hexoses  subsumes  aldonates . In order for the reasoner to infer an  is_a  link from  aldonates  to  hexoses  (which is one way to resolve this nonalignment) from these terms and their definitions, necessary and sufficient definitions for  aldonates  (perhaps as a subclass of  molecular entities  and an  is transported in aldonate transport  condition) and  hexoses  (perhaps as a subclass of  molecular entities  and an  is transported in hexose transport  condition) would have to be created. However, this is too strong a condition, as, for example, an aldonate is not necessarily transported elsewhere; it may be used where it was synthesized. Without these necessary and sufficient definitions, this inference cannot be made.
 Fig. 6. The relationships between a pair of terms from the GO BP ontology and a pair of terms from ChEBI from which a nonalignment was identified. This is an example of a nonalignment that is not currently examined in other ontology-enrichment methodologies, which require necessary and sufficient conditions to make new inferences. It can be argued that a reasoner in one of these other projects can infer an  is_a  link between chemicals by creating ontologically valid necessary and sufficient definitions in terms of, for example, parts or functions of these chemicals. However, this presupposes that not only such a more basic ontology but the required specific object terms exist. Such an approach laboriously requires the creation of an entirely new set of assertions, and there may be recursion in that the more basic object terms may not exist in a hierarchical relationship, thus once again preventing the inference of the  is_a  link between the more composite subject terms. Our approach only requires one set of assertions and their automatically generated inverse assertions and relies on a different kind of reasoning than the deduction used by reasoners in the aforementioned projects. However, we assert that a functionally equivalent methodology could be implemented, e.g. using an OWL API, without the use of explicitly represented inverse assertions. We have found that 39.8% of the total nonredundant nonalignments identified in this study are those in which the subject terms of the nonalignments are built up from the object terms; these correspond to the instances in which it is difficult to produce the required ontologically valid necessary and sufficient conditions, in which case new inferences by the aforementioned reasoners cannot be made using the linked terms of the ontologies. Our methodology essentially uses subsumptive analysis of term attributes toward quality assurance of ontologies, a technique which has been used by others in the field. The BERNWARD system reconstructed sets of medical concepts into hierarchies based on five subsumptive principles, but it is different in that it takes into account partonomy in its subsumption without resolution of the type we perform as in  Figures 4  and  5  (Bernauer,  1994 ). In an analysis of UMLS, Cimino ( 1998 ) found that the semantic type of 0.5% of concepts was neither the same as nor more specific than the semantic type of their respective parents. In an analysis of the links between diseases and their respective anatomical locations in SNOMED CT, Burgun  et al.  ( 2005 ) looked for differences between sets of disorders associated with all descendants of given anatomical entities and the sets of descendant disorders of the disorders associated with the given anatomical entities. Bodenreider  et al.  ( 2007 ) found that SNOMED CT contained 7226 parent-child pairs in which a role or value present in the parent was not present in the child and 21 799 pairs in which a value of a role present in the parent was not identical or more specific in the child. In addition to being the first subsumptive study of links among OBO terms, ours suggests both fully automatic and semiautomatic solutions to correct the inconsistencies that result upon linking the terms and highlights those that are not currently found by existing reasoning methods in other biomedical ontology-enrichment projects. We are not calling for the abolition of the use of the OWL, Obol or OBO-Edit reasoners. Rather, we assert that functionality that identifies the type of nonalignments for which inferences cannot be made (due to absence of required necessary and sufficient conditions) can and should be built into ontology-enrichment tools such as BONG. A methodology analogous to ours appears possible through the use of an OWL API through a subsumptive analysis of directly asserted and inherited property-value pairs. Consider  Figure 7 , in which the nonalignment of  Figure 6  has been resolved through the addition of an  is_a  link from  aldonates  to  hexoses . The links from the subject terms to the object terms can be represented as necessary and sufficient existential (i.e.  someValuesFrom ) conditions. Comparing the value of  results in transport of  at the level of  aldonate transport  ( aldonates ) to the value of  results in transport of  at the level of  hexose transport  ( hexoses ), it can be determined that the former is subsumed by the latter; thus, there is no inconsistency. Conversely, considering  Figure 6 , using the same procedure,  aldonates  is not subsumed by  hexoses , which could result in the suggestion of a nonalignment. The same methodology could be used to suggest nonalignments where necessary and sufficient definitions can be made, but this appears unnecessary, since existing reasoners can suggest new inferences for such cases. Moreover, this would require the use of statements for which ontologically valid necessary and sufficient conditions likely could not be made. Thus, the subsumptive inferences made by currently used reasoners and the nonalignments discovered by our methodology are complementary if the OBO curators continue to solely examine those nonalignments indicated by the inferences made by the reasoners using necessary and sufficient definitions.
 Fig. 7. The relationships between a pair of terms from the GO BP ontology and a pair of terms from ChEBI that result from the resolution of the nonalignment of  Figure 6  via the addition of an  is_a  link from  aldonates  to  hexoses . 5 SUMMARY We have described a methodology by which we have identified over 1900 instances of nonredundant nonalignments between terms from GO, ChEBI and CL. Analysis of the ratios of nonalignments to assertions from which the nonalignments were identified suggests that BP–MF, BP–BP, BP–CL and CC–CC terms are relatively well-aligned, while ChEBI–MF, BP–ChEBI and CCMF terms are relatively not aligned well. We propose that three ways to resolve an identified nonalignment are the addition of an  is_a  link between the object terms, the removal of an  is_a  link between the subject terms and the upward propagation of the object term to the superclass level. Many of the 39.8% of these nonalignments in which the object terms are more atomic than the subject terms likely are not currently examined in other ontology-enrichment projects due to the fact that the necessary and sufficient conditions required for the inferences likely could not be added, as they are semantically too strong. We assert that a methodology analogous to ours could be implemented using an OWL API in ontology-enrichment tools in order to identify such nonalignments that are currently not examined. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>A gene–phenotype relationship extraction pipeline from the biomedical literature using a representation learning approach</Title>
    <Doi>10.1093/bioinformatics/bty263</Doi>
    <Authors>Xing Wenhui, Qi Junsheng, Yuan Xiaohui, Li Lin, Zhang Xiaoyu, Fu Yuhua, Xiong Shengwu, Hu Lun, Peng Jing</Authors>
    <Abstract/>
    <Body>1 Introduction The biomedical literature is vast ( Cohen and Hersh, 2005 ), and there is an urgent need to process publications automatically and mine embedded knowledge in the literature to create research hypotheses. Recently, biomedical relationship extraction has gained attention for many downstream text-mining applications, such as event extraction, database creation, knowledge discovery, question answering and decision-making. Natural language processing (NLP) systems have been used for mining special relationships from texts as protein–protein interactions ( Papanikolaou  et al. , 2015 ;  Yang  et al. , 2011 ;  Zhu  et al. , 2015 ), genes and diseases ( Coulet  et al. , 2010 ;  Kim  et al. , 2017 ), drug–drug interactions ( Segura Bedmar  et al. , 2011 ,  2013 ), as well as among genes, drugs and mutations ( Cheng  et al. , 2008 ;  Rindflesch  et al. , 1999 ). Such relationship extraction contributes to the development of pharmacogenomics, clinical trial screening and adverse drug reaction identification ( Luo  et al. , 2017 ). The central challenge of modern genetic analysis is to establish genotype–phenotype correlations ( Cobb  et al. , 2013 ;  Fu  et al. , 2014 ), which are often found in the biomedical literature, but the volume warrants an automatic and reliable system to extract these information from the text. Although relationships have been identified among numerous biological entities, the system for extracting gene–phenotype relationships from the literature is very limited. Regarding species types, the current research focuses more on the relationships between human genes and phenotypes ( Collier  et al. , 2015 ;  Yang  et al. , 2015 ). To our knowledge, there is few such studies for plants. Regarding entity types, research on identifying specific phenotypes such as diseases and gene relationships has received great attention ( Kim  et al. , 2017 ;  Özgür  et al. , 2008 ;  Singhal  et al. , 2016 ). However, text-mining systems that can recognize various phenotype and gene relationships are more difficult and are less robust. The system generally involves annotating raw text with named entities and extracting relationships between these entities. ( Luo  et al. , 2017 ) Named entity recognition (NER) is the foundation of relationship extraction and the effect of entity recognition greatly affects relationship extraction results. ( Chun  et al. , 2006 ) With gene–phenotype relationship extraction, gene and phenotype should be identified. Because lexical features are relatively regular, there are many methods to identify genes in the text. ( Campos  et al. , 2012 ;  Wei  et al. , 2015 ) However, although research on NER has been improved ( Gaizauskas  et al. , 2003 ;  Horn  et al. , 2004 ;  Segura-Bedmar  et al. , 2008 ), phenotype identification is still challenging and this negatively influences relationship extraction. First, a phenotype is usually composed of multiple words, such as ‘ calcium sensitivity ’ or ‘ genic male sterility-photoperiod sensitive ’. Thus, name boundaries are complex. Second, phenotypic descriptions are often study- or author-specific due to a lack of standard expressions, complicating this search. For example, in the two sentences ‘…resulting in root growth inhibition, smaller rosettes, and  leaf curling' . (PMID: 26734017) and ‘…leading to early flowering and  curly leaves phenotypes’ . (PMID: 25693187), the same leaf morphology has two different descriptions, i.e. ‘ leaf curling ’, ‘ curly leaves ’. In addition, while there are specialized lexicons in many areas, no lexicon can be directly used to identify overall phenotypic descriptions in text, especially for plants. For example, the Unified Medical Language System (UMLS) MetaThesaurus ( Humphreys  et al. , 1998 ) is a vocabulary database that includes numerous semantic types, except for  Phenotype  type. In the plant domain, the controlled vocabulary plant trait ontology (PTO) ( http://bioportal.bioontology.org/ontologies/PTO  ) is too general, so it may not include all species traits. The Arabidopsis Information Resource (TAIR) ( Lamesch  et al. , 2012 ) is curated by manually summarizing published literature so it is limited and difficult to organize for future use. The AraPheno ( Seren  et al. , 2017 ) database is an organization of the Genome-Wide Association Study (GWAS) phenotypic results in only six published studies, so the data are few. These manual curation processes are time-consuming and cannot keep up with rapidly increasing literature. Here, we propose a novel gene–phenotype relationship extraction pipeline using model plant  Arabidopsis thaliana . First we improved the word-embedding-to-sentence-embedding cascaded approach ( Xing  et al. , 2017 ) as representation learning to recognize various broad phenotypic descriptions in large-scale biomolecular literature. Then, genes from the same phenotype-containing sentence were found, using the dictionary-based method. Next, a relationship extraction system Open Language Learning for Information Extraction (OLLIE) was applied to extract gene–phenotype relationships. The proposed pipeline improves relationship extractions by identifying more phenotypic descriptions in the text. We identified many types of phenotypic descriptions based on their boundary delimitation: phenotypic phrases and phenotypic long/short sentences. To locate sentences that include the phenotype, we use word embedding to learn distributed representations for words and phrases. Then, we can extract phenotypic phrases missed by ontology, thus extracting more sentences containing phenotypes. Then we cascade the sentence-embedding method for specific phenotype-containing sentences. Due to numerous candidate phenotypic sentences, expert verification is time-consuming. According to the similarity mechanism, we find that sentences with high similarity to the phenotype-containing sentences have similar sentence structures. This prompted us to design a Phenotypic Sentence Template Extraction arChitecture (PSTEC) algorithm that automatically extracts phenotype sentence templates. With these templates, we can extract complex non-phrase forms of long/short phenotypic sentences. Ultimately, we evaluated the proposed pipeline from two aspects. (i) We designed three baselines to compare with our proposed relationship extraction pipeline. From the results, we identified more phenotypes (expanding the original ontology almost 3-fold), which significantly improved recall value (improving 24.05% compared to the traditional ontology-based method). Meanwhile, identifying phenotypic descriptions from multiple perspectives also increased the precision of whole recognition. Using the OLLIE system based on machine learning method, we effectively improved F1-Measure compared with traditional relation extraction approach. Thus, our pipeline had a F1-Measure of 66.83%, the greatest of all baselines. (ii) We applied the pipeline to 481 full articles from the TAIR gene–phenotype relationship dataset, and the coverage was 70.94%. Moreover, we added 373 relationships to expand this dataset. Our pipeline automatically identified new relationships with a growing body of literature showing strong scalability. The proposed pipeline is versatile and can be used not only for extraction of relationships in  Arabidopsis  but also for other plant species such as soybean and cotton. 2 Our gene–phenotype relationship pipeline 2.1 The overview of our pipeline The pipeline starts with scanning abstracts in PubMed using the keyword  ‘A.thaliana’  and the Entrez Programming Utilities (E-utilities) web service ( https://www.ncbi.nlm.nih.gov/books/NBK25501 ). We clean irrelevant author information and acquire 63 459 abstracts that mention  A.thaliana . Next, we improve the proposed cascaded representation learning approach ( Xing  et al. , 2017 ) to recognize various broad phenotypes in the literature. Our representation learning approach, combined with the syntactic and semantic analysis of texts, identifies phenotypes in multiple directions from phenotypic phrases to complex short/long phenotypic sentences. Using ontology terms as input, our approach greatly expands the recognition of ontology term synonyms in the literature and establishes a bridge from ontology to literature description, so that study- or author-specific terms can be identified. Then we use the results of phenotypic identification to extract gene–phenotype relationships. We use dictionary- and rule-based methods to identify  Arabidopsis  genes in the literature. Then, we combine the workflow of the Open Information Extraction (IE) system with our entity recognition to extract and establish an  Arabidopsis  gene–phenotype binary relationship. The pipeline was implemented and run on a 24 2.4 GHz Xeon core server running on Ubuntu Linux 16.04.  Figure 1  shows the overview of the pipeline.
 Fig. 1. The overview of our gene–phenotype relationships extraction pipeline 2.2 Cascaded approach for phenotype extraction Before entity recognition, we used domain-resource ontology to establish the original phenotypic dataset. We extracted phenotypic descriptions from phenotypic phrases and sentences based on different boundaries. We used the parse tree combined with the word embedding method to extract phenotypic phrases, the majority of which were described by noun phrases. Because some synonyms in ontology are not described as phenotype in the text, the previous approach did not consider it leading to some errors. Therefore, we added abbreviation recognition and revision algorithm into the improved cascaded approach. Because some special phenotypes are non-phrase forms or long/short sentence descriptions, we used phenotypic sentences from word embedding results as positive samples to cascade the sentence embedding method for finding phenotype sentences. We transformed the unsupervised sentence-embedding model into a weakly supervised model. Due to the lack of training of positive and negative samples, we use the Negative Class Label Enhanced (NCLE) algorithm ( Xing  et al. , 2017 ) to label negative samples and train the sentence-embedding model in combination with the positive samples of the word-embedding results. We analyzed results of sentence embedding, finding that phenotypic sentences gathered by the similarity mechanism had similar structures. However, the previous approach estimated these results through expert verification, which is time-consuming. Therefore, we extracted sentence templates that described the phenotype by improving the algorithm of the statistical combination to expand phenotype recognition. 2.2.1 Constructing the phenotype dataset First, we use two ontologies to create the original phenotype dataset  P , i.e. PTO and Arabidopsis Hormone Database 2.0 ( http://ahd.cbi.pku.edu.cn/cgi-bin/phenotypeBrowse.pl  .) ( Jiang  et al. , 2011 ). PTO is an important controlled vocabulary that describes phenotypic traits in plants. Each trait is a distinguishable, characteristic, quality or phenotypic feature of a developing or mature plant or a plant part. Arabidopsis Hormone Database 2.0 provides a systematic and comprehensive view of genes participating in plant hormonal regulation of the model organism  A. thaliana . Its phenotypic ontology was developed to describe precisely myriad hormone-regulated morphological processes with standardized vocabularies in  Arabidopsis . When processing PTO, we extract ‘name’ and ‘synonym’ from every term in the ontology. Approximately 84% of these names are associated with synonyms; on average, each name has 1.07 synonyms. For example, the phenotype ‘ alkali soil sensitivity ’ has two synonyms: ‘ AlkS ’ and ‘ alkali sensitivity ’. Not all of terms in these ontologies appear in the literature. We found 805 terms in abstracts after removing duplicate entries. We combined these into a complete phenotype dataset  P . 2.2.2 Word embedding We followed the word embedding method published in ( Xing  et al. , 2017 ). First, we used the collected PubMed texts to train the word-embedding model, which gave each word or phrase a distributed representation in low and dense dimensional vector space. By finding phrases with high similarity to phenotypic entities in  P , the original ontology of the phenotype is expanded as  P update . Therefore, we can obtain more sentences containing phenotypic information. Because some phenotypic synonyms contained in  P  are abbreviated forms, they may not represent as phenotype in the text and are incorrectly identified. For example, the abbreviation ‘ AC ’ in the ontology corresponds to the full name of ‘ leaf sheath auricle color ’. However, in the sentence ‘Many of these proteins have complex domain architectures with AC or GC centers …’ (PMID: 26721677), ‘ AC ’ is not a phenotype. The previous method did not consider abbreviation recognition such as this, so we required post-processing of word-embedding results. After obtaining a high similarity phenotype phrase, we recognized and revised the abbreviation. We used ( Xu  et al. , 2009 ) algorithms for identifying abbreviations in the biological literature, matching pairs of all abbreviations and full names in the processed texts. When we used an updated phenotype dataset  P update  to reidentify the phenotype in the literature, if there was an abbreviated form, it was first matched with a full name. Only the full name of the abbreviation also in  P update , remained as a phenotype, otherwise it was deleted. The abbreviation recognition and revision can increase pipeline precision value and identify phenotypes more accurately. 2.2.3 Sentence embedding Using the word-embedding results, we classified and tagged PubMed texts as input for the sentence-embedding ( Le and Mikolov, 2014 ) method. The trained model can find sentences containing phenotypic information, acquiring new phenotypic sentences. To improve diversity of phenotype recognition, we transformed the unsupervised sentence-embedding model into a weakly supervised model. We used the results of word-embedding as positive samples,  S pos , and combined the NCLE algorithm for negative samples,  S neg , for the training of the Sen2Vec model. Sentence embedding can aggregate similar phenotypic expressions. We found that large-scale gathered sentences have a similar sentence context structure. For example, the more similar sentences with ‘Solute import across the pollen plasma membrane, which occurs via proteinaceous transporters,  is required to  support pollen development  and also for  subsequent germination and pollen tube growth ’  always have the same structure  ‘be required {prep_*} + [phenotype]’ , such as:
 ‘During pollination, constant communication between male pollen and the female stigma  is required for  pollen adhesion, germination, and tube growth ’. ‘Two  A.thaliana  genes, QRT1 and QRT2,  are required for  pollen separation during normal development ’ . Due to many similar sentences, it is time-consuming to identify all phenotypic sentences and analyze their phenotype with expert evaluation. Therefore, we used sentence structure to automate extraction of complicated long/short phenotypic sentences of non-phrase types. These structures may contain complex phenotypic descriptions, likely with punctuation, prepositions, and conjunctions. We designed an automated algorithm to find frequently occurring sentence templates and with this, we extracted relatively complex descriptions of phenotypic long/short sentences from many sentence-embedding results. At present, there are few studies about automatic generation of sentence templates in NLP. We borrowed the idea of modular algorithms from Sentence Pattern Extraction arChitecturte (SPEC) systems in ( Michal  et al. , 2011 ) and proposed our own solution for combinatorial explosion problem. With the SPEC algorithm, a ‘sentence template’ is considered as  n -element ordered combination of sentence elements. It generates all possible combinations of patterns from a sentence and selects the frequency occurrence combination as a sentence pattern. However, we focused on the phenotype-containing structure and created the algorithm Phenotypic Sentence Template Extraction arChitecture (PSTEC) which consists of three components:
 Preprocessing Generation of all ordered combinations from sentence elements Insertion of a wildcard 
 Preprocessing : We tokenized all positive sentences  S pos  of sentence embedding. Because we must extract phenotype-containing sentence structures, we treated phenotypic phrases as a whole and replaced phenotypic descriptions appearing in the sentence with ‘PHE’. 
 Generation ordered combinations : In every  n -element sentence, there is  k -number of ordered combination groups (1 ≤  k  ≤   max). After processing all sentences in corpora, we choose a combination of frequencies greater than a threshold  fre  as a  k -length template. Because the phenotype-containing template is not too long, so we set max as the length of the element threshold. We set two restrictions to prevent the combination explosions:
 Combination of the  k -element must include the specific word ‘PHE’ Any ‘PHE’ contained ( k -1)-element subset of  k -element combination must be in the ( k -1)-element template. After iteration processing, we obtained all ordered, not duplicated, high frequency combinations for all values of  k  from the range of {1, …, max} as  k -element sentence templates. 
 Insertion of a wildcard : During combination, we combined the original word order. To improve the applicability of templates, we specified whether the elements appeared next to each other or were separated. Therefore, we placed a wildcard between all non-subsequent elements using one heuristic rule. If an absolute difference of word order assigned to the two subsequent elements of a combination &gt;1, we added a wildcard between them. An example of PSTEC algorithm appears in  Figure 2 .
 Fig. 2. The procedure for sentence template extraction using high frequency three-element combinations to generate four-element template When we obtained the high-frequency  max -element sentence templates, we applied these templates to the results of a large number of sentence embeddings. Extracting the description of the more complex phenotypes in sentences that are highly similar to the positive samples improved phenotype recognition. 2.3 Gene–phenotype relationship extraction For gene–phenotype relationship extraction, the gene is required and gene lexical features are relatively regular in texts, gene IDs or gene names may be used to represent them. Therefore, we used a dictionary- and rule-based method to identify genes. After entity recognition was complete, our pipeline extracted the relationship with the open information extraction (IE) system. Results of the relationship extraction are expressed as triplets (arg1; r; arg2). The r (relationship phrase) represents arg1 and arg2 entity relationships. 2.3.1 Gene extraction First, we searched all related genes in the UniProt database ( http://www.uniprot.org  ) using ‘ A.thaliana ’ as a key word and obtained 129 648 records. Each record contained the fields ‘Organism’, ‘Gene locus’, ‘Gene name’. Although we use  Arabidopsis  as a keyword, the results included other species, such as ‘Oryza sativa subsp. japonica (Rice)’. After processing, we obtained 89 287 Arabidopsis  gene ID and gene name pairs and these were used as a dictionary to identify genes. Due to the large number of gene names and not a gene locus in the literature, part of the gene name is not in the dictionary. Therefore, we use gene lexical rules and semantic description rules in the text to improve gene recognition. Gene name spelling had some character-level rules as follows:
 All capital letters. A combination of uppercase and lowercase letters. A combination of numbers, uppercase and lowercase letters. Those containing hyphens. Therefore, we used two rule types, mixed character-levels and contextual-levels, to identify the gene. When an input sentence contained these expressions:  Expression of, Accumulation of, Expression levels/patterns of, Targets of, mRNA abundance of, Transcript profiles/levels of , and the ‘NNP’ (Proper noun, singular) tagged parts in the part-of-speech (POS) tagged sentence complies with our character-level rules, we extracted this special expression as a gene. For example, with the POS tagged sentence: “…HTR4K27Q (‘ NNP ’) overexpression (‘ NN ’) lines (‘ NNS ’) exhibited (‘ VBD ’) deregulated (‘ JJ ’)  expression  (‘ NN ’)  of  (‘ IN ’)  H3K27me3-enriched (‘NNP’)  genes  (‘ NNS ’).” (PMID: 27926813) contains the specific contextual-level description ‘ Expression of  ’, and the ‘NNP’ tagged words satisfy the third and fourth character-level rules. Thus, we can identify gene ‘ H3K27me3-enriched ’. Then, we used all sentences that contained the phenotype as input, and the output is two entities that cooccur in sentences. These sentences were used as input to subsequent relationship extraction. 2.3.2 Relationship extraction To the best of our knowledge, there is a limited document annotation corpus of gene–phenotype relationships in  Arabidopsis  species. Currently we are only concerned with gene–phenotype relationships in single sentences. Most relationship recognition systems are not generic and portable so we used the open information extraction (OpenIE) system for this specific relationship identification. OpenIE can extract assertions from massive corpora without a specified vocabulary ( Fader  et al. , 2011 ) from open-domain corpora, such as the Internet and Wikipedia, but in recent years, OpenIE has used biological literature for systematic testing. We used an existing OpenIE system, OLLIE ( Schmitz  et al. , 2012 ) as a relationship phrase recognition tool. OLLIE improved several shortcomings of the state-of-the-art system, extracting only relationships mediated by verbs and ignoring context, extracting tuples not asserted as factual. OLLIE is popular for information extraction and used in many fields, such as Question-Answer ( Berant  et al. , 2013 ), knowledge graphs ( Nickel  et al. , 2016 ), and named entities’ network ( Tariq  et al. , 2017 ). OLLIE uses high-precision results of the previous generation OpenIE system i.e. REVERB ( Fader  et al. , 2011 ). With many syntactic analyses of sentences that contain relationships, learning relationship patterns can be extended to find relationships of new input sentences. We input the co-occurring sentences into the OLLIE system and extracted relationship sentences and their corresponding relationships. OLLIE automatically gives NP pairs of sentences as arguments in the relationship. However, these NP pairs contain too much noise, and the partially extracted arguments are not genes or phenotypes. Therefore, we limited our screening to eligible relationship groups. For the first (agr1) and third (agr2) parts of one triple, we need map them to the previous phenotype and gene entity list. When one or some genes and phenotypes are in each of the two arguments, we consider the relationship as a gene–phenotype relationship and stored such a relationship. 3 Results and discussion 3.1 Phenotype extraction results 3.1.1 Word-embedding results We used Word2Vec ( https://code.google.com/p/word2vec  ) to train a skip-gram model with a 4 D size, i.e. 300, 500, 700 and 900. Due to a lack of standards for this topic, we needed expert evaluation and annotation. Therefore, the results of word embedding first were semi-automatically classified and then manually evaluated by one expert and confirmed by another. Ultimately, the word-embedding method can extend original phenotype datasets  P , increasing 1303 new phenotype data by up to 161.86%. We used the extended dataset  P update  to match the phenotypic descriptions in the abstracts. Mapping sentences numbered 88 243. After abbreviations were identified and revised, 87 613 sentences containing phenotypes were obtained. Some examples of phenotypes recognized by the word-embedding method appear in  Table 1 . ‘Ontology term’ as the original input, using the similarity mechanism to get ‘Phenotype’ and the corresponding ‘Similarity Score’. ‘Class’ represents the corresponding categories in the PTO 10 basic categories (10 basic categories are: TO: 0000277 biochemical trait; TO: 0000283 biological process trait; TO: 0000183 other miscellaneous trait; TO: 0000357 plant growth and development trait; TO: 0000017 plant morphology trait; TO: 0000597 quality trait; TO: 0000133 stature or vigor trait; TO: 0000392 sterility or fertility trait; TO: 0000164 stress trait; TO: 0000371 yield trait).
 Table 1. Examples of word-embedding results Ontology term Phenotype Similarity Score Class  Cell elongation Cell expansion 0.671 TO: 0000357 Cell enlargement 0.531 Organ expansion 0.528 Cell proliferation 0.526  Chlorophyll content Lower ion leakage 0.625 TO: 0000277 Photosystem II activity 0.557 Photosynthetic quantum yield 0.550 Higher relative water content 0.531  Chloroplast structure Photosynthetic phenotype 0.498 TO: 0000017 Thylakoid structure 0.495 Leaf chloroplast ultrastructure 0.484 Pale green leaves 0.479  Leaf curling Dark green leaves 0.613 TO: 0000357 Altered leaf shape 0.581 Curly leaves 0.576 Serrated leaves 0.558  Drought sensitivity Reduced water loss 0.550 TO: 0000164 Enhanced drought resistance 0.544 Drought stress tolerance 0.539 Reduced drought tolerance 0.535 Note:  According to the original ‘Ontology term’, we use similarity mechanisms to extract ‘Phenotype’ and its corresponding ‘Similarity Score’. ‘Class’ represents the corresponding categories in the PTO 10 basic categories. As shown in  Table 1 , the word-embedding method can find a phenotypic description according to the syntax and context of the text. For example, for the same ontology term ‘ leaf curling ’ (TO: 0002681), the method can extract similar words by considering syntax (‘ leaf curling ’—‘ curly leaves ’) and context semantics (‘ leaf curling ’—‘ altered leaf shape ’). Some new phenotypes are not synonyms of their corresponding original ontology terms. For example, the new phenotype ‘ serrated leaves ’ is not synonymous with ‘ leaf curling ’. This may because the contextual environment that describes the new phenotype and the original term is similar, but the semantics of expression are not the same. 3.1.2 Sentence-embedding results We used Doc2Vec ( http://radimrehurek.com/gensim/models/doc2vec.html  ) to train the PV-DBOW model, and the trained corpora are positive/negative labeled abstracts. Then, we used the results of word embedding  S pos  as inputs and acquired candidate sentences with similarities greater than  Sim  after calculating for cosine distance with  S pub . A reasonable  Sim  value greatly influenced the results. After testing, if  Sim  was too high (&gt;0.4), high similarity sentences were too few and an average of 1.2 high-similarity sentences was obtained for each original sentence. If  Sim  was too low (¡0.2), we get a lot of dissimilar sentences. Therefore, we set  Sim  as 0.3, and an average of 4.5 high-similarity sentences was obtained for each original sentence. The sentence-embedding method can find many candidate phenotypic sentences, which contain many non-phrase, complex long/short phenotypic sentences. For example, the phenotypic structure ‘ response to …stress ’ in the sentence ‘GmaPHO1 genes had altered expression in  response to salt, osmotic, and inorganic phosphate stresses ’ . Such phenotypic descriptions are special and numerous and can improve relationship identification. Therefore, we designed a PSTEC algorithm to automatically generate phenotypic sentence templates for extracting them. We tested and selected template length  max  and template frequency  fre  of the PSTEC algorithm. When  max  is too long (&gt;6), the template will contain a lot of noise, such as too many prepositions and stop words. When  max  is too short (&lt;4), the template cannot contain complete template structure information. Thus, we set  max  as 5. The size of  fre  directly affects the efficiency and uptime of the algorithm. After testing, we set  fre  as 100 and only kept templates that appeared more often than 100 in the corpus. Ultimately, we obtained 250 sentence templates. There are many types of duplicate templates and high frequency but not intention-containing templates, such as  ‘Show/Suggest + prep_*’ . Therefore, we merged and selected these results.  Table 2  shows 5 high frequency sentence templates that can recognize combination type phenotypes (‘ Tolerance to salt/drought/methyl viologen stress in Arabidopsis’ ), with environmental or time factors (‘ Hypocotyl growth in response to unilateral blue-light illumination ’) and are rich in diversity of phenotypes. Meanwhile, we noticed that phenotypes recognized by different templates may differ. For example, the ‘Respond’ template can identify more ‘stress trait’ types.
 Table 2. Examples of sentence templates Sentence template Example of phenotype Number of phenotype Inhibition of + (PHE) Root growth the root-swelling phenotype; Germination and elongation of  Arabidopsis  seedling 127 Involve(d) in + (PHE) Host cell death in the hypersensitive disease-resistance response;  A. thaliana  seedling root to a rapid change in salinity 532 (Play a/an adj./n.) Role in + (PHE) Coordinate the directional growth of plant tissue; Tolerance to salt/drought/methyl viologen stress in  Arabidopsis 243 Regulator/regulation of + (PHE) Secondary wall synthesis in fiber of  A.thaliana  stem; Stomatal clustering and density early in  Arabidopsis  leaf development 197 (In) Response to + (PHE) Both high- and low-temperature stress; Signal emanate from cell undergo pathogen-induced hypersensitive cell death 215 Note : PHE represents phenotype, parentheses indicate optional parts. We can extend 1314 phenotypic descriptions using the sentence template. After merged results of word-embedding, we expanded 2409 phenotypic expressions and increased them 2.99-fold compared to the original phenotype dataset  P . 3.2 Gene–phenotype relationship results We evaluated results of gene–phenotype extraction from two perspectives.
 According to different phenotype recognition and relation extraction methods, we compared with baselines. We used the entire pipeline in the TAIR database, which manually extracted gene–phenotype relationships from 555 full papers. 3.2.1 Performance comparison with baselines Using the phenotype recognition cascaded approach can improve the identification of phenotypes in the literature and improve relationship identification. To illustrate the importance of phenotypic recognition in relationship extraction and to verify the accuracy of our approach, we establish two baselines for performance comparison.
 B1: Using the traditional ontology-based method ( Müller  et al. , 2004 ) to recognize phenotype and extracting the gene and relationship using method described in this article. B2: Using the ontology-based with word embedding method ( Mikolov  et al. , 2013 ) to recognize phenotype and extracting the gene and relationship using method described in this article. we also compare with another baseline that use traditional relation extraction methods. B3: Using method described in this article to extract phenotype and gene, the relation extraction method is based on syntatic rules ( Coulet  et al. , 2010 ) which uses the collapsed dependencies graph representation. We randomly selected 100 abstracts to identify the relationships by expert verification and to calculate Precision, Recall, F1-Measure. Results are shown in  Table 3 .
 Table 3. Performance of baselines compared with our pipeline Type Phenotype extraction Relation extraction Precision (%) Recall (%) F1-Measure (%) B1 Ontology-based ( Müller  et al. , 2004 ) OLLIE 52.98 33.76 41.24 B2 Ontology-based + word embedding ( Mikolov  et al. , 2013 ) OLLIE 73.91 50.21 59.80 B3 Representation learning approach Syntatic rules  ( Coulet  et al. , 2010 ) 55.75 26.58 36.00 Our pipeline Representation learning approach OLLIE 7 9.19 5 7.81 6 6.83 Among the different methods on phenotype recognition, the effect of recognizing the gene–phenotype relationship using only ontology-based efforts is the poorest. Because of loss of many phenotypes, recall value in relationship recognition is low. For example, the phenotype ‘ NaCl stress-sensitive phenotype ’ is not in ontology, so the relationship (MCK1; complemented;  NaCl stress-sensitive phenotype ) cannot be found. However, we can identify this phenotype using the proposed approach and obtain relationships with the best recall. This is because we recognized the phenotypic phrase and the more complex phenotypic long/short sentences based on the sentence template. As the integrity of the phenotype increased, the precision is improved. For this sentence, ‘…a structurally related Arabidopsis MADS-box gene involved in the  negative control of Arabidopsis flowering time , …’ (PMID: 15539492), due to the template: ‘ (gene) involve + {prep.} + PHE ’, we can identify the whole phenotypic description ‘ negative control of Arabidopsis flowering time ’, and get the relationship (MADS-box gene; involved in;  negative control of Arabidopsis flowering time ). However, the first two baselines only extracted part of the whole expression ‘ flowering time ’ and missed the complete relationships. Thus, our approach can extend relationship extraction by improving phenotype recognition. Compared with the B3 baselines, which only change the relation extraction method, our pipleline also has the best performance. Because the syntactic rule method misses many results and only getting 26.58% recall value, its F1-Measure is about 36.00%. We have considered to use generic tools such as GNormPlus ( Wei  et al. , 2015 ), GenNorm ( Wei and Kao, 2011 ) and so on for gene identification but found that these tools identify the gene of all species that appear in the text. Therefore, noise information is mixed in the targeted identification of  Arabidopsis  gene information, which requires expert screening. So, we finally chose a more targeted rule- and dictionary-based approach and obtained 88.76% precision value in the above test dataset. This is slightly higher than the results given in the article ( Wei  et al. , 2015 ) by GNormPlus (precision 87.1%) and GenNorm (precision 78.9%). Although the proposed pipeline can improve the effectiveness of final relationship identification compared with baselines, there are misidentifications and omissions due to the following reasons:
 Error of relationship recognition. The OLLIE system is limited as it can only identify the relationship in a single sentence, and the length of the sentence cannot be too long. Sentences &gt;20 words have increased errors for relationship analysis ( Schmitz  et al. , 2012 ). For example, the sentence ‘Hence, the narrow organ shape, reduced plant height, and reduced whorl 4 organ primordia are consistent with a general reduction of cell number, and, perhaps, reflect a role of SEU in promoting cell proliferation’ can be assessed by OLLIE to get this relationship (whorl 4 organ primordia; perhaps reflect; a role SEU in promoting cell proliferation). The wrong relationship association results in inaccurate identification of it. Inaccurate phenotypic boundary. Although we can identify phenotype from phrases and long/short sentences, more complex phenotypes cause errors or incomplete identification. For example ‘The AGAMOUS gene of Arabidopsis is necessary for the  proper development of stamens and carpels and the prevention of indeterminate growth of the floral meristem ’ . We did not recognize this sentence structure, resulting in incomplete recognition of relationships. Problem of gene recognition. Although we use a relatively complete  Arabidopsis  gene database as a dictionary for gene ID and gene name identification, and get high precition value of 88.76%, the database may still missing some gene name as well as the corresponding relationship for it. These errors reduce precision and recall because each case results in an incorrect or incomplete relationship extraction. 3.2.2 Comparison with TAIR The TAIR database ( Lamesch  et al. , 2012 ) is one of the most informative databases for storing  Arabidopsis  information, which contains a gene–phenotype relationships dataset. This information was manually extracted from 555 full texts. To verify pipeline effectiveness, we calculated coverage of relationship for these papers. Because some documents cannot be downloaded, we retrieved only 481 full papers. Preprocessing the TAIR dataset by deleting irrelevant fields, i.e. ‘Phenotype not described’ and ‘No visible phenotype’ was done and we retrieved 1397 sets of gene–phenotype relationships. We noticed that there are duplicate types of relationships in the dataset. For example, the gene ‘MSSP1’ is related with:
 Under normal growth temperature conditions, the double mutant leaves’ content in glucose and fructose is slightly reduced (30%) in a similar fashion to that observed with the tmt1 single mutants. Under normal temperature conditions, a substantial reduction in glucose and fructose contents in leaves is observed compared to wild type, and even the single tmt1 and double tmt1/tmt2 mutants. As they are the same type, we treat them as the identical relationships. We applied our pipeline to this dataset, extracted data were compared with processed TAIR datasets by four experts and offered coverage of 70.94%. Moreover, our pipeline can identify 373 new relationships, which the TAIR dataset does not include. The results are shown in  Supplementary Material . We had limited coverage for a few reasons:
 Many relationships in TAIR come from cross-sentence or even cross-paragraph relationships. Such relationships are unrecognizable to our pipeline that only extracts from a single sentence, so there is the main reason of limited coverage. However, due to redundancy of much information, our pipeline use repetitive relationships extracted from many studies to compensate extraction of the relationship representations in small samples. Such work cannot be done manually. Many phenotypes in TAIR have not been described in the original literature after subsequent manual processing and summary and this will influence coverage. There is only a gene locus name in the TAIR dataset, but most documents only describe the gene name. Some gene loci in the gene database do not have corresponding names. Thus, our pipeline cannot recognize these genes or any corresponding relationships. After analysis, we found that articles in the TAIR dataset are relatively old (most prior to 2000). Due to limitations to manual reading, this dataset failed to update gene–phenotype relationships as the literature grew, so scalability was poor. However, with our pipeline we can quickly find relationships for updated literature, greatly improving efficiency for summarizing data. 4 Conclusion and future works Much plant gene–phenotypic information exists in the biomedical literature, and it continues to grow. Thus, we propose a pipeline to extract relationships between genes and phenotypes using  A.thaliana  as an experimental object. Our pipeline can expand the expression of original phenotype ontology terms in the literature using an improved cascaded representation learning approach of phenotype recognition. This can enhance relationship extraction. Our pipeline obtained an F1-score (66.83%) that outperformed other baselines. Applying the pipeline to the TAIR dataset, we can complement 373 new relationships. Future studies may include considering environmental influences and phenotypic conditions for constructing gene–phenotype event extraction instead of binary relationships. If the division of phenotype and relationship boundaries is more detailed, performance will be improved. Funding This work was supported by National Key Research and Development Program [grant no. 2016YFD0101900], National Natural Science Foundation of China [grant no. 31701144]. 
 Conflict of Interest : none declared. Supplementary Material Supplementary Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>An optimization framework for network annotation</Title>
    <Doi>10.1093/bioinformatics/bty236</Doi>
    <Authors>Patkar Sushant, Sharan Roded</Authors>
    <Abstract/>
    <Body>1 Introduction With increased mapping of physical interactions in living cells ( Huttlin  et al. , 2017 ), we now have a blueprint of the inner workings of the cell. However, the functional interpretation of this map to simulate the behavior of the cell under different genetic and environmental cues is still beyond reach. One fundamental piece of information that is often missing is the annotation of the network interactions with direction of signal flow and functional activation/repression (sign) effects. The interpretation of the latter effects depends on the type of the physical interaction being considered. For protein–DNA interactions (PDIs), a +/– sign describes a regulatory effect; for protein–protein interactions (PPIs), it represents a functional activation/repression effect. Currently, direction and sign information is available to only a few well-studied pathways (see  Fig. 1  for an example), although a large fraction (40–70%) of the PPIs are expected to admit such an annotation ( Silberberg  et al. , 2014 ). The inference of such annotation information is a pre-condition to any logical model of a system under study [see, e.g. ( Morris  et al. , 2010 )].
 Fig. 1. The yeast signaling pathways from KEGG in one network depicting the organization of different types of physical interactions with their respective experimentally derived signs (activation/repression) and directions A pioneering work by  Yeang  et al.  (2004)  for network annotation suggested a simple yet effective logical model for signaling whereby physical interactions are directed and signed, and a signal can flow along a directed path with its effect being the aggregate effect of its member interactions, i.e. the product of their signs. To tackle the annotation challenge, Yeang  et al.  suggested a machine learning framework, but their method was limited to physical networks of small scale where it is possible to enumerate all paths. Subsequent work in this area adopted the signaling model suggested by Yeang  et al.  but employed (to the most part) combinatorial methods to learn the hidden annotations. In the most common scenario, one is given a partially annotated physical interaction network and a list of pairs of genes obtained from knockout experiments in which a knockout gene (cause, or source) affected the expression of some other gene (effect, or target). The goal is to annotate the remaining interactions in the physical network with directions and signs such that a maximum number of knockout pairs can be explained by the model. The problem of inferring interaction directions so that a maximum number of pairs admit a directed path from the cause to the effect was shown to be non-deterministic polynomial time (NP)-hard and a sub-logarithmic approximation algorithm was given for it ( Blokh  et al. , 2013 ). Using SAT solvers and integer linear programming (ILP) techniques, optimal algorithms were given for various variants of the problem ( Gitter  et al. , 2011 ;  Silverbush  et al. , 2011 ;  Silverbush and Sharan, 2014 ), potentially restricting the length of the path connecting each cause-effect pair. In addition, a machine learning based inference method was suggested by  Stelzl  et al.  (2005) . In contrast, the problem of inferring interaction signs received far less attention.  Ourfali  et al.  (2007)  considered explanatory paths of very short length (3) and provided ILP formulations to maximize the expected number of pairs that can be explained in a probabilistic network.  Peleg  et al.  (2010)  showed that the sign assignment problem is NP-hard and developed network-free algorithms for predicting genome-wide effects of gene knockouts. A related approach using regression was adopted by  Cosgrove  et al.  (2008)  to distinguish direct and indirect targets of cell perturbation.  Houri and Sharan (2012)  were the first to tackle the problem of inferring physical interaction signs on a network while accounting for paths of any length. Specifically, they searched for an assignment that maximized the number of pairs that admit a path of the required sign. They provided network reduction techniques and an ILP formulation to solve this problem to optimality on current physical interaction networks. However, their algorithm could only account for a small fraction of physical interactions in the network (low coverage), as most were contracted in their network reduction step. In this paper, we present novel network based ILP formulations for the purpose of predicting interaction signs in a physical network. The models we propose bypass the issue of network reduction and thus significantly improve the scale of predictions that can be made. In particular, we consider signaling models where a pair is explained by (i) a shortest path connecting the pair having a desired sign (ASP), (ii) a directed shortest path connecting its nodes having a pre-defined sign (AdirSP) and (iii) all shortest paths connecting its nodes having a desired sign (AllSP). We then evaluate the performance of each model in predicting physical interaction signs in yeast over two different gene expression datasets. We show that these models lead to  ≈ 15-fold higher coverage and higher accuracy than the state-of-the-art method of  Houri and Sharan (2012) . Additionally, we propose a machine learning approach for predicting interaction signs that combines features from each of these models and show that it improves over any individual model in predicting signs of previously annotated interactions. 2 Materials and methods 2.1 An optimization framework for sign prediction In this section we describe novel algorithms for inferring signs of physical interactions. We start by formally defining the problem and sketching the previous approach of ( Houri and Sharan, 2012 ). Then, we study three variants of the original problem (each describing a hypothetical signaling model) and develop novel ILP formulations to solve them to optimality on current networks. We assume we are given a (potentially partially signed) physical interaction network along with a collection of cause-effect gene pairs, such as commonly obtained from knockout experiments. The maximum sign assignment (MSA) problem is to assign signs to the unsigned edges of the network in a way that best explains the given pairs. We say that a cause-effect pair ( s ,  t ) with sign  δ st  (+ encoding down-regulation of  t  in response to the knockout of  s , − encoding up-regulation of  t  in response to the knockout of  s ) is  explained  or  satisfied  by a sign assignment, if there exists a path in the network from  s  to  t  whose aggregate sign (the product of the signs along its edges) is  δ st . Formally, MSA is defined as follows: 
 Input.  A partially signed network G(V, E) and a set of k cause-effect pairs ( s 1 , t 1 ) , … , ( s k , t k ) with signs δ s 1 t 1 , … , δ s k , t k ∈ { + , − } Goal.  A sign assignment to the unsigned edges of the network such that a maximum number of input pairs are satisfied by the assignment. 
 This problem focuses on the hypothetical A-path signaling model of  Yeang  et al.  (2004) .  Houri and Sharan (2012)  showed that due to the nature of the model, any unsigned edge that lies on a cycle in the network cannot be uniquely signed. They generalized this notion to any 2-connected component (or block) by determining if these components are  strongly signed . They then proposed an approach to reduce the input network to an acyclic one by contracting all edges in these strongly signed components without affecting the maximum number of pairs that could be satisfied. In the reduced network, every pair is connected by a unique path, facilitating the formulation of an ILP to assign signs to the unsigned edges of this path such that the number of satisfied pairs is maximized. A key drawback of this approach is that reducing the network to an acyclic one severely restricts the number of edges participating in the ILP (coverage) and, hence, restricts the number of interactions that can be uniquely signed. In subsequent paragraphs, we discuss three variants of MSA, each describing a different plausible signaling model, where edges lying on cycles may have unique signs and, hence, may no longer be contracted. The first variant we consider, A-shortest-path (ASP), considers a signaling model where the length of a satisfying path is always assumed to be the shortest possible. The shortest path assumption is motivated from the observation that signaling pathways tend to be of short length ( Silverbush and Sharan, 2014 ). For each edge  ( u , v ) ∈ E , let  x uv  = 1 denote whether its sign is − (0 if +). Similarly, we re-write the signs  δ s t ∈ { + , − }  as  δ s t ∈ { 0 , 1 } . Due to the nature of knockout experiments, there are usually much fewer sources compared to targets. Hence, for each source  s , we construct a subnetwork  G s ( V s , E s )  such that each edge in this subnetwork lies along a shortest path from  s  to one of its targets  t . This is done by applying a breadth-first-search starting from each source and target ( Silverbush and Sharan, 2014 ). Furthermore, we denote by  N s ( v )  the set of neighbors of  v  in  G s  and by  d sv  the length of the shortest path from  s  to  v . Additionally, for each pair ( s ,  v ) in  G s , we define auxiliary variables  c sv ,  r sv  where  c sv  = 0 implies that under the selected sign assignment there exists a shortest path from  s  to  v  with aggregate sign  r sv , i.e. the node pair ( s ,  v ) is  satisfied  under the selected assignment. [Note, ( s ,  s ) is trivially assumed to be satisfied]. We also define  E + , E −  which represent subsets of edges in the ILP with known prior positive and negative signs, respectively. Then the following ILP formulation can be used to solve this variant of MSA:
 max ⁡ ∑ s t y s t s . t . 1 + ∑ u ∈ { N s ( v ) | d s v = d s u + 1 } ( c s u − 1 ) ≤ c s v ∀ s , v ∈ V s ∖ s r s v = XOR ( r s u , x u v | c s v = 0 ) ∀ s , ( u , v ) ∈ { E s : d s v = d s u + 1 } c s t + y s t ≤ 1   ∀ ( s , t ) r s s = 0 , c s s = 0 , r s t = δ s t ∀ ( s , t ) x u v = 0 ∀ ( u , v ) ∈ E + x u v = 1 ∀ ( u , v ) ∈ E − y s t , x u v , r s v , c s v ∈ { 0 , 1 } ∀ s , t , u , v The XOR relation between  r sv ,  r su  and  x uv  is conditioned on the value of  c sv . That is,  r s v = r s u ⊕ x u v  only if  c sv  = 0. It is linearized as follows:
 r s v − c s v ≤ 2 − x u v − r s u r s v − c s v ≤ x u v + r s u r s v + c s v ≥ x u v − r s u r s v + c s v ≥ r s u − x u v Let  l  denote a layer of  G s  such that all nodes belonging to this layer have  d s v = l . Given a feasible solution to the ILP, if  y st  = 1 we can show that there exists a shortest path from  s  to  t  with aggregate sign  δ st . Indeed, if  y st  = 1 then  c st  = 0 by the third constraint. This implies that  ∑ u ∈ N s ( t ) | d s t = d s u + 1 ( c s u − 1 ) &lt; 0 . Thus, if  t  is in layer  l  of  G s , there must exist a neighbor  u  of  t  in layer  l  −1 such that  c su  = 0. Furthermore, if  c st  = 0,  x ut  is bound by the XOR constraint to have a sign whose product with  r su  is  δ st . Similarly, if  c su  = 0, there must be a neighbor  w  in layer  l  − 2 where  c sw  = 0 and  r s w ⊕ x w u ⊕ x u t = δ s t . By carefully investigating the constraints applicable to the subsequent layers of  G s  (i.e.  l − 3 , … , 0 ) we find that there must exist a shortest path from  s  to  t  such that the product of signs along its edges is  δ st . The final two constraints incorporate prior knowledge of signs in the ILP. The second variant we study, ‘A-directed-shortest-path’ (AdirSP), additionally assumes each shortest path explaining a pair to be directed from the cause to the effect. It is worth noting that one cannot adapt existing ILP solutions to the orientation and sign assignment problems, as both rely on reducing the input graph into an acyclic one. This reduction does not work when simultaneously optimizing both. Instead, we simply adapt the ASP formulation above to simultaneously find sign and direction assignments to the network. Specifically, we consider a pair ( s ,  t ) to be satisfied by a sign and direction assignment over the network if a directed shortest path from  s  to  t  in this assignment has aggregate sign  δ st . We call this variant of MSA the ‘A-directed-shortest-path’ (AdirSP). Let  o uv  = 1 denote whether an edge ( u ,  v ) is directed from  u  to  v  (0 if from  v  to  u ) and let the flow variables  f u v s  indicate the existence of a flow from  u  to  v . The flow variables allow computing pair reachability in a directed network. The new ILP is:
 max ⁡ ∑ s t y s t s . t . o u v + o v u = 1   ∀ ( u , v ) ∈ E f u v s ≤ ∑ w ∈ N s ( u ) ∖ v f w u s ∀ s , ( u , v ) ∈ { E s : d s v = d s u + 1 , d s u ≥ 1 } f u v s ≤ o u v ∀ s , ( u , v ) ∈ E s a u v s = ( 1 − f u v s )   OR   c s u ∀ s , ( u , v ) ∈ { E s : d s v = d s u + 1 } 1 + ∑ u ∈ N s ( v ) | d s v = d s u + 1 ( a u v s − 1 ) ≤ c s v ∀ s , v ∈ V s ∖ s r s v = XOR ( r s u , x u v | c s v = 0 , f u v s = 1 ) ∀ s , ( u , v ) ∈ { E s : d s v = d s u + 1 } c s t + y s t ≤ 1   ∀ ( s , t ) r s s = 0 , c s s = 0 , r s t = δ s t ∀ ( s , t ) x u v = 0 ∀ ( u , v ) ∈ E + x u v = 1   ∀ ( u , v ) ∈ E − y s t , x u v , o u v , a u v s , r s v , c s v , f u v s ∈ { 0 , 1 } ∀ s , t , u , v 
The first constraint ensures that each edge has a unique orientation. In some feasible solution, if  f u v s = 1 , then the second and third constraint ensure that a directed path exists from  s  to  v  containing edge ( u ,  v ). Note that the XOR relation that helps determine the sign of an edge now additionally depends on the existence of a flow in that edge. The constraint is linearized as follows:
 r s v − c s v − 1 + f u v s ≤ 2 − x u v − r s u r s v − c s v − 1 + f u v s ≤ x u v + r s u r s v + c s v + 1 − f u v s ≥ x u v − r s u r s v + c s v + 1 − f u v s ≥ r s u − x u v 
Another change from the previous formulation is the definition of auxiliary variables  a u v s  for each edge participating the ILP. Their value depends on the flow in edge ( u ,  v ) originating from  s  and on  c su . The OR relation between these variables is linearized as follows.
 a u v s ≤ ( 1 − f u v s ) + c s u a u v s ≥ 1 − f u v s a u v s ≥ c s u 
Given a feasible solution in which  y st  = 1, we show that there exists a shortest path oriented from  s  to  t  such that its aggregate sign is  δ st . Let  t  be in layer  l  of the shortest path graph  G s . If  y st  = 1, then by the seventh constraint  c st  = 0. It follows that  ∑ u ∈ N s ( t ) | d s t = d s u + 1 ( a u t s − 1 ) &lt; 0  (by constraint 5), which implies that there exists a neighbor  u  in layer  l  − 1 where  a u t s = 0 . This implies  f u t s = 1 ,  c su  = 0 (constraint 4) and  δ st  must be the product of the signs given by  x ut  and  r su  (constraint 6). Additionally,  c su  = 0 implies there exists a neighbor  w  in layer  l  − 2 where  a w u s = 0  (constraint 5). This implies  f w u s = 1 ,  c sw  = 0 and  r s w ⊕ x w u ⊕ x u t = δ s t . In this manner after carefully investigating the constraints through subsequent layers of  G s  (i.e.  l − 3 , l − 4 , … , 0 ) we can find a directed shortest path from  s  to  t  such that the product of signs along its edges is  δ st . The last two constraints account for signs that are already known. The underlying assumption in both signaling models above is that a single path is sufficient to force a pre-defined effect. However, due to the inherent stochasticity in signaling, this might not always be the case ( Ladbury and Arold, 2012 ). Moreover, on careful examination of signed interactions of  Figure 1 , we find that for any node pair connected by more than one completely signed path, the product of signs on each path is the same. Hence, we strengthen the pair satisfaction assumption in the ASP model to require that a pair ( s ,  t ) is satisfied if all shortest paths connecting  s  to  t  admit the same aggregate sign  δ st . We call this variant ‘All-shortest-paths’ (AllSP) and solve for it using the following formulation:
 max ⁡ ∑ s t y s t s . t . c s u ≤ c s v ∀ s , ( u , v ) ∈ { E s : d s v = d s u + 1 } r s v = XOR ( r s u , x u v | c s v = 0 ) ∀ s , ( u , v ) ∈ { E s : d s v = d s u + 1 } c s t + y s t ≤ 1 ∀ ( s , t ) r s s = 0 , c s s = 0 , r s t = δ s t ∀ ( s , t ) x u v = 0 ∀ ( u , v ) ∈ E + x u v = 1 ∀ ( u , v ) ∈ E − y s t , x u v , r s v , c s v ∈ { 0 , 1 } ∀ s , t , u , v 
As above, let  t  belong to layer  l  of  G s . Given a feasible solution to this new formulation, if  y s t = 1 , c s t   must   be   0  (from third constraint). Hence, for every neighbor  u  of  t  that lies in layer  l  – 1 of  G s ,  c su  = 0 (from first constraint). This in turn constrains the sign assignment of the respective edges (i.e.  r s u ⊕ x u t = δ s t , for all neighbors  u  in layer  l  − 1). By carefully investigating the constraints through subsequent layers of  G s  (i.e.  l − 2 , l − 3 , … , 0 ), it becomes apparent that for any node  v  in  G s , all shortest paths from  s  to  v  must admit the same aggregate sign ( r sv ). Hence, all shortest paths from  s  to  t  must have an aggregate sign  δ st . Notably, the models discussed above permit mathematically efficient formulations. Specifically, if  p  is the number of sources ( p ≪ k ), then each formulation contains  O ( k + p | V | + | E | )  variables and  O ( k + p ( | V | + | E | ) )  constraints. 2.2 Quantifying the activation/repression potential of a physical interaction Each of the above models may admit multiple sign assignments with optimal or near optimal scores. Hence, it is necessary to quantify the robustness of a sign assignment to an edge. To this end, we solve each ILP repeatedly  n  times; each time adding a small Gaussian noise of mean 0 and variance 0.01 to the objective function as shown below. This stochastic approach, motivated by  Hazan and Jaakkola (2012) , effectively results in a random sampling of different likely solutions that exist nearby in the optimum solution space, thereby allowing us to assess the robustness of the sign on each edge. The procedure is as follows:
 1:  procedure  G et S cores ( ILP ,  n ) 2:  scores u v ← 0 ,   ∀ ( u , v ) ∈ E  that are in  ILP 3: for  i  = 1:  n 4:    set   objective :   ∑ s t ( 1 + ɛ s t ) y s t , where  ɛ st ∼ N (0, 0.01) 5:    x * ← solve ( I L P ) 6:    scores u v = scores u v + x u v * / n ,   ∀ ( u , v ) ∈ E  that are in  ILP 7:  return  scores An edge score close to 1 implies that the sign is negative with high confidence, a score close to 0 implies a positive sign with high confidence and a score close to 0.5 implies that the sign on that edge cannot be uniquely determined (possibly implicating the absence of an activation/repression effect). For efficiency, we use  n  = 10 throughout. Our results remain qualitatively the same for larger values of  n . 3 Results 3.1 Input data We focused our analysis on budding yeast ( Saccharomyces cerevisiae ). We obtained 4095 PDIs spanning 2079 proteins (conserved across at least two other yeast species) from  MacIsaac  et al.  (2006) . We additionally downloaded 2930 high-quality experimentally verified PPIs from  Yu  et al.  (2008 ), 1361 kinase–substrate/phosphatase–substrate interactions (KPIs) among 802 proteins from  Breitkreutz  et al.  (2010) , and 189 physical interactions from signaling pathways of yeast in Kyoto Encyclopedia of Genes and Genomes (KEGG). We merged these sets into a  unified  yeast network of 8268 unique physical interactions among 3695 proteins. We extracted all 110 487 knockout pairs spanning 6228 proteins from  Reimand  et al.  (2010)  and additionally 699 771 pairs spanning 6110 proteins from  Kemmeren  et al.  (2014) . A pair was assigned a positive sign if the target gene was repressed in response to knockout of the source, and a negative sign if the target gene was activated/up-regulated. We restricted ourselves to knockout pairs such that the absolute log fold change in expression of the target gene is &gt;2 and FDR &lt; 0.001. This leaves us with 1756 significant knockout pairs from  Reimand  et al.  (2010) , referred to here as the  Reimand set , and 3524 significant knockout pairs from  Kemmeren  et al.  (2014) , referred to here as the  Kemmeren set . The above choice of thresholds was made while taking into consideration the inherent computational complexity of the problem. 3.2 Validation data For a systematic validation of our sign prediction models we collected sign information as follows. Only 147 of 192 physical interactions in yeast had an experimentally confirmed sign from KEGG (See  Fig. 1 ). In addition, following  Houri and Sharan (2012) , we extracted gene ontology (GO) molecular function annotations related to transcriptional activators (GO: 0045893) and transcriptional repressors (GO: 0045892). PDIs originating from transcriptional activators were given a positive sign whereas PDIs originating from transcriptional repressors were given a negative sign. Finally, we also extracted information on protein kinases (GO: 0004672) and protein phosphatases (GO: 0004721). We reasoned that since there are roughly three times as many confirmed functionally activating phophorylation sites compared to repressive ones (PhosphoNET database,  www.phosphonet.ca ), and that 71% of phosphorylation interactions of yeast in KEGG are annotated as activating and 81% of de-phosphorylation interactions of yeast are annotated as repressing, kinase–substrate interactions tend to be activating while phosphatase–substrate interactions tend to be repressing. Thus, physical interactions linking a GO annotated kinase and a substrate were given a positive sign whereas interactions linking a GO annotated phosphatase to a substrate were given a negative sign. Any interaction in the unified network that had conflicting signs was left unsigned (unless it had sign information from KEGG, in which case this latter information was used). In summary, the validation set consists of three groups of signed interactions in the network: (i) 2014 (1131+, 883−) signed PDIs, (ii) 1044 (872+, 172−) signed kinase/phosphatase–substrate interactions and (iii) 147 (96+, 51−) signed KEGG interactions. 3.3 Performance evaluation of individual models We evaluated each of the four models presented above (A-path/ASP/AdirSP/AllSP) in a 5-fold cross-validation setting on the unified yeast network, focusing on the interactions covered by each model, i.e. participating in the corresponding ILP. To this end, we randomly divided all signed and covered interactions into five equal parts. Using each model, we predicted the activation/repression potential of the interactions in each part (see Section 2.2) while constraining the signs of interactions in the remaining parts. Then we measured the performance of the activation/repression scores of a given model across the five parts for different subsets of signed interactions covered by the model. For each subset, we denote its set of covered positive and negative interactions by  E +  and  E − , respectively. As a benchmark, we discuss the performance of the previous A-path model. Recall that in this model we should contract all interactions that lie in a strongly signed block of size  ≥ 3  (see Section 2.1). Since all blocks were strongly signed, this resulted in an acyclic network with 77% of the interactions contracted. When working with knockout pairs from the Reimand set, we observe that only 1% of all the network interactions participate in the ILP constraints due to network reduction, and 25 of them belong to the validation set. Due to low coverage over the validation set, we instead evaluated this framework using knockout pairs from the Kemmeren set. Overall, 4% of network interactions are covered in this instance and 73 interactions from the validation set were part of the ILP formulation, yielding an AUC of 0.66. Since there were only 73 interactions to validate our predictions, we could not evaluate the performance on individual subsets. Next, we evaluated the ASP, AdirSP and AllSP models over the unified network.  Tables 1  and  2  summarize the performance over the validation PDIs, KPIs and the KEGG interactions. We find that our new formulations lead to sign assignments on 35% of network interactions when working with the Reimand set and 59% of network interactions when working with the Kemmeren set;  ≈ 15-fold coverage increase compared to previous work (A-path).
 Table 1. Performance evaluation using the Reimand set (coverage of 35%) Interaction | E + | , | E − | AUC AUC AUC (ASP) (AdirSP) (AllSP) PDI 435, 458 0.75 0.63 0.84 KPI 205, 20 0.83 0.56 0.72 KEGG 40, 27 0.56 0.52 0.65 Table 2. Performance evaluation using the Kemmeren set (coverage of 59%) Interaction | E + | , | E − | AUC AUC AUC (ASP) (AdirSP) (AllSP) PDI 744, 653 0.63 0.59 0.83 KPI 522, 98 0.61 0.51 0.77 KEGG 46, 32 0.58 0.54 0.71 In order to directly compare the performance of the A-path model to our suggested alternative models, we evaluated them on the restricted validation set of 73 interactions covered by the A-path model. On this set ( | E + | = 49 , | E − | = 24 ) the performance of AdirSP was lower to A-path (AUC of 0.64), while ASP and AllSP had better performance (AUCs of 0.73 and 0.68, respectively). 3.4 Performance evaluation of the combined model Previous work as well as our models above vary in the assumptions they make on the way a knockout effect is explained, going all the way from requiring a single path of any length to requiring all paths of shortest length. Note that we adopt these models partly because they are grounded in our very own observations of cellular signaling pathways (see Section 2.1) and because they permit an efficient mathematical formulation. These descriptions are not perfect. In turn, the solution of each model allows different degrees of freedom on the signs of underlying interactions. To make the best inference possible for each physical interaction given the complex nature of cellular signaling, we integrate the predictions of each model in an ensemble. That is, using the sign scores from solutions to ASP, AdirSP and AllSP as features, we train a hybrid model, specifically a random forest classifier, that makes an overall prediction of the sign of an interaction (A-path was excluded due to low coverage). The ensemble model is evaluated via nested cross-validation. In detail, the validation set is divided into the same five parts as above. Four of the parts are used for training the individual models to score the fifth part. Next, we perform a 5-fold cross-validation on the fifth part to train and test the classifier. Finally, using the cross-validated predictions across all parts, we report the mean classifier performance (AUC) against the signs of different validation subsets.  Tables 3  and  4  summarize the performance of the random forest classifier on the different knockout sets and validation subsets.
 Table 3. Performance evaluation of the random forest classifier using the Reimand set Interaction | E + | , | E − | AUC (classifier) PDI 435, 458 0.86 KPI 205, 20 0.85 KEGG 40, 27 0.77 Table 4. Performance evaluation of the random forest classifier using the Kemmeren set Interaction | E + | , | E − | AUC (classifier) PDI 744, 653 0.80 KPI 522, 98 0.67 KEGG 46, 32 0.81 The performances of the classifier and the individual models are depicted in  Figures 2  and  3 . Importantly, we observe that the classifier outperforms all individual models on the set of curated interactions from KEGG. It also outperforms the different models with respect to PDIs and KPIs on the Reimand set. The lower performance of the classifier on the KPI set (compared with the AllSP model) when working with the Kemmeren set is likely an artifact resulting from the skewed distribution of class labels. Such a skew may influence ensemble classifier performance on unseen data. ( Galar  et al. , 2012 ).
 Fig. 2. Performance evaluation of all models using the Reimand set Fig. 3. Performance evaluation of all models using the Kemmeren set 4 Conclusions In summary, we developed novel ILP formulations for predicting signs of physical interactions under different signaling models. We discussed the underlying assumptions guiding the predictions of each model and its advantages in terms of coverage relative to prior work by  Houri and Sharan (2012) . We then measured the cross-validation accuracy of our models in predicting signs across two knockout datasets to find that our models lead to improvement in accuracy and coverage over the previous state-of-the art method by  Houri and Sharan (2012) . Finally, we derive a hybrid signaling model based classifier that uses the sign assignment confidence scores of each model studied for predicting interaction signs. This was partly motivated by the fact that the three models presented in the paper, although mathematically efficient to represent, may be insufficient to capture the complex nature of cell signaling. Furthermore, this warrants the exploration of other plausible models that could be potentially integrated into the classifier to improve its predictions. For instance, one could additionally formulate an ILP that considers All-Directed Shortest paths (All-dirSP) as a plausible signaling mechanism to explain some of the knockout effects. Notably, the AdirSP model is the first to combine both direction and sign prediction within the same optimization framework and may be of independent interest to readers to expand on for the general problem of annotating a network with directions and signs. A potential limitation of our approach is the computational cost involved in solving the models (especially if the network is sparsely annotated with signs). In this work, running the AllSP model on the Kemmeren set took the most time (up to 3 h to obtain a single solution). In all other cases it took on average 3 min to obtain a single solution (note that these values may differ based on the solver and the computer used). A theoretical comparison of efficiency can be done by analytically reasoning about the size of the search space for each model. The ASP and AdirSP models require  O ( d ) XOR constraints to be satisfied to explain a single knockout pair ( s ,  t ) (where  d  denotes the shortest path distance from  s  to  t ). In contrast, the AllSP model requires a much larger number of  O ( | E | )  XOR constraints to be satisfied to explain a single knockout effect (where  | E |  denotes the number of edges in the network). Each constraint introduces at least one new Boolean variable and hence the search space for AllSP is much larger than that for ASP or AdirSP. This might pose a major limitation when it comes to predicting physical interaction signs over much larger networks, e.g. the human network, which still remains mostly unsigned. To enhance the scalability of our methods, one can pre-process the network and either contract interactions involved in protein complexes or pre-annotate them with positive signs (which are logically consistent with their role as a means to propagate signal forward without influencing the overall effect of the path taken). This vastly cuts down the size of the solution space; by half per interaction. The different models we proposed represent a trade-off between model coverage (of networks edges) and complexity (of its solution). Another limitation of our results is that they are based on two approximate sources of sign information, namely KPI signs derived from whether the source protein is a kinase or a phosphatase and PDI signs derived from whether the involved transcription factor is an activator or a repressor. We expect our models to yield more accurate results as better quality sign information becomes available. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Recognition of beta-structural motifs using hidden Markov models trained with simulated evolution</Title>
    <Doi>10.1093/bioinformatics/btq199</Doi>
    <Authors>Kumar Anoop, Cowen Lenore</Authors>
    <Abstract>Motivation: One of the most successful methods to date for recognizing protein sequences that are evolutionarily related, has been profile hidden Markov models. However, these models do not capture pairwise statistical preferences of residues that are hydrogen bonded in β-sheets. We thus explore methods for incorporating pairwise dependencies into these models.</Abstract>
    <Body>1 INTRODUCTION Profile hidden Markov models (HMMs) have been one of the most successful methods to date for recognizing both close and distant homologs of given protein sequences. Popular HMM methods such as HMMER (Eddy  et al. ,  1998a ,  b ) and SAM (Hughey and Krogh,  1996 ) have been behind the design of databases such as Pfam (Finn  et al. ,  2006 ), PROSITE (Hulo  et al. ,  2006 ) and SUPERFAMIILY (Wilson  et al. ,  2007 ). However, a limitation of these HMMs is, since there is only finite state information about the sequence that can be held in any particular position, HMMs cannot capture dependencies that are far, and variable distance apart, in sequence. On the other hand, in β-structural motifs, as was noticed by Lifson, Sander and others (Hubbard and Park,  1995 ; Lifson and Sander,  1980 ; Olmea  et al. ,  1999 ; Steward and Thornton,  2002 ; Zhu and Braun, 1995), amino acid residues that are hydrogen bonded in β-sheets exhibit strong pairwise statistical dependencies. These residues, however, can be far away and a variable distance apart in sequence, making them impossible to capture in an HMM. Early work of Bradley  et al.  (Bradley  et al. ,  2001 ; Cowen  et al. ,  2002 ) show that these pairwise correlations help to recognize protein sequences that fold into the right-handed parallel β-helix fold. More recent work has used a conditional random field or Markov random field framework, both of which generalize HMMs beyond linear dependencies, to identify the right-handed parallel β-helix fold (Liu  et al. ,  2009 ), the leucine rich repeat fold (Liu  et al. ,  2009 ) and the β-propeller folds (Menke  et al. ,  2010 ). While these conditional random field and Markov random field models are extremely powerful in theory, in practice, substantial computational barriers remain for template construction, training and computing the minimum energy threading of an unknown sequence onto a template. Thus, a general structure software tool designed for β-structural folds, in the same manner as HMMER and SAM packages recognize all protein structural folds, remains a challenging unsolved problem. In this article, we find an unusual and different way to incorporate pairwise dependencies into profile HMM. In particular, we generalize our recent work (Kumar and Cowen,  2009 ) on augmenting HMM training data to include these very pairwise dependencies as a part of a larger training set (see below). While this method of incorporating pairwise dependencies is undoubtedly less powerful than MRF methods, it has the advantage of being simple to implement, computationally fast and allows the modular application of existing HMM software packages. We show that our augmented HMMs perform better than ordinary HMMs on the task of recognizing β-structural SCOP (Lo Conto  et al. , 2002) protein superfamilies. In particular, we consider the problem of how well an HMM trained on only one family β-structural SCOP superfamily can learn to recognize members of other SCOP families in that SCOP superfamily, as compared to decoys. We show a median AUC improvement of nearly 5% for our approach compared to ordinary HMMs on this task. 2 APPROACH Our approach is based on the simulated evolution paradigm introduced in Kumar and Cowen ( 2009 ). The possibility that motif recognition methods could be improved with the addition of artificial training sequences had been previously suggested in the protein design community (Koehl and Levitt,  1999 ), though the methods of Koehl and Levitt ( 1999 ); Larson  et al.  ( 2003 ) and Am Busch  et al.  ( 2009 ) to generate these sequences are much more computationally intensive than the simple sequence-based mutation model of Kumar and Cowen. In particular, Kumar and Cowen created new training sequences by artificially adding point mutations to the original sequences in the training set, using the BLOSUM62 matrix (Eddy,  2004 ). The HMM training was then used on this larger, augmented training set unchanged. In this article, we compare ordinary HMMER Profile HMMs, HMMER Profile HMMs augmented with a point mutation model (similar to Kumar and Cowen,  2009 ), and HMMs augmented with training sequences based on pairwise dependencies of β-sheet hydrogen bonding (see  Fig. 1 ). Thus we have generalized the single frequency approach of Kumar and Cowen ( 2009 ), to pairwise probabilities. More specifically, to create our new training sequence based on β-strand constrained evolution, the following pipeline is followed:
 The input to HMM training is a set of PDB files for sequences that lie in the same SCOP family. The sequences are aligned by way of multiple structure alignment program. Positions corresponding to paired residues that hydrogen bond in adjacent β-strands are found using SmurfParse package. For each sequence that lies in the original training set, additional sequences are added to the training set using random mutations according to a probability distribution based on the paired positions within β-strands, as described below. The multiple sequence alignment, including sequences in the original training set as well as the new sequences generated by simulated evolution, is passed to the ordinary HMM training module. 
This pipeline is illustrated in  Figure 1 B, along with HMM-C, an approach that combines both point mutations and pairwise mutations in the training set.
 Fig. 1. Training HMMs by ( A ) a pointwise mutation model, ( B ) a pairwise mutation model and ( C ) combining (A and B). We use these augmented HMMs to solve the following task: trained only on the sequences from single SCOP family can our HMMs distinguish between the following two classes: (i) sequences from other SCOP families in the same SCOP superfamily as the training set and (ii) decoy sequences that lie outside the fold class of the family of the training set. 3 METHOD 3.1 Datasets We employed an approach similar to that of Wistrand and Sonnhammer ( 2004 ) to pick SCOP families and superfamilies from among those that belong to the ‘mainly beta proteins’ class in SCOP and train HMMs. First, we chose sequences from SCOP that are &lt;95% identical based on the ASTRAL database version 1.73 (Chandonia  et al. ,  2004 ). The dataset was then filtered to include only the SCOP families that belonged to ‘mainly beta proteins’ class and had at least 10 sequences. Another constraint imposed in order to have test sets was to make sure that other SCOP families in the superfamily hierarchy had at least one sequence but not more than 50 sequences. Our test set consisted of all the sequences from the rest of the families in the superfamily and an equal number of decoy sequences chosen at random from different SCOP folds. The dataset is available at:  http://bcb.cs.tufts.edu/pairwise/ . 3.2 Multiple sequence alignment This is the process of aligning the homologous residues in protein sequences into columns and thus generating a multiple sequence alignment (MSA). 3.2.1 Aligning sequences with MUSCLE For the single frequency augmented training model, we used the popular program MUSCLE Version 4 (Edgar,  2004 ) to generate the MSA that was provided to the HMM training methods. It is one of the fastest programs available and produces global sequence alignments for the set of sequences from a family. We developed a script to transform the MUSCLE alignment output to .ssi (STOCKHOLM) format since other MUSCLE output formats are not supported by HMMER 3.0a2. 3.2.2 Aligning sequences with Matt For the pairwise augmented training model, and the hybrid model, we used multiple alignment with translations and twists (Matt) (Menke  et al. ,  2008 ) to align the sequences based on the structure. By allowing local flexibility and allowing small translations and rotations, Matt demonstrates an ability to better align the ends of α-helices and β-strands. We used Matt in default configuration for aligning the sequences in a family. Alignment based on structure is essential to determine the location of β-strands in the sequences and thus augment the dataset based on conserved residue pairs in β-strands. 3.3 Mutation models 3.3.1 Simple mutation model We used the BLOSUM62 matrix as our simple model of evolutionary mutations (Eddy,  2004 ). Mutations in a sequence are added by randomly picking a position in the sequence and the replacing the amino acid in that position with a new amino acid based on the BLOSUM62 probability until a desired threshold of  s % mutations is reached. For each training sequence,  N  new mutated sequences with  s % mutations are created and added to the training set. Therefore a family with 100 sequences will have 100 +  N  × 100 (100 original +  N  × 100 mutated) sequences in the training set. In this study, we create training sets with 5, 10, 15, 20 and 25% mutations per the length of sequence and tested several values of  N  ranging from 10–1000. We picked a value of  N  at the 20% mutation rate for which the results were stable (see  Section 3.5 ). 3.3.2 β-Strand mutation model In this step we augment the MSA with a set of sequences that are produced by mutating the original sequences in such a way that the frequency of pairs of amino acids hydrogen bonded in β-sheets resembles the frequency observed in known protein fold space. We use the pairwise conditional probability frequency tables from the recent paper of Menke  et al.  ( 2010 ). There are two tables, representing the in–out residue positions, respectively, for β-sheets that have one side buried and one side exposed to solvent. The tables were learned from solved protein structures in the PDB. β−Strands in the aligned set of structures are found by the program SmurfPreparse which is part of the Smurf Package (Menke,  2009 ; Menke  et al. ,  2010 ). The program not only outputs the positions of the consensus β-strands in the alignment, it also declares a position buried or exposed based on which of the two tables is the best fit to the amino acids that appear in that position in the training data. For each sequence in the training set,  M  mutated sequences with  p % mutations are created and added to the training set. Here ‘ p ’ is set not to be proportional to the total length of the entire sequence, but instead to the total length of the β-strand positions in the alignment. New sequences are created as follows. Residue positions contained in β-strands are selected uniformly at random. If position ‘ i ’ is selected, its pair residue ‘ j ’ is found (note that  j  may appear before or after ‘ i ’ in sequence) and  i  is mutated according to the appropriate pairwise table, conditioned on it being hydrogen bonded to the residue of type in position ‘ j ’. This process is repeated  p  times and the resulting sequence is added to the augmented training set. At the end of this process, for example, a family with 100 original sequences in the training set will have 100 +  M  × 100 (100 original +  M  × 100 mutated) sequences in the augmented training set. In this study, we set values of  p  that would result in training sets with 10–100% mutations ( note : we allow sites to mutate more than once, for example some of the positions even a sequence with a 100% mutation rate may not end up mutated) and tested multiple values of  M  ranging from 10–1000. We picked a value of  M  at which the results were stable at the 20% mutation rate (see  Section 3.5 ). 3.4 Building the HMM In our approach, the primary steps in building the HMM remain the same except the training set is augmented with mutated sequences based on the two evolutionary models. The process is shown in  Figure 1 . Two packages are widely adopted to work width profile HMMs: SAM (Hughey and Krogh,  1996 ) and HMMER (Eddy,  1998a ,  b ). SAM has been demonstrated to be more sensitive overall, while HMMER's model scoring is more accurate (Wistrand and Sonnhammer,  2004 ). In this study we use HMMER versions 3.0a2 to evaluate the models of protein families as it is freely available and can be easily downloaded from the website. We construct HMMs from the MSAs using the  hmmbuild  program which is part of the HMMER package. In this approach, the model of the HMM is made up of a linear set of match ( M ) states, one per consensus column in the MSA. Each  M  state emits a single residue, with a probability score that is determined by the frequency that residues have been observed in the corresponding column of the MSA. Each match state therefore carries a vector of 20 probabilities, for scoring the 20 amino acids. The HMMs also model the gapped alignments by including insertion (I) and deletion (D) states in between the match states. The match, insertion and deletion states are connected by the transition probabilities. In our experiment, HMMER is used as a black box except the constraints on choosing match states are made tighter. Using default settings, HMMER creates a match state whenever a column in the MSA has &lt;50% gaps. We found empirically in Kumar and Cowen ( 2009 ) that the default cutoff was not optimal for our datasets because homology was too remote, and creating a column whenever there are &lt;20% gaps yielded the best HMMs on our datasets. Thus we duplicate this threshold in the current study. By default, HMMER uses a maximum a posteriori (MAP) architecture algorithm to find the model architecture with the highest posterior probability for the alignment data. The algorithm is guaranteed to find a model and constructs the model by assuming that the MSA is correct and then marks columns that correspond to match states. An HMM is created for every MSA, thus there is a one to one correspondence between an MSA and an HMM, generating a library of HMMs. Therefore, for any sequence from the MSA, the HMM can be used to determine if it belongs to the MSA. In addition, the HMM can be used to check if a new sequence is similar to the sequences in the MSA and if it is then one can place the new protein in the same family. We used the default ‘glocal’ setting to construct the models which are global with respect to model and find multiple hit local with respect to sequence. In order to reduce the skewness in the distribution of sequences used to construct an HMM, HMMER supports several options to weight the sequences in training data. The default option GSC assigns lower weights to sequences that are over-represented (Gerstein  et al. ,  1994 ). In addition, HMMER supports external and internal sequence weighting strategies based on information theoretic principles. Based on our study of different sequence weighting options for HMMs with and without the point mutation augmented training for the task of learning SCOP superfamilies (Kumar and Cowen,  2009 ) we used SAM sequence entropy (Karplus  et al. ,  1998 ) throughout the present study. 3.5 HMM scoring Once an HMM is build from an MSA, a new sequence can be scored by the HMM. The score ( S ) is the log of the probability of observing the sequence from a HMM divided by the probability of observing the same sequence from the ‘null hypothesis’ model or HMM.
 P (seq|HMM) is the probability of the target sequence according to a HMM and  P (seq|null) is the probability of the target sequence given a ‘null hypothesis’ model of the statistics of random sequence. In HMMER, this null model is a simple one-state HMM that says that random sequences are independently and identically distributed sequences with a specific residue composition. In addition, HMMER also generates an  E -value which is the expected number of false positives with a score as high as the hit sequence. While the log odd scores ( S ) provides information on the quality of a hit, the  E -value gives a measure relative to other sequences. Therefore a lower  E -value implies that the sequence matches more closely to the HMM. After constructing an HMM, a cutoff for the score ( S ), or  E -value, is set. A new sequence that lies within the cutoff is said to belong to the family that is associated with the HMM. Thus by varying the cutoff, the true positive and false positive rates of the classifier can be tuned. We run experiments over a range of cutoffs to generate receiver operating characteristics (ROC) plots that graph the tradeoffs of the true and false positives, as the cutoffs are tuned. We also compute the area under the ROC curve (AUC) to summarize the classifier statistic in a single number (Sonego  et al. ,  2008 ). We also use average errors at minimum error point (MEP) statistics to assess the performance of HMMs. An MEP is the score threshold at which the classifier makes fewest errors of both kinds, i.e. false positives and false negatives (Karchin  et al ,  2002 ). The percentage of both types of errors provides a comparison of both sensitivity and specificity. 3.6 HMM stability Because our method for augmenting the training data is randomized there is a legitimate concern that any reported result might vary each time the algorithm is run. While results will in fact vary, in fact the variation decreases as  N  and  M  grow larger. We refer to the variation between different runs of the algorithm as the  stability  of the procedure and we empirically experimented with different values of  N  and  M  in order to ensure sufficiently consistent results. We augmented the training set with 10, 50, 100, 200, 500 and 1000 mutated sequences for each original sequence in the training set, for both pointwise and pairwise mutation models. We generated the augmented training set 40 times at 20% mutation rate for each protein family in our training set with a different random seed and constructed the HMMs as described above. For each HMM, we computed the MEP for each iteration.  Figure 2  shows the variation in the SD of the MEP for the single mutation model, and  Figure 3  shows the variation in the SD of the MEP for the pairwise mutation model. Based on these results we set  N  and  M  to each be 150 in this artilcle.
 Fig. 2. Variation in SD of MEP for HMM training augmented with 10–100 sequences based on the point mutation model. 
 Fig. 3. Variation in SD of MEP for HMM training augmented with 10–1000 sequences based on pairwise β-sheet mutation model. 4 RESULTS As described in  Section 3.1 , our dataset consisted of the 41 SCOP families from the ‘mainly beta’ section of SCOP hierarchy, each of which had at least 10 structures, after filtering at the 95% sequence identity level and for which between 1 and 50 sequences in their associated SCOP superfamily but outside the SCOP family existed. In each of the 41 cases, the training set was derived from the training sequences from the SCOP family, and the test set consisted of the sequences outside the SCOP family from the same SCOP superfamily (the positive examples) as well as an equal number of decoy sequences chosen randomly from outside the associated SCOP fold (the negative examples). For each SCOP family in the training set, we trained an ordinary HMM model and plotted the ROC curve and calculated the AUC for this task. Over all 41 families, the median AUC was 69%. We then augmented the training set with the point mutation model ( Fig. 4 ), our new pairwise β-sheet model ( Fig. 5 ) and using training sequences generated from both models simultaneously ( Fig. 6 ).  Figure 4  displays how the median AUC varies with the pointwise mutation rate. Similar to Kumar and Cowen,  2009 , the median AUC improves by training set augmented with simulated evolution all the way up to just above a 15% mutation rate, which gives a median AUC improvement of 3.72%. When we look at the same statistics for the pairwise mutation model in  Figure 5 , the results are less linear with a peak 3.94% improvement at the 10% mutation rate and maximum median AUC improvement of 4.79%. Combining both types of augmented data it is the first peak of pairwise mutations combined with pointwise mutation rate of 15% that give the maximum median AUC for our experiment, an AUC improvement of 4.95%. However, the variance in different runs of this randomized procedure might mean that the best setting is sometimes here and sometimes closer to the second highest peak in  Figure 6  (around 50% pairwise mutations).
 Fig. 4. Median percent AUC improvement with mutation rate for HMMs trained with pointwise mutations. The maximum median improvement is 3.72% at 15% mutation rate. 
 Fig. 5. Median percent AUC improvement with mutation rate for HMMs trained SAM with pairwise mutations. The maximum median improvement is 4.79% at 50% mutation rate. 
 Fig. 6. Median percent AUC improvement with mutation rate for HMMs trained with and dataset augmented with combined pointwise and pairwise mutations. The maximum median improvement is 4.95% at pairwise mutation rate of 10% and pointwise mutation rate of 15%. Finally in  Figures 7  (pointwise) and  8  (pairwise), we break down the increase and decrease in AUC as a function of mutation rate family by family. Most families show some positive increase in AUC in all augmented training models, but for around a fifth of the families performance degrades for the pointwise mutation model. The non-linearity in the median AUC as a function of mutation rate in the pairwise mutation model is partially explained by examining the proportion of families where performance improves versus degrades in  Figure 8 . In particular, performance degrades for &lt;25% of the families at the 10% pairwise mutation rate, but this jumps up to 25% or more thereafter. Meanwhile the families where AUC improves with pairwise mutations shows a peak improvement level between 40 and 50% mutation rate.
 Fig. 7. Distribution of families with improved performance for pointwise mutation model. 
 Fig. 8. Distribution of families with improved performance for pairwise mutation model. It would be nice if there was a biological characterization of what families will have improved versus degraded AUC with pairwise mutated augmented training data. However, in this study, the biological variation is almost certainly swamped by the variation we see due to the different extent varying families within the same superfamily are represented among solved structures in the PDB and hence the size and diversity of our test sets, as well as the difficulty of the different random decoy structures that were chosen when we constructed our datasets. Although the present study cannot therefore address exactly how to tune mutation rate parameters on a per family level, it is clear from our results that out pairwise mutation model is successful in improving the detection of remote homologs of β-structural motifs. While we cannot make any strong conclusions, we did find, as a general rule, that the pairwise mutations helped the most when there was the smallest diversity in the training sequences at a family level, that is, when there were the fewest number of known families for a given superfamily. 5 DISCUSSION We have shown how pairwise dependencies in β-sheets can be incorporated into an augmented HMM training set using simulated evolution, resulting in improved recognition of β-structural motifs. Our datasets, augmented training sets, and our HMMs are all available online at  http://bcb.cs.tufts.edu/pairwise/ . In the present work, it was assumed that the structural information was available for sequences in the training set; thus structural information was used to construct the multiple sequence alignment, to locate β-strands, and to determine how the β-strands were hydrogen bonded into β-sheets. However, ordinary HMMs and our earlier, simpler, point mutation model of simulated evolution require only sequence information, not structure. Extending our work to the case where no solved protein strcuture is known is an interesting open question. Secondary-structure prediction programs (Rost,  2001 ) could be used to find β-strands, but determining how they are paired and hydrogen bonded is a much more difficult issue. Computationally predicting how β-strands are paired in the absence of structural information is a well-studied problem since 1995 (Cheng and Baldi,  2006 ; Hubbard and Park,  1995 ; Jeong  et al. ,  2007 ; Steward and Thornton,  2002 ; Zhu and Braun,  1999 ). Recent work that has tried to computationally model transmembrane β-barels (Waldispuhl  et al. ,  2008 ) and β-amyloids (Bryan  et al. ,  2009 ) without a structural template may also be relevant. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>An evolutionary model motivated by physicochemical properties of amino acids reveals variation among proteins</Title>
    <Doi>10.1093/bioinformatics/bty261</Doi>
    <Authors>Braun Edward L</Authors>
    <Abstract/>
    <Body>1 Introduction Many studies have examined the role of models in phylogenetic estimation using maximum likelihood (ML). Most studies have focused on tree topology (e.g.  Hoff  et al. , 2016 ), but the estimates of model parameters also have the potential to provide biological insights. For example, early studies revealed a bias toward transition (rather than transversion) substitutions ( Yang, 1994 ) and later studies examined neighboring-nucleotide effects ( Hwang and Green, 2004 ) and strand asymmetries ( Polak and Arndt, 2008 ). The ratio of non-synonymous to synonymous substitutions in coding regions (called  K A / K S  or  d N / d S ) is the most common use of a model parameter for inference in molecular evolution; the  K A / K S  ratio can be estimated using models of codon evolution ( Yang, 1998 ;  Yang and Nielsen, 2002 ). Codon models are used in many studies, sometimes at the whole-genome scale (e.g.  Weber  et al. , 2014 ;  Zhang  et al ., 2014 ). In contrast to models that use nucleotide multiple sequence alignments (MSAs), either coding or non-coding, there is a relative paucity of methods to conduct similar analyses of protein MSAs. In principle, the ratio of ‘radical’ to ‘conservative’ amino acid substitutions ( K R / K C ) could be used in a manner similar to the  K A / K S  ratio ( Hanada  et al. , 2007 ;  Hanada  et al. , 2009 ;  Smith, 2003 ;  Zhang, 2000 ), although the  K R / K C  ratio is harder to interpret than the  K A / K S  ratio. There are two challenges associated with using ML methods to understand patterns of protein evolution. First, there are many ways to define radical versus conservative amino acid substitutions ( Hanada  et al. , 2007 ), unlike non-synonymous versus synonymous substitutions, which can be defined unambiguously. Second, estimates of the  K R / K C  ratio will ultimately reflect the estimates of parameters in the instantaneous rate matrix (IRM), or  Q  matrix, which describes amino acid evolution for specific proteins. However, amino acid models have an IRM with a much larger number of free parameters than models of nucleotide sequence evolution. If we assume time reversibility the IRM, which is used to calculate the likelihood, can be is the product of a symmetric rate matrix ( R ) reflecting the ‘exchangeability’ for specific pairs of character states (i.e. nucleotides, codons or amino acids) and a diagonal matrix  (Π ) with the equilibrium frequencies of each state ( Swofford  et al. , 1996 ). The general time reversible model of nucleotide evolution (GTR 4 ) has eight free parameters (five for  R  and three for  Π ), so the variance of the parameter estimates will be acceptable if they are estimated using typical nucleotide MSAs. The analogous amino acid model (GTR 20 ) has 208 free parameters (189 for  R  and 19 for  Π ). Individual proteins are often fairly short (e.g. 280–600 amino acids;  Tiessen  et al. , 2012 ) so typical protein MSAs are unlikely to provide enough information to generate accurate estimates of that many free parameters. This raises the question of how a codon model can be implemented in a practical manner. After all, GTR 61  is the analogous codon model (assuming the universal code); that model has a very large number of free parameters (1829 for  R  and 60 for  Π ). However, the dimension of codon models can be reduced using an IRM where all elements that require multiple simultaneous substitutions are set to zero and the remaining elements are assigned values based on a single transition–transversion ratio and  K A / K S  ratio (the κ and ω parameters, respectively, in the study by  Yang, 1998 ). This dimension reduction actually reveals valuable biological information because estimates of  K A / K S  are easier to interpret than the collection of values in the IRM. An analogous approach for models of amino acid evolution would be useful. Most phylogenetic studies that use proteins eschew estimation of the  R  matrix parameters using a fixed  R  matrix generated using a training set of protein MSAs. This approach was pioneered by  Kishino  et al.  (1990) , who used  Dayhoff  et al . (1978)  PAM matrix as the  R  matrix, and it solves the problem of parameter estimation as long as the training set is large enough. Subsequent studies have used other  R  matrices ( Table 1 ). Although these ‘empirical models’ with fixed  R  matrices may be useful for phylogenetics they cannot provide insights into the process of protein evolution. For example,  Keane  et al.  (2006)  reported that rtREV is the best-fitting model for 33% of archaeal, 21% of proteobacterial and 4% of vertebrate proteins MSAs. However, rtREV was trained using retroviral  pol  proteins ( Dimmic  et al. , 2002 ) so it is unclear why diverse archaeal proteins would fit rtREV better than the more general models trained on a diverse set of proteins. This observation raises a fundamental question: does identifying the best-fitting model provide any useful information about specific proteins? That question can only be answered in the negative if we focus on empirical models.
 Table 1. Empirical models of protein sequence evolution Model Training data References General models:  JTT — Jones  et al.  (1992)  LG — Le and Gascuel (2008)  PAM (Dayhoff) — Dayhoff  et al.  (1978)  PMB — Veerassamy  et al.  (2003)  VT — Müller and Vingron (2000)  WAG — Whelan and Goldman (2001) Specialized models:  HIVb HIV (eight proteins) Nickle  et al.  (2007)  rtREV retroelement  pol Dimmic  et al.  (2002) Note:  ‘—’ indicates that many protein MSAs were used for training. Many different methods were used to estimate  R  matrix parameters. Only a selected subset of specialized models is shown; many specialized models were trained using viral data (e.g. FLU) or organelle-encoded proteins (e.g. mtREV24 and cpREV). A lower dimensional model of protein evolution with parameters that have a clear biological interpretation would allow us to examine the ways that patterns of evolution differ among proteins. Simply using the  K R / K C  ratio described above is unlikely to solve this problem, since there are many ways to divide amino acid substitutions into radical versus conservative subsets ( Fig. 1 ). This reflects the fact that there are likely to be many axes in ‘selection space’ (e.g. one for selection against radical changes in amino acid side chain size, a second related to radical changes in side chain polarity and so forth). The complexity of amino acid properties ( Fig. 1 ) suggests that it would be better to eschew simply classifying amino acids interchanges as radical or conservative and devise a parameter that can capture different degrees of ‘radicalness’ (e.g. the selection against a large-to-tiny interchange is likely to be stronger than selection against a large-to-small substitution). Finally, information about the relative rates at which different non-synonymous mutations enter populations is also likely to be important ( Yampolsky and Stoltzfus, 2005 ). I propose a six-parameter model, with two parameters related to mutational input and four parameters that capture the physicochemical properties of amino acids (to address the impact of selection against radical substitutions). Thus, the model only has one more  R  matrix parameter than the GTR 4  model (although it does have 19 equilibrium frequency parameters). It is likely to be possible to estimate these parameters from typical protein MSAs. The biological interpretability of these parameters should allow us to ask about general patterns across all proteins and to assess the degree to which different proteins exhibit distinct patterns of evolution. The proposed model is used to examine several datasets to explore those general patterns and the variation among proteins in their patterns of sequence evolution.
 Fig. 1. 
 Dividing amino acid interchanges into radical and conservative is difficult.  Amino acids can be divided into many different groups; radical changes are those between groups whereas conservative changes are within groups.  Dayhoff  et al.  (1978)  groups reflect patterns in their PAM matrix and their physicochemical properties.  Hanada  et al.  (2007)  groups maximized the correlation between  K R / K C  and  K A / K S  for mammalian proteins. Many studies (e.g.  Weber  et al. , 2014 ) calculate  K R / K C  using a simple polar-nonpolar and/or large-small categorization. However, changes in many amino acid properties (i.e. any interchanges that cross lines in the diagram) can be radical, at least in some contexts. In fact, certain amino acids (C and P, shaded) have unique properties and any substitution involving them might be radical. Thus, radical versus conservative changes should be viewed as a matter of degrees rather than absolutes 2 Materials and methods This section focuses on generating the  R  matrix; readers are referred to various reviews ( Felsenstein, 2004 ;  Swofford  et al. , 1996 ;  Warnow, 2018 ;  Yang, 2006 ) for general information about likelihood calculations in phylogenetics. The models proposed here populate an  R  matrix using the general approach shown as follows:
 (1) r i j = K i j e x p - φ 1 Δ i j 1 e x p - φ 2 Δ i j 2 e x p - φ 3 Δ i j 3 … 
where  r ij  are  R  matrix elements,  φ  are weighting parameters and  K ij  is a constant. The weighting parameters are estimated by ML (see below). Δ ij  are the absolute value of the difference between amino acids  i  and  j  in some property (e.g. polarity) divided by the maximum absolute value for all possible differences between pairs of amino acids. Thus, Δ ij  are fixed numbers between zero and one for any specific property and pair of amino acids (see  Supplementary File S1 ).  Equation (1)  has the property that setting any  φ  value to zero yields a sub-model in which the amino acid property related to that  φ  parameter has no impact on the model. The amino acid properties examined here were side chain volume ( V ), polarity ( P ), composition ( C ) and aromaticity ( A ). The first three are from the work by  Grantham (1974)  and the fourth is from work by  Xia and Li (1998) . The general approach shown in  Equation (1)  can be rewritten in a more specific manner as 
 (2)   r i j = e x p - V Δ i j V e x p - P Δ i j P e x p - C Δ i j C e x p - A Δ i j A 
where the letters are the  φ  parameters for the properties studied hare and all  K ij  are set to one. There are 16 models based on  Equation (2) , ranging from the simplest model, where  V  =  P = C  =  A  =   0, to the most complex where all parameters are free to vary. The simplest model is actually an F81-like ( Felsenstein, 1981 ) model for proteins. 
 Equation (2)  models [hereafter, eq2 models] only capture the impact of selection (hereafter,  V ,  P ,  C  and  A  are called selective parameters). Mutational input was modeled by incorporating the structure of the genetic code, using ‘gencode’ ( G ) and transversion ( T ) parameters. The full model is shown as
 (3) r i j = e x p - V Δ i j V e x p - P Δ i j P … N i j - G e x p - T Δ i j T N ij  is the minimum number of nucleotide substitutions necessary for an interchange of amino acids  i  and  j  and  Δ i j T  is one if at least one of those substitutions is a transversion and zero otherwise. Gencode is dealt with in a different way than the other parameters, but it has the same behavior as the other parameters (i.e.  G  =   0 means the number of substitutions necessary for the interchange does not have an impact on the model). There are 64 potential eq3 sub-models. However, the 16 sub-models where  T  is free to vary but  G  =   0 will penalize a single transversion more than simultaneous changes to multiple nucleotide so they were not considered. Thus, 48 eq3 models (16 of which are eq2 models) were examined. The models are named based on the free parameters. Although the eq2 and eq3 models can reveal the amino acid properties that contribute the most to the patterns of evolution for a specific protein they cannot be used to examine the ways that available empirical models fail to capture those processes. However, it is possible to modify  Equation (2)  to include information from an empirical model:
 (4)   r i j = K i j E M P e x p - V Δ i j V e x p - P Δ i j P … N i j - G e x p - T Δ i j T 
where  K i j E M P is the relevant  R  matrix element from an empirical model (e.g. those listed in  Table 1 ). The eq4 models can be used to establish which properties an empirical model fails to capture for specific protein. These parameters were optimized using a perl program that calls IQ-TREE v. 1.5.5 ( Nguyen  et al. , 2015 ) to perform the likelihood calculations. Briefly, IQ-TREE was called and used to optimize the amino acid frequency parameters and Γ-distribution shape parameter (α); this study only considered +F + Γ models. Then α and the amino acid frequencies were fixed and the eq3 or eq4 model parameters ( V ,  P ,  C ,  A ,  G  and  T ) were optimized. A simple one-dimensional optimization was performed for each parameter in succession. The optimization began by determining whether adding or subtracting a fixed value (δ) to the focal parameter improves the likelihood. If  φ +δ or  φ −δ had a higher likelihood than the starting  φ  value, then δ was added (or subtracted) until the likelihood was maximized. After optimizing all free parameters δ was reduced and another round of optimization was conducted. After δ value reached a minimum (0.00001), the α and amino acid frequencies were re-optimized using the  R  matrix generated using the estimated parameter values. This procedure was repeated until the likelihood failed to change any further. The best-fitting model was identified using the corrected Akaike information criterion (AIC c ;  Hurvich and Tsai, 1989 ), using the number of aligned sites in the protein MSA as the sample size. Empirical models were identified in IQ-TREE using the settings ‘-m TESTONLY -mfreq FO -mrate G -merit AICc’; this finds the best fitting model from a set of 18 candidate models (all models in  Table 1  and eight additional specialized models). The eq4 models used  K i j E M P from the best-fitting empirical model identified using IQ-TREE. 3 Results and discussion This study had the following four major goals: (i) to establish which parameters are necessary to fit eq3 models to protein MSAs; (ii) to determine whether the eq3 parameter estimates differ among proteins; (iii) to compare the fit of eq3 models to empirical models; and (iv) to examine whether the fit of empirical models can be improved using the eq4 models. To accomplish these goals, we examined proteins from yeasts ( Rokas and Carroll, 2005 ), vertebrates ( Chen  et al. , 2015 ) and birds ( Jarvis  et al. , 2014 ). The specific proteins were chosen arbitrarily and only the MSAs judged free of homology errors by  Springer and Gatesy (2018 ) were chosen from birds. Individual gene trees can differ from the species tree ( Maddison, 1997 ) and the true species tree is unknown (it is especially uncertain for birds;  Reddy,  et al. , 2017 ), so we optimized the model parameters on the ML tree generated using the best-fitting empirical model. To complement the analyses of individual genes I used eight concatenated datasets from  Wolf  et al.  (2004) . Each  Wolf  et al.  (2004)  dataset was limited to proteins with a specific function, so the potential of the eq3 and eq4 models to highlight differences among classes of proteins could be assessed. All datasets and trees are available in  Supplementary File S2 . 3.1 The most important parameters vary among proteins All single-parameter eq3 models resulted in substantial likelihood increases relative to the F81-like model. Polarity ( P ) was the selective parameter that increased per site Δ ln L the most; the median Δ ln L/site for eq3 models with  P  as the only free parameter increased by 0.8359 for vertebrates, 0.5845 for yeasts and 0.2788 for birds. The estimate of the  P  parameter was also larger on average than the other selective parameters ( Table 2 ). The least important selective parameters based on those criteria were composition ( C ) for the vertebrates and yeasts and aromaticity ( A ) for birds. Gencode ( G ), the primary mutational input parameter, was very important; the median Δ ln L/site increased by 1.054 for vertebrates, 0.5102 for yeasts and 0.3352 for birds. Estimates of  G  were especially high in birds ( Table 2 ). Indeed, adding the  G  parameter resulted in a larger likelihood increase than any other parameter for birds and vertebrates and the second largest (after  P ) for the yeasts.
 Table 2. Parameter estimates for single parameter eq3 models Dataset Sites V P C A G G + T Yeasts:  Flc2p 494 4.86 4.87 3.33 2.73 3.25 3.05/0.65  Ptc1p 217 4.43 5.72 2.50 2.85 3.61 3.53/0.27  Rfc2p 296 4.69 4.79 3.22 3.71 3.07 2.90/0.54  Ung1p 185 3.71 4.39 1.99 3.22 2.08 1.89/0.48  Tkl1p 629 4.69 4.33 2.64 3.08 2.50 2.42/0.26 Mean 4.48 4.82 2.74 3.12 2.90 2.76/0.44 Birds:  APC (54) 2862 3.51 4.88 2.57 3.76 7.14 6.56/1.03  GFPT1 (15) 700 2.60 4.21 3.43 2.07 3.88 3.70/0.42  HMBS (76) 353 3.54 4.16 3.12 1.53 5.52 5.25/1.09  IFGN1 (78) 845 3.34 4.75 3.12 1.53 7.07 7.21/1.06  PCNX (79) 2359 3.31 4.31 2.52 3.01 5.33 4.89/0.96 Mean 3.26 4.46 3.05 2.55 5.79 5.52/0.91 Vertebrates:  AQR 1020 4.07 4.43 2.94 3.64 4.72 4.59/0.73  COX10 490 3.13 4.06 2.57 4.15 4.70 4.54/0.69  EDC4 761 4.22 5.43 2.80 4.02 4.52 4.43/0.58  GPATCH1 515 3.52 4.74 3.01 3.92 4.12 3.97/0.67  VPS54 347 3.94 4.91 2.40 4.37 4.52 4.24/0.77 Mean 3.78 4.71 2.74 4.02 4.52 4.36/0.69 Parameter estimates are rounded to the nearest 0.01. Estimates of the  T  parameter were only obtained in combination with  G ; those parameter estimates are listed in the order  G / T . Bird gene numbers are from the study by  Jarvis  et al . (2015) . Complete output of the parameter optimization program is available in  Supplementary File S3 . Single-parameter eq3 models provide information analogous the commonly used  K R / K C  and  K A / K S  ratios. Unlike the  K A / K S  ratio (ω), there is no obvious expected value of the  K R / K C  ratio. Assuming synonymous sites evolve at the neutral rate (which may not be true;  Chamary  et al. , 2006 ;  Lawrie  et al. , 2013 )  K A / K S  = 1 provides evidence of neutral evolution. In contrast,  K R / K C  only allows the exploration of differences among proteins (or lineages). The single parameter eq3 models provide similar information while eschewing a simplistic radical versus conservative classification of interchanges. The number of free parameters in the best-fitting eq3 models for each of the 15 test datasets ranged from two to six ( Table 3  and  Supplementary File S3 ); the more parameter-rich (i.e. five- or six-parameter) models had the best fit for most datasets. However, the same patterns revealed in single-parameter eq3 analyses were also evident in the best-fitting models.  C  had the least impact on the likelihood in the single parameter analyses and  C  was not included the best-fitting model for 10 of the 15 proteins. In fact,  C  was not included in the best-fitting model for any yeast protein.  P  and  G  had the largest impact on the likelihood in single-parameter analyses and both of those parameters were included in the best-fitting models for the test datasets. However, the parameter estimates obtained using more parameter-rich models tended to be much lower than those obtained using the single-parameter models. This appeared to reflect interactions among the parameters.  C  presented an interesting case since it was negative for the vertebrate VPS54 dataset. This can happen when certain amino acid interchanges that might be viewed as radical based on composition alone were actually overly penalized by the other parameters (i.e. their instantaneous rate is too low).
 Table 3. Parameter estimates and Δ ln L/site for the best-fitting eq3 models Dataset V P C A G T Δ ln L Best  EMP Yeasts:  Flc2p 2.13 3.24 — 1.97 1.59 0.63 0.9621 LG (−0.2062)  Ptc1p 1.91 4.08 — 1.47 2.48 — 0.8091 LG (−0.1132)  Rfc2p 2.27 3.63 — 2.55 1.50 0.56 0.8790 LG (−0.1692)  Ung1p 1.85 3.51 — 2.66 0.72 0.35 0.7615 rtREV (−0.1217)  Tkl1p 2.65 2.91 — 1.71 1.08 0.24 0.6869 LG (−0.2438) Mean 2.16 3.48 0.00 2.07 1.47 0.35 Birds:  APC — 2.66 0.55 2.72 5.95 0.89 0.7518 HIVb (−0.0002)  GFPT1 — 2.91 — — 3.36 — 0.0862 JTT (−0.0183)  HMBS 1.71 2.00 — — 4.71 0.93 0.7319 JTT (−0.0006)  IFGN1 0.55 2.40 0.55 1.53 6.46 0.83 2.9576 HIVb (−0.0667)  PCNX 0.91 2.08 0.66 1.78 4.14 0.86 0.2512 HIVb (−0.0039) Mean 0.63 2.41 0.35 1.21 4.92 0.70 Vertebrates:  AQR 1.33 2.56 — 2.54 3.67 0.57 0.8500 JTT (−0.1015)  COX10 — 2.48 — 3.04 3.73 0.53 1.9200 JTT (−0.1689)  EDC4 0.85 3.32 — 3.13 3.36 0.57 1.8728 JTT (−0.1061)  GPATCH1 0.76 2.55 0.75 2.45 3.12 0.48 2.2052 JTT (−0.2815)  VPS54 0.88 3.15 −0.89 3.13 3.46 0.72 1.2291 JTT (−0.0804) Mean 0.76 2.81 −0.03 2.86 3.47 0.57 Note : ‘—’ indicates parameters that were not in the best-fitting eq3 model (based on the AIC c ). Any parameters absent from the best-fitting model were assumed to be zero when the mean was calculated. Parameter estimates are rounded to the nearest 0.01. Δ ln L is the likelihood difference per site (Δ ln L/site) relative to the F81-like model. Δ ln L/site is rounded to the nearest 0.0001. The best-fitting empirical model (‘Best  EMP ’) is followed by the Δ ln L/site relative to the best-fitting eq3 model. Complete output of the parameter optimization program is available in  Supplementary File S3 . The best-fitting empirical models differed among proteins in the test datasets. LG had the best fit for most of the yeast proteins (the exception had the best fit to rtREV), whereas JTT was the best-fitting model for all of the vertebrate proteins. The avian proteins were split between HIVb (three proteins) and JTT (two proteins). The likelihood of the best-fitting empirical model was higher than the best-fitting eq3 model in all cases, although the best eq3 model had a likelihood that of the best empirical model for two avian proteins. In fact, adding the  V  parameter to analyses of APC actually resulted in a slightly higher likelihood than that of the best-fitting empirical model ( ln L = −23308.4891 for the full VPCAGT model and  ln L = −23308.9690 for the HIVb model; Δ ln L = 0.4799). However, the estimate of  V  was quite low ( V  = 0.23) when the VPCAGT model was used to analyze APC; that is why the  V  parameter was not included in the best-fitting eq3 model for that protein. The  V  parameter was also absent from the best-fitting eq3 model for one other avian protein and one vertebrate protein. Regardless, eq3 parameter estimates for proteins with the same best-fitting empirical model were often very different. This suggests that eq3 models can reveal patterns of evolution for different proteins; simply identifying the best-fitting empirical model cannot reveal that information. 3.2 The fit of empirical models can be improved Although the six-parameter models result in substantial likelihood improvements relative to a simple F81-like model, they did not fit the data for any protein MSA and the best-fitting empirical model (with the exception of APC). The raises two questions. First, can the fit of empirical models be improved? Second, which aspects of the evolutionary process do empirical models fail to capture? Using eq4 to adjust the best-fitting empirical model resulted in improvements (based on the AIC c ) in all but one case (GFPT1;  Table 4 ). Parameter estimates for the eq4 models were much lower than those obtained using eq3 models (compare  Tables 3  and  4 ); this was expected since the ‘starting point’ for the models (i.e. the empirical model) was presumably much better than the F81-like model. However, the parameters that played a role in best-fitting eq4 model differed among proteins, emphasizing the fact that the ‘one size fits all’ nature of empirical models is inappropriate.
 Table 4. Parameter estimates for eq4 models using the best-fitting empirical model Dataset V P C A G T Best  EMP Yeasts:  Flc2p — — — — 0.60 — LG (0.0098)  Ptc1p — 1.03 — — 1.00 −0.44 LG (0.0550)  Rfc2p 1.34 — — — — — LG (0.0164)  Ung1p — 1.06 −1.06 1.13 — — rtREV (0.0464)  Tkl1p 0.92 — — — — — LG (0.0086) Mean 0.45 0.42 −0.21 0.23 0.32 −0.09 Birds:  APC −0.26 0.97 −0.63 1.38 1.55 0.25 HIVb (0.0251)  GFPT1 — — — — — — JTT (—)  HMBS — — — — 2.57 0.56 JTT (0.0834)  IFGN1 0.35 0.40 — −0.28 2.76 — HIVb (0.0381)  PCNX — 0.45 — 0.80 — — HIVb (0.0027) Mean 0.02 0.36 −0.13 0.38 1.18 0.16 Vertebrates:  AQR 0.56 — — 0.85 1.47 — JTT (0.0452)  COX10 −0.56 — — 1.11 1.56 — JTT (0.0961)  EDC4 — 1.01 — 1.71 1.11 0.20 JTT (0.1488)  GPATCH1 — — 0.63 0.52 0.92 — JTT (0.0605)  VPS54 — 0.86 −1.22 1.54 1.16 0.30 JTT (0.0858) Mean 0.00 0.38 −0.12 1.15 1.25 0.10 LG (−0.2062) Note:  ‘—’ indicates parameters that were not in the best-fitting (based on the AIC c ) eq4 model. In all cases, the best-fitting empirical model was used as the ‘base model’ that provided the  K ij  values in eq3. Any parameters not present in the best-fitting model were assumed to be zero for calculating the mean. Parameter estimates are rounded to the nearest 0.01. The best-fitting empirical model is followed by the Δ ln L per site relative to that model (‘—’ indicates the empirical model was not improved using eq4). Complete output of the parameter optimization program is available in  Supplementary File S3 . 3.3 Parameter estimates for concatenated datasets of functionally related proteins Empirical models obtained ultimately correspond to fixed  R  matrix values estimated using large training sets. From a conceptual standpoint, the simplest way to estimate those parameters would be optimize the GTR 20  model given a diverse set of functionally unrelated proteins. In practice, many empirical models used less computationally demanding approximate methods for parameter estimation. However, the general point is that  R  matrix values for empirical models should be close to the average GTR 20  model parameters for many different proteins. Thus, one might expect parameter estimates for concatenated datasets to converge on some average value that is as close as possible to the empirical model parameters. However, this might not be true for concatenated datasets that comprise functionally related proteins. Most concatenated datasets used in phylogenomics (e.g.  Chen  et al. , 2015 ;  Jarvis  et al. , 2014 ;  Rokas and Carroll, 2005 ) comprise diverse and functionally-unrelated proteins. An early phylogenomic study ( Wolf  et al. , 2004 ) represents an exception to this; that study analyzed eight separate six-taxon concatenated datasets, each of which comprises functionally related proteins. The six focal taxa for  Wolf  et al.  (2004)  are three animals (a vertebrate, an insect and a nematode), two fungi (fission yeast and budding yeast) and a plant. There are two plausible trees for those taxa: (i) Ecdysozoa (an insect + nematode clade) and (ii) Coelomata (an insect + vertebrate clade); these trees are available in  Supplementary File S2 .  Wolf  et al . (2004)  supported Coelomata but later phylogenomic studies with larger taxon samples have strongly supported Ecdysozoa (e.g.  Dunn  et al. , 2008 ;  Hejnol  et al. , 2009 ). Thus,  Wolf  et al.  (2004)  data provide an opportunity to ask two questions. First, do parameter estimates for functionally related sets of proteins differ, like those for individual proteins? Second, do analyses using the proposed models support the Ecdysozoa tree? To do this the likelihood given the best-fitting models (empirical, eq3, and eq4) was calculated using both plausible topologies (Ecdysozoa and Coelomata). The eq3 parameter estimates for the concatenated datasets did show variation, albeit less than for individual proteins (compare  Tables 3  and  5 ). The  C  and  A  parameter estimates were especially variable. All  Wolf  et al.  (2004)  datasets had the same best-fitting eq3 model (VPCAGT) and empirical model (LG). LG always had a better fit than the VPCAGT model. However, it was always possible to improve model fit relative to the LG model using eq4 ( Table 5 ). The  V  and  G  parameters were included in every eq4 model, although the estimates of  G  were always negative, suggesting LG overcorrects for the impact of the genetic code on these data.
 Table 5. Parameter estimates for concatenated datasets Dataset Sites V P C A G T Eq3 models:  Chaperonins 3970 2.68 3.87 −0.20 1.86 0.74 0.19  Clathrin 2138 2.11 3.62 −0.25 3.06 0.68 0.39  DNA polymerase 1782 2.19 2.99 0.53 2.15 1.03 0.16  DNA replication 2284 2.36 3.10 0.47 2.08 0.98 0.15  Proteasome 2474 2.43 3.18 0.21 2.44 0.76 0.18  Ribosomal proteins 11 586 2.23 2.92 0.29 2.28 0.62 0.10  RNA polymerase 3274 2.26 2.97 0.29 1.96 0.86 0.27  Translation factors 2045 2.13 3.21 0.32 2.36 0.80 0.21 Mean 2.30 3.23 0.21 2.27 0.81 0.21 Eq4 models:  Chaperonins 3970 1.10 0.88 −0.55 — −0.44 −0.18  Clathrin 2138 0.49 0.74 −0.65 1.11 −0.54 —  DNA polymerase 1782 0.68 — — 0.24 −0.24 −0.21  DNA replication 2284 0.89 — — — −0.21 −0.21  Proteasome 2474 0.84 0.27 — 0.51 −0.47 −0.19  Ribosomal proteins 11 586 0.73 −0.23 — 0.45 −0.34 −0.55  RNA polymerase 3274 0.54 — — 0.25 −0.31 −0.11  Translation factors 2045 0.44 — — 0.42 −0.33 −0.20 Mean 0.71 0.21 −0.15 0.37 −0.38 −0.18 All parameter estimates reflect the Ecdysozoa tree. ‘—’ indicates parameters that were not in the best-fitting eq4 model (based on the AIC c ). Any parameters not present in the best-fitting model were assumed to be zero for calculating the mean. Parameter estimates are rounded to the nearest 0.01. The empirical model used for the eq4 models was always LG. ‘DNA replication’ refers to DNA replication licensing factors, i.e. the MCM family. Complete output of the parameter optimization program is available in  Supplementary File S4 . Parameter estimates in  Table 5  were calculated using the Ecdysozoa topology but analyses using the Coelomata topology resulted in similar values ( Supplementary File S4 ).  Wolf  et al.  (2004)  found that different datasets supported different trees, with three (proteasome subunits, ribosomal proteins and RNA polymerase) supporting Ecdysozoa and the other five datasets supporting Coelomata. Analyses using LG, eq3 and eq4 also revealed conflict; four datasets (chaperonins and the same datasets as  Wolf  et al. , 2004 ) supported Ecdysozoa and the other four supported Coelomata. Although that result was equivocal, the Ecdysozoa tree had the highest overall likelihood in all analyses, consistent with the results of studies with more taxa (e.g.  Dunn  et al. , 2008 ;  Hejnol  et al. , 2009 ). The overall likelihood is the sum of the likelihoods of for eight of the concatenated MSAs given a specific tree; this is the likelihood given a model where each of the eight MSAs has distinct parameters and distinct branch lengths. Surprisingly, the likelihood difference (Δ ln L) favoring Ecdysozoa was actually larger for eq3 than for eq4 (Δ ln L = 44.9233 for eq3; Δ ln L = 33.5569 for eq4), despite the better fit (based on AIC c ) of the eq4 models. However, the relationship between model and topology was complex; the overall Δ ln L favoring Ecdysozoa was smallest for LG (Δ ln L = 29.3364) and largest for the F81-like model (Δ ln L = 60.1499). Moreover, one additional dataset (clathrin) supported Ecdysozoa when the F81-like model was used (see Supporting File 4 for details). Despite these complexities the fact that the eq4 models resulted in a modest increase in the likelihood difference relative to LG should be viewed as encouraging. Broader surveys will be necessary to explore the potential of these models for estimating phylogenetic tree topologies. 3.4 General patterns and variation among proteins A general framework for models of protein evolution that can be used to explore general patterns of protein evolution and variation among proteins was proposed (eq1). Specific versions of that general model that focused amino acid physicochemical properties (eq2, eq3 and eq4) emphasized the important roles of side chain volume ( V ), polarity ( P ), aromaticity ( A ) and the structure of the genetic code ( G ) in determining relative rates of amino acid interchanges. The role of polarity, volume, and the genetic code in determining rates of amino acid interchanges has long been appreciated, but a role for aromaticity independent of volume might be viewed as surprising since aromaticity and volume are correlated (Pearson’s  r  = 0.716). Composition ( C ) had less impact on protein evolution. That could reflect way composition is calculated; composition has a modest correlation with the other parameters (the maximum is with aromaticity;  r  = -0.474) if all amino acids are considered, but it is strongly correlated with polarity ( r  = 0.809) if cysteine is excluded (the composition-polarity correlation is  r  = 0.37 when cysteine is included). Thus,  C  could reflect two distinct aspects of protein evolution (polarity and the special nature of cysteine). This may explain why negative estimates of the  C  parameter emerged in some analyses using eq3 ( Tables 3  and  5 ). It also suggests that it may be desirable to abandon the  C  parameter in favor of other properties. Regardless of the details of the amino acid properties used for analyses, it is clear that the patterns of evolution vary among proteins in ways that cannot be examined using empirical models. The models proposed here can reveal that variation and highlight the best amino acid properties to examine in future studies. The goal of this study was to develop an amino acid model that could reveal the ways that patterns of molecular evolution vary among proteins. The eq3 models include six parameters, four of which reflect selection against radical amino acid substitutions. This could make the eq3 models testable in a way that empirical models (e.g.,  Table 1 ) are not. For example, if analyses of a specific protein using eq3 results in a high estimate of  V  that protein is likely to be more sensitive to volume changing substitutions than another protein associated with a lower estimate of  V . Thus, eq3 could be tested by mutagenesis experiments (e.g. using methods similar to the work by  Georgelis  et al. , 2007 ). The fact that  K R / K C  and effective population size appear to be negatively correlated ( Hughes and Friedman, 2009 ;  Weber  et al. , 2014 ) could permit another test of eq3. The correlation probably reflects the higher efficiency of selection in organisms with large population sizes ( Akashi  et al. , 2012 ). Since the eq3 model parameters are analogous to  K R / K C  they should exhibit the same correlation. However, eq3 also of highlights the properties of amino acids that contribute to the radical versus conservative nature of substitutions in different proteins. Overall, eq3 provides a novel tool to explore the differences among proteins. Eq4 models provide different information than the eq3 models. Specifically, eq4 reveals the ways that empirical models fail to capture specific patterns of amino acid substitution. The results shown in  Tables 4  and  5  reflect the use of eq4 with the best-fitting empirical model; they do not show the degree to which other empirical models might be improved. Testing the full set of empirical models in  Table 1  revealed three cases (APC, IFGN1 and PCNX) where eq4 with a suboptimal ‘base model’ performed better than eq4 with the best-fitting empirical model ( Supplementary File S5 ). In all three cases, combining eq4 and the JTT model resulted in a better likelihood than eq4 with the HIVb model; the estimate of the  G  parameter was much larger for the JTT + VPCAGT model than for the HIVb + VPCAGT model for all three proteins ( Supplementary File S5 ). The parameter estimates that can be obtained using the eq4 models ( Supplementary Files S5 and S6 ) provide an interesting way to examine the ways each empirical model fails to capture the patterns of amino acid substitution for individual proteins. This study did not address variation among sites in patterns of sequence evolution or the impact of these models on the estimation of tree topology. Many studies have revealed variation among sites within proteins in their evolutionary rate ( Echave  et al. , 2016 ); substantial variation in the pattern of evolution is also likely to exist. These models do not address that variation (except to the extent that the Γ distribution captures variation in rates for all models examined here). However, it would be straightforward to extend eq3 or eq4 to a mixture model where one or more of the parameters are drawn from a prior distribution (e.g. a Γ distribution or a uniform distribution); the mean and variance of that distribution could be estimated by ML. Likewise, the potential for the proposed models to improve tree topology estimation is unclear, although the fact that eq4 improves the fit of empirical models makes it reasonable to speculate that it could be useful. However, other analytical approaches should also be considered in studies focused on tree estimation (e.g. site heterogeneous CAT models;  Lartillot and Philippe, 2004 ;  Le,  et al. , 2008 ). However, information analogous to the  K R / K C  ratio cannot be obtained from analyses using standard empirical models (or the CAT models). Ultimately, the value of the proposed models is their potential to reveal differences among proteins in their patterns of evolution and to identify the characteristics of amino acids that contribute to protein evolution. Supplementary Material Supplementary Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Multifactor dimensionality reduction for graphics processing units enables genome-wide testing of epistasis in sporadic ALS</Title>
    <Doi>10.1093/bioinformatics/btq009</Doi>
    <Authors>Greene Casey S., Sinnott-Armstrong Nicholas A., Himmelstein Daniel S., Park Paul J., Moore Jason H., Harris Brent T.</Authors>
    <Abstract>Motivation: Epistasis, the presence of gene–gene interactions, has been hypothesized to be at the root of many common human diseases, but current genome-wide association studies largely ignore its role. Multifactor dimensionality reduction (MDR) is a powerful model-free method for detecting epistatic relationships between genes, but computational costs have made its application to genome-wide data difficult. Graphics processing units (GPUs), the hardware responsible for rendering computer games, are powerful parallel processors. Using GPUs to run MDR on a genome-wide dataset allows for statistically rigorous testing of epistasis.</Abstract>
    <Body>1 INTRODUCTION Genome-wide association studies hold promise for the discovery of the genetic factors that underlie common human diseases (Hirschhorn and Daly,  2005 ; Wang  et al. ,  2005 ). Unfortunately this promise has largely not been realized (Shriner  et al. ,  2007 ; Williams  et al. ,  2007 ). It is thought that this failure could be due to epistasis, the role of gene–gene interactions, which has commonly been ignored in these studies. Powerful and model-free methods such as multifactor dimensionality reduction (MDR) have been developed (Ritchie  et al. ,  2001 ), but an exhaustive examination of even pair-wise interactions in a 550 000 SNP dataset would require the analysis of 1.5 × 10 11  combinations. While an analysis of this scale is approachable with modern cluster computing, an analysis that includes permutation testing to assess the statistical significance of results remains infeasible with CPU-based approaches. Rendering photo-realistic video games in real time is also computationally difficult. For video game graphics, specific hardware (the graphics processing unit or GPU) has been developed. The GPU is a massively parallel computing platform that can be adapted to some scientific tasks. We have previously shown that MDR is one of these tasks (Sinnott-Armstrong  et al. ,  2009 ). Here we provide software which makes practical the analysis of epistasis in genome-wide data through the use of GPUs and demonstrate its application to a genome-wide analysis of epistasis of sporadic amyotrophic lateral sclerosis (ALS). 2 METHODS MDRGPU, a software tool capable of analyzing genome-wide data, is a Python implementation of MDR, which uses the PyCUDA library to run MDR on GPUs. MDRGPU 1.0 supports balanced accuracy, large datasets, execution across an arbitrary number of GPUs, permutation testing and the analysis of high-order interactions. It runs on GPUs which support CUDA (i.e. the NVIDIA GeForce 8800 series and higher). Parallel execution of one realization across multiple GPUs is supported with the pp library for Python. MDRGPU provides a command-line interface for scripted analysis. The GPU architecture has various memory spaces available. MDRGPU uses the constant cache, global memory, shared memory and registers. Shared memory is used to store the intermediate case and control counts for each attribute combination and to store the number of true and false positives and negatives. The global memory is accessed directly to fetch attributes. The constant cache is used in MDRGPU to store the case–control status. Dataset sizes of greater than 65 536 attributes require splitting which is handled seamlessly by MDRGPU. This splitting does not cause linear slowdown; there is simply more overhead of launching, so datasets with large numbers of instances see less of a performance reduction than datasets with few instances. The largest number of addressable attributes is 4 billion requiring 4 GB RAM per instance. In order for the case–control status to be held in constant memory, there can be at most 16 384 instances. Our proof of concept analysis was performed on three GPU workstations (detailed in  Supplementary Material S1 ). These systems contain three GeForce 295 cards, each of which contains two GPUs. For the first stage of this analysis, we used an ALS dataset from Schymick  et al.  ( 2007 ) as our detection dataset. This dataset was obtained from QUEUE at Coriell, but has since been moved to dbGaP. It contains 276 individuals with sporadic ALS and 271 control individuals. These individuals are genotyped at 555 352 SNPs using the Illumina Infinium II HumanHap550 SNP chip. We processed this dataset by removing SNPs with a minor allele frequency &lt;0.2 or those in which &gt;10% of values were missing for either cases or controls. We further used Haploview 's tagSNP algorithm (Barrett  et al. ,  2005 ) to select representative SNPs from groups of correlated SNPs ( r  &gt; 0.8). After this, 210 382 SNPs remained and were used in the analysis. For the replication stage, we used a dataset of Irish individuals containing of 221 sporadic ALS patients and 211 controls described in Cronin  et al.  ( 2008 ). We used MDRGPU to perform a two-way analysis across the entire detection dataset. We selected the SNP combination with the best balanced accuracy measure. We then permuted the dataset 1000 times while repeating this analysis. We measured the accuracy of the best pair in each permuted dataset. We then used the 50th best accuracy obtained from these permuted datasets as our significance cutoff. This permutation test yields an experiment-wise α of 0.05. A pair of SNPs with a significant association in the detection phase was tested in the replication dataset. In this phase, the two detected SNPs were selected from the dataset and MDR was used to evaluate only this pair. A permutation test was performed here using MDR on only these two SNPs, and an α of 0.05 was used to assess significance. 3 RESULTS Our three GPU systems completed an analysis of pairwise interactions in a single permutation approximately every 6 min. The time to analyze the dataset itself for pairwise interactions is the same as the time required for one permutation. One thousand permutations were used to assess statistical significance which required ∼100 h. The time to analyze the same dataset on a cluster with 200 AMD Opteron 2384 (2.7 GHz) CPU cores was just over 1 h without permutation testing and thus a CPU-based permutation test was considered infeasible as the estimated time required on 200 CPU cores was &gt;40 days. In the proof-of-concept analysis, the highest accuracy combination in our dataset was SNPs rs4363506 and rs6014848 with a balanced accuracy of 0.6551. In our permutation test, this accuracy was statistically significant ( P  &lt; 0.048). In the replication dataset this pair had a balanced accuracy of 0.5821. Permutation testing the replication dataset showed that this result was also statistically significant ( P  &lt; 0.021). Therefore, not only have we discovered a statistically significant pair of SNPs using an experiment-wise α of 0.05, but we have replicated the significant relationship in an independent dataset. Here is evidence of how the permutation testing allowed by MDRGPU enables the discovery of combinations of SNPs that are significantly associated with a disease. 4 DISCUSSION While SNP rs4363506 has been reported as associated with disease in Schymick  et al.  ( 2007 ), it did not have a statistically significant effect in Cronin  et al.  ( 2008 ) when considered alone (χ 2 ,  P  = 0.18) and would have failed to replicate without considering pairwise effects. SNP rs6014848 has not previously been described as associated with sporadic ALS, although it shows main effects (uncorrected χ 2 ,  P  &lt; 0.05) in both datasets. Greene  et al.  ( 2009 ) have shown that SNPs can fail to replicate a significant association when the joint effect of those SNPs is ignored. This is particularly likely when the populations from which patients are ascertained differs. Schymick  et al.  ( 2007 ) collected individuals from the USA, while Cronin  et al.  ( 2008 ) collected individuals from Ireland. By considering the joint effect of SNPs, MDRGPU discovers a novel association which replicates in an independent dataset. GPUs provide a platform for epistasis analysis in genome-wide data where computational requirements far exceed what CPUs can cost-effectively provide. MDRGPU is a software package for this emerging computing platform that enables human geneticists to tackle analyses previously found to be intractable. 
          Supplementary Material 
          
             
                [Supplementary Data] 
             
             
             
          
       </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Large scale microbiome profiling in the cloud</Title>
    <Doi>10.1093/bioinformatics/btz356</Doi>
    <Authors>Valdes Camilo, Stebliankin Vitalii, Narasimhan Giri</Authors>
    <Abstract/>
    <Body>1 Introduction and background Microbes are ubiquitous and a microbiome is a collection of microbes that inhabit a particular environmental niche such as the human body, earth soil and the water in oceans and lakes. Metagenomics is the study of the combined genetic material found in microbiome samples, and it serves as an instrument for studying microbial biodiversities and their relationships to humans. Profiling a microbiome is a critical task that tells us what microorganisms are present, and in what proportions; this is particularly important as many human diseases are linked to changes in human microbiome composition ( Haiser  et al. , 2013 ;  Koeth  et al. , 2013 ;  Wu and Lewis, 2013 ;  Zhang  et al. , 2015 ), and large research projects have started to investigate the relationships between the two ( The Integrative HMP iHMP Research Network Consortium, 2014 ). A powerful tool for profiling microbiomes is high-throughput DNA sequencing ( Metzker, 2010 ), and whole metagenome sequencing experiments generate data that give us a lens through which we can study and profile microbiomes at a higher resolution than 16S amplicon-based sequencing analyses ( Ranjan  et al. , 2016 ). Advances in sequencing technologies have steadily reduced the cost of sequencing and have led to an ever increasing number of extremely large and complex metagenomic  datasets (Ansorge, 2009 ;  Caporaso  et al. , 2012 ). The resulting computational challenge is the production of even larger intermediate results, and need for large indexes of the reference genome collections ( Vernikos  et al. , 2015 ), making it impossible to process on commodity workstations or laptops. Powerful multi-user servers and clusters are an option, but the cost of higher processor speeds, greater storage volumes and huge memory sizes are out of reach for small laboratories. To deal with the barrage of sequencing data, distributed cloud computing platforms and frameworks such as Amazon Web Services ( Amazon.com Inc., Amazon Web Services, 2018 ), Apache Hadoop ( Apache Hadoop, 2018 ) and Apache Spark ( Apache Spark, 2018 ) have been used by researchers by taking advantage of parallel computation and economies of scale: large sequencing workloads are distributed in a cloud cluster that is comprised of many cheap, off-the-shelf compute nodes. These cloud-based solutions have been successfully used for human genomics ( Langmead  et al. , 2009a ), transcriptomics ( Roberts  et al. , 2013 ) and more recently for metagenomics applications ( Huang  et al. , 2018 ;  Zhou  et al. , 2017 ). Standard genomics and transcriptomics analyses for sequencing datasets usually begin by aligning sequencing reads to a reference genome ( Trapnell and Salzberg, 2009 ;  Wang  et al. , 2009 ), and producing abundance counts ( Trapnell  et al. , 2010 ); but in metagenomic analyses, the alignment step is performed against a collection of reference genomes that can be extremely large, slowing down the entire operation. The MapReduce model ( Dean and Ghemawat, 2008 ) along with the Spark framework have been popular in speeding up these crucial steps in the analysis of single-organism sequencing datasets, as researchers have framed the read-alignment and quantification tasks in terms of  map  and  reduce  operations: Langmead  et al.  used it to align human sequencing reads using the Bowtie read-mapping utility ( Langmead  et al. , 2009b ) and searching for single nucleotide polymorphisms (SNPs); while  Roberts  et al.  (2013 ) used it to speed up the quantification of human gene transcripts by the expectation-maximization (EM) algorithm. 2 Approach 2.1 Spark and MapReduce The MapReduce model was originally developed by Google ( Dean and Ghemawat, 2008 ), and most notably popularized by the  Apache Hadoop (2018 ) open-source project from the Apache foundation ( The Apache Software Foundation, 2018 ). The  Apache Spark (2018 ) project further expanded the Hadoop project, and introduced new optimizations for calculation speeds, and programming paradigms ( Zaharia  et al. , 2012 ). The MapReduce model abstracts away much of the boiler-plate programming details of developing distributable applications, and frees scientists and developers to focus their work on other critical, domain-specific, areas. The model is composed of two distinct steps: the  map()  step, and the  reduce()  step. Hadoop and Spark offer basic functions that can be used as the building blocks of a distributed computing model: the  map()  function takes as input a pair of parameters that make up a tuple consisting of a key and a value; while the  reduce()  function merges the output of the  map()  function by coalescing tuples with the same key. The MapReduce model, and the Spark framework in particular, have been employed in many DNA sequencing workflows for a number of years now ( Cattaneo  et al. , 2016 ;  Guo  et al. , 2018 ). The Crossbow project ( Langmead  et al. , 2009a ) from 2009 used Spark’s MapReduce implementation to identify Single Nucleotide Polymorphisms (SNPs) in human samples; eXpress-D ( Roberts  et al. , 2013 ) also used Spark to implement the expectation maximization (EM) algorithm for ambiguous DNA-fragment assignment. Spark has also been used in metagenomic analyses ( Guo  et al. , 2018 ) for mapping sequencing reads against small reference databases and for clustering metagenomes ( Rasheed and Rangwala, 2013 ). A natural approach to use the Spark framework for the analysis of mWGS datasets is to partition the input of reads into smaller subsets of reads to be processed by worker nodes in a Spark cluster. This strategy works well when the dataset of reads is large. The limitation of this strategy is that it does not scale to large collections of reference genomes because a data structure (index) of the reference collection of genomes must either be duplicated in each of the worker nodes, or multiple passes of the input can be used. Indexes built from large reference collections using a k-mer based strategy are often too large to be accommodated on a single commodity machine on the cloud ( Nasko  et al. , 2018 ). Fast k-mer based profiling strategies have been used for profiling of mWGS datasets ( Schaeffer  et al. , 2015 ;  Wood and Salzberg, 2014 ). But they trade-off speed for enormous indexes. More recently, alternative index-building strategies have been developed to allow the use of large collections of references with k-mer based tools, albeit only at species-level resolutions ( Zhou  et al. , 2018 ), but were not designed for use with a cloud-based infrastructure. Zhou  et al.  developed MetaSpark ( Zhou  et al. , 2017 ) to align metagenomic reads to reference genomes. The tool employs Spark’s Resilient Distributed Dataset (RDD) ( Zaharia  et al. , 2012 )—the main programming abstraction for working with large datasets—to cache reference genome and read information across worker nodes in the cluster. By using Spark’s RDD, MetaSpark is able to align more reads than previous tools. MetaSpark was developed with two reference datasets of bacterial genomes: a 0.6 GB reference, and the larger 1.3 GB from RefSeq’s bacterial repository. These reference sets are small compared to the 170 GB reference set of Ensembl, and because of MetaSpark’s use of an RDD to hold its index, it is unlikely that MetaSpark can scale to use them: the contents of an RDD are limited to available memory, and large reference sets would require correspondingly large memory allocations. It is worth pointing out the RDD memory limitations of MetaSpark in aligning reads: it took 201 min (3.35 h) to align 1 million reads to the small 0.6 GB reference using 10 nodes ( Zhou  et al. , 2017 ). SparkHit ( Huang  et al. , 2018 ) was developed by Huang  et al.  as a toolbox for scalable genomic analysis and also included the necessary optimizations for the preprocessing. SparkHit includes a metagenomic mapping utility called ‘SparkHit-recruiter’ that performs much faster than MetaSpark with similar sets of reference genomes. SparkHit performs well with large dataset of reads and small reference genome sets—the authors profiled 2.3 TB of whole genome sequencing reads against only 21 genomes in a little over an hour and a half. The limitation of SparkHit is that it builds its reference index using a k-mer strategy that does not scale to large collections of reference genomes ( Nasko  et al. , 2018 ), assuming that the reference database will change with each study that is analyzed. This assumption, and the method of index building, makes SparkHit unsuitable for profiling large metagenomic datasets against large collections of reference genomes. 2.2 Streaming techniques In order to process the large quantities of both input metagenomic datasets, and the large collections of reference genomes to profile against, new analysis paradigms are required that take advantage of highly parallelizable cloud infrastructure, as well as real-time data streams for consuming large input datasets. LiveKraken ( Tausch  et al. , 2018 ) was developed as a real-time classification tool that improves overall analysis times, and is based on the popular Kraken ( Wood and Salzberg, 2014 ) method for profiling metagenomic samples in Kraken-based workflows. LiveKraken uses the same approach as the HiLive ( Lindner  et al. , 2017 ) real-time mapper for Illumina reads, but extends it to metagenomic datasets. LiveKraken can ingest reads directly from the sequencing instrument in illumina’s binary basecall format (BCL) before the instrument’s run finishes, allowing real-time profiling of metagenomic datasets. Reads are consumed as they are produced at the instrument, and the metagenomic profile produced by LiveKraken is continuously updated. LiveKraken points the way to future classification systems that use streams of data as input, but its limitation is that it uses a k-mer based reference index—in its publication, LiveKraken was tested with an archived version of RefSeq (circa 2015) that only contained 2787 bacterial genomes. Since then, RefSeq has grown to over 50k genomes in the latest release (version 92), and creating a K-mer based index of it would require substantial computational resources. More recently, a Spark streaming-based aligner has been developed that uses streams of data to map reads single reference genomes. The tool, StreamAligner (Rathee and Kashyap, 2018), is implemented with Spark and the Spark-streaming API, and uses novel MapReduce-based techniques to align reads to the reference genome of a single organism. Unlike other methods, it creates its own reference genome index using suffix arrays in a distributed manner that reduces index-build times, and can then be stored in memory during an analysis run. By using the Spark streaming API, StreamAligner can continuously align reads to a single reference genome without the need of storing the input reads in local storage, and although StreamAligner has high performance when using a single genome, there is no evidence if it can scale to metagenomic workflows where tens of thousands of genomes are used, and the footprint of the reference genomes are much larger than could be fit in memory. 3 Materials and methods A natural approach to using MapReduce for large metagenomic analyses tasks is as follows. The  map  step divides the task of mapping the reads against a genomic index and the  reduce  step collects all the hits to each genome and constructs the microbial profile of the metagenomic sample. This approach works well when the same copy of the full genomic index can be farmed out to each node in the cluster. The approach fails when the index is too large to be provided to each cluster node or the collection of reads is too large for each cluster node. Streaming the reads allows for arbitrarily large collections of reads to be processed by each cluster node. Building an index of a ‘shard’ of the reference genome database and providing each cluster node with a smaller index allows for much larger reference databases to be used for mapping the reads ( Fig. 1 ).
 Fig. 1. Overview of the  Flint  System. Reference genomes are partitioned so that a large reference set is be distributed across a Spark cluster, and the number of partitions matches the number of worker nodes. Samples are streamed into the cluster to avoid storage overheads as shards of 250k reads. Reads are aligned to the distributed reference genomes using a double MapReduce pipeline that continually updates metagenomic profiles as samples are streamed into the cluster. Read alignments are never stored, and are processed by each worker node as soon as they are produced Our computational framework is primarily implemented using the  MapReduce  model ( Dean and Ghemawat, 2008 ), and deployed in a cluster launched using the  Elastic Map Reduce  (EMR) service offered by AWS (Amazon Web Services) ( Amazon.com Inc., Amazon Web Services, 2018 ). The cluster consists of multiple ‘commodity’ worker machines (a computational ‘worker’  node ), each with 15 GB of RAM, 8 vCPUs (each being a hyperthread of a single Intel Xeon core) and 100 GB of disk storage. Each of the worker computational nodes will work in parallel to align the input sequencing DNA reads to a ‘shard’ of the reference database ( Fig. 2 ); after the alignment step is completed, each worker node acts as a regular Spark executor node. By leveraging the work of multiple machines working at the same time,  Flint  is able to align a large number of reads to a large database of reference genomes in a much more efficient manner than that achieved by using a single powerful machine.
 Fig. 2. MapReduce workflow. Metagenomic samples can be streamed in from a distributed filesystem into the cluster were they are stored in an RDD. The first Map step generates alignments through Bowtie2 and feeds its resulting pairs to the first Reduce step, which aggregates the genomes that a single reads aligns to. The second Map step generates read contributions that are used in the second Reduce step to aggregate all the read contributions for a single genome. An output abundance matrix is generated which contains the abundances for each genome 3.1 Cluster provisioning A Spark ( Apache Spark, 2018 ) cluster was created using the AWS Console with the following software configuration: EMR-5.7.0, Hadoop 2.8.4, Ganglia 3.7.2, Hive 2.3.3, Hue 4.2.0, Spark 2.3.1 and Pig 0.17.0 in the US East (N. Virginia) region. The cluster is composed of homogeneous machines for both the driver node and worker nodes, and each machine is an Amazon machine instance of type c4.2xlarge. These instances contain 8 vCPUs, 15 GB of RAM, 100 GB of EBS storage and each cost on average $0.123 USD to run per hour on the ‘us-east’ availability zone on the Spot ( EC2 Spot Market, 2018 ) market as of this writing in January 2019. Newer instances (c5.2xlarge) are also available for use, but their availability is infrequent in large numbers, in addition to having a higher cost per hour to run. Resilient Distributed Datasets (RDD) ( Zaharia  et al. , 2012 ) are robust programming abstractions that can be used to persist data across a cluster of machines. We ingest reads from datastreams in batches of 500 000 reads that are processed by our mapreduce pipeline. Reads are consumed either directly from their location in an Amazon S3 bucket, or from a datastream source such as a Kafka or Kinesis source. An RDD of the input read stream is created in the master node that is then broadcasted out into all the worker nodes in the cluster. The input RDD of reads is partitioned into sets of reads that are each independently aligned to a reference genome partition in each of the worker nodes. 3.2 A ‘double’ MapReduce An obvious way to perform MapReduce for metagenomic analysis is to have the Map function produce tuples of the form  〈 g , 1 〉 , for every read  r  that is aligned to genomes  g , while the Reduce function aggregates all tuples of the form  〈 g , 1 〉  to obtain the abundance of genome  g  in the sample being analyzed, effectively generating output tuples of the form  〈 g , A ( g ) 〉 , where  A ( g )  is the reported abundance of genome  g  in the sample being analyzed. Unfortunately, a read may align to multiple genomes. Instead of counting a hit for every genome that the read aligns to, or counting it for only one of the genomes that the read aligns to, we follow the algorithm of  Valdes  et al.  (2015 ), which assigns fractional counts for the genomes that a read aligns to. In order to implement this, we employ a novel double MapReduce steps, thus making it a multi-stage operation. In the modified MapReduce, the Map function generates alignments in SAM format ( Li and 1000 Genome Project Data Processing Subgroup, 2009 ) by dispatching a subprocess of the Bowtie2 aligner and produces tuples of the form  〈 r , ( g , 1 ) 〉 , for every read  r  that is aligned to genomes  g . All tuples for the same read are aggregated by the first Reduce step to generate tuples of the form  〈 r , ( g , 1 / C ( r ) ) 〉 . The second Map step generates contributions of reads for a given genome, and the second Reduce step aggregates all tuples of the form  〈 g , c 〉  to obtain the abundance of genome  g  in the sample being analyzed, effectively generating output tuples of the form  〈 g , A ( g ) 〉 , where  A ( g )  is the reported abundance of genome  g  in the sample being analyzed obtained by aggregating all the fractional contributions of reads that map to that genome. Note that all intermediate tuples are stored in RDDs, one for each step. 3.3 Reference genome preparation Before we can use the bacterial genomes in the cluster, they need to be prepared. The process entails creating a Bowtie2 index for each shard of the reference database, and specific details on this procedure can be found in Section 2.1 of the supplementary manuscript. Briefly, the reference genomes are divided into smaller partitions that are each independently indexed by Bowtie2. The index preparation step can take considerable computational resources and time with a single machine. A parallel version of the indexing system can greatly improve performance and will be completed in the next release of  Flint . Once the partitions have been indexed they are then copied to an Amazon S3 (2018) bucket that serves as a staging location for the reference shards. The staging S3 bucket holds the index so that worker nodes can copy it during their provisioning step and the analysis can start; the S3 bucket is also public, and researchers can download copies of the prepared indices for their use. It should be noted that Ensembl’s bacterial genome collections have grown only modestly in the last couple of releases to minimize redundancy, and reference indices for new Ensembl releases can be built relatively quickly with utility scripts provided by  Flint . The cost of building a partitioned reference index is only accrued the first time it is built for a cluster of a particular size, and as part of the release of the  Flint  project, we are making available partitioned indices of Ensembl (v.41) of sizes 48, 64, 128, 256 and 512 which should be useful for researchers employing clusters of those sizes. These indices, along with the scripts necessary to build future versions, can be found at the GitHub repository. We currently use minimal annotations that keep track of basic attributes for each bacterial strain; these include taxonomic identifiers, assembly lengths, etc. Future releases of the software will include a more robust annotations package that will contain data on gram staining, pathogenicity and other properties. 
 Flint  uses a streaming model to quickly map a large number of reads to a large collection of reference bacterial genomes by using a distributed index. The Bowtie2 DNA aligner is used internally in Spark worker nodes to align reads to the local partition of the reference index, by using a MapReduce that continuously streams reads into worker nodes. Output alignments are parsed and tabulated by worker nodes, and then sent back to master node as alignment tasks finish.  Flint  can be deployed on any Spark cluster, as long as the necessary software dependencies are in place; the partitioned reference index for Ensembl’s 43k genomes is made available at the  Flint  website, and scripts are provided as part of the provisioning step that copy the partitions into worker nodes. 4 Results and discussion 4.1 Comparison to existing tools 
 Flint  was evaluated by comparing abundance profiles generated with  Flint  to those provided by HMP and those generated by Kraken ( Wood and Salzberg, 2014 ). Note that Kraken is a  k -mer based algorithm to align reads to genomic sequences and is known to be one of the most accurate ones ( McIntyre  et al. , 2017 ). We selected an anterior nares sample (SRS019067) with 528k reads from the Human Microbiome Project (HMP) and analyzed it with Kraken (2.0.7-beta) and  Flint  and compared the results to those provided by HMP in their community abundance profiles. HMP reported 36.7% aligned reads using a bacterial database of 1751 genomes, while Kraken was able to classify 36% of the reads using their RefSeq bacterial database of 14 506 genomes; in contrast,  Flint  was able to align 81% of the reads using Ensembl’s 43k bacterial genomes. The increase number of aligned reads is due to the larger number of genomes in Ensembl—Kraken uses RefSeq’s so-called ‘complete’ bacterial genomes, while Ensembl contains many draft genomes that increases the probability for mapping a read.  Flint  also aligns reads with Bowtie2 directly to the bacterial strain genomes, and does not apply lowest common ancestor (LCA) assignment to reads as Kraken does, which should mitigate any database diversity influences (genus, species and strain ratios) as noted by Nasko  et al.  ( Nasko  et al. , 2018 ). As shown in  Supplementary Figure S4 , both  Flint  and Kraken identify roughly the same set of genera, but at the species level,  Flint  identifies significantly more species. MetaSpark ( Zhou  et al. , 2017 ) and SparkHit ( Huang  et al. , 2018 ) are spark-based methods with a cluster infrastructure similar to  Flint  but their lack of support for large genome references makes direct comparison impossible. MetaSpark has a 201 min runtime for 1 million reads with 10 nodes, profiled against a 0.6 GB reference of bacterial genomes from NCBI. In comparison,  Flint  takes 67 s to profile 1 million paired-end reads against Ensembl’s 43 552 genomes (170 GB) with 64 nodes. 4.2 Reference genome collections To test the speed of our read alignment step, we downloaded a reference collection of bacterial genomes from the  Ensembl Bacteria (2018 ) repository (version 41). A total of 43 552 bacterial genomes (strain level) were downloaded in FASTA format, accounting for 4.6 million individual FASTA assembly references. The collection included reference sequences for fully assembled chromosomes and plasmids, as well as containing sequences for draft-quality supercontigs, the latter accounting for most of the reference files in the database. The Ensembl bacterial genomes (v.41) were downloaded from the public FTP site at  ftp.ensemblgenomes.org . Ensembl stores the FASTA files in ‘collection’ directories, and we recursively downloaded the ‘dna’ directory in each of the bacterial sub-folders. In total, 4 672 683 FASTA files were downloaded, with a data footprint on disk of just over 170 GB, accounting for 43 552 bacterial strains. Creating the Bowtie2 index for the bacterial genomes is a one-time operation as the index can be reused across cluster deployments. With a 64 worker-node cluster, we created 64 reference shards, each having a size of 2.6 GB on average. The total sequential indexing time for the 64 shards was 1d 20 h 4 m 33 s on a single machine, but we also used an LSF cluster (IBM Spectrum LSF., 2019) that indexed the 64 shards in parallel, and brought down the total indexing time to just over 3 h. Existing metagenomic profiling tools such as MetaSpark and SparkHit use an archived version of RefSeq as their reference genomes database—MetaSpark’s RefSeq bacterial references was for 1.3 GB of size. Given the fact that the Ensembl database used by  Flint  is roughly ten times larger, we looked into how a metagenomic profile could be different by looking at how many genomes are identified by using a large or small reference collection. To do this we randomly selected 1 M reads from an HMP anterior nares sample (SRS015996) and aligned its reads using Bowtie2 to two genome reference indices: the large collection created from the 43k Ensembl bacterial genomes, and the small collection created from 5591 bacterial representative and reference genomes from NCBI’s Genomes (RepNG). We investigated how many clades are identified by both references, and  Figure 3  displays the results.  Figure 3  shows a phylogenetic tree [created with the Interactive Tree Of Life (iTOL) visualization tool ( Letunic and Bork, 2016 )] showing the differences in the phylogenetic diversity of the taxa identified in the anterior nares sample. Genomes are called as ‘present’ by selecting only those genomes that have an average coverage greater than 80% along their genomic sequence. Nodes at the inner level of the figure represent the phylum taxonomic level, while nodes in the outer rings are at the species level. Green branches represents the clades identified by both references, blue branches represent clades identified by Ensembl, and red branches are clades identified by the RepNG reference set. Note that the number of clades identified by Ensembl at the higher Class and Genus taxonomic levels outnumber those identified when only using the RepNG subset.
 Fig. 3. Phylogenetic tree of taxa identified by Flint using 43k Ensembl bacterial genomes (blue), and 5k NCBI’s Genomes references (red) with an input of 1 M randomly selected reads from the HMP anterior nares sample (SRS015996). Genomes are identified if the average coverage in their genomic sequence is 80% or more 4.3 Experimental setup As mentioned earlier, the computational framework is primarily implemented using the  MapReduce  model ( Dean and Ghemawat, 2008 ), and deployed in a cluster launched in Amazon Web Services ( Amazon.com Inc., Amazon Web Services, 2018 )  Elastic Map Reduce  (EMR) service. The cluster consists of multiple worker machines (i.e. a computational ‘worker’  node ), each with 15 GB of RAM, 8 vCPUs (each being a hyperthread of a single Intel Xeon core) and 100 GB of disk storage. Each of the worker computational nodes will work in parallel to align the input sequencing DNA reads to a shard of the reference database; after the alignment step is completed, each worker node acts as a regular Spark executor node. By leveraging the work of multiple machines working at the same time, we are able to align millions of reads to the over 43k reference genomes in a much more efficient manner than either using only a single machine with considerable computational resources, or using other parallel computation approaches. Benchmarking tests were performed in Spark clusters of size 48, 64 and 128 worker nodes, all deployed in Amazon’s EMR service for very low costs. 4.4 Measuring accuracy using simulated datasets To get a measure of the accuracy of  Flint’ s read-alignment pipeline, and to test the robustness of the streaming infrastructure, we simulated synthetic Illumina reads using the InSilicoSeq ( Gourlé  et al. , 2018 ) metagenomic simulator. We created three replicate dataset groups to test the accuracy of the overall pipeline, and to verify that the streaming system would not introduce any duplicate artifacts, or that the reduce steps in the Spark cluster would not exclude any of the output alignments. Each replicate group consists of 12 datasets ranging from a dataset with 1 read to a dataset with 1 million reads, created with a log-normal abundance profile, and using the default error model for the HiSeq sequencing instrument available in InSilicoSeq. Specific details on the simulation protocol, cluster configuration and detailed results for each replicate set are available in the  Supplementary Materials . 
 Table 1  outlines the results for the synthetic HiSeq datasets. Dataset evaluations were performed on a 64 worker-node cluster in AWS, with each worker node containing 8 vCPUs and 15 GB of memory.  Flint  achieves good performance with the HiSeq dataset achieving 99% sensitivity across all three HiSeq replicates. Alignment times on the 64 node Spark cluster using the database of over 43k Ensembl bacterial genomes show that 1 million reads are aligned in just over 1 min with no loss of sensitivity. The ‘Alignments’ column contains the number of alignments that are produced as output for each dataset—these output alignments are not stored by the system, but rather they are processed as soon as they are generated by the worker nodes in the cluster. Table 1. HiSeq synthetic datasets Reads Alignments Time Alignment rate (%) % Sensitivity 1 1 2 s 344 ms 100 100 10 23 2 s 400 ms 100 100 100 172 2 s 376 ms 100 100 1000 1356 2 s 455 ms 100 100 5000 8592 2 s 517 ms 90 98 10 000 23 791 3 s 193 ms 94 99 50 000 74 543 5 s 138 ms 96 100 100 000 103 835 8 s 320 ms 93 99 250 000 187 349 15 s 788 ms 95 100 500 000 275 917 29 s 18 ms 93 97 750 000 513 954 45 s 91 ms 95 99 1M 617 933 1 m 14 s 713 ms 96 99 
 Note : Average alignment times and alignment rates for three synthetic datasets aligned against Ensembl’s 43k bacterial genomes. Sensitivity is the proportion of paired-end reads that were mapped correctly to the genome from which they were generated. Evaluations were performed on a 64 worker-node Spark cluster. 4.5 Human metagenomic samples After verifying the performance of the  Flint  system on simulated datasets, we tested the capabilities of the system on real metagenomic samples from the Human Microbiome Project (HMP) ( Human Microbiome Project Consortium, 2012 ), which was generated using an Illumina-based sequencing system. We therefore expected a comparable performance with the HMP data as with the synthetic dataset. 4.6 Cluster benchmarks Before testing the system with full human metagenomic samples, we ran a benchmark of randomly sampled paired-end reads from a HMP anterior nares sample (SRS015996) to confirm our previous observations on the synthetic datasets. Each of these read datasets was then processed through the  Flint  system running on a 64 worker-node cluster in AWS.  Table 2  presents the runtimes for each of the datasets, and  Flint  can process 1 million reads in about 67 s. Table 2. Initial cluster benchmarks Paired-end reads Alignments Time (ms) Memory (GB) 1 0 2 s 320 ms 4 10 36 2 s 422 ms 4 100 902 2 s 336 ms 4 1000 9252 2 s 316 ms 4.3 5000 53 918 2 s 455 ms 4.5 10 000 106 160 2 s 700 ms 4.9 50 000 538 594 5 s 437 ms 5.2 100 000 1 006 122 8 s 318 ms 5.8 250 000 2 349 518 17 s 164 ms 6.4 500 000 5 327 040 33 s 950 ms 7.6 750 000 8 439 356 50 s 880 ms 9.5 1M 10 710 420 1 m 7 s 609 ms 10.3 
 Note : Average alignment times in a 64 worker-node cluster for a set of randomly selected reads from a HMP anterior nares sample. The number of alignments column contains the output alignments that are generated by each set of reads; these alignments are processed as soon as they are produced and are not stored, therefore minimizing the local storage requirements necessary for profiling metagenomic samples. 4.7 Full human samples We analyzed 173 million paired-end reads from three HMP samples sequenced from anterior nares (SRS019067, 528k reads), stool (SRS065504, 116 M reads), and supragingival plaque (SRS017511, 56 M reads). These paired-end reads represent samples with varying levels of metagenomic diversity. For the purposes of analysis and the comparison of our execution pipeline, we created diversity classes defined by the number of unique genera present in each sample. To obtain our diversity classes, we analyzed 753 HMP samples for their abundance profiles and surveyed the number of unique genera as reported in the community abundance profiles provided by HMP (see  Supplementary Materials  for details); we then selected representative samples that contained 133 unique genera (high diversity class), 60 unique genera (medium diversity class) and 8 unique genera (low diversity class). The reasoning for using these samples was to test the performance of the  Flint  system in samples with varying degrees of metagenomic diversity. We speculated that low diversity samples would contain reads from a relatively small number of organisms, and therefore the alignment system would not spend too much time finding their genomes of origin. In contrast, the high diversity samples would contain reads from a large number of organisms, and the alignment system would spend more time and resources locating their origins. 
 Table 3  contains the results from running the three samples through the  Flint  system. The sample with the biggest number of paired-end reads, sample SRS065504 with 116 million paired-end reads, was profiled against Ensembl’s 43k genomes in about 105 min. The sample with the second largest number of paired-end reads, i.e. sample SRS017511 with 56 million paired-end reads, was profiled against the 43k genomes in about 94 min; while the sample with the lowest number of paired-end reads was profiled in 53 s. Note that the sample with 116 million paired-end reads was processed in about 10 min more than the sample with 56 million paired-end reads—this sample with 56 million reads is the sample that contains the highest number of unique genera (highest metagenomic diversity, 133 versus 60 in the larger sample). Since more alignments were found, the reads required more time to be processed. Table 3. HMP sample analysis Diversity class Unique genera Sample ID Paired-end reads Alignment execution time Streamed shards Avg. alignments per stream shard Low 8 SRS019067 528 988 0 h 0 m 53 s 2 1 763 227 Medium 60 SRS065504 116 734 970 1 h 45 m 30 s 234 1 471 036 High 133 SRS017511 56 085 526 1 h 34 m 51 s 113 1 535 626 
 Note : Diversity classes were established based on the number of unique genera in 753 HMP samples. Three samples were selected from each diversity class and analyzed in a 64 worker-node cluster. Alignment execution time measures the total time to align all the sample reads against Ensembl’s 43k bacterial genomes. The streamed shards are the number of 250k read sets that are streamed into the cluster, and the average alignment per shard is the average number of alignments produced by each shard. 4.8 Streaming performance The samples in  Table 3  were streamed into the cluster through Spark’s streaming engine. The entire sample is never ingested all at once, but rather, we stream in shards of each sample so that we do not overrun the cluster with so much data that it would cause a cluster failure. To find the ideal number of reads that we could use a size of a stream shard, we looked at the results in  Table 2  and  Figure 4 .  Figure 4B  displays a logarithmic curve of the alignment times for all 12 sizes of the paired-end read datasets, and while we can align 1 million reads in about 67 s, doing so creates so many alignments that each of the Spark executor processes running in each worker node could run out of memory. We looked for the ‘knee-in-the-curve’ in  Figure 4B , marked by the vertical magenta line, and identified a size of 250k paired-end reads as a good trade-off between shard size and cluster performance. When we analyzed the three HMP samples in  Table 3  we set the streaming shard size to 250k reads, and 2 shards were created for the anterior nares sample (low diversity, 500 k reads), 234 shards were created for the stool sample (medium diversity, 116 M reads) and 113 shards for the supragingival plaque sample (high diversity, 56 M reads).
 Fig. 4. Initial Benchmarks. ( A ) The running time for 12 paired-end read datasets in a 64 worker-node cluster. These 12 datasets were used to estimate the optimal number of reads that a 64 worker-node cluster could handle without any memory pressure, or network issues. Note that while 1 million paired-end reads can be mapped in 67 s against 43k bacterial strains, it is not ideal as the cluster’s memory is overwhelm with alignments. ( B ) The logarithmic running time of the 12 datasets, and the 250k paired-end read dataset was chosen as a good trade-off between speed and resource availability 4.9 Cloud costs All experiments were conducted in Amazon’s Elastic MapReduce service (EMR) ( Amazon EMR, 2018 ) and used the ‘c4.2xlarge’ machine instance type. These machines contain 8 vCPUs, 15 GB of RAM and 100 GB of EBS storage; at the time of the experimental runs, each machine cost $0.123 USD in the Amazon’s Spot market ( EC2 Spot Market, 2018 ). All results reported here were obtained on a cluster of 65 total machines (64 worker-nodes, 1 master node) with a cost of $0.123 USD per node, for an overall cluster cost of $8.00 per hour. 5 Conclusion In this work we have shown how large metagenomic samples comprising millions of paired-end reads can be profiled against a large collection of reference bacterial genomes in a fast and economical way. Our implementation relies on the freely available Spark framework to distribute the alignment of millions of sequencing reads against Ensembl’s collection of 43k bacterial genomes. The reference genomes are partitioned in order to distribute the genome sequences across worker machines, and this allows us to use large collections of reference sequences. By using the well-known Bowtie2 aligner under the hood in the worker-nodes, we are able to maintain fast alignment rates, without loss of accuracy. To date, profiling metagenomic samples against thousands of reference genomes has not been possible for research groups with access to modest computing resources. This is due to the size of the reference genomes and the financial costs of the computing resources necessary to employ them. By using distributed frameworks such as Spark, along with affordable cloud computing services such as Amazon’s EMR, we are able to distribute a large collection of reference genomes (totaling 170 GB of reference sequence, and 4.6 million assembly FASTA files) and use a MapReduce strategy to profile millions of metagenomic sequencing reads against them in a matter of hours, and at minimal financial costs, thus bringing sophisticated metagenomic analyses within reach of small research groups with modest resources. 
 Flint  is open source software written in Python and available under the MIT License (MIT). The source code can be obtained at the following GitHub repository:  https://github.com/camilo-v/flint . The repository includes instructions and documentation on provisioning an EMR cluster, deploying the necessary partitioned reference genome indices into worker nodes, and launching an analysis job.  Supplementary Materials , simulation datasets and partitioned reference indices can be found in the  Flint  project website at  http://biorg.cs.fiu.edu/ . Supplementary Material btz356_Supplementary_Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>An R package to analyse LC/MS metabolomic data: MAIT (Metabolite Automatic Identification Toolkit)</Title>
    <Doi>10.1093/bioinformatics/btu136</Doi>
    <Authors>Fernández-Albert Francesc, Llorach Rafael, Andrés-Lacueva Cristina, Perera Alexandre</Authors>
    <Abstract>Summary: Current tools for liquid chromatography and mass spectrometry for metabolomic data cover a limited number of processing steps, whereas online tools are hard to use in a programmable fashion. This article introduces the Metabolite Automatic Identification Toolkit (MAIT) package, which makes it possible for users to perform metabolomic end-to-end liquid chromatography and mass spectrometry data analysis. MAIT is focused on improving the peak annotation stage and provides essential tools to validate statistical analysis results. MAIT generates output files with the statistical results, peak annotation and metabolite identification.</Abstract>
    <Body>1 INTRODUCTION Liquid chromatography and mass spectrometry (LC/MS) is an analytical technique used widely in metabolomics to detect molecules in biological samples ( Theodoridis  et al. , 2012 ). A wide array of software tools is available for LC/MS profiling data analysis, including commercial, programmatic and online tools. A commercial example is Analyst®, whereas some open-source packages permit programmatic processing, such as the R package XCMS ( Smith  et al. , 2006 ) to detect peaks or CAMERA ( Kuhl  et al. , 2012 ) and AStream ( Alonso  et al. , 2011 ) for peak annotations. There have been efforts on just peak annotation using JAVA ( Brown  et al. , 2011 ). MZmine and mzMatch are modularized tools coded in JAVA that are focused on LC/MS data preprocessing and visualization ( Katajamaa  et al. , 2006 ;  Pluskal  et al. , 2010 ;  Scheltema  et al. , 2011 ). Online tools permit sample processing through a web Graphical User Interface, such as XCMSOnline ( http://xcmsonline.scripps.edu ) or MetaboAnalyst ( Xia  et al. , 2009 ). Refer to  Supplementary Table S1  for a comparative between the capabilities for some of the main available tools. In this context, we introduce a new R package called Metabolite Automatic Identification Toolkit (MAIT) for automatic LC/MS analysis. The goal of the MAIT package is to provide an array of tools that makes programmable metabolomic end-to-end statistical analysis possible (see Section 3 of the  Supplementary Material  for details about the MAIT modularity). MAIT includes functions to improve peak annotation through the process called biotransformations and to assess the predictive power of statistically significant metabolites that quantify class separability. 2 METHODS MAIT includes the stages peak detection, peak annotation, statistical analysis and table and plots creation ( Fig. 1 ). The peak detection stage detects the peaks in the LC/MS sample files. The peak annotation stage improves the identification of the metabolites in the metabolomic samples by increasing the chemical and biological information in the dataset. A statistical analysis reveals the significant sample features and measures their predictive power. MAIT uses the R package XCMS to detect and align peaks. For the peak annotation step, MAIT uses three steps:
 First, MAIT uses the CAMERA package to perform the first annotation step ( Kuhl  et al. , 2012 ). In this stage, MAIT uses a peak correlation distance and a retention time window to find which peaks came from the same source metabolite based. The peaks within each peak group are annotated following a reference adduct/fragment table and a mass allowance window. Biotransformations could be related to specific in-source mass losses. Therefore, in the second annotation step, they are detected using a mass allowance window inside the peak groups ( Breitling  et al. , 2006 ). For this search, MAIT already includes a biotransformations table (here Human biotransformations). User-defined biotransformation tables can be set as input, following the procedure defined in  Supplementary Text  (Section 6.6). Finally, a predefined metabolite database is mined for significant masses. This identifies metabolites with the help of the Human Metabolome Database ( Wishart  et al. , 2009 ), 2009/07 version. 
 Fig. 1. Correspondence between MAIT functions (centre column), generated output files (left column) and their functionality (right column) The objective of analysing the metabolomic profiling data is to obtain the statistically significant features (SSF) that contain the highest amount of class-related information. To gather these features, MAIT can apply statistical tests such as ANOVA or Student’s t-test to every feature, selecting the significant set of features given a threshold  P -value. A validation test is included to quantify SSF class separability by a repeated random subsampling cross-validation using three methods: partial least squares and discriminant analysis, support vector machines and K-nearest neighbours ( Hastie  et al. , 2009 ). MAIT computes overall and class-related classification ratios to evaluate the SSF class-related information. 3 RESULTS The example data files are a subset of the data used in the reference ( Saghatelian  et al. , 2004 ), which are distributed freely through the faahKO package ( Smith, 2012 ). MAIT was used to read and analyse these samples using the functions depicted in  Figure 1  (see the tutorial in the  Supplementary Information ). The significant features for each class are found using statistical tests and analysed through the different plots that MAIT produces. Using the following function call, 2640 peaks were detected:
 R&gt; MAIT &lt;- sampleProcessing(dataDir =  “ Dataxcms ” , project 
 =  “ MAIT_Demo ” , snThres = 2, rtStep = 0.03 ) 
 At this point, the first annotation stage is launched:
 R&gt; MAIT &lt;- peakAnnotation(MAIT.object = MAIT) 
 Next, we gather the significant features from the peaks detected. After the Welch’s tests, 106 of these features were found to be significant through the spectralSigFeatures function. Statistical plots such as heat maps, boxplots and principal component analysis score plots can be generated ( Supplementary Figs S3  and  S4 ). Significant features are annotated after checking for certain neutral losses (biotransformations).
 R&gt; MAIT &lt;- spectralSigFeatures(MAIT,  P  = 0.05) R&gt; MAIT &lt;- Biotransformations(MAIT, peakPrecision = 0.005) 
 By using only the SSF, a validation stage is launched, obtaining a classification ratio of 100% with three training samples for all classifiers. These results suggest that the significant variables separate both classes completely.
 R&gt; MAIT &lt;- Validation(MAIT, Iterations = 20, trainSamples 
 = 3 ) 
 Finally, the database is mined to identify the significant features.
 R&gt; MAIT &lt;- identifyMetabolites(MAIT, peakTolerance = 0.005) 
 4 CONCLUSIONS MAIT provides a set of tools and functions to perform an automatic end-to-end analysis of LC/MS metabolomic data, putting special emphasis on peak annotation and metabolite identification. In addition, MAIT validation functions make it possible to estimate predictive power for significant variables. Funding :  Spanish national  (grants  AGL2009-13906-C02-01/ALI  and  AGL2010-10084-E ), the CONSOLIDER INGENIO 2010 Programme,  FUN-C-FOOD  ( CSD2007-063 ) from the  MICINN  and Merck Serono 2010 Research Grants (Fundación Salud 2000).  Spanish Ministerio de Ciencia y Tecnología  through  TEC2010-20886-C02-02  and  TEC2010-20886-C02-01  (in part) A.P. is part of the  2009SGR-1395  consolidated research group of the  Generalitat de Catalunya, Spain . CIBER-BBN is an initiative of the Spanish ISCIII. R.L. thanks the MICINN and the European Social Funds for their financial contribution to the R. L. Ramón y Cajal contract (Ramon y Cajal Programme, MICINN-RYC). F.F.-A. thanks EVALXARTA-UB and Agència de Gestió d’Ajuts Universitaris I de Recerca, AGAUR (Generalitat de Catalunya), for their financial support. Conflict of Interest:  none declared. 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>ISA software suite: supporting standards-compliant experimental annotation and enabling curation at the community level</Title>
    <Doi>10.1093/bioinformatics/btq415</Doi>
    <Authors>Rocca-Serra Philippe, Brandizi Marco, Maguire Eamonn, Sklyar Nataliya, Taylor Chris, Begley Kimberly, Field Dawn, Harris Stephen, Hide Winston, Hofmann Oliver, Neumann Steffen, Sterk Peter, Tong Weida, Sansone Susanna-Assunta</Authors>
    <Abstract>Summary: The first open source software suite for experimentalists and curators that (i) assists in the annotation and local management of experimental metadata from high-throughput studies employing one or a combination of omics and other technologies; (ii) empowers users to uptake community-defined checklists and ontologies; and (iii) facilitates submission to international public repositories.</Abstract>
    <Body>1 HIGH-THROUGHPUT OMICS STUDIES The development of high-throughput genomic and post-genomic (hereafter, ‘omics’) technologies entails changes in the handling, processing and sharing of data (Schofield  et al. ,  2009 ). Omics datasets are often complex and rich in context. Studies may run material through several kinds of assay, using both omics and other technologies; for example, studying the effect of a compound on rat liver through transcriptome, proteome and metabolome profiling (using high-throughput sequencing and two kinds of mass spectrometry, respectively) alongside conventional analyses (e.g. histopathology). Such data must be accompanied by enough contextual information (i.e. metadata; sample characteristics, technology and measurement types; instrument parameters and sample-to-data relationships) to make datasets comprehensible and reusable if they are to underpin future investigations. Many funders and journals require that researchers share data, and encourage the enrichment and standardization of experimental metadata (Field  et al. ,  2009 ). Consequently, more and richer studies are flowing into public databases. However, two bottlenecks can significantly hamper this process, necessitating urgent solutions. First, international public repositories for ‘omics data such as GEO (Barrett  et al. ,  2009 ), ArrayExpress (Parkinson  et al. ,  2009 ), PRIDE (Vizcaíno  et al. ,  2010 ), ENA, SRA and DRA (Shumway  et al. ,  2010 ), have their own submission formats, data models and terminologies, created for specific types of assay. This complicates the submission process for researchers producing multi-assay studies (and greatly increases the risk that these datasets become irrevocably fragmented). Secondly, the shortage of curators to check and annotate submissions to public repositories—a situation unlikely to change soon—necessitates better annotation at source (by experimentalists or community-based efforts; Howe  et al. ,  2008 ). Free software, with automated content validation, is required to facilitate the collection, management and curation of a variety of study inhouse, and to format those data for submission to public repositories. Such software should support community-defined reporting standards, such as the minimum information checklists listed by the MIBBI Portal (Taylor  et al. ,  2007 ), and ontologies, (Côté  et al. ,  2006 ; Smith  et al. ,  2007 ; Noy  et al. ,  2009 ). The Investigation/Study/Assay (ISA) infrastructure described here is the first general-purpose format and freely available desktop software suite designed to regularize local management of experimental metadata by enabling curation at source, supporting community-defined reporting standards and preparing studies for submission to public repositories. 2 THE ISA FORMAT AND SOFTWARE SUITE The software suite comprises five platform-independent Java-based software components for local use, including a relational database ( Fig. 1 ), built around the ISA-Tab format. The components work both as stand-alone applications and as a unified system to assist in the local management and storage of experimental metadata, and to facilitate data submission to international public repositories. All components run as ‘desktop’ applications; in addition, the database component features a web-based query interface.
 Fig. 1. The role of each ISA software component, showing their interrelations, target users and the flow of information through the system. 2.1 ISA-Tab: an extensible, cross-domain format ‘ Investigation ’, ‘ Study ’ and ‘ Assay ’ are the three key entities around which the general-purpose ISA-Tab format for structuring and communicating metadata is built (Sansone  et al. ,  2008 ).  Investigation  contains all the information needed to understand the overall goals and means used in an experiment;  Study  is the central unit, containing information on the subject under study, its characteristics and any treatments applied. Each  Study  has associated  Assay(s) , producing qualitative or quantitative data, defined by the type of measurement (i.e. gene expression) and the technology employed (i.e. high-throughput sequencing). The hierarchical structure of ISA-Tab enables the representation of studies employing one or a combination of omics and other technologies, overcoming the fragmentation of the existing submission formats built for specific types of assay. To ensure conversion, ISA-Tab has been designed with reference to these existing ‘omics formats (Jones  et al. ,  2007 ), complementing and extending their work where necessary; for example, it shares both syntax and the use of easily-manipulable tab-delimited text files with ArrayExpress’ MAGE-Tab (Rayner  et al. ,  2006 ). Additionally, where omics-based technologies are used in clinical or non-clinical studies, ISA-Tab complements existing biomedical formats such as the Study Data Tabulation Model ( http://www.cdisc.org/sdtm ), endorsed by the US Food and Drug Administration. ISA-Tab also complements the XML formats used by the PRIDE, ENA, SRA and DRA repositories, and consequently offers a way to render their experimental metadata documents in a more user-friendly format. Note though that ISA-Tab is simply a format; the decision on how to regulate its use (i.e. enforcing the filling of required fields, or the use of ontologies) is left to local administrators' use of ISA software components, or the growing number of other systems and groups implementing the format (e.g. Krestyaninova  et al. ,  2009 ; SysMO-DB  http://www.sysmo-db.org/community ; XperimentR,  http://www.imperial.ac.uk/bioinfsupport/resources/data_management/ ; more given on the ISA web site). 2.2 ISAcreator: a user-friendly editor This desktop application enables users (i.e. experimentalists) to compile experimental metadata sets, and to import and edit existing ISA-Tab formatted files. It breaks down overall descriptions into relatively simple parts, uses graphical abstraction to enable visualization of the information described and facilitates time-efficient description of experimental steps by remembering prior behaviour (through user profiles). ISAcreator's aesthetically pleasing interface makes extensive use of Java Swing and external open source libraries (e.g. Prefuse,  http://prefuse.org/ ). The editor uses a style of form- and spreadsheet-based data entry that is likely to be familiar to researchers, augmenting basic functionality such as ‘auto-fill’ and ‘undo’ with advanced features, listed below. 2.2.1 Ontology support A dedicated ‘widget’ allows ontology terms to be searched for and inserted in real time  via  the BioPortal (Noy  et al. ,  2009 ) and the Ontology Lookup Service (Côté  et al. ,  2006 ). Terms from those sources are imported along with core metadata (identifiers, definitions and ontology version); term selection is facilitated by a search history displaying prior choices (through user profiles). 2.2.2 Design wizard An alternative way for users to enter information that leverages common patterns to reduce repetitive tasks by guiding users through a series of questions that elicit information about the design of the  Study  and associated  Assay(s) . 2.2.3 Spreadsheet import As a second alternative, this widget enables the mapping and import of information from existing spreadsheets; also the reformatting and reannotation of  legacy  data. 2.2.4 Data file chooser This widget appends data files located either local to the operator, or identified by FTP on a remote system, to an experimental metadata sets. Upon completion of a valid investigation report, ISAcreator outputs a compressed ‘ISArchive’ containing the ISA-Tab-formatted metadata and either the actual data files, or a reference to them, if necessary (e.g. because of their large size), consisting of their address and file name. 2.3 ISAconfigurator: standards-compliant templates This desktop application allows ‘power users’ (i.e. community curators) to customize the fields displayed by ISAcreator, and for example, to meet the requirements of one or more MIBBI minimum information checklists by declaring certain fields mandatory, or by specifying allowed values (e.g. drawn from a set of ontology terms, or formatted in a specific manner). Configuration files from ISAconfigurator are read by ISAcreator, which then generates interface components as required. 2.4 ISAvalidator: adherence to templates This desktop application also reads configuration files and checks both that completed ISA-Tab files meet specified requirements and that associated data files have been linked. Whether ISA-Tab files are created with ISAcreator or another way (e.g. with spreadsheet software), ISAvalidator checks that the document is syntactically correct and internally consistent, and reports on errors (i.e. missing or incorrect values). 2.5 BioInvestigation Index: local storage An ISArchive provides a simple way to store and share information in a structured manner, but those tasks are better performed by uploading such a file to an instance of our ‘BioInvestigation Index’ (BII), or another system that implements ISA-Tab import. The BII includes a management tool and relational database (tested with Oracle, MySQL and PostgreSQL). The former enables validation and loading of an ISArchive and provides simple permissions functionality to link users (or groups of users) to studies. The latter manages the storage of experimental metadata, which can be collectively searched and browsed  via  a query interface or web services; the destination for associated data files, and their protocol for transfer, is custom defined by the local administrator on installation. As an example, a publicly accessible instance of the BII, maintained by the European Bioinformatics Institute ( http://www.ebi.ac.uk/bioinvindex ), has proven useful as a curation and storage system for multi-assay studies, and as a mechanism for submitting data files to ArrayExpress, PRIDE, ENA and SRA. Installation of the BII system requires some knowledge of database management. However, it is portable enough to be easily installed in individual labs, to maximize the efficiency with which high-throughput studies can be managed and shared among users that have been granted access to them. 2.6 ISAconverter: submission to public repositories ISAconverter recodes the relevant parts of ISArchives as MAGE-Tab, PRIDE XML or SRA-XML (used by ArrayExpress, PRIDE and ENA, SRA and DRA, respectively), enabling combined submission to public omics repositories. It is readily extensible to support export of other formats, e.g. SOFT required by GEO (Barrett  et al. ,  2009 ). Mappings for format elements are available in the ISA-Tab specification and documentation on the ISA web site. 3 COLLABORATIONS AND CASE STUDIES Developed for the European multi-site ‘CarcinoGENOMICS’ project (Vinken  et al. ,  2008 ), the ISA software suite version one was released in early 2009. The core ISA developers are engaged with an ever-growing number of collaborators: case studies from early implementers already provide evidence of the diverse life science scenarios in which the suite's various components have been successfully tested and are being used with large datasets (details on the ISA web site). The main limitations recorded to date are simply the person hours required to specify the standards and ontologies to be used and to actually curate studies. Demonstrable acceptance and community engagement has also brought a new funding stream for this project, allowing us to continue the collaborative development of this exemplar system that supports data sharing policies, promotes the uptake of community-defined reporting standards and ontologies and enables curation at source (Field  et al. ,  2009 ). The ISA components, in particular the BII, have been designed to provide core functionalities. Inevitably, each collaborator has additional in-house requirements that are too specific to be included as core functionality. This may be due to the nature of their studies or their need for one or more ISA software components to be interoperable with existing systems. To support further collaborative development, the core ISA developers are setting up an environment for distributed development, and are augmenting the ISA code base with Application Programming Interfaces (APIs). Ongoing collaborative activities include: a module to enable the analysis of ISA-Tab formatted metadata and any associated data, using R; integration with other data management and analysis systems (e.g. Fang  et al. ,  2009 ; MetWare,  http://metware.org ); and giving assistance to the growing number of projects exploring the tools and underlying format (e.g. Sage  http://sagecongress.org/WP/workstreams/Standards ; Kawaji  et al. ,  2009 ). Other collaborative activities include an enhanced user authentication system, support for additional formats such as RDF, OWL and SOFT, converters to/from lab equipment-related file formats (e.g. sampling robots and mass spectrometers) and improved packaging and distribution mechanisms to offer a single download bundle to facilitate installation. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Utopia documents: linking scholarly literature with research data</Title>
    <Doi>10.1093/bioinformatics/btq383</Doi>
    <Authors>Attwood T. K., Kell D. B., McDermott P., Marsh J., Pettifer S. R., Thorne D.</Authors>
    <Abstract>Motivation: In recent years, the gulf between the mass of accumulating-research data and the massive literature describing and analyzing those data has widened. The need for intelligent tools to bridge this gap, to rescue the knowledge being systematically isolated in literature and data silos, is now widely acknowledged.</Abstract>
    <Body>1 INTRODUCTION The typhoon of technological advances witnessed during the last decade has left in its wake a flood of life-science data, and an increasingly impenetrable mass of biomedical literature describing and analysing those data. Importantly, the modern frenzy to gather more and more information has left us without adequate tools either to mine the rapidly increasing data- and literature-collections efficiently, or to extract useful knowledge from them. To be usable, information needs to be stored and organized in ways that allow us to access, analyze and annotate it, and ultimately to relate it to other information. Unfortunately, however, much of the data accumulating in databases and documents has not been stored and organized in rigorous, principled ways. Consequently, finding what we want and, crucially, pinpointing and understanding what we already know, have become increasingly difficult and costly tasks (Attwood  et al. ,  2009 ). A group of scientists for whom these problems have become especially troublesome are biocurators, who must routinely inspect thousands of articles and hundreds of related entries in different databases in order to be able to attach sufficient information to a new database entry to make it meaningful. With something like 25 000 peer-reviewed journals publishing around 2.5 million articles per year, it is simply not possible for curators to keep abreast of developments, to find all the relevant papers they need, to locate the most relevant facts within them, and simultaneously to keep pace with the inexorable data deluge from ongoing high-throughput biology projects (i.e. from whole genome sequencing). For example, to put this in context, Bairoch estimates that it has taken 23 years to manually annotate about half of Swiss-Prot's 516 081 entries (Bairoch,  2009 ; Boeckmann  et al. ,  2003 ), a painfully small number relative to the size of its parent resource, UniProtKB (The UniProt Consortium,  2009 ), which currently contains ∼11 million entries. Hardly surprising, then, that he should opine, ‘It is quite depressive to think that we are spending millions in grants for people to perform experiments, produce new knowledge, hide this knowledge in a often badly written text and then spend some more millions trying to second guess what the authors really did and found’ (Bairoch,  2009 ). The work of curators, and indeed of all researchers, would be far easier if articles could provide seamless access to their underlying research data. It has been argued that the distinction between an online paper and a database is already diminishing (Bourne,  2005 ); however, as is evident from the success stories of recent initiatives to access and extract the knowledge embedded in the scholarly literature, there is still work to be done. Some of these initiatives are outlined below. The Royal Society of Chemistry (RSC) took pioneering steps towards enriching their published content with data from external resources, creating ‘computer-readable chemistry’ with their Prospect software (Editorial,  2007 ). They now offer some of their journal articles in an enhanced HTML form, annotated using Prospect: features that may be marked up include compound names, bio- and chemical-ontology terms, etc. Marked-up terms provide definitions from the various ontologies used by the system, together with InChI (IUPAC International Chemical Identifier) codes, lists of other RSC articles that reference these terms, synonym lists, links to structural formulae, patent information and so on. Articles enriched in this way make navigation to additional information trivial, and significantly increase the appeal to readers. In a related project, the  ChemSpider Journal of Chemistry  exploits the ChemMantis System to mark up its articles ( http://www.chemmantis.com ). With the ChemSpider database at its heart, ChemMantis identifies and extracts chemical names, converting them to chemical structures using name-to-structure conversion algorithms and dictionary look-ups; it also marks up chemical families, groups and reaction types, and provides links to Wikipedia definitions where appropriate. In an initiative more closely related to the life sciences,  FEBS Letters  ran a pilot study (Ceol  et al. ,  2008 ) with the curators of the MINT interaction database (Chatr-aryamontri  et al. ,  2007 ), focusing on integration of published protein–protein interaction and post-translational modification data with information stored in MINT and UniProtKB. Key to the experiment was the Structured Digital Abstract (SDA), a device for capturing an article's key facts in an XML-coded summary, essentially to make them accessible to text-mining tools (Seringhaus and Gerstein,  2007 ); these data were collected from authors via a spreadsheet, and structured as shown in  Figure 1 —while clearly machine-readable, this format has the notable disadvantage of being rather human unfriendly.
 Fig. 1. Structured summary for an article in  FEBS Letters  (Lee  et al. ,  2008 ). Three interactions are shown, with their links to MINT and UniProtKB. A different approach was taken with BioLit (Fink  et al. ,  2008 ), an open-source system that integrates a subset of papers from PubMed Central with structural data from the Protein Data Bank (PDB) (Kouranov  et al. ,  2006 ) and terms from biomedical ontologies. The system works by mining the full text for terms of interest, indexing those terms and delivering them as machine-readable XML-based article files; these are rendered human-readable via a web-based viewer, which displays the original text with colored highlights denoting additional context-specific functionality (e.g. to view a 3D structure image, to retrieve the protein sequence or the PDB entry, to define the ontology term). A more adventurous approach was taken by Shotton  et al.  ( 2009 ), who targeted an article in  PLoS Neglected Tropical Diseases  for semantic enhancement. The enrichments they included were live Digital Object Identifiers and hyperlinks; mark-up of textual terms (disease, habitat, organism, etc.), with links to external data resources; interactive figures; a re-orderable reference list; a document summary, with a study summary, tag cloud and citation analysis; mouse-over boxes for displaying the key supporting statements from a cited reference; and tag trees for bringing together semantically related terms. In addition, they provided downloadable spreadsheets containing data from the tables and figures, enriched with provenance information and examples of ‘mashups’ with data from other articles and Google Maps. To stimulate further advances in the way scientific information is communicated and used, Elsevier offered its Grand Challenge of Knowledge Enhancement in the Life Sciences in 2008. The contest aimed to develop tools for semantic annotation of journals and text-based databases, and hence to improve access to, and dissemination of, the knowledge contained within them. The winning software, Reflect, focused on the dual need of life scientists to jump from gene or protein names to their molecular sequences and to understand more about particular genes, proteins or small molecules encountered in the literature (Pafilis  et al. ,  2009 ). Drawing on a large, consolidated dictionary that links names and synonyms to source databases, Reflect tags such entities when they occur in web pages; when clicked on, the tagged items invoke pop-ups displaying brief summaries of entities such as domain and/or small molecule structures, interaction partners and so on, and allow navigation to core biological databases like UniProtKB. All of these initiatives differ slightly in their specific aims, but nevertheless reflect the same aspiration—to get more out of digital documents by facilitating access to underlying research data. As such, it is interesting to see that a number of common themes have emerged: most are HTML- or XML-based, providing hyperlinks to external web sites and term definitions from relevant ontologies via color-coded textual highlights; most seem to ignore PDF as a foundation for semantic enrichment (despite a significant proportion of publisher content being offered in this format). The results of these projects are encouraging, each offering valuable insights into what further advances need to be made: clearly, we need to be able to link more than just a single database to a single article, or a single database to several articles, or several databases to a single issue of a single journal. Although necessary proofs of principle, these are just first steps towards more ambitious possibilities, and novel tools are still needed to help realize the goal of fully integrated literature and research data. In this article, we describe a new software tool, Utopia Documents, which builds on Utopia, a suite of semantically integrated protein sequence/structure visualization and analysis tools (Pettifer  et al. ,  2004 ,  2009 ). We describe the unique functionality of Utopia Documents, and its use in semantic mark-up of the  Biochemical Journal  ( BJ ). We also outline the development of a number of new plugins, by means of which we have imported additional functionality into the system via web services. 2 SYSTEM AND METHODS Utopia Documents was developed in response to the realization that, in spite of the benefits of ‘enhanced HTML’ articles online, most papers are still read, and stored by researchers in personal archives, as PDF files. Several factors likely contribute to this reluctance to move entirely to reading articles online: PDFs can be ‘owned’ and stored locally, without concerns about web sites disappearing, papers being withdrawn or modified, or journal subscriptions expiring; as self-contained objects, PDFs are easy to read offline and share with peers (even if the legality of the latter may sometimes be dubious); and, centuries of typographic craft have led to convergence on journal formats that (on paper and in PDF) are familiar, broadly similar, aesthetically pleasing and easy to read. In its current form, Utopia Documents is a desktop application for reading and exploring papers, and behaves like a familiar PDF reader (Adobe Acrobat, KPDF, OS X Preview, etc.); but its real potential becomes apparent when configured with appropriate domain-specific ontologies and plugins. With these in place, the software transforms PDF versions of articles from static facsimiles of their printed counterparts into dynamic gateways to additional knowledge, linking both explicit and implicit information embedded in the articles to online resources, as well as providing seamless access to auxiliary data and interactive visualization and analysis tools. The innovation in the software is in implementing these enhancements without compromising the integrity of the PDF file itself. Suitably configured, Utopia Documents is able to inspect the content and structure of an article, and, using a combination of automated and manual mechanisms, augment this content in a variety of ways: 2.1 Adding definitions Published articles are typically restricted to a defined page count, and are usually written for a specific audience. Explanations of terms that might be useful to newcomers to a particular field are therefore frequently omitted. Utopia Documents allows editors and authors to annotate terms with definitions from online resources (Wikipedia, UniProtKB, PDB, etc.), and permits readers to easily find definitions for themselves. 2.2 Interactive content and auxiliary data Figures and tables in printed form are typically static snapshots of richer data (which are nowadays often available elsewhere online). For example, a table may represent the salient fragment of a much larger experimental dataset, or an image of a protein structure might highlight one specific feature of that molecule. Utopia Documents is able to transform such static tables and figures,  in situ , into dynamic, interactive objects, providing richer access to the underlying data. 2.3 Linking references to source articles Most articles published today are made available in electronic form, and substantial efforts are also made by publishers to make their back-catalogues electronically accessible. Navigating the multitude of online repositories and bibliographic tools, however, is complex. Utopia Documents simplifies the process of finding related articles by automatically linking references to their digital online versions. 3 IMPLEMENTATION The software architecture comprises three main components, as shown in  Figure 2 : ‘the core’, providing generic mechanisms for displaying and manipulating articles, both programmatically and interactively; ‘the plugins’, which analyze, annotate and visualize document features, either automatically or under the guidance of a user; and ‘the ontology’, which is used to semantically integrate the other components.
 Fig. 2. The architecture of Utopia Documents, showing the relationship between the GUI (top), plugins (middle) and ontology (bottom). 3.1 The core Optimized for interactivity, the multi-threaded core of Utopia Documents is written in C++ using Trolltech/Nokia's Qt toolkit. The core serves two purposes: (i) it performs the relatively mundane tasks necessary to generate and manage the interactive Graphical User Interface (GUI) and to co-ordinate the behavior of the plugins, which are loaded on demand at run-time; (ii) it carries out the low-level analysis of PDF documents, including reading their file format and converting them into both a visual representation to be displayed on-screen and a hierarchical semantic model for later higher-level analysis and annotation by the plugins. The analysis performed by the core is generic in nature, and is restricted at this stage to identifying typographical and layout-based features common to scholarly papers from any discipline. Once this raw structure has been generated, using various heuristics, the system then identifies higher-level typographical constructs (titles, sections, headings, figures, tables, references, etc.). Annotations identifying these features are assembled, and added to the raw hierarchy to form a semantic model that is then shared with, and further annotated by, the plugins. From these ‘structural semantics’, a ‘fingerprint’ is created that uniquely identifies the article being read, and allows the annotations to be associated with it. 3.2 The plugins Two broad classes of plugin are defined. ‘Annotators’ inspect a document's content and semantic structure, then either apply local algorithms or communicate with external services in order to create annotations containing additional content (e.g. definitions of terms, user comments, links to other resources). Annotator plugins may be configured to execute automatically when a document is loaded, typically performing document-wide tasks, such as identifying terms of biological or chemical interest; alternatively, they may be invoked manually via the GUI—in these cases, the plugins have access to the GUI's state, and can generate context-specific annotations (e.g. associating a highlighted region of text with a specific comment made by a user, or finding the definition of a highlighted concept in an online database.) ‘Visualizers’ provide various mechanisms for displaying and interacting with annotations: e.g. an annotation containing static images, links and ‘rich text’ may be displayed using a browser-like visualizer, whereas one containing the structure of a molecule from the PDB might be displayed as an interactive 3D object. Both types of plugin may be written in C++ or Python, and are executed in their own asynchronous environment, marshalled by the core. 3.3 The ontology Rather than create ‘hard-wired’ relationships between the system's components, a simple ontology (in its current form, a hierarchical taxonomy) connects the plugins to the core and to one another. This form of semantic integration allows the components to cooperate flexibly in the analysis of document content and structure, and allows plugins to be developed independently of one another, with sensible relationships and behavior being inferred at run-time rather than being pre-determined: e.g. an annotator plugin may mark content as containing ‘protein structure’; a visualizer plugin, encountering this annotation at a later stage, can then decide whether to display this as a 2D static image, an interactive 3D model or as a 1D amino acid sequence. 3.4 Access to remote resources Via its plugins, Utopia Documents has access to a wealth of bioinformatics data. Each plugin can use whatever client libraries are appropriate to access web-service endpoints (both SOAP- and REST-style), as well as other remotely accessible resources, such as relational databases and RDF stores. Of particular note here are two substantial ‘linked data’ initiatives that have proven to be of enormous value to our work. The first of these, the Bio2RDF project (Belleau  et al. ,  2008 ), combines the content of many of the major life-science databases as a federated linked-data network accessible via SPARQL and REST interfaces. This both offers a single mechanism via which Utopia Documents can search multiple primary databases, and enforces a consistent naming scheme between sources, allowing results to be interrelated. The second (and more general), DBPedia, is a machine-readable RDF-based conversion of the popular human-readable Wikipedia (Auer  et al. ,  2007 ). Although containing much information that is irrelevant to the life sciences, Wikipedia (and thus DBPedia) has evolved to represent a significant and mostly authoritative corpus of scientific knowledge—a study performed by the journal  Nature  concluded that its entries were as accurate (or indeed, as error prone) as those published in  Encylopaedia Britannica  (Giles,  2005 ,  2006 ). The combined application of ontologies and RDF in DBPedia allows queries performed by Utopia Documents to traverse only the portions of the DBPedia network that are semantically related to the life sciences. Thus, in the context of a paper on enzymatic substrate cleavage, a search initiated via Utopia Documents for the term ‘cleavage’ returns far more appropriate definitions than would the same search in a more generic context. Utopia Documents is freely available via the project web site for Mac OS X (10.4 and later), Microsoft Windows XP and Vista and Ubuntu Linux. We welcome any feedback on the software. 4 RESULTS AND DISCUSSION Utopia Documents was developed in response to the need to achieve tighter coupling between published articles and their underlying data, ultimately to facilitate knowledge discovery. The tool was designed with two classes of user in mind; the reader, as consumer of published material; and the journal editor, as curator. To this end, the software was piloted with Portland Press Limited (PPL) with the goal of rendering the content of  BJ  electronic publications and supplemental data richer and more accessible. To achieve this, an ‘editor's version’ of Utopia Documents, with customized plugins, was integrated with PPL's editorial and document-management workflows, allowing  BJ  editors to mark up article content prior to publication. In terms of functionality, the editor's version of the software behaves much the same as the reader's, with the additional feature that relationships between concepts in a document and online definitions/records can be made permanent in order to be shared with readers ( Fig. 3 g). The role of the editors was therefore to explore each pre-publication PDF, annotating terms and figures with definitions and interactive content and then validating them with a ‘stamp of approval’ (i.e. the  BJ  icon).
 Fig. 3. Utopia Documents' user interface showing: ( a ) a selected term in the article; ( b ) manual term lookup; ( c ) resulting definitions of that term retrieved from Wikipedia (via DBPedia) and the PDB; ( d ) metadata relating to the whole document (shown when no specific term definition is selected); ( e ) live links to articles in the article's bibliography; ( f ) an icon indicating the ‘authority’ for a particular annotation (here, the  BJ ) and ( g ) the panel used by  BJ  editorial staff to associate terms with annotations (note that this is only available in the ‘editor's version’ of Utopia Documents). With the customized software in-house, article annotation was fairly swift, individual papers taking 10–30 min, depending on their suitability for mark-up. The launch issue of the Semantic  BJ  (December 2009;  http://www.biochemj.org/bj/424/3/ ) was primarily handled by two editors; since then, the whole editorial team has been involved in the successful mark-up of eight further issues. Entities relating to protein sequences and structures have been, of necessity, the main targets for mark-up, because this was the functionality built into the original Utopia toolkit. The kinds of additional mark-up provided by the software include links from the text to external web sites, term definitions from ontologies and controlled vocabularies, embedded data and materials (images, videos, etc.) and links to interactive tools for sequence alignment and 3D molecular visualization. To allow readers to benefit from these semantic enhancements, a reader's version of the software was made freely available ( http://getutopia.com ). The tool installs easily on the desktop as an alternative PDF viewer. Once opened, it displays a window consisting of three regions ( Fig. 3 ): the main reading pane displays the article itself and supports the pagination, searching, zooming and scrolling features typical of PDF readers. Below this, thumbnail images give an overview of the document and allow rapid navigation through it. The sidebar on the right displays the contents of annotations, providing term definitions and access to auxiliary data as the article is explored. When no specific terms are selected, the sidebar defaults to displaying document-wide metadata [including the title, authors, keywords, abbreviations, etc. (3d)], in addition to the cited references (3e)—these are linked, where available, via open-access publishing agreements or institutional or individual subscriptions, to the online versions of the original articles. Where the PDF version is not available to the reader, clicking on the reference currently launches a Google Scholar search instead. To avoid cluttering the text with ‘highlighter pen’-type marks, the presence of annotations, or availability of auxiliary data, is indicated by discreet colored glyphs in the margin. Similar marks are added to the corner of the corresponding thumbnail in the pager, to indicate that additional information exists somewhere on that page. Mousing-over a glyph highlights the nearby terms, or document regions, that contain annotations; selecting these areas causes the associated data to be displayed—this may involve populating the sidebar with definitions, or may activate an embedded interactive visualization. Highlighting any word or phrase in the paper (3a) initiates a context-sensitive search of the online resources to which Utopia Documents is connected, all results again appearing in the sidebar. At the bottom of the sidebar (3b), a ‘lookup’ feature allows searches for terms not explicitly mentioned in the paper. 4.1 Annotations An annotated term or region in a document may be associated with definitions and/or database records from a variety of sources. Selecting a term invokes the display of all possible definitions, allowing the reader (or editor) to select for themselves the most appropriate version. The provenance of these definitions is indicated in their headers, as illustrated in  Figure 3 : the icon on the left (3c) represents the item's origin [e.g., UniprotKB, Wikipedia, KEGG (Kanehisa  et al. ,  2010 )], while the presence of an icon on the right-hand side of the header (3f) indicates the person, group or organization who made, and endorsed, the association between a term and this specific definition (here, publisher-validated annotations carry the  BJ  logo). 4.2 Interactive content The current version of Utopia Documents supports three forms of embedded interactive content; as with term definitions, these are indicated by red glyphs in the margins. Selecting these causes a ‘media player’-like panel to appear, which the reader can use to control the behavior of the interactive content. Activating the triangular ‘play’ button replaces the static content,  in situ , with its interactive version; the neighboring ‘pop-up’ button opens a new window leaving the static page unchanged. Each type of interactive content has its own functionality: 3D molecules ( Fig. 4 ), for example, can be rotated, zoomed and rendered in a variety of styles (e.g. space-fill, backbone or cartoon); sequences and their associated features can be inspected individually, or edited as multiple alignments; and tables of data can be manipulated or converted automatically into scatter-plots or histograms.  Figure 4  illustrates the simple transformation from static images of tables and figures into semantically annotated, interactive objects.
 Fig. 4. Image sequences showing the transformation of a 2D image (left-hand panel) and of a static table of figures (right-hand panel) into interactive objects: i.e. a manipulable 3D model (coordinates extracted from the PDB) and a set of ‘live’ figures and a customizable semantic graph. Utopia Documents provides new ways of reading, of interacting with and ultimately of assimilating the knowledge embodied within research articles. The approach taken here departs from many initiatives in scholarly publishing in that the focus for enrichment is the hitherto-largely-neglected static PDF file, rather than HTML- or XML-based files. The subject of ‘static PDF’ versus ‘dynamic online’ articles has been hotly contested in the literature, the general consensus being that PDF is semantically limited by comparison with other online formats and is thus antithetical to the spirit of web publishing (Lynch,  2007 ; Renear and Palmer,  2009 ; Shotton  et al. ,  2009 ; Wilbanks,  2007 ). We argue that PDFs are merely a mechanism for rendering words and figures, and are thus no more or less ‘semantic’ than the HTML used to generate web pages. Utopia Documents is hence an attempt to provide a semantic bridge that connects the benefits of both the static and the dynamic online incarnations of published texts. Inevitably, those who prefer to read articles online in a web browser will view the need to download a new, desktop-based PDF reader as a weakness. Our view is, rather, that Utopia Documents complements browser-based tools, providing a novel mechanism for unleashing knowledge that is otherwise locked in personal, publisher and/or institutional PDF-file archives. In contrast with approaches for creating dynamic (as opposed to ‘semantic’) life-science PDF articles (Kumar  et al. ,  2008 ; Ruthensteiner and Hess,  2008 ) that use Adobe Acrobat's support for ‘Universal 3D Data’ (U3D), Utopia Documents does not insert its augmented content into the PDF file itself, but instead blends additional visual material into the display process at the final stages of rendering. This mechanism presents a number of benefits over the generic U3D approach: (i) the underlying PDF file remains small and compact, and does not become bloated by the large polygonal meshes necessary for rendering 3D molecules; (ii) rather than the ‘one size fits all’ U3D approach, Utopia Documents is able to select appropriate rendering and interaction algorithms for different types of artifact; (iii) Utopia Documents is able to maintain a semantic relationship between the underlying scholarly article and the object being rendered; and importantly; (iv) the original PDF, as an ‘object of record’, remains unadulterated and its integrity can be verified by examining it with a conventional PDF viewer. The philosophy embodied in Utopia Documents is to hide as much of the underlying complexity as possible, to avoid requiring users (whether editors, authors or readers) to change their existing document-reading behaviors, and to present no significant extra hurdles to publication. Like the initiatives in semantic publishing outlined earlier, the Semantic  BJ , powered by Utopia Documents, is a pilot, the success of which will depend on various factors, including whether the barriers to adoption are sufficiently low, and whether the approach is considered to add sufficient value. Although it is too early to assess the impact of the pilot on readers of the Semantic  BJ , the take-up of the software by the  BJ 's full editorial team, and it use to mark up every issue since the launch, is a testament to the software's ease-of-use. Of course, as the project with PPL develops, we will gather relevant usage and usability data in order to provide a more meaningful evaluation. Many of the projects discussed in this article have exploited fairly traditional text-mining methods, in conjunction with controlled vocabularies and ontologies, to facilitate the launch of relevant external web pages from marked-up entities in documents. As such, they come with all the limitations in precision of current text-mining tools; this brings a significant overhead to readers in terms of having to identify errors. Of course, the difficulty for non-experts in any given field is to be able to recognize when particular annotations really are errors, and failure to identify them as such leads to the danger of error propagation. In light of these issues, we took a slightly different approach to entity mark-up in this first incarnation of Utopia Documents, taking advantage of linked-data initiatives to facilitate mark-up and add value to published texts. However, because the functionality of the system is easily customizable via its flexible plugin architecture, any text-mining tool or database that is accessible via web services can be trivially added to the suite. As a demonstration of the potential of this architecture, in collaboration with their developers, three prototype plugins that link Utopia to other systems have been implemented: Reflect: as mentioned earlier, the Reflect system is primarily used as a means of augmenting HTML content online, either by accessing a web page via the project's portal, or by installing a browser plugin ( http://reflect.ws/ ). Its entity-recognition engine, however, may also be accessed programmatically via a web service, which, given a section of text, identifies objects of biological interest and returns links to the summary pop-ups. Integration of Reflect's functionality with Utopia Documents is therefore a comparatively straightforward task: as a user reads a PDF document, its textual content is extracted and sent to the Reflect web service; the resulting entities are then highlighted in the PDF article, and linked to the appropriate pop-up, which is displayed when a highlighted term is selected. A particular advantage of this integration is that it provides the reader with a light-weight mechanism for verifying or cross-checking results returned from multiple sources (e.g. Reflect, Bio2RDF, DBpedia/Wikipedia). GPCRDB: this is a specialist database describing sequences, ligand-binding constants and mutations relating to G protein-coupled receptors ( http://www.gpcr.org/ ). Its recently developed web-service interface provides programmatic access to much of its content, enabling Utopia Documents to identify and highlight receptors and their associated mutants when encountered in PDFs. Thus, the presence of a GPCR in an article triggers the creation of a link to a description of that receptor in the database, which is displayed in the sidebar. The article is then scanned for mutants, which in turn are linked to the relevant mutant records in GPCRDB. Having identified an appropriate receptor, the software then automatically translates between the sequence co-ordinates, allowing ‘equivalent’ residues to be readily mapped between them. ACKnowledge Enhancer and the Concept Wiki: the Concept Wiki is a repository of community-editable concepts, currently relating to people and proteins, stored as RDF triples and fronted by a wiki-like interface ( http://www.conceptwiki.org ). Its associated ACKnowledge Enhancer is an analysis tool that links HTML content to relevant objects in the Concept Wiki and other online sources, exposing these to the user as selectable HTML highlights that, when activated, generate dynamic pop-ups. As with the Reflect plugin, integration with these systems via their web services provides a straightforward way of migrating functionality previously only available for HTML content to scientific PDF articles. Videos showing these plugins in use are available at  http://getutopia.com . Utopia Documents is at an early stage of development and there is more work to be done. In the future, as well as opening its APIs to other developers, we plan to extend its scope to systems and chemical biology, and to the medical and health sciences, as many of the requisite chemical, systems biology, biomedical, disease and anatomy ontologies are already in place and accessible via the OBO Foundry (Smith  et al. ,  2007 ). Furthermore, the growing impetus of ‘institutional repositories’ as vehicles for collecting and sharing scholarly publications and data, and an increase in the acceptance of open access publishing, together present many interesting possibilities that we are keen to explore. Another planned extension is to allow readers to append annotations and notes/comments to articles. There are various scenarios to consider here: (i) a reader might wish to make a ‘note to self’ in the margin, for future reference; (ii) a reviewer might wish to make several marginal notes, possibly to be shared with other reviewers and journal editorial staff; (iii) a reader might wish to append notes to be shared with all subsequent readers of the article (e.g., because the paper describes an exciting breakthrough or because it contains an error)—these scenarios involve different security issues, and hence we will need to investigate how to establish appropriate ‘webs of trust’. Ultimately, allowing users to append their own annotations (in addition to those endorsed by publishers) should help to involve authors in the manuscript mark-up process. Utopia Documents brings us a step closer to integrated scholarly literature and research data. The software is poised to make contributions in a number of areas: for publishers, it offers a mechanism for adding value to oft-neglected PDF archives; for scientists whose routine work involves having to attach meaning to raw data from high-throughput biology experiments (database curators, bench biologists, researchers in pharmaceutical companies, etc.), it provides seamless links between facts published in articles, information deposited in databases and the requisite interactive tools to analyze and verify them; for readers in general, it provides both an enhanced reading experience and exciting new opportunities for knowledge discovery and ‘community peer review’. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>The EBI RDF platform: linked open data for the life sciences</Title>
    <Doi>10.1093/bioinformatics/btt765</Doi>
    <Authors>Jupp Simon, Malone James, Bolleman Jerven, Brandizi Marco, Davies Mark, Garcia Leyla, Gaulton Anna, Gehant Sebastien, Laibe Camille, Redaschi Nicole, Wimalaratne Sarala M., Martin Maria, Le Novère Nicolas, Parkinson Helen, Birney Ewan, Jenkinson Andrew M.</Authors>
    <Abstract>Motivation: Resource description framework (RDF) is an emerging technology for describing, publishing and linking life science data. As a major provider of bioinformatics data and services, the European Bioinformatics Institute (EBI) is committed to making data readily accessible to the community in ways that meet existing demand. The EBI RDF platform has been developed to meet an increasing demand to coordinate RDF activities across the institute and provides a new entry point to querying and exploring integrated resources available at the EBI.</Abstract>
    <Body>1 INTRODUCTION The European Bioinformatics Institute (EBI) is the largest bioinformatics resource provider in Europe. Our databases are accessible via dedicated interfaces, web services, data download and (in a few cases) direct database access. Modern research in the life sciences necessitates an understanding of data at many different levels: multi-omics, from cells to biological systems, across many different species and studying many different experimental conditions. The biology underpinning these research questions is intrinsically connected, yet data are often collected and stored in technology or domain-specific repositories. Efforts in the Semantic Web community are already beginning to invest in technology that enables data to be readily integrated ( Belleau  et al. , 2008 ;  Katayama  et al. , 2010 ;  Marshall  et al. , 2008 ). One method used among the Semantic Web community is using the W3C’s resource description framework (RDF) model to represent data. RDF provides a common mechanism for describing data and querying data using SPARQL. To better serve complex research questions across resources, and to meet an increased demand on the EBI to produce RDF, we have developed an RDF platform. The aim of such a platform is to offer users the ability to ask questions using multiple connected resources that share common identifiers and have a common format (RDF) and query interface (SPARQL). This platform complements other existing data access modes such as our Web site and RESTful web services, but additionally contains explicit links between the different data resources. This enables a single query to be asked across multiple distributed datasets and across a range of biological domains. This approach has been applied for the following EBI resources: Gene Expression Atlas ( Kapushesky  et al. , 2012 ), ChEMBL ( Gaulton  et al. , 2011 ), BioModels ( Li  et al. , 2010 ), Reactome ( Matthews  et al. , 2008 ), BioSamples ( Gostev  et al. , 2012 ) and also includes a collaboration with the UniProt Consortium to deliver UniProt RDF ( Redaschi and UniProt Consortium, 2009 ). 2 METHODS The RDF platform presents a coordinated effort to bring together RDF resources from multiple services and databases at the EBI. The development of the platform began by collecting requirements from both a scientific and a technical perspective. The scientific requirements were gathered as a series of use cases and competency questions collected from research scientists and users of EBI services. In particular, we were looking for questions that required data to be integrated from multiple resources and that are not trivial to answer with our existing infrastructure due to the disparate nature of the data. These questions were used to identify points of integration between resources. The scientific use cases informed the technical requirements on what infrastructure, in terms of both software and hardware, would be needed to deliver a stable and scalable platform. Given RDF technology is still maturing, there are open questions on how to deliver such a platform on this scale; our existing infrastructure is delivered after evaluation of various technologies that will be the subject of another paper. Data from UniProt, ChEMBL, Reactome and BioModels represents curated knowledge from protein sequence and function, bio-active molecules and their targets, to biochemical pathways and computational models of molecular interactions. The Gene Expression Atlas database provides differential gene expression data from a variety of samples that are highly annotated and curated using the Experimental Factor Ontology (EFO) ( Malone  et al. , 2010 ). Generating linked RDF for these resources provides a new entry point for exploring the data, such as putting gene expression in the context of protein function, pathways and drug targets. An outline of how resources are connected is shown in  Figure 1 .
 Fig. 1. Connections between services (boxes) and ontologies (circles). The graph illustrates how the data are linked within the RDF platform, enabling queries to span all data. Asterisk: ENSEMBL to UniProt (gray line) mappings are included via expression atlas The graph-based nature of the RDF data model provides a natural fit for explicitly publishing how data are connected. In RDF, resources are identified using uniform resource identifiers (URIs), which provide a web-based global identification system. Guidelines for minting new URIs for EBI resources were established using the new rdf.ebi.ac.uk domain (details can be found at  http://www.ebi.ac.uk/rdf/documentation/uris-ebi-data ). Canonical URIs are used when existing databases, such as UniProt, already provide stable URIs. In cases where no canonical URIs are provided by external resources, the Identifiers.org registry of scientific identifiers ( Juty  et al. , 2012 ) was used to provide a referencing URI. As part of the URI strategy, every effort has been made to ensure all EBI RDF datasets only use URIs that can be dereferenced using  http,  supporting content negotiation for human-orientated HTML views, alongside machine processable versions in various RDF syntaxes. Using common URI schemes assists data integration with RDF. In addition, ontologies provide a mechanism to semantically describe the data, and the OWL ontology language can be serialized in RDF. The EBI makes extensive use of ontologies to annotate data, however, the richness of these annotations is rarely available in native RDF for exploitation by external applications. The EBI RDF platform adopts a range of common vocabularies and ontologies to annotate data. The ontologies used span common biomedical terminologies such as the Gene Ontology, Chemical Entities of Biological Interest, UBERON, Cell Type Ontology, Biological Pathways Exchange, EFO and more. Additionally, we adopted metadata standards for describing datasets and provenance such as Dublin Core, Data Catalog Vocabulary and Vocabulary of Interlinked Datasets. 3 RESULTS Complete dumps of the RDF data are available via FTP downloads. These are published in line with existing production and release cycles, ensuring the most up-to-date data are readily available. We are also using triple store technology to index the RDF files and make them available for querying and exploration via SPARQL endpoints and our linked data browser. The underlying infrastructure at the EBI is built on open source triple store technology provided by OpenLink, ( http://www.openlinksw.com/ ), whereas the UniProt data are served by the SIB’s Vital-IT HPC platform using technology from OntoText ( http://www.ontotext.com/ ). We developed LODEStar ( http://www.ebi.ac.uk/fgpt/sw/lodestar/ ) as a generic SPARQL endpoint and linked data browser to provide a consistent interface and some enhanced functionality for querying and browsing EBI-based datasets. In addition to providing access to the underlying data, an equally important component of the platform is the Web site at  http://www.ebi.ac.uk/rdf  that provides an entry point to discover all RDF resources being served by the EBI. This site includes documentation on how to find the datasets and provides examples of how to query the data using the SPARQL endpoints ( http://www.ebi.ac.uk/rdf/example-sparql-queries ). We also provide examples showing developers how they can use the SPARQL API programmatically from common programming environments like Perl, Java and R.  4 CONCLUSION The EBI RDF platform allows explicit links to be made between datasets using shared semantics from standard ontologies and vocabularies, facilitating a greater degree of data integration. SPARQL provides a standard query language for querying RDF data. Data that have been annotated using ontologies, such as EFO and the Gene Ontology, enable data integration with other community datasets and provides the semantics to perform rich queries. Publishing these datasets as RDF along with their ontologies provides both the syntactic and semantic integration of data long promised by semantic web technologies. As the trend toward publishing life science data in RDF increases, we anticipate a rise in the number of applications consuming such data. This is evident in efforts such as the Open PHACTS platform ( http://www.openphacts.org ) and the AtlasRDF-R package ( https://github.com/jamesmalone/AtlasRDF-R ). Our aim is that the EBI RDF platform enables such applications to be built by releasing production quality services with semantically described RDF to enable pertinent biomedical use cases to be addressed. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Fast and accurate long-read alignment with Burrows–Wheeler transform</Title>
    <Doi>10.1093/bioinformatics/btp698</Doi>
    <Authors>Li Heng, Durbin Richard</Authors>
    <Abstract>Motivation: Many programs for aligning short sequencing reads to a reference genome have been developed in the last 2 years. Most of them are very efficient for short reads but inefficient or not applicable for reads &gt;200 bp because the algorithms are heavily and specifically tuned for short queries with low sequencing error rate. However, some sequencing platforms already produce longer reads and others are expected to become available soon. For longer reads, hashing-based software such as BLAT and SSAHA2 remain the only choices. Nonetheless, these methods are substantially slower than short-read aligners in terms of aligned bases per unit time.</Abstract>
    <Body>1 INTRODUCTION Following the development of sensitive local alignment software, such as FASTA (Pearson and Lipman,  1988 ) and BLAST (Altschul  et al. ,  1997 ) around 1990, a new generation of faster methods to find DNA sequence matches was developed since 2000, including MegaBLAST (Morgulis  et al. ,  2008 ; Zhang  et al. ,  2000 ), SSAHA2 (Ning  et al. ,  2001 ), BLAT (Kent,  2002 ) and PatternHunter (Ma  et al. ,  2002 ), greatly speeding up matching capillary sequencing reads against a large reference genome. When new sequencing technologies arrived that generated millions of short (&lt;100 bp) reads, a variety of new algorithms were developed which were 10–1000 times faster, including SOAP (Li,R.  et al. ,  2008 ), MAQ (Li,H.  et al. ,  2008 ), Bowtie (Langmead  et al. ,  2009 ) and BWA (Li and Durbin,  2009 ). However, Roche/454 sequencing technology has already produced reads &gt;400 bp in production, Illumina gradually increases read length &gt;100 bp, and Pacific Bioscience generates 1000 bp reads in early testing (Eid  et al. ,  2009 ). Reads coming from the new sequencing technologies are not short any more, which effectively rules out many of the new aligners exclusively designed for reads no longer than 100 bp. Efficiently aligning long reads against a long reference sequence like the human genome poses a new challenge to the development of alignment tools. Long-read alignment has different objectives from short-read alignment. First, in short-read alignment, we would usually like to align the full-length read to reduce the reference bias caused by the mismatches toward the ends of the read. Given this requirement, we can design spaced seed templates (Ma  et al. ,  2002 ) spanning the entire read (Jiang and Wong,  2008 ; Lin  et al. ,  2008 ; Smith  et al. ,  2008 ), or quickly filter out poor matches, for example, by applying q-gram filtration (Rumble  et al. ,  2009 ; Weese  et al. ,  2009 ) or by bounding the search process (Li and Durbin,  2009 ), and thus accelerate the alignment. In long-read alignment, however, we would prefer to find local matches because a long read is more fragile to structural variations and misassemblies in the reference but is less affected by the mismatches close to the ends of a read. Secondly, many short-read aligners are only efficient when doing ungapped alignment or allowing limited gaps, e.g. a maximum of one gap. They cannot find more gaps or the performance quickly degrades when they are tuned for this task. Long-read aligners, however, must be permissive about alignment gaps because indels occur more frequently in long reads and may be the dominant source of sequencing errors for some technologies such as 454 and Pacific Bioscience. When considering algorithms to speed-up long-read alignment, hash table indexing as is used in most current software is not the only choice. Meek  et al.  ( 2003 ) found a Smith–Waterman-like dynamic programming that can be applied between a query sequence and the suffix tree of the reference, effectively aligning the query against each subsequence sampled from the suffix tree via a top-down traversal. As on a suffix tree identical sequences are collapsed on a single path, time is saved by avoiding repeated alignment of identical subsequences. Lam  et al.  ( 2008 ) furthered this idea by implicitly representing the suffix tree with an FM-index (Ferragina and Manzini,  2000 ), which is based on the Burrows–Wheeler Transform (BWT; Burrows and Wheeler,  1994 ), to achieve a small memory footprint. Their new algorithm, BWT-SW, is able to deliver identical results to the standard Smith–Waterman alignment, but thousands of times faster when aligning against the human genome sequence. While BWT-SW is still slower than BLAST on long query sequences, it finds all matches without heuristics. One can imagine that introducing heuristics would further accelerate BWT-SW. Our BWA-SW algorithm follows this route. To some extent, BWA-SW, as well as BWT-SW, also follows the seed-and-extend paradigm. But different from BLAT and SSAHA2, BWA-SW finds seeds by dynamic programming between two FM-indices and allows mismatches and gaps in the seeds. It extends a seed when the seed has few occurrences in the reference sequence. Speed is gained by reducing unnecessary extension for highly repetitive sequences. In this article, we will describe this new alignment algorithm, BWA-SW, for long-read alignments and evaluate its practical performance along with BLAT and SSAHA2 on both simulated and real data. We will also give a brief introduction to suffix array and FM-index, but readers are referred to Li and Durbin ( 2009 ) for more details. 2 METHODS 2.1 Overview of the BWA-SW algorithm BWA-SW builds FM-indices for both the reference and query sequence. It implicitly represents the reference sequence in a prefix trie and represents the query sequence in a prefix directed acyclic word graph (prefix DAWG; Blumer  et al. ,  1985 ), which is transformed from the prefix trie of the query sequence ( Section 2.3 ). A dynamic programming can be applied between the trie and the DAWG, by traversing the reference prefix trie and the query DAWG, respectively. This dynamic programming would find all local matches if no heuristics were applied, but would be no faster than BWT-SW. In BWA-SW, we apply two heuristic rules to greatly accelerate this process. First, traversal on the query DAWG is carried in the outer loop, and therefore without finishing the dynamic programming, we know all the nodes in the reference prefix trie that match the query node with a positive score. Based on the observation that the true alignment tends to have a high alignment score, we can prune low-scoring matches at each node to restrict the dynamic programming around good matches only. The scale of dynamic programming can thus be dramatically reduced. It is possible for the true alignment to be pruned in this process, but in practice, this can be controlled by the use of heuristics and happens rarely, given long or high-quality query sequences. Secondly, BWA-SW only reports alignments largely non-overlapping on the query sequence instead of giving all the significant local alignments. It heuristically identifies and discards seeds contained in a longer alignment and thus saves computing time on unsuccessful seed extensions. 2.2 Notations and definitions 2.2.1 Suffix array and BWT Let Σ={ A ,  C ,  G ,  T } be the alphabet of nucleotides and $ be a symbol that is lexicographically smaller than all the symbols in Σ. Given a nucleotide sequence  X = a 1 … a n −1  with  a n −1 =$, let  X [ i ]= a i  be the  i -th symbol,  X [ i ,  j ]= a i … a j  a subsequence of  X  and  X i = X [ i ,  n −1] a suffix of  X . The suffix array  S  of  X  is a permutation of integers 0,…,  n −1 such that  S ( i )= j  if and only if  X j  is the  i -th lexicographically smallest suffix. The BWT of  X  is a permutation of  X , where  B [ i ]=$ if  S ( i )=0 and  B [ i ]= X [ S ( i )−1] otherwise. 2.2.2 Suffix array interval Given a sequence  W , the  suffix array interval  or  SA interval    of  W  is defined as
 
In particular, if  W  is an empty string,  R ( W )=1 and  . The set of the positions of all the occurrences of  W  is  . Let  C ( a )=#{0≤ j ≤ n −2: X [ j ]&lt; a } and  O ( a , i )=#{0≤ j ≤ i : B [ j ]&lt; a }, where #{·} calculates the cardinality (or size) of a set. Ferragina and Manzini ( 2000 ) proved that
 
and that   if and only if  aW  is a substring of  X . 2.2.3 FM-index The suffix array  S , array  C  and  O  suffice for the exact search of a pattern in  X . FM-index (Ferragina and Manzini,  2000 ) is a compressed representation of the three arrays, consisting of the compressed BWT string  B , auxiliary arrays for calculating  O , and part of the suffix array  S . BWA-SW, however, uses a simplified FM-index where we do not compress  B  and store part of the occurrence array  O  without auxiliary data structures. The simplified version is more efficient for DNA sequences with a very small alphabet. Details on the construction are presented in our previous paper (Li and Durbin,  2009 ). 2.2.4 Alignment An  alignment  is a tuple ( W 1 ,  W 2 ,  A ) where  W 1  and  W 2  are two strings and  A  is a series of copying, substitution, insertion and deletion operations which transform  W 2  into  W 1 . Insertions and deletions are  gaps . Gaps and substitutions are  differences . The  edit distance  of the alignment equals the total number of differences in  A . A score can be calculated for an alignment given a scoring system. We say  W 1  matches  W 2  if  W 1  and  W 2  can be aligned with a positive score, and in this case, we also say ( W 1 ,  W 2 ) is a match. A match ( W 1 ,  W 2 ) is said to be  contained  in ( W ′ 1 ,  W ′ 2 ) on the first sequence if  W 1  is a substring of  W ′ 1 . Similarly, we can define the ‘contained’ relationship between alignments (a stronger condition) and between an alignment and a match. 2.3 Prefix trie and prefix DAWG The  prefix trie  of string  X  is a tree with each edge labeled with a symbol such that the concatenation of symbols on the path from a leaf to the root gives a unique prefix of  X . The concatenation of edge symbols from a node to the root is always a substring of  X , called the string represented by the node. The  SA interval  of a node is defined as the SA interval of the string represented by the node. Different nodes may have an identical interval, but recalling the definition of SA interval, we know that the strings represented by these nodes must be the prefixes of the same string and have different lengths. The  prefix DAWG , of  X  is transformed from the prefix trie by collapsing nodes having an identical interval. Thus in the prefix DAWG, nodes and SA intervals have an one-to-one relationship, and a node may represent multiple substrings of  X , falling in a sequence where each is a prefix of the next as is discussed in the previous paragraph.  Figure 1  gives an example.
 Fig. 1. Prefix trie and prefix DAWG of string ‘ GOOGOL ’. ( A ) Prefix trie. Symbol ‘∧’ marks the start of a string. The two numbers in a node gives the SA interval of the node. ( B ) Prefix DAWG constructed by collapsing nodes with the identical SA interval. For example, in the prefix trie, three nodes has SA interval [4, 4]. Their parents have interval [1, 2], [1, 2] and [1, 1], respectively. In the prefix DAWG, the [4, 4] node thus has parents [1, 2] and [1, 1]. Node [4, 4] represents three strings ‘ OG ’, ‘ OGO ’ and ‘ OGOL ’ with the first two strings being the prefix of ‘ OGOL ’. (A) is modified from  Figure 1  in Li and Durbin ( 2009 ). 2.4 Aligning prefix trie against prefix DAWG We construct a prefix DAWG 𝒢( W ) for the query sequence  W  and a prefix trie 𝒯( X ) for the reference  X . The dynamic programming for calculating the best score between  W  and  X  is as follows. Let  G uv = I uv = D uv =0 when  u  is the root of 𝒢( W ) and  v  the root of 𝒯( X ). At a node  u  in 𝒢( W ), for each of its parent node  u ′, calculate
 
where  v ′ is the parent of  v  in 𝒯( X ), function  S ( u ′,  u ;  v ′,  v ) gives the score between the symbol on the edge ( u ′,  u ) and the one on ( v ′,  v ), and  q  and  r  are gap open and gap extension penalties, respectively.  G uv ,  I uv  and  D uv  are calculated with:
 
 
where pre( u ) is the set of parent nodes of  u .  G uv  equals the best score between the (possibly multiple) substrings represented by  u  and the (one) substring represented by  v . We say a node  v matches u  if  G uv &gt;0. The dynamic programming is performed by traversing both 𝒢( W ) and 𝒯( X ) in the reverse post-order (i.e. all parent nodes are visited before children) in a nested way. Noting that once  u  does not match  v ,  u  does not match any nodes descending from  v , we only need to visit the nodes close to the root of 𝒯( X ) without traversing the entire trie, which greatly reduces the number of iterations in comparison to the standard Smith–Waterman algorithm that always goes through the entire reference sequence. 2.5 Acceleration by the standard Smith–Waterman In comparison to the standard Smith–Waterman alignment whose time complexity is  O (| X |·| W |), BWA-SW has better time complexity since it is no slower than BWT-SW whose time complexity  O (| X | 0.628 | W |) (Lam  et al. ,  2008 ). This conclusion comes because for short sub-alignments we are considering multiple possible matches with a single  uv  comparison. However, the constant associated with each iteration is much larger due to the complex steps related to the traversal of prefix trie and prefix DAWG, which makes BWA-SW inefficient when we use BWA-SW to extend a unique alignment. A more efficient strategy would be to use BWA-SW to find partial matches and apply the Smith–Waterman algorithm to extend. In dynamic programming, we know the number of partial matches being considered at any pair because this can be calculated from the size of the SA interval. When  G uv  is good enough and the SA interval size of  v  is below a certain threshold (3 by default), we save the ( u , v ) pair, called a  seed interval pair , and do not go deeper from the  v  node in 𝒯( X ). By looking up the suffix array of  X  and  W , we can derive  seed matches , or simply  seeds , from seed interval pairs. These seeds are then extended by the Smith–Waterman algorithm later. If the entire query is a highly repetitive sequence, it will be aligned purely with the algorithm described in the last section without the Smith–Waterman extension. Because we are stopping the dynamic programming early to generate seeds, the global best alignment may contain multiple seeds and in practice this will tend to be the case for long alignments. Typically for 1 kb alignments there will be 10–20 seeds. Below we will take advantage of this observation to heuristically speed up the search. BWT-SW deploys a similar strategy in performing the dynamic programming between a sequence and a prefix trie to find seed matches followed by Smith–Waterman extension. The main difference from our algorithm is that BWT-SW initiates the Smith–Waterman alignment once the score is high enough, regardless of the SA interval size. Sometimes a repetitive sequence may match to thousands of places in the human genome and extending partial matches each time may be slow. 2.6 Heuristic accelerations 2.6.1 Z-best strategy The algorithm described so far is exact in that it is able to deliver the same results as the Smith–Waterman algorithm. Although it is much faster than the standard algorithm given a long reference sequence, it is not fast enough for aligning large-scale sequencing data. Closer investigation reveals that even for a unique 500 bp query sequence, a few million nodes in 𝒯( X ) may match the query with a positive alignment score. The majority of these nodes are random matches or matches in short low-complexity regions. Visiting all of them is wasteful. To accelerate alignment, we traverse 𝒢( W ) in the outer loop and 𝒯( X ) in the inner loop, and at each node  u  in 𝒢( W ) we only keep the top  Z  best scoring nodes in 𝒯( X ) that match  u , rather than keep all the matching nodes. This heuristic strategy is called  Z-best . Of course, when we apply the  Z -best strategy, we could miss a seed contained in the true alignment when a false match has a higher score. But if the query is nearly identical to the reference, this happens less often. In addition, if the true alignment is long and contains many seeds, the chance of all seeds being false is very small. On both simulated and real data ( Section 3 ), we find even  Z =1 works well with high-quality 200 bp reads (&lt;5% sequencing error rate). Increasing  Z  to 10 or higher marginally improves the accuracy but greatly reduces the alignment speed. To reduce alignment errors, we also align the reverse query sequence to the reverse reference sequence, namely reverse–reverse alignment, in addition to the forward–forward alignment. Ideally, the forward–forward and the reverse–reverse alignments should yield identical outcomes, but if a seed in the true alignment has a low-scoring suffix (or prefix), the forward–forward (or reverse–reverse) alignment is likely to miss it, while combining the two rounds of alignment reduces the chance. Moreover, if the best alignment from the forward–forward alignment contains many seed matches, the chance of it being false is also small. In implementation, we do not apply the reverse–reverse alignment if the best alignment contains, by default, 5 or more seeds. 2.6.2 Filtering seeds before the Smith–Waterman extension Like BLAST, both BLAT and SSAHA2 report all significant alignments or typically tens of top-scoring alignments, but this is not the most desired output in read mapping. We are typically more interested in the best alignment or best few alignments, covering each region of the query sequence. For example, suppose a 1000 bp query sequence consists of a 900 bp segment from one chromosome and a 100 bp segment from another chromosome; 400 bp out of the 900 bp segment is a highly repetitive sequence. For BLAST, to know this is a chimeric read we would need to ask it to report all the alignments of the 400 bp repeat, which is costly and wasteful because in general we are not interested in alignments of short repetitive sequences contained in a longer unique sequence. On this example, a useful output would be to report one alignment each for the 900 bp and the 100 bp segment, and to indicate if the two segments have good suboptimal alignments that may render the best alignment unreliable. Such output simplifies downstream analyses and saves time on reconstructing the detailed alignments of the repetitive sequence. In BWA-SW, we say two alignments are  distinct  if the length of the overlapping region on the query is less than half of the length of the shorter query segment. We aim to find a set of distinct alignments which maximizes the sum of scores of each alignment in the set. This problem can be solved by dynamic programming, but as in our case a read is usually aligned entirely, a greedy approximation would work well. In the practical implementation, we sort the local alignments based on their alignment scores, scan the sorted list from the best one and keep an alignment if it is distinct from all the kept alignments with larger scores; if alignment  a 2  is rejected because it is not distinctive from  a 1 , we regard  a 2  to be a suboptimal alignment to  a 1  and use this information to approximate the mapping quality ( Section 2.7 ). Because we only retain alignments largely non-overlapping on the query sequence, we might as well discard seeds that do not contribute to the final alignments. Detecting such seeds can be done with another heuristic before the Smith–Waterman extension and time spent on unnecessary extension can thus be saved. To identify these seeds, we chain seeds that are contained in a band (default band width 50 bp). If on the query sequence a short chain is fully contained in a long chain and the number of seeds in the short chain is below one-tenth of the number of seeds in the long chain, we discard all the seeds in the short chain, based on the observation that the short chain can rarely lead to a better alignment than the long chain in this case. Unlike the  Z -best strategy, this heuristic does not have a noticeable effect on alignment accuracy. On 1000 10 kb simulated data, it halves the running time with no reduction in accuracy. 2.7 Approximating mapping quality Li,H.  et al.  ( 2008 ) introduced the concept of mapping quality to estimate the probability of a query sequence being placed at a wrong position. If an alignment algorithm guarantees to find all local alignments, mapping quality is determined by these local alignments only. However, as BWA-SW deploys heuristic rules, the chance of producing a wrong alignment is also related to the heuristics. To estimate the mapping quality of a BWA-SW alignment, we fit an empirical formula: 250· c 1 · c 2 ·( S 1 − S 2 )/ S 1 , where  S 1  is the score of the best alignment,  S 2  the score of the second best alignment,  c 1  equals 1 if the alignment covers more than four seeds or 0.5 otherwise, and  c 2  equals to 1 if the best alignment is found by both forward–forward and reverse–reverse alignments or 0.2 otherwise. 3 RESULTS 3.1 Implementation The BWA-SW algorithm is implemented as a component of the BWA program (Li and Durbin,  2009 ), which is distributed under the GNU general public license (GPL). The implementation takes a BWA index and a query FASTA or FASTQ file as input and outputs the alignment in the SAM format (Li  et al. ,  2009 ). The query file typically contain many sequences (reads). We process each query sequence in turn, using multiple threads if applicable. Memory usage is dominated by the FM-index, about 3.7 GB for the human genome. Memory required for each query is roughly proportional to the sequence length. On typical sequencing reads, the total memory is &lt;4 GB; on one query sequence with 1 million base pairs (Mbp), the peak memory is 6.4 GB in total. In the implementation, we try to automatically adjust parameters based on the read lengths and sequencing error rates to make the default settings work well for inputs of different characteristics. This behavior is convenient to users who are not familiar with the algorithm and helps performance given the reads of mixed lengths and error rates. 3.2 Evaluation on simulated data On simulated data, we know the correct chromosomal coordinates from the alignment and the evaluation is straightforward. 3.2.1 Overall performance Table 1  shows the CPU time, fraction of confidently aligned reads and alignment error rates for BLAT (v34), BWA-SW (version 0.5.3) and SSAHA2 (version 2.4) given different read lengths and error rates. Unless necessary, we tried to use the default command-line options of each aligner. Fine tuning the options based on the characteristics of the input data may yield better performance.
 Table 1. Evaluation on simulated data Program Metrics 100 bp 200 bp 500 bp 1000 bp 10 000 bp 2% 5% 10% 2% 5% 10% 2% 5% 10% 2% 5% 10% 2% 5% 10% BLAT CPU sec 685 577 559 819 538 486 1078 699 512 1315 862 599 2628 1742 710 Q20% 68.7 25.5 3.0 92.0 52.9 7.8 97.1 86.3 21.4 97.7 96.4 39.0 98.4 99.0 94.0 errAln% 0.99 2.48 5.47 0.55 1.72 4.55 0.17 1.12 4.41 0.01 0.52 3.98 0.00 0.00 1.28 BWA-SW CPU sec 165 125 84 222 168 118 249 172 152 234 168 150 158 134 120 Q20% 85.1 62.2 19.8 93.8 88.7 49.7 96.1 95.5 85.1 96.9 96.5 95.0 98.4 98.5 98.1 errAln% 0.01 0.05 0.17 0.00 0.02 0.13 0.00 0.00 0.04 0.00 0.00 0.00 0.00 0.00 0.00 SSAHA2 CPU sec 4872 7962 9345 1932 2236 5252 3311 8213 6863 1554 1583 3113 – – – Q20% 85.5 83.8 78.2 93.4 93.1 91.9 96.6 96.5 96.1 97.7 97.6 97.4 – – – errAln% 0.00 0.01 0.19 0.01 0.00 0.01 0.00 0.01 0.04 0.00 0.00 0.00 – – – Approximately 10 000 000 bp data of different read lengths and error rates are simulated from the human genome. Twenty percent of errors are indel errors with the indel length drawn from a geometric distribution (density: 0.7·0.3 l −1 ). These simulated reads are aligned back to the human genome with BLAT (option -fastMap), BWA-SW and SSAHA2 (option −454 for 100 and 200 bp reads), respectively. The aligned coordinates are then compared with the simulated coordinates to find alignment errors. In each cell in this table, the three numbers are the CPU seconds on a single-core of an Intel E5420 2.5 GHz CPU, percent alignments with mapping quality greater than or equal to 20 (Q20), and percent wrong alignments out of Q20 alignments. SSAHA2 and BWA-SW report mapping quality; BLAT mapping quality is estimated as 250 times the difference of the best and second best alignment scores divided by the best alignment score (essentially the same calculation as the one for BWA-SW). 
 From  Table 1 , we can see that BWA-SW is clearly the fastest, several times faster than BLAT and SSAHA2 on all inputs, and its speed is not sensitive to the read length or error rates. The accuracy of BWA-SW is comparable with SSAHA2 when the query is long or has low error rate. Given short and error-prone reads, SSAHA2 is more accurate, although it has to spend more time on aligning such reads. SSAHA2 is not tested on the 10 kb reads because it is not designed for this task initially and thus does not perform well. BLAT with the -fastMap option is faster than SSAHA2, but less accurate. Under the default option, BLAT is several to tens of times slower than SSAHA2. The accuracy is higher in comparison to the -fastMap mode, but still lower than that of BWA-SW in general (data not shown). On memory, both BWA-SW and BLAT uses ∼4 GB memory. SSAHA2 uses 2.4 GB for ≥500 bp reads with the default option, and 5.3 GB for shorter reads with the −454 option which increases the number of seed sequences stored in the hash table and increases the memory as a result. In addition, BWA-SW supports multi-threading and thus may take less memory per CPU core if it is run on a multi-core computer. SSAHA2 and BLAT do not support multi-threading at present. 3.2.2 Chimera detection We first study the behavior of each aligner given a chimeric read. To do so, we fabricated two chimeric reads with both consisting of one 1000 bp piece from one chromosomal position and one 300 bp piece from another position. The main difference between the two reads is that the 1000 bp piece in the second read has a ∼750 bp repetitive sequence, while the first read is highly unique. When we align the two chimeric reads to the human genome, BWA-SW reports four alignments, one for each piece, as is desired. The latest SSAHA2 fails to find the alignment of the 300 bp pieces in both reads, although it is able to find the alignments if we align the 300 bp piece as an individual read. An older version (1.0.9) is able to align the 300 bp piece in the first read by default, but for the second read, we need to switch to a more thorough but much slower configuration that reports all the hits to the 750 bp repeat. BLAT with -fastMap does not find the alignment of the 300 bp piece for the second read. On the two examples, only BWA-SW has sufficient power to detect chimera. Furthermore, BWA-SW rarely produces high-quality false chimeric alignments. For example, given the 10 000 1 kb reads with 10% errors but without chimera in simulation, BWA-SW predicts 18 chimeric reads. The mapping quality of the wrongly aligned pieces on these reads is only 2.4 (maximum 11), implying that BWA-SW is aware that these chimera are unreliable. As is expected, BWA-SW produces fewer false chimeric reads given lower base errors. 3.3 Evaluation on real data Evaluation on real data is complicated by the lack of a ground truth. However, it is still possible to evaluate the relative accuracy by comparing the results from two aligners using the principle that the true alignment tends to have a considerably higher alignment score, because most errors arise from failing to find a seed. Suppose we align a read using two aligners  A  and  B  and get different results. If both  A  and  B  give low mapping qualities, the alignment is ambiguous and it does not matter if either alignment is wrong. If  A  gives high mapping quality and the  A  alignment score is worse than  B ,  A  alignment is probably wrong; even if  A  alignment score is just a little better than  B ,  A  alignment is not reliable and the high mapping quality given by  A  is still questionable. In practice, defining ‘a little better’ alignment score requires to set a arbitrary threshold on the score difference and therefore this evaluation method is approximate. Table 2  gives a summary of 454 reads which are mapped by only one aligner or mapped to different places, and are assigned a mapping quality greater or equal to 20 by either BWA-SW or SSAHA2. We can see that BWA-SW tends to miss short alignments with high error rates (946 of them), which agrees with the evaluation on simulated data. SSAHA2 misses alignments for a different reason. On 1188 reads, SSAHA2 produces obviously wrong alignments. It is aware that these alignments are wrong by assigning low mapping quality, but the true alignments are missed anyway.
 Table 2. Summary of alignments inconsistent between the BWA-SW and SSAHA2 on real data Condition Count BWA-SW SSAHA2 avgLen avgDiff avgMapQ avgLen avgDiff avgMapQ BWA-SW≥20; SSAHA2 unmapped 0 – – – – – – BWA-SW≥20 plausible; SSAHA2&lt;20 1188 398.2 1.3% 178.4 198.3 13.1% 3.9 BWA-SW≥20 questionable 40 183.0 7.8% 41.2 280.3 9.4% 2.4 SSAHA2≥20; BWA-SW unmapped 946 – – – 75.4 6.3% 51.2 SSAHA2≥20 plausible; BWA-SW&lt;20 47 129.0 9.3% 2.5 200.5 8.8% 34.4 SSAHA2≥20 questionable 185 400.2 1.7% 13.4 399.2 2.9% 216.4 A total of 137 670 454 reads uniformly selected from  SRR003161  were mapped against the human genome with BWA-SW and SSAHA2, respectively. A read is said to be aligned inconsistently if the leftmost coordinates of the BWA-SW and SSAHA2 alignment differs by over 355 bp, the average read length. A score, which equals to the number of matches minus three multiplied by the number of differences (mismatches and gaps) in the aligned region, is calculated for each alignment. A BWA-SW alignment is said to be  plausible  if the score derived from the BWA-SW alignment minus the one derived from the SSAHA2 alignment of the same read is greater than or equal to 20 (i.e. the BWA-SW alignment is sufficiently better); otherwise the BWA-SW alignment is said to be  questionable . Plausible and questionable SSAHA2 alignments are defined in a similar manner. In the table, ‘BWA-SW≥20’ denotes the BWA-SW alignments with mapping quality higher than 20. In all, BWA-SW misses 993 (=946 + 47) alignments which SSAHA2 aligns well, while SSAHA2 misses 1188; 40 BWA-SW Q20 alignments and 185 SSAHA2 Q20 alignments are possibly wrong. 
 For both aligners, most wrong alignments are caused by overlooking alignments with a similar score to the best reported alignment. For example, SSAHA2 aligns read  SRR003161.1261578  to X chromosome with mapping quality 244 and BWA-SW aligns it to chromosome 2 with identical alignment length and edit distance. The existence of two best scoring alignments means the read cannot be uniquely placed and a mapping quality as high as 244 is inaccurate. SSAHA2 gives this high mapping quality probably because it overlooks the match on chromosome 2. And in this specific example, BWA-SW properly gives a mapping quality zero, although it may overlook alternative matches in other examples. On simulated 100 and 200 bp reads, SSAHA2 with the −454 option delivers better alignments than BWA-SW. On this real dataset, BWA-SW is more accurate possibly because the average read length is relatively long (355 bp). To confirm this speculation, we compared the two aligners on 99 958 reads from run  SRR002644  with average read length 206 bp. This time BWA-SW misses 1092 SSAHA2 Q20 alignments and produces 39 questionable alignments; SSAHA2 misses 325 and produces 10 questionable ones. SSAHA2 is more accurate on this shorter dataset, although it is nine times slower than BWA-SW and uses 40% more memory. 4 DISCUSSION BWA-SW is an efficient algorithm for aligning a query sequence of a few hundred base pairs or more against a long reference genome. Its sensitivity and specificity tend to be higher given a long query or a query with low error rate, and on such query sequences, the accuracy of BWA-SW is comparable with the most accurate aligner so far. Furthermore, BWA-SW is able to detect chimera, potentially caused by structural variations or reference misassemblies, which may pose a challenge to BLAT and SSAHA2. BWA-SW, BLAT and SSAHA2 all follow the seed-and-extend paradigm. The major difference comes from the seeding strategy. BLAT and SSAHA2 identify short exact matches as seeds, typically of length 11 or 12 bp. For  k -mer seeding between two sequences of length  L  and  l , respectively, the expected number of seeds is  L · l /4 k , or of the order of 10 5  for alignment against the human genome. Extending these seeds each with the Smith–Waterman algorithm is expensive. To reduce unnecessary seed extension, both BLAT and SSAHA2 use non-overlapping seeds by default and require multiple seed matches, which should work well for random sequences, but still involves many seed extensions in highly repetitive regions. BWA-SW resolves this issue by using a few long gapped seeds in unique regions. On real biological data, it saves many unnecessary seed extensions and leads to a better overall performance. However, to reduce time when identifying long seeds, BWA-SW only maintains a very small fraction of the dynamic programming matrix, which may miss all seeds for true matches. This heuristic is the major source of alignment errors especially for short queries when there are only few valid unique seeds between the sequences to be aligned. On long alignments, fortunately, the chance of missing all seeds is small. We have shown BWA-SW works equally well as SSAHA2. BWA-SW differs from BWT-SW in several aspects. First of all, BWT-SW guarantees to find all local matches, whereas BWA-SW is a heuristic algorithm which may miss true hits but is much faster. Secondly, BWA-SW aligns two FM-indices while BWT-SW aligns one sequence and a FM-index. Building a prefix DAWG for the query sequences potentially helps to avoid repeatedly aligning identical substrings in the query, and thus improves the theoretical time complexity. Thirdly, BWA-SW traverses the reference prefix trie in the inner loop while BWT-SW loops through the query sequence in the inner loop. Without heuristics, the BWA-SW approach would hurt performance because we have to trade speed for memory in traversing the reference prefix trie, and it would be more efficient to traverse it in the outer loop. Nonetheless, applying the  Z -best strategy requires to know the top-scoring reference nodes matching a query substring without finishing the dynamic programming and thus only works when the reference is traversed in the inner loop. Fourthly, BWA-SW only reports alignments largely non-overlapping on the query sequence, while BWT-SW, like BLAST, reports all statistically significant alignments. BWA-SW retains key information of alignments and generates much smaller and more convenient output. For BWT-SW, end users usually need to post-process the results to filter out many alignments of little interest to them. In all, BWA-SW is tuned toward practical usefulness given large-scale real data. The high speed of BWA-SW largely comes from two strategies: the use of FM-indices and the suppression of short repetitive matches contained in a better match. While the first strategy is not applicable to hash table-based algorithms such as SSAHA2 and BLAT, the second strategy could be implemented in such programs and may substantially accelerate them by saving much time on the construction of repetitive alignments. And although the use of BWT reduces unnecessary alignments in repeats, each BWT operation comes with a large constant in comparison with a hash table look up. It is still possible that hash table-based algorithms could be faster than BWA-SW if they incorporated some of these features. 
          Supplementary Material 
          
             
                [Supplementary Data] 
             
             
             
          
       </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Presenting and sharing clinical data using the eTRIKS Standards Master Tree for tranSMART</Title>
    <Doi>10.1093/bioinformatics/bty809</Doi>
    <Authors>Barbosa-Silva Adriano, Bratfalean Dorina, Gu Wei, Satagopam Venkata, Houston Paul, Becnel Lauren B, Eifes Serge, Richard Fabien, Tielmann Andreas, Herzinger Sascha, Rege Kavita, Balling Rudi, Peeters Paul, Schneider Reinhard, Valencia Alfonso</Authors>
    <Abstract/>
    <Body>1 Introduction The European Translational Information and Knowledge Management Services (eTRIKS,  https://www.etriks.org/, 2017 ) is tasked with providing tools and services to support data management and analysis for &gt;60 diverse biomedical research projects which have been funded by the Innovative Medicines Initiative (IMI). As Europe’s largest public-private partnership, IMI funds projects ranging from molecular and systems biology to clinical trials and full translational research projects. The community translational research system under use is tranSMART ( Athey  et al. , 2013 ;  Dunn  et al. , 2017 ), first developed by the pharma industry, and then gifted to a global translational research development community. The tranSMART system has undergone extensive development extended by its own community and the eTRIKS project, which has focused on an implementation that serves IMI projects users based in the European Union (EU). The flexibility and capability of tranSMART is well presented in a recent paper showing the availability of workflows within a sandbox environment ( Satagopam  et al. , 2016 ). tranSMART serves as the central knowledge management system for eTRIKS, while other tools and complimentary services applicable to the data value chain, such as data harmonization, sharing, analysis, visualization and preservation, have been developed. To expedite medical breakthroughs the sharing of clinical research data is vital owing to legislative incentives and increased public pressure, many clinical trial registries are expanding their remit to share not only basic summary trial registration data but also results. Wider data sharing is one way of tackling reporting bias by increasing visibility of successful studies as well as failed ones. Additionally, data standards play a pivotal role in tackling the omnipresent problem of reproducibility. Begley  et al.  reproduced 53 experiments from landmark publications to find 47 out of 53 could not be replicated; a very worrying trend for preclinical studies that are used as the scientific basis for target identification for new drug development ( Begley and Ellis, 2012 ). The Data FAIRport initiative in 2014 prescribed a set of guiding principles known as FAIR: Findable, Accessible, Interoperable, Reusable which should be applied where data is deemed scientifically valuable ( Wilkinson  et al. , 2016 ). Those principles have gained official recognition from G20, NIH and the Directorate General for Research and Innovation of the European Commission. The consistent application of common semantics and data structures, as outlined within data standards, is a key factor to ensure interoperability and reusability of data. The eTRIKS Data Standards Work Package created a Standards Starter Pack ( https://doi.org/10.5281/zenodo.50398/ , 2016), which outlines the FAIR principles and recommendations for the main clinical and genomic standards as well as supporting vocabularies and minimum information guidelines that should be applied in the entire translational research landscape. eTRIKS has also produced the IMI Data catalogue which centralizes metadata of ongoing and past IMI projects. It is part of the service that eTRIKS provides in its key knowledge management performance with a focus on the findability of project level study description metadata. Furthermore, this well received initiative facilitates broader sharing and accessibility of data ( http://datacatalog.elixir-luxembourg.org/ckan/, 2017 ). For clinical research data, The Clinical Data Interchange Standards Consortium (CDISC,  https://www.cdisc.org/, 2018 ) data standards have been implemented in over 90 countries, and are now mandated by Food and Drug Administration of the United States ( FDA, 2014 ) and Pharmaceuticals and Medical Devices Agenda (PMDA) in Japan ( https://www.pmda.go.jp/files/000206449.pdf, 2018 ) in order to increase the uptake of data standards, which, when applied, contribute to higher data quality. The lack of implementing standards will render datasets from different cohorts inadequate when integrating with complementary research data for meta-analyses ( Elefsinioti  et al. , 2016 ). A recent paper by the American College of Medical Genetics and Genomics ( Acmg, 2017 ) discussed the importance of using the information from one patient cohort to benefit other patients. The ACMG’s framework for data sharing will work best if standards are implemented within the framework, as within tranSMART, and datasets are gathered by utilizing those standards from the beginning of the research, as is also recommended by CDISC. 2 Implementation The eTRIKS Standard Master Tree is based on the standards for clinical data representation developed by CDISC, mainly the Study Data Tabulation Model (SDTM) standard. The proposal of eTRIKS was to create a hierarchical navigation tree in which the raw data, collected at the multiple cohorts, should be promptly mapped to the elements of this tree so that data are loaded automatically with the correct topology into tranSMART i2b2 (Informatics for Integrating Biology and the Bedside) framework. The requirement for this is that all the data collected from a patient will be organized and formatted using the SDTM model. SDTM modeling increases the ability to compare information among systems and/or organizations, whilst also decreasing the time to initiate a new research study. The use of these data standards improves the data quality, their interoperability and their management, which allows easier, faster and more reliable data aggregation. The eTRIKS Standard Master Tree presents the clinical data within tranSMART i2b2. eTRIKS has radically updated the original tranSMART engine that sorts and presents the clinical data within the system. Users can choose to map their clinical data content to a favorite terminology prior to the SDTM modelling using global standards such as OMICS, NCI ( https://www.cancer.gov/digital-standards, 2017 ) or LOINC ( https://loinc.org/, 2017 ), as long as the SDTM variable names as maintained. Further, the clinical data is mapped to a ‘clinical mapping file’, which requires a good working knowledge of the CDISC foundational standards, in order to represent the SDTM structure of the clinical data correctly in the hierarchy of the tranSMART i2b2 repository ( Abend  et al. , 2009 ). In practice if one thinks about the outcome of a ‘glucose test’, this test may be named ‘sugar test’ or ‘glucose test’ in different differing cohorts, which may be well understood by experts but not a machine as the same concept. The use of standard name ‘Glucose Tolerance Test’ (NCBI’s, MeSH Unique ID: D005951) would avoid any confusion or wrong interpretation and enable data query across cohorts. Further to this, considering that the metabolite ‘glucose’ could be measured in different samples (e.g. blood, urine), the test results could be reported in different units (mg/dl or mmol/l) and/or the test could be performed at different periods of the time (screening, visit 1, visit 2, etc.), error prone aspects during the data analysis. If the problem is proposed, ‘How to standardize the manner by which this information should be organized and formatted for effective and precise cohorts comparisons?’ One answer should be: ‘Use a Standard Master Ontology Tree’ or in this case, the eTRIKS Standard Master Tree. The application of this tool coupled with a good application of controlled vocabularies will increase greatly the Reusability and Interoperability Principles mentioned above. In the ‘Glucose Tolerance Test’ example, upon mapping to the tranSMART Standard Master Tree, the outcome of this test would already be represented as displayed in  Figure 1A  below. The test result is reported in this example by means of 14 variables (columns A-N) for the subject CDISC01.100008 (column C). Note that column G collects one variable called LBTEST (Lab Test Examination Name), which is filled with the standard value ‘Glucose’ and another variable LBSPEC (Specimen Type) is used to distinguish ‘BLOOD’ from ‘URINE’ samples. In terms of readout values, the variable LBORRES (Result or Finding in Original Units) records the original values as collected reported units in LBORRESU (Original Units) the unit itself (e.g. mg/dl). The example shows results converted to numeric type and this reported value to a standard unit, which is achieved by using the pair variables LBSTRESN (Numeric Result/Finding in Standard Units) and LBSTRESU (Standard Units), for values and units (e.g. mmol/l), respectively. The SDTM Implementation Guide provides a comprehensive description including four sessions: 1—Overview of topics for specific general observation class associated with specific domains, 2—Specification for table of variables, 3—Rules for correct implementation of standards and 4—Examples.
 Fig. 1. Content representation of the eTRIKS Master Tree Package. ( A ) SDTM data file for one patient (USUBJID) for the LB domain. ( B ) SMOT_Lite definition session for the LB domain. ( C ) Hierarchical (i2b2) tree created for the LB domain and displayed in the tranSMART web app To avoid the all too common pitfalls of redundant data eTRIKS developed the Standard Master Tree, using the comprehensive SDTM domain structure, to support and give structure and context to the data so it can be easily identified. The Standard Master Tree follows a basic and easy-to-understand logic, which was built upon the premise of tranSMART rules for data loading. This means that multiple data collected for one patient for the same domain (e.g. Laboratory Test Results—LB) should be distinguished based on  Data Labels . This way, for the LB domain, results of ‘Glucose’ and ‘Creatinine’ tests for example, could be loaded in the same run. Moreover, multiple results for the same test should also be distinguished based of the  Visit Names . Respecting these two basic rules, any results from any sort of laboratory tests and even results for any other domains, can be easily represented in tranSMART via the Standard Master Tree. 3 Features The eTRIKS Standard Master Tree model consists of a package of three main components: (i) a CDISC clinical dataset reported as define.xml metadata and converted in . txt  tabulate files composing of 16 SDTM domains represented as review data as collected for 4 fictitious subjects ( Fig. 1A ); (ii) the tranSMART standard master ontology tree as TM SMOT-SDTM_lite.txt definition file, where all the information concerned about the correct positioning of the SDTM variables can be found ( Fig. 1B ); and the (iii) Mapper script where users can map their data files to the TM SMOT-SDTM_Lite definition, avoiding manual work. The script reads a target directory containing the input SDTM files and maps all the collected variables against the SMOT-SDMT_Lite.txt master tree file mentioned above. This is achieved with a single command line: ‘php mapper.php SMOT_Lite.txt CDISC01_ClinicalData’ (further details are explained on the package’s README file).  Figure 1A  depicts an example for the Glucose test of one such subject.  Figure 1B  depicts part of the TM SMOT-SDTM_Lite file where definitions for the domain LB is displayed (Note the seven columns required for the annotation of each of the SDTM variables used in this domain). This information can be found easily on the SDTM implementation guide as should be adopted by the data curators. Finally,  Figure 1C  displays the graphical hierarchy tree, known as tranSMART i2b2 tree, where the loaded data can be further queried and used to create comparison subsets on the tranSMART i2b2 web app, these can be visualized in a sandbox implementation available at  http://public.etriks.org/transmart/datasetExplorer  under the eTRIKS—Master Tree branch. The strategy for clinical research data standards representation proposed above offers a readily available method to integrate multiple translational research datasets while meeting the Interoperability and Reusability aspects of the FAIR principles. Once the data is within the eTRIKS Standard Master Tree, it can then take advantage of the tranSMART environment, where it will receive a unique study and server specific identifier and the metadata can be given greater and essential specificity. With an effective tranSMART search tool where multiple datasets and/or studies can be pooled and queried, coupled with an entry within the eTRIKS data catalogue the data has undergone FAIR-ification to a satisfactory degree. Now the data is Findable and also Accessible, and it can begin its hopefully long life adding scientific value to any number of future studies or aggregated data comparisons. 4 Conclusion The tranSMART Standard Master Tree presented here adds to other efforts to make other software data interoperable with tranSMART. Projects such as ‘ODM to i2b2’ converts data stored in XML/ODM based systems such as OpenClinica and REDCap into i2b2 format ( https://github.com/CTMM-TraIT/trait_odm_to_i2b2 , 2018); and ‘REDCap2SDTM’ converts electronic data capture system data to SDTM ( Yamamoto  et al. , 2017 ). Taken together, this software could benefit from the Master Tree concept in order to standardize the manner that SDTM studies should appear within a tranSMART navigation tree to users. If the tools and processes above are adopted in the scope of the NIH funded projects, it will contribute greatly to creating an overseas bridge for data sharing initiatives with the EU/EFPIA-funded (IMI) translational medicine research projects, of which over 60 are being supported by the eTRIKS project. While not all of the eTRIKS supported projects have implemented the tranSMART Standards Master Tree they have all received the appropriate guidance and advice from eTRIKS experts or as laid out in the eTRIKS standards starter pack. Tremendous curation efforts were necessary to guarantee that IMI data was collected in good quality, once that, frequently, the big challenge for translational research projects lies on the quality of the data itself, not only its metadata. The adoption of the technologies and standards developed and presented in this paper will support a significant step towards a position where IMI data can be shared, and the findings reproduced to benefit the health care research community, allowing a standardized representation of SDTM data across multiple tranSMART servers. The eTRIKS Standard Master Tree package can be downloaded at  https://doi.org/10.5281/zenodo.1009098 . </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Drug–target interaction prediction through domain-tuned network-based inference</Title>
    <Doi>10.1093/bioinformatics/btt307</Doi>
    <Authors>Alaimo Salvatore, Pulvirenti Alfredo, Giugno Rosalba, Ferro Alfredo</Authors>
    <Abstract>Motivation: The identification of drug–target interaction (DTI) represents a costly and time-consuming step in drug discovery and design. Computational methods capable of predicting reliable DTI play an important role in the field. Recently, recommendation methods relying on network-based inference (NBI) have been proposed. However, such approaches implement naive topology-based inference and do not take into account important features within the drug–target domain.</Abstract>
    <Body>1 INTRODUCTION Detecting and verifying new connections among drugs and targets is a costly process. From a historical point of view, the pharmaceutical chemist’s approach has been commonly focused on the development of compounds acting against particular families of ‘druggable’ proteins ( Yildirim  et al. , 2007 ). Drugs act by binding to specific proteins, hence changing their biochemical and/or biophysical activities, with many consequences on various functions. Furthermore, because proteins operate as part of highly interconnected cellular networks (i.e. the interactome networks), the ‘one gene, one drug, one disease’ paradigm has been challenged in many cases ( Hopkins, 2008 ). For this reason, the concept of polypharmacology has been raised for those drugs acting on multiple targets rather than a single one ( Hopkins, 2008 ). These polypharmacological features of drugs bring a wealth of knowledge and enable us to understand drug side effects or find their new uses, namely, drug repositioning ( Ashburn and Thor, 2004 ;  Boguski  et al. , 2009 ). Nevertheless, many interactions are still unknown, and given the significant amount of resources needed for  in situ  experimentation, it is necessary to develop algorithmic methodologies allowing the prediction of new and significant relationships among elements interacting at the process level. In the literature, several computational tools have been proposed to afford the problem of DTI prediction and drug repositioning. Traditional methods rely either on ligand-based or receptor-based approaches. Among ligand-based methods, we can cite quantitative structure-activity relationships, and a similarity search-based approach ( Gonzalez-Daz  et al. , 2011 ;  Keiser  et al. , 2007 ). On the other hand, receptor-based methods, such as reverse docking, have also been applied in drug–target (DT) binding affinity prediction, DTI prediction and drug repositioning ( Ashburn and Thor, 2004 ;  Li  et al. , 2006 ;  Xie  et al. , 2011 ). However, the latter have the shortcoming that cannot be used for targets whose 3D structures are unknown. Recently, much attention has been devoted to network-based and phenotype-based approaches. Most of these methods rely on the successful idea of using bipartite graphs. In  Yildirim  et al.  (2007) , a bipartite graph linking US Food and Drug Administration-approved drugs to proteins by DT binary associations is exploited.  Campillos  et al.  (2008)  identified new DTIs using side effect similarity. Iorio  et al.  (2010)  make use of transcriptional responses, predicted and validated new drug modes of action and drug repositioning. Recently,  Dudley  et al.  (2011)  and  Sirota  et al.  (2011)  have presented drug repositioning methods exploiting public gene expression data. Furthermore,  Yamanishi  et al.  (2008)  developed a bipartite graph learning method to predict DTI by integrating chemical and genomic data. Cheng  et al.  (2012)  present a technique based on network-based inference (NBI) implementing a naive version of the algorithm proposed by  Zhou  et al.  (2007) . All these results clearly show the good performance of this approach. On the other hand, knowledge about drug and protein domain is not properly exploited. van Laarhoven  et al.  (2011 ) use a machine learning method starting from a DTI network to predict new ones with high accuracy. The calculation of the new interactions is done through the regularized least squares algorithm. The regularized least squares algorithm is trained using a kernel (GIP—Gaussian interaction profile) that summarizes the information in the network. The authors developed variants of the original kernel by taking into account chemical and genomic information. This improved the accuracy, in particular for small datasets. Chen  et al.  (2012)  introduced their Network-based Random Walk with Restart on the Heterogeneous network (NRWRH) algorithm predicting new interactions between drugs and targets by means of a model based on a random walk with a restart in a ‘heterogeneous’ network. The model is constructed by extending the network of DTI interactions with drug–drug and protein–protein similarity networks. This methodology shows excellent performance in predicting new interactions. However, its disadvantage is due to its random nature, mainly caused by the initial probabilities selection. Mei  et al.  (2013)  proposed the Bipartite Local Model-Interaction-profile Inferring (BLM-NII) algorithm. Interactions between drugs and targets are deduced by training a classifier (i.e. support vector machine or regularized least square). This is achieved by exploiting interaction information, drug and target similarities. This classifier is appropriately extended to include knowledge on new drug/target candidates. This is used to predict the new target probability of a specific drug. The algorithm is highly reliable in predicting interactions between new drug/target candidates. On the other hand, its capability of training several distinct classifiers to obtain the final model is not strong enough. In this present article, we propose a novel method called domain tuned-hybrid (DT-Hybrid). It extends the NBI algorithm proposed in  Zhou  et al.  (2007)  and applied in  Cheng  et al.  (2012)  by adding application domain knowledge. Similarity among drugs and targets is plugged into the model. Despite its simplicity, the technique provides a complete and functional framework for  in silico  prediction of drug and target relationships. To demonstrate the reliability of the method, we conducted a wide experimental analysis using four benchmark datasets drawn from DrugBank. We compared our method with the one proposed by  Chen  et al. , 2012 . The experiments clearly show that DT-Hybrid overcomes the problems shown by the naive NBI algorithm, and it is capable of producing higher quality predictions. 2 METHODS 2.1 Algorithm The method we propose is based on the recommendation technique presented by  Zhou  et al.  (2007)  and extended by  Zhou  et al.  (2010) . Let   be a set of small molecules (i.e. biological compounds, molecules), and   a set of targets (i.e. genes, proteins); the X-T network of interactions can be described as a bipartite graph   where  . A link between  x i  and  t j  is drawn in the graph when the structure  x i  is associated with the target  t j . The network can be represented by an adjacency matrix  , where   if  x i  is connected to  t j ; otherwise,  . Zhou  et al.  (2010)  proposed a recommendation method based on the bipartite network projection technique implementing the concept of resources transfer within the network. Given the bipartite graph defined above, a two-phase resource transfer is associated with one of its projections: at the beginning, the resource is transferred from nodes belonging to  T  to those in  X , and subsequently the resource is transferred back to the  T  nodes. This process allows us to define a technique for the calculation of the weight matrix ( ) in the projection as follows:
 (1) 
where Γ determines how the distribution of resources takes place in the second phase, and   is the degree of the  x  node in the bipartite network. By varying the Γ function, we obtain the following algorithms ( Table 1 ):
 NBI, introduced by  Zhou  et al.  (2007)  and used by  Cheng  et al.  (2012)  for the prediction of the interactions between drugs and proteins; HeatS, introduced by  Zhou  et al.  (2010) ; Hybrid N+H, introduced by  Zhou  et al.  (2010) , in which the functions defined in NBI and HeatS are combined in connection with a parameter called λ; DT-Hybrid, introduced here, is an enhanced version of the Hybrid algorithm in which previous domain-dependent biological knowledge is plugged into the model through a similarity matrix. 
 Table 1. List of algorithms with the associated Γ functions Algorithm Γ Function (1) NBI ( Zhou  et al. , 2007 ) (2) HeatS ( Zhou  et al. , 2010 ) (3) Hybrid N+H ( Zhou  et al. , 2010 ) (4) DT-Hybrid Given the weight matrix  W  and the adjacency matrix  A  of the bipartite network, it is possible to compute the recommendation matrix   by the product:
 (2) 
 For each  x i  in  X , its recommendation list is given by the set  , where  r ji  is the ‘score’ of recommending  t j  to  x i . This list is then sorted in a descending order with respect to the score because the higher elements are expected to have a better interaction with the corresponding structure. Notice that the method described above does not make use of any previous biological knowledge of the application domain. Here we propose the DT-Hybrid algorithm, which extends the recommendation model by introducing: (i) similarity between small molecules (i.e. molecular compounds), and (ii) sequence similarity between targets. Let   be the target similarity matrix [i.e. either BLAST bits scores ( Altschul  et al. , 1990 ) or Smith-Waterman local alignment scores ( Smith and Waterman, 1981 )]. This information can be taken into account by using  equation (1)  with   defined as in row 4 of  Table 1 . Including structural similarity requires more effort. Therefore, it is necessary to manipulate such information to obtain a variant of the  S  matrix, and simplify the computation of the  equation (1) . Let   be the structure similarity matrix [i.e. SIMCOMP similarity score ( Hattori  et al. , 2003 ) in the case of compounds]. It is possible to obtain a matrix   (where each element   describes similarity between  t i  and  t j  based on the common interactions in the network weighted by compound similarity) by putting:
 (3) 
This matrix can be linearly combined with the target similarity matrix  S ,
 (4) 
where α is a tuning parameter. This additional biological knowledge yields faster computation and higher numerical precision. The matrix defined by  equation (4)  in connection with  equations (1)  and  (2)  allows the prediction of recommendation lists. 2.2 Datasets and benchmarks We evaluated our method using four datasets ( Cheng  et al. , 2012 ) containing experimentally verified interactions between drugs and genes. We analyzed the performances of NBI [ equation (1)  using Γ(i,j) in  Table 1 , row 1], Hybrid [ equation (1)  using Γ(i,j) in  Table 1 , row 3] and DT-Hybrid [ equation (1)  using Γ(i,j) in  Table 1 , row 4]. The datasets were built by grouping all possible interactions between genes and drugs (DTI) based on their main gene types: enzymes, ion channels, G-protein coupled receptors (GPCRs) and nuclear receptors ( Table 2 ). The following similarity measures have been used: (i) SIMCOMP 2D chemical similarity of drugs ( Hattori  et al. , 2003 ), and (ii) Smith-Waterman sequence similarity of genes ( Smith and Waterman, 1981 ).
 Table 2. Description of the dataset: number of biological structures, targets and interactions together with a measure of sparsity Dataset Structures Targets Interactions Sparsity Enzymes 445 664 2926 0.0099 Ion channels 210 204 1476 0.0344 GPCRs 223 95 635 0.0299 Nuclear receptors 54 26 90 0.0641 Complete DrugBank 4398 3784 12 446 0.0007 Note:  The sparsity is obtained as the ratio between the number of known interactions and the number of all possible interactions. Similarities have been normalized according to  Yamanishi  et al.  (2008) :
 (5) 
 Results are evaluated by combining the methods presented by  Zhou  et al.  (2010)  and  Cheng  et al.  (2012) . More precisely, we applied a 10-fold cross-validation and repeated the experiments 30 times. Notice that, the random partition used in the cross-validation could cause isolation of nodes in the network on which the test is performed. Because all the tested algorithms are capable of predicting new interactions only for drugs and targets for which we already have some information, we computed the partition so that for each node, at least one link to the other nodes remains in the test set. According to  Zhou  et al.  (2010) , the following four metrics were considered: precision and recall enhancement, recovery, personalization and surprisal. Precision and Recall Enhancement ,  
 and 
 . Quality is measured in terms of the top  L  elements in the recommendation list of each biological structure. Let  D i  be the number of deleted interactions recovered for drug  i , and let   be its position in the top  L  places of  i ’s recommendation list. The average precision and recall for the prediction process can be computed as follows:
 (6) 
 (7) 
where   is the number of structures with at least one deleted link. A better perspective can be obtained by considering these values within random models   and  . If the structure  i  has a total of  D i  deleted interactions, then   [given that  ]. Consequently, averaging for all structures we obtain  , where  D  is the number of links in the test set. On the other hand, the average number of links deleted in the first  L  positions is given by  . Again by averaging for all structures,  . Given these random models, it is possible to compute the precision and recall enhancement as follows:
 (8) 
 (9) 
 Finally, as opposed to the recommendation on social systems, the three other metrics—recovery, personalization and surprisal—are not so significant in drug–target systems. For this reason, we report the details of such metrics (their definitions together with the experimental results), just for completeness, in the  Supplementary Materials . 3 RESULTS In this article, we propose a method called DT-Hybrid, which extends NBI ( Cheng  et al. , 2012 ;  Zhou  et al. , 2007 ) and the Hybrid ( Zhou  et al. , 2010 ) algorithms by integrating previous domain-dependent knowledge. Experiments show that this extension improves both algorithms in terms of prediction of new biologically significant interactions. In the supporting materials, we report a comprehensive analysis of DT-Hybrid and Hybrid, together with their behavior varying the α (only for DT-Hybrid) and λ parameters.  Table 3  illustrates the result of comparing NBI, Hybrid and DT-Hybrid in terms of precision and recall enhancement. DT-Hybrid clearly outperforms both NBI and Hybrid in recovering deleted links. It is important to point out that hybrid algorithms are able to significantly improve recall ( e R ) measuring the prediction ability of recovering existing interactions in a complex network.  Figure 1  illustrates the receiver operating characteristic (ROC) curves calculated over the complete DrugBank dataset. Simulations were executed 30 times, and the results were averaged to obtain a performance evaluation. Experiments show that all three techniques have a high true-positive rate against a low false-positive rate. However, hybrid algorithms provided better performance than NBI. In particular,  Table 3  clearly shows an increase of the average areas under the ROC curves (AUC) in the complete dataset (a detailed analysis can be found in the supporting materials section). This indicates that hybrid algorithms improve the ability of discriminating known links from predicted ones. The increase of the AUC values for the DT-Hybrid algorithm demonstrates that adding biological information to prediction is a key choice to achieve significant results.  Table 4  demonstrates that exploiting biological information leads, in most cases, to a significant increase of the adjusted precision and recall.  Figure 2  illustrates the ROC curves calculated on the enzymes, ion channels, GPCRs, and nuclear receptor datasets using the top-30 predictions. Finally, it can be asserted that adding similarity makes prediction more reliable than an algorithm, such as NBI, which applies only network topology to score computation. Indeed, using only known interactions of a new structure without any target information makes it impossible to predict new targets for this drug. This weakness is a problem for all methods based on recommendation techniques. The introduction of new biological structures is equivalent to the addition of isolated nodes in the network, whose weight, based on the  equation (1) , is always zero. Such a weight, ultimately, leads to the impossibility of obtaining a prediction for this new molecule.
 Fig. 1. Comparison between DT-Hybrid, Hybrid, and NBI by means of receiver operating characteristic (ROC) curves, computed for the top- L  places of the recommendation lists, which were built on the complete DrugBank dataset 
 Fig. 2. Comparison between DT-Hybrid, Hybrid and NBI by means of receiver operating characteristic (ROC) curves, computed for the top-30 places of the recommendation lists, which were built on the four datasets (enzymes, ion channels, GPCRs and nuclear receptors) 
 Table 3. Comparison between DT-Hybrid, Hybrid and NBI Algorithm NBI 538.7 55.0 0.9619 ± 0.0005 Hybrid 861.3 85.7 0.9976 ± 0.0003 DT-Hybrid 1141.8 113.6 Note:  For each algorithm the complete DrugBank dataset was used to compute the precision and recall metrics, and the average area under ROC curve (AUC). The parameters used to obtain the following results are  , and  . Values are obtained using the top-20 predictions. Bold values represents best results. 
 Table 4. Comparison of DT-Hybrid, Hybrid, and NBI through the precision and recall enhancement metric, and the average area under ROC curve (AUC) calculated for each of the four datasets listed in  Table 2 Precision enhancement [ ] Recall enhancement [ ] Area Under Curve for the top-20 predictions [ ] Data set NBI Hybrid DT-Hybrid NBI Hybrid DT-Hybrid NBI Hybrid DT-Hybrid Enzymes 103.3 104.6 228.3 19.9 20.9 32.9 Ion channels 22.8 25.4 37.0 9.1 9.7 10.1 GPCRs 27.9 33.7 50.4 7.5 8.8 5.0 Nuclear receptors 28.9 31.5 70.2 0.3 1.3 1.3 Note:  The results were obtained using the optimal values for λ and α parameters as shown in the supporting materials. We set for both Hybrid and DT-Hybrid  . Concerning the α parameter, we have the following setting: enzymes  ; ion channels  ; GPCRs  ; nuclear receptors  . Bold values represents best results. Another important feature of the DT-Hybrid algorithm that we would like to highlight is its ability of increasing performance by keeping computational complexity acceptable. The asymptotic complexity of the NBI algorithm is  , whereas that of DT-Hybrid is  . However, parallelization and optimization techniques can be easily applied to speed computation. We investigated the dependence of DT-Hybrid prediction quality with respect to the α and λ parameters (see the supporting materials for the details). Results show that we cannot discern a law that regulates the behavior of the metrics based on the values of these parameters. They depend heavily on the specific characteristics of each dataset, and therefore require  a priori  analysis to select the best ones. In the reported results, we made such analysis before to run our experiments to establish the parameters yielding the best results in terms of precision and recall enhancement. Finally, notice that our analysis has shown an increase in the precision, recall and AUC, neglecting other metrics, such as recovery, personalization and surprisal. This was done because the latter measure only the capability of analyzing the structure of an interaction network without evaluating the biological significance of predictions. 4 CONCLUSION DT-Hybrid is a technique proposed for the prediction of new interactions between small molecules. Thanks to the domain-dependent additional knowledge, it clearly outperforms the NBI algorithm for DTI prediction. DT-Hybrid integrates biological knowledge and the bipartite interaction network into a unified framework. This yields high quality and consistent interaction prediction, allowing a speedup of the experimental verification activity. Finally, thanks to the hybrid approach, the algorithm overcomes numerical instability that we experienced in the NBI algorithm in presence of particular datasets (i.e. highly sparse). Funding : The publication costs for this article were funded by  PO grant  -  FESR 2007-2013 
 Linea di intervento 
 4.1.1.2 ,  CUP 
 G23F11000840004 . Conflict of Interest : None declared. 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Prediction of cancer driver genes through network-based moment propagation of mutation scores</Title>
    <Doi>10.1093/bioinformatics/btaa452</Doi>
    <Authors>Gumpinger Anja C, Lage Kasper, Horn Heiko, Borgwardt Karsten</Authors>
    <Abstract/>
    <Body>1 Introduction Cancer is a disease of unchecked cellular growth, caused by genetic alterations such as mutations, copy number variations or gene fusions in so called  cancer driver genes . Those alterations can modify both, the activity and cellular function of the gene, and can be classified into activating (proto-oncogenes), or loss of function (tumor suppressor genes and DNA repair genes). Identification of such cancer driver genes is one of the main goals of oncogenic research, as it facilitates mechanistic, diagnostic and therapeutic insights. Cancer genes can be identified through statistical tests that evaluate the mutational burden of the gene (e.g.  Kandoth  et al. , 2013 ;  Leiserson  et al. , 2015 ;  Mularoni  et al. , 2016 ). However, those analyses are complicated by the extensive mutational heterogeneity: Many genes are mutated in a small number of samples, and only few genes show significant mutation across many samples ( Vogelstein  et al. , 2013 ). This phenomenon convolutes the differentiation between genes that only carry passenger mutations, and rarely mutated cancer genes. A potential explanation of this diversity in candidate genes is that genes interact in various pathways ( Hanahan and Weinberg, 2011 ) and protein complexes, and the cancerous potential of a cell is a consequence of the disruption of the pathway, but not necessarily the mutation of  one  specific gene within the pathway. Recent research adopted this interaction-based view on cancer biology: the combination of biological networks and summary statistics that measure each gene’s association to cancer helped the identification of novel cancer driver genes (e.g.  Horn  et al. , 2018 ;  Leiserson  et al. , 2015 ;  Reyna  et al. , 2018 ). In those networks, nodes correspond to genes and edges represent relationships between the adjacent genes. There exists a vast number of biological networks, that are derived from different sources and that cover different scales. Prominent examples are co-expression networks ( Willsey  et al. , 2013 ), co-dependency networks (e.g. AchillesNet;  Li  et al. , 2018 ), co-evolution networks ( Niu  et al. , 2017 ), metabolic pathways ( Kanehisa  et al. , 2017 ) or protein–protein interaction (PPI) networks ( Lage  et al. , 2007 ;  Li  et al. , 2017 ;  Szklarczyk  et al. , 2019 ). Especially, PPI networks constitute an interesting representation of gene interactions, as they commonly combine information from different data sources, tissues, and molecular processes at different scales. However, those PPI networks are far from complete, and our knowledge of them is biased toward well-studied genes ( Horn  et al. , 2018 ). This phenomenon is referred to as  knowledge contamination : well-studied (cancer) genes have a tendency to have more connections in the networks. The potentially large impact on the interpretation of network analyses has to be considered and accounted for, as it might confound results. Methods that use networks as a representation of molecular relationships commonly start with superimposing scores on the nodes. These scores measure the marginal association of the gene to the disease of interest. A prominent choice to represent each gene’s association to cancer is the MutSig  P -value ( Lawrence  et al. , 2014 ): it is a meta- P -value describing whether there is a statistically significant difference in (i) the mutational burden, (ii) the clustering of mutations and (iii) the functional impact of mutations in a gene between healthy and cancer tissues. A plethora of methods has been developed to analyze such gene scores in combination with network information to identify altered subnetworks of genes within the original network. They can be broadly categorized into clustering methods, that aim to find modules of associated genes that cluster together in a network (e.g.  Jia  et al. , 2011 ;  Rossin  et al. , 2011 ) and methods that use network diffusion or network propagation (reviewed in  Cowen  et al. , 2017 ;  Reyna  et al. , 2018 ) to detect altered subnetworks. Both types of methods underlie the common paradigm that genes influencing the same phenotype interact within a network. Especially network propagation methods have shown success in identifying novel cancer driver genes ( Hristov  et al. , 2020 ;  Leiserson  et al. , 2015 ;  Reyna  et al. , 2018 ;  Ruffalo  et al. , 2015 ;  Vandin  et al. , 2011 ,  2012 ). However, network propagation methods exploit by construction the flow of information between genes along paths, and the longer the paths are, the more information gets diluted. This complicates the detection of cancer genes that do not lie on short paths between other cancer genes. Another approach that has proven successful and does not leverage this assumption is  NetSig . It identifies cancer genes based solely on the local neighborhood of genes in a network ( Horn  et al. , 2018 ). At its core lies the computation of an empirical  P -value for each gene that describes the aggregation of genes with low MutSig  P -values in the direct neighborhood. Due to knowledge contamination, the size of a gene’s local neighborhood is affecting the NetSig statistic. To circumvent this, NetSig implements various permutation schemes that take the node degree into account, thereby correcting for this bias. Although the aforementioned methods showed great success in many biomedical applications ( Cowen  et al. , 2017 ), including the discovery of novel cancer genes, they approach the task of gene identification from an unsupervised perspective. However, there exists knowledge on well-established cancer genes (e.g  Sondka  et al. , 2018 ), an important layer of additional information that has, to the best of our knowledge, only been leveraged in few methods for the prediction of cancer driver genes, namely Bayesian modeling ( Sanchez-Garcia  et al. , 2014 ) and unsupervised network propagation ( Hristov  et al. , 2020 ). In most cases, well-established cancer genes are only used to validate the importance and correctness of findings from new methods as a post-processing step. It seems to be an interesting approach to reformulate the task of identifying novel cancer genes as a supervised problem, and learning by exploiting  what we already know . Herein, we propose a novel approach to classify cancer genes in a supervised manner, leveraging the cancer gene annotations from the Cancer Gene Census (CGC) in the COSMIC database ( Sondka  et al. , 2018 ). We achieve this by formulating the problem of finding novel cancer driver genes as a node-classification problem in an interaction network. The heart of our contribution is a novel embedding of nodes in the network based on the distributions of node-features in  k -hop neighborhoods, coupled with a network propagation. We combine the InWeb PPI network ( Lage  et al. , 2007 ;  Li  et al. , 2017 ) with MutSig  P -values ( Lawrence  et al. , 2014 ), and the CGC genes, resulting in an imbalanced dataset due to the low number of known cancer genes compared with the gene corpus. To address this, we develop a cross-validation scheme that enables the supervised prediction of cancer driver genes with a set of classifiers. We compare our approach against both, supervised and unsupervised baselines, and show an improvement with respect to all classification metrics. Last, we evaluate the resulting set of high confidence novel cancer driver candidate genes and find strong links between the predictions and cancer. The list includes known tumor suppressors such as GATA4 ( Agnihotri  et al. , 2011 ), genes known to be affected by recurrent rearrangements FOS ( Fittall  et al. , 2018 ) as well as genes known to be involved in tumor relevant pathways ID2 ( Kijewska  et al. , 2019 ), MYLK ( Avizienyte  et al. , 2005 ;  Cui  et al. , 2010 ;  Zhou  et al. , 2008 ), RALA ( Seibold  et al. , 2019 ). 2 Materials and methods Before we present our novel node embedding procedure, we start by introducing the notation used in this Section, and formally state the problem at hand. 2.1 Notation and problem statement Consider a PPI network that describes interactions between genes. We can represent this interaction network as a graph  G , where the  n  nodes correspond to the genes, and the  m  edges correspond to interactions between genes. We denote the vertex-set as  V , and the edge set as  E . We denote an edge between two nodes  u , v ∈ V  as  e ( u ,  v ), and assume a weighting function  ω : V × V → [ 0 , 1 ]  that assigns each pair of nodes in the network a value between 0 and 1, such that  ω ( u , v ) &gt; 0 ⇔ e ( u , v ) ∈ E . In the case of a weighted network, the function  ω  might correspond to confidence scores of edges, in the case on an unweighted network,  ω  is a binary indicator. Additionally, we assume the existence of a  d -dimensional feature representation for every vertex  v ∈ V , denoted by  x v ∈ ℝ d  or in matrix notation by  X ∈ ℝ n × d . We write the graph as  G = ( V , E , X , ω ) . We define the  k -hop neighborhood of a vertex  v ∈ V  as the set of all genes that can be reached from  v  along at least  k  edges. This can be expressed recursively as
 (1) N v k = { u   |   e ( w , u ) ∈ E ,   ∀ w ∈ N v k − 1 ,         u ∉ N v l   ∀   l ∈ { 0 , .. , k − 1 } } , where  N v 0 = v  (see  Fig. 1a  for visualization of one- and two-hop neighborhoods).
 Fig. 1. Illustration of moment propagation embeddings for node feature  i . (a) Computation of moment embedding  η i k ( v )  for vertex  v  and  k  =   2. Blue nodes indicate the one-hop, orange nodes the two-hop neighborhood. Moments of the distributions  P v : i 1  and  P v : i 2  that describe the values of feature  i  in the one- and two-hop neighborhood of vertex  v  are computed and aggregated. ( b ) Computation of propagation embedding: The node representation is updated by aggregating over all nodes in its one-hop neighborhood. ( c ) Two paths  q 1  and  q 2  connect root vertex  v  with its two-hop neighbor  u     Problem statement We assume a partially labeled, one-class setting in which we have a positive label  l 1  for a subset  V l  of the nodes, but the majority of the nodes are unlabeled, denoted by the set  V u . Our goal is to develop a node embedding  γ G ( · )  based on the feature representation  X  of all vertices  v ∈ V  and the network  G  that, in combination with a binary classifier  C , enables the decision of whether any unlabeled node  u ∈ V u  belongs to the class  l 1 . That is  C ( γ G ( x u ) ) = P ( y u = l 1 ) , where  y v  denotes the label of node  v . More specifically, our goal is to develop a node embedding that serves as input for the supervised, binary classification task of identifying cancer driver genes. It is based on the integration of a PPI network with scores that measure the marginal association of each gene to cancer, when those scores are superimposed on the nodes in the network. 2.2 Generation of node embeddings for the prediction of cancer driver genes The node embeddings proposed in this article are based on two different concepts. The first one is the representation of each node as a distribution across its neighbors’ feature vectors. It is motivated by the success of methods such as NetSig ( Horn  et al. , 2018 ) that focus on the local neighborhood of nodes in the network. However, we extend this idea in three directions: (i) we do not restrict ourselves to one-hop neighborhoods, (ii) we condense the distributions into their  moments , resulting in a concise and computationally efficient representation and (iii) we integrate edge weights into the approach. This moment representation addresses the knowledge-bias in the network, as the description of a distribution via its moments is independent of the number of draws from that distribution. The second concept is a Weisfeiler–Lehman like ( Weisfeiler and Lehmann, 1968 ;  Shervashidze  et al. , 2011 ) aggregation of local features, that is an iterative combination of features from a node’s local neighborhood. It is similar to the network propagation approaches described in  Cowen  et al.  (2017) , and is widely used in methods such as hierarchical HotNet ( Reyna  et al. , 2018 ) and its earlier versions ( Leiserson  et al. , 2015 ;  Vandin  et al. , 2012 ), but also in graph convolutional networks (e.g.  Gilmer  et al. , 2017 ;  Kipf and Welling, 2016 ), where different regimes for aggregation over local neighborhoods are being actively researched. 2.2.1 Embedding genes using moments of local neighborhood distributions Each node  v ∈ V  in the network is represented by its  d -dimensional feature vector  x v ∈ ℝ d , and we denote the  i th feature by  x v : i . The moment embeddings described in this section are computed for each of the  d  features identically and independently, and eventually stacked together. We assume the existence of a probability distribution  P v : i k  that generates the  i th feature for all nodes in the  k -hop neighborhood of node  v . That is, the  i th feature values of  k -hop neighbors of  v  constitute draws from this distribution,  ∀ u ∈ N v k : x u : i ∼ P v : i k . We create an embedding for every vertex  v  that is based on a concise description of the distributions  P v : i k  for  i = 1 , .. , d , and hyperparameter  k ∈ ℕ + . For this, we start by defining a function  ν ¯ ( X )  that maps a scalar random variable  X ∼ P  to its first four moments, that is:
 (2) ν ¯ ( X ) = [ E X [ X ] ,   E X [ X 2 ] ,   E X [ X 3 ] ,   E X [ X 4 ] ) ] . In practice, the expectations in  Equation (2)  can be replaced with the sample mean  μ ( · ) , variance  σ ( · ) , skewness  ξ ( · )  and kurtosis  κ ( · )  and applied to a realization of the random variable  X , denoted by  x = [ x 1 , … , x q ] . We write this function as
 (3) ν ( x ) = [ μ ( x ) ,   σ ( x ) ,   ξ ( x ) ,   κ ( x ) ] , and call it a  moment embedding function . For a vertex  v , we denote with  X v : i k  the values of the  i th feature of vertices in the  k -hop neighborhood of  v , i.e.  X v : i k = { x u : i   |   u ∈ N v k } . Those values constitute a draw from the distribution  P v : i k . We describe the node embedding of vertex  v  with respect to feature  i  by applying the function  ν ( · )  up to its  k -hop neighborhoods, that is
 (4) η i k ( v ) = [ x v : i   , ν ( X v : i 1 ) , …   ,   ν ( X v : i k ) ] , The value  k  is a hyperparameter of the embedding that indicates the maximum neighborhood to be included (see  Fig. 1a  for an example of the node embeddings). The moment embedding  η i k  is a function that creates a representation of every vertex  v ∈ V  by describing its  k -hop neighborhoods with respect to a scalar feature indexed by  i , such that  η i k ( v ) ∈ ℝ ( 1 + 4 k ) . This function can be applied to each of the  d  node features separately, and the resulting representations are stacked to give
 (5) η k ( v ) = [ η 1 k ( v )   , …   , η d k ( v ) ] T . This results in the moment embedding function  η k : V → ℝ d × ( 1 + 4 k ) . 2.2.2 Embeddings using network propagation The second type of node embeddings is based on a Weisfeiler–Lehman like aggregation of nodes in the neighborhood with continuous node features. In this procedure, the representation of every node is simultaneously updated based on the representations of the node’s direct neighborhood (see  Fig. 1b  for an example). That is, given an initial feature representation  x v  of vertex  v , it is represented as
 (6) x v t = 1 | N v 1 | ∑ v ′ ∈ N v 1 x v ′ t − 1 at the  t th Weisfeiler–Lehman iteration. This aggregation corresponds to the element-wise mean across a node’s one-hop neighborhood, and can be used to generate node embeddings by stacking the representations for  t  iterations as follows:
 (7) ρ t ( v ) = [ x v 0 , x v 1 , … , x v t ] , and  ρ t ( v ) : ℝ d → ℝ ( 1 + t ) × d . The number  t  of iterations of this propagation scheme is treated as a hyperparameter that can be tuned during learning. 2.2.3 Combining moment and propagation embeddings to represent genes in a network Here, we propose a combination of the two concepts introduced above, and call the resulting node embedding a  moment propagation embedding , short  MoPro  embedding. It corresponds to a composition of the moment embeddings  η k  and the propagation embedding  ρ t  above, and can be written as:
 (8) γ t , k ( v ) = ( ρ t ° η k ) ( v ) . This function first creates the moment embedding from the feature vector  x v  of a vertex  v , and continues to propagate this representation of the local neighborhoods through the network. As the combination of both functions, it maps the original feature representations of the vertices to a higher dimensional space as follows:  γ t , k : V → ℝ ( 1 + t ) ( 1 + 4 k ) × d . 2.2.4 Extension to networks with weighted edges If a non-binary weighting function  ω  exists, i.e. the edges in the network are weighted and weights can for instance represent confidence scores, we can incorporate this layer of information into our approach: for every edge in the network, the value of the weighting function is non-zero, that is  ∀ e ( v , u ) ∈ E :   ω ( v , u ) ∈ ( 0 , 1 ] , with 1 indicating the highest confidence. These weights can be used to distribute importance of neighbors in a local neighborhood by rescaling the node-features in the moment embedding. This rescaling is done for each feature  i  separately, such that the values of features  i  in the  k -hop neighborhood of node  v  become
 (9) X v : i k , weight = { f ( x u : i , ω ( v , u ) )   |   x u ∈ N v k , } with a problem-specific weighting-function  f ( · , · ) . For  k -hop neighbors  u  of  v  with  k  &gt;   1 the weight  ω ( u , v )  is zero by definition. Hence, we compute it in the following three-step process, and denote it as  ω k ( u , v ) : (i) First, we enumerate all paths of length  k  between  v  and  u , denoted by the set  Q k , and individual paths in  Q k  are denoted as  q j ( v , u )  (see  Fig. 1c ). (ii) Second, we compute the weight of each path in  Q k  as the product of its  k  edge weights and (iii) third we compute the weight  ω k ( v , u )  as a function on the set of path weights,  g : [ 0 , 1 ] | Q k | → ℝ . We treat this function  g ( · )  as a hyperparameter, and use either  g ( · ) = max ( · )  or  g ( · ) = mean ( · ) . 3 Results 3.1 Dataset description In order to find novel cancer driver genes, we combine data from a The Cancer Genome Atlas (TCGA) pan-cancer study of 9 423 tumor exomes (comprising all 33 of TCGA projects;  Bailey  et al. , 2018 ) with the well-established InBio Map PPI network ( Lage  et al. , 2007 ;  Li  et al. , 2017 ). The network constitutes our view of interactions between genes on a protein level. The network has an average degree of 61.02 (±128.33), and the sizes of the  k -hop neighborhoods are illustrated in  Figure 2a . We represent each node in the network with its −log 10  transformed MutSig  P -value ( Lawrence  et al. , 2014 ). Those  P -values measure whether a gene shows significantly different mutational patterns in tumor versus normal tissues. In total, we have access to  P -values for 18 154 genes. As a pre-processing step, we remove all nodes from the network that cannot be represented with a MutSig  P -value, as well as all isolated nodes. This results in a total of 11 449 genes that are present in the InBio Map network, are connected to at least one other node and have been tested with the MutSig tool. Those constitute our candidates for network-based prediction of cancer driver genes.
 Fig. 2. Dataset description: ( a ) the distribution of neighborhood sizes, for neighborhoods defined as in  Equation (1) , for  k = 1 , 2 , 3 . ( b ) The distribution of the node degree, shown for 635 cancer genes and 10 816 unlabeled genes. ( c ) The distribution of the MutSig  P -values, in cancer genes and unlabeled genes. ( d ) The correlation between the degree and the MutSig  P -values for cancer genes and for unlabeled genes     Class labels In general, supervised machine learning requires access to labeled data to train a classifier. To obtain labels for the genes, we use the CGC data from the COSMIC database ( Sondka  et al. , 2018 ). We downloaded a list of 723 genes that have been causally implicated in cancer, and use this set as our ground truth. Genes in the CGC are categorized into Tiers 1 and 2, where genes in Tier 1 show a documented activity relevant to cancer, and genes in Tier 2 show strong indications to play a role in cancer. For our analysis we treat both tiers equally. We overlap the set of 723 genes with our network, giving a total of 635 cancer genes. This leads to a dataset, in which ‘positive’ samples make up &lt;6.0% of our dataset. We refer to the remaining genes as  unlabeled genes , and we are interested in finding new cancer genes among them. Using the CGC genes, we observe a knowledge-bias in the InBio Map PPI network (see  Fig. 2b ), that is cancer genes tend to have higher degrees in the network. We furthermore observe an increased correlation between degree and MutSig  P -values for cancer genes (Pearson correlation: 0.17) compared with unlabeled genes (Pearson correlation: 0.10), as can be seen in  Figure 2c and d . Although this indicates that MutSig can identify the highly mutated cancer genes, there exist many well-established cancer genes whose mutation rates lie within the background distribution (i.e. their MutSig  P -values are undistinguishable between cancer genes and unlabeled genes). This poses three challenges that have to be addressed: (i) we do not have a high-quality negative class, i.e. in general any gene not classified as a cancer gene might potentially be a cancer driver, (ii) the dataset is imbalanced, a fact that requires attention during supervised classification and (iii) the dataset is affected by knowledge contamination. We address challenges (i) and (ii) with an elaborate and unbiased cross-validation procedure to train and test a classifier, as well as to predict cancer driver genes from the unlabeled genes. The third challenge is addressed by using the moments in the MoPro embeddings. Although we observe that moments such as skewness and kurtosis exhibit positive correlations with the node-degree, this is the case for both, cancer genes and unlabeled genes (see  Supplementary Fig. S1 ). 3.2 Experimental setup 3.2.1 Cross-validation for one-class, imbalanced learning To address the above mentioned challenges imposed by the class imbalance and the lack of a negative class, we developed a cross-validation procedure that is based on the repeated undersampling of the majority class. The cross-validation procedure is illustrated in  Figure 3 , and a pseudocode can be found in the  Supplementary Algorithm S1 . The dataset can be represented as a matrix  D ∈ ℝ 11   449 × d , where  d  is the number of node features ( Fig. 3a ). The cross-validation procedure consists of three main steps: Step 1: Data splits We split the dataset  D  into two disjoint datasets,  D l  and  D u  ( Fig. 3b ), where  D l  consists of all genes in the positive class, and a random subsample of the unlabeled genes. We undersample the majority class such that 10% of samples in  D l  are cancer genes. For the sake of training a classifier, we assign the unlabeled samples in  D l  to the negative class, and assume this to be the ground truth for the current split. This dataset will be used in the second step to train and evaluate a classifier. The genes in  D u  remain unlabeled, and we use the classifier trained on  D l  to predict their cancer status. Step 2: Training and evaluation of the classifier Next, the dataset  D l  is split into a cross-validation (80% of data) and a hold-out test set (20% of data;  Fig. 3c ). On the cross-validation set, we do a 5-fold stratified cross-validation to find the best hyperparameters of the classifier  C , resulting in  C ′ . We retrain the classifier on the complete cross-validation set, and evaluate the predictive performance of  C ′  on the hold-out test set. Importantly, the cross-validation and hold-out test sets are disjoint. This implies that samples in the hold-out test set have never been seen during training, nor were they used to choose the best hyperparameters of the classifier. This set is solely used to evaluate the ability of the classifier to generalize to unseen samples. The strict separation of the cross-validation and the hold-out test set is necessary to avoid an inflation of the evaluation metrics. Furthermore, each classifier was run and evaluated on the test data only once. Step 3: Prediction. Last, we apply the classifier  C ′  from the previous step to predict the cancer status of genes in the unlabeled dataset  D u  ( Fig. 3d ).
 Fig. 3. ( a ) The dataset consists of 11 449 genes, each one represented by a set of features. 635 of those genes are classified as cancer genes ( Sondka  et al. , 2018 ), highlighted in yellow. The remaining 10 814 genes are unlabeled (green). Cross-validation scheme on  one  data split, resulting in one best classifier  C ′ . ( b ) The unlabeled genes are sub-sampled at random and combined with the cancer genes, giving rise to the labeled dataset  D l . The unlabeled genes in  D l  are assigned a negative class label. The remaining unlabeled genes make up the set  D u . Those are the genes for which the cancer status will be predicted in the current split. ( c ) The  D l  dataset is split, and 80% of the data are used to find the best hyperparameters of the classifier via 5-fold CV, resulting in the classifier  C ′ . The remaining 20% are used as test set for evaluation of the classifier  C ′ . ( d ) The classifier  C ′  that has been trained on the cross-validation set in  D l  is used to predict the cancer status of genes in  D u This cross-validation procedure learns to distinguish cancer genes from a random split of the unlabeled genes. However, this is a potentially incorrect assumption, since unlabeled genes in the set  D l  might be yet-to-discover cancer genes. For this reason, we repeat the complete cross-validation procedure for  r  different random splits of the dataset into a labeled subset  D l  and a unlabeled subset  D u , resulting in a  set of classifiers   E = { C ′ 1 , .. , C ′ r } . Each  D l  is divided into an 80% cross-validation dataset and a 20% test dataset. The cross-validation dataset is used for hyperparameter optimization of the classifiers and the test dataset is used for evaluation in terms of area under the precision recall curve (AUPRC), precision, recall and F1-score.  C ′ i  is the best classifier on the  i th random split of the data, determined on the cross-validation dataset. For each  C ′ i ∈ E , where  i = 1 , .. , r , we compute the performance metrics (AUPRC, precision, recall, F1-score) on the test set of the  i th split, and report the mean and standard deviation of the metrics across all  r  classifiers in the set  E  as the final result. In order to obtain comparable results for different classification algorithms, we ensure that each algorithm is trained and evaluated on the same  r  splits of the data. We determine the total number of splits  r  based on the minimum number of predictions we want to obtain for every gene without a cancer label in the dataset. We set this value to five, resulting in  r  =   11 data splits, and evaluate the effect of varying  r  in terms of the average AUPRC on the test sets in an experiment (see Section 3.3.3 and  Fig. 4b ). Fig. 4. Evaluation of a set of logistic regression classifiers (hyperparameters as in  Table 2 ). ( a ) AUPRC when varying the number of random splits  r , and therefore the number of classifiers in the set  E . ( b ) Evaluation metrics as functions of the training set size Since this splitting of the data is random, the underlying data distribution of the negative class varies from split to split, and the classifier optimized on each split learns the data modalities of the negative class in the current split. Every gene is predicted with each classifier in our set of classifiers (excluding the ones for which it was in the  D l  set used for training), and might be classified as a cancer gene by some of the classifiers, but not by others. This can be interpreted as the fact that a gene might be more similar to a cancer gene in some aspects, but more similar to a non-cancer gene in others. Eventually, a gene is classified as a cancer gene according to the majority vote across all classifiers (for which it was not in the training data). In the case of ties, we resort to the conservative prediction of ‘no cancer gene’. Importantly, as there exist no known labels for those genes, we analyze them qualitatively. 3.2.2 Classification We represent each node in the network by the MoPro embeddings computed from the log-transformed MutSig  P -values, as described in Section 2.2.3. We apply four different state-of-the-art classification algorithms to predict the binary class labels in the cross-validation procedure described above, using python’s sklearn module: logistic regression, random forests, support vector machines (SVMs) and gradient boosting. For every classifier, we optimize across a grid of standard hyperparameters, as well as the following data-specific hyperparameters: (i) whether or not to include a scaling step in the classification pipeline (SCALE), (ii) whether to use edge weights to generate node embeddings (WEIGHT), (iii) how to represent weights between two nodes in a  k -hop neighborhoods with  k  &gt;   1 (PATH; see Section 2.2.4), as well as (iv) the number of propagation steps  t  and (v) the number of  k -hops to generate moment embeddings from, where  k ∈ { 1 , 2 } ,  t ∈ [ 1 , … , 6 ] . We restrict the value of  k  to a maximum of 2, as we observe that three-hop neighborhoods in the InBio Map network already span major parts of the network (see  Fig. 2a ). In order to weight the contribution of node features in local neighborhoods during the generation of moment embeddings (function  f ( · , · )  in  Equation 9 ), we use a simple multiplication between the node features and the edge weights, resulting in a lowering of the contribution of the −log 10  transformed  P -values for edges that exhibit confidences below 1.0. In order to evaluate the predictive performance of each classifier, we use the AUPRC, as well as precision, recall and the F1-score. As described in the previous Section, to evaluate the performance of a set of classifiers  E  resulting from the cross-validation procedure, those metrics correspond to the average across the classifiers in  E . We furthermore report the number of predicted genes that are novel, i.e. those that are not contained in the COSMIC CGC gene set. We would like to note that, although the area under the ROC curve (AUROC) is a common metric to evaluate binary classifiers, its interpretation is difficult in our setting. Due to the high class imbalance and our primary interest in detecting members of the minority class (i.e. the cancer genes), measuring precision and recall on the minority class is a better suited metric for our task. The AUROC values can be found in the  Supplementary Tables S1 and S2 . 3.3 Classification of cancer genes 3.3.1 Baseline methods We compare our approach against univariate baselines, that is we determine the cancer driver status of a gene based on (i) its degree, (ii) its MutSig  P -value and (iii) its NetSig  P -value. Those results are listed in  Table 1 . For all node features, we compare a ranking of the genes by the respective feature ( ranking ) and a prediction with a set of logistic regressors ( LogReg ), generated with the cross-validation procedure described in Section 3.2.1. When ranking the genes based on features, we report precision and recall at the threshold that gave the best F1-score. For the MutSig and NetSig  P -values, we also evaluate predictive performance after Benjamini–Hochberg (BH) correction at a false discovery rate of 10%. Note that since there is only one classification result for the ranking and Benjamini–Hochberg procedure, there are no standard deviations reported in the table. Furthermore, in both cases, the number of novel genes does not correspond to a majority vote, but is based on a single prediction, using the prediction threshold that resulted in the highest F1-score. We also apply hierarchical HotNet ( Reyna  et al. , 2018 ), a state-of-the-art network propagation method for the detection of altered subnetworks in cancer, to the MutSig  P -values (after −log 10  transformation). We chose the score permutation scheme to obtain a measure of significance ( P  =   0.01) and report all genes in subnetworks of sizes &gt; 1 as positives. For all methods we observe that using the set of classifiers improves predictive performance with respect to the AUPRC. Furthermore, we observe that the degree of a gene in the network reaches AUROC values of up to 70% (see  Supplementary Table S1a ), hinting toward the problem of knowledge bias in biological networks. That is, the degree operates as a confounder in those networks.  Table 1. Results of cancer gene classification for the baselines Feature Method AUPRC Precision Recall F1 No. of novel Degree Ranking 0.096 0.105 0.436 0.169 2368 Degree LogReg 0.199 (0.007) 0.243 (0.012) 0.236 (0.000) 0.239 (0.006) 905 MutSig Ranking 0.248 0.474 0.202 0.283 142 MutSig LogReg 
 0.312 (0.007) 
 0.552 (0.060) 0.236 (0.000) 0.330 (0.011) 243 MutSig BH 0.248 0.490 0.191 0.274 126 MutSig Hier. HotNet — 0.137 0.111 0.123 444 NetSig Ranking 0.158 0.219 0.235 0.226 532 NetSig LogReg 0.275 (0.012) 0.278 (0.019) 0.228 (0.000) 0.250 (0.008) 704 NetSig BH 0.158 0.263 0.169 0.205 300 
 Note : The first column indicates the feature that was used to represent each gene during classification, the second column indicates the method that was used for classification. In case of LogReg, we used the cross-validation procedure described in Section 3.2.1 and fixed the recall at 23.5%. AUPRC is the area under the precision recall curve, the method with the highest AUPRC is printed in bold. The last column indicates the number of de novo cancer genes, i.e. those genes that are not contained in the set of cancer genes. 3.3.2 Cancer gene classification with MoPro embeddings We generate MoPro embeddings from the −log 10  transformed MutSig  P -values, and use these embeddings as input to the classifiers. The results are listed in  Table 2 . We evaluate the four classifiers on a grid of hyperparameters, and list the best values of the ones specific to our proposed approach (see Section 3.2.2) in the table. We observe a similar performance of all classifiers with respect to AUPRC, with a minor exception for the random forest classifier. The classification using MoPro embeddings combined with the cross-validation procedure to handle imbalanced classes clearly outperforms the baselines. The baseline with the best AUPRC is the logistic regression classification using MutSig  P -values (AUPRC = 31.2%). With the MoPro embeddings, AUPRC values of up to 43.7% are achieved with the gradient boosting classifier (closely followed by logistic regression and SVMs). A similar trend can be observed for AUROC scores (see  Supplementary Table S1 ).  Table 2. Results of cancer gene classification for the moment propagation embeddings Method Scale Weight Path 
 t 
 
 k 
 AUPRC Precision Recall F1 No. of novel LogReg True — -- 3 2 0.434 (0.014) 0.572 (0.046) 0.236 (0.000) 0.334 (0.009) 202 SVM False — -- 6 2 0.431 (0.012) 0.584 (0.058) 0.236 (0.000) 0.336 (0.010) 198 RandFor True standard Mean 3 1 0.396 (0.021) 0.560 (0.057) 0.234 (0.004) 0.330 (0.011) 193 GradBoost True standard Max 3 2 
 0.437 (0.020) 
 0.636 (0.088) 0.236 (0.000) 0.343 (0.012) 150 
 Note : Classification results for different classifiers using the proposed moment propagation embeddings and the described cross-validation procedure. The Columns 2–6 indicate the hyperparameters that gave the best classification performance for each set of classifiers.  t  and  k  are the hyperparameters of the moment propagation embeddings, namely the number of propagation steps and the neighborhood degree up to which moments are computed. AUPRC is the area under the precision recall curve, the method with the highest AUPRC is printed in bold. The last column indicates the number of de novo cancer genes, i.e. those genes that are not contained in the set of cancer genes. For all analyses, we fixed the recall at 23.5%, that is the recall achieved by ranking the NetSig  P -values. We observe that with MoPro embeddings, we obtain an up to three-fold improvement of precision at that same recall value compared with the NetSig approach (ranked NetSig  P -values: 21.9%, gradient boosting 63.6%). When contrasting the precision of MoPro embeddings with the one of the best baseline, that is logistic regression using the MutSig  P -value, we observe an improvement of ∼8%. We optimize the data-specific hyperparameters, and find that for all classifiers, using at least three propagation steps enables best classification. All methods but random forests performed best when deriving moments from the  k  =   2-hop neighborhoods. Although random forests and gradient boosting achieve better classification performance when using weighted neighborhood distributions, this was not the case in logistic regression and SVMs. There seems to be no clear winner between the generation of weights in  k -hop neighborhoods when using the mean or the maximum aggregation (Section 2.2.4). 3.3.3 Dependence of results on cross-validation parameters The results in  Table 2  are produced with  r  =   11 splits of the data into  D l  and  D u  (Section 3.2.1). We evaluate the performance of classification with MoPro embeddings for values of  r  in the range [5, 500] while keeping all data-specific hyperparameters (as described in Section 3.2.2) fixed (using logistic regression). We observe that the classification performance is not affected by changes in the parameter  r  (see  Fig. 4a ). Note that we lose ∼2% in AUPRC due to fixing the data hyperparameters. We furthermore evaluate how the size of the training set affects the classification performance. In our proposed cross-validation scheme, the training set size is fixed due to the 5-fold cross-validation, and contains 4064 genes (the cross-validation set contains 5080 genes, such that 4064 genes are used in a 5-fold cross-validation to train the classifier). We conducted an experiment where this number is reduced, ranging between 10 and 4000 samples. We observe a steep increase in performance with an increasing training set size up to 1000 training samples (see  Fig. 4b ), and a saturation when using more than 1000 samples. This indicates, that at least 1000 samples are required to represent the data distribution during classification. 3.3.4 Ablation study We conduct an ablation study to understand how the individual parts in the moment propagation embedding contribute to the improved performance. The results can be found in  Table 3 . We evaluate two different types of experiments: (i) we only use the node feature (i.e. the −log 10  transformed MutSig  P -value), and propagate it through the network with the propagation embedding  ρ t ( · )  described in Section 2.2.2. This representation of the node features is used to train a set of logistic regression classifiers. The results of this analysis are listed in the row with label ‘propagation only’. (ii) We represent each node with the moment embedding  η k ( · ) , k = 2  described in Section 2.2.1, without propagating the resulting representation through the network. The results of this approach are listed in the row with label ‘moments only’. We observe that removing the moment embedding results in a severe drop in performance of ∼8%, while keeping moments but not propagating them leads to a less severe reduction (∼2.8%). This observation indicates that the main improvement of performance compared with the baselines is due to the description of a node by means of the distribution of node features in its neighborhood, motivating the development of methods that improve the representation of local neighborhoods.  Table 3. Results of the ablation study for the set of logistic regression classifiers Setting Method AUPRC Baseline LogReg 0.434 (0.014) Propagation only LogReg 0.348 (0.010) Moments only LogReg 0.406 (0.011) 
 Note : In  propagation only , the node feature is propagated, but no moment embedding is computed. In  moments only , moments are computed, but no propagation embedding is computed. The first row repeats the baseline results (moment propagation embeddings) for comparability reasons. 3.4 Evaluation of predicted cancer driver genes To generate a candidate gene list, we created a consensus set of all genes as follows: for each of the four classification algorithms (logistic regression, SVM, random forest, gradient boosting), we took the intersection of genes that were classified as ‘novel’ (see  Table 2 ). This means that a gene in the consensus set has (i) not been classified as a cancer driver gene in the CGC dataset, and (ii) all four sets of classifiers identified the gene as a cancer driver (as described in Section 3.2.1). This led to 50 candidate genes, 31 of which were significant in the MutSig data ( P = 8.04 * 10 − 42 , hypergeometric test), 10 in the NetSig data ( P = 1.07 * 10 − 6 , hypergeometric test) and 12 with hierarchical HotNet ( P = 2.04 * 10 − 7 , hypergeometric test), where  P -values measure whether the set of 50 consensus genes is significantly enriched with MutSig, NetSig and hierarchical HotNet hits, respectively. By removing all genes from the consensus set that were detected with at least one other method, 14 novel genes remained. For those, we performed a literature review to estimate the evidence for links to cancer. In brief, four genes have a direct link to tumorigenesis in human. The transcription factor GATA4 is a known tumor suppressor in Glioblastoma Multiforme ( Agnihotri  et al. , 2011 ). . In breast cancer patients, ID2 is upregulated in brain metastasis and high expression is linked to an increased risk of developing relapse ( Kijewska  et al. , 2019 ). Last, FOS exhibits recurrent rearrangements Osteoblastoma ( Fittall  et al. , 2018 ). Five genes can be strongly linked to tumor relevant behavior and pathways. ACVR1B (also known as ALK4) is linked to tumorigenesis through its interaction with activin-A ( Kalli  et al. , 2019 ;  Rautela  et al. , 2019 ). CASP10 inhibition leads to reduced apoptosis, while loss-of-function of RAP1A causes a reversion to a non-malignant phenotype in a model of invasive carcinoma ( Stammer  et al. , 2017 ). MYLK is involved in proliferation and the migration of cancers of the breast, prostate and colon ( Avizienyte  et al. , 2005 ;  Cui  et al. , 2010 ;  Zhou  et al. , 2008 ). CSNK1A1, a member of the CK1 kinase family, is a regulator of the autophagic pathway in RAS-driven cancers, and knock-out experiments lead to cell death in Multiple myeloma ( Carrino  et al. , 2019 ;  Cheong  et al. , 2015 ). For the remaining six genes, five (CASP1, CASP14, RBL1, HNF4A and RALA) had weaker links (e.g. expression linked or pathway membership), but no clear experimental evidence ( Gouravani  et al. , 2020 ;  Krajewska, 2005 ;  Schade  et al. , 2019 ;  Seibold  et al. , 2019 ;  Wang  et al. , 2020 ). Only for one gene (DLGAP2) we could not find any evidence for a link to cancer. 4 Discussion and conclusions In this article, we proposed a novel approach for the identification of cancer driver genes by integrating MutSig summary statistics ( Lawrence  et al. , 2014 ) with PPI networks ( Lage  et al. , 2007 ;  Li  et al. , 2017 ). In stark contrast to state-of-the-art approaches that set out to solve this problem with unsupervised processes, we developed an innovative node embedding procedure (MoPro embeddings) to enable supervised classification of cancer driver genes. Reformulating the problem of cancer-gene prediction in a supervised fashion enables learning from  what we already know : we include knowledge on the data distributions of well-established cancer driver genes and learn from these distributions to improve the prediction task. We do so by combining two concepts: (i) the representation of a node based on the distribution of node features in its  k -hop neighborhood, followed by (ii) a network propagation. The neighborhood distributions in (i) are described concisely by their first four moments, which constitutes a computationally efficient summary, and addresses the knowledge contamination that often confounds analyses of biological networks. We show that our approach outperforms baselines with respect to AUPRC by a margin of more than 10%, and that results are stable with respect to the hyperparameters of the method. Interestingly, we find that the main improvement in predictive performance is presumably caused by the representation of the distributions of node features in a gene’s neighborhood, rather than the network propagation. This finding paves the way for further research: while the proposed representation of the distributions by means of their moments is straightforward and computationally efficient, another option is to exploit principles of optimal transport ( Villani, 2008 ) to compare two nodes based on the distributions of features in their neighborhoods.  Togninalli  et al.  (2019)  developed a kernel based on Wasserstein distances between distributions for graph classification, and this idea can be readily extended to node classification. Another possible route for future research is to build models that combine both, node embeddings and classification, and to train them end-to-end, as is done with graph convolutional networks (e.g.  Duvenaud  et al. , 2015 ;  Hamilton  et al. , 2017 ;  Kipf and Welling, 2016 ). The set of high confidence consensus genes discovered with our proposed approach contained both, genes that were previously identified as cancer drivers with methods such as MutSig, NetSig and hierarchical HotNet, as well as novel genes that were not detected with established methods. Those genes constitute promising targets for future biological evaluation, and their detection showcases the potential of combining network-derived features with supervised machine learning techniques for the prediction of cancer driver genes. Supplementary Material btaa452_Supplementary_Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Identification and removal of ribosomal RNA sequences from metatranscriptomes</Title>
    <Doi>10.1093/bioinformatics/btr669</Doi>
    <Authors>Schmieder Robert, Lim Yan Wei, Edwards Robert</Authors>
    <Abstract>Summary: Here, we present riboPicker, a robust framework for the rapid, automated identification and removal of ribosomal RNA sequences from metatranscriptomic datasets. The results can be exported for subsequent analysis, and the databases used for the web-based version are updated on a regular basis. riboPicker categorizes rRNA-like sequences and provides graphical visualizations and tabular outputs of ribosomal coverage, alignment results and taxonomic classifications.</Abstract>
    <Body>1 INTRODUCTION Metatranscriptomic approaches are drastically improving our understanding of metabolism and gene expression in microbial communities. By investigating all functional mRNA transcripts isolated from an environmental sample, metatranscriptomic analyses provide insights into the metabolic pathways important for a community at the time of sampling. Although metatranscriptomes are used to investigate metabolic activities, the majority of RNA recovered in metatranscriptomic studies is ribosomal RNA (rRNA), often exceeding 90% of the total reads ( Stewart  et al. , 2010 ). Even after various treatments prior to sequencing, the observed rRNA content decreases only slightly ( He  et al. , 2010 ) and metatranscriptomes still contain significant amounts of rRNA. Although rRNA-like sequences are occasionally removed from metatranscriptomes, the removal is performed only with a subset of the publicly available rRNA sequences. Failure to remove all rRNA sequences can lead to misclassifications and erroneous conclusions during the downstream analysis. It is estimated that misannotations of rRNA as proteins may cause up to 90% false positive matches of rRNA-like sequences in metatranscriptomic studies ( Tripp  et al. , 2011 ). The potential for false positives arrises from a failure to completely remove all rRNA prior to translating the putative rRNA and querying a protein database. The rRNA operons in Bacteria and Archaea are not known to contain expressed protein coding regions that at the same time code for rRNA and therefore, annotations of proteins in rRNA coding regions should be presumed to be misannotations ( Aziz  et al. , 2008 ). Metagenomic sequence data generated to asses the metabolic potential of a community will also be affected by false positive matches of rRNA sequences when querying a protein database. Therefore, transcript analysis should only proceed after it has been verified that all rRNA-like sequences have been found and removed from the dataset to allow accurate identification of the transcribed functional content. The high-throughput nature of community sequencing efforts necessitates better tools for the automated preprocessing of sequence datasets. Here, we describe an application able to provide graphical guidance and to perform identification, classification and removal of rRNA-like sequences on metatranscriptomic data. The application incorporates a modified version of the BWA-SW program ( http://bioinformatics.oxfordjournals.org/citmgr?gca=bioinfo;26/5/589 ), and is publicly available through a user-friendly web interface and as stand-alone version. The web interface allows online analysis using rRNA sequences from public databases and provides data export for subsequent analysis. 2 METHODS 2.1 Implementation and computational platform The riboPicker application was implemented as stand-alone and web-based version in Perl. The web application is currently running on a web server with Ubuntu Linux using an Apache HTTP server to support the web services. The alignments are computed on a connected computing cluster with 10 working nodes (each with 8 CPUs and 16 GB RAM) running the Oracle Grid Engine version 6.2. All graphics are generated using the Cairo graphics library ( http://cairographics.org/ ). 2.2 Identification of rRNA-like sequences The identification of rRNA-like sequences is based on sequence alignments using a modified version of the BWA-SW program. The modifications do not change the default behavior of the algorithm and include parameter forced changes in the alignment of ambiguous bases and the generation of an alternative output. The documentation provides a detailed list of changes and is available on the program website. riboPicker uses query sequence coverage, alignment identity and minimum alignment length thresholds to determine if an input sequence is an rRNA-like sequence or not. This approach is based on the idea that looking for similar regions consists of grouping sequences that share some minimum sequence similarity over a specified minimum length. Threshold percentage values are rounded toward the lower integer and should not be set to 100% if errors are expected in the input sequences. The results for multiple databases are automatically joined before generating any outputs. Using simulated datasets, we evaluated the classification of rRNA-like sequences and showed that riboPicker performed with high accuracy comparable to the latest version of meta_rna ( Huang  et al. , 2009 ) and BLASTn ( Supplementary Material ). A comparison on real metatranscriptomic data showed that riboPicker processes data more than twice as fast as Hidden Markov Model (HMM)-based programs and &gt;100 times faster than BLASTn ( Supplementary Material ). 2.3 Reference databases The web-based version offers preprocessed databases for 5S/5.8S,16S/18S and 23S/28S rRNA sequences from a variety of resources, currently including SILVA ( Pruesse  et al. , 2007 ), RDP ( Cole  et al. , 2009 ), Greengenes ( DeSantis  et al. , 2006 ), Rfam ( Gardner  et al. , 2011 ), NCBI ( Sayers  et al. , 2011 ) and HMP DACC ( The NIH HMP Working Group  et al. , 2009 ). To reduce the number of possibly misannotated entries, sequences were filtered by length to remove very short and long sequences and by genomic location to remove overlapping rRNA misannotations. The remaining sequences were then converted into DNA sequences (if required) and filtered for read duplicates to reduce redundancy in the sequence data. Detailed information for each reference database is provided on the website. Taxonomic information was either retrieved with the sequence data from the resources or was added based on the NCBI Taxonomy. The databases are automatically updated on a regular basis and can be requested from the authors for offline analysis. A non-redundant database is made available for the stand-alone version on the program website. 3 WEB-INTERFACE 3.1 Inputs The web interface allows the submission of compressed FASTA or FASTQ files to reduce the time of data upload. Uploaded data can be shared or accessed at a later point using unique data identifiers. It should be noted at this point that the input datasets should only contain quality-controlled, preprocessed sequences to ensure accurate results ( Schmieder and Edwards, 2011 ). In addition to the sequence data, the rRNA reference databases have to be selected from the list of available databases. Unlike the stand-alone version, the web-based program allows the user to define threshold parameters based on the results after the data are processed. This does not require an  a priori  knowledge of the best parameters for a given dataset and the parameter choice can be guided by the graphical visualizations. 3.2 Outputs Users can download the results in FASTA or FASTQ (if provided as input) format or its compressed version. Results will be stored for the time selected by the user (either 1 day or 1 week), if not otherwise requested, on the web server using a unique identifier displayed during data processing and on the result page. This identifier additionally allows users to share the result with other researchers. The current implementation offers several graphical and tabular outputs in addition to the processed sequence data. The Coverage versus Identity plot shows the number of matching reads for different coverage and identity threshold values. The coverage plots show where the metatranscriptomic sequences aligned to the rRNA reference sequences and provide an easy way to check for possible bias in the alignment or the rRNA-removal prior to sequencing. The coverage data for each database sequence is available for download. The taxonomic classifications of rRNA-like sequences are presented as bar charts for each selected database. The summary report includes information about the input data, selected databases and thresholds, and rRNA-like sequence classifications by database, domain and phyla. 4 BRIEF SURVEY OF ALTERNATIVE PROGRAMS There are different applications that can identify rRNA-like sequences in metatranscriptomic datasets. The command line program meta_rna ( Huang  et al. , 2009 ) is written in Python and identifies rRNA sequences based on HMMs using the HMMER package ( Eddy, 2009 ). Another program based on HMMER is rRNASelector ( Lee  et al. , 2011 ), which is written in Java and can only be used through its graphical interface. The web-based MG-RAST ( Meyer  et al. , 2008 ) uses the BLASTn program, identifying rRNA-like sequences based on sequence similarity. The HMM-based programs currently allow identification of bacterial and archaeal rRNAs. The sequence similarity-based programs make it easy to assign sequences to taxonomic groups. 5 CONCLUSION riboPicker allows scientists to efficiently remove rRNA-like sequences from their metatranscriptomic datasets prior to downstream analysis. The web interface is simple and user-friendly, and the stand-alone version allows offline analysis and integration into existing data processing pipelines. The tool provides a computational resource able to handle the amount of data that next-generation sequencers are capable of generating and can place the process more within reach of the average research lab. 
          Supplementary Material 
          
             
                Supplementary Data 
             
             
             
          
       </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Characterization of the N-ATPase, a distinct, laterally transferred Na+-translocating form of the bacterial F-type membrane ATPase</Title>
    <Doi>10.1093/bioinformatics/btq234</Doi>
    <Authors>Dibrova Daria V., Galperin Michael Y., Mulkidjanian Armen Y.</Authors>
    <Abstract>An analysis of the distribution of the Na+-translocating ATPases/ATP synthases among microbial genomes identified an atypical form of the F1Fo-type ATPase that is present in the archaea Methanosarcina barkeri and M.acetivorans, in a number of phylogenetically diverse marine and halotolerant bacteria and in pathogens Burkholderia spp. In complete genomes, representatives of this form (referred to here as N-ATPase) are always present as second copies, in addition to the typical proton-translocating ATP synthases. The N-ATPase is encoded by a highly conserved atpDCQRBEFAG operon and its subunits cluster separately from the equivalent subunits of the typical F-type ATPases. N-ATPase c subunits carry a full set of sodium-binding residues, indicating that most of these enzymes are Na+-translocating ATPases that likely confer on their hosts the ability to extrude Na+ ions. Other distinctive properties of the N-ATPase operons include the absence of the delta subunit from its cytoplasmic sector and the presence of two additional membrane subunits, AtpQ (formerly gene 1) and AtpR (formerly gene X). We argue that N-ATPases are an early-diverging branch of membrane ATPases that, similarly to the eukaryotic V-type ATPases, do not synthesize ATP.</Abstract>
    <Body>1 INTRODUCTION F 1 F o -type (F-type) and V/A-type ATPases are membrane-anchored rotary enzymes that couple translocation of H +  or Na +  ions across the membrane to the synthesis or hydrolysis of ATP. Although most of their subunits are evolutionarily related, the two classes of ATPases have clear differences in structure and phylogenetic distribution: F-type ATPases are found in bacteria, mitochondria and chloroplasts, whereas V/A-type ATPases are found in eukaryotic cell membranes (in particular, vacuoles), as well as in all archaea and in some bacteria (Hilario and Gogarten,  1998 ; Forgac,  2007 ; von Ballmoos  et al. ,  2008 ; Mulkidjanian  et al. ,  2009 ). V/A-type ATPases are often subdivided into two classes based (i) on the ability of the prokaryotic (A-type) enzyme to function in the direction of ATP synthesis and (ii) presence of additional subunits in the eukaryotic (V-type) enzyme that normally functions only as an ion-translocating ATPase, using the energy of ATP hydrolysis to acidify cellular compartments (Hilario and Gogarten,  1998 ; Forgac,  2007 ). Structural characterization of the Na + -binding sites in the  c -subunits of F- and V-type ATPases (Meier  et al. ,  2005 ; Murata  et al. ,  2005 ) allowed unequivocal assignment of the cation specificity for the membrane ATPases encoded in numerous sequenced genomes: only those  c -subunits that contain full sets of Na +  ligands appeared to transport Na + , the loss of at least one of those ligands correlated with the loss of Na +  specificity (Mulkidjanian  et al. ,  2008a ; Meier  et al. ,  2009 ). In the view of several reports on the sodium dependence of energy-coupled reactions in cyanobacteria (Willey  et al. ,  1987 ; Skulachev,  1988 ; Pogoryelov  et al. ,  2003 ), we have searched cyanobacterial genomes for the Na + -translocating ATPases. We found several apparently Na + -dependent cyanobacterial ATPases, but always as second copies, in addition to the H + -translocating ATP synthases. Here, we report the common properties of these ATPases, which are encoded in apparently highly mobile operons and have a set of specific traits that qualify them as a separate subfamily of the F-type ATPases. Since these ATPases, besides forming a  novel  subfamily, are always encoded  next to  the typical rotary ATPases and are predominantly  Na + - dependent , we refer to them hereafter as N-ATPases. 2 METHODS Phylogenetic distribution of the N-ATPase operons was deduced from protein BLAST (Altschul  et al. ,  1997 ) searches against the NCBI's RefSeq database (Pruitt  et al. ,  2009 ) (last searched February 1, 2010) and verified by examining gene neighborhoods of the retrieved ORFs and by checking for the presence of the N-ATPase-specific subunit AtpR. Phylogenetic trees were constructed using the neighbor-joining algorithm with MEGA (Kumar  et al. ,  2008 ). Multiple alignments were constructed from BLAST outputs with manual editing. Transmembrane segments were predicted using TMHMM (Krogh  et al. ,  2001 ). Sequence logos were drawn with WebLogo (Crooks  et al. ,  2004 ). 3 RESULTS Search of the NCBI's RefSeq database (Pruitt  et al. ,  2009 ) for cyanobacterial  c  (proteolipid) subunits that would have a full set of Na + -binding ligands (Mulkidjanian  et al. ,  2008a ; Meier  et al. ,  2009 ) returned five hits, all coming from marine cyanobacteria ( Supplementary Fig. S1 a). In each case, the operon encoding the Na + -binding  c  subunit was present in the genome along with another ATPase operon, encoding an H + -translocating F-type ATPase. As depicted below for  Synechococcus  sp. PCC 7002, an alignment of the Na + - and H + -binding  c -subunits (top and bottom, respectively) from the same cyanobacteria revealed a Glu substitution of the Gln residue (shown in blue) that serves as a Na +  ligand (uncharged residues of the transmembrane segments are highlighted in yellow, Na +  and H +  ligands are labeled with asterisks) Among the 4084 c -subunit sequences currently listed in the Pfam family ATP-synt_C (PF00137; Finn  et al. ,  2010 ), only 227 (5.5%) contain a Glu residue in that position and only ∼1% combine it with a typical ESTxxY Na + -binding motif ( Supplementary Fig. S1 ). The Na + -dependent ATPase operons in all cyanobacteria had a similar gene order, with a single gene insertion in  Acaryochloris marina  and a two-gene insertion in the two strains of  Cyanothece  sp. ( Supplementary Fig. S2 ). 3.1 Always the second: conservation and distinctive properties of the N-ATPase A search of the complete genome database identified homologous N-ATPase operons in some representatives of the bacterial phyla  Aquificae ,  Chlorobi  and  Planctomycetes , in certain members of α-, β-, γ- and δ-subdivisions of  Proteobacteria , and in two archaea,  Methanosarcina barkeri  and  M.acetivorans  (see  Supplementary Fig. S2 ). All these operons had the same  atpDCI - urf2 - atpBEFAG  gene order, encoding, respectively, β-, ε-, 1-, Urf2-, a-, c-, b−, α- and γ-subunits of this particular F-type ATPase, first described in  M.barkeri  by Sumi  et al.  ( 1997 ). The only exception outside of cyanobacteria was  Persephonella marina , a member of the  Aquificae , which has an N-ATPase operon with the  urf2 - atpBEFAGDCI  gene order and also harbors operons for F- and A-type ATPases ( Supplementary Fig. S2 ). Identification of the N-ATPase operons in diverse microorganisms was simplified by the fact that these operons are always present in the genomes as second copies alongside operons that encode typical H + -transporting F-type ATPases (in  M.barkeri  and  M.acetivorans , A-type ATPases). In contrast, most N-ATPase  c -subunits contain full sets of Na +  ligands, indicating that these enzymes are specific for Na +  ions. Just like the cyanobacterial Na + -binding  c -subunits,  c -subunits of most N-ATPases had Glu residues in the middle of both transmembrane helices ( Supplementary Fig. S1 ). The same subunits of the N-ATPases from diverse bacteria are closely related and form distinct branches on the phylogenetic trees, well separated from the equivalent subunits of the F- and V-type ATPases ( Fig. 1  and  Supplementary Fig. S3 , see also  Supplementary Fig. 5  in Swingley  et al. ,  2008 ). Phylogenetic trees built for individual N-ATPase subunits showed similar topologies, so that the whole N-ATPase operons appeared to co-evolve ( Supplementary Fig. S4 ).
 Fig. 1. A neighbor-joining tree comparing α-subunits of N-ATPases with α-subunits of F-type ATPases and B-subunits of A- and V-type ATPases. See  Supplementary Fig. S3  for the full tree. Another distinct trait of the N-ATPase operons was the presence of an extra gene,  urf2  (Sumi  et al. ,  1997 ). Its product is currently annotated as ‘F1/F0 ATPase,  Methanosarcina  type, subunit 2’ (F1F0_chp_2, TIGR03165, DH Haft, December 4, 2006) in the JCVI's Comprehensive Microbial Resource (Davidsen  et al. ,  2010 ) and as ‘ATPase_F1/F0-cplx_su2_Meth-typ’ (IPR017581) in the InterPro database (Hunter  et al. ,  2009 ), whereas UniProt uniformly annotates these proteins as ‘Putative uncharacterized protein’. The  urf2  (hereafter  atpR ) gene was found in every N-ATPase operon and could be used as a tell-tale sign of these operons. Its product is a hydrophobic protein with three predicted transmembrane segments, two of which carry conserved Arg residues ( Supplementary Fig. S5 ). Presence of charged residues in the hydrophobic core of the membrane is rare and usually indicative of a functional role. The N-ATPase operons also include so-called gene 1 (hereafter  atpQ ) that encodes another distinct membrane protein. Although this gene was originally marked as  atpI  (Sumi  et al. ,  1997 ) and is occasionally still labeled this way (Saum  et al. ,  2009 ), its product shows no statistically significant sequence similarity to the genuine AtpI (UncI) proteins, previously described in the genomes of  Escherichia coli  and other model organisms. In accordance with their distinct sequences, AtpI and AtpQ proteins are assigned to two different Pfam families, PF03899 and PF09527, respectively, and to two different COGs, COG3312 and COG5536 in the CDD database (Marchler-Bauer  et al. ,  2009 ). In contrast to  atpR , the  atpQ  gene is not limited to the N-ATPase operons. While the N-ATPase operon lacks the  atpH  gene that encodes the δ-subunit of the F-type ATPase, its  atpF  gene ( atpF N ) is unusually long (Sumi  et al. ,  1997 ). We were able to align C-terminal regions of the AtpF N  and AtpH gene products ( Supplementary Fig. S6 ). This showed that N-ATPase contains at least part of the δ-subunit, in agreement with the earlier analyses (Saum  et al. ,  2009 ). 3.2 Lateral transfer of N-ATPase genes Presence of the N-ATPase operon in the genomes of  M.barkeri  and  M.acetivorans , but not in the closely related  M.mazei , suggested that this operon had been acquired via lateral gene transfer. This suggestion is consistent with the gene neighborhoods of the  atpDCQRBEFG  operons in  M.barkeri  and  M.acetivorans  ( Fig. 2 ) and the absence of these operons in any other archaeal genomes sequenced so far. Gene neighborhoods of the N-ATPase operons in various bacteria are also consistent with the insertion of this operon ( Supplementary Fig. S7 ). The widespread presence of the N-ATPase genes among diverse bacteria deprecates the historical designation of these enzymes as ‘archaebacterial F 1 F o -ATPases’. The strict conservation of the gene order and co-linearity of the phylogenetic trees for distinct N-ATPase subunits suggests that the whole operon is being transferred as a single unit. However, the GC content of the N-ATPase operons shows a good correlation with the average GC content of the host genomes ( Supplementary Fig. S8 ), indicating either a relatively ancient gene transfer or a rapid adaptation of the N-ATPase genes to their host environment. An additional indication of the lateral mobility of the N-ATPase operon is its presence on plasmids, pREB4 in  A.marina  and pAQ7 in  Synechococcus  sp. PCC 7002. These data might be related to the earlier functional evidence of the presence of two ion-translocating ATPases in  M.mazei  Gö1. While the A-type ATP synthase was apparently H + -dependent, the second, F-type ATPase appeared to be Na + -translocating (Becher and Müller,  1994 ; Pisa  et al. ,  2007 ). Although this second ATPase has not been found in the sequenced genome of  M.mazei  ( Fig. 2 ), the respective genes could be plasmid-borne, as is the case of N-ATPases in at least two cyanobacteria.
 Fig. 2. Conserved gene neighborhoods in  M.acetivorans ,  M.mazei  and  M.barkeri , indicating the points of insertion of the N-ATPase operons in the former (top line, showing the subunit names) and the latter (bottom line, showing the gene names). Orthologous genes are indicated with the same colors. The arrows do not reflect the lengths of the genes. 4 DISCUSSION Following the description of an ‘archaebacterial F 1 F o -ATPase’ in  M.barkeri  (Sumi  et al. ,  1997 ), the presence of a ‘ Methanosarcina -like’ F-ATPase operon was repeatedly noted in bacterial genomes (Glöckner  et al. ,  2003 ; McInerney  et al. ,  2007 ; Swingley  et al. ,  2008 ), although the exact function(s) of these enzymes and their cation specificity remained obscure. McInerney  et al.  ( 2007 ) noted two F-type ATPases encoded in the genome of  Syntrophus aciditrophicus  and suggested that both of them were Na + -translocating (incidentally, the F-type ATPase of this organism is definitely H + -specific, whereas the cation specificity of its N-ATPase is unknown; it might be specific for Na + , see below). In their analysis of the  A.marina  genome, Swingley  et al.  ( 2008 ) noted the presence of a plasmid-encoded ‘set of ATP synthase genes that were arranged into a unique operon … conserved with full synteny in a remarkable array of organisms, including cyanobacteria, archaea, planctomycetes, chlorobi and proteobacteria’. The authors noted that the α-subunits encoded in these operons clustered together on a phylogenetic tree and suggested that these enzymes formed a separate new family of ATP synthases. However, they slightly overstated their case by claiming that its ‘individual proteins do not clearly fit into any of the described families’ (Swingley  et al. ,  2008 ). They also disputed the idea that these enzymes were Na + -translocating. In addition, the key observation by Daniel Haft that these enzymes always ‘represent a second F 1 /F o  ATPase system’ has only been published online in the JCVI's Comprehensive Microbial Resource (Davidsen  et al. ,  2010 ). As a result, there still exists a significant confusion as to the phylogenetic distribution, organization and the functional role(s) of these enzymes. The alignment in  Supplementary Fig. S1  shows that, despite the doubts of Swingley  et al.  ( 2008 ), the  c  subunit of the  A.marina  N-ATPase has a full set of Na + -binding ligands, including the recently recognized additional Thr residue of the ESTxxY motif (Mulkidjanian  et al. ,  2008a ; Meier  et al. ,  2009 ). While this residue is missing in  c -subunits of several N-ATPases, including the one from  S.aciditrophicus , a Glu residue is present instead of the Na + -coordinating Gln residue in the first transmembrane helix of the  c  subunit of nearly all N-ATPases ( Supplementary Fig. S1 ). As has been noted previously (Meier  et al. ,  2009 ; Saum  et al. ,  2009 ), this Glu residue could potentially provide two ligands for the Na +  ion and thereby complete the Na +  coordination shell. If so, all these N-ATPases would end up being capable of binding Na +  ions. A hallmark of the N-ATPase operons is the presence of the  atpR  gene. Because of the low dielectric permittivity of the membrane, the strategic positioning of two Arg residues of AtpR in the hydrophobic core of the membrane ( ) implies the presence of negatively charged residues in their vicinity. Given the absence in the N-ATPase operons of the  atpI  gene, whose product was recently shown to interact with the  c -ring (Suzuki  et al. ,  2007 ), we suggest that the product of the AtpR gene serves essentially the same function, regulating N-ATPase assembly and/or its activity. Just like AtpI assists  c -ring assembly by directly interacting with the  c -subunits (Suzuki  et al. ,  2007 ), AtpR could do that through the interaction of its two Arg residues with N-ATPase-specific  c -subunits, most of which carry two Glu residues in the middle of their transmembrane helices ( Supplementary Fig. S1 ). The observation that N-ATPases are always found alongside typical F- or A-type ATPases suggests that the N-ATPases cannot functionally replace those enzymes in their role as ATP synthases. Indeed, in  M.acetivorans , deletion of the N-ATPase operon had no visible effect on cell growth or ATP synthesis, whereas a mutant lacking the A-ATP synthase genes could not be obtained (Saum  et al. ,  2009 ). We conclude that, similarly to the eukaryotic V-ATPases, the N-ATPases do not catalyze ATP synthesis, which leaves ATP-driven ion pumping as the most plausible function for these enzymes. By analogy with V-ATPases, the N-ATPase  c -oligomer ring can be expected to consist of a smaller number of  c -subunits than the  c -oligomer ring of F-type ATP synthases. Acquisition of an operon capable of extrusion of Na +  ions would be beneficial to the marine bacteria and other organisms living in high-salt environments. Accordingly, many N-ATPase-encoding bacteria are either marine organisms or grow in the presence of salt. Since Na + -translocating ATPases can also translocate protons (von Ballmoos  et al. ,  2008 ), the N-ATPases could in principle function as outward proton pumps in low-sodium and/or acidic environments. A typical N-ATPase operon (with untranslated AtpR) has been found in an industrial strain of  Pseudomonas veronii  growing on 2-butanone (Onaca  et al. ,  2007 ). The presence of N-ATPase genes in such pathogens as  Burkholderia mallei  appears to be inherited from their free-living relatives and might be related to their survival in blood. Using a larger set of sequences than the one used by Swingley  et al.  ( 2008 ), we have confirmed that N-ATPases branch separately from other F-type ATPases ( Fig. 1 ,  Supplementary Fig. S3 ). This separate branching suggests a possible early divergence of the N-ATPases, which is compatible with the following, supposedly ancestral traits of these enzymes:
 Both AtpF N  subunit of the N-ATPase and the E subunit of the peripheral stalk of the V/A-type ATPases correspond to a fusion of b- and δ-subunits of the typical F-ATPases (Pallen  et al. ,  2006 ). The presence of the second membrane-embedded Glu residue is consistent with the evolution of the  c -subunit from a duplication of an amphiphilic helix that contained a Glu residue in the middle (Davis,  2002 ). Similar two-Glu  c -subunits are found in the A-type ATPases of methanogens and F-type ATPases of  Thermotogae . Outward pumping of Na +  ions, the predicted function of N-ATPases, appears to be an ancient trait and has been previously suggested as a function of the common ancestor of the F- and V-type ATPases (Mulkidjanian  et al. ,  2007 ;  2008a ,  b ;  2009 ). 
 All these features, which N-ATPases share either with V/A-ATPases—to the exclusion of most F-ATPases—or with the putative common ancestor of all rotary ATPases, suggest that N-ATPases represent a distinct early-diverging family of rotary ATPases. Thus, the Na-translocating common ancestor of all F-type ATPases apparently gave rise to two different families of ATPases: (i) the reversible ATPases/ATP synthases (‘genuine’ F-ATPases) and (ii) ATP-driven ion pumps (N-ATPases). In conclusion, the N-ATPases (until now usually referred to as ‘archaebacterial F 1 F o -ATPases’) are encoded in an apparently highly mobile operon that, most likely, confers on its hosts the ability of ATP-driven outward pumping of Na +  ions, which complements the H +  specificity of the native chromosome-encoded F-ATPase (or A-ATPase). We predict that, similarly to the eukaryotic V-ATPases, the N-ATPases do not catalyze ATP synthesis, which is why an N-ATPase is never found alone in a genome, only as the second enzyme in cells that already encode an F-type or a A-type ATP synthase. Accordingly, caution should be exercised when referring to their components as subunits of the ATP synthase. We believe that experimental verification of the predicted functions of the N-ATPases would be useful for the proper annotation of these interesting enzymes and could also help in understanding the evolution of energy conservation mechanisms. 
          Supplementary Material 
          
             
                [Supplementary Data] 
             
             
             
          
       </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>MicroRNA prediction with a novel ranking algorithm based on random walks</Title>
    <Doi>10.1093/bioinformatics/btn175</Doi>
    <Authors>Xu Yunpen, Zhou Xuefeng, Zhang Weixiong</Authors>
    <Abstract>MicroRNA (miRNAs) play essential roles in post-transcriptional gene regulation in animals and plants. Several existing computational approaches have been developed to complement experimental methods in discovery of miRNAs that express restrictively in specific environmental conditions or cell types. These computational methods require a sufficient number of characterized miRNAs as training samples, and rely on genome annotation to reduce the number of predicted putative miRNAs. However, most sequenced genomes have not been well annotated and many of them have a very few experimentally characterized miRNAs. As a result, the existing methods are not effective or even feasible for identifying miRNAs in these genomes.</Abstract>
    <Body>1 INTRODUCTION MicroRNAs (miRNAs) are endogenous single-stranded non-coding RNAs of ∼22 nt in length. They are derived from long precursors that fold into hairpin structures (Bartel,  2004 ; Jones-Rhoades  et al. ,  2006 ). miRNAs have been shown to play fundamentally important roles in animal and plant development (Bartel,  2004 ; Jones-Rhoades  et al. ,  2006 , in stress response in plants (Bari  et al. ,  2006 ; Chiou  et al. ,  2006 ; Jones-Rhoades and Bartel,  2004 ; Lu  et al. ,  2005 ; Sunkar and Zhu,  2004 ; Zhou  et al. ,  2007 ), and in genetic diseases including various types of cancer (Blenkiron  et al. ,  2007 ; Chen  et al. ,  2007 ; He  et al. ,  2007 ; Hobert,  2007 ; Ma  et al. ,  2007 ; Ozen  et al. ,  2008 ; Pedersen  et al. ,  2007 ; Tili  et al. ,  2007 ). In animals, most miRNAs bind to 3′ untranslated regions of their target mRNAs and repress the translation of the targets (Bartel,  2004 ). In contrast, in plants, most mature miRNAs directly base-pair with complementary sites in the coding regions of target mRNAs, resulting in cleavage or degradation of the targets (Bartel,  2004 ; Jones-Rhoades  et al. ,  2006 ). Identification of novel miRNA genes is an eminent and challenging problem towards the understanding of post-transcriptional gene regulation. Two major strategies for identifying novel miRNAs are experimental cloning and  in silico  prediction (Bartel,  2004 ; Berezikov  et al. ,  2006 ; Jones-Rhoades  et al. ,  2006 ). In a cloning-based approach, distinct ∼22 nt RNA transcripts were first isolated and then intensively cloned and sequenced. However, these methods are highly biased towards abundantly and/or ubiquitously expressed miRNAs; only abundant miRNA genes can be easily detected (Bartel,  2004 ; Berezikov  et al. ,  2006 ; Jones-Rhoades  et al. ,  2006 ). Note that not all miRNAs are well expressed in many tissues, cell types and developmental stages that have been tested (Bartel,  2004 ). Some miRNAs are expressed constitutively at low abundance, and some miRNAs have preferential or specific temporal and spatial expression patterns. Computational approaches have been developed to overcome some of the technical difficulties of experimental methods. First,  in silico  methods have been shown to be efficient for finding miRNAs that are expressed at constitutively low levels or in specific tissues (Bartel,  2004 ). Further, breakdown products of mRNA transcripts, other endogenous ncRNAs (e.g. tRNAs, rRNAs, nat-siRNAs) as well as exogenous siRNAs comprise a large portion of the non-coding small RNA population isolated from the cytoplasmic total RNA extracts (Bartel  2004 ; Berezikov  et al.   2006 ; Jones-Rhoades  et al.   2006 ; Lagos-Quintana  et al. ,  2001 ). To avoid erroneously designation of other non-coding small RNAs and even broken mRNA fragments as novel miRNAs, the secondary structures of flanking genomic sequences of cloned small RNAs are further assessed computationally. Only those small RNA sequences reside on arms of hairpin-structured precursors are likely to be miRNAs. However, due to their short length, cloned small RNAs may match to many genome regions that can potentially fold into hairpin structures which may not be unique to miRNAs. Thus, genome-wide screening of novel miRNA precursors is technically involved. Based on particular strategies adopted, the existing computational methods for miRNA prediction can be grouped into two categories. Methods in the first category were developed to find homologous miRNAs in  closely related  species. In these methods, known miRNA precursors were first folded into typical hairpin structures, local features in the hairpins were extracted, and extreme values of these featured were obtained from all known miRNAs. A filter was then constructed to screen novel hairpinned sequences. Those hairpinned sequences that passed the filter were further analyzed in related species to assess their evolutionary conservation (Berezikov  et al. ,  2005 ; Bonnet  et al. ,  2004 ; Grad  et al. ,  2003 ; Lai  et al. ,  2003 ). As they were designed for, these conservation-based methods built upon this framework were successful in detecting evolutionarilly conserved miRNAs. In order to further identify distantly homologous miRNAs, a probabilistic co-learning method using a paired hidden Markov model (HMM) was recently developed as a more general miRNA prediction method (Nam  et al. ,  2005 ). All these methods relied mainly on evolutionary conservation to eliminate a large number of false positive predictions. However, a substantial number of lineage- or species-specific miRNA genes do exist (Fahlgren  et al. ,  2007 ; Kasschau  et al. ,  2007 ; Lindow and Krogh,  2005 ; Molnar  et al. ,  2007 ; Rajagopalan  et al. ,  2006 ; Zhang  et al. ,  2006 ), which escape the detection of a conservation-based approach. In the second category, supervised classification methods, e.g. Support Vector Machines (SVMs), were adopted to train models based on positive sets of genuine miRNA precursors and negative sets of hairpins obtained from exon regions of protein coding genes (Hertel and Stadler,  2006 ; Ng and Mishra,  2007 ; Xue  et al. ,  2005 ). Thanks to the sequence features extracted from the training examples, such classification models were expected to preform well in predicting novel miRNAs from unseen sequences. However, many sequenced genomes are poorly annotated. Hence, it is difficult to obtain a good set of negative training samples for these methods. Moreover, miRNAs are located in intergenic regions and introns. A major task for miRNA prediction is to distinguish miRNA hairpins from other hairpin structures originated from intergenic or intronic sequences. In summary, classification-based methods are not effective on not well-annotated genomes. The methods in both categories discussed above all require a non-trivial set of positive samples of known miRNAs. Unfortunately, except well-studied model species such as human and  Caenorhabditis elegans , most sequenced genomes have a small number of miRNAs reported. For instance, only 38 miRNAs have been identified in the genome of  A.gambiae . This is particularly true for viral genomes for which not many miRNAs have been studied or reported (Stern-Ginossar  et al. ,  2007 ). Furthermore, the small number of known viral miRNAs rarely share any sequence homology (Stern-Ginossar  et al. ,  2007 ). Yet, viral miRNAs have been shown to play important roles in the pathology of viral infection by targeting host immune systems (Stern-Ginossar  et al. ,  2007 ). In short, the importance of miRNAs in post-transcriptional regulation, the lack of a sufficient number of known miRNAs and poorly annotated genomes collectively call for novel effective computational approaches to miRNA prediction. In this work, we propose and develop a novel ranking algorithm based on random works to computationally identify novel miRNAs from genomes with a few known miRNAs and with poor annotation. Our algorithm uses very few positive samples, requires no negative sample and does not rely on genome annotation. When applied to human data, our approach was able to identify known miRNAs with a relatively high precision and recall. As an application, we applied our algorithm to predict novel miRNAs in  A.gambiae , which is the most important vector of malaria in Africa and one of the most efficient malaria vectors in the world. Our analysis also showed that some of putative miRNAs encode mature miRNAs conserved in other species. 2 PROBLEM FORMULATION In our study, we cast the problem of miRNA prediction as a problem of information retrieval, in which novel miRNAs are to be retrieved from a pool of candidates by the known miRNAs as query samples. Specifically, we model this information retrieval process as belief propagation on a weighted graph, and develop a new ranking algorithm based on random walks. For a given species, the known miRNAs, putative candidate miRNAs and their relationship can be modeled by a weighted  graph   G =〈 V ,  E 〉. Each vertex  v ∈ V  in  G  represents a known miRNA precursor or a putative candidate, an edge  e ∈ E ⊆ V  ×  V  captures the relation between two vertices linked by the edge, and the  weight   w  of edge  e  quantifies the relation. In general, edge weights are determined by pairwise distances. For example, two closely related samples may share an edge with a large weight. The  degree  of vertex  v i  is  d i =∑ j   w ij , i.e. the total weight of all edges that are connected to  v i . Note that the graph is fully connected, so that there always exists a path between each pair of nodes in the graph. We refer the known miRNA precursors as  query samples  and putative candidates as  unknown samples . Consider query samples  X Q ={ x q 1 ,  x q 2 , …,  x qn } and unknown samples  X U ={ x u 1 ,  x u 2 , …,  x um }, where  x qi ,  x uj ,∈ℝ d . Our goal is to rank  X U  with respect to  X Q . To achieve this goal, we associate each sample  x i  with a relevancy value  f i , where  f i =1 for all query samples and  f i ∈[0, 1] for all unknown samples. A larger  f i  value means a higher relevancy of  x i  with respect to the queries. We then sort the relevancy values of all unknown samples and select the top ranked samples as retrieved samples, which constitute our predicted miRNA precursors. Therefore, the key to the ranking algorithm is to precisely compute the relevancy values of all unknown samples. In this study, we adopted the random walks method for this ranking problem, to be discussed next. 3 METHOD Query by samples is a paradigm for information retrieval in the information retrieval and machine learning fields. Zhou  et al.  proposed a manifold ranking method, which ranks the data with respect to the intrinsic manifold structure collectively revealed by the given data (Zhou  et al. ,  2004 ). Different from their work, our method uses Markov random walks to model a belief propagation process and therefore has a direct physical interpretation. 3.1 Ranking based on random walks Random walks is a classical stochastic process on a weighted finite state graph, which exploits the structure of the given data probabilistically. In the random walks formulation, each sample is treated as a graph vertex, which corresponds to a state on a Markov chain. The one-step transition probability  p ij  from vertex  v i  to vertex  v j  can be defined as
 (1) 
or written in the matrix form
 (2) 
 where  W =( w ij ) is the weight matrix,  D  is a diagonal matrix whose  i -th diagonal element is  d i . To facilitate our discussion, we reorder the vertices in a vector so that the query samples come first, followed by the unknown samples. We then partition the matrix as
 (3) 
 where  P QQ  is an  n  ×  n  transition matrix among the query states,  P QU  is an  n  ×  m  transition matrix from the query states to the unknown states,  P UQ  is a  m  ×  n  transition matrix from the unknown states to the query states and  P UU  is a  m  ×  m  transition matrix for the unknown states. Correspondingly, we partition the weight matrix  W  and the degree matrix  D  as
 (4) 
where  O  is a matrix with all 0. In our model, when a random walker transits from  v i  to  v j , it will transmit the current relevancy information of  v i  to  v j . This forms a dynamic process of relevancy information propagation: at each iterative step, a vertex transmits its relevancy information to its neighbors, and simultaneously receives the relevancy information from its neighbors. At the end of the iteration, each vertex updates its relevancy value according to the received information. Note that the query vertices only transmit their relevancy information and will never update their relevancy values. Furthermore, since query samples are more important than the unknown ones, the former are assigned higher weights for their relevancy information in transmission. This suggests the following relevancy updating rule,
 (5) 
where  k  is the iteration index, and α∈(0, 1) the weight of the relevancy from the unknown samples. In our method,  f i  is also called the ranking score of sample  i , which we use to rank the unknown samples. The matrix form of () is
 (6) 
By ( 2 ), () can be turned into
 (7) The convergence of the iterations is guaranteed by the following theorem. T HEOREM  1. The sequence     generated by the updating rule of  ()  converges when k approaches infinity . P ROOF . For convenience, let  , 𝔻=[0,1] m . Let  ,  . Then ( 7 ) can be written as
 (8) 
To finish our proof, we introduce a mapping  T  : 𝔻→𝔻 and the measure  d  on it as follows:
 (9) 
 (10) 
where  x i  is the  i -th element of vector  X . It is easy to show that (𝔻,  d ) is a complete metric space. According to the  Contractive Mapping Theorem of Banach  (Istratescu,  1981 ), it is suffice to prove that  T ( f ) is a contractive mapping, which holds if  R  satisfies
 (11) 
To prove ( 11 ), recall that 0 &lt; α &lt; 1, and
 (12) 
where  1 n =[1, 1, …, 1] T  is an  n -dimensional vector with all elements 1. It follows that
 (13) 
Therefore,  T ( f ) is a contractive mapping, and ( 7 ) converges to a unique fixed point.▪ Therefore, the limit of   and   can be substituted by  f U , resulting in
 (14) 
which is equivalent to
 (15) 
where  I  is an  m  ×  m  identity matrix. Therefore,
 (16) 
 3.2 The ranking algorithm The key to the above discussion and derivation is that we can compute the relevancy values of all unknown states according to ( 16 ) without actually performing the procedure of iterative random walks. Therefore, we propose the following ranking algorithm, which runs as follows:
 Step 1:  Construct graph  G =〈 V ,  E 〉. For each pair of vertices  x i  and  x j , we introduce an edge between them if they are close to each other. Step 2:  Measure graph weights  W . Here a heat kernel is adopted, i.e. if  x i  and  x j  are connected by an edge, the weight of the edge is defined as
 
where  d (·,·) is a distance measure defined on the graph, σ is the heat kernel parameter. Usually,  d ( i , j ) is the Euclidian distance between samples  i  and  j . Step 3:  Compute relevancy values of the samples by solving the matrix problem in equation ( 16 ). Step 4:  Rank the samples. Sort the relevancy values of all samples, and select some top ranked samples as the final result. For the problem of predicting new miRNAs, each sample is represented by a vector of 36 features ( Section 3.3 ), and distance between two samples is the Euclidean distance of the feature vectors. In order to reduce the data noise and computational expense, we make the graph sparse by removing the ‘weak’ edges with low weights. Since putative miRNAs are to be retrieved by known miRNAs according to the ranking scores of the candidates, we named our miRNA prediction method as  miRank . 3.3 Extraction of global and local sequence-structure features The most salient characteristics of miRNA precursors are their hairpinned secondary structures. In our study, we used RNAfold (Hofacker,  2003 ) to predict RNA secondary structures. We postulated that the entire hairpin structure of a miRNA precursor could be characterized solely by 36 global and local intrinsic attributes that capture sequence, structural and topological properties of the miRNA precursor. Four important global features at the structural and topological levels are the normalized minimum free energy of folding (MFE), the normalized base-pairing propensities (number of nucleotides that are paired) of both arms and the normalized loop length. Here the normalization factor is the length of the precursor sequence. Local features characterize various sequence properties in RNA sequences, which we discuss using an example. As shown in  Figure 1 , in the hairpin structure from RNAfold (Hofacker,  2003 ), each nucleotide in the sequence is either paired (indicated by a bracket, ‘(’ or ‘)’) or unpaired (denoted by a dot, ‘.’). The left bracket ‘(’ indicates that a nucleotide is located in the 5′ arm and the paired right bracket ‘)’ means the correspondingly paired nucleotide is in the 3′ arm. No evidence has shown that mature miRNAs have a preference of the 5′ or 3′ arms of their hairpinned precursors. Thus, it is not necessary to distinguish ‘(’ and ‘)’ in the local feature extraction; we use ‘(’ for both situations in the subsequent discussion. For any three adjacent nucleotides, there are eight (2 3 ) possible structure configurations: ‘(((’, ‘((.’, ‘(..’, ‘(.(’, ‘.((’, ‘.(.’, ‘..(’ and ‘·s’. We further differentiate each of these eight configurations by the nucleotide—A,C,G,U—in the middle position. Thus, there are 32 (4 × 8) possible sequence-structure configurations for each triplet in the precursors of miRNAs. Similar features have been used in some of the existing methods, e.g. that in Xue  et al.  ( 2005 ).
 Fig. 1. Extraction of local structure-sequence features. ( a ): the sequence and secondary structure of  hsa-mir-20a . ( b ): the configurations of 32 local sequence-structure features. Frequencies of the 32 features are obtained for each miRNA precursor or putative candidate, and further normalized by the length of the corresponding sequence. 
 4 EXPERIMENTAL EVALUATIONS AND DISCUSSIONS 4.1 Evaluation of the ranking algorithm on toy examples In order to show the belief propagation of our new ranking algorithm, along the manifold of the data, we generated a set of toy data, selected one data point as a query sample, and queried the rest samples to rank them. Figure 2  shows the results on two toy examples, where data are two-dimensional points distributed on X–Y plates. In each of these examples, we chose one point as the query, which is marked by a red plus. The rest points were treated as unlabeled data. In  Figure 1 , the relevancy values are color coded, i.e. a red indicates a high-relevancy value and a blue indicates a low-relevancy value. It can be seen that the method successfully propagates the query belief to the unknown samples so that the samples that are closer to the query have higher relevancy values.
 Fig. 2 Two toy examples of information propagation using the Markov random walks algorithm. Each dot represents a two-dimensional point distributed on an X-Y plate. In each figure, the dot marked by a red plus is the query. (See online version for the color figure). 
 4.2 Datasets for miRNA prediction All reported miRNAs precursor sequences of  H.sapiens  and  A.gambiae  (533 and 38, respectively) were downloaded from the miRBase (http://microrna.sanger.ac.uk/sequences/) (Griffiths-Jones  et al. ,  2006 ) as of September 1, 2007. Genome sequences of  H.sapiens  and  A.gambiae  were retrieved from UCSC Genome Browser (http://genome.ucsc.edu/). For the  H.sapiens  genome, we randomly extracted non-overlapping fragments of 90 nt from the genome so that no information of genome annotation was used. We first discarded all fragments overlapping with known miRNA precursors. For a fragment not overlapping with any known  H.sapiens  miRNA precursor, we further predicted its secondary structure using RNAfold (Hofacker,  2003 ). Nevertheless, not every such fragment would be kept for further analysis. The criteria for retaining hairpinned fragments are as follows: minimum 18 base pairings on the stem of the hairpin structure, maximum -12 kcal/mol free energy of the secondary structure and no multiple loops. The threshold 18 is the lowest number of base pairings, and the threshold -12 is the highest free energy among all genuine animal miRNA precursors. Finally, 1000 of such fragments were arbitrarily chosen and pooled together with some of the known human miRNA precursors—which were all known human miRNA precursors except the ones used as query samples in our experiments—to form the pool of candidates to be ranked by the miRank algorithm. The reason we added those known human miRNA precursors to this pool of samples is to evaluate the prediction performance of the miRank algorithm. Every chromosome of  A.gambiae  was fragmented, from 5′-end to 3′-end, using a sliding window of 90 nt and a shift increment of 45 nt. These fragments were folded using RNAfold (Hofacker,  2003 ), and hairpinned fragments were selected by the same criteria described above. The chosen hairpin sequences formed the initial candidate pool. In the fragmentation, some putative candidates might be cut into two pieces, and have lost their hairpin structures, hence were excluded from the candidate pool. To avoid this, we further fragmented, with the same sliding window and increment, the sequences between each pair of hairpinned fragments next to each other. The secondary structures of the new set of fragments were predicted and selected by the same tool and criteria. This process was iterated until no hairpinned fragment could be found. We obtained 22 297 hairpinned sequences, which included all 38 known miRNAs precursors for  A.gambiae . 4.3 Evaluation on human miRNAs We first evaluated the performance of our miRank algorithm on  H.sapiens  data based on the known miRNA precursors embedded in the pool of samples to be ranked ( Section 4.2 ). The prediction quality was assessed by the recall and the precision, which are, respectively, defined as:
 (17) 
 (18) 
where  TP ,  FP  and  FN  are numbers of true positive predictions, false positive predictions and false negative predictions, respectively. The number of query samples is the most critical parameter for algorithm miRank. We tested on  H. sapiens  data with 1, 5, 10, 15, 20 and 50 known miRNA precursors randomly chosen as query samples. To reiterate, in each of these experiments, the rest known miRNA precursors were combined with the 1000 hairpin sequences extracted from the genome to form the pool of candidates to be ranked ( Section 4.2 ). In each experiment, we chose  n  topmost ranked candidates, and determined the precision and recall of the result by comparing the chosen candidates with the known human miRNAs that were hidden in the candidate pool. By varying the number  n , we obtained the receiver operating characteristic (ROC) curves (Spackman,  1989 ) for the performance of miRank using different number of query samples, as shown in  Figure 3 .
 Fig. 3 Precision-recall curves obtained by setting different number of queries. All experiments were performed on human data. The curves from right to left show the results of experiments with query number equal to 1, 5, 10, 15, 20, 50 (See online version for the color figure). 
 In the extreme case of querying with only one sample, we obtained a precision greater than 70% in the 20 topmost retrieved samples. When we queried with more than 20 query samples, we obtained a high precision of over 95% in the 160 top ranked samples. These results suggest that we can expect miRank to be effective in predicting miRNAs on a species with a small number of reported miRNAs and a poorly annotated genome. Note that the precisions in these experiments could be under-estimated because genuine but unvalidated miRNA precursors may actually exist in the 1000 human hairpinned sequences that were analyzed in the experiment. Candidates with higher ranks are most likely to be true miRNAs; they are excellent candidates for further experimental validation. To obtain as many putative miRNAs with high confidence as possible, we need to set a ranking score cutoff.  Figure 4  shows the prediction precisions with respect to ranking score cutoffs in each of these six experiments. Evidently, each curve in  Figure 4  has an inflection point. The precise inflection points in  Figure 4  can be obtained by second-order differential. Notably, in the experiments with more query samples, the ranking scores of unknown sample tend to be higher, and prediction precisions by setting the scores at the inflection points as cutoffs are also improved. For each experiment, the score at the inflection point can be a reasonable cutoff that gives the largest number of predictions with a low false positive prediction rate. The second row in  Table 1  shows the recalls and precisions of these six experiments with scores at corresponding inflection points as cutoffs. In these experiments, the samples with ranking scores greater than the cutoffs at the inflection points of corresponding curves in  Figure 4  are about 25% of the sizes of the candidate pool.
 Fig. 4 Precision-cutoff score curves of six experiments on human data with different number of queries. 
 In order to further evaluate miRank, we compared it with some supervised classification algorithms (Hertel and Stadler,  2006 ; Ng and Mishra,  2007 ; Xue  et al. ,  2005 ), which are the best existing miRNA prediction algorithms, on the human data. Since these existing methods are Support Vector Machines (SVMs)-based classification models trained with more than a hundred known miRNA precursors, it is difficult to directly compare miRank with these methods. To overcome the difficulty, we followed the strategy in (Xue  et al. ,  2005 ), and trained six SVM-based models with 1, 5, 10, 15, 20 and 50 known miRNA precursors as positive training examples. We included twice as many negative samples as positive samples in the training sets as that gave the best results. All training examples and candidates used by the existing methods were represented by the same features as used by miRank. The results in  Table 1  show that miRank is more accurate than the SVM-based methods. With fewer known miRNA precursors, miRank outperforms them all. Essentially, miRank possesses the advantage of Semi-supervised Learning (Zhu,  2005 ), which incorporates the data distribution information of the unlabeled data in the training process. Therefore, miRank has a much better performance than SVMs when the number of training samples is small.
 Table 1. Test results on human data Name No. pos samples a 1 5 10 15 20 50 miRank Recall b 0.358 0.691 0.69 0.71 0.707 0.682 Precision c 0.64 0.75 0.805 0.845 0.86 0.939 SVM Recall d 0.68 0.621 0.709 0.715 0.719 0.720 Precision e 0.318 0.631 0.698 0.703 0.735 0.840 miRank Recall f 0.68 0.621 0.709 0.715 0.719 0.720 Precision g 0.429 0.765 0.795 0.836 0.853 0.888 a  Number of positive samples.  b , c  Recall and precision of miRank when the scores at the inflection points of the corresponding curves in  Figure 4  were set as cutoffs.  d , e  Recall and precision of the SVM-based methods.  f ,  g  Precisions of miRank were obtained by retrieving enough predictions to reach the same recalls as the SVM-based models. 4.4 Novel miRNA genes in  A.gambiae Anopheles gambiae  is one of the most important vectors of malaria in Africa, and one of the most efficient malaria vectors in the world. Identification of miRNAs is important for the study of the development of  A. gambiae , hence may broaden our perspectives on the control of the prevalence of malaria (Moffett  et al. ,  2007 ). To date, only 38 miRNAs of  A.gambiae  have been reported and curated in miRBase (Griffiths-Jones  et al. ,  2006 ), and its genome is not well annotated. As discussed in  Section 1 , the existing methods are not effective in predicting novel miRNAs in  A.gambiae . We took the 38 miRNAs curated in miRBase (Griffiths-Jones  et al. ,  2006 ) as query samples, and applied miRank to 22 259  A.gambiae  candidate sequences that have hairpinned secondary structures ( Section 4.2 ). Candidates with higher ranks are most likely to be true miRNAs, as indicated by  Figure 3  on the human data, and are thus suitable for further experimental validation. We took the top 200 candidates as our prediction. A further analysis showed that among these 200 candidates, 76 contain matured miRNAs that are conserved in at least one other animal species. Conservation across related species has been widely exploited for predicting miRNAs in animals and plants. Three major observations have been made on known conserved miRNAs (Reinhart  et al. ,  2002 ). First, the mature miRNA sequences are conserved, whereas the rest of the precursor sequences can be diverged. Second, the propensity of precursor sequences to form hairpinned secondary structures is conserved, although the actual structures may vary. Third, the conserved mature miRNA homologs are mostly located on the same arm of hairpinned secondary structures. In our research, we used hairpin structures as the most significant features to extract 22 259 candidates. We further analyzed the conservation of mature miRNAs of our top candidates.  Figure 5  shows the distribution of the conserved mature miRNAs in the top 500 candidates. As shown, the highly ranked candidates are more likely to be evolutionarilly conserved. Note that all homologs of these conserved candidates are located on the same arms of the hairpins in corresponding species.  Figure 6  shows two examples of novel putative miRNAs that we predicted. According to the IDs of their homologs in other species, we named these two miRNAs as aga-mir-135 and aga-mir-49.
 Fig. 5 Percentage of conserved miRNAs in different sets of putative candidates. X-axis shows the sets of putative candidates with different ranks. For instance, ‘200’ means that candidates in this set rank from 151 to 200. 
 Fig. 6. Two novel predicted miRNAs in  A.gambiae  which are conserved in more than one other animal species. According to the IDs of their homologs in other species, we named them  aga-mir-135 , and  aga-mir-49 , respectively. The precursor sequences of homologs in different species are shown and the mature miRNAs are highlighted and underscored.  mmu-mir-135b ,  rno-mir-135b ,  mdo-mir135b  and  hsa-mir-135b  are homologs of mir-135 in mouse, rat,  Monodelphis domestica  and human, respectively.  cel-mir-49  and  cbr-mir-49  are homologs of mir-49 in  C.elegans  and  C.briggsae . 
 5 CONCLUSIONS AND FINAL REMARKS In this study, we cast the problem of miRNA prediction as a problem of information retrieval where novel miRNAs were retrieved by the known miRNAs (as query samples) from a genome-scale pool of candidate sequences that can form hairpinned structures. We modeled the novel miRNA retrieval process by a process of belief propagation on a weighted graph, which was constructed from known miRNAs and candidate RNA sequences to be analyzed. We then developed a novel ranking algorithm based on random walks to propagate information of known miRNAs to candidates. We named our final miRNA prediction method based on ranking  miRank . The miRank method has the following remarkable properties. First, it does not require information of genome annotation. This is particularly important because many sequenced genomes have not been well annotated, and their closely related species are yet to be sequenced. Thus, a large number of false positive candidates with hairpinned secondary structures cannot be filtered out with genome annotation or by phylogenetic conservation. miRank can be applied to such newly sequenced genomes with little annotation. Second, it does not rely on cross-species conservation so that it can identify species-specific miRNAs. Third, miRank is able to accommodate a small number of known miRNAs while enjoys a high-prediction accuracy. Hence, miRank is a useful tool for many species including most viruses that have a very few reported miRNAs. Identification of novel miRNAs in various viral genomes will be our future research topic. To demonstrate these favorable features of the miRank algorithm, we tested it on human data where more than 530 miRNAs have been reported which we used to validate our result. miRank achieved a prediction accuracy of more than 95% using a small number of known miRNAs as labeled samples. We also applied miRank to  A.gambiae  to predict 200 novel miRNA precursors. Our result showed that 78 of the novel miRNA precursors encode mature miRNAs that are conserved in at least one other animal species. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>A geometric approach for classification and comparison of structural variants</Title>
    <Doi>10.1093/bioinformatics/btp208</Doi>
    <Authors>Sindi Suzanne, Helman Elena, Bashir Ali, Raphael Benjamin J.</Authors>
    <Abstract>Motivation: Structural variants, including duplications, insertions, deletions and inversions of large blocks of DNA sequence, are an important contributor to human genome variation. Measuring structural variants in a genome sequence is typically more challenging than measuring single nucleotide changes. Current approaches for structural variant identification, including paired-end DNA sequencing/mapping and array comparative genomic hybridization (aCGH), do not identify the boundaries of variants precisely. Consequently, most reported human structural variants are poorly defined and not readily compared across different studies and measurement techniques.</Abstract>
    <Body>1 INTRODUCTION Characterizing the DNA sequence differences that distinguish individuals is a major challenge in human genetics. Until recently, these differences were thought to be mostly single nucleotide changes. There is increasing appreciation for the prevalence of structural variation, including duplications, deletions and inversions of large blocks of DNA sequence, in the human genome (Sharp  et al. ,  2006 ). Structural variants have recently been linked to diseases such as autism (Marshall  et al. ,  2008 ), and cataloging these variants is an important step in determining the genetic basis of disease. Structural variants typically span thousands of nucleotides and are more difficult to define than single nucleotide polymorphisms (SNPs). Two techniques have been used to identify structural variants in the human genome:  array comparative genomic hybridization (aCGH)  and  end sequence profiling (ESP) , also called  paired-end mapping . Both of these techniques were first developed for the analysis of somatic structural rearrangements in cancer genomes (Pinkel and Albertson,  2005 ; Pinkel  et al. ,  1998 ; Volik  et al. ,  2003 ) and later applied to discover structural variation in normal genomes (Iafrate  et al. ,  2004 ; Kidd  et al. ,  2008 ; Korbel  et al. ,  2007 ; Sebat  et al. ,  2004 ; Tuzun  et al. ,  2005 ). In aCGH, differentially fluorescently labeled DNA from test and reference genomes are hybridized to an array of genomic probes derived from the reference genome. Measurements of test:reference fluorescence ratio at each probe identifies locations of the test genome that are present in higher or lower copy in the reference genome. This technique detects copy number variants but is blind to copy neutral variants such as inversions. In paired-end mapping approaches, DNA fragments, or clones, from a test genome are sequenced from both ends, and these sequences are mapped to a reference genome sequence. Pairs of end sequences, called  end sequence pairs  or  mate pairs , with discordant mappings identify inversions, translocations, transpositions, duplications and deletions that distinguish the test genome from the reference genome. Next-generation DNA sequencing technologies such as those from Illumina, Applied Biosystems and 454 Life Sciences now make it possible to apply this approach to a large number of individuals. We distinguish paired-end mapping from whole-genome assembly, which would provide the ultimate dataset for studies of structural variation (Levy  et al. ,  2007 ; Wheeler  et al. ,  2008 ), but remains cost prohibitive for a large number of human genomes. Both aCGH and paired-end mapping do not precisely identify the boundaries, or  breakpoints , of the measured variant. In aCGH, a breakpoint is localized only to the distance between the genomic probes straddling the copy number change, while in paired-end mapping the localization depends on the number and size of fragments that span the variant ( Fig. 1 A). There are no standard methods for identifying the boundaries of structural variants in paired-end mapping studies and different heuristics have been used (Kidd  et al. ,  2008 ; Korbel  et al. ,  2007 ). Remarkably, despite ambiguity resulting from both measurement and analysis, published studies of structural variants report the breakpoints to single nucleotide resolution without revealing the measurement uncertainty, or ‘error bars’, in the localization of the variant. Consequently, the existing databases of human structural variants (Iafrate  et al. ,  2004 ) also do not contain information on the uncertainty of the breakpoints.
 Fig. 1. Derivation of regions of uncertainty ( breakpoint regions ) from ESP and aCGH data. ( A ) (Top panel) In ESP, or paired-end mapping, both ends of a fragment of a test genome are sequenced and aligned to the reference genome. Here, alignment of ends of fragments  C  and  D  yields ES pairs ( x C ,  y C ) and ( x D ,  y D ) on the reference genome that suggest an inversion. (Bottom panel) The intersection of breakpoint regions defined by Equation ( 1 ) indicates the possible locations of inversion breakpoints  a  and  b  that are consistent with the ES pairs. ( B ) (Top panel) In an aCGH experiment, the reference genome is segmented into regions of equal copy number according to measurements at genomic probes (boxes). A deletion with breakpoints  a  and  b  is identified as a change in copy number between probes  p i  and  p i +1  and between probes  p j  and  p j +1 . (Bottom panel) The intervals [ p i ,  p i +1 ] and [ p j ,  p j +1 ] define a rectangular breakpoint region. This region is intersected with the breakpoint region defined by an ES pair ( x C ,  y C ) to refine the locations  a  and  b  of the deletion. Each new study of structural variation compares the newly discovered variants to those previously reported. In addition to the ambiguities described above, there is also the problem of deciding when two variants (perhaps measured via different approaches) are the same. The usual approach is to define two variants to be the same if they are ‘near’ each other, where near is defined using an arbitrary and study-dependent threshold. With such an approach, there is no assurance that the two variants are indeed the same, or are merely closely located on the genome. The situation is further exacerbated by reports that different human structural variants may overlap or have multiple states (Perry  et al. ,  2008 ; Scherer  et al. ,  2007 ), and that recurrent (but not identical) variants may exist at the same locus. Standard methods for defining structural variants and publicly available tools for comparing structural variants across different studies are urgently needed. Two recent works introduced more refined approaches for analysis of structural variants and are promising steps in this direction. Lee  et al.  ( 2008 ) describe a probabilistic method for resolving ambiguities in mapping end-sequenced fragments using the distribution of fragment lengths in a single sample. Bashir  et al.  ( 2008 ) estimate the probability that paired-end sequenced clones from cancer genomes contain fusion genes and explicitly incorporate the uncertainty in measurement of rearrangement breakpoint into their calculation. Neither of these approaches address the comparison of variants across multiple samples, and are further limited in their handling of measurement uncertainty and consideration of all classes of structural variants, respectively. Here, we introduce a general geometric framework for classification and comparison of structural variants. Our approach provides a principled way to cluster multiple measurements of a variant in a single sample and to compare variants across samples. We explicitly model the underlying measurement uncertainty of both paired-end mapping (from both older and next-generation sequencing technologies) and aCGH. We represent the uncertainty in the measurement of a structural variant, which we refer to as the  breakpoint region , as a polygon in the plane. We formulate the problems of comparing variants as computing all intersections and maximal intersections of breakpoint regions. These formulations allow the user to examine conserved variants at varying levels of granularity, instead of only producing a single best cluster of overlapping variants. We derive an efficient plane sweep algorithm from computational geometry to compute these intersections. We demonstrate our Geometric Analysis of Structural Variants (GASV) program with three applications. First, we apply our method to recent paired-end sequencing studies of nine human individuals. We show that GASV identifies rearrangement breakpoints with high precision. In dozens of cases, we localize rearrangement breakpoints to &lt;2.5 kb by combining the measurements from ≈40 kb clones across multiple individuals. In the most extreme example, eight end-sequenced clones from four different individuals localize the inversion breakpoints to within 286 bp. Such precise localization was not reported in the original published analysis of these nine individuals. Moreover, we show that the published locations of many variants are different from the breakpoints supported by the data. Second, we perform a comparative analysis of variants from the nine normal individuals with variants identified in paired-end sequencing of several cancer samples. We find that a significant fraction (5–53%) of rearrangements identified in the cancer genomes are consistent with inversion and deletion variants found in the normal genomes. Finally, we show how GASV integrates both aCGH and paired-end sequencing measurements of variants in three cancer genomes. Our geometric method for multi-sample and multi-platform identification and comparison of structural variants should prove useful for studies of human structural variation such as the 1000 Genomes Project and for cancer genome sequencing studies such as The Cancer Genome Atlas. 2 METHODS Consider a  reference genome  represented as a single interval  G  (i.e. we concatenate multiple chromosomes) and a closely related  test genome . We define a structural variant to be a difference between a  test genome  and  reference genome  that is due to a rearrangement resulting from DNA breakage followed by a aberrant repair or insertion of a new DNA. Structural variants include inversions, translocations, transpositions, and insertions/deletions. Each of these variants is thus associated with a set of breakpoints where DNA breaks and/or repair occurs. For example, an inversion is a result of the reference genome being cut at two genomic coordinates,  a  and  b , and the DNA segment between  a  and  b  flipped in the test genome so that the nucleotide at position  a  − 1 is adjacent to the nucleotide at position  b  and  a  is adjacent to  b  + 1 ( Fig. 1 A). Similarly, a deletion is defined by coordinates  a  and  b  in the reference such that  a  − 1 is joined to  b  + 1 in the test genome ( Fig. 1 B). Note that this is a simplification of the underlying biology, as there are sometimes small insertions or deletions at breakpoints, but these small changes have limited effect on the analysis of larger structural variants. 2.1 Breakpoint regions and variant uncertainty Neither paired-end mapping nor aCGH measure the breakpoints of a structural variant exactly. Rather, each technique localizes breakpoints to a region of the reference genome, which we refer to as the  breakpoint region . We describe the derivation of this region for each of these experimental techniques. 2.1.1 Paired-end mapping In the paired-end mapping, or ESP, fragments of genomic DNA from a test genome are sequenced from both ends, and the resulting pair of end sequences are aligned to the reference genome. We assume that each fragment 1   C  has ends that map uniquely to the reference genome. Thus, each fragment  C  corresponds to a pair of locations in the reference genome where the end sequences map. An end sequence may align to either DNA strand, and so each mapped end has a sign (+ or −) indicating the mapped strand. We call such a signed pair ( x C ,  y C ) an end sequence pair ( ES pair ), where by convention | x C |&lt;| y C |. Typically, the length of the fragment,  L C , is known to lie within a range [ L min ,  L max ]. Fragment sizes range from ≈150 kb for BAC clones to a few hundred base pairs for next-generation sequencing methods. We say that a ES pair is a  valid pair  (Raphael  et al. ,  2003 ) if the ends have opposite, convergent orientations and the distance between the mapped ends is within the range of fragment lengths: i.e. (+ x C , − y C ) is valid if  L min ≤| y |−| x |≤ L max . Otherwise, if the ends have abnormal distance or orientation, we say that the pair is an  invalid pair . Invalid pairs indicate putative genome rearrangements or possibly mapping/assembly errors. For concreteness, consider the case of a test genome that differs from the reference genome by a single inversion with breakpoints  a  and  b  ( Fig. 1 A) that fuse at a single coordinate ζ in the test genome. A fragment  C  from the test genome with length between  L min  and  L max  and containing ζ is end-sequenced. The resulting ES pair ( x C ,  y C ) will be an invalid pair indicating that  C  is not a contiguous piece of the reference genome ( Fig. 1 A). The invalid pair ( x C ,  y C ) does not uniquely identify the breakpoint ( a ,  b ). However, if we assume that: (i) only a  single  breakpoint is contained in the fragment  C ; and (ii)  a  &gt;  x C  and  b  &gt;  y C  (without loss of generality); then the length  L C  of  C  is equal to ( a  −  x C )+( b − y C ). Thus, a breakpoint ( a ,  b ) that is consistent with ( x C ,  y C ) must satisfy
 (1) 
We define the  breakpoint region B ( C ) of an invalid fragment  C  to be the breakpoints ( a ,  b ) satisfying the above equation. The constraint ( 1 ) has a straightforward geometric interpretation: if we plot an invalid pair ( x C ,  y C ) as a point in the 2D space  G  ×  G  then the breakpoint region defines a trapezoid ( Fig. 1 A). We emphasize that  a  and  b  cannot be chosen independently; doing so corresponds to defining the breakpoint region to be a rectangle, and allows breakpoints that give insert sizes outside the allowed range [ L min ,  L max ]. If another fragment  D  contains the same fusion point ζ, then the corresponding breakpoint ( a ,  b ) lies within the intersection  B ( C )∩ B ( D ) of the trapezoids  B ( C ) and  B ( D ) ( Fig. 1 A). Conversely, we will assume that if the trapezoids defined by several invalid pairs intersect, then they share a common breakpoint. As the number of fragments that are end-sequenced increases, more fragments will contain the same fusion point and the area of the intersection of breakpoint regions will decrease. Thus, the uncertainty in the location of the breakpoint ( a ,  b ) decreases. We define a  cluster  to be a set of fragments whose breakpoint regions have non-empty intersection. The description above generalizes to other types of structural variants including translocations, insertions, deletions and transpositions. For example, invalid pairs with sign( x C )=sign( y C )=− also indicate inversions (corresponding to the other fusion point), while invalid pairs with sign( x C )=+ and sign( y C )=− indicate insertions or deletions. Fragments with ends mapped to different chromosomes indicate translocations. As above, we assume that the fragment  C  contains only a  single  breakpoint. The breakpoints ( a , b ) that are consistent with the invalid pair ( x C ,  y C ) satisfy the inequalities
 (2) 
This equation generalizes ( 1 ) and is summarized by the rule: ‘end sequences point toward the breakpoint’. 2.1.2 Array comparative genomic hybridization In aCGH, a  breakpoint region  is defined as the genomic interval between the two adjacent probes  p i  and  p i +1  that define the endpoints of segments with unequal copy number ( Fig. 1 B). A pair of such breakpoint regions (e.g. those resulting from a deletion) give two intervals  U =[ p i ,  p i +1 ] and  V =[ p j , p j +1 ] that define a rectangle  U  ×  V  in 2D space  G  ×  G . This rectangle determines the locations of breakpoints ( a ,  b )∈ U  ×  V  consistent with the segmentation. Note that in addition to fragments that span deletions ( Fig. 1 B), the boundaries of aCGH segments often indicate the locations of other types of rearrangements including translocations (Aerni  et al. ,  2009 ; Campbell  et al. ,  2008 ). 2.2 Efficient computation of overlapping breakpoint regions Given a set  B 1 ,…,  B n  of breakpoint regions, our goal is to identify subsets of intersecting breakpoint regions. Such a subset suggests these breakpoint regions are multiple measurements of the same structural variant. In addition, we want to identify all such regions of intersection, and to label these by the breakpoint regions that are part of the intersection. We formalize these problems as follows:
 A ll  I ntersections of  B reakpoint  R egions .  Given a set  ℬ={ B 1 ,…,  B n }  of breakpoint regions ,  identify and label all non-empty intersections of subsets of  ℬ. 
 Since each breakpoint region  B i  is a convex polygon (trapezoid or rectangle), the solution to the above problem relies on computing intersections of convex polygons, a well-known problem in computational geometry (Preparata and Shamos,  1985 ). A naive brute-force approach that checks all 2 n  subsets of ℬ for intersection is very inefficient. Moreover, a single breakpoint region can have distinct intersections with different subsets of other breakpoint regions ( Fig. 2 ). Thus, it is not sufficient to consider only pairwise intersections or iteratively merge breakpoint regions to existing intersections. Below, we describe an efficient plane sweep algorithm that solves the ‘All Intersections Problem’.
 Fig. 2. Breakpoint regions determined by fragments from Kidd  et al.  ( 2008 ) whose orientations suggest an inversion variant(s). Breakpoint region 2 has distinct intersections with regions 1 and 3, and thus iterative merging of breakpoint regions will not identify all intersections. While the ‘All Intersections Problem’ provides the most comprehensive description of the overlaps between breakpoint regions, the output can be quite large since the number of regions of intersections grows rapidly as  n  increases. However, many of these regions of intersection are not interesting because they are dominated by intersections of a larger number of breakpoint regions. For example, if three breakpoint regions  B i ,  B j  and  B k  have a non-empty intersection, then reporting this intersection is perhaps more desirable than reporting the (geometrically larger) intersections  B i ∩ B j  and  B i ∩ B k , particularly as the number of such intersecting regions becomes large. Thus, it is desired to identify regions of intersection of a maximal number of  B i . We formalize this problem by defining a partial order on intersections of subsets of ℬ. For  S ⊆{1,…,  n }, let  B ∩ S =∩ s ∈ S B s  denote the intersection of the breakpoint regions indexed by  S . Let ℐ n  be the set of subsets of {1,…,  n } whose corresponding breakpoint regions have non-empty intersection. Formally,
 (3) 
ℐ n  has the natural partial order of subset inclusion ⊆, where for two elements  I  and  J  of ℐ n ,  I  ≺  J  provided  I ⊆ J . We denote this partially ordered set (poset) as (ℐ n , ⊆). We formalize the problem as follows. M aximal  I ntersections of  B reakpoint  R egions .  Given a set  ℬ={ B 1 ,…,  B n }  of breakpoint regions ,  identify all maximal elements of  (ℐ n , ⊆). Below, we describe how to solve the ‘Maximal Intersections Problem’ by extending the plane sweep algorithm for the ‘All Intersections Problem’. 2.3 Plane sweep algorithm The plane sweep algorithm was introduced by Shamos and Hoey ( 1976 ) for the problem of determining whether  n  line segments in the plane have any intersections. Clearly this question can be answered in  O ( n 2 ) time by checking all pairs of segments for intersection. A plane sweep algorithm performs the same task in  O ( n  log  n ) time by first sorting the segments by the  x -coordinate of their left endpoint, and then moving the line  x = c , called the  sweep line  through the plane from left to right. The efficiency of the plane sweep algorithm is derived from two observations. First, not all coordinates  c  need to be considered. A data structure called the event-point schedule ℰ records the necessary values of  c , and is updated dynamically as the sweep line moves from left to right. Second, for a given position  c  of the sweep line, the segments intersecting the sweep line can be ordered by the  y -coordinate of the intersection. These ordered segments are stored in a data structure called sweep-line status ℒ. Only adjacent segments in ℒ need to be examined for intersections. By employing appropriate data structures for ℰ and ℒ one obtains an efficient algorithm for segment intersection. Further details of this algorithm can be found in Preparata and Shamos ( 1985 ). The basic framework of the plane sweep algorithm has been extended to numerous related problems in computational geometry such as the counting of the  k  intersections of  n  segments in provably optimal  O ( n  log  n + k ) time (Chazelle and Edelsbrunner,  1992 ), and reporting the regions of intersection of polygons in the plane (Nievergelt and Preparata,  1982 ). Here, we modify the algorithm of Nievergelt and Preparata ( 1982 ) to solve the ‘All Intersections’ and ‘Maximal Intersection’ problems described above. Our extension exploits the particular geometry of the trapezoids and rectangles that define breakpoint regions in order to: (i) efficiently compute their intersection; (ii) label the intersecting regions by the breakpoint regions that are inside; and (iii) iteratively determine the maximal elements of (ℐ n , ⊆). 2.3.1 Overview of the algorithm We provide an overview of the plane sweep algorithm for the case of breakpoint regions defined by inversion variants; i.e. fragments with parallel orientations (+,+) or (−, −). These breakpoint regions are trapezoids with the two parallel sides having slope −1 ( Fig. 1 A). Thus, we define the sweep line to be a line  y =− x + c  of slope −1. The sweep line will encounter all inversion breakpoint regions as  c  increases from  c min  to  c max . The algorithm is identical for insertion/deletion variants except the sweep line is chosen to have slope +1,  y = x + c , to match the parallel sides of the trapezoids in this case ( Fig. 1 B). As the sweep line advances through the plane, one of three possible events ( Fig. 3 ) can occur: (i)  addition  of a breakpoint region; (ii)  intersection  between two line segments defining the boundaries of breakpoint regions; (iii)  removal  of a breakpoint region ( Fig. 3 ). Since the sweep line is parallel to the two sides of each trapezoid, it is only necessary to consider intersections between the horizontal/vertical sides of the trapezoid. For each breakpoint region  B  in ℬ, we define  B top  and  B bottom  as the horizontal/vertical sides of the trapezoid. We designate the side with the largest  y  value as ‘top’.
 Fig. 3. Examples of the three events of the plane sweep: ( A ) addition, ( B ) intersection and ( C ) removal. In each case black dots label the points recorded in the cyclic lists  a  and  b  (indicated as dashed paths) that form ℛ. In addition, we show in {}'s the labels assigned to the intersecting breakpoint regions. 2.3.2 Data structures As in the plane sweep algorithm for line segment intersection, we maintain two data structures ℰ and ℒ. We define the event point schedule ℰ as the list of positions for the sweep line. The event point schedule ℰ is initialized with the starts and ends of  B top  and  B bottom ; these correspond to the addition/removal events. ℰ is updated with new intersection points as they are discovered. For a given event point (position of the sweep line), the sweep line status structure ℒ stores an ordered list of the segments intersecting the sweep line and two terminal segments  y =±∞. ℒ is analogous to the same structure in the plane sweep algorithm for line segment intersection. In this case all line segments intersecting the sweep line are either horizontal or vertical edges of breakpoint regions. We first check for intersection between line segments that are adjacent in ℒ ( Fig. 3 B). If a non-empty intersection is computed, the intersection point is added to the event schedule ℰ. The regions of intersection are recorded using a third data structure ℛ introduced by Nievergelt and Preparata ( 1982 ). ℛ is attached to the sweep−line status ℒ and records the vertices of the regions of intersection encountered thus far on the sweep. For each segment  s  in ℒ, ℛ maintains two cyclic lists,  a ( s ) and  b ( s ), that contain the points on the boundaries of the regions above and below  s , respectively. Equivalently, if  s  and  t  are adjacent line segments in ℒ, let [ s ,  t ] denotes the region to the left of the sweep line and between  s  and  t . Then ℛ contains the vertices defining the boundary of [ s ,  t ]. We augment the ℛ structure of Nievergelt and Preparata ( 1982 ) with a label for each region of intersection. This label is the set of breakpoint regions that contain the region of intersection. For example, the region consisting of the non-empty intersection of breakpoint regions  B 1  and  B 2  is labeled {1, 2}. Finally, we maintain a interval tree (Preparata and Shamos,  1985 ) ℋ from which we derive the maximal elements of the poset (ℐ n , ⊆) encountered thus far on the sweep line. The algorithm (Algorithm 1) consists of iterating through the event point schedule and updating the regions of intersection found at each step according to whether the event point is an addition, removal, or intersection. This update is briefly described in the next section. 2.3.3 Computing regions of intersection The procedure  ProcessEvent  in Algorithm 1 updates the data structures, ℰ, ℒ, ℛ and ℋ according to the type of event. For a removal event,  ProcessEvent  ends the regions [ s ,  t ] for each pair of adjacent segments in ℒ by joining  b ( s ) and  a ( t ) with the points  p ( s ) and  p ( t ) where  s  and  t  intersect the sweep line ( Fig. 3 C). For an intersection event,  ProcessEvent  also swaps the order of  s  and  t  in ℒ ( Fig. 3 B). Further details of these operations are described in (Nievergelt and Preparata,  1982 ) and in the Supplementary Text (available at  http://www.cs.brown.edu/people/braphael/supplements/structvar ). Finally, each identified region of intersection is labeled by the constituent breakpoint regions. Region labels are represented as sets of breakpoint region names and are updated using the following procedure. If  s  and  t  are consecutive line segments along a sweep line, let  I ([ s ,  t ]) denote the label set of the region [ s ,  t ]. When processing an addition event of breakpoint region  i , new regions are introduced with labels  I ([ s ,  t ])∪ i . When a processing a removal event of breakpoint region  i , new regions are introduced with label  I ([ s ,  t ])\ i . Region labels also change during intersection events. When regions are completed, their labels and list of boundary vertices are inserted into the interval tree ℋ. Finally, all regions, or alternatively only maximal regions, are output as they are identified. 2.4 Extensions We briefly describe two natural extensions of our method. First, we include aCGH data. Second, we compute the probability that a paired-end sequenced fragment matches an existing structural variant. 2.4.1 Incorporating aCGH data As described above, the uncertainty in the breakpoints of a copy number change measured by aCGH is represented as a rectangle ( Fig. 3 B). The plane sweep algorithm is readily extended to include intersections with the rectangular breakpoint regions. 2.4.2 Incorporating fragment length distribution In most paired-end sequencing approaches, various procedures are used to select fragments of a specified size  L , with the resulting fragments having lengths distributed around this selected size. Thus far, we considered each fragment length between  L min  and  L max  to be equally likely. We can instead derive the empirical distribution  f ( L ) of the values | y | − | x | over all valid pairs ( x ,  y ) and use this distribution to better ascertain whether fragments provide evidence for a specific rearrangement. The breakpoint ( a ,  b ) defines a length  l C ( a ,  b )=(sign( x C ) a  −  x C )+(sign( y C ) b  −  y C ) for fragment  C . Given a polygon  P  defining the breakpoint region of a structural variant, we compute the probability that the invalid pair ( x C ,  y C ) is consistent with this variant as  . Note that this length is constant for all points ( a ,  b ) on the same line of slope −1 or 1, according to the orientation of the invalid pair (Bashir  et al. ,  2008 ). 3 RESULTS We implemented our geometric approach in a program called GASV. We applied GASV to: (i) analyze recent paired-end sequencing data of nine human individuals; (ii) perform a comparative analysis of genetic structural variants and those identified in paired-end sequencing of several cancer samples; and (iii) integrate data across measurement techniques by comparing variants identified by both aCGH and paired-end sequencing in cancer samples. 3.1 Paired-end sequencing of human structural variants We used GASV to analyze fosmid paired-end sequencing data from eight individuals from the HapMap populations (Kidd  et al. ,  2008 ) and another individual from an earlier study (Tuzun  et al. ,  2005 ). The Kidd  et al.  ( 2008 ) study reported a total of 224 inversion, 724 insertion and 747 deletion variants, which were validated by fingerprint analysis, clone sequencing or FISH. These studies are presently the most comprehensive, high-resolution survey of structural variants in the human genome. The mean insert sizes for the fosmid clones ranged from 36 kb to 41 kb with SD from 1.4 kb to 3.9 kb. In our analysis, we used  L min  = 20 kb and  L max  = 60 kb to provide a generous buffer for intersecting breakpoint regions. 3.1.1 Analysis of reported inversion variants We first analyzed the 180 validated inversions reported on the 22 autosomes in Kidd  et al.  ( 2008 ). We obtained the list of the boundaries of each inversion, the names of the clones that support each variant and the mapped coordinates of the end sequences. We used our geometric approach to compute the intersections of the breakpoint regions for each set of supporting clones, and we compared the reported boundaries of the inversions with the intersections we obtained. Surprisingly, 41/180 of the validated inversions had an empty intersection of breakpoint regions. That is, there were no candidate inversion breakpoints common to all of the reported supporting clones suggesting that the mapped clones are inconsistent with only a single inversion at the locus.  Figure 4 A shows an example of one such set, where multiple, distinct non-overlapping intersections are visible. One hypothesis is that the three distinct regions of intersection might represent slightly different breakpoints in different individuals. However, one individual contains clones from all three regions, suggesting that this genomic locus harbors a more complex rearrangement.
 Fig. 4. Geometric analysis of inversion polymorphisms from Kidd  et al.  ( 2008 ) reveals disparities between the reported boundary of variants (black dots) and the intersections of breakpoint regions. ( A ) An inversion on chr1 with 79 reported supporting clones from all nine individuals has no point in common to all breakpoint regions. The number  x  next to each of the three regions indicates a clone from individual labeled ABCx in Kidd  et al.  ( 2008 ) is present in the cluster; a ‘G’ indicates the G95 individual from Tuzun  et al.  ( 2005 ). The bottom right region contains clones from all nine individuals, while individual ABC13 has clones from all three regions suggesting multiple distinct structural variants or mapping difficulties at this locus. ( B ) An inversion from chr3 with 22 supporting clones from all eight HapMap individuals. We examined one fully sequenced clone (dashed trapezoid) from individual ABC7 and found two possible inversion breakpoints (black squares). Both of these lie in the intersection of all breakpoint regions but are ∼37 kb from the reported boundary. In the remaining 139 cases the reported boundaries were not in the region of intersection.  Figure 4 B shows one example where the reported coordinates for an inversion are clearly outside the region of intersection. In this case, Kidd  et al.  ( 2008 ) sequenced one of the clones in this cluster. We aligned this sequence to the reference genome and obtained two possible inversion breakpoints, both of which lie in the region of intersection computed by GASV. These two breakpoints could not be further resolved due to repetitive sequence near the inversion breakpoints. Analysis of additional sequenced clones from Kidd  et al.  ( 2008 ) showed a number of additional inversion breakpoints that occur within segmental duplications. Thus even with complete sequence data available, resolving the breakpoint with greater precision is challenging. The method used to derive the reported boundaries of the variants in Kidd  et al.  ( 2008 ) is mysterious. It is possible that the reported boundaries were intended to represent a ‘consensus’ of a single structural variant locus. For complicated loci with multiple, overlapping rearrangements ( Fig. 4 A), consensus coordinates might provide a reasonable summary of the data. However, we find that in many cases, the data allow us to refine the breakpoint region, and that significant information is lost when only a single pair of coordinates is reported for an inversion. 3.1.2 Analysis of intersecting breakpoint regions Using the complete set of mapped locations provided by Kidd  et al.  ( 2008 ), we computed the intersections of breakpoint regions for all nine individuals using GASV. In total, 30 853 clones on the 22 autosomes were consistent with an inversion. There were 1361 groups of intersecting breakpoint regions. Of these, 1200 had non-empty intersections, indicating that these clusters have a putative breakpoint in common for all of the clones. The remaining 161 groups had no breakpoints common to all the clones. Thus, over 10% the intersecting breakpoint regions suggest either complicated loci that are not easily explained as single inversion variants, or mapping/alignment artifacts. For the 1200 clusters with non-empty intersection, we computed the area of the intersection and defined the  localization  of a breakpoint as the square root of this area. Thus, if the region of intersection was a square, the localization would give the genomic range allowed for each breakpoint. There are 19 clusters with breakpoint localization &lt;2500 bp. In the best example, eight clones from four individuals localize the breakpoints to within 286 bp on each end (Supplementary Text). This is a remarkably small region of uncertainty, considering that a single fosmid clone localizes a breakpoint to ≈40 kb. We also found that breakpoint localization is not directly correlated with the number of clones in the breakpoint region. In 21/1200 cases the breakpoint region was supported by more than 50 clones. In only two of these cases was the breakpoint localization &lt;5000 bp. A possible reason for this discrepancy is the presence of repeats/duplications near the inversion breakpoints. These would lead to a relatively small genomic region where end sequences can be mapped uniquely. 3.1.3 Overlap of inversion and deletion variants We used GASV to compare the locations of inversions and deletions in the data of Kidd  et al.  ( 2008 ). We identified 5054 instances of intersection between inversion and deletion breakpoint regions.  Figure 5  shows a sample cluster containing 33 clones indicating an inversion and 4 clones indicating a deletion at the same locus. There are several examples where inversion heterozygotes lead to deletions in progeny (Stankiewicz and Lupski,  2002 ), a possible explanation for these overlapping variants. Alternatively, this overlap might suggest that these regions are unstable and subject to repeated rearrangement (Stankiewicz and Lupski,  2002 ).
 Fig. 5. Intersection of 33 inversion breakpoint regions (blue) and 4 deletion breakpoint regions (red), indicates common genomic location of two structural variants. 3.2 Cross-study comparison of structural variants Our geometric approach allows for the comparison of structural variants identified in different individuals with different measurement techniques. We tested this feature by comparing the genetic structural variants identified by Kidd  et al.  ( 2008 ) with variants identified in ESP studies of cancer genomes (Raphael  et al. ,  2008 ; Volik  et al. ,  2006 ). The later studies aimed to identify somatic, and possibly cancer-related, rearrangements in three breast cancer cell lines and five primary tumors from various tissues. The cancer ESP studies used large insert clones (BACs) with average sizes of 150 kb. We first identified clusters of invalid pairs in each cancer dataset that were suggestive of either inversions or deletions, and then computed the intersection of these clusters with the inversion and deletion clusters computed from the nine normal individuals. Approximately 5–53% of invalid clusters from the cancer clusters are consistent with inversion or deletion variants identified in normal individuals ( Table 1 ). The larger percentages are found in the primary tumor samples; this is consistent with the lower sequence coverage in the primary tumor samples, and the fact that tumor samples frequently contain significant admixture of normal cells resulting from difficulty of separating normal from tumor cells.
 Table 1. A comparison of the inversion and deletion variants identified in nine normal individuals (Kidd  et al. ,  2008 ; Tuzun  et al. ,  2005 ) and several cancer genomes (Raphael  et al. ,  2008 ; Volik  et al. ,  2006 ) Cancer No. of concordant No. of concordant sample inversions (%) deletions (%) MCF7 8 (5) 40 (28) BT474 12 (19) 8 (11) SKBR3 8 (13) 7 (11) Breast 11 (19) 21 (27) Breast 12 (32) 19 (38) Prostate 3 (9) 12 (27) Ovary 8 (53) 12 (29) Brain 2 (11) 10 (26) 
 We then clustered all the cancer data together with the nine normal individuals, and identified overlapping breakpoint regions containing at least two invalid pairs from different cancer samples. Of the 22 such clusters, 10 are consistent with inversion variants identified in at least one normal individual (9/10 cases were observed in at least four normal individuals), demonstrating that a large fraction of structural variants found in more than one cancer dataset are inherited genetic variants and not somatic rearrangements. 3.3 Comparing variants identified by paired-end sequencing and aCGH We used GASV to compare the breakpoints identified by ESP and aCGH for three cancer cell lines, MCF7, BT474 and SKBR3, using data from Volik  et al.  ( 2006 ), Raphael  et al.  ( 2008 ), and Aerni  et al.  ( 2009 ). We formed rectangles corresponding to pairs of copy number changes identified by segmentation ( Fig. 1 B) of aCGH data using CBS (Olshen  et al. ,  2004 ). We found that 35/152, 20/380 and 35/149 of the clusters defined from paired-end sequenced data intersected aCGH breakpoint regions in BT474, MCF7 and SKBR3, respectively.  Figure 6  shows an example of a cluster containing 19 breakpoint regions identified by ESP in the BT474 cell line, intersecting a breakpoint region determined by aCGH.
 Fig. 6. Intersection between six breakpoint regions from ESP data (blue trapezoids) and two breakpoint regions determined by aCGH (red rectangle) on chr17 in the BT474 breast cancer cell line. In this case, the spacing between aCGH probes provides a more precise localization of the breakpoint region that the paired-end sequencing data. 4 DISCUSSION We introduced GASV, a geometric approach for classification and comparison of structural variants. To our knowledge, this is the first comprehensive method for structural variant analysis across multiple samples that supports both paired-end sequencing data with arbitrary fragment sizes and aCGH with varying array resolutions. We illustrated the generality of our approach through several applications, including the clustering of variants from a paired-end sequencing study of nine individuals, the comparison of variants in normal and cancer genomes derived through different sequencing approaches, and the comparison of variants identified by aCGH and paired-end sequencing of the same cancer samples. In many cases we are able to localize the breakpoints of single variants, but in other cases the end-sequence pairs suggest more complicated variants. The precise localization of the boundaries of structural variants provided by the GASV is helpful for distinguishing simple variants shared across multiple individuals from more complex variants resulting from repeated rearrangements at the same locus. These results also demonstrate the importance of identifying and reporting the uncertainty in structural variant boundaries. The current convention of publishing approximate coordinates that were derived from study-specific heuristics can lead to unnecessary errors and misannotations of complicated variants. We expect that GASV will be useful for analyzing data from the 1000 Genomes Project and for cancer genome sequencing efforts that are part of The Cancer Genome Atlas. In the latter application, GASV will help distinguish genetic from somatic rearrangements. There are several directions for future work. First, it would be useful to perform a more comprehensive comparison of the variants that are identified by different measurement techniques. aCGH has limited power to detect variants whose breakpoints lie in repeat-rich regions of the genome due to the inability to identify probes in these regions. Paired-end sequencing approaches can be similarly limited, particularly if small fragment sizes are used, since the end sequences will not align uniquely to repeat-rich regions of the genome. Current studies of structural variants with next-generation sequencing technologies have used small fragment sizes from 200 bp (Campbell  et al. ,  2008 ) to 3 kb (Korbel  et al. ,  2007 ). An unresolved question is the optimal fragment size to use for studies of human structural variation. We have shown that clustering of breakpoint regions from relatively large clones (40 kb) with GASV can yield very precise localization of variant breakpoints (a few hundred base pairs). Kidd  et al.  ( 2008 ) reported that most of the clones that they sequenced had highly repetitive sequence at the breakpoints, complicating the precise breakpoint identification and assembly of the clone sequence. Thus, even in cases where complete sequence is available GASV can be used to record uncertainty in breakpoint location. An additional area of future work is to incorporate breakpoint uncertainty into databases of known structural variants. Our geometric approach could then be used to query this database and thus provide a more robust procedure for comparing newly discovered and existing variants. In addition, knowledge of existing structural variants can be used to guide mapping of end sequences that do not map uniquely to the reference genome. This is a common problem in human genome resequencing, where up to 60% percent of ES fragments are not used because of their ambiguous mappings (Korbel  et al. ,  2007 ). Lee  et al.  ( 2008 ) recently described a probabilistic model for resolving ambiguities that arise when mapping ES pairs in a single sample. Developing a model for multi-sample comparison that incorporates variant ambiguity across samples is a promising future endeavor. Finally, we focused exclusively on structural variation in the human genome, but such variation is also found in the mouse genome (Egan  et al. ,  2007 ) and other model organisms (Dopman and Hartl,  2007 ). Thus, there will continue to be an increasing demand for better analysis tools for structural variation. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Predicting small ligand binding sites in proteins using backbone structure</Title>
    <Doi>10.1093/bioinformatics/btn543</Doi>
    <Authors>Bordner Andrew J.</Authors>
    <Abstract>Motivation: Specific non-covalent binding of metal ions and ligands, such as nucleotides and cofactors, is essential for the function of many proteins. Computational methods are useful for predicting the location of such binding sites when experimental information is lacking. Methods that use structural information, when available, are particularly promising since they can potentially identify non-contiguous binding motifs that cannot be found using only the amino acid sequence. Furthermore, a prediction method that can utilize low-resolution models is advantageous because high-resolution structures are available for only a relatively small fraction of proteins.</Abstract>
    <Body>1 INTRODUCTION Many proteins rely on non-covalently bound metal ions or small molecules for their enzymatic function or regulation of their activity. The presence and location of these binding sites can therefore give useful clues for deducing the biochemical function of an uncharacterized protein. As the pace of protein sequence and structure determination quickens (Burley  et al. ,  2008 ), the assignment of protein function is becoming an increasingly important task. Computational methods can assist in this search by generating experimentally verifiable predictions of ligand binding sites in proteins. A number of successful methods have been developed for predicting ligand binding sites by finding characteristic sequence patterns (Andreini  et al. ,  2004 ; Passerini  et al. ,  2006 ; Shu  et al. ,  2008 ; Sigrist  et al. ,  2002 ). Such methods are particularly useful when there is no protein structure with detectable sequence similarity. Although ligands bind to residues that are localized in 3D space, the binding residues are generally not contiguous in the amino acid sequence. Because of this fact, methods that also utilize structure information, when it is available, are expected to perform better than those that use sequence information alone. Several distinctive properties of metal ion binding sites can be used for their prediction. Metal ion binding sites were found to have an outer shell of hydrophobic atomic groups that contains the inner shell of hydrophilic groups that coordinate the ion (Yamashita  et al. ,  1990 ). However, as pointed out in that study, this may simply be due to the fact that the coordinating atoms are covalently bound to hydrophobic carbon-containing groups in the protein rather than favorable enthalpic or entropic contributions to binding. It was also observed that metal binding sites often correspond to statistically significant clusters of negatively charged residues (Zhu and Karlin,  1996 ). Furthermore, divalent metal cations have characteristic preferences for coordinating groups (backbone carbonyl, specific side-chain groups or water molecules) and coordination number (Dudev and Lim,  2003 ; Harding,  2004 ). Finally, binding site residues are usually conserved in orthologous proteins, presumably because of their crucial role in the protein's function. One early prediction method used the total electrostatic valence of oxygen ligands, calculated from the inter-atomic distances, in order to predict calcium ion binding sites (Nayal and Di Cera,  1994 ). Another study used the FEATURE method (Bagley and Altman,  1995 ), which calculates properties in concentric shells around a potential binding site, with a Bayesian score to predict calcium ion binding sites (Wei and Altman,  1998 ). A later study used a similar method to predict zinc ion binding sites and demonstrated its applicability to unbound structures (Ebert and Altman,  2008 ). Also properties of clustered triplets of residue types that commonly coordinate zinc ions were successfully used to predict transition metal ion binding sites in apo protein structures (Babor  et al. ,  2008 ). Binding sites for several different metal ions were also identified using the Fold-X force field applied to the atomic structure of the protein (Schymkowitz  et al. ,  2005 ). Finally, the method of Sodhi  et al.  ( 2004 ) used neural networks trained on PSI-Blast position-specific scoring matrices (PSSMs), secondary structure states, solvent accessible surface area (SASA) and the inter-residue distance matrix for neighboring residues to predict different metal ion binding sites. The prediction of small molecule binding sites is facilitated by their tendency to bind in pockets on the surface of the protein, due to the requirement of forming sufficient energetically favorable contacts. A variety of algorithms that locate such surface pockets have been developed (An  et al. ,  2005 ; Harris  et al. ,  2008 ; Laurie and Jackson,  2005 ). Many structure-based binding site prediction methods use predicted pockets. One study combined pocket prediction with evolutionary conservation in order to predict binding sites for an arbitrary small molecule (Huang and Schroeder,  2006 ). Another study (Glaser  et al. ,  2006 ), used an optimized pocket finding algorithm along with conservation to locate pockets that bind ligands. The study of Burgoyne and Jackson ( 2006 ) compared different properties for ligand binding prediction and found that the total electrostatic potential, desolvation energy and conservation were the best. One challenge with such methods for predicting general ligand binding sites, irrespective of the identity of the ligand, is that it is difficult to compile a reliable negative dataset of pockets that do not bind any small molecule. Two papers, Guo  et al.  ( 2005 ) and Nebel  et al.  ( 2007 ), described methods that use 3D motifs to predict binding sites for adenosine triphosphate (ATP) and adenine-based ligands, respectively. Finally, Brylinski and Skolnick ( 2008 ) employed threading of the query sequence to identify similar structures in the PDB with bound ligands. We have developed the SitePredict ligand binding site prediction method that can be applied, with minor modification, to both metal ion and small molecule binding sites. The primary difference is that only surface pockets are considered for small molecule binding sites whereas clusters of residues throughout the protein are considered as potential metal ion binding sites. A machine learning method, Random Forests, is used to make a prediction for a specific ligand based on a combination of diverse properties including evolutionary conservation, median SASA, counts of nearby residue pairs and statistically significant clustering of residue types in the site. This method was motivated by the success of a similar approach for predicting protein–protein interfaces (Bordner and Abagyan,  2005 ). Unlike many previous approaches, SitePredict does not rely on the exact positions of side chains contacting the ligand. This is critical since actual predictions are performed on unbound structures and the protein structure undergoes conformational changes upon binding. In fact, except for the median SASA, which is relatively insensitive, there is no dependence on side-chain conformations at all. This means that it can be applied to unrefined homology models or low-resolution structures. The method's performance was evaluated using cross-validation on comprehensive non-redundant sets of both holo and apo protein structures. Also the relative contribution of the different site properties to the prediction accuracy revealed specific determinants of binding for different ligands. Binding site predictions were made for all protein structures in the PDB and are available for searching or download at  http://sitepredict.org/ . Also, as a demonstration of the method's utility in functional annotation, novel binding sites in proteins of unknown function whose structures were solved by structural genomics projects were examined and several examples which corroborate independent functional assignments are discussed. Finally, SitePredict was applied to predict ligand binding sites in homology models of human proteins. 1.1 Non-redundant sets of protein structures and ligand binding sites The data used to train the machine learning classifier and assess its prediction performance was derived from a non-redundant set of X-ray structures created for each ligand. Each set was generated by first clustering all protein chains in X-ray structures from the wwPDB database (Berman  et al. ,  2003 ) at 25% sequence identity using the CD-HIT program (Li and Godzik,  2006 ). Next, each cluster was examined and a ligand-bound protein, if present, was selected and otherwise a protein without the bound ligand was selected. If multiple chains were found in a cluster then the highest resolution chain without breaks was chosen. This procedure resulted in a non-redundant set of proteins containing the largest number of ligand-bound structures. This is important for creating a good benchmark set with an adequate number of diverse ligand binding sites, since most proteins in the structure database do not have the particular ligand bound. Although the methodology may be easily extended to include the prediction of ligands that interact with multiple protein molecules, for simplicity we only considered structures in which the ligand interacts with a single protein molecule. 1.2 Overview of the SitePredict method Small molecules usually bind in pockets on the protein surface where they can form sufficient energetically favorable interactions with the protein. Furthermore these pockets usually remain even in the apo structure (An  et al. ,  2005 ). Metal ions are also coordinated by multiple chemical groups (Harding,  2004 ), however they do not bind to pre-existing pockets in the surface because of their small size. Because of this difference in binding proclivities, two different prediction procedures are employed for each class of ligand. For small molecules, only the binding pockets are considered as potential binding sites and a prediction is made for each pocket as to whether or not it binds a particular ligand, based on the properties of the pocket. In contrast, for metal ions, the predictions are made for approximately spherical regions of the protein, each comprised of a cluster of a fixed number of neighboring residues. The prediction aims to identify the residue clusters that significantly overlap with a binding site for the metal ion of interest. 1.3 Residue cluster properties used for metal ion binding site prediction The overlapping residue clusters were defined by starting with each of the residues in the query protein as a central residue and adding the nine nearest residues to form 10-residue clusters. Increasing the size of the clusters did not significantly improve prediction performance (data not shown). The fractional overlap of a cluster with the binding site, which varies between 0 and 1, was defined by the ratio:
 
in which the numerator is the number of residues in common between the cluster and the binding site and the denominator is the least of either the number of cluster residues or the number of binding site residues. Clusters with an overlap fraction &gt;0.5 were considered as positive instances, i.e. ligand binding site regions, whereas the remainder were considered as negative instances, i.e. non-binding site regions. The following properties were calculated for each residue cluster: nearby residue pair counts, number of nearby backbone O atom pairs, residue propensity log  P -values, evolutionary conservation log  P -value and median-relative SASA. The residue pair counts were calculated by counting the total number of nearby cluster residue pairs of each residue type. Because there are 20 different standard residue types, the number of unordered pairs of residue types, and hence the number of residue pair counts, is 210. Nearby residue pairs were required to have C β  atom, or C α  atom for glycine, separation &lt;10 Å. The number of nearby backbone O atom pairs was calculated by first creating a graph in which nodes represent O atoms and are connected by an edge if the corresponding atoms are separated by &lt;5 Å and then counting the number of nodes in the maximal clique. This procedure efficiently locates backbone O atom clusters. The residue propensity  P -values were calculated for each of the 20 residue types by the statistical significance of observing as many residues of that type in the cluster versus what would be expected from a random arrangement of the protein's residues. The random probability is described by the hypergeometric cumulative distribution function as
 
in which  N i cluster  is the number of residues of type  i  in the cluster and  N i total  ( N j ≠ i total ) is the number of residues of type  i  (not of type  i ) in the protein. The log  P -value is expected to be more robust than raw residue-type counts, since it accounts for the different distributions of residue types occurring in distinct proteins. In other words, the log  P -value is high only if the fraction of residues of a particular type in a cluster is considerably higher than the fraction of the same residue type among all residues in the protein. 1.4 Pocket properties used for small molecule binding site prediction As mentioned above, only surface pockets on the protein surface were considered as potential binding sites for small molecules. Also pockets that are too small to contain the ligands, with volumes &lt;200 Å 3 , were excluded. Surface pockets were calculated using the PocketFinder algorithm (An  et al. ,  2005 ) as implemented in the ICM program version 3.5 (Molsoft LLC). Properties were calculated for the set of residues lining each pocket, which are within 4 Å of the calculated pocket surface. Properties that describe the size and shape of the surface pocket as well as some of the same properties used for the metal ion site prediction were used for small molecule site prediction. The following properties of each pocket were used for the prediction of small molecule binding sites: nearby residue pair counts, residue propensity log  P -values, evolutionary conservation log  P -value, pocket volume and pocket principal components. The properties not involving the pocket shape are the same as those defined for metal ion sites and described in  Section 1.3 , except that they were calculated using the set of pocket residues. The three principal components are the axis lengths of an ellipsoid that approximates the pocket boundary. They conveniently summarize the generally complicated shape of the pocket in a few numbers. 1.5 Random Forest classifiers Binding site predictions are made using a supervised learning method called Random Forests (Breiman,  2001 ). The Random Forest method has many advantages compared with other machine learning methods including: high accuracy, speed, resistance to overfitting, the ability to use heterogeneous training data without rescaling, estimation of the generalization error during training, and the ability to estimate the contribution of each variable to the overall prediction accuracy. A Random Forest is a collection of classification trees that are randomized by training on a bootstrap sample of the training data and also using only a subset of M (&lt; N ) of the variables. A prediction is made based on the fraction of trees selecting each class. In this application the two classes are binding site and non-binding site. A cutoff is chosen and if the fraction of trees predicting the site to be a binding site is higher than the cutoff then the overall prediction is binding site, otherwise the site is predicted to be a non-binding site. Because of bootstrap sampling, approximately one-third of the data samples are not used for training each tree. The importance of each variable to the prediction performance can be assessed by calculating the decrease in prediction accuracy for this so-called out-of-bag data upon permuting values for the variable. 1.6 Evaluation of prediction performance The machine learning method requires a sufficient number of independent examples of known binding sites for a ligand in order to evaluate the prediction performance and avoid potential overfitting. Only ligands with least 40 independent binding sites in the non-redundant set of protein–ligand structures, described in  Section 1.1 , were considered. The following metal ions fulfilled this criterion: Ca, Cu, Fe, Mg, Mn and Zn. Fe ions annotated in both oxidation states Fe(III) and Fe(II), with PDB heterocompound codes FE and FE2, respectively, were grouped together. Because many small molecules are converted into another molecule by the enzymatic action of the protein in the binding pocket, both the reactant and product were considered together as a group. For example, ATP is often hydrolyzed to adenosine diphosphate (ADP) so that these two molecules are in one group. Also, structures of protein–ligand complexes are often solved with non-hydrolyzable analogs bound so that these structures are also included within the same group. Datasets were compiled for the following groups of small molecules:
 adenosine monophosphate (AMP), ATP, ADP and analogs (ACP, ANP, ATS, SAP and TAT), flavin-adenine dinucleotide (FAD), heme (HEM and HEB), nicotinamide adenine dinucleotide (NAD) and derivatives (NAP, NDP and NAI). 
The PDB heterocompound ID, which coincides with the usual abbreviations for some of the compounds, are shown in parentheses.  Table 1  shows the total number of proteins and sites for each ligand.
 Table 1. Number of ligand binding proteins and sites in the non-redundant datasets for each ligand Ligand Number of proteins in the training set Number of sites in the training set Ca 273(355) 434(536) Cu 33(45) 51(67) Fe 84(95) 105(123) Mg 433(479) 549(575) Mn 148(172) 232(239) Zn 401(541) 517(687) AMP 48(71) 48(83) ATP, ADP+analogs 172(269) 173(299) FAD 52(85) 52(105) Heme 53(123) 57(208) NAD, NADP, NADPH 107(170) 107(199) The values in parentheses are the numbers of proteins or sites before removing sites
that contact multiple proteins. Random Forest input datasets, which contain all of the properties described above, were then generated for each metal ion or group of small molecules. Missing conservation values, due to an insufficient number of related protein sequences (&lt;20), were imputed as the median of all conservation values in the input dataset. This procedure results in the assignment of a neutral value, which does not bias the prediction, for evolutionary conservation in examples with missing data. Because there are many more negative examples (non-binding sites) in the data than positive examples (binding site residue cluster or pocket) it was necessary to randomly select only a subset of the negative data to obtain a balanced dataset. This is important since training on highly unbalanced data gives a predictor that is accurate only for the majority class. Datasets with twice as many negative as positive examples were used because they gave slightly better performance than evenly balanced (1:1 ratio) datasets (data not shown). All positive examples were included in the datasets. To evaluate the performance of the Random Forest prediction, 10-fold cross-validation was used. The cross-validation sets were constructed so that the corresponding training and test sets contain data for proteins from different Pfam families (Finn  et al. ,  2006 ). This insures the independence of the cross-validation sets, which is important for an accurate assessment of the actual prediction performance. The overall cross-validation prediction performance was summarized by the total area under the receiver operating characteristic (ROC) curve, which plots sensitivity versus (1 - specificity). The ROC curves were generated by varying the Random Forest score cutoff. The area under the curve (AUC) can vary from 0.0 to 1.0. A random prediction is expected to yield an AUC of 0.5 and the closer the AUC value is to 1.0 the more accurate the prediction is. Furthermore, the AUC is proportional to the Wilcoxon rank-sum statistic and so can be associated with a  P -value for discriminating the two classes. 1.7 Class likelihood ratio for prediction confidence The Random Forest score, defined as the fraction of trees voting for the positive class, varies from 0.0 to 1.0. Potential sites with high scores, near 1.0, are expected to be more confidently predicted as binding sites than those with lower scores. Likewise, potential sites with low scores, near 0.0, are expected to be more confidently predicted as non-binding sites than those with higher scores. This implies that the Random Forest score contains more useful information than simply whether it is above or below the binary classification cutoff. The confidence of each prediction was estimated as a class likelihood ratio calculated from class-dependent score distributions using a method similar to that described for support vector machine classification in Bordner and Abagyan ( 2005 ). First, the distributions of cross-validation prediction scores for negative and positive examples were estimated using kernel density estimation as implemented in R (R Development Core Team,  2008 ). The likelihood ratio  R (score) was then calculated as the ratio of the distributions, i.e.
 
A high value for  R  (much greater than the fraction of all residues expected to bind the ligand) indicates that the site is confidently predicted as a binding site for the ligand, a low value indicates that the site is confidently predicted not to be a binding site for the ligand, and an intermediate value indicates an ambiguous prediction. The likelihood ratio is useful for prioritizing predictions for experimental validation. Residue-level scores and likelihood ratios for metal ion site predictions were calculated as the median values for all residue clusters containing a particular residue. 2 RESULTS 2.1 Prediction accuracy The accuracy of the SitePredict method in predicting binding sites for different ligands, as assessed by the 10-fold cross-validation AUC values, is shown in  Table 2 . Results using both ligand-bound and ligand-unbound structures are given.
 Table 2. Area under the ROC curve for 10-fold cross-validation prediction of ligand binding sites using ligand-bound and ligand-unbound protein structures AUC Ligand Bound structure Only top 20 variables (bound structure) Without conservation (bound structure) Unbound structure Ca 0.861 0.850 0.856 0.813 Cu 0.952 0.784 0.952 Fe 0.960 0.948 0.953 Mg 0.823 0.809 0.794 0.763 Mn 0.897 0.884 0.879 0.895 Zn 0.964 0.958 0.958 0.913 AMP 0.799 0.842 0.797 ATP, ADP+analogs 0.884 0.901 0.852 0.836 FAD 0.941 0.928 0.941 Heme 0.971 0.955 0.971 NAD, NADP, 0.959 0.959 0.959 0.876 NADPH AUC values are also given for predictions using only the top 20 most important variables and without evolutionary conservation. The transferability and generality of SitePredict is demonstrated by the high AUC values for the cross-validation prediction because the prediction is made for proteins in different Pfam families than those used for training the Random Forest classifier. In other words, high cross-validation AUC values imply that the method is expected to perform well on proteins dissimilar to those used for training, such as those without any known binding sites. Table 1  also shows the cross-validation AUC values for binding site predictions without using evolutionary conservation. There is only a relatively small decrease in prediction performance for all ligands. This is advantageous since the evolutionary conservation could not be calculated for about 20% of the proteins due to a lack of similar protein sequences. This shows that the accuracy does not appreciably degrade for such proteins. Other methods that rely on a multiple sequence alignment through, e.g. PSSMs, cannot be applied to the significant fraction of proteins with few or no known orthologs. The prediction results for all PDB structures are available at the SitePredict website ( http://sitepredict.org/ ). Results for individual PDB entries can be retrieved and visualized in 3D using the Jmol viewer (Jmol,  2008 ) or the complete prediction tables can be downloaded for analysis. The training datasets of binding sites for each ligand, including those for bound/unbound pairs, are provided as  Supplementary Material . 2.2 Prediction results for unbound structures The decrease in the AUC for unbound as compared with bound structures was relatively small (≤0.083) showing that SitePredict is insensitive to rearrangements in the protein structure induced by ligand binding. Only surface pockets are considered as potential binding sites for small molecules so that the lack of a pocket in the unbound structure that sufficiently overlaps the binding sites (O&gt;0.5) results in a false negative prediction. All unbound structures for NAD retained pockets in the binding sites, however, pockets for two out of the 56 unbound structures (chain A of PDB entries 1BYI and 1I7N) for ATP had too little overlap with the binding site (O=0.47 and 0.28, respectively) so that the method missed these binding sites. As expected, this can be attributed to large conformational changes in loops near the binding site leading to relatively high RMSDs between binding site residues in bound and unbound structures of 3.0 Å and 2.3 Å, respectively. Interestingly, there is a surface pocket that sufficiently overlaps the ATP binding site (O=0.61) for chain B of PDB entry 1I7N, which is the same protein as chain A (C domain of rat synapsin II), and this gives a correctly predicted ATP binding site with a high likelihood ratio of 3.9. The structural differences between the chains A and B are mostly in the binding site loop, probably due to its flexibility in the apo protein. Overall, these results support the findings of An  et al.  ( 2005 ) that ligand binding pockets are almost always retained in unbound structures and furthermore that the prediction accuracy is not significantly degraded by differences in these pockets due to structural changes upon ligand binding. 2.3 Contribution of binding site properties to prediction performance The top 20 properties that contribute the most to the prediction accuracy for ligand sites were calculated using the procedure described in  Section 1.5  (see  Supplementary Material  for results). As can be seen in  Table 2 , the AUC changes little for all ligands except Cu if only these top 20 properties are used for prediction. The most important residue propensities for each metal ion include the most common coordinating residues for that particular ion according to the analysis of Harding ( 2004 ). Evolutionary conservation and SASA are also among the important variable for predicting metal ion binding sites since they appear in the top 20 properties for all ions examined. Metal ion binding sites are usually conserved and generally, but not always, on the protein surface. In contrast, evolutionary conservation only appears among the most important properties for two small molecules, ATP and NAD. Also, residue propensities appear less among the top properties for the small molecules. This may be due to several factors: (i) metal ions are positively charged and bound by clusters of negatively charged residues, (ii) small molecule binding sites are larger so that the spatial arrangement of residues within the site is more important than for smaller ion binding sites and (iii) there are more residues in a typical binding pocket than in the 10-residue clusters used for metal ion prediction so that any clustering of residue types is less statistically significant. Residues that are present in previously identified sequence motifs are also contained in the important residue pairs. For instance, A, G, K and S are in both the important residue pair variables for ATP binding sites and also in the Walker A motif. Also glycine appears in most of the important residue pairs for NAD binding sites and a glycine-rich turn was found to be a determinant of such sites (Baker  et al. ,  1992 ). 2.4 Discrimination between different ligands The ability of a Random Forest classifier trained on one ligand to reject binding sites of different ligands was assessed by comparing Random Forest scores from 10-fold cross-validation for one ligand (ligand 1) with scores from training on all cross-validation data for ligand 1 and then predicting for data from a different ligand (ligand 2). The average scores, AUC values and associated Wilcoxon rank-sum  P -values for all pairs of metal ions and small molecules are given  Tables 1  and  2  in the  Supplementary Materials .  Figure 1  shows an example of the successful discrimination between distinct Ca and Cu ion binding sites in the same protein.
 Fig. 1. An example illustrating the successful discrimination between two different metal ions, which in this case bind to the same protein (fungal lysyl oxidase, PDB entry 1N9E). Calcium and copper binding site predictions are shown in the top and bottom figures, respectively. Predicted binding residues, with  R &gt;20, are colored red, predicted non-binding residues, with  R &lt;10, are colored blue, and residues with intermediate ambiguous values are colored yellow. The bound copper ion is orange and the calcium ions are green. The insets show a detailed view of the binding sites. It is apparent from the tables that the discrimination performance is non-symmetric, i.e. a Random Forest trained on ligand 1 may have poor selectivity for ligand 2 but one trained on ligand 2 may be good at rejecting ligand 1 binding sites in favor of the correct ligand 2 sites. There are only two sets of ligands for which the method cannot discriminate in both reciprocal predictions: Ca and Mg ions and AMP and ATP. It is known that one of the most prevalent Ca binding motifs, the EF-hand motif, can also bind Mg in some cases (Lewit-Bentley and Rety,  2000 ) and that both ions have similar coordination propensities (Kaufman-Katz  et al. ,  1996 ). This is reflected by the fact that 15 out of the 20 top variables are shared by both Ca and Mg ion binding sites. More generally, predictors trained on Mg or Mn had lower specificity that those trained on other ions. The difficulty in differentiating between the binding of some metal ions, such as Mg and Mn, is also probably due in large part to the fact that many sites are known to actually bind different ions  in vitro  and may do so  in vivo  (Bock  et al. ,  1999 ). The difficulty in distinguishing binding sites for the ATP group ligands, which also include ADP, and AMP is due to the similar properties of the binding sites. Half of the 20 top variables contributing to the prediction accuracy are shared between these two ligands. One possible explanation for this similarity in the important binding site properties is the chemical similarity of the ligands; ADP and AMP differ only by a phosphate group. In addition, all small molecules considered, with the exception of heme, contain adenine moieties, which make their discrimination relatively difficult. In fact, none of the cases of poor ligand discrimination involve the chemically dissimilar heme. 2.5 Binding site predictions for modeled human proteins Although current comparative modeling methods can usually accurately reproduce the core backbone geometry for proteins with available homologous template structures, they have difficulty in predicting the correct conformations of side chains and loops (Ginalski,  2006 ). Because SitePredict only requires backbone structural information it can locate ligand binding sites in homology models, even if they contain errors in side chain conformations. Furthermore, because the prediction procedure is fast it can be applied to homology models on a genome-wide scale for functional annotation. Typically, between 30% and 60% of proteins in sequenced genomes have a related protein of known structure and these percentages are increasing as new structures become available (Xiang,  2006 ). The application of SitePredict to homology models was demonstrated by predicting ligand binding sites in a set of models for 688 human proteins downloaded from MODBASE (Pieper  et al. ,  2006 ), a database of protein structures generated by an automated modeling pipeline using the MODELLER program (Sali and Blundell,  1993 ). Only models that are expected to be accurate [score&gt;0.7 (Melo  et al. ,  2002 ) and PSI-BLAST  E -value&lt;1×10 −4 ] but that have low sequence identity to the template protein sequence (&lt;25%), and thus potentially yield novel binding site predictions not inferable from sequence homology alone, were considered. Prediction results for all ligands are available on the SitePredict website. Although the binding sites, or even which ligands bind, are unknown for most of the human proteins, high-resolution structures with bound ligands are available for comparison in a few cases. Structures with bound ATP group ligands (PDB entries 2GK6 and 2GT0, respectively) were available for human type 1 RNA helicase (Upf1) and nicotinamide riboside kinase 1 (NRK1). In both cases, the highest scoring predicted binding pockets in the model structures, with  R =7.5 and 11.9, respectively, overlapped the actual binding sites. Also the structure of one human protein in the set with heme bound, prostacyclin synthase (PGIS, PDB entry 3B6H) is available. Again the highest scoring binding pocket ( R =50) overlapped with the heme binding site. Even though the sequence similarity was low, the template structures for NRK1 and PGIS (2BBW and 1TQN) contained the corresponding ligands. No ligands are bound to the Upf1 template structure (1PJR). 2.6 Identifying new ligand binding sites in structures of uncharacterized proteins In recent years, the number of new X-ray structures of proteins with no significant sequence similarity to those already solved has been rapidly increasing, largely due to structural genomics projects (Chandonia and Brenner,  2006 ). Even with available high-resolution structures, the biological function of many of these proteins remains unknown. Knowledge of what ligands bind to a protein and where they bind can help in inferring the function. Binding site predictions were examined for a set of proteins with available X-ray structures but lacking functional annotation, downloaded from the PSI Structural Genomics Knowledgebase ( http://kb.psi-structuralgenomics.org/KB/ ). Although the binding site predictions will require experimental verification there were some proteins for which independent evidence suggests that they are correct. One example is a predicted NAD binding site in  Haemophilus influenzae  shikimate dehydrogenase-like protein HI0607 (PDB entry 1NPY). Phylogenetic analysis showed that this protein is in a distinct group from the two previously known functional classes (Singh  et al. ,  2005 ). The protein was also shown to catalyze the NADP + -dependent oxidation of shikimate. In addition, mutagenesis of two conserved residues, D103 and K67, inactivated the enzyme, thus implying that they are important catalytic groups. Both of these residues are in the predicted NAD-binding pocket. Finally, the protein has 30% sequence identity to an AroE shikimate dehydrogenase (PDB entry 1NVT) that has the same fold and NADP bound in the predicted pocket region. Another example is a predicted NAD-binding pocket in a mouse protein annotated as a putative NADPH-dependent oxidoreductase (PDB entry 1VJ1). A structural alignment revealed similarity to a quinone oxidoreductase (PDB entry 1QOR) even though the sequence identity is only 18% (Levin  et al. ,  2004 ). Furthermore, the quinone oxidoreductase structure has an NADPH molecule bound in the pocket corresponding to the predicted NAD binding pocket in the 1VJ1 structure. 3 CONCLUSIONS SitePredict was shown to perform well in predicting specific metal ion and small molecule binding sites in protein structures, with AUC≥0.80 for bound structures. Its performance on unbound structures was only slightly lower, demonstrating that the method is insensitive to most ligand-induced conformational changes in the benchmark set, which include side-chain reorganization and small to moderate backbone changes. SitePredict was also applied to predicting binding sites in uncharacterized proteins in PDB structures as well as automatically generated homology models of human proteins. Almost all of these predictions await experimental verification and potentially offer valuable clues to each protein's function. There are several possible areas of future investigation. One is to extend the binding site predictions to additional small molecules for which there are not enough ligand–protein complexes in the PDB for adequate training and validation. This could be accomplished, for example, by either collecting independent experimental binding data or training the method on clusters of similar binding sites, which presumably bind the same or chemically similar ligands. Finally, it would be useful to train additional classifiers that are optimized for discrimination between difficult to distinguish ligands. Funding : Mayo Clinic and a Biopilot project from the DOE Office of Advanced Scientific Computing Research; ERKP558 ‘An integrated knowledge base for the Shewanella Federation’ from the DOE Office of Biological and Environmental Research. Conflict of Interest : none declared. 
 Supplementary Material 
 
 
 [Supplementary Data] 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Close lower and upper bounds for the minimum reticulate network of multiple phylogenetic trees</Title>
    <Doi>10.1093/bioinformatics/btq198</Doi>
    <Authors>Wu Yufeng</Authors>
    <Abstract>Motivation: Reticulate network is a model for displaying and quantifying the effects of complex reticulate processes on the evolutionary history of species undergoing reticulate evolution. A central computational problem on reticulate networks is: given a set of phylogenetic trees (each for some region of the genomes), reconstruct the most parsimonious reticulate network (called the minimum reticulate network) that combines the topological information contained in the given trees. This problem is well-known to be NP-hard. Thus, existing approaches for this problem either work with only two input trees or make simplifying topological assumptions.</Abstract>
    <Body>1 INTRODUCTION Reticulate evolution, a form of evolution with hybridization and genetic exchanges between two species, are common in many organisms: bacteria, plants, fish, amphibians and many others. For better understanding of reticulate evolution, several reticulate evolutionary models have been proposed and actively studied to address various reticulate processes, such as hybrid speciation, lateral gene transfer and recombination. Since most of these models are in the forms of networks, we call them reticulate networks 1 . We refer the readers to (Huson,  2007 ; Huson and Bryant,  2006 ; Nakhleh,  2009 ; Semple,  2007 ) for surveys of different reticulate network models. The  key  computational problem related to these models is the  inference  of reticulate networks. Depending on the types of biological processes involved, data for network inference may be in different forms, such as phylogenetic trees for some short genomic regions (called genes in this article) or aligned DNA sequences. In this article, we focus on inferring reticulate networks from a set of correlated phylogenetic trees. Here is the biological motivation for our problem. Suppose multiple phylogenetic trees (called  gene  trees in this article) are reconstructed, each from some gene for these species. Due to reticulate evolution, different genomic regions (say genes) may be inherited from different ancestral genomes and their evolutionary histories may not be the same (but are still related). Thus, these trees are  correlated  but  not  identical.  No  single phylogenetic tree can faithfully model the evolution of the species, and a more complex network model (i.e. reticulate network as studied in e.g. Baroni  et al. ,  2004 ; Huson,  2007 ; Huson  et al. ,  2005 ; Nakhleh  et al. ,  2004 ; Semple,  2007 ) is needed. Imagine we are given a set of ‘true’ gene trees and a ‘true’ reticulate network that models the evolutionary history of these genes. The network can be considered as a compact representation of these gene trees in the sense that one should be able to ‘trace’ a gene tree within the network. We say such a gene tree is  displayed  in the network. This motivates a natural problem, which is called ‘the holy grail of reticulate evolution’ in (Nakhleh,  2009 ): given a set of gene trees, reconstruct a reticulate network that displays  every  given gene tree. Such an inferred network reveals important correlation of evolutionary history of multiple genes. Since there exists many such networks, a common formulation is to find the one with the  fewest  reticulation events. Such a network is called the  minimum  reticulate network. The central computational problem on reticulate networks, the  minimum reticulate network problem , is: given a set of gene trees, reconstruct the minimum reticulate network that displays these gene trees. This formulation may be reasonable when reticulation is believed to be rare. In general, this problem is computationally challenging: even the case with only  two  gene trees is known to be NP-complete (Bordewich and Semple,  2004 ,  2007 ; Hein  et al. ,  1996 ). There are several existing approaches for reconstructing the  exact  minimum reticulate networks when there are only two gene trees (Bordewich  et al. ,  2007 ; Linz and Semple,  2009 ; Wu,  2009 ; Wu and Wang,  2010 ). Clearly restricting to just two gene trees is a  big  limitation: more gene trees will be more informative to phylogenetic inference, and DNA sequences of many genes are available. Alternatively, there are also a number of approaches making simplifications to the reticulate network model, e.g. by imposing additional topological constraints on reticulate networks (Gusfield,  2005 ; Huson and Klopper,  2007 ; Huson  et al. ,  2009 ; Nakhleh  et al. ,  2005 ) or working with small-scale tree topological features (Huson  et al. ,  2005 ; Huson and Klopper,  2007 , van Iersel  et al. ,  2008 ). Such simplification often leads to significantly faster approaches. However, it is sometimes unclear how biologically meaningful these added topological constraints are. Even in the case where additional simplifications are reasonable, one may still want to compare with the unconstrained minimum reticulate networks. Contributions: In this article, we present new approaches for the minimum reticulate network problem with  three or more  gene trees for unconstrained, general, reticulate networks (e.g. without needing to assume that the network has some restricted form, such as being a galled-tree or galled network). Thus our work is more general than some previous approaches (Huson and Klopper,  2007 ; Huson  et al. ,  2005 ; Huson  et al. ,  2009 ). In particular, we develop a  lower  bound (the  RH  bound) and an  upper  bound (the  SIT  bound) for the minimum reticulate network problem with multiple gene trees. We show the correctness of the bounds. We give a closed-form formula for the  RH  bound for the case of three gene trees. We also show how to compute these bounds efficiently in practice using integer linear programming (ILP). Practical results on simulated and real biological data show that the bounds can be computed for wide range of data. Moreover, the lower and upper bounds are often  close , especially when the number of trees is small or reticulation level is relatively low. In fact, for many simulated datasets of this type, the lower and upper bounds often match, which means our methods can reconstruct the  exact  minimum reticulate networks for these datasets. We also show the  RH  bound clearly outperforms a simple bound. 2 DEFINITIONS AND BACKGROUND Throughout this article, we assume trees are rooted. A phylogenetic tree is rooted and leaf-labeled by a set of species (called taxa). A leaf of a phylogenetic tree corresponds to an extant species. An internal vertex corresponds to a speciation event. In-degrees of all vertices (also called nodes), except the root, in a tree are one, while out-degrees are zero for leaves and at least two for internal nodes. A binary phylogenetic tree requires out-degrees of internal nodes to be two. A non-binary phylogenetic tree contains nodes with out-degree of three or more. Many existing phylogenetic methods assume binary phylogenetic trees, although sometimes only non-binary trees can be reconstructed in practice. Our definition of reticulate networks is similar to that in (Huson,  2007 ; Huson  et al. ,  2005 ; Semple,  2007 ) (Hallett and Lagergren,  2001 ; Nakhleh,  2009 ; Nakhleh  et al. ,  2004 ). A reticulate network (sometimes simply network) is a directed acyclic graph with vertex set  V  and edge set  E , where some nodes in  V  are labeled by taxa.  V  can be partitioned into  V T  (called tree nodes) and  V R  (called reticulation nodes).  E  can be partitioned into  E T  (called tree edges) and  E R  (called reticulation edges). Moreover,
 No nodes with total (in and out) degree of two is allowed. Except the root, each node must have at least one incoming edge. V R  contains nodes whose in−degrees are two or more.  V T  contains nodes whose in-degrees are one. E R  contains edges that go into some reticulation nodes.  E T  contains edges that go into some tree nodes. A node is labeled by some taxon iff its out-degree is zero. This helps to ensure labeled nodes correspond to extant species and remove some redundancy in the network. 
 In addition, we have one more restriction:
 R 1  For a reticulate network 𝒩, when only  one  of the incoming edges of each reticulation node is kept and the rest are deleted, we always derive a tree  T ′. 
 We first consider the derived tree  T ′ (that is embedded in 𝒩) as in restriction  R 1 . When we recursively remove non-labeled leaves and contract edges to remove degree-two nodes of  T ′ (called cleanup), we obtain a phylogenetic tree  T  (for the same set of species as in 𝒩). Now suppose we are given a phylogenetic tree  T . We say  T  is  displayed  in 𝒩 when we can obtain an induced tree  T ′ from 𝒩 by properly choosing a single edge to keep at each reticulation node so that  T ′ is topologically  equivalent  to  T  after cleanup. We denote the induced  T ′ (if exists) as  T 𝒩 . See  Figure 1  for an illustration.
 Fig. 1. An illustration of a reticulate network with three reticulation events for three trees. Each tree is displayed in the network: the tree can be obtained by keeping one incoming edge at each reticulation node. Note restriction  R 1  implies the network is  acyclic . Biologically, reticulate networks often forbid cycles. This is because many reticulation events need to be properly time-ordered. Thus, we focus on  acyclic  reticulate networks in this paper. That is, when we refer to a reticulate network, we mean an acyclic reticulate network (unless otherwise stated). There are subtle issues related to networks with nodes whose out-degrees are more than two (called non-binary nodes). See the  Supplementary Materials  for more discussion. Note that we do not require that in-degrees of reticulation nodes are precisely two as what was imposed in (Huson  et al. ,  2005 ). We also assume the root of each input tree  T i  is attached to an outgroup species  o . The root of a reticulate network for these trees is also attached to  o . We define the reticulation number of a reticulation node as its in-degree minus one. For a reticulate network 𝒩, we define the reticulation number (denoted as  R 𝒩 ) as the summation of the reticulation number of each reticulation node in the network. Sometimes  R 𝒩  is also called the number of reticulation events in 𝒩. For the reticulate network in  Figure 1 , the reticulation node 2 has three entering edges, and the other reticulation node 1 has two entering edges. Thus,  R 𝒩 =(3−1)+(2−1)=3. Our definition of reticulation number is similar to that of the hybridization number in (Bordewich  et al. ,  2007 ; Semple,  2007 ). Suppose we are given a set of  K  gene trees  T 1 ,  T 2 , …,  T K  (for the same set of species). The minimum reticulate network 𝒩 min  for  T 1 ,  T 2 ,…,  T K  is a reticulate network 𝒩 that displays  each T i  and  R 𝒩  is  minimized  among all possible 𝒩. We call  R 𝒩 min  the reticulation number of  T 1 ,…,  T K , which is denoted as  R ( T 1 ,  T 2 ,…,  T K ). For the special case of  K =2, we call  D T i , T j = R ( T i ,  T j ) the reticulation  distance  between two trees  T i  and  T j . Now we formulate the central problem in this article. The general minimum reticulate network (GMRN) problem: Given a set of phylogenetic trees  T ={ T 1 ,…,  T K }, reconstruct the minimum reticulate network 𝒩 min  for  T 1 ,  T 2 ,…,  T K . This formulation is based on parsimony, and may be justified when reticulation is relatively rare in the evolutionary history. One should note that the GMRN problem can be further specified by the type of input trees. There are two types of input phylogenetic trees: binary or non-binary. For a non-binary tree  T , we say  T  is displayed in 𝒩 if some refinement of  T  (i.e. splitting the non-binary nodes in  T  in some way to make  T  binary) is displayed in 𝒩. When input trees are binary, network reconstruction may be easier. For simplicity, in this following, the input trees are assumed to be  binary , unless otherwise stated. We remark that some of our results are applicable to non-binary input trees: the  RH  bound in  Section 3  clearly works for non-binary trees too, and the high-level approach of the  SIT  bound may also be applicable to non-binary trees. Previous work on the GMRN problem: There is an exact method for the  K =2 case of the minimum reticulate network problem (Bordewich  et al. ,  2007 ), although this special case is known to be NP-complete (Bordewich and Semple,  2007 ). It is useful to note that when we allow cycles in the network, the minimum reticulate network problem is equivalent to the rooted subtree prune and regraft (rSPR) distance problem. The rSPR distance problem, another NP-complete problem (Bordewich and Semple,  2004 ; Hein  et al. ,  1996 ), is well known to be closely related to reticulate evolution. Previously, we showed that the rSPR distance can often be practically computed for many moderately sized trees (Wu,  2009 ). We give more background to the rSPR distance problem in the  Supplementary Material . It was shown in (Baroni  et al. ,  2005 ) that the reticulation number (called hybridization number in (Baroni  et al. ,  2005 )) for trees  T 1  and  T 2  is closely related to the rSPR distance between  T 1  and  T 2 , although the two values are not always equal. The main difference between the rSPR distance and the reticulation number is that the latter forbids  cycles  and thus can be more realistic biologically. Recently, we have extended our previous approach in (Wu,  2009 ) to allow computing the pairwise  reticulation  distance between two rooted binary trees (Wu and Wang,  2010 ). Although the worst case running time of the practical methods in (Bordewich  et al. ,  2007 ; Wu,  2009 ; Wu and Wang,  2010 ) are exponential, these methods may work reasonably well in practice. As shown in (Bordewich  et al. ,  2007 ; Wu,  2009 ; Wu and Wang,  2010 ), exact reticulation number (with or without cycles) can be computed for two quite different trees with 20 or more leaves. Thus, although intractable theoretically, the two-tree minimum reticulate network problem can be solved in practice if the size of two trees is moderate or the two trees are not very different topologically. It becomes more computationally challenging when there are three or more gene trees. There is currently  no  known practical methods for either computing the reticulation number  R ( T 1 ,…,  T K ) or reconstructing 𝒩 min  for trees  T 1 ,…,  T K  when  K  ≥ 3. Often approximation is made. A common approach is to impose structural constraints to limit the complexity of the network (Gusfield,  2005 ; Huson and Klopper,  2007 ; Huson  et al. ,  2009 ; Nakhleh  et al. ,  2005 ). Although these approaches are theoretically interesting and have been shown to work for some biological data, it is still very desirable to explore the reconstruction of reticulated networks displaying multiple complete gene trees  without  additional structural constraints. 3 A LOWER BOUND We now focus on developing a lower bound on  R ( T 1 ,…,  T K ). The lower bound helps to better quantify the  range  of  R ( T 1 ,…,  T K ). Recall that several exact methods (Bordewich  et al. ,  2007 ; Wu and Wang,  2010 ) exist for computing the  pairwise  reticulation distance  D T i , T j  for two trees  T i ,  T j , which are practical for many pairs of trees of moderate sizes. Now suppose that we compute  D T i , T j  for each pair of trees  T i  and  T j , using the methods (Bordewich  et al. ,  2007 ; Wu and Wang,  2010 ). We store these pairwise distances in a matrix  D , where  D [ i ,  j ]= D T i , T j 2 . Admittingly, computing  D [ i ,  j ] for all  T i  and  T j  can be slow when  K  and/or the size of trees are large (unless  T i  and  T j  are very similar). One should note that the GMRN problem is much more complex, and thus, calculation of  D [ i ,  j ] is justifiable computationally. This leads to the following question: can we use the pairwise reticulation distances  D  to estimate  R ( T 1 ,…,  T K ) when  K  ≥ 3? Clearly, the  largest  value  D [ i 0 ,  j 0 ] in  D  is necessarily a  lower  bound of  R ( T 1 ,…,  T K ) when  K  ≥ 3: a reticulate network displaying all trees certainly also displays trees  T i 0  and  T j 0  and thus is a reticulate network for  T i 0  and  T j 0 . We now show a  stronger  lower bound (called  RH  bound) based on  D  values. Here is the high-level idea. The pairwise distance  D T i , T j  specifies how similar trees  T i  and  T j  are: the larger  D T i , T j  is, the more different  T i  and  T j  are. Recall that if tree  T i  is displayed in a network 𝒩, we should be able to derive  T i  by keeping only one incoming edge at each reticulation node and performing cleanup. The choice (called display choice for  T i ) of keeping which incoming edge at each reticulation node for a tree  T i  may not be unique. However, clearly if one makes the same display choices for  T i  and  T j  when displaying  T i  and  T j  in 𝒩, then  T i  and  T j  will be identical. More generally, the more similar the display choices for trees  T i  and  T j , the closer  T i  and  T j  will be. Thus, to allow an 𝒩 for trees with pairwise distances  D [ i ,  j ]= D T i , T j , we need to make display choices for the trees different enough: if the display choices for  T i  and  T j  are too similar, it will lead to contradiction when  D [ i ,  j ] suggests  T i  and  T j  are more different. In the following, a rigorous analysis based on this idea allows us to decide whether an 𝒩 with a specific number (say  r ) of reticulation events is feasible. To fix ideas, we first consider the situation where each reticulation node in 𝒩 has in-degree of two. We will remove this assumption in a moment. Suppose that 𝒩 has  r  reticulation nodes (each with two incoming edges). For each reticulation node, we arbitrarily call one incoming edge the left edge and the right edge for the other. We encode the left edge as 0 and the right edge as 1, and call these two edges 0-edge and 1-edge. Recall that to display a tree, we need to keep exactly one of these two edges. Since a tree  T i  is displayed in 𝒩, we create a binary vector  v i [1 …  r ] to represent which incoming edge  T i  is kept at each reticulation node  V j  in 𝒩. Here,  v i [ j ] is 0 if  T i  keeps the 0-edge at reticulation node  V j , and 1 if  T i  keeps the 1-edge at  V j . We call  v i  the  display vector  for  T i . For example, in  Figure 1 , consider the reticulation node labeled as 1, and we assign the left/right edges as shown.  T 1  and  T 3  keep the left edge at this node, while  T 2  keeps the right edge. Thus,  v 1  and  v 3  have value 0, and  v 2  has value 1 at the node. For a given  T i  and a network 𝒩,  v i  can always be constructed (at least conceptually) based on how  T i  is displayed in 𝒩. Note that if there are multiple choices to display  T i , we simply pick an arbitrary one and this does not affect our solution. We define  D h [ v i ,  v j ] as the Hamming distance between two display vectors  v i  and  v j . Here,  v i  and  v j  (and thus  D h [ v i ,  v j ]) depend on 𝒩. To simplify notations, we do not explicitly include 𝒩 in their definitions. Lemma 3.1 is crucial to our lower bound. L emma  3.1. For any two trees T i   and T j   displayed in a reticulate network  𝒩,  D h [ v i ,  v j ] ≥  D [ i ,  j ]. P roof . For contradiction, assume  D h [ v i ,  v j ]&lt; D [ i ,  j ]. Thus  T i  and  T j  make different choices at less than  D [ i ,  j ] reticulation nodes of 𝒩. Imagine we remove from 𝒩 those incoming edges at reticulation nodes that are  not  kept by both  T i  and  T j . This produces a network with less than  D [ i ,  j ] reticulation nodes. This is because all reticulation nodes where  v i  and  v j  match (and thus  T i  and  T j  keep the  same  incoming edges) have only one incoming edge and are no longer reticulation nodes in the reduced network. This contradicts the fact that  D [ i ,  j ] is the reticulation distance between  T i  and  T j . ▪ Lemma 3.1 implies that if a network 𝒩 with  r  reticulation events exists, then we should be able to find binary vectors  v i  (of length  r ) for each tree  T i , and  D h [ v i ,  v j ] ≥  D [ i ,  j ] for any two such vectors  v i  and  v j . On the other hand, if such vectors do  not  exist, we know that at least  r +1 reticulation events are needed (and the value  r +1 is a lower bound on  R ( T 1 ,…,  T K )). We can illustrate this formulation more intuitively using a binary  hypercube . On a hypercube with  r  binary bits per node, we want to know whether we can pick  K  points  v 1  …  v K  that are far apart enough such that the Hamming distance between  v i  and  v j  is at least  D [ i ,  j ] for each  i  and  j . One should note this is not always feasible due to the limited size of the hypercube. Formally, The binary hypercube point placement problem: Can we choose  K  nodes  v 1 ,…,  v K  from a  r -dimensional binary hypercube so that  D h [ v i ,  v j ] ≥  D [ i ,  j ] for each pair of  v i  and  v j ? A lower bound on  R ( T 1 ,…,  T K ) based on the Hypercube Point Placement problem is to find (possibly in a binary search style) the  smallest  integer  r  such that the hypercube point placement problem  has  a solution. Such  r  is necessarily a lower bound on  R ( T 1 ,…,  T K ). We call this lower bound reticulation on hypercube bound (or  RH  bound). We do not know a polynomial-time algorithm for the binary hypercube point placement problem with more than three trees. When  K =3, however, the  RH  bound has a simple analytical form (see  Section 3.2 ). To develop a practical method for the general case, we use integer linear programming (ILP) to solve this problem. We create a binary variable  V i , k  to represent the coordinates for point  v i . That is, the coordinates of  v i  on the hypercube are specified by  V i ,1  …  V i , r . Without loss of generality, we set  V 1, k =0 for all 1≤ k ≤ r . We create a binary variable  M i , j , k  for each  v i ,  v j  and position k (1≤ k ≤ r ) to indicate whether two vectors  v i  and  v j  match at position  k .  M i , j , k =1 if  V i , k = V j , k , and 0 otherwise. Now, we have the following formulation.
 Optimization goal: none (since this is a feasibility problem) Subject to 
 
 M i , j , k + V i , k + V j , k  ≥ 1, for each  v i ,  v j , where  i &lt; j  and 1≤ k ≤ r . M i , j , k  −  V i , k  −  V j , k  ≥ −1, for each  v i ,  v j , where  i &lt; j  and 1≤ k ≤ r . ∑ k =1 r   M i , j , k  ≤ r − D [ i ,  j ], for each  v i ,  v j , where  i &lt; j . 
 
 For each 1≤ i ≤ K  and 1≤ k ≤ r , there is a binary variable  V i ,  k . For each 1≤ i &lt; j ≤ K , and 1≤ k ≤ r , there is a binary variable  M i , j , k . 
 Constraint 1 says if both  V i , k  and  V j , k  are 0,  M i , j , k  is 1 (i.e. matched). Similarly, constraint 2 says if both  V i , k  and  V j , k  are 1,  M i , j , k  is 1 (i.e. matched). Constraint 3 imposes the pairwise Hamming distance requirement. Our experience shows that the ILP is practical to solve for all datasets we simulated (see  Section 5 ). 3.1 Networks with in-degree of three or more We now resolve the remaining issue where some reticulation nodes have in-degree of three or more. In this section, we call a reticulation node ‘refined’ if its in-degree is two, and ‘unrefined’ if its in-degree is at least three. Here, we can no longer represent a reticulation node as binary value, as done previously. So we extend our definitions of display vectors  v i  to allow  v i  to be non-binary. That is, if there are  d  incoming edges at a reticulation node, we allow  v i  to be from 0 to  d −1, where the value indicates which one of the  d  branches  T i  is kept at this node. The incoming edges are numbered starting from zero on the left and to the right with increment of one. We still let  D h [ v i ,  v j ] be the Hamming distance between vectors  v i  and  v j . In this general case, Lemma 3.1 still holds for non-binary vectors  v i  and  v j . To see this, we prune any incoming edge at reticulation nodes if it is not chosen by  T i  and  T j . Then each remaining reticulation node has only  two  incoming edges (since we only have two trees). Thus, there are  D h [ v i ,  v j ] reticulation events in this reduced network, and the rest of proof for Lemma 3.1 follows. We now show that it is not necessary to consider unrefined reticulation nodes in the sense that if a network 𝒩 with unrefined reticulation nodes satisfies pairwise distances  D , then there exists another network 𝒩′ that has only refined reticulation nodes and gives the binary vectors  v i  satisfying the pairwise distance constraints of  D . That is, if we can not find a network with only refined reticulation nodes, we also can not find a network with unrefined reticulation nodes and the same reticulation number. To see this property, we consider a network 𝒩 with one reticulation node  q  with  d  ≥ 3 incoming edges. Then we transform 𝒩 to 𝒩′ by replacing  q  with  q 1 ,…,  q d −1 , where each  q i  is a reticulation node with in-degree of two. Note that we do not have to ensure 𝒩 and 𝒩′ are equivalent: we only need to show 𝒩′ gives a solution to the Binary Hypercube Point Placement problem. Clearly, 𝒩 and 𝒩′ have the same reticulation number (although vectors for 𝒩′ are longer). Now, suppose tree  T i  keeps edge  j  at  q  (where 0≤ j ≤ d −1), we then keep edge 1 at  q j  in 𝒩′ if  j  ≥ 1 (and 0 if  j =0), and keep edge 0 for all other  q j ′  (where  j ′≠ j ). In other words, we create a mapping of the display vectors  v i  from 𝒩 to 𝒩′ for each  T i . Note that such mapping ensures that if two trees keep the same edge at  q , they will keep the same edges at  q 1 ,…,  q d −1  in 𝒩′; otherwise, they will keep at least one different incoming edge at  q 1 ,…,  q d −1  in 𝒩′. In either case, if the pairwise distance constraints are satisfied in 𝒩, they are also satisfied in 𝒩′. So, if we can not find display vectors 𝒩′ for networks with refined reticulation nodes only, we also can not find display vectors for networks allowing unrefined reticulation nodes. In other words, the  RH  bound holds for networks with unrefined nodes. Remark.  The  RH  lower bound is still applicable when the input trees are non-binary, as long as the pairwise reticulation distances are obtained for the non-binary trees. These are easy to verify and we omit the details due to the lack of space. Remark.  A commonly used concept in reticulate networks is the so-called maximum agreement forest (MAF). A brief description on MAF is given in the  Supplementary Material . Also see e.g. in (Semple,  2007 ) for more details. It is easy to see that the size of a MAF of multiple trees is a lower bound on  R ( T 1 ,…,  T K ). However, experience show that the  RH  bound is often higher than the MAF bound (see  Section 5 ). 3.2 Special case of three trees The special case of  K =3 allows us to study the  RH  bound in an analytical way. We let  d 1 ,  d 2  and  d 3  be the pairwise reticulation distances of the three trees, where  d 1  ≥  d 2  ≥  d 3 . Proposition 3.2 shows the  RH  bound for three trees in an analytical form. P roposition  3.2. The RH lower bound for three trees T 1 ,  T 2   and T 3   is equal to     if d 2 + d 3 &gt; d 1 ,  and equal to d 1   if d 2 + d 3 ≤ d 1 . P roof . We first consider the case  d 2 + d 3 &gt; d 1 . Clearly, the  RH  bound is at least  d 1 , which is the minimum size of the hypercube. Now we investigate whether there exists a reticulate network with  d 1 + e  reticulation nodes for these three trees. Without loss of generality, let  T 1  be the input tree where  d 1 = D T 1 , T 2  and  d 2 = D T 1 , T 3 , and the display vector  v 1  (for  T 1 ) is fixed to be all-0. Then, the display vector  v 2  for  T 2  must have at least  d 1  positions with value 1 (and thus  v 2  has no more than  e  positions with value 0). Similarly, the display vector  v 3  must have at least  d 2  positions with value 1 (and thus  v 3  has no more than  d 1 + e − d 2  positions with value 0). Note that  D h [ v 2 ,  v 3 ] ≥  d 3 . We claim that  D h [ v 2 ,  v 3 ]≤ d 1 +2 e − d 2 . This is because the Hamming distance between  v 2  and  v 3  counts the positions where  v 2  has value 0 and  v 3  has value 1 (or vice versa). Since the number of 0s in  v 2  is no more than  e , there are at most  e  positions where  v 2  has 0 and  v 3  has 1. Similarly, there are at most  d 1 + e − d 2  positions where  v 2  has 1 and  v 3  has 0. Thus,  D h [ v 2 ,  v 3 ]≤ e + d 1 + e − d 2 = d 1 +2 e  −  d 2 . Also note that we can always construct  v 2  and  v 3  so that  D h [ v 2 ,  v 3 ]= d 1 +2 e − d 2 . See  Figure 2  for an illustration.
 Fig. 2. Vectors  v 1 ,  v 2  and  v 3  (listed from top to bottom) that maximize the Hamming distance between  v 2  and  v 3 .  v 1  is all-0, while the suffix of  v 2  and prefix of  v 3  are zeros. Therefore, if  d 1 +2 e − d 2  &lt;  d 3 , we can  not  find three vectors  v 1 ,  v 2  and  v 3  satisfying the pairwise distances  D  and thus  R ( T 1 ,  T 2 ,  T 3 ) ≥  d 1 + e +1 in this case. The largest such  e  is equal to   (which is non-negative since  d 2 + d 3  &gt;  d 1 ). The  RH  bound is then   =  . The case when  d 2 + d 3 ≤ d 1  is simple. We create three vectors of  d 1  bits:  v 1  is an all-0 vector,  v 2  is an all-1 vector and  v 3  contains  d 2  1s. It is easy to verify these three vectors satisfy all three pairwise distance constraints. ▪ In practice, it is very likely  d 2 + d 3  &gt;  d 1 . In this case,  , where  d 1  is a trivial lower bound. 4 AN UPPER BOUND We now present an upper bound on  R ( T 1 ,…,  T K ). The combination of the  RH  lower bound and the upper bound quantifies the  range  of  R ( T 1 ,…,  T K ). In the best scenario, if the upper bound  matches  the  RH  bound, these bounds would actually determine the  exact  value of  R ( T 1 ,…,  T K ) (and also reconstruct 𝒩 min ). On the high level, the upper bound performs  s tepwise  i nsertion of  t rees into a reticulate network (and thus is called the  SIT  bound). The  SIT  bound is very accurate and also computable in practice for many datasets. The basic idea of the  SIT  bound is to reconstruct a reticulate network 𝒩 in a step-by-step way: ‘insert’ the given gene trees one by one into 𝒩 in some fixed order. When we say a tree  T  is inserted into 𝒩, we mean adding reticulation edges into 𝒩 such that  T  is displayed in the updated network 𝒩′. Note that addition of new reticulation edges increases  R 𝒩 . Thus, every time we insert a new tree, we seek to add as  few  new reticulation edges as possible by  reusing  existing reticulation edges. At the same time, we also ensure no cycles exists in 𝒩′. Often, it is unclear which order of inserting trees gives the best result. For now, we assume that  K  is relatively small so that we can enumerate all possible orders of insertion to find the best result. See  Section 4.2  for ways to handle larger  K . Thus, we can assume the order of tree insertion is fixed to  T 1 ,  T 2 ,…,  T K . The general procedure of the  SIT  bound (for a fixed order) is as follows.
 Initialize 𝒩 to be  T 1 . for  i =2 to  K Insert  T i  into 𝒩 by adding the smallest number of new reticulation edges. 
 Note that we only add reticulation edges in 𝒩 and do not delete any existing edges. Thus, any tree already displayed in 𝒩 is still displayed in the updated 𝒩′ by choosing the original reticulation edges when the tree is first inserted for their display vectors in 𝒩′. This ensures that each of the input trees is displayed in the final 𝒩. Obviously, step 3 is most critical, which we will discuss next. 4.1 Inserting tree  T  into 𝒩 We consider the ‘min-cost tree insertion problem’, where we want to update 𝒩 by adding the  fewest  reticulation edges to 𝒩 so that a given tree  T  is displayed in the updated 𝒩′, and 𝒩′ remains acyclic. Note that the min-cost tree insertion problem is NP-complete because it contains the two-tree minimum reticulate network problem (an NP-complete problem) as a sub-problem. That is, constructing the minimum reticulate network for trees  T 1  and  T 2  can be solved by inserting  T 2  into  T 1  with the minimum cost. In the following, we develop a practical method to solve the min-cost tree insertion problem. Each node of the reconstructed network here has one or two incoming edges (except the root), and one or two outgoing edges (except the leaves). After inserting  T  (and some new reticulation edges are added),  T  is displayed in the updated network 𝒩′. Suppose we  remove  all the new reticulation edges in 𝒩′. The edge removals break tree  T (𝒩′) (the tree created by keeping edges in 𝒩′ according to a display vector of  T ) into a  forest F ( T (𝒩′)). Thus, the number of newly added reticulation edges is  exactly  the number of trees in  F ( T (𝒩′)) minus one. To minimize the number of needed new reticulation events, we need to minimize the number of trees in  F ( T (𝒩′)). A useful observation is that the problem of finding  F ( T (𝒩′)) with the fewest subtrees is closely related to the maximum agreement forest problem (see the  Supplementary Material ) as follows. Imagine that we choose a tree  T ′ that is displayed in 𝒩 so that the display vector of  T ′ agrees with that of  T  for 𝒩′ at each reticulation node of 𝒩. Recall that 𝒩′ may contain a number of new reticulation nodes that are not in 𝒩. Also note  T ′ is not necessarily one of the input trees  T i . We claim that  F ( T (𝒩′)) is an agreement forest for  T  and  T ′. To see this, we note that the display choices made by  T ′ are identical to  T  except those at the new reticulation nodes (where  T ′ follows the original edge and  T ( N ′) follows the new edge). So the subtrees in  F ( T (𝒩′)) must also be subtrees of  T ′. So, we have: L emma  4.1. The forest induced by removing newly added reticulation edges of T (𝒩′)  is an agreement forest between T and some tree T ′  that is displayed in the original  𝒩. Lemma 4.1 implies that to find the best tree insertion, we can find some tree  T ′ displayed in 𝒩 s.t. the number of trees in the maximum agreement forest between  T  and  T ′ is minimized.  Figure 3  shows an example of tree insertion. The dashed lines in the tree (left) divide the tree into a forest, which also appears in the existing network (middle, thick lines). Inserting the tree into the network is to add new reticulation edges (right, thick lines) into the networks so that the subtrees in the forests are properly connected to match the given tree.
 Fig. 3. Inserting a tree (left) to a network (middle). After adding new reticulation edges (thick lines), the resulting network (right) displays the tree. When the number of reticulation nodes in 𝒩 is small, we may simply enumerate all trees  T ′ displayed in 𝒩 and then find which  T ′ gives the smallest agreement forest with  T . This quickly becomes infeasible as the number of reticulation nodes in 𝒩 grows: when there are  r  reticulation nodes in 𝒩, there may exist 2 r  trees  T ′ displayed in 𝒩. To develop a practical method, we develop an integer linear programming (ILP) formulation to solve the tree insertion problem in an optimal way (without explicit enumeration). The output of the ILP formulation includes the display choices of  T ′ as well as the associated agreement forest formed by cutting edges in  T . See the  Supplementary Material  for detailed description of the formulation. Updating 𝒩: after tree  T ′ and the associated agreement forest are found, we update 𝒩 as follows. We add new reticulation edges in 𝒩 to connect subtrees of  T ′ in the found agreement forest to make  T  displayed in the updated network 𝒩′. First, we determine the order of subtree connection with an approach similar to the algorithm building two-tree hybridization networks in (Semple,  2007 ). The subtree with the special outgroup taxon  o  acts as the base. Then we repeatedly pick the subtree not intersecting any already connected subtree as the next to connect. Now, for each tree connection:
 Find the root  r  of the next subtree (in 𝒩) to attach. Find the node  v  in the existing network as the attaching point to accept this subtree. Create a new reticulation node in 𝒩 to connect the subtree. 
 This operation depends on the types of  r  and  v . Two cases are shown in  Figure 4 . The other cases are similar. In all the cases, only a single new reticulation edge is created to connect a subtree.
 Fig. 4. Attaching a subtree in 𝒩. Left: insert a reticulation edge between two tree nodes  r  and  v . Right: insert a reticulation edge between two reticulation nodes  r  and  v . The dashed lines are the newly added edges. Cycles: a remaining issue is that cycles can be introduced when connecting subtrees in 𝒩. There are two sources of cycles. First, the found agreement forest may induce cycles (see (Baroni  et al. ,  2005 )). Enhancing the ILP formulation to avoid cycles may significantly complicate the formulation and slow the ILP solving. A practical observation is that cycles in an agreement forest are often caused by two pairs of leaves  a ,  b  and  c ,  d  so that the a/b pair is ancestral to c/d pair in  T  and the c/d pair is ancestral to a/b pair in  T ′. Here, we say a pair of leaves a and b is ancestral to a pair of leaves c and d if the the MRCA of a and b is ancestral to the MRCA of c and d. MRCA stands for the most recent common ancestor, and node a is ancestral to node b in tree  T  if a is on the path from b to the root of  T . To forbid this type of simple cycles, we enhance the ILP formulation: for such pairs a/b and c/d, we require either a and b are not in the same subtree, or c and d are not in the same subtree of the resulting forest. Although this does not guarantee to remove all cycles, we found that cycles in the agreement forest are rare after this change. This observation is also useful for the method of computing pairwise reticulation distances in (Wu and Wang,  2010 ). Second, cycles can appear in other parts of the network when subtrees in the agreement forest are connected. In practice, however, we find this happens relatively rare. When this type of cycles does occur, we simply start over and try another order of tree insertion. This works well in practice: in  Section 5 , we build acyclic networks successfully for all (thousands of) simulated datasets. 4.2 Handling larger datasets When the size and the number of trees grow, the running time increases. To handle larger datasets, we make several simplifications. (i) Instead of enumerating all possible orders of tree insertion, we start with an arbitrary tree. At each step, we pick a tree with the smallest reticulation distance to one of the already inserted trees. (ii) Solving the min-cost tree insertion problem optimally becomes more difficult when data grows. So instead of considering all possible  T ′ displayed in 𝒩 when inserting  T , we randomly choose a fixed number (say 10) of trees  T ′ displayed in 𝒩 (in addition to all the inserted gene trees) and find the best way of inserting  T  based on one of the chosen  T ′. This heuristic is called the  coarse  mode (and the original approach is called the  full  mode). Our experience shows that the coarse mode works reasonably well in practice (see  Section 5 ). 5 EXPERIMENTAL RESULTS We have implemented a software tool called PIRN (which stands for Parsimonious Inference of Reticulate Network) to compute the  RH  and  SIT  bounds. Program  PIRN  is available for download from:  http://www.engr.uconn.edu/˜ywu/ . The tool is written in C++ and uses either CPLEX (a commercial ILP solver) or GNU GLPK ILP solver (mainly a demo of the functionalities for users without a CPLEX license). In computing the  SIT  bound,  PIRN  can run full mode (slower but can give better results) or coarse mode (faster but less accurate). We test our methods for both simulated and biological data on a 3192 MHz Intel Xeon workstation. 5.1 Simulation data We generate simulation data using a two-stage approach: first simulate reticulate networks, and then generate a fixed number of trees displayed in the networks according to randomly generated display vectors. We simulate reticulate networks using a scheme similar to the coalescent simulation implemented in program ms (Hudson,  2002 ). For a given number of taxa (denoted as  n ), we start with  n  isolated lineages and simulate reticulation  backwards  in time. At each step, there are two possible events: (i) lineage merging, which occurs at rate 1; (ii) lineage splitting, which occurs at rate  r . We choose the next event according to relative probabilities of all feasible events. Lineage merging generates speciation events, while lineage splitting generates reticulation events. To speedup the simulation, lineage splitting is disabled when the number of current lineages is no more than three. The parameter  r  dictates the level of reticulation in the simulated network: larger  r  will lead to more reticulation events in simulation. Full mode of the  SIT  bound: to test the performance of the bounds, we generate data with varying number of trees  K , number of taxa  n  and level of reticulation  r . For each settings of these three parameters, we simulate 100 datasets. We report the percentage of datasets where  optimal  solution is found (i.e. lower bound matches upper bound) in  Figure 5 a. To show how close the lower and upper bounds are, we report the average gap (the difference between the upper and the lower bounds, divided by the lower bound) in  Figure 5 b. We also report the average lower bound in  Figure 5 c, which somewhat reflects how complex the simulated networks are. We also give the average running time for each setting in  Figure 5 d. For five larger datasets, there are a small number of test cases that are too slow to run the full mode, and are excluded. The percentage of unfinished computation is usually one or two out of 100 datasets, except the cases with  n =30/ r =3.0/ K =5 (18% unfinished) and  n =30/ r =5.0/ K =4 (6% unfinished). This suggests the current practical range of the full mode of the  SIT  bound.
 Fig. 5. Performance of the  RH  bound and  SIT  bound (full mode), for 10, 20, 30, 40 and 50 taxa, number of trees K from 3 to 5, and reticulation level  r  at 1.0, 3,0 and 5.0. ( a ). The percentage of exact reticulation number found among 100 simulated datasets. ( b ) Average gaps (in percentage) between the  SIT  bound and  RH  bound (normalized by the  RH  bound), and the average  RH  bound is shown in ( c ). ( d ) The average running time (in seconds). The reticulation level r for left, middle and right figures is 1.0, 3.0 and 5.0, respectively. Horizontal axis is the number of taxa, and each curve in a figure is for a value of  K . As shown in  Figure 5 a,  PIRN  performs very well when the number of trees  K =3 or reticulation level r is small:  optimal  solution can be found for at least 80% of simulated datasets when  r =1.0 and  K =5. Even with higher reticulation level ( r =3.0) and larger number of taxa (say 50), still about 60% of datasets can be solved exactly when  K =3. As expected, as the number of taxa, reticulation level and the number of trees grow, fewer datasets can be solved to exact, and correspondingly,  Figure 5 b shows gaps between the  RH  and  SIT  bounds increase.  Figure 5 c shows the complexity of networks increases too. Nevertheless, the gaps are still relatively small in these cases. For the more difficult settings simulated (i.e. 30 taxa, high reticulation level and five trees as input), the gap is about 25%. Running time depends on the complexity of the networks.  Figure 5 d shows that  PIRN  is practical for data of medium size. Coarse mode of the  SIT  bound: we also test the coarse mode of our methods for larger data, as described in  Section 4.2 . The results are shown in  Figure 6 . The figure on the left shows the effects (on accuracy and running time) of increasing the number of taxa  n . The figure on the right shows the effects of having more trees (i.e. increasing  K  from three to nine) for 10 taxa and reticulation level 5.0. There is clear trade-off between the accuracy of solutions and efficiency. The coarse mode under-performs in terms of the quality of solutions, but is more scalable, especially when  K  increases. When the number of taxa increases, the coarse mode is likely to run faster than the full mode, but the difference is less significant.
 Fig. 6. Comparing the coarse (C) mode and the full (F) mode of the  SIT  bound. Left: fix  r =3.0 and  K =4, while varying  n . Right: fix  r =5.0 and  n =10 and vary  K . Both percentages of optimal solutions found and running time (in minutes for the left figure and seconds for the right one) are shown. GLPK: the CPLEX solver is used in the simulation. The GLPK version in general is less robust and can handle smaller data than the CPLEX version. Our experience shows that the GLPK solver can often solve for five trees with 30 taxa when reticulation level is low and fewer number of taxa when reticulation level is higher. The  RH  bound: we now compare the performance of the  RH  bound with the MAF bound. We note that the MAF bound is not easy to compute: finding the MAF of only two trees is known to be NP-hard. When data is small, the MAF bound can be computed (e.g. using ILP similar to that in (Wu,  2009 )). In  Table 1 , we compare the  RH  bound and the MAF bound for 100 datasets. Each data has 10 or 20 taxa and contain three to seven correlated trees. The trees are selected from the local trees for recombining sequences generated by a coalescent simulator, program ms (Hudson,  2002 ).
 Table 1. Compare the  RH  and MAF bounds for K trees K =3 K =4 K =5 K =6 K =7 n =10 RH  &gt; MAF 46 56 61 58 72 RH  = MAF 54 44 38 41 27 RH  &lt; MAF 0 0 1 1 1 100 ( RH  −  MAF / MAF ) 16.5 18.4 17.2 18.0 20.9 n =20 RH  &gt; MAF 65 66 68 70 - RH  = MAF 34 34 30 30 - RH  &lt; MAF 1 0 2 0 - 100 ( RH  −  MAF / MAF ) 12.6 11.9 12.1 12.8 - RH  &gt; MAF: number of datasets where the  RH  bound is larger than the MAF bound among 100 datasets. The average gaps (in percentage) between the  RH  and MAF bounds are also shown. 
 Table 1  shows that the  RH  bound outperforms the MAF bound in a majority of the simulated datasets and only very rarely the  RH  bound is lower than the MAF bound. In general, as the number of trees increases, the  RH  bound tends to outperform the MAF bound in both accuracy and running time. For example, for a dataset with 20 sequences and six input trees, CPLEX runs for over 11 h without reporting a solution (it found a solution of 11, but did not validate its optimality). In contrast, it only takes less than 1 minute to compute the  RH  bound of value 12 (higher than the MAF result). 5.2 Biological data To evaluate how well our bounds work for real biological data, we test our methods on a Poaceae dataset. The dataset was originally from the Grass Phylogeny Working Group (Grass Phylogeny Working Group,  2001 ). The dataset contains sequences for six loci: internal transcribed spacer of ribosomal DNA (ITS); NADH dehydrogenase, subunit F (ndhF); phytochrome B (phyB); ribulose 1,5-biphosphate carboxylase/oxygenase, large subunit (rbcL); RNA polymerase II, subunit β′′ (rpoC2); and granule bound starch synthase I (waxy). The Poaceae dataset was previously analyzed and rooted binary trees were inferred for these loci (Schmidt,  2003 ). Pairwise comparison were performed in (Bordewich  et al. ,  2007 ; Wu and Wang,  2010 ). Here, we provide in  Table 2  our results on estimating the reticulation number using multiple (three to five) trees. Only  shared  taxa of a set of trees are kept. Thus, the pairwise distances reported here are different from those in (Bordewich  et al. ,  2007 ; Wu,  2009 ). As shown in  Table 2 ,  PIRN  finds  optimal  solutions for both datasets with three trees, and computes lower and upper bounds that are  close  for the dataset with five trees.  Figure 7  shows the reconstructed reticulate network for these five trees. See  Supplementary Material  for a graphical display of the five grass trees. The network contains 13 reticulation events, and the lower bound is 11. Although the network may not be optimal, the gap between the lower and upper bounds is relatively small.
 Fig. 7. A reticulate network found by program  PIRN  for five trees of a grass dataset. Red (shaded) balls represent reticulation nodes. 
 Table 2. Results for the grass data Trees n D RH SIT Time rpoC2, waxy, ITS 10 1, 6, 6 7 7 1 s ndhF, phyB, rbcL 21 4, 5, 6 8 8 1 s ndhF, phyB, rbcL,rpoC2, ITS 14 11 13 26 min 38 s Both  RH  and  SIT  bounds are shown, as well as the running time .  n : the number of taxa.  D : pairwise distances. Remark : the following lists several aspects on the performance of  PIRN . (i) Simulation shows that  PIRN  is often able to find the exact reticulation number when  K  or  r  is small, even when the number of taxa increases to medium size (say 50). Moreover,  PIRN  can compute the  RH  and  SIT  bounds for a wide range of data, despite the fact that we do not currently have polynomial-time algorithms for computing the bounds. We achieve this with the help of integer linear programming. (ii) Computing the  RH  bound is often much faster and more scalable than the  SIT  bound. Experience shows that the ILP formulation for computing the  RH  bound is often very fast to solve and computing the pairwise reticulation distances usually takes less time than finding a good upper bound for all trees. The  RH  bound computation will also benefit from future improvements in computing the pairwise reticulate distances. The simulation results in this section are based on an earlier version of the method in (Wu and Wang,  2010 ) and speedup may be possible with the latest methods. (iii) The number of trees  K  and the similarity of tree topologies have impact on  PIRN 's optimality and running time. Using more powerful ILP solver (e.g. CPLEX) and/or more powerful machines may also help for more difficult cases. (iv) Finally, our general approaches can be applied to larger data by using efficient computable lower bounds of pairwise rSPR distances in computing the  RH  bound, and faster but less accurate heuristics to insert trees into a network. 
          Supplementary Material 
          
             
                [Supplementary Data] 
             
             
             
          
       </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Robust quantitative scratch assay</Title>
    <Doi>10.1093/bioinformatics/btv746</Doi>
    <Authors>Vargas Andrea, Angeli Marc, Pastrello Chiara, McQuaid Rosanne, Li Han, Jurisicova Andrea, Jurisica Igor</Authors>
    <Abstract>The wound healing assay (or scratch assay) is a technique frequently used to quantify the dependence of cell motility—a central process in tissue repair and evolution of disease—subject to various treatments conditions. However processing the resulting data is a laborious task due its high throughput and variability across images. This Robust Quantitative Scratch Assay algorithm introduced statistical outputs where migration rates are estimated, cellular behaviour is distinguished and outliers are identified among groups of unique experimental conditions. Furthermore, the RQSA decreased measurement errors and increased accuracy in the wound boundary at comparable processing times compared to previously developed method (TScratch).</Abstract>
    <Body>1 Introduction Since cell migration is significant in repair processes and disease advancement such as metastasis and cancer angiogenesis, it can be used as a parameter to describe the state of health of various cell lines ( Gebäck  et al. , 2009 ;  Zordan  et al. , 2011 ). Furthermore, quantification of cell migration can be used to measure the effect of drug treatments on the same cell line. The Wound Healing Assay (WHA) is used to quantify motility by monitoring the evolution of a wounded monolayer of cells. The experimental set-up consists of growing a confluent monolayer of cells under specific conditions (i.e. treated or not treated with a drug), then creating a wound or scratch on each layer/well, and then imaging all wounds multiple times over a period of time. Since the rate how wounds close may depend on conditions used, measuring wound areas from individual images are used to estimate migration rates, and thus quantify condition-specific cell motility. ( Jonkman  et al.  2014 ) provide extensive insight on experimental design considerations and workflow of the WHA measurements. In order to obtain reliable results, it is necessary to measure wound areas across multiple replicates of the same cell condition, and record multiple frames per replicate to determine whether changes are statistically significant. To make this task feasible, an automated system is required to process the high volumes of data and make measurements independent of image artefacts such as uneven illuminations, smudges or scratches on the well plate lid, in addition to some variation of initial wound width and shape. Sample images of wound areas are shown in  Supplementary Material . While previous methods by Gebäck  et al.  and Zordan  et al.  support automated WHA image analysis, they lack a final statistical output estimating the migration rates per condition, and lack an image set for validation, therefore depending on the acquisition parameters, these methods may or may not work. Therefore, we have developed the Robust Quantitative Scratch Assay (RQSA) algorithm using MATLAB 7.11.0 to address the challenges imposed by the large datasets, variability in image illuminations and implemented a statistical output for an improved quantification of cell motility using the wound healing assay. Furthermore, we provide image datasets for reproducibility and have developed a set of guidelines for future WHA image acquisitions. 2 Methods The algorithm requires MATLAB 7.11.0 (or GNU Octave) and uses the Statistics and Image Processing Toolboxes. In order to design and validate the RQSA algorithm a training and testing image datasets were acquired and are described in detail in Section 1.1 of the  Supplementary Material . The image and data processing comprises measuring individual open wound areas for each well using a sequence of morphological and spatial filters (Gonzales, 2003;  http://www.mathworks.com ). Then the resulting  open wound area vs. frame  curve is fitted to a polynomial curve using the Least Squares method in order to calculate migration rates, coefficients of determination ( R 2 ), and to label the temporal behavior for each well. The estimated migration rates for all wells in a single condition are grouped, then outliers are flagged and discarded before calculating the average migration rate for each experimental condition. Last, the average migration rates are compared across all experimental conditions. The statistical analysis is output as a text file; Section 1.3 in  Supplementary Material  shows sample open wound area curves, sample output text file for a single condition and describes the process for outlier detection. 3 Results The robustness and improved performance of the RQSA algorithm was evaluated over a ‘Robustness set’—a set of images ( N  = 268) with various features, such as high/low image illumination, resolution, cell confluence, and stray cells in the wound area.  Open Wound Areas  and  Wound Estimate  were measured with the RQSA algorithm and TScratch were compared to manual measurements,  Figure 1  shows the error in  Open Wound Area  for this subset. Section 2.1 in  Supplementary Material  describes these metrics of comparison with more detail and discusses the robustness of the algorithm. A paired t-test was performed on the errors in  Open wound area  and errors in  Wound estimate  using both methods, where resulting p-values ( P  &lt; 0.05) determined that there is a statistical difference between both methods. Overall RQSA showed better representation of the wound boundaries resulting in a more accurate estimate of migration rates over all conditions.
 Fig. 1. Measurement errors resulting from TScratch (dashed line) and RQSA (solid line) 
 Both methods were tested using a validation set, and results were compared by determining  R 2  from the fitted curves in both experimental conditions. The non-treated condition with RQSA shows an  R 2  value centered at 1.0, suggesting that the  open wound areas vs. frame  from the RQSA algorithm yield a better description of the temporal behavior of the wound, and shows less variability across time points. Meanwhile, the lower  R 2  values of the TScratch polynomial fits suggest that measurements fluctuate highly from one frame to the other. The RQSA was run using an additional independent set with a different cell line and treatments, to demonstrate its robustness. Results of both datasets with statistical analysis including average migrations rates, uncertainties, identified possible well outliers, and are further discussed in Section 2.2 of  Supplementary Material . Future directions for the RQSA include the incorporation of cell segmentation routines to provide a morphological and proliferation assessment on individual cells, and implementation of parallel computing techniques to decrease processing times and increase efficiency. Conflict of Interest : none declared. 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>ISOLATE: a computational strategy for identifying the primary origin of cancers using high-throughput sequencing</Title>
    <Doi>10.1093/bioinformatics/btp378</Doi>
    <Authors>Quon Gerald, Morris Quaid</Authors>
    <Abstract>Motivation: One of the most deadly cancer diagnoses is the carcinoma of unknown primary origin. Without the knowledge of the site of origin, treatment regimens are limited in their specificity and result in high mortality rates. Though supervised classification methods have been developed to predict the site of origin based on gene expression data, they require large numbers of previously classified tumors for training, in part because they do not account for sample heterogeneity, which limits their application to well-studied cancers.</Abstract>
    <Body>1 INTRODUCTION While most cancerous tumors present at their cancer site of origin (CSO), ∼4% of all new tumors do not (American Cancer Society,  2001 ). Without knowledge of this site, treatment regimens are highly limited in their specificity and result in high mortality rates (Blaszyk  et al. ,  2003 ; Shaw  et al. ,  2007 ). In an effort to identify CSO, patients routinely undergo extensive clinical examination, radiology and histoimmunological testing (Hainsworth and Greco,  1993 ). However, these drastic interventions fail to identify the site of origin more than half of the time (Blaszyk  et al. ,  2003 ). Gene expression profiling provides a precise, high-resolution molecular fingerprint of a tumor that also offers insight into the underlying transcriptional activity that gave rise to its aberrant behavior (Liotta and Petricoin,  2000 ). To date, a number of supervised classification methods have been used to categorize tumors according to their site of origin based on gene expression profiles, including support vector machines (Ramaswamy  et al. ,  2001 ; Su  et al. ,  2001 ; Tothill  et al. ,  2005 ), decision trees (Dennis  et al. ,  2005 ; Shedden  et al. ,  2003 ),  K -nearest neighbors (Bridgewater  et al. ,  2008 ; Giordano  et al. ,  2001 ; Horlings  et al. ,  2008 ), neural networks (Bloom  et al. ,  2004 ) and others (Buckhaults  et al. ,  2003 ; Dennis  et al. ,  2002 ; Varadhachary  et al. ,  2008 ). These studies all share a similar three-step strategy: transcriptional profiles of many tumors with known sites of origin are used to identify individual marker genes whose expression levels discriminate cancers of different origin; then the expression levels of these marker genes in each tumor are used to train a classifier that is subsequently used to classify new tumors not previously labeled with a site of origin. These microarray-based models have shown great diagnostic potential for identifying the site of origin of patients with carcinomas of unknown primary origin: accuracies of &gt;80% were commonly reported for some types of carcinomas. However, because these methods are supervised classification methods, they require large amounts of transcriptionally profiled tumors with identified origin upon which to train. While this data may be available for mature tumors from common sites of origin, there are many less well-characterized cancers or poorly differentiated tumors that often have very little or no data available upon which to train. Reported prediction accuracy on these underrepresented tumors is little better than random performance (Ramaswamy  et al. ,  2001 ; Shedden  et al. ,  2003 ; Su  et al. ,  2001 ). Classifier performance also depends critically on the CSO-specific marker genes identified in the preprocessing step, making downstream analysis highly sensitive to the marker set (Tothill  et al. ,  2005 ). Unsupervised methods that neither rely on previously collected training data nor prescreen for marker genes are therefore of high value. However, to the best of our knowledge, such methods are not currently available. It is often of interest to not only identify the site of origin, but also to identify the genes differentially expressed in the cancer cells with respect to the site of origin. Ideally, the tumor expression profile can be directly compared with that of the CSO to identify those differentially expressed genes. However, tumors are not homogeneous masses of cancer cells, but are mixtures of cell populations with varying levels of heterogeneity, not only between tumors of the same cancer but also even across the samples from the same tumor (Dennis  et al. ,  2005 ). Contaminating cell populations can include surrounding healthy tissues (such as the site of origin or the local site of a metastatic tumor) and supporting stroma (Masters and Lakhani,  2000 ). Sample heterogeneity contributes significantly to the large diversity of expression profiles observed even from similar tumor samples, and in many cases contaminating non-cancer cells can dominate the expression profile (Golub  et al. ,  1999 ; Liotta and Petricoin,  2000 ; Reya  et al. ,  2001 ), as illustrated in  Figure 1 . While methods exist for de-convolving heterogeneous expression profiles into their individual component profiles and inferring the so-called mixing proportions (also known as coefficients), based on techniques like Independent Components Analysis (ICA) (Hyvarinen,  2001 ; Lahdesmaki  et al. ,  2005 ; Venet  et al. ,  2001 ), they have been developed independently of the models for identifying CSO and therefore are currently applied as a preprocessing step.
 Fig. 1. Multiple samples taken from even the same tumor can be composed of different mixing proportions of component sources, giving rise to significantly different tumor expression profiles compared with the expression profile of the homogeneous cancer cell population. Methods for de-convolving sample heterogeneity and removing the contributions of non-cancerous cell populations to the measured expression profile aim to re-construct the expression profile of the homogeneous cancer cell population. 
 The advent and rapidly decreasing cost of high-throughput sequencing (HTS) methods for expression profiling promises much higher reproducibility and a wider dynamic range of detectable gene expression than microarrays (Marioni  et al. ,  2008 ; Mortazavi  et al. ,  2008 ). HTS methods are quickly becoming feasible for highly accurate characterization of the transcriptome profile of tumors. However, the digital counting of sequence tags in HTS methods leads to a different observation of noise process compared with the analog measurement of probe intensity in microarrays. This change requires an update to the statistical models used to analyze these data. In this article, we present ISOLATE, a model for the Identification of Sites of Origin by LATEnt variables. ISOLATE is the first method that simultaneously identifies sites of origin in an unsupervised fashion and addresses sample heterogeneity using HTS cancer expression profiling. ISOLATE is designed to achieve three goals: identification of the site of origin from a set of profiled candidate sites, de-convolution of heterogeneous expression profiles into their individual components and identification of differentially expressed genes. We demonstrate on both synthetic and clinical datasets that ISOLATE achieves higher accuracy on all of these goals than a similar ICA-based unsupervised strategy that mirrors existing tools. The high accuracy levels achieved by ISOLATE demonstrate the feasibility of unsupervised methods for complementing traditional immunohistological and supervised classification models for identifying the site of origin of and characterizing tumors from carcinomas of unknown primary origin. 2 APPROACH OVERVIEW ISOLATE is designed to achieve three goals, illustrated in  Figure 2 : identification of the CSO, identification of the differentially expressed genes in the homogeneous cancer cell population and characterization of the cellular composition of each heterogeneous tumor sample (by estimation of the mixing proportions of their component cell populations). We compared ISOLATE with an ICA-based strategy that can be applied using existing tools to address the three challenges. In this study, we use Latent Dirichlet Allocation (LDA) (Blei  et al. ,  2003 ) to implement the ICA strategy. LDA is an equivalent model to ICA (Shashanka  et al. ,  2008 ) but with an observation noise model more appropriate for digital HTS data.
 Fig. 2. Outline of the ISOLATE and LDA methods for de-convolving cancer gene expression data and identifying the site of origin. The input to each method consists of the expression profile of a heterogeneous tumor sample(s), as well as the Source Panel representing previously profiled cell populations that may act either as contaminants or as candidate sites of origin. Each method performs three tasks: identification of the site of origin, identification of those genes differentially expressed in the cancer cells and characterization of the cellular composition of each heterogeneous sample by estimating the mixing proportions of each component cell population. The ICA-based strategy operates serially by first de-convolving the heterogeneous sample without constraining the cancer profile to be derived from the Source Panel, then predicts the site of origin and differentially expressed genes. This is in stark contrast to ISOLATE, which solves all three problems cooperatively. 
 Both ISOLATE and LDA model the expression profile of a heterogeneous tumor sample as a weighted mixture of expression profiles of ‘source’ cell populations (representing candidate sites of origin or contaminants), all of which have been previously characterized except for the homogeneous cancer cell population. The set of source cell populations are herein called the ‘Source Panel’. Candidate sites of origin and potential contaminating cells can be treated similarly in the context of LDA and ISOLATE and are hence both referred to as sources. ISOLATE differs from LDA in that it explicitly models the similarities in expression profile between the cancer cell population and the site of origin by representing the homogeneous cancer expression profile as a sparsely perturbed version of the profile of its site of origin. Tumor cells display functional, developmental and morphological similarities to their site of origin (Lobo  et al. ,  2007 ; Sell and Pierce,  1994 ). This similarity is also reflected at the gene expression level, both between the primary tumor and the site of origin (Khan  et al. ,  2001 ; Ross  et al. ,  2000 ), and between the primary and metastatic tumor (D'Arrigo  et al. ,  2005 ; Weigelt  et al. ,  2003 ). By explicitly modeling the cancer cell expression profile as a perturbation of the site of origin profile, our model is a more precise representation of cancer that naturally leads to the identification of differentially expressed genes as those whose expression was perturbed to produce the tumor expression profile. ISOLATE then uses the estimate of the homogeneous cancer profile in conjunction with the Source Panel to decompose each tumor sample. A key feature of ISOLATE is that it recognizes the interdependence of the solutions of all three goals and iteratively solves all of them simultaneously. In contrast, the ICA strategy first iteratively decomposes the tumor samples while estimating the profile of the homogeneous cancer cell population. Then it compares the estimated homogeneous cancer profile to the Source Panel and identifies the parent site as the most similar profile, and finally identifies differentially expressed genes by comparing the estimated cancer profile to that of the identified site of origin. This makes the ICA-based strategy for identifying the site of origin sensitive to imperfect de-convolution of the tumor expression profiles and often leads to misidentification of the site of origin as the surrounding tissue. The following sections describe how we generated the synthetic datasets and collected and processed the clinical datasets that we used to test our model. We also describe statistical inference with ISOLATE and LDA. 3 METHODS 3.1 Synthetic data collection We measured the performance of ISOLATE on a comprehensive set of synthetic data for which the correct answer is known. Our strategy for generating data is shown in  Figure 3  and summarized below, with more details provided in following sections. Each experiment is defined by five parameters: the number of genes whose expression is perturbed in the cancer cells, their multiplicative perturbation factor, the number of heterogeneous tumor samples profiled, the number of sources in the Source Panel and the level of biological variability of the expression profiles of the same sources between different cancer patients. First, using human kidney and liver data from Marioni  et al.  ( 2008 ), we generate expression profiles for each source, both for the (a) training profiles that make up the Source Panel, and (b) for a template healthy patient. From the template healthy patient, (c) we randomly select one component source as the site of origin, from which we perturb the expression profile to construct a cancer cell expression profile. The original template of healthy source expression profiles together with the cancer cell expression profile make up a template cancer patient, from which we (d) generate one or more unique cancer patients by adding variability to the template cancer patient independently for each cancer patient. (e) One heterogeneous tumor sample is generated from each individual using a unique set of mixing proportions to combine the source profiles of the cancer patient. Finally, we use the Source Panel and the heterogeneous tumor samples as input into the LDA and ISOLATE models, to (f) identify the CSO, (g) de-convolve the heterogeneity of each tumor sample, and identify differentially expressed genes.
 Fig. 3. Overall experimental strategy for generating the heterogeneous tumor samples from three sources (i.e. candidate sites of origin) to input into the LDA and ISOLATE models. The sources color-matched between the Source Panel and the template healthy patient differ only by technical variability in their expression profiles. Yellow represents cancer cells, while orange represents the site of origin. 
 3.1.1 Dataset Human liver and kidney transcriptome profiling data from a single human male was obtained from Marioni  et al.  ( 2008 ) who sequenced each tissue seven times, split across two runs of an Illumina Genome Analyzer and at two concentrations, 1.5 pM and 3 pM. All reads were mapped to the genome using the Illumina ELAND algorithm, and only uniquely mapped reads were retained. A gene copy number is computed by counting the number of reads mapped to each known transcript, then computing the median number of copies for each gene over all of its respective transcripts. We discarded all genes for which there was not even one copy in all of the runs of both tissues, leaving 13 061 genes. Gene abundances (also called the expression profile) were computed from gene copy numbers by dividing each copy number by the sum of all gene copy numbers. 3.1.2 Generating a new source expression profile We first applied a differential expression test (Lu  et al. ,  2005 ) to identify the top 40% of all genes that were most likely to be constitutively expressed across all of the kidney and liver datasets and deemed these to be candidate house-keeping genes (Zhu  et al. ,  2008 ). We then randomly selected two runs from either the kidney or liver datasets, and permuted the expression levels of their non-house-keeping genes randomly in the same order. One run is used in the Source Panel ( Fig. 3 a) as previously profiled abundances in LDA and ISOLATE, while the other is used in the template healthy individual ( Fig. 3 b). 3.1.3 Generating a cancer cell expression profile To generate a cancer cell expression profile given the expression profile of the site of origin, the number of genes to perturb and their perturbation factor, we first randomly selected the set of genes to become differentially expressed, then randomly perturbed the abundances of each gene in that set either up or down (with equal probability), then renormalize the abundances to sum to 1 to make gene abundances correspond to parameters of a multinomial distribution. This cancer expression profile and the healthy tissue profiles in the template healthy individual combine to make the sources in the template cancer patient ( Fig. 3 c). 3.1.4 Generating a cancer patient from the template cancer patient We use the template cancer patient to obtain a cancer patient profile by adding biological variability to each source expression profile. We represent biological variability by resampling the expression levels of a fraction of genes from the entire set of expression levels observed in that source's original expression profile. The expression profile is subsequently rescaled to sum to 1. 3.1.5 Generating a heterogeneous tumor sample from a cancer patient We first determine what proportion of each source will compose the tumor sample, then we generate the sequence reads that are observed in the sample. For the tumor sample  d , the mixing proportions of the sources  θ d  are drawn from a Dirichlet distribution with parameters  α  = {α s c , α s 1 , α s 2 ,…, α s S }, where  s c  indicates the cancer source. In our experiments, for all non-cancer sources  i  ≠  c , α s i  = 1, and α s c  = 3 by default. Larger values of α s c  will result in tumor samples containing larger proportions of cancer cells. Once the mixing proportions  θ d  are generated, for each transcript read to generate, we randomly select a source using the mixing proportions  θ d , then randomly select a transcript from which to generate a read using the multinomial distribution specified by the expression profile of the chosen source. Each tumor sample was generated with 1 675 078 reads, the average number of reads collected per experiment in Marioni  et al.  ( 2008 ), though the results were not sensitive to the total number of reads generated per tumor sample (data not shown). 3.2 Clinical data processing Both the ISOLATE and LDA strategies require a fully profiled Source Panel and heterogeneous tumor samples, but owing to the current lack of such data available, we took advantage of the vast quantities of microarray data available and chose to digitize such datasets to make them compatible with our model. We downloaded a total of 93 tumor expression profiles from Su  et al.  ( 2001 ), consisting of 10 kidney, 6 liver, 24 lung, 23 ovary, 6 pancreatic and 24 prostate-originating tumors collected using Affymetrix U95a GeneChip arrays. Following the procedure of Su  et al.  ( 2001 ), for each tumor sample, raw intensity values were thresholded at 20. Mappings from the probe identifier to Ensembl gene identifiers were downloaded from the Affymetrix web site, and multiple probes matching the same Ensembl gene identifier were averaged together to produce a single measurement for each gene. The resulting array intensities were rounded to the nearest integer and treated as transcript counts from a HTS experiment. As a Source Panel, we downloaded a separate set of microarray data collected using Affymetrix Human Genome U133A arrays (Su  et al. ,  2004 ), giving us a healthy profile version of those same six tissues. Intensities for replicate array measurements were averaged together for each respective tissue, and using the provided annotation files, each probe was mapped to its respective Ensembl gene identifier, and multiple probes matching the same Ensembl gene identifier were averaged together. The total set of common genes profiled in the Source Panel and the tumor profiles were 8667 genes. For these 8667 genes, their raw averaged intensities in each source of the Source Panel were divided by the total intensity measured to produce a proper multinomial distribution over the profiled genes. 3.3 Inference with the LDA model The input to the LDA model (Blei  et al. ,  2003 ), for both the synthetic and clinical datasets, consists of the expression profiles over all the genes in each source. Each profile is represented by a vector from the set {β s } s =1 S  that contains one vector for each of the  S  sources from the Source Panel. Also input to the LDA model are  D  sets of reads { t d , n } d =1 D  originating from transcriptome profiling experiments of  D  heterogeneous tumor samples that each generate  N d  reads. LDA estimates the expression profile  β s c  of all genes in the cancer source  s c  and performs de-convolution by inferring hidden variables { z d , n } (one for each read  t d , n ) that indicate from which of the  S  + 1 sources ( S  from the Source Panel, and 1 from the cancer source) the transcript most likely originates. In doing so, LDA estimates the fraction of each cancer sample  d  (the mixing proportions), θ d , s , coming from each of the  S  + 1 sources. The full model is specified below:
 (1) 
 (2) 
 (3) 
The model was trained using the same variational Expectation Maximization (EM) framework used in Blei  et al.  ( 2003 ) with 100 iterations, and rerun  S  times with random parameter initializations. The initialization that resulted in the highest log likelihood of the data is chosen. The model parameters estimated include  α ,  θ d  for all tumor samples  d , and cancer abundances  β s c . Using the output of LDA, we predict the site of origin by choosing the source (from the Source Panel) whose expression profile has the least Kullback–Liebler divergence from the estimated cancer expression profile  β s c . To rank genes in order of differential expression, we applied a two-class differential expression test (Lu  et al. ,  2005 ) to compare the expression profile of the predicted site of origin against the set of reads the cancer cells are responsible for in each tumor sample ( z d , n  =  s c ), and sorted the genes based on the resulting  P -value. Lastly, the mixing proportions (heterogeneity) of each sample  d  are estimated directly from the learned parameters of the model,  θ d . 3.4 Inference with the ISOLATE model ISOLATE maintains the same probabilistic framework as LDA [Equations ( 1 –  3 )], but introduces the following key constraints on the learned parameters  β s c , where β :, g  is a column vector of abundances of gene  g  across all  S  non-cancer sources:
 (4) 
 (5) 
 (6) 
 ω  is a ( S  × 1)-dimensional parameter, where ω s  = 1 denotes that source  s  is the site of origin. ρ g  is the estimated perturbation (multiplicative) factor that describes how much the cancer cells perturb the expression of gene  g  relative to the site of origin described by  ω . Since we expect many genes to maintain similar expression levels to that of the CSO, we put a Gamma prior on ρ g  [Equation ( 5 )], with mean  E [ρ g ] = 1 to emphasize that we expect many genes to not have perturbed expression. ISOLATE uses the same variational EM framework as LDA (Blei  et al. ,  2003 ) with 100 iterations, and rerun using  S  different initializations to test different candidate CSO. There is exactly one initialization per source  s  where the value ω s  is set to 1, and the remaining entries set to 0. The initialization that resulted in the highest log likelihood of the data is chosen. To rank genes in order of differential expression, we sorted the genes based on the distance of ρ g  from the value 1. That is, the farther ρ g  is from 1, the more perturbed its abundance is from that of the site of origin. Finally, mixing proportions of each sample  d  is estimated directly from the learned parameters of the model,  θ d . 3.5 Performance metrics The error rate in identifying the primary site of origin is the fraction of experiments in which the CSO was incorrectly identified. For the synthetic datasets, the reported error is averaged over the 20 datasets generated for each specific setting of the parameters. The model heterogeneity error is computed by averaging, over all tumor samples, the mean absolute error of the mixing proportions  θ d  of the cancer cells and the true site of origin. We only measure the error with respect to these two sources because we found that almost all of the error in the mixing proportion estimates is from these two sources. Finally, we assess the error of the identification of differentially expressed genes for the synthetic datasets by using the ranks of the genes (in order of differential expression as defined by each model) and our knowledge of which genes are truly differentially expressed to compute an area under the receiver operator curve (ROC), where larger values correspond to higher accuracy. Error is computed as [1 − (Area under ROC)]. 4 RESULTS 4.1 Synthetic datasets We have evaluated the relative performance of ISOLATE and LDA as a function of realistic parameter settings to demonstrate their robustness to different conditions. We also compared naiveLu, a simple method for identifying differentially expressed genes by simply applying a differential expression test directly without accounting for sample heterogeneity. We do these comparisons because of the dearth of clinical data and the difficulties associated with defining gold standards therein. We are also able to query a larger variety of experimental conditions. In the absence of analytical estimates of performance, which are likely impossible due to the complexity of our models, these comparisons provide the best support for our claims of improved performance over LDA. We varied the following parameters: the number of differentially expressed genes, the perturbation factor by which their expression levels are differentially expressed in cancer, the number of heterogeneous tumor samples, the number of sources in the Source Panel, and the (biological) variability between our profiled Source Panel and the corresponding profiles used to generate the tumor samples (see  Section 3 ). This variability represents the expected differences between the normal source profiles in our Source Panel, which will likely come from different individuals, and the corresponding source profiles for the patient from which the tumor sample is drawn. Biological variability, which could represent either biological variation or technical noise, is a key parameter because it limits our ability to detect differentially expressed genes, as seen below. In the following, we vary only a single parameter from the default; the default parameters we use are 100 perturbed genes, 3 tumor samples, 10 sources, a perturbation factor of two, perturbation scale prior κ = 10 [see Equation ( 5 )], and a biological variability of 0.16 (16%), which empirically leads to ∼14% of genes differentially expressed, as measured by Lu  et al.  ( 2005 ). This level of differential expression between simulated individuals is similar to reported variation between unrelated individuals (Sharma  et al. ,  2005 ). 4.1.1 Identification of differentially expressed genes One of the principal objectives of identifying CSO is to identify genes that are differentially expressed in the cancer cell population with respect to healthy cells of the site of origin. We tested each models' ability to identify the differentially expressed genes, defined as those genes whose expressions were perturbed to differentiate the cancer source from the site of origin source, and measured the performance by the area under the ROC curve (see  Section 3 ).  Figure 4 a demonstrates that ISOLATE consistently achieves higher accuracy at identifying differentially expressed genes than LDA across all three parameters at almost all settings. Surprisingly, the performance of both LDA and ISOLATE seem to stay constant despite increasing the number of tumor samples available.  Figure 5 a illustrates that beyond a variability level of 15%, increasing the amount of data does not improve ISOLATE performance. Because 15% is near the average variability between unrelated individuals (Sharma et al,  2005 ), this result suggests that ISOLATE's identification of differentially expressed genes can be improved by analyzing multiple tumor samples from the same individual but not necessarily by analyzing multiple samples from different individuals. Both ISOLATE and LDA improve performance as the perturbation factor increases—a direct result of its increasing differentiation from the site of origin source and hence easier de-convolution of sample heterogeneity—though ISOLATE improves at a much faster rate. The performance of naiveLu, which does not address heterogeneity, illustrates that de-convolution clearly improves the identification of differentially expressed genes. The ISOLATE performance gain over LDA is not just simply due to a difference in the specific method that ISOLATE uses to compute differential expression: we computed differential expression using the same method as for LDA (ISOLATE-Lu) and see that its performance is still better than LDA in many cases.
 Fig. 4. Performance of ISOLATE and LDA on synthetic datasets. Each row represents a different parameter tuned: the number of heterogeneous tumor samples, the number of sources (non-cancer) in the Source Panel and the perturbation factor of the differentially expressed genes (manipulating the number of perturbed genes within the range of 50–500 genes did not result in changes in performance to either model and are not shown). Each column represents a different performance metric applied to each dataset. ( a ) Differential expression error, ( b ) origin error and ( c ) heterogeneity error are as defined in  Section 3 . Two additional models are plotted in (a): ISOLATE-Lu is the performance achieved when applying the same method as ICA for identifying differentially expressed genes (Lu  et al. ,  2005 ) to the output of the ISOLATE model, and naiveLu is the performance achieved when directly comparing the heterogeneous tumor expression profiles to the site of origin to identify differentially expressed genes, without accounting for sample heterogeneity. 
 Fig. 5. Performance of ISOLATE and LDA on synthetic datasets under different biological variability conditions. Each column represents a different performance metric, and each row a different model (top, ISOLATE; bottom, LDA). Here, we co-vary the biological variability added to each tumor sample independently, and the number of tumor samples made available to each model. The performance metrics of ( a ) differential expression error, ( b ) origin error and ( c ) heterogeneity error are as defined in  Section 3 . 
 4.1.2 Identification of CSO Figure 4 b compares LDA and ISOLATE based on how often they are able to correctly identify the site of origin. ISOLATE consistently outperforms LDA across all datasets. Most importantly, while ISOLATE is robust against the number of sources in the Source Panel, the performance of LDA diminishes rapidly after six sources. This makes LDA and other ICA-like techniques impractical for considering many potential candidates for the CSO, an important feature given the potentially large set of candidate CSO to query. The difference between ISOLATE and LDA also illustrates the improvement in CSO identification, sometimes as staggeringly as 70%, achieved by solving for both cell population mixture coefficients and CSO simultaneously within the same framework. From  Figure 5 b, we see that ISOLATE achieves high accuracy at identifying CSO under even high variability conditions, while LDA accuracy varies quite widely even under low variability conditions. ISOLATE is therefore able to capture the underlying signal of the site of origin even despite large amounts of noise in the expression profiles. Most importantly, even when looking at very small sets of tumor samples, ISOLATE performs as well as it does with many more samples, an important feature given the cost of profiling tumors in a diagnostic setting. 4.1.3 Correction of sample heterogeneity Figure 4 c illustrates that when considering Source Panels containing fewer than 10 profiles, ISOLATE achieves better de-convolution of heterogeneity than LDA. However, their performance is nearly identical regardless of the perturbation factor applied to the differentially expressed genes, suggesting that the amount of tumor sample data is far more important for de-convolution than the difference in expression profiles of the cancer cells and the site of origin. As expected, as the number of tumor samples increases, both LDA and ISOLATE increase in performance.  Figure 5 c illustrates that ISOLATE achieves better accuracy for the same number of data points and level of variability, although it takes more samples for a given level of biological variability than LDA to achieve its maximum performance. Most of the performance loss in LDA appears to be due to confusion of the contributing expression signatures from the cancer cells and the site of origin source ( Fig. 6  illustrates an example). This is a problem that ISOLATE is able to mitigate because of the constraints it places on the learned cancer expression profile.
 Fig. 6. An example of heterogeneity error for a single tumor sample. Here, we illustrate a representative tumor sample whose mixing proportions of component sources were estimated by ISOLATE (top row) and LDA (bottom row), compared with the actual values (middle row). Each of the four columns on the left represent a source from the Source Panel (Sources 1–3, as well as the site of origin source), while the right-most fifth column represents the cancer cells. The area of each square is proportional to the fraction of the sample composed of that particular source. Whereas ISOLATE estimated mixing proportions fairly close to the truth, the LDA estimate of the CSO and cancer sources were quite erroneous. 
 4.2 Clinical dataset We used ISOLATE and LDA to predict the site of origin of 93 tumor samples from Su  et al.  ( 2001 ). Heterogeneity and differential expression error could not be measured on these datasets as the true values are not known. Each tumor, regardless of its site of origin, is predicted independently of all other tumors, to reproduce clinical diagnostic conditions. As a benchmark besides LDA, we constructed a predictor that chooses the source from the Source Panel whose Kullback-Leibler (KL)-divergence is least with respect to the tumor sample's expression profile (KL). To set the perturbation scaling prior κ of Equation ( 5 ), we tried several values of κ (10 0 , 10 1 , 10 2 , 10 3 , 10 4 , 10 5 ), and performed 2-fold cross-validation by choosing the κ that maximized performance of half the data, in order to predict the other half of the dataset, and vice versa. The optimal value of κ was 10 5  for both halves of multiple splits of the data, and so was used to generate the results shown in  Figure 7 . Over the entire dataset of 93 tumor samples, ISOLATE achieves the highest performance of 65.59% accuracy, compared with 52.69% of both LDA and the KL measure. On a class-by-class basis, ISOLATE ties or performs better than LDA and ISOLATE in the larger classes, only performing worse when predicting tumors originating from the pancreas, the smallest class. Note that though previously reported performance of supervised classification methods is higher on some of these cancer types, ISOLATE achieves the observed performance considering each tumor separately without reference to any of the other tumor samples and without any training, mirroring clinical settings for the CSO identification of tumors underrepresented among previously profiled samples.
 Fig. 7. The performance of ISOLATE, LDA and another Kullback-Leibler (KL) divergence-based measure on the clinical dataset of 93 tumor samples. Each sample is predicted independently of all other samples in the dataset. The number of samples in each class is shown beside the class name, and classes are in decreasing order of size, from left to right, with the overall performance shown in the leftmost column. The black line shows random performance. 
 5 DISCUSSION We have developed ISOLATE to provide a molecular diagnostic tool to aid in identifying the site of origin of tumors of poorly characterized cancers, situations in which classical supervised models perform poorly. ISOLATE simultaneously de-convolves tumor expression profiles, and identifies the CSO and genes differentially expressed in the cancer cells, three tasks that were previously solved independently. Our experiments detail the performance of ISOLATE under a wide range of realistic experimental conditions for synthetic and digitized clinical microarray data, showing that solving all three tasks simultaneously leads to greater predictive performance than solving them individually. ISOLATE, unlike previous methods for classifying cancers of unknown primary origin, is an unsupervised classification algorithm, which provides it with several inherent advantages. It does not require a large training set of tumors of known primary origin, and in our clinical validation we only use data from a single tumor, a particularly important feature given the difficulty and cost of procuring many high-quality tumor samples in a diagnostic setting. Because it is an unsupervised algorithm, ISOLATE's performance is also less sensitive to the number of candidate sites of origin, as suggested by our synthetic data validation, in contrast to supervised learning methods that have difficulty with more than 10 classes (Su  et al. ,  2001 ). By its construction, ISOLATE is also less prone to overfitting than supervised learning algorithms, and as such, does not require a prescreening stage to identify marker genes upon which cancers could be discriminated. Despite our use of microarray data for our clinical validation, we recommend that ISOLATE be used exclusively with HTS expression profiles, which we believe support more accurate tumor diagnosis. HTS methods promise a substantial reduction in sample-to-sample variability, which our synthetic data-based validation shows limits the accuracy. Also, because HTS measurements are not probe-based, they are both less platform-specific, allowing easier integration of data from multiple labs, and less sensitive to polymorphisms in transcript sequence that are common in highly polymorphic cancer genomes. For these reasons, we have tailored ISOLATE's statistical model for HTS gene expression data. The successful application of ISOLATE or other expression-based models will depend on the availability of expression profiles for a wide range of human tissues, in order to consider them as potential sites of origin. With the costs of high-throughput expression profiling dropping quickly and the number of studies using these technologies to profile tumors increases (Jones  et al. ,  2008 ; Parsons  et al. ,  2008 ), soon it will be practical to collect a compendium of expression data from many of the individual tissues of humans along with multiple tumor samples, as is currently available for microarrays (see, e.g. Su  et al.  ( 2004 ). Molecular-based diagnostic tools for identifying cancer sites of origin represent an important class of tools that can potentially facilitate faster, more accurate diagnoses leading to the successful identification of primary sites. ISOLATE will be an invaluable tool for exploring new, uncharacterized cancers of unknown primary origin for which little expression data are available, or clinically ambiguous samples for which more traditional models cannot classify with high accuracy. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Tablet—next generation sequence assembly visualization</Title>
    <Doi>10.1093/bioinformatics/btp666</Doi>
    <Authors>Milne Iain, Bayer Micha, Cardle Linda, Shaw Paul, Stephen Gordon, Wright Frank, Marshall David</Authors>
    <Abstract>Summary: Tablet is a lightweight, high-performance graphical viewer for next-generation sequence assemblies and alignments. Supporting a range of input assembly formats, Tablet provides high-quality visualizations showing data in packed or stacked views, allowing instant access and navigation to any region of interest, and whole contig overviews and data summaries. Tablet is both multi-core aware and memory efficient, allowing it to handle assemblies containing millions of reads, even on a 32-bit desktop machine.</Abstract>
    <Body>1 INTRODUCTION The advent of next-generation sequencing (NGS) technologies such as Roche 454 (Margulies  et al. ,  2005 ) and Illumina Solexa ( http://www.illumina.com/sequencing ) has brought about a need for fast, efficient and user-friendly tools for analyzing the outcome of sequencing runs. This includes visualization software for viewing the resultant assemblies or alignments, for example, Consed (Gordon  et al. ,  1998 ), Hawkeye (Schatz  et al. ,  2007 ), EagleView (Huang and Marth,  2008 ), MapView (Bao  et al. ,  2009 ), SAMtools' tview (Li  et al. ,  2009 ) and Maqview ( http://maq.sourceforge.net/maqview.shtml ). All assembly visualization packages face the following challenges when dealing with NGS data: processing a very large number of reads, providing high-quality rendering and navigation of assembled reads, and supporting a widening range of assembly formats. Additionally, as analysis and interpretation of the data moves from large-genome centers to smaller laboratories, there is an increasing need for biologist-friendly software that has an intuitive user interface, is available for a range of common desktop platforms and has no complicated installation dependencies. With these features in mind, we have developed Tablet, a lightweight, high-performance and memory efficient assembly viewer. Tablet is aimed at users of all abilities and combines simple installation on a desktop machine with ease of use and a visually rich interface. The application supports both single and multi-core processor architectures and will scale its performance according to the number of processor cores available. 2 FEATURES Tablet can import data from ACE, AFG, MAQ and SOAP assembly formats (with preliminary support for SAM), and can handle both 454 and Solexa data. Its visualizations are split into several areas; the main display provides a view of a single contig at a time, with reads aligned against their consensus sequence. Reads are colored according to nucleotide type and subtle use of gradients and color choice allow visual structure to be maintained even when fully zoomed out. Tablet will lay out the data in either packed (showing as many reads per line as possible without overlap) or stacked (showing one read per line) formats, and allows the user to switch instantly between them. A sortable list, containing all available contigs shows contig lengths as well as numbers of reads and annotation features, and can be dynamically filtered by any of its fields. Continuous zooming of the entire contig in real time is supported by means of a slider, and there is also an option for varying the contrast between variant and non-variant nucleotides which adjusts the brightness used to display read bases that differ from the consensus, thus aiding identification of potential single nucleotide polymorphisms (SNPs) or sequencing errors. An overview window located above the consensus can display either a scaled-to-fit summary of all the reads in a contig, or a coverage graph which shows the read coverage along the entire length of the contig independent of the current zoom level. Navigation within a contig is catered for in several ways. First, the current view point is controlled by manipulating the scroll bars to move in either direction around the display or by dragging with the mouse directly on the canvas itself. We also provide a page-at-a-time navigation option that will move the view left or right by the number of bases that are currently visible. High-speed navigation to any area of the view is also available by clicking and dragging on the overview window, which always displays a bounding rectangle representing the portion of the overall data currently visible within the main display. Protein translations are optionally provided for all six reading frames of the consensus sequence. Annotation features such as SNPs and indels can either be imported with the assembly file itself or separately in GFF3 format, and are then listed on a separate tab attached to the contig list. Information on a given read is provided as a graphical overlay as soon as the mouse is moved over it. Tablet will display its name, start and end positions (optionally with unpadded consensus values), the sequence length, whether it is complemented or not, and also provide a scaled-to-fit graphical representation of the bases within the read. All of the raw data can be copied to the clipboard at any time. 3 IMPLEMENTATION AND PERFORMANCE Tablet is written in Java and is compatible with any Java-enabled system with a runtime level of ≥1.6. We provide installable versions that include everything required to run the application, including a suitable Java runtime. The installers are available for Windows, Mac OS X, Linux and Solaris, in both 32- and 64-bit versions. Once installed and running, Tablet will also monitor our server for new versions and will prompt, download and update quickly and easily whenever a new release is available, along with redirecting the user to a web page describing the new features that have been added. A prime requisite during development of Tablet has been computing efficiency and speed. The two main approaches to handling assembly data in viewers are either memory-based, where all the data are loaded into memory, or disk-cached, where the data reside on disk with only the currently visible segment of the dataset held in memory. Memory-based applications are faster for viewing and navigation (after an initial delay while loading the data) and can provide whole dataset overviews and statistical summaries, but the size of dataset they can handle is limited by the amount of available memory. In contrast, cache-based applications can display views from much larger datasets using a minimum of memory, but access to the data can be orders of magnitude slower (which then affects navigation and rendering), and the feature sets available are often limited. With Tablet, we have chosen a hybrid solution that provides us with advantages from both approaches. We hold a ‘skeleton’ layout of the reads in memory, with data on each read limited to just an internal ID, its position against the consensus or reference sequence and its length. The nucleotide data itself (efficiently compressed so it can be read as quickly as possible), along with other supplementary information—such as the read's name and its orientation—is held in an indexed disk-cache and is only accessed (via the read's ID) when required. Tablet also allocates memory on a per-contig basis, including information for features such as how to pack the data for display, coverage calculations, padded-to-unpadded mappings, etc. These data are calculated and stored before each contig is rendered and discarded again after display. This approach allows us to provide maximum functionality—instant access to any portion of the data; extremely fast and high-quality rendering; entire dataset overviews—yet memory usage is kept relatively low. Comparing data indexing/loading times and memory consumption across a range of tools for an assembly file containing ∼2.9 million Illumina Solexa reads of length 51, we found that the cache-based viewers (Maqview, MapView, tview) were fairly constant in memory usage (between 35 MB and 70 MB while viewing), with indexing times varying from 10 s to 50 s, although memory consumption during indexing did peak as high as 350 MB with MapView. For the memory-based viewers, we compared Hawkeye (5500 MB; 107 s), Consed (2600 MB; 73 s) and EagleView (2450 MB; 98 s). Tablet, being a hybrid, loads the data in 25 s, and uses just 175 MB of memory.
 Fig. 1. Tablet showing Illumina Solexa reads against an  Arabidopsis thaliana  cDNA reference sequence (additional screenshots can be seen online at  http://bioinf.scri.ac.uk/tablet/screenshots.shtml ). 4 FUTURE WORK Work is in progress to support paired-end sequence data, and to enhance Tablet's visualization of annotation data. We also plan to further reduce Tablet's memory requirements by cutting down on the amount of reference/consensus information held at any time. Experiments have shown that further reductions should be possible without compromising data access times, graphical rendering speed or visualization quality. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>GimmeMotifs: a de novo motif prediction pipeline for ChIP-sequencing experiments</Title>
    <Doi>10.1093/bioinformatics/btq636</Doi>
    <Authors>van Heeringen Simon J., Veenstra Gert Jan C.</Authors>
    <Abstract>Summary: Accurate prediction of transcription factor binding motifs that are enriched in a collection of sequences remains a computational challenge. Here we report on GimmeMotifs, a pipeline that incorporates an ensemble of computational tools to predict motifs de novo from ChIP-sequencing (ChIP-seq) data. Similar redundant motifs are compared using the weighted information content (WIC) similarity score and clustered using an iterative procedure. A comprehensive output report is generated with several different evaluation metrics to compare and evaluate the results. Benchmarks show that the method performs well on human and mouse ChIP-seq datasets. GimmeMotifs consists of a suite of command-line scripts that can be easily implemented in a ChIP-seq analysis pipeline.</Abstract>
    <Body>1 INTRODUCTION The spectacular development of sequencing technology has enabled rapid, cost-efficient profiling of DNA binding proteins. Chromatin immunoprecipitation followed by high-throughput deep sequencing (ChIP-seq) delivers high-resolution binding profiles of transcription factors (TFs) ( Park, 2009 ). The elucidation of the binding characteristics of these TFs is one of the obvious follow-up questions. However, the  de novo  identification of DNA sequence motifs remains a challenging computational task. Although many methods have been developed with varying degrees of success, no single method consistently performs well on real biological eukaryotic data ( Tompa  et al. , 2005 ). The combination of different algorithmic approaches, each with its own strengths and weaknesses, has been shown to improve prediction accuracy and sensitivity over single methods ( Hu  et al. , 2005 ). Here, we report on GimmeMotifs, a motif prediction pipeline using a ensemble of existing computational tools (Supplementary Fig. S1). This pipeline has been specifically developed to predict TF motifs from ChIP-seq data. It uses the wealth of sequences (binding peaks) usually resulting from ChIP-seq experiments to both predict motifs  de novo , as well as validate these motifs in an independent fraction of the dataset. GimmeMotifs incorporates the weighted information content (WIC) similarity metric in an iterative clustering procedure to cluster similar motifs and reduce the redundancy which is the result of combining the output of different tools (see Supplementary Material). It produces an extensive graphical report with several evaluation metrics to enable interpretion of the results ( Fig. 1 ).
 Fig. 1. An example of the GimmeMotifs output for p63 ( Kouwenhoven  et al. , 2010 ). Shown are the sequence logo of the predicted motif ( Schneider and Stephens, 1990 ), the best matching motif in the JASPAR database ( Sandelin  et al. , 2004 ), the ROC curve, the positional preference plot and several statistics to evaluate the motif performance. See the Supplementary Material for a complete example. 2 METHODS 2.1 Overview The input for GimmeMotifs is a file in BED format containing genomic coordinates, e.g. peaks from a ChIP-seq experiment or a FASTA file. This dataset is split: a prediction set contains randomly selected sequences from the input dataset (20% of the sequences by default) and is used for motif prediction with several different computational tools. Predicted motifs are filtered for significance using all remaining sequences (the validation set), clustered using the WIC score as described below, and a list of non-redundant motifs is generated. 2.2 Motif similarity and clustering The WIC similarity score is based on the information content (IC) and is defined for position  i  in motif  X  compared with position  j  of motif  Y  as:
 (1) 
where  c  is 2.5, and DIC( X i ,  Y j ) is the differential IC defined in Equation ( 3 ). The IC of a specific motif position is defined as:
 (2) 
where IC( X i ) is the IC of position  i  of motif  X ,  f x i , n  is the frequency of nucleotide  n  at position  i  and  f bg  is the background frequency (0.25). The differential IC (DIC) of position  i  in motif  X  and position  j  in motif  Y  is defined as:
 (3) The WIC score of all individual positions in the alignment is summed to determine the total WIC score of two aligned motifs. To calculate the maximum WIC score of two motifs, all possible scores of all alignments are calculated, and the maximum scoring alignment is kept. Similar motifs are clustered using an iterative pair-wise clustering procedure (Supplementary Material). 2.3 Evaluation The motifs can be evaluated using several different statistics: the absolute enrichment, the hypergeometric  P -value, a receiver operator characteristic (ROC) graph, the ROC area under the curve (AUC) and the mean normalized conditional probability (MNCP) ( Clarke and Granek, 2003 ). In addition to these evaluation metrics, GimmeMotifs generates a histogram of the motif position relative to the peak summit, the positional preference plot. Especially in case of high-resolution ChIP-seq data, this gives valuable information on the motif location. 2.4 Implementation The GimmeMotifs package is implemented in Python, while the similarity metrics are written as a C extension module for performance reasons. It is freely available under the MIT license. Sequence logos are generated using WebLogo ( Schneider and Stephens, 1990 ). 3 BENCHMARK RESULTS We performed a benchmark study of GimmeMotifs on 18 TF ChIP-seq datasets. The ROC AUC and MNCP of the best performing motif were calculated and compared with the best motif of two other ensemble methods: SCOPE ( Carlson  et al. , 2007 ) and W-ChipMotifs ( Jin  et al. , 2009 ) (Supplementary Tables S1 and S2) . The results show that GimmeMotifs consistently produces accurate results (median ROC AUC 0.830). The method also significantly improves on the results of SCOPE (ROC AUC 0.613). The recently developed W-ChIPmotifs shows comparable results to GimmeMotifs (ROC AUC 0.824), although this tool does not cluster similar redundant motifs. In addition, the focus of GimmeMotifs is different. While the web interface of W-ChipMotifs is very useful for casual use, the command-line tools of GimmeMotifs can be integrated in more sophisticated analysis pipelines. 4 CONCLUSION We present GimmeMotifs, a  de novo  motif prediction pipeline ideally suited to predict transcription factor binding motifs from ChIP-seq datasets. GimmeMotifs clusters the results of several different tools and produces a comprehensive report to evaluate the predicted motifs. We show that GimmeMotifs performs well on biologically relevant datasets of different TFs and compares favorably to other methods. 
          Supplementary Material 
          
             
                Supplementary Data 
             
             
             
             
             
             
             
          
       </Body>
  </Article>
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Variable locus length in the human genome leads to ascertainment bias in functional inference for non-coding elements</Title>
    <Doi>10.1093/bioinformatics/btp043</Doi>
    <Authors>Taher Leila, Ovcharenko Ivan</Authors>
    <Abstract>
          Motivation: Several functional gene annotation databases have been developed in the recent years, and are widely used to infer the biological function of gene sets, by scrutinizing the attributes that appear over- and underrepresented. However, this strategy is not directly applicable to the study of non-coding DNA, as the non-coding sequence span varies greatly among different gene loci in the human genome and longer loci have a higher likelihood of being selected purely by chance. Therefore, conclusions involving the function of non-coding elements that are drawn based on the annotation of neighboring genes are often biased. We assessed the systematic bias in several particular Gene Ontology (GO) categories using the standard hypergeometric test, by randomly sampling non-coding elements from the human genome and inferring their function based on the functional annotation of the closest genes. While no category is expected to occur significantly over- or underrepresented for a random selection of elements, categories such as ‘cell adhesion’, ‘nervous system development’ and ‘transcription factor activities’ appeared to be systematically overrepresented, while others such as ‘olfactory receptor activity’—underrepresented.
        </Abstract>
    <Body>
       1 INTRODUCTION 
       Almost 20 vertebrate genomes have been fully sequenced up to date. Gene annotation of the human, mouse and several other genomes reaches high confidence levels (Pruitt  et al. ,  2007 ), and functional classification databases provide valuable information to understand the biological processes associated with different groups of genes in these genomes. Gene Ontology (GO) (Ashburner  et al. ,  2000 ), KEGG (Kanehisa  et al. ,  2006 ,  2008 ), OMIM (Boyadjiev and Jabs,  2000 ; Hamosh  et al. ,  2002 ,  2005 ) and OBO Cell Ontology (Smith  et al. ,  2007 ), are just some of the most widely used functional annotation databases, which have enabled intriguing discoveries during the last decade (Hvidsten  et al. ,  2001 ; King  et al. ,  2003 ). 
       The classical approach to functional inference identifies annotation terms that are significantly over- or underrepresented within a given class of genes; over- or underrepresentations are identified by comparing the count of occurrences for each annotation term to the expected value, which usually arises from considering the number of genes assigned to each category in the complete genome. Several tools have been developed to perform the classification analysis that are mainly based on the hypergeometric test [among them BiNGO (Maere  et al. ,  2005 ), GO::TermFinder (Boyle EI,  2004 ) and GOToolBox (Martin,  2004 )] or Fisher's exact test, which relies on properties of the hypergeometric distribution [like GOstat (Beissbarth and Speed,  2004 ) and FatiGO (Al-Shahrour  et al. ,  2004 ,  2007 )]. 
       However, the vast majority of the genome consists of non-protein-coding (non-coding) sequences. Functional non-coding sequences may be associated with protein-coding sequences by either directly or indirectly regulating the expression of protein-coding genes, or playing structural roles in chromosome architecture or encoding RNA genes. In any case, annotation databases for non-coding elements are still in their infancy. In particular, there are at least two databases that store and openly share functional annotation of candidate gene regulatory elements in vertebrates, tested  in vivo  in mice and zebrafish—Vista Enhancer Database (VED) (Pennacchio  et al. ,  2006 ) and CONDOR (Woolfe  et al. ,  2007 ). However, ∼1000 elements profiled in these databases represent only a small fraction of gene regulatory elements in a vertebrate genome, which are expected to exceed the number of exons (∼200 000) (Waterston  et al. ,  2002 ). In practice, this precludes the application of these databases to the functional annotation of non-coding elements, which could be represented by a set of non-coding SNPs (Schwarz  et al. ,  2008 ), a set of transcription factor binding sites from ChIP-chip experiments (Hu  et al. ,  2007 ), or a set of candidate regulatory elements scattered across a vertebrate genome (Ovcharenko  et al. ,  2005 ; Woolfe  et al. ,  2005 ), for example. A sensible solution to this problem proposes to infer the function of a given non-coding element from that of the gene it belongs to or the closest neighboring gene; this strategy is especially well justified for promoter or UTR elements. However, the interpretation of the results is not always straightforward—promoter elements only represent a small component of the complex gene regulatory machinery, also constituted by distant intergenic and intronic elements (Machon  et al. ,  2002 ; Nobrega  et al. ,  2003 ), which do not necessarily regulate the gene they are inserted in or close to (Lettice  et al. ,  2003 ; Santagati  et al. ,  2003 ). 
       Uncertain association of putative regulatory elements and genes aside, gene annotation databases can be useful to characterize non-coding elements. While the association with the gene is often straightforward for promoters and UTR elements, intronic elements are commonly associated with the gene containing them and intergenic elements are usually associated with the nearest gene. After that, it is reasonable to infer the function of non-coding elements by examining the function of the corresponding set of genes. In a classical approach, the number of genes assigned to a given functional category in this set is compared with the number of genes assigned to that category in the entire genome, and deviations are evaluated according to a statistical test. The problem with this logic is the implicit assumption that the probability of sampling a particular annotation term is equal to the fraction of genes associated with it in the genome, which does not depend on the total number of non-coding elements a particular gene is associated with. Basically, a gene with many non-coding elements and a gene with zero non-coding elements are assumed to have equal probability of discovery through the analysis of their non-coding DNA space, which is obviously wrong and leads to a GO ascertainment bias. As a result, non-coding elements will be predicted in some loci of the genome more often than in others purely by chance, and any random subset of non-coding DNA may appear significantly enriched or depleted for some annotation terms, i.e. the above-mentioned strategy for an indirect functional analysis is biased due to the variable locus length. To correct for the GO ascertainment bias, within the context of functional inference on non-coding elements, the probability of a given annotation term should be set proportional to the fraction of the length of non-coding DNA assigned to it, which is strongly correlated with the length of the locus that contains it, and is highly variable across different loci ( Supplementary Fig. 1 ).  
           Fig. 1. 
           
             Distribution of GO categories with respect to the locus length. Left and right tables list the GO categories particularly associated with short and long loci, respectively. 
           
           
         
       
       The aim of this work is to evaluate the effect of the variability in locus length on the functional analysis of non-coding DNA. We consider the total population of non-coding DNA elements in the human genome and the annotation terms attributed to their neighbor genes, and assess whether a set of non-coding DNA elements randomly sampled from the genome will appear artificially enriched and/or depleted for any annotation term. In our study, we report systematic false positive associations for a particular set of GO categories; the choice of the GO database is just exemplary. Finally, we propose a statistical method and a set of correction coefficients to perform an unbiased functional analysis for a set of non-coding elements in a genome. 
     
       2 METHODS 
       
         2.1 GO assignment 
         We performed the functional classification of non-coding elements based on the GO gene annotation database. Each non-coding DNA element can be associated with a set of GO categories that corresponds to either the gene containing that element or the closest flanking gene, in case of intergenic or promoter elements. Therefore, a locus will consist of a gene together with half of its adjacent intergenic regions; a delimitation closer to the real transcription units would be desirable, but impracticable, while the boundaries proposed here eliminate any ambiguity in the gene assignment of the non-coding elements. 
         Also, each gene usually has several associated GO categories. Furthermore, the structure of the GO database is hierarchical, so that each GO category is connected to other categories, which may be associated with other genes. The version of the GO database that we employed contained 6592 terms, each assigned to an average of 17 genes. Three quarters of the GO categories are ascribed to at most five genes, while the average gene count for the remaining quarter is 64. Only 18 GO categories are attributed to 1000 or more genes; from these, five describe some molecular ‘binding’, and seven refer to cellular components. We downloaded the RefSeq gene annotation of the human genome (NCBI Build 36.1; hg18) from the UCSC Genome Browser (Karolchik  et al. ,  2003 ), and identified 17 475 discrete gene loci, with an average locus length of 152 057 bp (the shortest locus was 612 bp, while the longest locus was 4 767 747 bp). The average locus length for a GO category was 159 918 bp, ranging from 1979 bp to 3 204 335 bp; the average locus length of 25% GO categories was longer than 194 230 bp. 
       
       
         2.2 Sampling 
         For the purpose of this study, we define a non-coding DNA element as a non-repetitive non-coding DNA sequence stretch within a gene locus, much shorter than the complete locus length. This allowed direct sampling of genes from the genome with a probability being a function of the non-coding non-repetitive length of the gene locus. 
         The population of non-coding DNA elements in the genome is finite, and its probability distribution is discrete. The probability of a given non-coding DNA element is given by its length divided by the total non-coding DNA in the genome ( L 
           nc 
           HG =1 359 884 776 nucleotides). We took 1000 random samples for each sample size ( n  ranging from 100 to 200 000), using the algorithm described in  Supplementary Figure 5 . We computed the frequency of the GO categories corresponding to each different gene associated with the non-coding elements in each of the 1000 samples.
         
       
       
         2.3 GO enrichment/depletion 
         The usual statistical test for functional enrichment compares the count of GO category associations for a given set of genes to the expected number, which is derived from the count of GO category associations in the complete genome. For each GO category, the test evaluates the probability of observing a number of genes associated with a particular GO category, by comparing it with the total number of genes in the genome that are assigned to that category. This analysis assumes that all genes are equally likely, and the probability of attributing a given function or GO category to a gene only depends on the total number of genes carrying that GO category. 
         Under such hypotheses, the probability of associating a certain non-coding DNA element to a given GO category can be regarded as  
             (1) 
             
           
         
         where  N 
           GO  is the number of genes/loci associated with a given GO category in the set of  N 
           HG  genes analyzed. Enrichment in a certain GO category can be quantified by computing the probability that the number of non-coding DNA elements in a sample of size  n  that are associated with a GO category,  N 
           GO , is larger than or equal to the observed value  m  assuming the frequency  P 
           GO  in all genes. This follows a hypergeometric distribution which approximates the binomial distribution when  n / N 
           HG  is small  n / N 
           HG &lt;20.
         
         For each of the 1000 random samples, we identified the set of different genes associated with non-coding elements, and subsequently counted the frequency of the GO categories associated with these genes. The frequencies of the GO categories were compared with the genomic frequencies. This procedure allows distinguishing enrichments or depletions of specific GO categories in the sample. The probability of obtaining  k  non-coding DNA elements for a given GO category among a sample of size  n  by chance, knowing that the reference dataset contains  N 
           GO  such annotated genes/loci out of  N 
           HG =17 475 genes/loci, can be calculated using the hypergeometric distribution. The significance of the enrichment in each of the GO categories was evaluated by summing over the upper tail of the hypergeometric distribution (α=0.05) and applying Bonferroni's multiple test correction. For the later, we multiplied the nominal  P -values calculated as described above by the number of tests performed, i.e. the total number of GO categories.
         
       
     
       3 RESULTS 
       
         3.1 Variable locus length of GO categories 
         In the present study, we utilized RefSeq gene annotation to define the genomic location of genes and their corresponding loci. First, we utilized all available transcripts to identify 17 475 non-overlapping genes. Next, we split the genome into a set of loci by dividing intergenic intervals in half. We also tested an alternative locus definition, in which the locus boundaries were determined by proximity to the transcription start site, and did not observe an impact on our conclusions (see next section for details). 
         In assessing the variation in the average locus length of different GO categories belonging to the three hierarchies ‘biological process’, ‘molecular process’ and ‘cellular component’, we observed a wide distribution of average locus lengths centered at around 100 kb ( Fig. 1 ). A substantial fraction of GO categories was found to be assigned to genes with either unusually short or unusually long loci. Interestingly, we noted a particular bias towards specific GO categories, and therefore, biological functions, in short and long loci. Concretely, several GO categories related to metabolic processes, as well as some involved in specific responses, are particularly overrepresented in short loci (&lt;80 kb), while GO categories corresponding to development, morphogenesis, regulation and signaling are significantly overrepresented in loci longer than 120 kb ( Table 1 ). Although the substrate of this study is the GO database, other functional annotation databases will most probably present a conceptually similar bias in locus length, as this bias has a biological origin, namely the heterogeneity in the locus length.  
             Table 1. 
             
               GO categories significantly associated with genes in shorter loci and in longer loci 
             
             
               
                 
                   Process/function 
                   Locus length (kb) 
                   
                     P -value
                   
                 
               
               
                 
                   Genes in shorter loci 
                   
                   
                 
                 
                   Response 
                   
                   
                 
                 
                       To unfolded protein 
                   28.7 
                   2.4e-5 
                 
                 
                       To bacterium, defense 
                   58.5 
                   1.3e-5 
                 
                 
                       To biotic stimulus 
                   58.8 
                   1.8e-12 
                 
                 
                   Oxidative phosphorylation 
                   32.6 
                   6.1e-9 
                 
                 
                   Oxidoreductase activity 
                   38.3 
                   1.2e-5 
                 
                 
                   Electron transport 
                   
                   
                 
                 
                       Mitochondrial 
                   34.3 
                   1.1e-5 
                 
                 
                       ATP synthesis coupled 
                   36.1 
                   1.4e-6 
                 
                 
                   Ribosome 
                   
                   
                 
                 
                       Structural constituent 
                   36.8 
                   1.7e-8 
                 
                 
                       Biogenesis and assembly 
                   44.9 
                   1.8e-7 
                 
                 
                   Keratinization 
                   38.2 
                   1.2e-6 
                 
                 
                   Epidermal cell differentiation 
                   43.3 
                   1.2e-5 
                 
                 
                   rRNA 
                   
                   
                 
                 
                       Processing 
                   50.2 
                   9.1e-7 
                 
                 
                       Metabolic process 
                   51.3 
                   8.4e-7 
                 
                 
                   Genes with longer loci 
                   
                   
                 
                 
                   Morphogenesis 
                   
                   
                 
                 
                       Embryonic limb 
                   525.2 
                   6.8e-7 
                 
                 
                       Neurite 
                   185.2 
                   1.4e-7 
                 
                 
                   Development 
                   
                   
                 
                 
                       Limb 
                   483.1 
                   6.0e-8 
                 
                 
                       Lung 
                   283.2 
                   7.3e-6 
                 
                 
                       Respiratory tube 
                   277.0 
                   4.4e-6 
                 
                 
                       Brain 
                   228.2 
                   1.3e-7 
                 
                 
                       Central nervous system 
                   228.1 
                   1.3e-11 
                 
                 
                       Tube 
                   202.0 
                   4.4e-9 
                 
                 
                   Regulation of 
                   
                   
                 
                 
                       Developmental process, positive 
                   325.3 
                   2.1e-5 
                 
                 
                       Cell differentiation, negative 
                   316.4 
                   2.5e-5 
                 
                 
                       Transcription, positive 
                   183.4 
                   1.8e-6 
                 
                 
                   Axon guidance 
                   320.5 
                   2.5e-5 
                 
                 
                   Signaling 
                   
                   
                 
                 
                       Cyclic-nucleotide-mediated 
                   214.7 
                   2.7e-6 
                 
                 
                       G-protein 
                   214.7 
                   1.3e-6 
                 
               
             
           
         
       
       
         3.2 Ascertainment bias impact 
         The effect of the ascertainment bias caused by the locus length non-uniformity in GO categories will vary depending on the number of genes each GO category is assigned to and the number of non-coding elements used in a study. A GO category associated with very few genes is less likely to result in an incorrect prediction than a GO category associated with many genes, simply because a GO category with few genes is less likely to be detected at all. A small set of non-coding elements is also less likely to produce false positive associations, as it is less likely to produce any associations at all. 
         To explore the need of accounting for such ascertainment bias, we randomly selected sets of non-coding elements in the human genome, associated them with their closest genes, and performed a classical GO analysis on the indirectly selected sets of genes. (It should be noted that although this study concentrates on the GO database, the conclusions can be generalized to any other system of functional classification.) We also excluded repetitive elements from the analysis, as functional non-coding elements are expected to be mainly non-repetitive. We will refer to the process of sampling  n  non-coding DNA elements from the human genome as an experiment. We performed 1000 independent experiments for each sample size  n , which ranged from 100 to 200 000, and evaluated enrichment and depletion for different GO categories. We adjusted the significance level by applying the strict Bonferroni's multiple-testing correction (Bonferroni,  1935 ). Unexpectedly often, aleatory sets of non-coding DNA elements were found to be significantly associated with multiple GO categories ( Fig. 2  shows the number of GO categories that appeared to be significant in at least 5% of the experiments for different sample sizes). The number of significantly overrepresented GO categories reached the maximum of 22 for sample size 20 000, and decreased to five as the sample size increased to 200 000. The number of underrepresented categories rapidly plateaued at 10 GO categories in the range of sample sizes plotted. By sampling non-coding elements we indirectly select genes, but the occurrence of each gene is considered only once. For that reason, 20 000 non-coding samples result in ∼43% of the total number of genes in the human genome. When the sample size is large enough so that every gene is effectively represented, the sample coincides with the population. In this case, the number of occurrences for each category meets its expected value. In other words, the number of occurrences of a given GO category converges to the expected value as more genes become represented in the sample, and this accounts for the variation in the number of artificially over- or underrepresented GO categories with the sample size.  
             Fig. 2. 
             
               The average number of GO categories that show up as significantly over- or underrepresented in experiments with random sets of non-coding elements for different sample sizes. 
             
             
           
         
         To test whether this effect is a simple consequence of our locus definition, in which intergenic space is split in half, we repeated this experiment using an alternative locus definition, in which a non-coding element is associated with the gene that has the most proximal transcription start site to the element. We found that the alternative locus definition has no impact on the observed effect ( Supplementary Fig. 2 ). 
         In summary, we found that up to 31 GO categories were significantly over- or underrepresented, depending on the sample size. Specifically, within the usual sample size ranges, over 10 GO categories were overrepresented with a striking confidence level. Considering that each experiment consisted of randomly sampled non-coding DNA elements, and that the experiment was independently repeated a large number of times, this result is not expected. However, the outcome can be easily explained by considering that the non-coding DNA elements do not all have the same probability of being assigned to a gene, but instead have a probability that depends on the locus length. 
       
       
         3.3 Systematically biased GO category assignments 
         The fact that random sets of non-coding elements appear to be significantly enriched in certain GO categories is alarming. Nevertheless, an even more worrisome question is whether any of such associations between random sets of non-coding elements and GO categories occurs systematically, as this would suggest that some particular GO categories are likely to be reported as significant on a regular basis. For that purpose, for a given sample size  n , we analyzed GO categories that were reported as significantly over- or underrepresented in at least 25% of the experiments ( Fig. 3 ). We observed a systematic significant association for a total of 13 GO categories (nine overrepresentations and four underrepresentations). It is interesting to note that the majority of the overrepresented GO categories relate to basic cellular processes (cell adhesion, binding, transcription factors and development), while underrepresented GO categories correspond to lineage-specific and adaptive features (response and receptor categories). Not surprisingly, these constitute a subset of the GO categories for which we observe a large deviation from the uniform distribution in relation to the locus length ( Table 2  summarizes the ratio between the average locus length for the loci associated with each particular GO category  
             
            represented in  Figure 3  and the average locus length in the human genome  
             
           .  
             Fig. 3. 
             
               Significantly over- and/or underrepresented GO categories (showing only categories which are significant in at least 25% of the experiments). The  x -axis represents different sample sizes, only within a range in which the number of GO categories over- and/or underrepresented shows high variation. 
             
             
           
           
             Table 2. 
             
               Significantly over/underrepresented GO categories (showing only categories which are significant in at least 25% of the experiments) 
             
             
               
                 
                   GO id 
                   Description 
                   
                     
                       
                     
                   
                 
               
               
                 
                   Overrepresentation 
                   
                   
                 
                 
                   GO:0007156 
                   Homophilic cell adhesion 
                   4.7 
                 
                 
                   GO:0007155 
                   Cell adhesion 
                   2.3 
                 
                 
                   GO:0007399 
                   Nervous system development 
                   1.9 
                 
                 
                   GO:0005509 
                   Calcium ion binding 
                   1.7 
                 
                 
                   GO:0007242 
                   Intracellular signaling cascade 
                   1.4 
                 
                 
                   GO:0043565 
                   Sequence-specific DNA binding 
                   1.4 
                 
                 
                   GO:0007275 
                   Multicellular organismal development 
                   1.3 
                 
                 
                   GO:0006468 
                   Protein amino acid phosphorylation 
                   1.3 
                 
                 
                   GO:0003700 
                   Transcription factor activity 
                   1.2 
                 
                 
                   Underrepresentation 
                   
                   
                 
                 
                   GO:0007186 
                   G-protein coupled receptor protein signaling pathway 
                   0.7 
                 
                 
                   GO:0050896 
                   Response to stimulus 
                   0.6 
                 
                 
                   GO:0007608 
                   Sensory perception of smell 
                   0.4 
                 
                 
                   GO:0004984 
                   Olfactory receptor activity 
                   0.3 
                 
               
             
             
               
                 Overrepresented GO categories appear to have ratios &gt;1, while underrepresented GO categories consist of shorter loci, on average. 
               
             
           
         
         For example, the category ‘homophilic cell adhesion’ appeared to be consistently overrepresented in random sets of 500 and more non-coding elements. More precisely, sets of 500 non-coding elements were significantly associated with this category in 25% of independent experiments, while sets of 2500 non-coding elements were significantly associated with this category in more than 85% of experiments. Interestingly, 55 of the 94 genes associated with homophilic cell adhesion are cadherins. Cadherins ( Supplementary Table 2 ) are a superfamily of adhesion molecules with function in cell recognition, tissue morphogenesis and tumor suppression (Angst  et al. ,  2001 ). Cadherin genes are often flanked either on one or on both sides by a so-called gene desert [an extremely long intergenic region (Ovcharenko  et al. ,  2005 )], and this genome architecture is well conserved in mammals and birds (Angst  et al. ,  2001 ; Wu and Maniatis,  2000 ; Wu  et al. ,  2001 ). The characteristic long locus length of these cadherins contributes to the association bias of the homophilic cell adhesion category, which appears as one the top candidates for the systematic false positive annotation of non-coding elements. 
         In summary, these results indicate that the effect of the locus length heterogeneity and the unevenness of the GO category distribution with regards to it are not negligible and should be appropriately accounted for in functional inference of non-coding elements. The consequence of observing artificially overrepresented categories is conceptually different from that of detecting underrepresented categories. Given a non-coding element, in the former case the results might suggest a function that it does not actually fulfill (false positive), while in the latter case, evidence for a certain function might be simply omitted (false negative). 
       
       
         3.4 Locus length correction 
         We have shown that the distortion in the distribution of the GO categories in relation to the locus length may lead to erroneous conclusions in the context of the functional annotation of non-coding elements. However, such bias can be excluded by simply introducing probability correction coefficients that depend on the average locus length of each GO category. To account for the heterogeneous locus length in the human genome, we suggest considering the length of the non-coding DNA associated with each GO category, as described below. 
         If we randomly sample non-coding elements from the human genome, the probability of observing a certain GO category is  
             (2) 
             
            where  L 
           nc 
           GO  is the total length of the non-coding DNA in the loci a given GO category, and  L 
           nc 
           HG  is the total length of the non-coding DNA in the human genome ( Supplementary Fig. 3 ).
         
         The probability of observing a certain GO category assuming that all genes in the human genome occur randomly with the same frequency is  
             (3) 
             
            where  N 
           GO  is the number of genes/loci associated with a given GO category and  N 
           HG  genes is the number of genes/loci in the human genome.
         
         Then, we define a correction coefficient CC GO  for each GO category ( Supplementary Table 1 ), such that  
             (4) 
             
            and  
             (5) 
             
            The selection of  n  GO categories at random from the entire genome can be modeled as a binomial distribution where the success of an event is defined as selecting a certain GO category with a probability that depends on the length of the non-coding DNA in the loci that GO category is associated with.
         
         If we observe  m  instances of a GO category, we can calculate its  P -value under a random model, as 1 minus the cumulative binomial probability of selecting that particular GO category with a frequency  m −1, which is calculated as  
             (6) 
             
            In order to correct for multiple testing, we must multiply that probability by the number B of hypothesis we test for (Bonferroni's multiple-comparison correction). The expected frequency of a GO category is  
             
           
         
         We propose to use the ratio of observed to expected frequencies as a rough indicator of enrichment; a ratio above one indicates that the GO category is enriched in the sample with respect to its average expectation, while a ratio below one indicates a depleted GO category. However, it must be noted that this ratio will overestimate GO categories with few expected occurrences. 
       
       
         3.5 Validation 
         In addition to the aforementioned experiments, we discarded artifacts caused by the sampling method, correlation between GO categories or the threshold chosen for establishing the significance by repeating 1000 sampling experiments from the finite population of non-coding DNA elements at random and testing each GO category for enrichment/depletion using a binomial distribution. As expected, when we computed the  P -values using a binomial distribution with parameters  n  (sample size) and  
             
           , where  
             
            is the probability of observing the total length of non-coding DNA indirectly assigned to a particular GO, we could not detect any particular GO category significant in 5% or more of the experiments. However, when we repeated the calculations using a binomial distribution with parameters  n  (sample size) and  P 
           GO , where  P 
           GO  is the probability of observing all genes in the genome that are assigned to a particular GO, we obtained a list of over- and underrepresented categories very similar to that produced with the hypergeometric distribution.
         
         Finally, we would like to mention that the inclusion of repetitive elements in the analysis does not alter the results, as their locus span is strongly correlated with the locus length (data not shown). Also, to confirm that the observed effect is not associated with either repeat-rich or repeat-poor regions we analyzed the relation between the number of non-coding non-repetitive elements in a locus and repeat density. We found that loci with the excessive number of non-coding non-repetitive elements that contribute to an enrichment of artificial GO associations have average repeat density and are not biased towards either repeat-rich or repeat-poor regions ( Supplementary Fig. 4 ). 
       
     
       4 DISCUSSION 
       GO databases provide a variety of tools for the functional analysis of genes. Due to the current lack of exhaustive databases describing functional non-coding DNA elements, it has become a usual practice to indirectly infer the biological role of selected non-coding elements from the functional analysis of their flanking genes. As we have shown, the high heterogeneity locus length in the human genome and the uneven distribution of the GO categories in relation to the locus length can bias functional inference. Therefore, the  P -values for the GO categories that clearly deviate from the assumptions made by the hypergeometric test should be computed considering that the probability of a given GO category does not only depend on the number of genes assigned to it, but also on the length of their loci. Otherwise, categories that are particularly associated with very long or very short loci might appear artificially over- or underrepresented, respectively. As an approximate solution to the problem caused by the variability in the locus length, we propose the use of correction coefficients, which take into consideration the genome span of non-coding DNA corresponding to different GO categories. The coefficients in  Supplementary Table 1  can be easily recomputed for other genomes and other annotation databases according to the procedure described in  Section 2 . 
       An increasing number of studies report that conserved non-coding sequences tend to cluster in the vicinity of genes implicated in development and transcriptional regulation (termed trans-dev genes) (see for example, Bejerano  et al. ,  2004 ; Dermitzakis  et al. ,  2005 ; McEwen  et al. ,  2006 ; Ovcharenko,  2008 ; Sandelin  et al. ,  2004 ; Woolfe  et al. ,  2005 ). We observe the association of similar GO categories with random sets of non-coding DNA, suggesting that the heterogeneity of the locus length might have had an adverse effect on previous reports. In a reanalysis of studies describing ultraconserved elements (Bejerano  et al. ,  2004 ) and non-coding elements conserved between human and fish (Ovcharenko  et al. ,  2004 ; Woolfe  et al. ,  2005 ), we found that the originally reported association with transcriptional regulation and transcription factors can be strongly confirmed even after the application of the correction for the GO ascertainment bias, while the  P -values for associations related to the nervous system and multicellular organismal development fall below the level of statistical significance ( Tables 3  and  4 ). However, it is important to note that our results do not necessarily object the validity of previously published conclusions—if the extreme length of some loci is the result of evolutionary selection and not simply of the locus length variability, the proposed non-coding length correction might artificially reduce the significance of biologically important associations. Obviously, without the availability of extensive annotation databases for non-coding elements, it might be quite difficult to establish a bulletproof approach for using gene annotation databases for an indirect annotation of non-coding elements, but it is also unwise to ignore the potential impact of the locus length on the inference of the function for non-coding elements. Therefore, until we have a large-scale sampling of non-coding functional elements in the human genome that we can use to infer function of other non-coding elements, a practical solution might consist of utilizing the classical GO analysis approach, applying the proposed correction, and analyzing differences and commonalities in the results.  
           Table 3. 
           
             Overrepresented GO categories computed using the usual hypergeometric test (panel A) and accounting for variable locus length (panel B) on the datasets described by Ovcharenko  et al.  ( 2004 ) and Woolfe  et al.  ( 2005 ) 
           
           
             
               
                 
                   
                 
               
             
           
           
             
               Categories removed by the GO ascertainment correction are highlighted, as well as additional categories found after applying the correction. 
             
           
         
         
           Table 4. 
           
             Overrepresented GO categories computed using the usual hypergeometric test (panel A) and accounting for variable locus length (panel B) on the datasets described by Bejerano  et al.  ( 2004 ) 
           
           
             
               
                 
                   
                 
               
             
           
           
             
               Categories removed by the GO ascertainment correction are highlighted, as well as additional categories found after applying the correction. 
             
           
         
       
     
       Supplementary Material 
       
         
           [Supplementary Data] 
         
         
         
       
     </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>TREAT: a bioinformatics tool for variant annotations and visualizations in targeted and exome sequencing data</Title>
    <Doi>10.1093/bioinformatics/btr612</Doi>
    <Authors>Asmann Yan W., Middha Sumit, Hossain Asif, Baheti Saurabh, Li Ying, Chai High-Seng, Sun Zhifu, Duffy Patrick H., Hadad Ahmed A., Nair Asha, Liu Xiaoyu, Zhang Yuji, Klee Eric W., Kalari Krishna R., Kocher Jean-Pierre A.</Authors>
    <Abstract>Summary: TREAT (Targeted RE-sequencing Annotation Tool) is a tool for facile navigation and mining of the variants from both targeted resequencing and whole exome sequencing. It provides a rich integration of publicly available as well as in-house developed annotations and visualizations for variants, variant-hosting genes and host-gene pathways.</Abstract>
    <Body>1 INTRODUCTION Next-generation sequencing offers the promise of scientific discovery with the challenge of results interpretation ( Schuster, 2008 ). One experiment such as exome sequencing can generate tens of thousands of single nucleotide variants (SNVs) and small insertions or deletions (INDELs), which must be elucidated in the search for disease associated mutations ( Ansorge, 2009 ;  Metzker, 2010 ). Whole exome sequencing is an application of NGS that has been successfully used to identify disease-associated variants in several monogenic disorders ( Gilissen  et al. , 2010 ;  Lupski  et al. , 2010 ;  Ng  et al. , 2009 ,  2010 ) and complex diseases ( Bonnefond  et al. , 2010 ;  Harbour  et al. , 2010 ). While these studies demonstrated the power of NGS, they also highlighted the challenge of efficiently sifting through thousands of variants to identify a subset that is potentially clinically relevant. Bioinformatics solutions are beginning to be released that address this challenge and facilitate filtering and interpretation of human sequence variation data ( Nix  et al. , 2010 ;  Sana  et al. , 2011 ;  Shetty  et al. , 2010 ;  Wang  et al. , 2010 ). We developed TREAT to extend the functionality of these tools and directly integrated structured and sortable formats with embedded hyperlinks to sequence alignment, gene specificity and gene pathway visualizations. In addition, to enable broad accessibility, we have fully deployed TREAT to the Amazon Cloud. TREAT is optionally offered as part of a complete workflow for exome or targeted sequencing, providing users with a convenient method for integrated sequence alignment, mutation detection and results interpretation. We believe this tool offers investigators with an accessible and convenient method for annotating and visualizing sequencing data and a means of efficiently identifying variants of interest. 2 METHODS AND RESULTS 2.1 Variant annotation TREAT provides four categories of variant annotations ( Supplementary Figure S1 ): (i) the general variant annotations which provide the physical locations, and the dbSNP IDs and allele frequencies of known variants from HapMap and 1000 Genome Pilot Project in Caucasian (CEU), Yoruban (YRI) and East Asian (CHB/JPT) populations; (ii) sample-specific read depths supporting A, C, G, T bases at each variant position, and the quality scores for base calls and read mappings. These annotations are only available when the users choose to use TREAT for read alignment and variant calling; (iii) publically available annotations from SIFT ( Kumar  et al. , 2009 ) and SeattleSeq ( http://gvs.gs.washington.edu/SeattleSeqAnnotation/ ) that include variant classifications (synonymous, missense, non-sense, frame-shift, etc.) and the predictions of the functional impact of the variants from SIFT and PolyPhen2 and (iv) in-house developed novel annotations including the tissue expression specificity measures for variant-hosting genes (detailed in  Supplementary Data  S2), and the identification of variants adjacent to exon–intron boundaries that potentially disrupt known splice-sites. An additional novel function of TREAT is the hyperlinks of each variant hosting gene to its associated KEGG pathway(s) ( http://www.genome.jp/kegg ) and Gene Ontology terms ( http://www.geneontology.org/ ). 2.2 Reporting and visualization TREAT automatically creates output in one easy-to-navigate HTML page, which provides the project description, QC reports, target coverage and sequencing depth information, descriptions of the annotations provided by TREAT, and links to the SNV and INDEL reports. The Microsoft Excel formatted SNV and INDEL reports provide row-based synopses of per-variant annotation. Each variant is hyperlinked to Integrative Genomics Viewer (IGV) ( Robinson  et al. , 2011 ) for the visualization of read alignments and variant calling information at the variant position. The functions of the variant hosting genes are illustrated via hyperlinks to the KEGG pathways and Gene Ontology terms, and the tissue expression specificity graph. 2.3 Access TREAT is deployed in two formats, a standalone annotation application and an integrated version for an end-to-end analysis of exome or targeted sequencing data. The standalone annotation tool takes the list of called variants as input files and allows users the flexibility of generating the variants using alignment and variant calling tools of their own choosing. The integrated version accepts either FASTQ or BAM files as input files and carries out sequence alignment using BWA ( Li and Durbin, 2009 ) or Bowie ( Langmead  et al. , 2009 ), local sequence re-alignment (GATK;  McKenna  et al. , 2010 ) and variant calling (GATK or SNVMix;  Goya  et al. , 2010 ), which provides users with a convenient solution to their informatics needs. Both TREAT versions can be downloaded for local runs, or can be launched on the Amazon Elastic Compute Cloud (EC2) ( http://en.wikipedia.org/wiki/Amazon_Elastic_Compute_Cloud ) using Amazon Machine Images provided at our Website. The Machine Images are loaded with all the open-source tools and necessary annotation files for the direct execution of TREAT. The run time and cost estimate of TREAT Cloud version are provided in the  Supplementary Data . 3 DISCUSSIONS We have developed a bioinformatics tool, TREAT, which addresses the current challenges in analyzing and interpreting targeted and whole exome sequencing data. The annotations provided by TREAT have been carefully evaluated and selected from a pool of available open source tools and databases, and complimented by additional in-house developed annotations (details at the TREAT website). The variant reports in Excel format integrate the visualizations of the sequence alignment at variant positions, pathways and expression specificity of the variant hosting genes via clickable hyperlinks for each reported INDELs and SNVs. In addition, the summary of the targeted resequencing results is stored in a centralized HTML report with links to the TREAT website, the targeted region coverage report and the read QC report, the description of the TREAT workflow, and links to the website of the annotation tools and databases. For maximum flexibility, two versions of TREAT were implemented: an annotation only version, and a version integrating read alignment, variant calling and annotations. Both versions can be downloaded as local installations or as Amazon Cloud images which makes TREAT available for users with no access to local bioinformatics infrastructures. By targeting all user groups and enabling rapid integration of emerging analytic methods, we believe that TREAT provides a sustainable NGS analytic workflow with wide applicability to the research community. We plan to continue adding new functionality and features to TREAT to make it a comprehensive tool for targeted and exome analysis. These include the development of an in-house variant database that collects all variants detected from hundreds of individuals with various types of diseases using exome and whole genome sequencing. This database will provide critical annotations whether the observed variants are truly ‘novel’ or disease specific. In addition, we are in the process of making TREAT applicable to whole genome sequencing data analysis, this would require adding annotation tracks for non-coding regions such as the conservations and regulatory domains. In summary, the rich set of annotations provided by TREAT, the easy to use, centralized HTML summary report, and the Excel-formatted variant reports with hyperlinked visualization utilities enable the filtering of detected variants based on their functional characteristics, and allow the researchers to navigate, filter and elucidate tens of thousands of variants to focus on potential disease-associated variant(s). 
          Supplementary Material 
          
             
                Supplementary Data 
             
             
             
          
       </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Gene expression models based on transcription factor binding events confer insight into functional cis-regulatory variants</Title>
    <Doi>10.1093/bioinformatics/bty992</Doi>
    <Authors>Shi Wenqiang, Fornes Oriol, Wasserman Wyeth W, Valencia Alfonso</Authors>
    <Abstract/>
    <Body>1 Introduction Understanding the functional roles of genetic variants in human disease is a fundamental challenge in medical genetics. Whole genome sequencing (WGS) enables clinicians to systematically seek variants that contribute to disease phenotype. Current clinical approaches focus primarily on the ∼2% of the genome coding for proteins, yet up to 88% of disease-related variants in genome-wide association studies are located within noncoding regions ( Hindorff  et al. , 2009 ). However, predicting the functional impact of noncoding variants remains a challenge. With the rapid accumulation of WGS data, there is a recognized need for bioinformatics methods that provide mechanistic insights into noncoding variants. Gene expression is a key intermediate phenotype for genetic studies. Substantial progress has been made on detecting statistical relationships between variants (single-nucleotide variants and short indels) and gene expression levels. These expression quantitative trait loci (eQTL) are enriched in regulatory regions, including promoters, enhancers and transcription factor (TF) bound regions, revealing the potential functional mechanisms of these variants ( Lappalainen  et al. , 2013 ;  The Encode Project Consortium, 2012 ). Partially based on the success of eQTL analysis, regression-based models trained on common variants (minor allele frequency, MAF ≥ 0.05) proximal to genes have been developed to predict gene expression levels ( Gamazon  et al. , 2015 ;  Manor and Segal, 2013 ). Such correlative approaches are useful, yet they lack the resolution to direct researchers to specific functional variants for two reasons. First, functional variants are hard to infer in association studies due to linkage disequilibrium (LD) between variants ( Farh  et al. , 2014 ). Second, uncommon variants (minor allele frequency, MAF &lt; 0.05) are excluded from most association studies, yet, rare variants (MAF &lt; 0.01) are often causal for familial genetic disorders ( Gibson, 2012 ;  Lappalainen, 2015 ) and have been hypothesized to contribute to human complex traits ( Bomba  et al. , 2017 ). Both reasons can be considered from the perspective of model feature engineering (i.e. how to incorporate model features). Current models use genotypes as predictors and defer the annotation of variant function until a model is constructed. To focus upon function, an alternative choice is to introduce model features emphasizing regulatory regions, which should increase the biological insights of gene expression models. It has been hypothesized that altered TF-binding events are a central mechanism by which  cis -regulatory variants impact gene expression ( Pai  et al. , 2015 ). TFs bind to specific locations in the genome, which can be identified experimentally by methods such as chromatin immunoprecipitation combined with sequencing (ChIP-seq). Machine learning approaches coupled to extensive TF ChIP-seq data have enabled predictions of TF-bound regions across the genome ( Zhou and Troyanskaya, 2015 ). Altered or disrupted TF-binding events have been associated with various diseases, including osteoarthritis ( Dodd  et al. , 2013 ), type-2 diabetes ( Claussnitzer  et al. , 2015 ) and colorectal cancer ( Wang  et al. , 2016 ). Recently, the compilation of altered TF-binding events has increased, and computational models have emerged to predict such events ( Chen  et al. , 2016 ;  Shi  et al. , 2016 ). However, bioinformatics approaches that quantify the relationship between altered TF-binding events and personalized gene expression levels remain elusive. To bridge this gap, we have developed TF2Exp models to infer relationships between personalized gene expression and altered TF-binding events caused by  cis -regulatory variants. We have explored the utility of TF2Exp in answering four important questions: (i) are alterations of TF-binding events predictive of personalized gene expression levels?; (ii) what are the characteristics of the functional altered TF-binding events?; (iii) do TF2Exp models perform as well as the state-of-the-art variant-based models?; and (iv) are TF2Exp models able to infer functional variants in LD and uncommon variants? Our results show that TF2Exp models successfully predict the alteration of gene expression for over three thousand genes, with an average performance comparable to that of models based on variants. Our framework systematically reveals the mechanism by which  cis -regulatory variants impact gene expression, providing unique interpretive capacity for future human genetic studies. 2 Materials and methods 2.1 Quantifying gene expression from RNA-seq data Lymphoblastoid cell line (LCL) RNA-seq and variant-calling data for 358 individuals from European populations were downloaded from the GEUVADIS project ( Lappalainen  et al. , 2013 ) and the 1000 Genomes Project ( 1000 Genomes Project Consortium  et al. , 2015 ) ( Supplementary Note s). Individuals covered 4 populations, including 89 Utah residents with Northern and Western European ancestry (CEU), 92 Finns (FIN), 86 British (GBR) and 91 Toscani (TSI). For each population, we built sex-specific transcriptomes in which SNP positions with MAF ≥ 0.05 were replaced by N (representing any of the four nucleotides A, C, G, T) using scripts from ( Grubert  et al. , 2015 ). RNA-seq data were processed using Sailfish (version 0.6.3) ( Patro  et al. , 2014 ), and expression levels of each gene were quantified as transcripts per million reads. The resulting expression data were normalized via multiple steps, including standardization, variation stabilization, quantile normalization and batch effects removal (i.e. population and gender, and 22 hidden covariates) by PEER ( Stegle  et al. , 2012 ) ( Supplementary Fig. S1 ). Any gene that was either on the sex chromosomes or showed near-zero variance in expression levels was removed, leaving 16 354 genes for model training. 2.2 Associating TF-binding events to genes using Hi-C data We obtained Hi-C proximity scores measuring physical interactions between DNA regions (Hi-C fragments) in GM12878 cells (an LCL) from ( Grubert  et al. , 2015 ). The average size of Hi-C fragments was 3.7 kb ( Grubert  et al. , 2015 ). For each gene, the proximal region was defined as the ±2 kb region centered at the start position of that gene [outermost transcript start position annotated by Ensembl ( Aken  et al. , 2016 ) in genome assembly GRCh37]. Proximal regions were extended to include any overlapping Hi-C fragments, and extended proximal regulatory regions have a median length of 11.3kb ( Supplementary Fig. S2 ). Within 1 Mb of gene body (as delimited by the outermost transcript start and end), distal regulatory regions were defined as Hi-C fragments interacting with (proximity score &gt; 0.4), but not overlapping, the proximal region of that gene. The median distance between distal regulatory regions and TSSs is 300.0kb ( Supplementary Fig. S3 ). Uniformly processed GM12878 DNase I hypersensitivity sites (DHSs) and ChIP-seq peaks for 77 TFs were downloaded from the ENCODE project ( The Encode Project Consortium, 2012 ). As DHS is a general indicator of TF binding ( Neph  et al. , 2012 ), DHSs are referred to as part of the set of TF ChIP-seq peaks within this manuscript for editorial convenience. A TF-binding event was associated to a gene if the ChIP-seq peak overlapped the proximal or a distal regulatory region of the gene. The resulting associations between genes and TF-binding events derived from GM12878 cells were used as the reference for all studied individuals. 2.3 Predicting sequence variation impact on TF-binding events Variant-calling data of each individual was downloaded from the 1000 Genomes Project (release 20130502) ( 1000 Genomes Project Consortium  et al. , 2015 ). We only considered single nucleotide variants and small indels (&lt;100bp). For each individual, the impact of a variant within a TF-binding event was evaluated as the binding score difference between the altered and reference alleles, as determined by the corresponding DeepSEA (v0.93) TF-binding model trained on GM12878 data ( Zhou and Troyanskaya, 2015 ). DeepSEA is a deep learning-based tool that accurately predicts the binding probability of a TF to any DNA sequence in certain cell types. To allow for the analysis of multiple variants within a TF-binding event, we modified DeepSEA to calculate the binding score of each allele using the 1100 bp region centered at the ChIP-seq peak-max position (the original code would center the 1100 bp region at each variant). Score differences of multiple variants within the same TF-binding event were aggregated to represent the overall alteration of that event. TF ChIP-seq peaks with multiple peak-max positions and overlapped peaks from the same experiment were split at the center of each pair of neighboring peak-max positions. At heterozygous positions, the binding score difference was divided by two. Lastly, we calculated the LD between variants across studies individuals using plink2 ( Chang  et al. , 2015 ). 2.4 Quantitative models of gene expression 
 LASSO regression on gene expression:  We developed a regression model to predict the expression level of a gene using altered TF-binding events associated with that gene based on the following equation:
 (1) Y i ∼ ∑ k = 1 n β k Δ T F i , k + ϵ 
where  Y i  is the expression levels of gene  i  across the studied individuals,  n  is the number of TF-binding events associated with gene  i ,  Δ T F i , k  is the alteration of TF-binding event  k  across the studied individuals and β k  is the effect size of TF-binding event k. In  equation (1) ,  Y i  is the response and  Δ T F i , k  is the input feature for the LASSO regression model, which was trained using the R glmnet package ( Friedman  et al. , 2010 ) on a training set of 358 LCLs. Features with near zero variance were filtered out before model training using the caret package ( Kuhn, 2015 ). Model performance was evaluated by 10-fold nested cross-validation, in which internal folds identified the optimal hyper-parameter lambda, and outer layers tested the model performance. Model performance was measured as the square of the correlation between predicted and observed expression levels (R 2 ). The trained models would select a subset of TF-binding events as key features of which effect sizes were not zero. When Hi-C proximity scores were used as the prior to select features, the prior (penalty.factor in the glmnet function) was set to ‘1 – proximity score’. 
 Defining TF-TF interactions:  For TFs known to interact in the BioGrid database ( Chatr-Aryamontri  et al. , 2015 ), we created interaction terms between pairs of TF-binding events (one from each TF) if they satisfied one of the following conditions: 1) two binding events overlapped by at least 200 bp; or 2) their regulatory regions were reported to interact in the Hi-C data. 
 Variant-based models:  For each gene, we trained regression models based on multiple variants to predict the expression level of that gene following the procedure as in the work of Gamazon  et al.  ( Gamazon  et al. , 2015 ). We only considered common variants (single-nucleotide variants and short indels with MAF ≥ 0.05) within 1 Mb of gene body regions. The regression formula for variant-based models is as follows:
 Y i ∼ ∑ k = 1 n β k X i , k + ϵ 
where  Y i  is the expression levels of gene  i  across studied individuals,  n  is the number of variants and  X i , k  is the number of minor alleles of  v a r i a n t i , k . 2.5 External validation with expression data For external validation of TF2Exp models on microarray data, we relied on expression levels of 15 997 Ensembl genes for LCLs of 80 CEU, 87 Chinese (CHB) and 89 Japanese (JPT) individuals ( Stranger  et al. , 2012 ). For these individuals, variant data was retrieved from the 1000 Genomes Project. We applied TF2Exp models to predict gene expression levels from potentially altered TF-binding events based on the variant data, and compared these predictions with the gene expression levels reported from the microarray. To test TF2Exp models on GTEx data ( GTEx Consortium, 2017 ), called genotype variants and expression levels derived from GTEx project were obtained from dbGAP (release phs000424.v7.p2). For each tissue, expression data were normalized similarly to LCLs (see section ‘Quantifying gene expression from RNA-seq data’): standardization of the data, followed by quantile normalization and batch effects removal (i.e. gender and 20 hidden covariates) by PEER ( Stegle  et al. , 2012 ). GTEx eQTLs data (version 6) were downloaded from  https://gtexportal.org/home/datasets . 3 Results 3.1 TF2Exp: regression models to predict the impact of altered TF binding on gene expression levels We developed TF2Exp, a gene-based computational framework to assess the impact of altered TF-binding events on gene expression levels ( Fig. 1 ). As detailed in Section 2, variant-calling data (single nucleotide variants and small indels) and gene expression data for 358 lymphoblastoid cell lines (LCLs) were obtained from the 1000 Genomes ( 1000 Genomes Project Consortium  et al. , 2015 ) and GEUVADIS projects ( Lappalainen  et al. , 2013 ). Moreover, TF-bound regions for 77 distinct TFs and DNase I hypersensitivity sites (DHSs) were obtained from the ENCODE project for the GM12878 LCL ( The Encode Project Consortium, 2012 ). TF-binding events (inclusive of DHSs) were associated to a gene if they overlapped either the proximal or distal regulatory region of that gene (see Section 2). The impact of each single variant within a TF-binding event was scored using DeepSEA ( Zhou and Troyanskaya, 2015 ), and multiple variants within the same TF-binding event were aggregated to generate an overall alteration score of that TF-binding event in each individual. On average, each gene had 420.0 altered TF-binding events within 36.6 regulatory regions (both proximal and distal) across the 358 individuals. Based on the computed alteration scores of TF-binding events in each individual, a regression model was trained by LASSO ( Friedman  et al. , 2010 ) for each gene to predict expression levels and to identify the key contributing TF-binding events. For users seeking to apply the trained models, note that TF2Exp models only require genotype data as input and will output the predicted expression changes for the corresponding genes. The TF2Exp framework is publicly available at  https://github.com/wqshi/TF2Exp .
 Fig. 1. Overview of the TF2Exp framework. ( A ) Infer regulatory regions and TF-binding events of each gene based on the reference cell line (GM12878). Distal regulatory regions are associated to a gene according to Hi-C data. TF-binding events on the proximal or distal regulatory regions of a gene are assigned to that gene. ( B ) Score the alteration of TF-binding events based on the overlapped variants for each individual. ( C ) Train regression models for each gene across the collected individuals 3.2 TF2Exp predicts the expression levels for a subset of genes We successfully trained TF2Exp models for 15 887 genes. Average model performance (R 2 ) by 10-fold cross-validation was 0.049, with most models having low predictive power ( Fig. 2 ). To assess the impact of random noise in the model training process, we set up control models in which gene expression levels were shuffled across individuals while preserving TF binding features. Control models achieved an average R 2  of only 3.6 × 10 −5  ( Fig. 2 ), indicating that the signal captured by TF2Exp models is not random. Repeating the randomization process 20 times for the genes on chr1 showed mean performance of 4.0 × 10 −5  and maximum performance of 0.048, respectively. To focus on predictive models, we applied an R 2  threshold of 0.05 as in ( Manor and Segal, 2013 ), resulting in models for 20.1% of genes (hereinafter referred to as predictable genes). As in the work of Manor  et al.  ( Manor and Segal, 2013 ), we observed a significant correlation between model performance and variance of expression levels for the predictable genes (Spearman’s correlation = 0.21,  P -value &lt; 2.2 × 10 −16 ;  Supplementary Fig. S4 ). We performed gene ontology (GO) enrichment analysis using GREAT ( McLean  et al. , 2010 ). The top 10% predictable genes were enriched in pathways including graft-versus-host disease and allograft rejection, terms which are relevant to the roles of B cells (i.e. the cell type before transforming to LCL) in the immune system. In contrast, we did not observe any enriched GO term for the top 10% of genes with the highest expression variance, highlighting the ability of TF2Exp to capture expression levels relevant to the sample of interest.
 Fig. 2. Performance comparison of alternative TF2Exp models. For each type of TF2Exp model, performances (R 2 ) of investigated genes (y axis) are plotted in ascending order with respect to the cumulative percentage of genes (x axis). The horizontal dashed line indicates the defined performance threshold of 0.05 for predictable genes We next assessed whether prior knowledge, such as Hi-C proximity scores and known TF-TF physical interactions, could improve TF2Exp models. We introduced the proximity scores of Hi-C interactions to guide model fitting, so that TF-binding events on highly-interacting regions would be less regularized by LASSO (Section 2). We observed that adding Hi-C proximity scores resulted in a slight R 2  improvement of 1.6 × 10 −3  (Wilcoxon signed-rank test,  P -value = 1.5 × 10 −41 ), suggesting that the original TF2Exp models had captured most of the signal from the Hi-C data. We also tested models including interaction terms for known TF-TF physical interactions (Section 2). Adding TF-TF interactions significantly reduced model performance by 2.2 × 10 −3  (Wilcoxon signed-rank test,  P -value = 4.6 × 10 −152 ,  Fig. 2 ), potentially due to our incomplete knowledge of TF-TF interactions and/or limited training samples. Taken together, models incorporating prior knowledge achieved similar performance to the original ones. Thus, we focused on the original (and simpler) TF2Exp models in the next stages of the analysis.  3.3 Alterations of DHS, CTCF and tissue-specific TF binding are the most frequently selected features We next sought to identify TFs for which binding events were more frequently selected in TF2Exp models. For the predictable genes, models selected an average of 4.2 key features (where a feature was the alteration score of a single TF-binding event). Frequently selected TFs had more binding events across the genome (Pearson correlation 0.97,  P -value &lt; 2.2 × 10 −16 ). The top 5 selected TF features were DHS, RUNX3, CTCF, EBF1 and PU.1, accounting for 33.6% of the selected features ( Fig. 3 ). Particularly, 42.0% of the predictable genes had at least one DHS feature, which is in agreement with the well-known relationship between chromatin accessibility and gene expression ( Natarajan  et al. , 2012 ). CTCF has diverse roles in gene regulation across multiple tissues ( Ong and Corces, 2014 ), and the remaining three TFs perform important roles in LCL tissue-specific regulation: RUNX3 in immunity and inflammation ( Lotem  et al. , 2015 ), EBF1 in B lymphocyte transcriptional network expression ( Hagman  et al. , 2012 ) and PU.1 in lymphoid development ( Iwafuchi-Doi and Zaret, 2014 ). Lastly, we observed that RAD21 and SMC3, despite being among the top 10 TFs with the highest number of peaks in the training set ( Fig. 3 ), were selected less frequently than the other 8 TFs (&lt;0.65%), in accordance with their nature of non-sequence specific DNA-binding.
 Fig. 3. Top 10 TFs with the highest number of binding events and their selection frequency in predictable genes. Red bars indicate the total number of TF-binding events selected by TF2Exp models. Blue bars indicate the total number of genes that selected binding events of the indicated TF as key features. The percentage on top of each red bar indicates the ratio between the number of selected events in TF2Exp models and the total number of peaks for that TF 3.4 Selected TF-binding events correlate with gene expression levels in vivo We next sought to assess whether  in vivo  TF binding of selected features correlated with gene expression levels. We obtained CTCF and PU.1 ChIP-seq LCL data for two independent sets of 45 originally-training individuals (see  Supplementary Note s). TF-binding signals were extracted from the reference GM12878 TF-binding events (i.e. the ChIP-seq features used in the TF2Exp for model construction). In predictable genes, 83 CTCF and 72 PU.1 binding events were selected for testing based on their high variance of binding score change (see Section 2). Eight CTCF (9.7%) and seven PU.1 (9.6%) of the tested  in vivo  binding events significantly correlated with gene expression levels (Pearson correlation, FDR &lt;0.05), and their correlation coefficients were consistent with the correlation estimated between the TF sequence alteration score and gene expression ( P -value= 1.4 × 10 −4 , coefficient = 0.81). Due to limited size of test samples (n = 45), we did not have sufficient statistical power to detect weakly correlated TF-gene relationships (e.g. coefficient &lt; 0.29, see  Supplementary Note s), which accounted for most (89.7%) of the tested  in vivo  binding events. In summary, we observed that 9.7% of TF-binding events selected by TF2Exp displayed detectable correlation (correlation coefficient &gt; 0.29) between  in vivo  binding and gene expression levels. 3.5 Selected TF-binding events within proximal regions have greater effect sizes We next examined the locations and effect sizes of selected features. In proximal regions, selected features were mostly within 10 kb from gene start positions, while, in distal regulatory regions, they were distributed within ∼500 kb. We observed significant depletion of selected features in distal regulatory regions compared with proximal regions (Fisher’s exact test, odds ratio = 0.11,  P -value &lt; 2.2 × 10 −16 ). Effect sizes of TF-binding events decreased rapidly with respect to the distance from gene start positions ( Supplementary Fig. S5A ). Such a trend has been reported for effect sizes of eQTLs ( Battle  et al. , 2014 ). Selected features in proximal regions also exhibited significantly larger absolute effect sizes (Wilcoxon rank-sum test,  P -value = 7.3 × 10 −68 ,  Supplementary Fig. S5B ) and more positive effects (Wilcoxon rank-sum test,  P -value = 4.22 × 10 −5 ) than features in distal regulatory regions. Nevertheless, the selected distal features of a gene were significantly enriched in the enhancer regions associated to that gene, as specified in the FANTOM5 project ( Andersson  et al. , 2014 ) (Fisher's exact test, odds ratio = 1.3,  P -value = 0.002, see  Supplementary Note s), supporting a functional role of the selected distal TF-binding events. Thus, TF2Exp models are identifying  cis -regulatory sequence variants that bring functional insights into the mechanisms underlying gene expression levels. 3.6 Uncommon variants increase the number of predictable genes As TF2Exp models can distinguish the impact of variants in TF-binding events, we investigated the contribution of uncommon (MAF &lt; 0.05) variants to model performance. TF2Exp models trained only on uncommon variants achieved higher average performance (R 2  = 0.004) than control models, indicating that the contribution of uncommon variants was not random. To further explore the predictive potential of uncommon variants, we divided TF-binding events into two classes: (i) events altered only by uncommon variants (i.e. do not include any common variants); and (ii) the rest of events overlapping at least one common variant. Note that, by definition, class-2 events can still contain uncommon variants. After adding class-1 events on top of class-2 events, there was a mean performance improvement of 3.8 × 10 −4 , and the number of predictable genes increased to 3177 compared with 3139 genes for models trained only on class-2 events. To test whether this observation was due to random effects, we shuffled class-1 events across individuals. For shuffled models, the number of predictable genes decreased to 3076, suggesting that the benefit of using TF-binding events altered only by uncommon variants was not random. Moreover, in pairwise comparison between the two models for the same gene (i.e. adding class-1 events or not), 31.4% of shared predictable genes gained performance increase (6.7×10 −3  on average) after adding, while 37.2% of genes showed performance decrease but with smaller alteration (4.7×10 −3  on average). Furthermore, the newly selected features in the improved models were enriched at proximal regulatory regions compared with those models with decreased performance (Fisher's exact test,  P -value = 3.2 × 10 −5 , odd ratio = 1.8). To avoid noisy uncommon variants, we next focused on TF-binding events altered only by uncommon variants (class-1 events) within proximal regions. After adding these events, the number of predictable genes increased to 3179. Although the two types of models performed similarly for most cases (Wilcoxon rank sum test,  P -value = 0.51;  Fig. 4 ), there was a significant mean performance improvement of 5.9 × 10 −4  after adding class-1 events (Welch's t-test,  P -value = 4 × 10 −4 ). While 95% of the genes showed small absolute performance alteration (1.6 × 10 −3  of R 2  on average), for the remaining ∼5% of genes, adding class-1 events enabled significantly larger performance improvement ( Fig. 4 ; Wilcoxon rank sum test,  P -value = 4.6 × 10 −5 , estimated difference = 1.5 × 10 −2 ).
 Fig. 4. Performance comparison after adding uncommon-variant only events in proximal regulatory regions. Each dot represents an evaluated gene-model. Coordinates ( x  and  y ) indicate the cross-validation performances before or after adding TF-binding events altered only by uncommon variants in proximal regulatory regions, respectively. The dot shape indicates the magnitude of absolute performance alteration, solid for small alteration (&lt;1.6 × 10 −3 , 95% quantile of the absolute performance alteration) and circle for the rest genes with larger alteration 3.7 Alteration of TF-binding events improve the performance of variant-based models We compared our TF2Exp models with state-of-the-art models that predict alteration of gene expression levels based on proximal variants ( Gamazon  et al. , 2015 ;  Manor and Segal, 2013 ) (see Section 2). First, we trained TF2Exp and variant-based models on the same set of common variants (variants within TF-binding events, MAF ≥ 0.05) for each gene. Both models showed comparable performance across the shared predictable genes (Wilcoxon signed-rank test,  P -value = 0.15;  Supplementary Fig. S6 ). In addition, the default variant-based models using all the proximal common variants within 1 Mb of the gene body showed better performance than TF2Exp models trained on common variants (Wilcoxon signed-rank test,  P -value = 0.06), potentially due to variants in unknown TF-binding events. As uncommon variants are informative for a subset of TF2Exp models, we trained models on combined features of the default variant-based model and the default TF2Exp model for each gene. Combined models achieved better performance than variant-based models (Wilcoxon signed-rank test,  P -value = 0.02, estimated median difference = 1.8 × 10 −4 ), in agreement with the positive contribution of proximal uncommon variants observed in the previous section. We further explored whether introducing binding events of new TFs would improve model performance. We collected and added TF-binding events of 92 new TFs from other cell types (175 ENCODE ChIP-seq datasets) to TF2Exp models, and tested the model performance of genes in 5 chromosomes (chr1-2 and chr20-22). High performance genes (R 2  &gt; 0.25) in TF2Exp models gained significant improvement (Wilcoxon signed-rank test,  P -value = 0.03, estimated median difference = 5.3 × 10 − 4 ) after the addition of binding events for new TFs, while the rest of the genes were better represented with the original models (Wilcoxon signed-rank test,  P -value = 7.7 × 10 − 5 , estimated median difference = 1.6 × 10 − 3 ). These findings suggest that additional TF sets are informative, but model performance is limited by the size of training samples.  3.8 TF2Exp models distinguish variants in LD compared with variant-based models Unlike variant-based models, TF2Exp models are able to infer the functional roles of variants in linkage disequilibrium (LD) based on the predicted impact of variants on TF-bound regions. Comparing TF2Exp and variant models on the same set of common variants (variants within TF-binding events), most selected binding events in TF2Exp models (62.7%,  n  = 12 663) overlapped selected variants (59.8%,  n  = 9386) in variant-based models for the same gene. Of the total of overlapped variants, 18.4% were in high LD (r 2  &gt; 0.9) with other variants in the same TF-bound regions, hindering the inference of the causal variants by variant-based models. Using TF2Exp models, we found that 36.8% of the linked variants showed at least a two-fold impact on the overlapped TF-bound region compared with the selected variants ( Supplementary Fig. S7 ), suggesting a more dominant contribution of the linked variants. In addition, a subset of selected variants (20.1%) overlapped with more than one selected TF-binding event, indicating that individual variant could alter multiple mechanisms of gene regulation. Overall, TF2Exp models provide a quantitative way to evaluate the impact of variants in LD, suggesting a broader utility for genomic studies than variant-based models. 3.9 TF2Exp models exhibit robust performance in external validation datasets We finally sought to evaluate TF2Exp models of predictable genes on external datasets. We obtained microarray expression data from LCLs of 256 individuals ( Stranger  et al. , 2012 ), including 80 Utah residents with Northern and Western European ancestry (CEU), 87 Chinese (CHB) and 89 Japanese (JPT) (Section 2). As 79 of the CEU individuals overlapped with the training individuals of TF2Exp models, we first evaluated the agreement between the microarray and RNA-seq data on these individuals. Relative expression levels across all genes within each individual were concordant between microarray and RNA-seq experiments (average Spearman’s correlation = 0.76), supporting an overall consistency between the two datasets. However, when we considered a single gene across the 79 individuals, the correlation between the two platforms was low (average Spearman correlation = 0.19). Therefore, we expected models trained on RNA-seq data to have an upper limit performance when applied to microarray data. We used TF2Exp models (trained on CEU individuals) to predict gene expression levels on the CHB and JPT individuals. Predictable TF2Exp models achieved an average correlation of 0.16 for both populations. Similarly, predictable variant-based models achieved an average correlation of 0.17 for both populations. An example of a high performing gene (FAM105A) in the external validation is illustrated in  Figure 5 . FAM105A is associated with pancreatic islet function and type 2 diabetes ( Pedersen  et al. , 2017 ;  Taneera  et al. , 2015 ). For this gene, TF2Exp identified 4 contributing TF-binding events ( Fig. 5 ), of which two of them had greater weights: DHS (chr22: 45711760-45711910, effect size: −0.325) and MEF2A (chr22: 45771822-45772122, effect size: 0.334). Alterations of these key events largely explained the changes of gene expression across the different individuals. For example, NA18640 had the lowest observed expression level in CHB individuals, as variant rs104664 of this individual was predicted by TF2Exp to increase the score of DHS; while rs5765304 in NA18573 increased MEF2A binding scores, resulting in the highest predicted expression across all individuals.
 Fig. 5. Key features of TF2Exp for FAM105A gene in the external validation set. The top panel illustrates the key TF-binding events learned from the training datasets. The figure legend is the same as for  Figure 1 . The middle and bottom panel show the variants within the key TF -binding events and their inferred roles on gene expression for the two individuals To test model performance on tissues other than LCLs, we applied TF2Exp models of predictable genes on chromosome 1 to ten tissues with the largest sample sizes from the GTEx project ( GTEx Consortium, 2017 ). Average Pearson correlations between predicted and observed expression levels ranged from 0.11 to 0.08 across the 10 tissues ( Supplementary Table S1 ). Despite the expected performance loss in non-LCL tissues, TF2Exp models displayed robust performance, highlighting their potential application to other (non LCL) cells or tissues. Moreover, for eQTLs identified by GTEx project in these tissues on chromosome 1, TF2Exp correctly predicts the direction of expression change for 79% of the variants within key TF-bound regions and, akin to the assessment used in the recently published ExPecto paper ( Zhou  et al. , 2018 ), for 91% of top 500 variants with strongest predicted expression impact overlapping key TF-bound regions within 20 kb of the TSS. 4 Discussion Deciphering the functional roles of regulatory variants is a critical challenge in the post-sequencing era. To address this challenge, we have introduced a novel framework, TF2Exp, which uses alterations of TF binding as key features to elucidate the functional impact of regulatory variants and predict personalized gene expression levels. TF2Exp models based on lymphoblastoid cell line data showed predictive capacity for 3196 genes, incorporating an average of 4.2 altered TF-binding events per gene model. The most frequently selected TF-binding events included both general properties (e.g. alterations within DNase I hypersensitive regions) and tissue-specific properties (e.g. alterations in TF-bound regions for TFs relevant to the studied lymphoblastoid samples). TF2Exp models could incorporate uncommon variants to improve model performance, and provide mechanistic insights into  cis -regulatory variants. TF2Exp models have the potential to address two challenges left unresolved by variant-based models and classical eQTL studies. For these approaches, it is difficult to: (i) infer variant function (the studied variants can be in high linkage disequilibrium with many others); and (ii) evaluate the impact of uncommon variants (which are excluded from such analyses). By treating TF-binding events as functional units, TF2Exp models can evaluate the relative impact of any variant (single nucleotide variants or small indels) within a TF-bound region. As in the example presented in  Figure 5 , for individual variants, the derived impact within the model is independent of linkage disequilibrium or allele frequency. Moreover, even though the inclusion of uncommon (and rare) variants only improved model performance for a small portion of genes, the resulting TF2Exp models offer a unique advantage for the inference of functional  cis- regulatory variants, compared with previous variant-based methods ( Gamazon  et al. , 2015 ;  Manor and Segal, 2013 ). Similarly to variant-based methods, the predictive performance of TF2Exp models is limited, showing utility only for a subset of genes (20.1%), and even within these genes, model performance was modest (R 2  = 0.21). Such a limited performance is likely attributable to multiple causes. First, variance of gene expression due to common variants is quite low [e.g. 15.3% as estimated by  Gamazon  et al.  (2015 )], suggesting that models restricted to DNA sequence features alone can only account for a portion of the observed variance in gene expression levels. Second, TF2Exp models are limited by the availability of ChIP-seq data (78 TFs in LCLs), while transcriptome studies have revealed that human cells express an average of 430 TFs ( The Fantom Consortium, 2014 ). Though we anticipate an increasing amount of available ChIP-seq data in the future, a practical and more immediate solution would be to computationally predict TF-bound regions. To improve TF-binding prediction, multiple tools combine DNA sequence and chromatin properties (e.g. ATAC-seq, DHS or histone modifications) of the target tissue ( Pique-Regi  et al. , 2011 ); these types of data are currently available for &gt;100 primary tissues and cells ( Roadmap Epigenomics Consortium  et al. , 2015 ). Third, TF2Exp models focus on TF-binding events potentially involved in transcriptional regulation, but other regulatory mechanisms (e.g. post-transcriptional regulation) or genomic features (e.g. DNA methylation or sequence conservation) might explain an additional portion of the observed variance of gene expression. Fourth, TF2Exp models are likely constrained by the small number of available training samples, as including additional features (e.g. TF-TF interactions and uncommon variants) decreased model performance. We expect that the expansion of reference transcriptome datasets will provide more samples for exploring more complex relationships between genes and TF-binding events, thereby improving model performance. During the review process of this manuscript, a new related tool named ExPecto was published ( Zhou  et al. , 2018 ). Though both TF2Exp and ExPecto take DNA sequences as input to predict gene expression levels, the tools differ significantly from each other as they focus on distinct types of variance of gene expression. ExPecto uses a single regression model per tissue to predict the variance of gene expression levels across all the genes (i.e. ‘intra-individual’ variance). In contrast, TF2Exp uses one model per gene per tissue to predict the variance of gene expression levels of that gene across the different individuals (i.e. ‘inter-individual’ variance). Past studies have shown that intra-individual variance is easier to predict than inter-individual variance based on regulatory features. For instance, H3K27ac levels correlate well with broad gene expression in CD4+ T cells (intra-individual variance; Pearson’s correlation coefficient = 0.72) ( Karlic  et al. , 2010 ), while in lymphoblastoid cells from 47 individuals, they only correlate with the expression levels of 22% of genes (inter-individual variance) ( Waszak  et al. , 2015 ). While the reported model performance measures for TF2Exp and ExPecto are not directly comparable, emerging independent datasets should enable benchmarking of the two tools in the future. It should also be noted that while the performance of ExPecto did not increase with the inclusion of distal regions (more than 20 kb distal from the TSS), the inclusion of distal features in TF2Exp was beneficial (as shown in  Supplementary Fig. S5 ). We suspect that such difference lies in the TF2Exp focus on candidate distal regulatory regions supported by Hi-C and TF binding data. In conclusion, identifying the impact of  cis -regulatory variants on gene expression is a critical step towards understanding the genetic mechanisms contributing to diseases. TF2Exp models are able to predict the impact of TF-binding on gene expression levels and provide mechanistic insights into the roles of selected TF-binding events and  cis -regulatory variants. We anticipate that future enlarged omics data, in LCLs and other cell types, will greatly expand the application scope of TF2Exp models. Supplementary Material bty992_Supplementary_Material Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Identification of causal genes for complex traits</Title>
    <Doi>10.1093/bioinformatics/btv240</Doi>
    <Authors>Hormozdiari Farhad, Kichaev Gleb, Yang Wen-Yun, Pasaniuc Bogdan, Eskin Eleazar</Authors>
    <Abstract>Motivation: Although genome-wide association studies (GWAS) have identified thousands of variants associated with common diseases and complex traits, only a handful of these variants are validated to be causal. We consider ‘causal variants’ as variants which are responsible for the association signal at a locus. As opposed to association studies that benefit from linkage disequilibrium (LD), the main challenge in identifying causal variants at associated loci lies in distinguishing among the many closely correlated variants due to LD. This is particularly important for model organisms such as inbred mice, where LD extends much further than in human populations, resulting in large stretches of the genome with significantly associated variants. Furthermore, these model organisms are highly structured and require correction for population structure to remove potential spurious associations.</Abstract>
    <Body>1 Introduction Genome-wide association studies (GWAS) have been extremely successful in reproducibly identifying variants associated with various complex traits and diseases ( Altshuler  et al. , 2008 ;  Hakonarson  et al. , 2007 ;  International Multiple Sclerosis Genetics Consortium  et al. , 2013 ;  Kottgen  et al. , 2013 ;  Ripke  et al. , 2013 ). The most common type of genetic variants comes in the form of single nucleotide polymorphisms (SNPs), which we make the focus of this study. Because of the correlation structure in the genome, a phenomenon referred to as linkage disequilibrium (LD) ( Pritchard and Przeworski, 2001 ;  Reich  et al. , 2001 ), each GWAS-associated variant will typically have hundreds to thousands of other variants which are also significantly associated with the trait. Identifying the variants responsible for the observed effect on a trait is referred to as fine mapping ( Hormozdiari  et al. , 2014 ;  Kichaev  et al. , 2014 ;  Maller  et al. , 2012 ;  Yang  et al. , 2012 ). In the context of association studies, the genetic variants which are responsible for the association signal at a locus are referred to in the genetics literature as the ‘causal variants’. Causal variants have biological effect on the phenotype. Generally, variants can be categorized into three main groups. The first group is the causal variants which have a biological effect on the phenotype and are responsible for the association signal. The second group is the variants which are statistically associated with the phenotype due to LD with a causal variant. Even though association tests for these variants may be statistically significant, under our definition, they are not causal variants. The third group is the variants which are not statistically associated with the phenotype and are not causal. We note that this usage of the term causal has little to do with the concept of causal inference as described in the computer science and statistics literatures ( Pearl, 2000 ;  Spirtes  et al. , 2000 ). Fine-mapping methods take as input the full set of association signals in a region and attempt to identify a minimum set of variants that explains the association signals. A common approach is to calculate marginal association statistics for each variant and, depending on the study budget, select the top  K  ranked variants for follow-up studies. However, the local correlation structure at a fine-mapping locus will induce similar association statistics at neighboring, non-causals variants, thereby making this approach suboptimal in this context. Furthermore, it fails to provide a guarantee that the true causal variant is selected. A recent work ( Maller  et al. , 2012 ) addressed this issue by estimating the probabilities for variants to be causal under the simplifying assumption that each fine-mapping locus contains a single causal variant. Ranking variants based on association strength (similar to top  k ) and this probabilistic approach ( Maller  et al. , 2012 ) assuming a single causal variant give identical relative rankings. However, the probabilistic approach provides the added benefit that we can now select enough variants to guarantee that we have captured the true causal variants with ρ level of confidence. Unfortunately, the key underlying assumption that a fine-mapping locus contains a single causal variant is likely to be invalidated at many risk loci ( Hormozdiari  et al. , 2014 ;  Kichaev  et al. , 2014 ). For regions that putatively harbor multiple independent signals, a common strategy is to use iterative conditioning to tease out secondary signals ( Yang  et al. , 2012 ). This process is analogous to forward stepwise regression, where at each iteration, the variant with the strongest association is selected to enter the model and then marginal statistical scores are re-computed for the remaining variants condition on the ones that have been selected. This process is repeated until there are no remaining variants that are statistically significant. However, it has been shown that this approach is highly sub-optimal ( Hormozdiari  et al. , 2014 ;  Kichaev  et al. , 2014 ) due to lack of LD consideration. To address these issues, we recently proposed probabilistic fine-mapping methods ( Hormozdiari  et al. , 2014 ;  Kichaev  et al. , 2014 ) that build on the concept of a standard confidence interval by providing a well-calibrated, minimally sized confidence set of variants using principled, LD-aware modeling of multiple causal variants. In these methods, we assign probability to each variant to be causal and subsequently select the smallest number of variants that achieve the desired posterior probability. Many accurate fine-mapping methods have been designed for human studies where there are a relatively small number of associated variants in a region. In model organism studies, however, pervasive LD patterns result in GWAS-associated loci that may span several megabases and contain thousands of variants and dozens of genes. For example, in a widely utilized design for mouse studies, the Hybrid Mouse Diversity Panel (HMDP) ( Bennett  et al. , 2010 )—the typical associated region—is approximately 1–2 megabases. Identifying which genes underlie an associated locus in model organism studies is a major, labor-intensive process involving generating gene knockouts. Therefore, it is often the case that identifying the causal genes at an associated locus requires a larger effort than the initial GWAS ( Flint and Eskin, 2012 ). In addition to large LD blocks, fine-mapping studies in model organisms are complicated by population structure (i.e. the complex genetic relationship between different individuals in the study;  Flint and Eskin, 2012 ;  Kang  et al. , 2008 ;  Price  et al. , 2006 ) that invalidate commonly used association statistics that assume the individuals in the study are independent. Model organisms such as mice have a high level of population structure, typically larger than what is observed in human populations; therefore, correcting for the population structure for mouse GWAS is imperative to mitigate the chance of false positive signals of association ( Flint and Eskin, 2012 ;  Kang  et al. , 2008 ;  Price  et al. , 2006 ). In this article, we propose CAVIAR-Gene (CAusal Variants Identification in Associated Regions), a statistical method for fine mapping that addresses two main limitations of existing methods. First, as opposed to existing approaches that focus on individual variants, we propose to search only over the space of gene combinations that explain the statistical association signal, and thus drastically reduce runtime. Second, CAVIAR-Gene extends existing framework for fine mapping to account for population structure. The output of our approach is a minimal set of genes that will contain the true casual gene at a pre-specified significance level. This gene set together with its individual gene probability of causality provides a natural way of prioritizing genes for functional testing (e.g. knockout strategies) in model organisms. Through extensive simulations, we demonstrate that CAVIAR-Gene is superior to existing methodologies, requiring the smallest set of genes to follow-up in order to capture the true causal gene(s). To validate our approach, we applied CAVIAR-Gene to real mouse data and found that we can successfully recover  Apoa2 , a known causal gene for high-density lipoprotein (HDL) ( Flint and Eskin, 2012 ;  van Nas  et al. , 2009 ), for the HDL phenotype in the HMDP. 2 Methods 2.1 Overview of CAVIAR-Gene CAVIAR-Gene takes as input the marginal statistics for each variant at a locus, an LD matrix consisting of pairwise Pearson correlations computed between the genotypes of a pair of genetic variants, a partitioning of the set of variants in a locus into genes, and the kinship matrix which indicates the genetic similarity between each pair of individuals. Marginal statistics are computed using methods that correct for population structure ( Kang  et al. , 2008 ;  Lippert  et al. , 2011 ;  Listgarten  et al. , 2012 ;  Zhou and Stephens, 2012 ). We consider a variant to be causal when the variant is responsible for the association signal at a locus and aim to discriminate these variants from ones that are correlated due to LD. Our previous proposed method CAVIAR, is a statistical framework that provides a ‘ρ causal set’ that is defined as the set of variants that contain all the causal variants with probability of at least ρ. The intuition is that due to LD structure, it is impossible to identify exactly the causal variants, but it is possible to identify a set which contains these causal variants. CAVIAR was designed to work on human GWAS where we deal with regions that have at most 100 variants in a locus and we consider all possible causal combinations of at most 6 causal variants to detect the ρ causal set. However, in model organisms, the large stretches of LD regions result in a large number of variants associated in each region, thus making CAVIAR computationally infeasible. CAVIAR-Gene mitigates this problem by associating each variant to a proximal gene, and instead, operating on the gene level, thus reducing the computational burden by an order of magnitude while facilitating interpreting of GWAS results. Similarly, CAVIAR-Gene detects a ‘ρ causal gene set’ which is a set of genes in the locus that will contain the actual causal genes with probability of at least ρ. Note that not all the genes selected in the ρ causal gene set will be causal. A trivial solution to this problem would be to output all the genes as the ρ causal gene set. However, because this provides no additional information, we are interested in detecting the ρ causal gene set which has the minimum number of genes. We demonstrate that CAVIAR-Gene is well-calibrated as it fails to detect the actual causal gene 1−ρ fraction of the time. 2.2 Standard GWAS Consider a GWAS on a quantitative trait where we collect phenotypic values for  n  individuals and genotype all the individuals on  m  variants. Let  y i  indicate the phenotypic value of the  i th individual and  g i k ∈ { 0 , 1 , 2 }  indicate the minor allele count of the  i th individual for the  k th variant. We use  Y  to denote the  ( n × 1 )  vector of phenotypic values and  X k  to denote the  ( n × 1 )  vector of normalized genotype values for the  k th variant for all the  n  individuals in the study. Without loss of generality, we assume that genotype values for each variant have been standardized to have mean 0 and variance 1 yielding the following relationships:  1 T  X k  = 0 and  X k T X k = n , where  1  denotes the  ( n × 1 )  vector of ones. We assume that the data generating model follows a linear additive model, and for simplicity the variant  c  is the only variant associated (causal) with the phenotype. Each variant is categorized into one of the three groups. The first group is variants which are associated with the phenotype and are considered causal. The second group is variants which are statistically associated with the phenotype due to LD with a causal variant—these variants are considered not causal. The third group is variants which are not associated with the phenotype and are considered not causal. Standard GWAS analysis for the  c th variant is performed utilizing the following model equation:
 (1) Y = μ 1 + β c X c + e 
where  μ  is the mean of the phenotypic values,  β c  is the effect size of the  c th variant, and  e  is the residual noise. In this model, the residual error is the  ( n × 1 )  vector of i.i.d and normally distributed error. Let  e ∼ N ( 0 , σ e 2 I ) , where  I  is the ( n  ×  n ) identity matrix and  σ e  is a covariance scalar. The estimates of  β c , which are indicated by  β c ^ , are obtained by maximizing the likelihood,
 β ^ c = X c T Y X c T X c , β ^ c ∼ N ( β c , σ e 2 ( X c T X c ) ) 
and the statistics is computed as follows:
 S c = β ^ c σ e ^ ( X c T X c ) ∼ N ( λ c , 1 ) . 
where  λ c  is the non-centrality parameter (NCP) and is equal to  β c σ e n . We obtain the estimated value for  μ ,  e , and  σ e  as follows:  μ ^ = 1 T X c n ,   e ^ = Y − 1 μ ^ − β ^ c X c , and  σ ^ e = e ^ T e ^ n − 2 . 2.3 The effect of LD in GWAS In the previous section, we consider that there is only one variant (variant c), and this variant is causal. Now, we extend the previous case and for simplicity we assume there are two variants,  c  and  k . Similar to the previous section, the variant  c  is causal and variant  k  is correlated to  c  through LD but has no phenotypic effect. The correlation between the two variants is  r  which is approximated by  1 n X k T X c . Thus, the estimate for the effect size for the variant  k  is as follows:
 β ^ k = X k T Y X k T X k , β ^ k ∼ N ( r β c , σ e 2 ( X k T X k ) ) 
and the statistics is computed as follows:
 S k = β ^ k σ e ^ ( X k T X k ) ∼ N ( r λ c , 1 ) . 
 We compute the covariance between the estimated effect size of the two variants as follows:
 Cov ( S c , S i ) = Cov ( ( X c T X c ) β ^ c σ e , ( X i T X i ) β ^ k σ e ) = 1 σ e 2 Cov ( X c T Y X c T X c , X i T Y X i T X i )   = X i T X c X i T X i X c T X c = r . 
 Thus, the joint distribution of the marginal association statistics for the two variants given their NCPs follows a multivariate normal distribution (MVN),
 ( [ S i S j ] | [ λ i λ j ] ) ∼ N ( [ λ i λ j ] , [ 1 r i j r i j 1 ] ) ,   
where  r ij  is the genotype correlation between the  i th and  j th variants. In the case that both variants are not causal, we have  λ i = λ j = 0 . In the case that the  j th variant is causal and the  i th variant is not causal, we have  λ i = r λ j . In the case that  j th variant is not causal and the  i th variant is causal, we have  λ j = r λ i . This result is known from previous studies ( Han  et al. , 2009 ;  Hormozdiari  et al. , 2014 ;  Kichaev  et al. , 2014 ;  Zaitlen  et al. , 2010 ). 2.4 Computing the likelihood of causal SNP status from GWAS data Given a set of  m  variants, the pair-wise correlations denoted by Σ, we use the  ( m × 1 )  vector  S = [ S 1 … S m ] T  to denote the marginal association statistics. We extend the joint distribution mentioned above for  m  variants. The joint distribution follows an MVN distribution,
 (2) ( S | Λ ) ∼ N ( Σ Λ , Σ ) , 
where Λ is the  ( m × 1 )  vector of normalized true effect sizes and Σ is a  ( m × m )  matrix of pair-wise genotype correlations between different SNPs. Let  X = [ X 1 , X 2 ⋯ X m ]  be a  n  ×  m  matrix of genotype. We can approximate Σ using genotype data as follows:  Σ = 1 n X T X . In CAVIAR ( Hormozdiari  et al. , 2014 ), we introduce a new parameter  C , which is a  ( m × 1 )  binary indicator vector used to represent causal status of  m  SNPs in a region (i.e.  c ( i )  is 1 if the  i th SNP is causal and 0 otherwise). We define a prior probability on the vector of Λ for a given causal status using an MVN distribution,
 (3) ( Λ | C ) ∼ N ( 0 , Σ c ) , 
where Σ c  is a diagonal ( m  ×  m ) matrix. The diagonal elements of Σ c  are set to  σ e 2  or  ϵ  where  ϵ  is a very small constant to make sure the matrix Σ c  is full rank. The  i th element on the diagonal is set to  σ e 2  if the  i th variant is causal and set to  ϵ  if the  i th variant is non-causal. We know that the LD between two variants is symmetric ( Σ T = Σ ). We combine  Equations (2)  and  (3)  to compute the joint marginal association statistics of all the variants. The joint distribution follows an MVN distribution,
 (4) ( S | C ) ∼ N ( 0 , Σ + Σ Σ c Σ ) . 
 2.5 Computing the posterior probability of causal SNP status from GWAS data Given the observed marginal association statistics,  S = [ S 1 , ⋯ S m ] T , we can compute the posterior probability of the causal SNP status  P ( C * | S )  as,
 (5) P ( C * | S ) = 1 Z P ( S | C * ) P ( C * ) = P ( S | C * ) P ( C * ) ∑ C ∈ C P ( S | C ) P ( C ) . 
where  C  is the set of all possible causal SNPs. Thus, the size of  C  is  2 m . Furthermore,  P ( C * )  is the prior probability for a particular causal SNP status,  C * . We use  Z  to indicate the normalization factor. In CAVIAR, we use a simple prior for a causal SNP status. We assume that the probability of an SNP to be causal is independent from other SNPs and the probability of an SNP to be causal is  γ . Thus, we compute the prior probability as  P ( C * ) = ∏ i = 1 m γ | c i | ( 1 − γ ) 1 − | c i | . In our work, we set  γ  to 0.01 ( Darnell  et al. , 2012 ;  Eskin, 2008 ;  Jul and Eskin, 2011 ). It is worth mentioning that although we use a simple prior for our model, CAVIAR can incorporate external information such as functional data or knowledge from previous studies. As a result, we can have SNP-specific prior where  γ i  indicates the prior probability for the  i th SNP to be causal. Thus, we can extend the prior probability to a more general case,  P ( C * | γ = [ γ 1 , γ 2 , ⋯ γ ℓ ] = ∏ i = 1 ℓ γ i | c i | ( 1 − γ i ) 1 − | c i | . To compute the posterior probability for each causal SNP status, we need to consider all the possible causal SNP status which is the denominator of  Equation (5) . To ease the computational burden, we assume we have at most six causal SNP in each region. Assuming we have an upper bound on the number of causal variants is a common procedure in fine-mapping methods ( Hormozdiari  et al. , 2014 ;  Kichaev  et al. , 2014 ). We show the upper bound of six causal variants have small effect on the results ( Hormozdiari  et al. , 2014 ). This assumption reduces the size of  C  from  2 m  to  m 6  which is computationally feasible. 2.6 ρ causal SNP set Give a set of SNPs  K , we define a causal SNP configuration as all the possible causal SNP status which excludes any SNP as causal outside the set  K . Note, our definition of causal SNP configuration includes the causal SNP status where no SNP is considered as causal. We use  C K  to denote the causal SNP configuration for the  K . We compute the posterior probability of set  K  capturing all the true causal genes,
 P ( C K | S ) = ∑ C ∈ C K P ( C | S ) . 
 Let ρ denote the value of the posterior probability, where  ρ = P ( C K | S ) , and we refer to it as the confidence level of  K  capturing the actual causal SNPs. We refer to  K  as the ‘ρ confidence set’. Given a confidence threshold  ρ * , there may exist many confidence sets that have a confidence level greater than the threshold. However, among all the possible  ρ *  confidence sets, the sets which have the minimum number of SNPs are more informative or have higher resolution to detect the actual causal SNPs. Thus, we are interested in finding the  ρ *  confident set with the minimum size (with minimum number of selected SNPs),  P ( C K * | S ) ≥ ρ * , where  K *  has the minimum size. 2.7 ρ causal gene set Unfortunately, the ρ causal SNP sets for mice can select many variants due to the high LD. Instead, we would like to find a set of genes that harbors causal variants. We define a ρ causal gene set as a set of genes which captures all the genes which harbor the causal variants with probability at least ρ. One of the benefits of detecting the ρ causal gene set requires less computation than detecting the ρ causal SNP set. For simplicity, we use genes as a way to group the SNP to detect the causal SNPs. Thus, SNPs are partition to sets and this partition of the SNPs is done based on the genes. As a result, when a gene is selected in the ρ causal gene set, we can consider all the SNPs which are assigned to that gene which are selected in the ρ causal SNP set in the CAVIAR model. We use a simple way to assign SNPs to a gene—we assign an SNP to the closest gene. We would like to emphasize that CAVAIR-Gene can incorporate more complicated SNP to gene assignment. Let  G  be a set of genes and  K ( G )  indicate all the SNPs assigned to the genes in the set  G . Then, we formally define the ρ causal gene set as a  G *  set where the total posterior probability of all the SNPs in  K ( G * )  that captures all the causal SNPs is ρ. Among all the ρ causal gene set, we are interested in the set which has the minimum number of genes selected.
 P ( C K ( G * ) | S ) ≥ ρ . 
Thus, to detect the ρ causal gene set, we need to search over all the possible sets of genes. Given  ℓ  genes in loci, we have  2 ℓ  possible causal gene set which is much smaller than all the possible sets of SNP, which are  2 m . 2.8 Greedy algorithm to detect the ρ causal gene set We would like to emphasize that ρ causal gene set should capture all the causal genes; however, not all the genes selected in the ρ causal gene set are causal. Thus, even if we set an upper bound of six on the number of causal genes, the size of the ρ causal gene set can be larger than six genes. For example, if we have one causal variant and all the variants in that region have perfect LD, just utilizing the marginal statistics is impossible to distinguish which gene is the actual causal gene. Thus, in order to have 95% causal gene set, we have to select all the genes in the region. This is similar to what we observe in the variant level from previous studies ( Hormozdiari  et al. , 2014 ;  Kichaev  et al. , 2014 ). Instead of considering all the possible causal gene set to find the ρ causal gene set, we propose the following greedy algorithm to ease the computational burden. For each gene, we define a weight that indicates the amount that each gene contributes toward the posterior probability of the ρ causal gene set. Genes which have higher weights will have higher probability of being selected in the ρ causal gene set. Thus, we pick the top set of genes for which the summation of their weights is at least ρ fraction of total weights of all genes in the region. We use  W = [ w 1 , w 2 , ⋯ w ℓ ]  as a  ( ℓ × 1 )  vector for the weights of all the genes, where  w i  is the weight of the  i th gene and we compute the weight for the  i th gene as follow:
 (6) w i = ∑ C ∈ C : c ( i ) = 1 P ( C | S ) = ∑ C ∈ C P ( S   |   C ) P ( C ) c ( i ) ∑ C ∈ C P ( S   |   C ) P ( C ) . 
 We compute the weight for the  i th gene by summing over all the causal gene statuses where the  i th gene is selected as causal. We show in Section 3 that the proposed greedy and the brute force algorithm which consider all possible causal gene status tend to have similar results. 2.9 Handling marginal statistics corrected for population structure The linear model which is used in the standard GWAS assumes only one causal SNP as shown in  Equation (1) . Moreover, in this linear model, we assume that the phenotypic value of each individual is independent from the phenotypic value of another individual. This assumption is not true in general for GWAS, especially in model organisms such as inbred mice. The model that accounts for this dependency is as follows:
 (7) Y = μ 1 + ∑ i = 1 m β i X i + e 
 Unfortunately, in a typical GWAS, the number of individuals in a study is much smaller than the number of SNPs ( n &lt; &lt; m ). Thus, estimating the effect size of all the SNPs is not possible. We test each SNP one at a time,  Y = μ 1 + β c X c + u + e , where  u = ∑ i ≠ c β i X i  models the random effects. In this model, we assume that each SNP has an effect and the effect of each SNP is distributed normally as  β i ∼ N ( 0 , σ g m ) . The total genetic variance is defined as  σ g 2  and we use  σ ^ g 2  as the estimated genetic variance. We compute the variance of the random effect as  V a r ( u ) = σ g 2 K , where  K = X X T / m  is referred to as the kinship matrix. The kinship matrix defines pair-wise genetic relatedness which is computed from the genotype data. Let  V  be the total variance of phenotype  Y , which is computed as  V = σ e 2 I + σ g 2 K . Let  σ ^ e  be the estimated environment and measurement error variance. Thus, the total estimated variance is  V ^ = σ ^ e 2 I + σ ^ g 2 K . We assume that the collected phenotype has an MVN distribution as follows:  Y ∼ N ( μ 1 + β c X c , σ e 2 I + σ g 2 K ) . Similar to linear regression, we compute the estimate of the effect size of the causal SNP  β ^ c  by maximizing the likelihood. Moreover, we can estimate the effect size of the SNP  β ^ i  which is indirectly associated to the causal SNP,
 β ^ c = X c T V ^ − 1 Y X c T V ^ − 1 X c , β ^ c ∼ N ( β c , ( X c T V ^ − 1 X c ) − 1 ) 
and the statistics is computed as follows:
 S c = β ^ c X c T V ^ − 1 X c ∼ N ( λ c , 1 ) 
 We would like to emphasize all the existing methods ( Kang  et al. , 2008 ;  Lippert  et al. , 2011 ;  Listgarten  et al. , 2012 ;  Zhou and Stephens, 2012 ) which correct for population structure computes the marginal statics for each variant. However, corrected marginal statistics cannot be used by existing fine-mapping methods ( Hormozdiari  et al. , 2014 ;  Kichaev  et al. , 2014 ). As in these methods, we assume that the correlation between the computed marginal statistics is equal to the correlation between the two corresponding variants. As shown in our experiment below, the correlation between the marginal statistics which are corrected for population structure is not equal to the correlation of genotypes corresponding to the two variants. We compute the covariance between the observed statistics for a causal SNP (variant) and an SNP (variant) which is indirectly associated with the causal SNP as follows:
 Cov ( S i , S c ) = Cov ( X i T V ^ − 1 Y X i T V ^ − 1 X i , X c T V ^ − 1 Y X c T V ^ − 1 X c )   = X i T V ^ − 1 X c X i T V ^ − 1 X i X c T V ^ − 1 X c 
Let matrix  L  be the Cholesky decomposition of matrix  V ^ − 1 ,   V ^ − 1 = L T L . Let  X c ′ = L X c  and  X i ′ = L X i . We assume that  L X c ,   L X i , and  L Y  are normalized to mean 0 and variance 1. Thus, we can re-write the covariance between the computed statistics for two SNPs as follow:
 Cov ( S i , S c ) = Cov ( X i ′ , X c ′ ) = Cov ( L X i , L X c ) 
 This indicates that the covariance between the two marginal statistics corrected for population structure follows an MVN where the correlation between the two statistics is the correlation between the transformed genotype for both SNPs. Thus, we re-write  Equation (2)  for the case the marginal statistics is corrected for population structure as follows:  ( S | Λ ) ∼ N ( Σ ′ Λ , Σ ′ ) , where  Σ ′  is the pair-wise correlation matrix which is computed by transforming the genotyped data and then computing the pair-wise correlation of transformed genotypes. In principle, this result could also be applied to other problems such as imputing the missing variants that utilize the summary statistics ( Lee  et al. , 2013 ;  Pasaniuc  et al. , 2014 ). 3 Results 3.1 CAVIAR-Gene is computationally efficient CAVIAR and CAVIAR-Gene at high level can consider all possible causal combinations for variants and genes, respectively. However, considering all possible causal combinations is intractable. In CAVIAR, we make an assumption that in each locus we have at most six causal variants. However, in CAVIAR, in order to detect the ρ causal variants, we consider all possible causal sets which can be very slow depending on the number of variants selected in the ρ causal variant set. In the worst case, the running time of CAVIAR can be  O ( 2 m ) , where  m  is the total number of variants in a region. In CAVIAR-Gene, we use the proposed greedy method which is mentioned in Section 2.8. This greedy algorithm reduces the complexity of CAVIAR from  O ( 2 m )  to  O ( m 6 ) . Applying CAVIAR on loci with 100 of variants will take around 30 h. However, it will take 2 h for CAVIAR-Gene to finish on the same loci and 3 h for CAVIAR-Gene to finish on loci with 200 variants.  Figure 1  indicates the running time compression between CAVIAR and CAVIAR-Gene for different number of variants in a region.
 Fig. 1. CAVIAR-Gene is computationally more efficient than CAVIAR. Running time comparison between CAVIAR and CAVIAR-Gene. The experiments are run on a 64 bit Intel(R) Xeon(R) 2 G with 5 GB RAM 
 3.2 CAVIAR-Gene-estimated causal gene sets are well-calibrated To assess the performance of our method, we conducted a series of simulations. To make our simulations more realistic, we utilize real genotypes from three different datasets: outbred dataset ( Zhang  et al. , 2012 ), F2 dataset ( van Nas  et al. , 2009 ), and HMDP dataset ( Bennett  et al. , 2010 ). After obtaining the real genotype for each dataset, we partition the genome into segments containing 200 genes. For each segment, we implant one, two, or three causal genes in the region where a gene is considered causal if it harbors at least one causal variant. We then generate simulated phenotypes for each segment using a linear mixed model as in the previous studies ( Han  et al. , 2009 ;  Zaitlen  et al. , 2010 ). We extend the existing methods, which are designed to detect the causal variants, to detect the causal genes. For these methods, we consider a gene to be causal if any of the variants in that gene are selected as causal. We run TopK-Gene, conditional method (CM-Gene) ( Yang  et al. , 2012 ), 1Post-Gene ( Maller  et al. , 2012 ), and CAVIAR-Gene. Among these methods, CAVIAR-Gene is the only method that is well-calibrated to detect causal genes as shown in  Table 1 . We consider a method to be well-calibrated if it accurately captures the causal genes in ρ fraction of the time. It is worth mentioning that 1Post-Gene is well-calibrated when we only have one true causal gene; however, 1Post-Gene is mis-calibrated when there are more than one causal gene in the locus as shown in  Table1 .
 Table 1. CAVIAR-Gene estimated causal gene-sets are well-calibrated Causal gene Recall rate (%) Causal gene size 1Post-Gene CM-Gene CAVIAR-Gene 1Post-Gene CM-Gene CAVIAR-Gene 1 0.995 0.941 0.990 2.59 1.16 2.10 2 0.790 0.526 0.964 3.93 2.28 3.17 3 0.760 0.610 0.951 3.23 3.28 6.65 a Note:  We implanted one, two, or three causal genes in a region. 1Post-Gene is well-calibrated to detect the causal genes in regions where we have only one true causal gene. CAVIAR-Gene is well-calibrated in all our experiments. We consider a method to be well-calibrated when the recall rate is at least 95%. We compute the recall rate of a method as a percentage of the total simulations where all the true causal variants are detected. a Although we allow for only six causal genes in a region, we can have more than six causal genes in the ρ causal gene set (see Section 2.8). 3.3 CAVIAR-Gene provides better ranking of the causal genes To compare the performance of each method, we compare the recall rate and the number of causal genes selected by each method. We calculate the recall rate as a percentage of the total simulations where all the true causal variants are detected. Unfortunately, each method selects a different number of genes as causal. Thus, to make the comparison fair, we compute the recall rate for each method as a function of the number of genes each method selects. The results for all the methods across all three datasets are shown in  Figure 2 . In this figure, the  X -axis is the number of genes selected by each method and the  Y -axis is the recall rate for each method.  Figure 2 c and e indicates the recall rate for Outbred, F2, and HMDP datasets where we have implanted one causal gene. Although the difference between the TopK-Gene and CAVIAR-Gene in the case of one causal gene is negligible, we observe a 10% higher recall rate when there are multiple causal genes in a region ( Fig. 2 b, d, and f).
 Fig. 2. CAVIAR-Gene provides better ranking of the causal genes for Outbred, F2, and HMDP datasets. Panels  a  and  b  illustrate the results for Outbred genotypes for case where we have one causal and two causal genes, respectively. Panels  c  and  d  illustrate the results for F2 genotypes for case where we have one causal and two causal genes, respectively. Panels  e  and  f  illustrate the results for Outbred genotypes for case where we have one causal and two causal genes, respectively 
 Although in  Figure 2  we only compare recall rate of different methods as we vary the number of causal genes selected by each method, these figures are similar to receiver operating characteristic (ROC) curves which are used as a measure to compare results for different methods in statistics and machine learning. In ROC curves, the  y -axis is the true positive rate which is equivalent to the recall rate in our result, and the  x -axis is the false positive rate which indicates the fraction of simulations where the non-causal genes are selected as causal. Because of the fact that all methods are forced to pick the same number of causal genes, the false positive rate is the same for all the methods. Moreover, similar to ROC curves in our results, as we increase the false positive rate, the recall rate increases and as we reach false positive rate of 1, which means if we select all the genes as causal, we have a recall rate of 1. 3.4 Greedy algorithm and brute force algorithm have similar results We proposed a greedy algorithm in Section 2.8 to detect the ρ causal gene set in order to speed up the process. In this section, we show that the results obtained from the greedy algorithm and the brute force algorithm are very close. The brute force algorithm considers all the possible  2 ℓ  different causal gene sets in order to compute the ρ causal gene set. We consider a region with 20 genes and then we simulated data similar to the previous sections. We implant one, two, or three causal genes in the region. We ran both methods and computed the recall rate as well as the size of the ρ causal gene set selected by each method.  Table 2  shows the results. We calculate the recall rate as a percentage of the total simulations where all the true causal variants are detected.
 Table 2. Greedy algorithm and brute force algorithm have similar results Causal gene Recall rate (%) Causal gene size Greedy Brute force Greedy Brute force 1 0.999 0.999 1.72 1.67 2 0.983 0.990 3.84 3.30 3 0.956 0.976 4.82 4.73 Note:  We implanted one, two, or three causal genes in a region. We run both the greedy and brute force algorithm on the simulated data sets. This result indicates that the differences between these two methods are negligible. 3.5 CAVIAR-Gene adjusts for population structure It is known that in the case where there exists no population structure, the correlation between the marginal statistics of two variants is the same as the correlation between the genotypes from which the statistics were computed. CAVIAR utilizes this fact to compute the likelihood for each possible causal combination. However, when population structure is present and corrected for, this may not hold. We demonstrate in our experiments that the correlation between the marginal statistics for any two variants which are corrected for population structure is the same as the correlation of a transformed version of genotype for the same two variants. We provide the description of this transformation in Section 2. CAVIAR-Gene utilizes this transformation to adjust for the population structure to compute the correct likelihood. We use an HMDP dataset ( Bennett  et al. , 2010 ) which we determine to have population structure. We generate phenotypes with population structure and compute the marginal statistics for each variant both corrected and not corrected for population structure. We then compute the correlation between each pair of marginal statistics and the correlation between each pair of variants for the original genotype and the transformed genotype. We calculate the difference between the correlation computed from the marginal statistics for each pair of variants and the correlation of the genotype of the same variants. The boxplot of these differences are shown in  Figure 3 .
 Fig. 3. CAVIAR-Gene adjusts for population structure. Panel  a  illustrates the case where the data have population structure and the statistics is not corrected for the population structure. Panels  b  and  c  illustrate the cases where we have corrected the statistics for the population structure. However, in Panel b, we compute the correlation between the original genotypes and in Panel c the correlation is computed from the transformed genotypes. Then, we calculate the difference between the correlation computed from the marginal statistics for each pair of variants and the correlation of the genotype of the same variants. The difference between the correlation of the marginal statistics and the correlation of the transformed genotype shown in Panel c is close to zero and their variance is much smaller than other cases as shown in Panels a and b. To compare the results, we plot the residual difference between −0.4 and 0.4, as a result some points for Panel b are not shown 
 As expected, the difference between the correlation of the marginal statistics and the correlation of the transformed genotype is close to zero and their variance is much smaller than other cases. Thus, the correlation between the marginal statistics when population structure is corrected is closer to the correlation between the genotype which is transformed using the right transformation matrix. 3.6 CAVIAR-Gene identifies  Apoa2  as causal gene in HDL To illustrate an application of our method in real data, we use an HDL dataset which was collected for three different mouse strains: outbred dataset ( Zhang  et al. , 2012 ), F2 dataset ( van Nas  et al. , 2009 ), and HMDP dataset ( Bennett  et al. , 2010 ). We ran CAVIAR-Gene on a region ∼80 megabases in length containing 595 genes (chr1: 120,000,000–197,195,432). This region harbors  Apoa2 , a gene previously established to influence HDL levels ( Flint and Eskin, 2012 ;  van Nas  et al. , 2009 ). We applied CAVIAR-Gene on the HMDP dataset considering all the genes in this region which yielded a 95% ρ causal set of 130 genes. Next, we conducted a more refined experiment, using domain-specific knowledge of the phenotype, to create a list of 53 potential candidate genes. CAVIAR-Gene selected a 23 gene subset of this list as the ρ causal gene set. Running CAVIAR-Gene on the Outbred dataset for all 595 genes resulted in a 95% gene set of only 13 genes. Because of the fact that the Outbred mice have a smaller degree of population structure than the HDMP, it is expected that the gene set resolution should be greater in this data. Most importantly, across all the datasets, CAVIAR-Gene includes  Apoa2  in the gene set.  Figure 4  illustrates the genes which are selected by CAVIAR-Gene for each datasets. The five genes which are common between all the datasets are  Nr1i3 ,  Tomm40l ,  Apoa2 ,  Fcer1g , and Ndufs2 . All these genes are known to be highly associated with the HDL. This suggests that CAVIAR-Gene not only recovers the actual causal gene, but simultaneously reduced the number of genes that need to undergo functional validation.
 Fig. 4. Venn diagram of the genes selected by CAVIAR-Gene on each of the dataset. HMPD ALL is the results of CAVIAR-Gene on HMDP when we utilize all the genes. HMDP CG is the result of CAVIAR-Gene on HMDP when we utilize candidate genes 
 4 Discussion In this article, we propose a novel method, CAVIAR-Gene, for performing fine mapping on the gene level. CAVIAR-Gene computes the probability of each set of genes capturing the true causal genes. Then, CAVIAR-Gene selects the set which has the minimum number of genes selected as causal and the probability of the set capturing the true causal gene is higher than a user-defined threshold (e.g. typically 95% or higher). We note that the usage of the term causal has little to do with the concept of causal inference as described in the computer science and statistics literature ( Pearl, 2000 ;  Spirtes  et al. , 2000 ). In the context of association studies, we consider a variant to be causal if the variant is responsible for the association signal in the locus. CAVIAR-Gene can incorporate marginal statistics which is corrected for population structure. This property makes CAVIAR-Gene suitable for performing fine mapping on the model organism such as inbred mice. We show using simulated data that CAVIAR-Gene has higher recall rate compared with the existing methods for fine mapping on the variants level, while the size of the causal set selected by CAVIAR-Gene is smaller than these methods. CAVIAR-Gene incorporates external information such as functional data as a prior to improve the results. Funding This work was supported by the  National Science Foundation  ( 0513612 ,  0731455 ,  0729049 ,  0916676 ,  1065276 , 1302448 , and  1320589  to F.H., W.Y., and E.E.) and the  National Institutes of Health  ( K25-HL080079 ,  U01- DA024417 ,  P01-HL30568 ,  P01-HL28481 ,  R01-GM083198 ,  R01-MH101782 , and  R01-ES022282  to F.H., W.Y., and E.E.). E.E. is supported in part by the NIH BD2K award, U54EB020403. We acknowledge the support of the  National Institute of Neurological Disorders and Stroke Informatics Center for Neurogenetics and Neurogenomics  ( P30 NS062691  and  T32 NS048004-09 ). G.K. and B.P. are supported in part by the  National Institutes of Health  ( R01 GM053275 ). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. Conflict of Interest : none declared. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Inferring parental genomic ancestries using pooled semi-Markov processes</Title>
    <Doi>10.1093/bioinformatics/btv239</Doi>
    <Authors>Zou James Y., Halperin Eran, Burchard Esteban, Sankararaman Sriram</Authors>
    <Abstract>Motivation: A basic problem of broad public and scientific interest is to use the DNA of an individual to infer the genomic ancestries of the parents. In particular, we are often interested in the fraction of each parent’s genome that comes from specific ancestries (e.g. European, African, Native American, etc). This has many applications ranging from understanding the inheritance of ancestry-related risks and traits to quantifying human assortative mating patterns.</Abstract>
    <Body>1 Introduction Recent developments in DNA technology bring personal genomics to reality. This opens up unprecedented possibilities for individuals to learn about their genomic history (e.g. ancestry, family history) as well as their genomic future (e.g. disease risk). A particular aspect of personal genomics that has garnered significant public and medical interest is the ability to precisely quantify the ancestry composition of one’s genome ( Royal and Kittles, 2004 ;  Royal  et al. , 2010 ). Consider a Mexican individual as an example. Her genome consists of alternating blocks of DNA sequences, where each block has African, European or Native American ancestry. The length and frequency distributions of blocks from different ancestries reflect the patterns of admixtures over the last several centuries. A substantial fraction of humans today are offsprings of historical mixing between distinct populations and their genomes are such mosaics of ancestry blocks ( Hellenthal  et al. , 2014 ). The ability to quantify genomic ancestries has important bio-medical implications. For example, African ancestry is a risk factor for asthma. This partially explains the high prevalence of asthma in African American as well as Puerto Ricans with larger African genomic ancestry ( Vergara  et al. , 2013 ). In addition, genomic ancestry gives insights into many social science questions, and expands the common notions of ethnicity and race ( Bryc  et al. , 2015 ;  Hochschild and Sen, 2015 ). Given the genome of an individual, recent machine learning methods can accurately determine the fraction of this person’s genome that originates from each ancestry ( Alexander  et al. , 2009 ;  Pritchard  et al. , 2000 ). However, for many applications in biomedical and social science research, it is important to go beyond the individual’s ancestry and to infer the genomic ancestries of the parents (since most genetic datasets do not have genotype information from the parents). In studies of ancestry linked risk factors, genomic ancestry information of parents can be used to investigate how risks propagate through generations. In social science applications, parental genomic ancestry can be used to understand genetic basis of human mate selection, a subject of substantial recent interest. Latino parents, e.g. were shown to have significant correlations in their genomic ancestries ( Risch  et al. , 2009 ). However, current methods cannot be used to infer the genomic ancestry of each of the two parents of an individual given only the DNA of the individual. Inferring parental genomic ancestry is challenging since the observed DNA are unordered pools of the DNA from the two parents. We show that this problem of parental genomic ancestry inference can be well modeled as a pooled semi-Markov process. To the best of our knowledge, this is the first method that can accurately infer the parameters of the parental ancestries in admixed populations. We applied the efficient algorithms we have developed for pooled semi-Markov process to infer parental ancestry. On experimental data from 231 Mexican families and 258 Puerto Rican families for whom we know the true genomic ancestry of each parent, we show that our method provides accurate estimates of parental genomic ancestry. Our method applies to any common genotyping data from an individual; importantly, no family or phasing information is needed, and hence it is broadly applicable to existing genetic data. Although in this article, we focus on the application of inferring genomic ancestry, we believe that many other settings can also be modeled as pooled semi-Markov process. For example in a tumor sample, there are many clonal subpopulations of cells, each with its own copy number aberrations which can be modeled as a semi-Markov process ( Wu  et al. , 2014 ). When we sequence the tumor in clinics, we typically obtain a pooled collection of reads from the various subpopulations. 1.1 Contributions Our main contributions are:
 We set up the mathematical framework of pooled semi-Markov processes and construct efficient, scalable inference algorithms. Using this framework of pooled semi-Markov processes, we develop a method to infer the parameters of the parental ancestries in admixed populations. This is important because it allows for a better understanding of how certain disease risks are associated with ancestries. We demonstrate the accuracy of our method on a real genotype dataset of 489 families where we can measure the true genomic ancestry of each parent. We further validate it using simulations. 
 1.2 Related work Semi-Markov models have been well studied in literature and have many applications ranging from economics to biology ( Ross, 1999 ). A related class of models for sequential data is factorial HMMs (FHMMs) ( Ghahramani and Jordan, 1997 ). FHMMs model outputs that are function of several hidden states where each hidden state evolves according to an independent Markov model. Because exact inference in FHMMs is intractable, a number of approximate inference procedures have been developed. The pooled semi-Markov process differs from FHMM in significant ways. First, the holding time in each HMM state is geometrically distributed, while we allow for arbitrary distributions. Second, the pooling model introduces hard combinatorial constraints that make standard variational inference inapplicable. There is a large body of work on the inference of local ancestry in admixed populations, in which the ancestry of each position in the genome is inferred. These methods typically use hidden Markov chain models, (e.g.  Price  et al. , 2009 ;  Pritchard  et al. , 2000 ;  Sankararaman  et al. , 2008a ;  Tang  et al. , 2006 ) variants such as switch HMMs ( Sankararaman  et al. , 2008b ) and factorial HMMs ( Baran  et al. , 2012 ). Principal component analysis (PCA) has been shown to correlate well with global ancestry, and variants of PCA have been proposed ( Yang  et al. , 2012 ). In the case of African-Americans, these models have been applied to show that African-Americans today are an admixture of African and European ancestries in the ratio 0.8:0.2 over the last 6–10 generations ( Smith  et al. , 2004 ). Further, it has been shown that local ancestry can be accurately inferred in African-Americans. A limitation of these approaches is that they do not distinguish between the maternal and the paternal contributions to the genetic ancestry. Methods, such as Hapmix ( Price  et al. , 2009 ), estimate the unordered pair of local ancestry states at each position but do not assign the local ancestry to each parental haplotype and hence do not tell us the genomic ancestries of each parent. 2 Methods 2.1 Pooled semi-Markov processes A semi-Markov process generalizes continuous time Markov process to settings where the holding time in a state may not be exponentially distributed. We recall the generative procedure for sampling from a semi-Markov process of  K  states.
 Definition Let  f  denote the probability density function of a random variable parametrized by  λ  ( λ  could represent either a scalar or a vector depending on the form of the density function). To sample from a  K -state semi-Markov process parametrized by  { λ k , α k } k = 1 K ,   ∑ α k = 1 , we do the following:
 i ← 1 . Sample the state of the first block,  ϕ 1 ∼ Discrete ( { α k } ) . Sample the length of the first block,  L 1 ∼ f ( λ ϕ 1 ) . Repeat while  ∑ j = 1 i L i &lt; L , where  L  is a specified length:
 i ← i + 1 Sample the state of block  i ,  ϕ i ∼ Discrete ( { α ^ k } ϕ i − 1 ) , where  α ^ k = α k 1 − α ϕ i − 1  if  k ≠ ϕ i − 1  and  α ^ ϕ i − 1 = 0 . Sample the length of block  i ,  L i ∼ f ( λ ϕ i ) . 
 
We call  f ( λ k )  the  holding distribution  of state  k  and a  jump  is a transition between two consecutive blocks. For our applications it is sufficient to work with this parametrization of the transitions using  α k ’s. All results can be extended to general semi-Markov process. The output sample is a chain of length  L  composed of blocks of distinct states. For the last block, we cut it off so that it stops at  L . For  x ∈ [ 0 , L ] , we denote by  ϕ ( x )  the state of the block that  x  belongs to, i.e.  ϕ ( x ) = ϕ i  if  x  is in block  i . 
 If  f  is the exponential distribution, then the corresponding semi-Markov process is equivalent to a continuous time Markov chain. For state  k , the holding time is the time spent in that state and is an exponentially distributed random variable with rate  λ k . If we observe the states  ϕ ( x )  for an individual semi-Markov process and  L  is sufficiently long, then it is straightforward to perform maximum likelihood inference of the parameters  { λ k , α k } . In genetics and other applications however, we do not observe each individual process but a  pool  of multiple semi-Markov processes where the identity of which process a given state is from is lost.
 Definition Suppose we have  M  independent semi-Markov processes, each of length  L . The  j -th process is parametrized by  { λ k j , α k j } k = 1 K  and the state of the  j -th process in position  x  is denoted by  ϕ j ( x ) . The  pooled semi-Markov process  (abbreviated as PSMP) is obtained by the assignment of each  x ∈ [ 0 , L ]  to the  K -dimensional vector of counts,  Φ ( x ) , such that the  k -th entry is number of elements in  { ϕ j ( x ) } j = 1 M  that equals to  k . We call  Φ ( x ) ,   x ∈ [ 0 , L ] , the observations of the pooled semi-Markov process. The model is parametrized by  Θ ≡ { λ k j , α k j } k = 1 , ... K j = 1 , ... , M . 
 We focus on continuous holding distributions  f  such that with probability 1 each process has a finite number of jumps in  [ 0 , L ] . Let  N  denote the sum of the number of jumps across all  M  processes. Then the continuous observations of the pooled semi-Markov process can be concisely described by the finite set  X = { Φ i , L i } i = 1 N , where  Φ i  is the counts vector observed at the  i -th block across the  M  processes, and  L i  is the length of this block. Note that for continuous distributions, the probability that two blocks of two different semi-Markov chains end at the same point  x  is zero, thus with probability one  Φ i  and  Φ i + 1  differ by one transition. When it is clear from context, we also use the equivalent representation  Φ ( x ) = { ϕ j ( x ) } j = 1 M . Under a pooled semi-Markov process, the likelihood of the parameters Θ is 
 P ( X | Θ ) = ∫ 1 [ ∑ j = 1 M e ( ϕ j ( x ) ) = Φ ( x ) , ∀ x ∈ [ 0 , L ] ] ∏ j = 1 M d P ( ϕ j | { λ k j , α k j } k = 1 , … , K ) 
where  e ( ϕ j ( x ) )  is the unit vector with 1 in the  ϕ j ( x ) th  entry and  d P ( ϕ j )  is the measure induced by the  j th semi-Markov process. In the above integral, the set of hard constraints  1 [ ∑ j = 1 M e ( ϕ j ( x ) ) = Φ ( x ) , ∀ x ∈ [ 0 , L ] ]  are finite (there being a single constraint for each of the  N  blocks). Nevertheless, these constraints make it intractable to exactly compute the likelihood in general. We develop efficient approximations below.
 Example An application of the pooled semi-Markov process naturally arises in the field of population genetics. The diploid genome of an individual consists of one transmitted genome from each parent (the two segments in  Fig. 1 ). The transmitted genome of each parent is a sequence of intervals, where each interval has a different genomic ancestry–(E)uropean, (A)frican, (N)ative American, etc. For example, if the mother is African American, then the genome that she passes on to the offspring is a mosaic of blocks of state A of some length distribution and state E of a possibly a different length distribution, and similarly for the father. Hence the genome passed from the mother to the offspring is well-modeled by a semi-Markov process ( Donnelly, 1983 ;  Gravel, 2012 ). When we genotype the offspring, say the one in  Figure 1 , we can infer that first region has ancestry AA and the second region has ancestry EA (using method described in the next section); however, we do not know whether the E part come from parent 1 or parent 2, and thus the information about the parents’ ancestry is lost. Given the pooled observations of the ancestries (e.g. AA, AE, EE) at every point in the genome, the goal is to infer the parameters  { λ k j , α k j } k = 1 K  for both parents. The  λ ’s parametrizes the length distribution of each ancestry state in a parent and the  α ’s capture the frequency of the ancestry states. With estimates for these parameters, we can infer the global ancestry of each parent, i.e. the fraction of the individual’s genome that is European, Native American or African, as well as the number of generations since the admixture. 
 Fig. 1. Illustration of pooled semi-Markov process. A and E are the states and have different length distributions in the two semi-Markov processes 
 2.2 Algorithms for inference We first consider the parameter estimation problem for pooled semi-Markov processes with exponential holding distributions (which are equivalent to continuous time Markov processes). Exponential holding distribution captures many of the genetic datasets of interests and can often be used as a reasonable approximation to other more complex distributions. We treat this problem in a Bayesian setting in which we place a prior on the parameter vector  Θ ≡ { λ k j , α k j } k = 1 , ... K j = 1 , ... , M  and for a given observation  X ≡ { Φ i , L i } i = 1 N , we compute the posterior probability  Pr ⁡ ( Θ | X ) . We approximate the posterior using a Markov chain Monte Carlo (MCMC). To compute the likelihood as part of the MCMC, we developed a dynamic programming algorithm. In the next section, we show that this method gives accurate posterior estimates of the parameters of the model on genotype data from Mexican and Puerto Rican families. For the non-exponential case, the dynamic programming algorithm needs to keep track of all the possible lengths for which a block of ancestry extends. This becomes expensive when the number of blocks,  N , is large. Hence, we develop a more general stochastic- Expectation-Maximization (EM) algorithm to perform maximum likelihood inference on general pooled semi-Markov chains, as described in the next subsection. Note that there is an inherent symmetry in terms of which process we label as 1, 2, etc. that is not identifiable. Here we assume that the processes are labeled according to some arbitrary but fixed order, and the goal is to recover the parameters up to permutation of labels. 2.3 MCMC inference algorithm for pooled Markov process For given observations  X ≡ { Φ i , L i } i = 1 N  and parameters  Θ ≡ { λ k j , α k j } k = 1 , ... , K j = 1 , ... , M , we use dynamic programming to compute the exact likelihood of the parameters,  P ( X | Θ ) . For each  i = 1 , ... , N , we keep track of all the distinct  ordered  states that are permutations of the observation  Φ i = { ϕ i j } j = 1 M . For example, if  Φ i = { A , E } , then the consistent ordered states are ( A ,  E ), where state  A  is generated by process 1, and ( E ,  A ), where state  A  is generated by process 2. Note that we denote ordered states by  ( )  and unordered states by  { } . For a given unordered set  Φ i , we denote by  { π ( Φ i ) | π ∈ S i }  all the distinct ordered tuples that are consistent with  Φ i . Here  S i  is the permutations of  [ 1 , ... , M ]  that give rise to distinct tuples. For each  π ( Φ i ) , let  P i ( π ( Φ i ) | Θ )  be the probability of observing  { Φ 1 , ... , Φ i − 1 , π ( Φ i ) }  in a pooled Markov chain parametrized by Θ. In other words, this is the probability of seeing the unordered observation up to  i  − 1 and then observing the ordered tuple  π ( Φ i ) . There are in the worst case  K M  such  P i ( π ( Φ i ) | Θ )  for each  i .
 For  i  = 1, if the ordered state is  π ( Φ 1 ) = ( ϕ 1 1 , ... , ϕ 1 M ) , then  P 1 ( π ( Φ 1 ) | Θ ) = ∏ j = 1 M α ϕ 1 j j λ ϕ 1 j j exp ⁡ ( − λ ϕ 1 j j L 1 ) . Given all the probabilities  P i − 1 ( π ( Φ i − 1 ) | Θ ) , we have  P i ( π ( Φ i ) | Θ ) ) = ∑ π ^ ∈ S i − 1 ( π ) P i − 1 ( π ^ ( Φ i − 1 ) | Θ ) P ( π ^ ( Φ i − 1 ) → π ( Φ i ) | Θ ) 
 where  S i − 1 ( π )  are all the tuples that are one edit distance from  π  (since we know exactly one jump in one process has occurred between block  i  − 1 and  i  almost surely) and consistent with  Φ i − 1 , and  p ( π ^ ( Φ i − 1 ) → π ( Φ i ) | Θ )  is the probability for transitioning from  π ^ ( Φ i − 1 )  to  π ( Φ i ) , which can be computed analytically as a product of exponentials and  α k j ’s. For example, suppose the states are  E  and  A , and the observation at  i  is { E ,  E }. In this case there is just one tuple, ( E ,  E ), consistent with it, and
 P i ( ( E , E ) | Θ ) = P i − 1 ( ( E , A ) | Θ ) e − λ E 1 L i λ E 2 e − λ E 2 L i + P i − 1 ( ( A , E ) | Θ ) λ E 1 e − λ E 1 L i e − λ E 2 L i . 
In the first term of the right hand side,  e − λ E 1 L i  is the contribution from continuing  E  with  E  in the first chain, and  λ E 2 e − λ E 2 L i  comes from continuing  A  with  E  in the second chain. And similarly for the second term of the right-hand side. Because there are two states in each chain,  α ’s do not appear. Given this method for computing the likelihood of any observed data  X  for parameters  Θ = { λ k j , α k j } k = 1 K , we use adaptive MCMC to compute the posterior distribution over Θ. The advantage of this approach is that we obtain full posterior distributions of Θ, and for several human populations, it gives accurate estimations (next section). A drawback is that computing the exact likelihood is expensive when the state space is large or when there are many chains—the run time is  O ( K M ) . 2.4 Stochastic EM inference algorithm for general pooled semi-Markov process For general semi-Markov processes with non-exponential holding times, the dynamic programing would have to keep track of the last ordered state as well as its length, making it prohibitively expensive to compute the likelihood. We therefore propose a stochastic EM algorithm to perform parameter inference in general pooled semi-Markov processes. For each block  i , the observation is the unordered set of states  { ϕ j i } j = 1 M . Let  Z  be an  M -by- N  matrix where  Z [ j , i ] ∈ [ 1 , ... , M ]  denote the process that generated the state  ϕ j i  at block  i .  Z  is the matrix of the latent variables. 2.4.1 E-step Given the current values of the parameters Θ and observations  X , it is in general intractable to compute the posterior  p ( Z | X , Θ ) . However we can generate samples  { Z s }  from  p ( Z | X , Θ )  using an efficient sequential procedure using the expansion
 P ( Z | X , Θ ) = P ( Z [ : , 1 ] | X , Θ ) P ( Z [ : , 2 ] | Z [ : , 1 ] , X , Θ ) ... P ( Z [ : , N ] | Z [ : , N − 1 ] , X , Θ ) . 
For the base case, let  Φ 1 = { ϕ 1 1 , ... , ϕ 1 M } , then
 p ( Z [ : , 1 ] | X , Θ ) = p ( Z [ : , 1 ] | Φ 1 , Θ ) ∝ ∏ j = 1 M α ϕ 1 j Z [ j , 1 ] 
subject to the contraint that  Z [ : , 1 ]  is a permutation of  [ 1 , ... , M ] . This can be sampled efficiently using rejection sampling. The vector  Z [ : , i ]  is one edit distance from  Z [ : , i − 1 ] , so that given  Z [ : , i − 1 ]  there are at most  KM -feasible values for  Z [ : , i ] . If vector  W  differs from  Z [ : , i − 1 ]  in index  j , then  p ( W | Z [ : , i − 1 ] , X , Θ )  can be computed as a function of the length of the current state for the  j -th semi-Markov process and the  α k j ’s. Therefore we can explicitly compute the conditional probability of  Z [ : , i ]  given  Z [ : , i − 1 ]  for all values of  Z [ : , i ] . To sample  Z [ : , i ]  we just sample from these conditional probabilities. 2.4.2 M-step Given samples  Z s  of the latent variables, we compute the maximum likelihood Θ by maximizing  ∏ s p ( X , Z s | Θ ) . Given  Z , the parameters  λ k j  and  α k j  are independent for different  j ∈ [ 1 , ... , M ] . For that  r -th semi-Markov process, the optimization problem is  arg max ∏ s p ( { ϕ i j s . t . Z s [ j , i ] = r } , { L i } | λ r , α r ) . For standard distributions, this optimization can be solved analytically. We iterate the E and M steps until convergence. 3 Results 3.1 Mexican and Puerto Rican trios We used 231 Mexican mother-father-child trios and 258 Puerto Rican trios from the Genetics of Asthma in Latino Americans (GALA) study ( Risch  et al. , 2009 ) For each trio, we have the genotypes of the two parents and the offspring across the entire genome. The trios were genotyped using the Affymetrix 6.0 GeneChip Array, which provides measurements of the genome at over 900 000 positions, called single nucleotide polymorphisms. Subjects were filtered based on call rates &gt;95%, consistency between reported and genetic sex, and the absence of any unexpected identity by descent (IBD) or by state. Familial relationships were confirmed using measures of IBD and Mendelian inconsistencies. We used LAMP-LD, a commonly used method, to infer the local ancestry state at each position in the genome in each individual ( Baran  et al. , 2012 ). LAMP-LD uses a generative model in which the genome is divided into non-overlapping windows. An admixed genome is generated as an emission within each window from a HMM with  ( K 2 )  states, where  K  is the number of ancestral populations. Transitions between the hidden states occur between adjacent windows. LAMP-LD computes a Viterbi decoding of the pairs of local ancestries along the genome. Since Puerto-Ricans and Mexicans have mixed ancestry with (E)uropean, (A)frican, and (N)ative American ancestries, LAMP-LD assigns to each position in the genome one of 6 states: EE, NN, AA, EA, EN and NA, depending on the ancestry of that position (e.g. NA corresponds to the case where one copy originated in Africa and the other in America). In these datasets, we observed that the genomes of each of the parents are well approximated by exponential length distribution and hence by a Markov process. The genome of the child can then be modeled as a pooled Markov process, with  M  = 2 and  K  = 3. Note that in general, the genomes of the parents themselves cannot be modeled as a Markov process but as a semi-Markov process ( Gravel, 2012 ). However in these data the exponential distribution proved to be a good approximation, likely because admixture occurred many generations ago in these samples and have been continuing ever since. For the validation experiment, we take as input the observed local ancestry blocks of each offspring, and use the MCMC algorithm described earlier, with uniform priors, to infer the posterior distribution over the parameters of the model. In these data, the MCMC estimates are more accurate than estimates from the stochastic EM (not shown). There are six  λ  parameters and four independent  α  parameters. The global European (or African, Native American) genomic ancestry of an individual is defined to be the proportion of the total genome that is identified to be of European (or African, Native American) descent. For each set of parameters, we infer the global ancestry proportion of the corresponding parent by running a Markov chain with these parameters to equilibrium and computing the fractions. Then we compare the inferred global genomic ancestry of each parent with the true genomic ancestry of the parent computed explicitly by running LAMP-LD. Genomes of Mexican samples contain primarily European (average of 43%) and Native American (49%) ancestries, and a small amount of African ancestry. In contrast, Puerto Ricans genomes contain mostly European (62%) and African (23%) ancestries, with a minor component of Native American. Moreover the two populations have distinct demographic histories leading to different statistical properties of their ancestries, corresponding to different distributions of  λ ′ s  and  α ′ s  ( Bryc  et al. , 2010 ). Hence these two datasets are complementary in exploring the performance of our approach under different conditions. Table 1  contains the  r 2  between our estimated genomic ancestries using PSMP and the true genomic ancestries in the 462 Mexican (MX) parents and 516 Puerto Rican (PR) parents. We report the  r 2  for each of the ancestry states: European (E), Native American (N), African (A). In Mexican trios, our estimated proportions of European and Native American ancestries agree very well with the ground truth (coefficient of determination  r 2  = 0.84 for both). It performs worse in estimating the African proportion, likely because African blocks are only observed a few times in most samples. In Puerto Rican trios, our estimates for the European and African ancestries closely match the ground truth. It performs worse for the less frequent Native American ancestry ( Figs. 2  and  3 ).
 Fig. 2. Comparisons of the estimated genomic ancestry of each parent with the ground truth. The top row is for Mexican samples, each dot corresponding to one parent: European proportions (left), Native American proportions (middle) and African proportions (right). The bottom row is for Puerto Rican samples: European proportions (left), Native American proportions (middle) and African proportions (right) 
 Fig. 3. Comparisons of the estimated and true average block length of each ancestry type. The top row is for Mexican samples, each dot is one parent: average European block length (left), average Native American block length (middle) and average African block length (right). The bottom row is the average block length in Puerto Ricans for European (left), Native American (middle) and African (right) ancestries 
 Table 1. Ancestry estimation accuracy  r 2 MX E MX N MX A PR E PR N PR A PSMP 0.84 0.84 0.35 0.72 0.5 0.75 Offspring 0.76 0.75 0.33 0.63 0.43 0.66 Independent 0.83 0.82 0.27 0.58 0.18 0.41 The first three columns correspond to the Euroean (E), Native American (N) and African (A) ancestries of the Mexican individuals. The last three columns correspond to the European, Native American and African ancestries of the Puerto Rican individuals. In addition to accurately estimating the global genomic ancestries of each parent, our method also infers finer grained information. In particular, since the holding distributions are exponential,  1 / λ  gives the average block length of each ancestry type in a parent. From standard coalescent models of population genetics, these length scales inform us the number of generations since the interbreeding of these populations in the family history of that individual. We compare the inferred length scales for each parent and ancestry type with the ground truth measured on the transmitted allele. In Mexicans, there’s strong correlation between length estimates from our method and the ground truth for European and Native American ancestries ( r 2  of 0.73 and 0.75, respectively). The estimate is less accurate for the less frequent African block lengths ( r 2  = 0.25). For Puerto Ricans, we find the strongest agreement in the block lengths of Africans ( r 2  = 0.75), followed by Europeans ( r 2  = 0.54) and Native Americans ( r 2  = 0.45). 3.1.1 Scalability Our algorithms treat the samples independently and can be run in parallel on all the samples. For each human sample, it required ≤5 min on a standard desktop. 3.2 Comparison to benchmarks In practice, it is often assumed that the genomic ancestry of the offspring is a good approximation of the ancestry of the parents. This only works if the genomic ancestries of the two parents are very similar, since the offspring’s ancestry essentially averages the parents’. This assumption is especially problematic in admixed populations (Latinos, African Americans, etc.) where the two parents may have very different ancestries. We tested this assumption in our trios, where we use the empirically measured genomic ancestry of the offspring as estimations of the parents’ ancestries. The correlation with the true genomic ancestries is reported in the second row of  Table 1 , and it is significantly worse than the results of the pooled semi-Markov process. For more heterogeneous populations, we expect the offspring to be even worse estimators of the parent’s ancestry. The pooled semi-Markov process explicitly models the spatial correlation of nearby states. A simpler algorithm is to assume that all the observations are independent. The accuracy of this simpler model is reported in the third row of  Table 1 . It performs worse than the pooled semi-Markov process in all the categories and is especially poor in Puerto Ricans. 3.3 Simulations 3.3.1 Random trios As an additional validation, we tested our algorithm on simulated Mexican male–female–offspring trios. In the actual trios, using the genotype of the three individuals, we inferred the transmitted allele from each parent to the offspring. To generate a random trio, we then randomly selected a male and a female parent and computationally combined their transmitted alleles to form a new offspring. This creates realistic offspring genotypes while preserving the complex demography encoded in the parents’ transmitted alleles. Using this process, we simulated 100 new trios for which we knew the true genomic ancestry of each individual. As before, we applied our method to the offspring data to infer the ancestries of the parents. Comparison of the inferred ancestries with the ground truth showed very good agreement ( Fig. 4 ). For the European, Native American and African ancestries, we achieved  r 2  of 0.9, 0.89 and 0.77, respectively.
 Fig. 4. On simulated Mexican trios, comparisons of the estimated genomic ancestry of each parent with the ground truth. Each dot corresponds to one parent. The  x -value shows the actual ancestry of the parent and the  y -value shows the inferred ancestry. European, Native American and African ancestries are shown in the left, middle and right panels, respectively 
 3.3.2 Non-exponential chains We also investigated how well we can do inference on pooled semi-Markov processes where the distributions are very different from exponential, as these could be relevant for other demographic models and applications. We consider the particular case where the block lengths of each state are Gaussian distributed. We use the more general stochastic EM algorithm given above to perform inference. In the experiments, we varied  K = 2 , ... , 6  and  M = 2 , ... , 5 . For each combination of  K  and  M , we simulate 50 pools of semi-Markov processes. We consider unit variant Gaussians with mean  λ k . For each process, we sampled  α  uniformly from the  K -dim simplex and sample  λ k  uniformly from [5, 10]. Different processes in the same pool have different  α ’s and  λ ’s. Each observed dataset is created by pooling  M  different Gaussian semi-Markov processes. To better match the quantity and noise of realistic genomic data, we use only the first  N  = 500 blocks of the pooled semi-Markov process as observations. This is the input into our stochastic EM algorithm. To evaluate the estimation, we compute the  r 2  between the estimated  λ ’s and the true  λ ’s and between the estimated and true  α ’s, across all pools and all processes. The results are summarized in  Figure 4 . For  M  = 2, 3, we obtain accurate estimations for even large numbers of states, with  r 2  &gt; 0.8. The accuracy of inference declines as the number of processes in a pool increases. In these more complex models, we can improve our accuracy by collecting larger number of observations ( N ) from each pool ( Fig. 5 ).
 Fig. 5. Experimental validation of inference for  λ  (left) and  α  (right) for pooled Gaussian semi-Markov processes. The X axis corresponds to the number of states  K  and different line colors correspond to different number of processes  M . For the  α  estimations,  K  = 2 is trivial since all the  α ’s are 0.5, and is omitted 
 4 Discussion We developed an efficient method to infer the genomic ancestry of the parents from the genotype of an offspring. We applied our method to genotype data of 231 Mexican and 258 Puerto Rican individuals to infer the parents’ ancestries. We showed that the method is highly accurate by comparing the inferred ancestries with each parent’s true genomic ancestries. We further validated the method on simulated trios. For pooled Markov processes, we showed how to compute likelihood exactly using dynamic programming. For general pooled semi-Markov processes, we developed a stochastic EM algorithm to infer the model parameters. We additionally validated accuracy of our inference algorithm in settings where the semi-Markov length distributions are Gaussians. We tested our algorithm on Latino trios, but it can be applied to other admixed populations and can be used to infer ancestries other than European, Native American and African. The method can be used on general genotype datasets of unrelated, unphased individuals, for which large cohorts exist, to infer the genomic ancestries of the parents. This has immediate applications in investigating assortative mating in human populations. The current approach assumes that the local ancestry of the offspring has been computed from his/her genotype. This is reasonable for large admixed populations such as Latinos and African Americans, where existing algorithms (e.g. LAMP-LD) can accurate infer the local ancestries. For other admixed populations, an interesting direction of future work is to jointly infer the local ancestry of the offspring and the global ancestries of the parents Funding EH is a faculty fellow of the Edmond J. Safra Center for Bioinformatics at Tel Aviv University. EH was partially supported by the  Israeli Science Foundation  (grant  1425/13 ), by the  National Science Foundation  grant  III-1217615 , and by the  United States-Israel Binational Science Foundation  (grant  2012304 ). Conflict of Interest : none declared. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>EthSEQ: ethnicity annotation from whole exome sequencing data</Title>
    <Doi>10.1093/bioinformatics/btx165</Doi>
    <Authors>Romanel Alessandro, Zhang Tuo, Elemento Olivier, Demichelis Francesca, Stegle Oliver</Authors>
    <Abstract/>
    <Body>1 Introduction Interrogation of the entire coding genome for germline and somatic variations through Whole Exome Sequencing (WES) is rapidly becoming a preferred approach for the exploration of large cohorts (such as The Cancer Genome Atlas initiative) especially in the context of precision medicine programs ( Beltran  et al. , 2015 ). In this setting, the estimation of individual’s ethnical background is fundamental for the correct interpretation of variant association studies and of personal genomic variations importance ( Petrovski and Goldstein, 2016 ;  Price  et al. , 2006 ;  Spratt  et al. , 2016 ;  Zhang  et al. , 2016 ). To enable effective annotation of individual’s ethnicity and improve downstream analysis and interpretation of germline and somatic variations, we developed EthSEQ, a tool that implements a rapid and reliable pipeline for ethnicity annotation from WES data. The tool can be used to annotate ethnicity of individuals with germline WES data available and can be integrated in any WES-based processing pipeline. EthSEQ also exploits multi-core technologies when available. 2 Approach EthSEQ provides an automated pipeline, implemented as R package, to annotate the ethnicity of individuals from WES data inspecting differential SNPs genotype profiles while exploiting variants covered by the specific assay. As input the tool requires genotype data at SNPs positions for a set of individuals with known ethnicity (the  reference model ) and either a list of BAM files or genotype data of individuals with unknown ethnicity. EthSEQ then annotates the ethnicity of each individual using an automated procedure ( Supplementary Fig. S1a ) and returns detailed information about individual’s inferred ethnicity, including aggregated visual reports. The  reference model  builds on genotype data of individuals with known ethnicity; 1000 Genome Project individuals data is here used to construct platform-specific reference models relying on the most conserved ethnic groups EUR (Caucasian), AFR (African), EAS (East Asian) and SAS (South Asian) for multiple WES designs: Agilent HaloPlex, Agilent SureSelect and Roche Nimblegen ( Supplementary Methods ). More generally, given a set of genomic regions and genotype data of a set of individuals annotated for ethnicity, a procedure to automatically generate a reference model is also provided by EthSEQ. The  target model  is created either from the input list of individual’s germline BAM files that are genotyped at all reference model’s positions using the genotyping module of ASEQ ( Romanel  et al. , 2015 ) (depth of coverage ≥ 10X and read/base mapping qualities ≥ 20 here required by default to guarantee confident genotype calls) or from genotypes provided as input to EthSEQ in VCF format. Principal component analysis (PCA) is next performed by means of  SNPRelate  R package ( Zheng  et al. , 2012 ) on aggregated target and reference models genotype data; only SNPs that satisfy user-defined call rate are retained. The space defined by the first two PCA components is then automatically inspected to first generate the smallest convex sets identifying the ethnic groups described in the  reference model  and next to annotate the ethnicity of the individuals of interest ( Supplementary Fig. S1b ). Individuals positioned inside an ethnic group set (or intersecting group sets) are annotated with the corresponding ethnicity and labeled with INSIDE. For individuals positioned outside all ethnic group sets, the relative contribution of each group is computed through the distances from the centroids using the procedure described in  Supplementary Figure S2 , and top ranked contributing groups are reported (labeled CLOSEST). To better discern ethnicity annotations across ancestrally close groups within a study cohort (for instance Ashkenazi and Caucasians), a multi-step inference procedure is provided. Given a tree of ethnic group sets such that sibling nodes have non-intersecting ethnic groups and child nodes have ethnic groups included in the parent node ethnic groups, ethnicity of individuals is inferred by a pre-order traversal of the tree. At each node with ethnic groups  S , annotations resulted from the analysis of the parent node is refined by reducing both  reference  and  target  models on individuals with annotations in  S  only. Global annotation of all individuals is updated throughout the tree traversal. 3 Performances and results Performances of EthSEQ ethnicity inference method were tested for precision, computational time and dependence on SNP set size on two main datasets, 1000 Genomes Project genotype data and germline samples TCGA data. Initial precision tests utilized 1000 Genomes Project data; we randomly divided 2096 individuals into reference and target model groups while preserving the ethnic groups proportions, and ran EthSEQ relying only on SNPs in WES platform-specific captured regions ( Supplementary Methods ). Analyses were performed using reference models either built considering major ethnic groups annotations (EUR, AFR, EAS and SAS) or considering annotations for all the corresponding 21 populations reported in the 1000 Genome Project. In the first case, individuals’ ethnicities were all correctly classified (100% precision and more than 97% of the individuals annotated with the INSIDE label) ( Supplementary Fig. S3, Table S1 ). When the fine-grained annotation was used, ethnicity inference reached a precision of 92.2% with the multi-step refinement analysis. For instance, when considering European individuals only that includes 5 populations, precision reached 94% ( Supplementary Methods  for details). Finally, EthSEQ performances were compared to LASER 2.0  trace  module ( Wang  et al. , 2015 ) performances on the same data. Results in the PCA space were highly concordant ( Supplementary Figs S4 and S5 ), but EthSEQ was up-to 10X faster using a single core and up-to 18X faster when exploiting parallel computation ( Supplementary Fig. S6 ) on multi-individual analyses ( Supplementary Methods  for details). Further, EthSEQ was ran on germline WES data from 604 TCGA (cancergenome.nih.gov) individuals with reported interview-based race classification (as per TCGA nomenclature, race is annotated as 513  White , 42  Black or African American  and 49  Asian ). 505  White  individuals were annotated by EthSEQ as EUR, 37  Black or African American  individuals as AFR and 48  Asian  individuals as EAS or SAS for an overall precision of 97.7%. EthSEQ results were compared to results from fastSTRUCTURE tool ( Raj  et al. , 2014 ) fed with genotype data generated by EthSEQ pre-processing module. For 594 individuals (98.3%) the two analyses inferred the same major ethnic contribution. Both tools inferred 5 individuals originally annotated as  Black or African American  in TCGA as admixed with a major Caucasian contribution, one originally annotated as  Asian  as non-admixed Caucasian, and two originally annotated as  White  as major African contribution (see  Supplementary Methods and Table S1 ). In terms of tool specific results, 4.6% of individuals were inferred as admixed by fastSTRUCTURE that explained the TCGA dataset population structure with 3 clusters achieving a precision of 98%; 7.9% of individuals were inferred as CLOSEST by EthSEQ with the majority with SAS main contributions, not captured by fastSTRUCTURE, and secondary African contribution correctly detected by EthSEQ when above 15%. EthSEQ analysis resulted 3.2X faster. The effectiveness of the multi-step refinement analysis was recently proven in a precision medicine setting study ( Zhang  et al. , 2016 ) were ethnicity based stratification was key to interpret the relevance of germline cancer-associated variants. Specifically, our analysis ruled out the possibility that the high fraction of germline cancer-associated variants observed in the clinical cohort of 343 patients with metastatic tumors ( Beltran  et al. , 2015 ) was due to the presence of Ashkenazi inheritance ( Carmi  et al. , 2014 ) shown to carry high percentage of cancer-associated variants. Provided an Agilent HaloPlex reference model including Ashkenazi genome data ( Carmi  et al. , 2014 ) the identification of Ashkenazi individuals required the multi-step analysis to precisely discern them from the ancestrally close European individuals; 29.7% of Ashkenazi individuals were identified confirming the anticipated fraction of about 30% based on an internal cancer registry. To measure the impact of the number of available SNPs on EthSEQ precision, we extended the performance analyses by randomly down-sampling the number of SNPs both in the 1000 Genome Project and in the TCGA dataset ( Supplementary Methods ).  Supplementary Figure S7  shows that using the multi-step refinement analysis, 2000 SNPs are sufficient to reach more than 98% precision. Overall, this data indicates that EthSEQ is also amenable to targeted sequencing NGS data. 4 Conclusions We presented EthSEQ, a rapid, reliable and easy to use R package to annotate individuals ethnicity from WES data. EthSEQ can be used to process single sample or multi-sample datasets, provides a large variety of pre-computed platform-specific reference models, a simple and transparent mode to generate ethnicity annotations starting from a list of BAM files and can be easily integrated into any WES based processing pipeline also exploiting multi-core capabilities. Funding This work has been supported by the Prostate Cancer Foundation Challenge Award 2014 (F.D., A.R.), the Caryl and Israel Englander Institute for Precision Medicine, New York and the European Research Council ERCCoG648670 (F.D.). 
 Conflict of Interest : none declared. Supplementary Material Supplementary Data Click here for additional data file. Supplementary Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Inference of historical migration rates via haplotype sharing</Title>
    <Doi>10.1093/bioinformatics/btt239</Doi>
    <Authors>Palamara Pier Francesco, Pe’er Itsik</Authors>
    <Abstract>Summary: Pairs of individuals from a study cohort will often share long-range haplotypes identical-by-descent. Such haplotypes are transmitted from common ancestors that lived tens to hundreds of generations in the past, and they can now be efficiently detected in high-resolution genomic datasets, providing a novel source of information in several domains of genetic analysis. Recently, haplotype sharing distributions were studied in the context of demographic inference, and they were used to reconstruct recent demographic events in several populations. We here extend the framework to handle demographic models that contain multiple demes interacting through migration. We extensively test our formulation in several demographic scenarios, compare our approach with methods based on ancestry deconvolution and use this method to analyze Masai samples from the HapMap 3 dataset.</Abstract>
    <Body>1 INTRODUCTION Recent advances in high-throughput genomic technologies enable population-wide surveys of genetic variation. Although exacerbating challenges associated with data handling, this increase in volume and resolution had the effect of exposing new genomic features, creating the need for new models and computational tools. Among these new genomic features, identical-by-descent (IBD) haplotypes have recently emerged as a new source of information in several genetic applications, ranging from genotype–phenotype association studies ( Browning and Thompson, 2012 ;  Gusev  et al. , 2011 ) to the reconstruction of recent familial relationships ( Huff  et al. , 2011 ;  Kirkpatrick  et al. , 2011 ), the inference of recent demographic events ( Gusev  et al. , 2012 ;  Palamara  et al. , 2012 ) or the study of natural selection ( Albrechtsen  et al. , 2010 ;  Han and Abney, 2012 ). IBD segments are co-inherited from recent common ancestors by pairs of individuals and are delimited by historical recombination events. Such recombination events can now be detected in cohorts that have been densely genotyped (although not requiring the availability of full sequences), and several methods have now been developed for efficient IBD detection in large datasets ( Browning and Browning, 2011 ;  Gusev  et al. , 2009 ). Although shared haplotypes are found to be common even across populations that diverged hundreds of generations ago ( Gusev  et al. , 2012 ), the average detectable IBD segment is transmitted from shared ancestors that lived tens to a few hundreds of generations before present. Haplotype sharing analysis is, therefore, suitable to reveal the signature of the relatively recent demographic events that followed the agricultural revolution, where most classical methods provide limited insight. Leveraging this property of IBD, several recent surveys relied on shared haplotypes to analyze population demographics ( Atzmon  et al. , 2010 ;  Henn  et al. , 2012 ;  Lawson and Falush, 2012 ). In a recent work ( Palamara  et al. , 2012 ), we studied several theoretical quantities of IBD haplotypes, as a function of the demographic history of a population. We used the derived framework to infer the demographic history of two populations with different characteristics: (i) a population that underwent substantial recent isolation (Ashkenazi Jews) and (ii) a cohort that deviates from a single population model, with migration across small demes likely playing an important role in shaping recent genomic diversity (Kenian Masai). The analytical models we previously described are limited by the assumption that all the analyzed samples belong to a single population. Although such models can be used to provide insights in cases of extreme historical isolation, fine-scale interactions across populations were frequent in recent history, and the reconstruction of these events is of great interest for genetic-driven investigation of historical events ( Henn  et al. , 2010 ) and genetic analysis at large. In this article, we propose an extension of the analytical framework described in  Palamara  et al.  (2012) , allowing to explicitly model the presence of multiple populations that interact through migration rates. We test our approach on several synthetic populations with known population size changes and migration rates, finding that our model accurately matches the empirical distributions and provides a novel tool for the inference of recent demographic events that involve multiple interacting demes. We compare our method with existing approaches based on the distribution of migrant tracts obtained through ancestry deconvolution, and we revisit the analysis of the Masai population using the presented formulation. 2 METHODS 2.1 Haplotype sharing and demographic history Here, we provide a brief summary of the formulation developed in  Palamara  et al.  (2012) , and we invite the reader to refer to that article for additional details. At a chosen genomic site, a pair of modern day individuals from a population will have a common ancestor that lived a number of generations in the past. Such common ancestor transmits several adjacent sites along with the one being considered, in a region that is delimited by any recombination event happening along the lineage between the two individuals on either side of the locus ( Fig. 1 ), and by chromosome boundaries. We define such non-recombinant region as IBD. Recombination shortens IBD segments during meiotic transmission, and the genetic length of shared haplotypes is probabilistically linked to the number of generations separating two individuals from their most recent common ancestor. In addition, standard assumptions of coalescent theory ( Kingman, 1982 ) postulate that when tracing the ancestry of a pair of individuals back in time, the chance of randomly finding their common ancestor is inversely proportional to the effective size of the analyzed population, with a smaller effective population size resulting on average on earlier common ancestors. Combining these principles, the length of IBD segments detected in a cohort of studied individuals can be used to gain insight into the distribution of coalescent times, which in turn can be used to gain insight into the effective population size within and across populations at different time scales.
 Fig. 1. An IBD segment (blue) is co-inherited by two present day individuals from a common ancestor that lived four generations in the past. Recombination shortens the IBD segment, as meiotic events occur along the lineage between the two individuals In the remainder of this article, a population’s effective population size in a coalescent model will simply be referred to as population size. We represent the demographic history of the studied population via the vector  , which may hold just one parameter in the simplest case of a constant (Wright–Fisher) population, or several parameters in more complex cases (e.g. current population size, ancestral population size and duration of an exponential expansion). The probability of the considered genomic site to be spanned by a shared IBD haplotype of genetic length comprised in the range   can be expressed as
 (1) 
where  , in the reminder simply written  , represents the probability of finding the common ancestor for the considered site at (continuous) time  t  in the past, measured in generations;  , later indicated as  , represents the probability of a segment spanning the site to have length  l  after being transmitted for  t  generations. In this model, population size is allowed to arbitrarily change in time. In the simple case of a Wright–Fisher population of constant size  N e , the coalescence probability is simply  . Recombination events may happen on either side of the considered locus at an exponential rate that depends on the number of meiotic events in the lineage to a common ancestor.  , therefore, assumes the form of a sum of two exponential random variables or Erlang-2 distribution:   (note that length here is expressed in Morgans). Combining these into  Equation (1)  and integrating, we obtain
 (2) 
or   for the particular case of  . As shown in  Palamara  et al.  (2012) ,  Equation (2)  can be used to obtain a closed form estimator of recent effective population size. Taking the limit of such estimator for  , it assumes the form
 (3) 
where   is the average observed fraction of genome shared through segments longer than a length threshold  u  (here in morgans). The computation of   allows us to derive several additional theoretical quantities of IBD sharing. Because of the linearity of the expectation operator, the average fraction of genome shared through IBD segments in the length range  R  is simply  . The distribution of the length of a randomly sampled segment shared by the pair of individuals is obtained as  , and it can be used to compute the expected length of a randomly sampled shared segment in the chosen length range,  s R . For a region of length γ, a pair of individuals is expected to share   segments, and the distribution for the number of shared segments can be modeled as a Poisson random variable with the aforementioned expectation. Using this information, an expression for the variance of the fraction of genome shared in an interval  R  can be computed. Finally, the full probability distribution for the fraction of genome shared by a pair of individuals through segments in the length range  R  can also be computed using the previously described quantities. The quantity   is, therefore, central in this formulation, as it allows expressing a number of different measures of IBD sharing as a function of demographic history. Furthermore,   only depends on   through  , the probability of a coalescence event in the demographic scenario  . If the goal is to infer the demographic parameters in a model comprising multiple populations, we, therefore, need to express the coalescence probability  , where   now includes historical size variation for multiple populations and migration rates across them. 2.2 IBD distributions in the presence of migration We begin discussing the case of multiple populations referring to a simple scenario, where two populations of constant size  N e  exchange individuals at a fixed rate  m  per individual, per generation (see model in  Fig. 2 a). We encode this migration rate using the matrix
 
 Fig. 2. Two demographic models that involve two populations and migration between them. In model ( a ), the populations have the same constant size  N e , and exchange individuals at the same rate  m . In model ( b ), a population of constant ancestral size  N atot  splits  G  generations in the past, resulting in two populations whose sizes independently fluctuate from   and   individuals to   and   individuals during  G  generations. During this period, the populations interact with asymmetric migration rates  m 12  and  m 21 We consider two individuals,  i  and  j , each sampled from either population. We trace the ancestors of these individuals at one genomic site and encode their state (in terms of population their ancestors belong to), using a vector of dimensionality 2. If individual  i  is sampled from population 1 and individual  j  from population 2, for example, the state at generation 0 is known and we write it as  . If both are sampled from population  . After  t  generations (measured in continuous time), the probability that the ancestor of individual  i  at this genomic location belongs to either population is given by
 (4) 
if individual  i  was sampled from population 1, or, symmetrically
 (5) 
if it was sampled from population 2. We are interested in expressing the probability of individuals  i  and  j  to coalesce at time  t . This requires both individuals to be in the same population, in which case coalescence happens at rate  . Formally  , which in this setting becomes
 (6) 
if  , and
 (7) 
otherwise. To compute  , we plug the coalescence probability in  Equation (1) . Also, for simplicity, we take  , obtaining
 (8) 
if  , and
 (9) 
otherwise. Recall that  , which is the expected fraction of genome shared through segments of length between  u  and  v  by an individual pair. To infer   and  , we, therefore, consider the observed average fraction of genome shared through IBD segments longer than a threshold  u , for all pairs of individuals sampled from the same population or from different populations (which we call   and  , respectively, now omitting the dependence on the length range). We then solve the system obtained by equating   and   to the quantities in ( 9 ) and ( 10 ), to obtain the estimators
 
 (10) 
 A simple generalization of the aforementioned scenario consists in allowing the two considered populations to differ in their effective population sizes,   and  . In this scenario, it is still possible to obtain a closed form expression for  , and a closed form estimator for  , which are reported in the Appendix. 2.3 The general case Although the previously discussed case of constant population sizes and migration rates has a simple formulation and can be used to gain initial insight into the recent demography of a study cohort, such population dynamics are oversimplified and generally unrealistic. Luckily, given a few reasonable assumptions, population sizes and migration rates can be allowed to arbitrarily fluctuate in time, still permitting a closed form computation of  . Consider two populations whose sizes at generation  g  are expressed as   and  . The rate at which these two populations exchange individuals can be encoded in a discrete migration matrix
 (11) 
where   represents the probability of an individual migrating from population 1 to population 2 at generation  g  (backwards in time). After  g  generations, the probability that the ancestor of individual  i  at a genomic location belongs to either population is given by the vector  . Define the matrix   to be diagonal with   and   as its diagonal elements. The probability of coalescence from generation  g  − 1 to generation  g  is then
 (12) 
and the probability of the two individuals to coalesce  g  generations before present is
 (13) 
 Equation (13)  can be used in  Equation (1) , in its discrete version, to compute
 (14) 
 Note that  Equation (14)  is general, and we can allow additional demographic changes to take place. For instance, by setting   and   for all  , we encode a population split that occurred  G  generations ago. In practice, a pair of populations will have split a number of generations back in time, and it is, therefore, convenient to consider models of the kind depicted in  Figure 2 b. In this model, a population of constant size  N atot  splits  G  generations in the past, forming two populations of size   and  . The size of these two groups then fluctuates in time, to reach a present size of   and  . During their separation, the populations exchange individuals at a rate of  m 12  and  m 21  per generation, per individual. Of course, other models can be defined, allowing variable migration rates, and different population size dynamics. For mathematical convenience, it is safe to assume the ancestral population size becomes constant a number of generations in the past. Models where the ancestral population size ( N atot  in  Fig. 2 b) is constant from generation  G  to infinity allow for a closed form computation of  Equation (14) , no matter which demographic dynamics take place from generation 0 to  G  [see  Palamara  et al.  (2012)  for this expression]. Furthermore, extremely remote demographic events have negligible impact on shared haplotypes of currently detectable lengths (e.g.   cM). 2.4 Simulations, ancestry deconvolution and real data We tested our framework using extensive simulation of realistic chromosomes under several demographic models, using the GENOME coalescent simulator ( Liang  et al. , 2007 ). For computational convenience, we set the size of the simulator’s non-recombinant segments between 0.01 and 0.025 cM, as specified in  Section 3 , always using a recombination rate of 1cM/Mb. A modified version of the simulator was used to extract ground truth IBD haplotypes from the simulated genealogies, defined as non-recombinant segments co-inherited by pairs of individuals from their most recent common ancestor. For some of the simulations, we inferred shared haplotypes using the GERMLINE software package ( Gusev  et al. , 2009 ) on phased genotype data, which were obtained setting GENOME’s mutation rate to   per base pair ( Roach  et al. , 2010 ). Genotypes were post-processed to mimic the information content of array data. To this extent, we computed the allele frequency spectrum of European individuals from the HapMap 3 dataset ( Frazer  et al. , 2007 ), using frequency bins of 2%. We then randomly selected the same proportion of alleles from the simulated genotypes. We obtained an average density of   single nucleotide polymorphisms/Mb. To compare the proposed IBD-based approach for migration inference to the approach of  Gravel (2012) , which is based on ancestry deconvolution, we simulated synthetic datasets under several demographic models and extracted genotype data as previously described. We then ran the PCAdmix software ( Brisbin  et al. , 2012 ) with windows of size 0.3cM and the genetic map used in the simulations. The output of PCAdmix was used to infer migration rates via the Tracts software package ( Gravel, 2012 ). IBD information was computed in the same datasets running the GERMLINE software, and the output was used to infer migration rates using the DoRIS software package, which implements the proposed framework. Perfectly phased haplotypes were used in input for both PCAdmix and GERMLINE. Only migration rates were inferred, whereas all other demographic parameters were set to the true simulated values for both Tracts and DoRIS. To demonstrate the use of the DoRIS framework on real data, we analyzed 56 trio-phased samples from the HapMap 3 dataset. Phased genotypes were downloaded from the HapMap 3 webpage at  http://hapmap.ncbi.nlm.nih.gov . IBD haplotypes were extracted using GERMLINE, as previously described in  Palamara  et al.  (2012) . 3 RESULTS 3.1 Constant size and symmetric migration rates To test the accuracy of demographic inference based on the proposed model, we initially simulated a number of populations of constant size  N e , which exchange individuals at a constant, symmetric migration rate  m , as depicted in the model of  Figure 2 a. We simulated 15 possible sizes of synthetic populations, ranging from 2000 to 30 000 haploid individuals, with increments of 2000. For each population size, we simulated 11 possible migration values, uniformly chosen between   and  . For a total of 165 datasets, we simulated a chromosome of 300 cM for 500 haploid individuals from each subpopulation and computed IBD sharing within and across populations. The simulations used non-recombining blocks of 0.02 cM. This resolution may introduce small biases in the analysis, which we found to be negligible in our previous work. We then used  Equation (10)  to estimate   and  , with results shown in  Figure 3 . To test the model’s accuracy, for this analysis, we only considered ground-truth IBD segments extracted from the synthetic genealogies (see  Section 2 ).
 Fig. 3. True versus inferred parameters for the model in  Figure 2 a. Estimates were obtained using  Equation (10) We obtained a good correspondence between the true population size and the size inferred via the estimator of  Equation (10) , with almost perfect correlation shown in  Figure 3 a. Inferred migration rates were also close to the simulated rates, although a moderate upward bias and higher estimation variance for large migration rates was observed in this case ( Fig. 3 b). In addition to using the effective population size estimator of  Equation (10) , we used the estimator previously computed in  Palamara  et al.  (2012)  for the case of constant population with no migration, reported in  Equation (3) . As expected, the inferred recent effective population size was in this case inflated by the presence of migration, as shown in  Figure 4 . When migration rates are increased, the inferred population size quickly approaches the total population size (in this case  ).
 Fig. 4. Inference of recent effective population size using  Equation (3) , which neglects migration. The ratio between inferred and true population size ( y -axis) increases as the migration rate ( x -axis) is increased, approaching the sum of population sizes for both populations (twice the true size) 3.2 Dynamic size and asymmetric migration rates We then tested our model’s performance in the more complex demographic scenario depicted in  Figure 2 b, where a population splits into two subpopulations that grow at different exponential rates, interacting with asymmetric migration rates. We simulated a chromosome of   cM for 500 haploid individuals per subpopulation. Simulated non-recombinant blocks had size 0.025 cM. In all simulated scenarios, we kept  N atot  fixed to 10 000 haploid individuals, whereas   and   were kept fixed at 5000 individuals. For   and  , we simulated all possible combinations of sizes between 5000 and 205 000 haploid individuals, with increments of 15 000 (excluding cases where  ). Note that on average, the simulated values of   were smaller, resulting in higher inference accuracy compared with  . For each pair of population sizes, we simulated values of  m 12  and  m 21  using all combinations of the migration rates 0.0001, 0.0167, 0.0334 and 0.5. A total of 540 synthetic populations were tested. For each synthetic population, we extracted the average fraction of genome shared through haplotypes of different length intervals by pairs of individuals within each population or across populations. As in our previous work, we used a combination of intervals of uniform length and length intervals corresponding to quantiles of the Erlang-2 distribution, which is used in  . Inference performance was tested via minimization of the root-mean-squared deviation between observed and predicted average fraction of shared genome. Note that a likelihood-based approach (e.g. considering the number of shared segments) could be used based on the quantities derived in  Section 2.1 . We scanned several possible values for one parameter at a time, performing a line search while fixing the remaining model parameters to the true simulated value. The results of this analysis are reported in  Figure 5 .
 Fig. 5. Results of the evaluation of our method on synthetic populations with demographic history depicted in the model of  Figure 2 b. Higher variance in the method’s accuracy is observed because of limited sample sizes and increased population sizes. Higher migration rates further decrease the rate of coalescent events in the recent generations ( Fig. 5 b), resulting in additional uncertainty. However, no significant bias is observed in the inference As expected, because of the large recent effective population sizes we simulated, the variance of the inference accuracy was higher in this scenario, suggesting that more than a single chromosome for 500 diploid individuals may be required for the analysis of these demographics. A single chromosome of   cM sampled in 500 diploid individuals is in fact equivalent for the purpose of this inference to the analysis of all the autosomal chromosomes for   diploid samples (see  Palamara  et al. , 2012 ). Larger population sizes result in lower signal-to-noise ratio for the estimation of the expected fraction of genome shared via IBD segments, and increasing sample size or analyzing additional chromosomes is expected to reduce the variance in the inference performance. Lower accuracy was observed in the inference of   since, as previously mentioned, this simulated subpopulation was on average larger. Inferred population sizes were more accurate in the presence of low-migration rates (represented by colors in  Fig. 5 a and b), as high migration further reduces the chance of early coalescent events, exacerbating the effects of large population sizes. Overall, no significant bias was observed in the recovered parameter values, suggesting our model provides a good match for the empirical distributions. 3.3 Applicability of the model to genotype data Although the previous analysis was mainly concerned with testing the model’s accuracy, and it relied on ground-truth IBD sharing extracted from the simulated genealogies, it is interesting to ask whether this approach can be used on genotype data. To this extent, we simulated genotypes for the demographic model of  Figure 2 a. We set the population sizes to 4000 or 12 000 diploid individuals per population, and extracted 300 diploid sampled from each group. The migration rate was symmetric and set to 0.04 per individual, per generation. Chromosomes of 150 cM were simulated using non-recombinant blocks of size 0.01 cM, and the synthetic genotypes were post-processed to reproduce the density and allele frequency spectrum of realistic SNP array data (see  Section 2 ). In addition to extracting the ground truth IBD information as previously described, we inferred IBD haplotypes from the simulated genotypes using the GERMLINE software. The results suggest that when accurate phase information is available (e.g. for the X Chromosome, or for trio-phased samples), GERMLINE is able to recover the IBD sharing distribution across any pair of samples with high fidelity ( Fig. 6 ). However, when the samples were computationally phased using the Beagle software ( Browning and Browning, 2007 ), GERMLINE had an inconsistent performance, accurately recovering the IBD sharing in the case of  N  = 4000, whereas poorly inferring long haplotypes in the case of  N  = 12 000. This suggests that additional care must be taken when analyzing computationally phased data, particularly when analyzing cross-population IBD spectra, were the quality of the inferred IBD haplotypes will likely vary from population to population, as a result of different underlying demographic histories.
 Fig. 6. We simulated a chromosome of 150 cM for 600 individuals using the model in  Figure 2 a, setting population sizes to 4000 and 12 000 diploid individuals, with a migration rate of 0.04. IBD sharing was extracted directly from the simulated genealogy (diamonds), or inferred trough GERMLINE using perfectly phased (circles) or computationally phased (triangles) chromosomes 3.4 Real data analysis To demonstrate the applicability of our method to real data, we analyzed the HapMap 3 Masai dataset, which was already studied in our previous work using a simulation-based approach. We here revisit this analysis, using the described analytical framework. Cryptic relatedness across individuals in this dataset is extremely common, and it does not appear to be because of the presence of occasional outliers among the samples. Demographic reports are not supportive of recent population bottlenecks in this group, which is, though, to be slowly but steadily expanding ( Coast, 2001 ). The Masai are a semi-nomadic people, and individuals often reside in small communities ( Manyatta ) of tens to few hundreds of members. To study their demography, we, therefore, use a model where  V  villages of constant size  N  exchange individuals at a constant and symmetric rate  m . This model is similar to the one depicted in  Figure 2 a, with symmetric migration rates across several populations. We assumed that all samples were extracted from the same village and used the model described in  Section 2.3  for the analysis. We performed a grid search testing migration rates from 0.01 to 0.4, with intervals of 0.01, village sizes from 50 to 4000 with steps of 10 and number of villages from 3 to 150 with increments of 1. We also obtained 95% confidence intervals for the inferred values using a bootstrap approach, by creating 400 re-samples randomly selecting individuals with replacement, then re-computing the optimal parameters using a gradient-driven procedure, which was initialized using the parameters inferred using the original samples (note, however, that small correlations exist for IBD sharing across individual pairs, and this method may provide optimistic intervals). Using this approach, we obtained the following estimates:  V  = 58 (95% CI: 46–75),  N  = 400 (95% CI 360–470) and   (95% CI 0.09–0.12). 3.5 Comparison with existing methods The structure of long-range haplotypes is known to carry relevant information about recent population dynamics, but this genomic feature has only recently become observable thanks to the development of modern high-throughput genomic technologies. As a consequence, methods that rely on a population’s haplotypic structure to reconstruct demographic events have only recently arose. A model proposed in  Pool and Nielsen (2009) , and recently expanded in  Gravel (2012) , provides a way to analyze the distribution of migrant tracts and infer the timing and intensity of recent migration events. To analyze the distribution of migrant haplotypes, however, ancestry deconvolution needs to be accurately performed. This typically requires the availability of two suitable reference populations, which are required to be sufficiently diverged from each other. The amount of required divergence depends on the specific method used for the deconvolution, but in general, this poses significant constraints in terms of the demographic scenarios that can be analyzed using these methods. To compare our IBD-based approach with methods based on ancestry deconvolution, we simulated the demographic scenario of  Figure 7 , where two populations split  G s  generations in the past, and  G a  generations in the past contribute a fraction of genomes to the creation of a group of admixed individuals, with probability  m  and  , through a unique pulse of migration. All three population sizes were fixed to either  N  = 5000 or  N  = 10 000, m was set to 0.2 and  G a  was 25 in all simulations. We varied  G s  from 40 to 600, with increments of 20, and extracted genotype data on a single 400 cM chromosome for 250 diploid samples in each of the three extant populations (see  Section 2 ). We used the output of the PCAdmix software as input for the Tracts program ( Gravel, 2012 ), and the IBD segments retrieved by GERMLINE as input for the DoRIS software. Note that for the IBD analysis, we only used the 250 admixed samples and the 250 samples from the population contributing   haplotypes at generation  G a , whereas the samples from the third population were ignored. In both cases, we inferred the value of  m , setting all other parameters to the true simulated values, with results shown in  Figure 8 .
 Fig. 7. The model used to simulate admixed populations 
 Fig. 8. We created several simulation genotype datasets using the model in  Figure 7 , varying  G s  while keeping  , and using constant populations of size 5000 or 10 000 diploid individuals. We inferred the value of  m  using PCAdmix + Tracts, or GERMLINE + DoRIS, here reported as a function of  G s DoRIS performed better on average (mean inferred  , std 0.025), although providing slightly noisy results, suggesting the need for a larger sample size and/or the analysis of additional chromosomes. The migration rate inferred by Tracts (mean  , std 0.0233) was strongly biased. We note that in this setting, Tracts is essentially used to only report the proportion of ancestry inferred by the deconvolution method, which is the actual source of inaccuracy. Even for populations that diverged 600 generations in the past (∼15 000 years before present assuming a generation of 25 years), the recovered rate was substantially lower than the simulated rate. The case of  N  = 5000 yielded better estimates because of the higher drift found in smaller populations, which improved the power of PCAdmix to call migrant tracts. We additionally run the PCAdmix + Tracts analysis on longer time scales, simulating values of  G s  from 200 to 6000, with intervals of 200 generations, using  N  = 10 000. Even for several thousand generations since the split of the reference populations, a small bias was observed ( Fig. 9 ).
 Fig. 9. We created several datasets using the model in  Figure 7 , varying  G s  from 200 to 6000, and using   with population sizes of 10 000 diploid individuals. We inferred the value of  m  using PCAdmix + Tracts from phased genotype data This analysis suggests that although the methods that rely on ancestry deconvolution are a useful tool for the specific case of recently admixed groups arising from strongly diverged populations, they may not be suitable for the analysis of fine-scale migration events, such as those that occurred across populations that split few tens to hundreds of generations in the past. It is, however, possible that adjusting some of the parameters used for the GENOME simulations and for the PCAdmix software, or using other deconvolution methods, the obtained accuracy may be increased. Furthermore, the development of methods for ancestry deconvolution in sequence data, where rare variants are observable, is expected to substantially increase the power of this analysis, although the effects of limited population divergence are likely to still affect the accuracy of methods that do not explicitly take this aspect into account. An additional difference to be noted between the two considered approaches is that Tracts does not model population size changes in the populations, focusing on relative migration rates, whereas DoRIS allows recovering both population size fluctuations and migration rates, thus providing insights into the magnitude of migration events. This increased flexibility, however, may complicate the inference, also in light of our observation that large sample sizes are required for the IBD analysis. 4 DISCUSSION In this article, we have extended our previous work on the relationship between long-range haplotypes that are shared IBD across individuals from a study cohort and the demographic history of the individuals’ populations of origin. Specifically, the described framework removes the limiting requirement that all sampled individuals belong to a single population and allows for explicitly modeling and inferring demographic interactions across multiple demes. The evaluation we performed on   synthetic populations confirms the accuracy of the derived IBD model and suggests that haplotype sharing can be used to gain insight into fine-scale demographic dynamics for the past tens to few hundreds of generations, provided enough samples are collected. Our analysis of the HapMap 3 Masai samples, as well as our previously reported analysis of an Askenazi cohort, suggests that this method can be applied to currently available datasets, provided that the quality of haplotype phasing and IBD detection is carefully considered. Among available methods for demographic inference, another approach that explicitly models the effects of recombination (the Pairwise Sequentially Markovian Coalescent model, PSMC) was recently proposed in  Li and Durbin (2011) . This model relies on a Markovian approximation of the coalescent with recombination ( McVean and Cardin, 2005 ) and is able to simultaneously consider the effects of mutation and recombination. The PSMC, however, differs from the proposed IBD-based model for its applicability, as it requires full sequence information and is currently focused on the analysis of remote demographic events using single individuals, or pairs of phased chromosomes. Because of the scarcity of coalescent events in the recent history, the simultaneous analysis of multiple samples is needed to infer recent demographics. An extension of the PSMC to handle the analysis of multiple samples, however, is computationally challenging, and efficient approximations are being developed ( Sheehan  et al. , 2013 ). In addition to these whole-sequence–based methods, independent current work ( Ralph and Coop, 2013 ) infers historical demographic changes from length distributions of IBD segments, taking a complementary, less parametric approach, thereby allowing increased flexibility during inference of plausible coalescent time distributions, but without providing explicit modeling of migration and population size changes. Among other methods aimed at inferring migration, our approach is conceptually related to those that rely on the frequency and length of migrant tracts. These methods, however, do not model population size fluctuations and are dependent on the possibility of reliably performing ancestry deconvolution to assign chromosomal tracts to a set of reference populations. These populations may not be available and, more importantly, need to be substantially divergent to attain high-quality deconvolution, as shown in our analysis. Although whole-sequence datasets and methodological developments may improve the performance of deconvolution methods, this limitation may prevent methods based on migrant tracts from being effectively used in the reconstruction of fine-scale migration patterns of the recent millennia. Methods based on ancestry deconvolution, however, may in some scenarios be used in concert with methods based on IBD sharing. Knowing whether an IBD tract was co-inherited from a specific population, in fact, may provide information on the directionality of migration, and also offer further insight into deeper time scales, as shown in  Campbell  et al.  (2012)  and  Velez  et al.  (2012) . This direction may be further explored in light of the recently developed analytical model for migrant tracts and the presented model for IBD. The proposed IBD framework will further be enhanced by accurate whole-genome sequence information, as the presence of mutations on IBD segments will improve the timing of common ancestors and IBD detection of shorter segments. Finally, our model still relies on the assumption of selective neutrality. Natural selection has been shown to have an impact on long-range haplotype sharing ( Albrechtsen  et al. , 2010 ;  Gusev  et al. , 2012 ). Although selective forces are mostly visible at local scales, demography affects the entire genome. This framework could, therefore, be used to test local deviations from neutrality, and the presented extension, which handles the case of multiple population models, may further assist the analysis of cross-population IBD sharing in this context. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>smCounter2: an accurate low-frequency variant caller for targeted sequencing data with unique molecular identifiers</Title>
    <Doi>10.1093/bioinformatics/bty790</Doi>
    <Authors>Xu Chang, Gu Xiujing, Padmanabhan Raghavendra, Wu Zhong, Peng Quan, DiCarlo John, Wang Yexun, Birol Inanc</Authors>
    <Abstract/>
    <Body>1 Introduction Detection of low-frequency variants is important for early cancer diagnosis and is a very active area of research. Targeted DNA sequencing generates very high coverage over a specific genomic region, therefore allowing low-frequency variants to be observed from a reasonable number of reads. However, distinguishing the observed variants from experimental artifacts is very difficult when the variants’ allele frequencies are near or below the noise level. Providing an error-correction mechanism, unique molecular identifiers (UMIs) have been implemented in several proof-of-concept studies ( Jabara  et al. , 2011 ;  Kennedy  et al. , 2014 ;  Kukita  et al. , 2015 ;  Newman  et al. , 2016 ;  Peng  et al. , 2015 ;  Schmitt  et al. , 2012 ) and used in translational medical research ( Acuna-Hidalgo  et al. , 2017 ;  Bar  et al. , 2017 ;  Young  et al. , 2016 ). In these protocols, UMIs (short oligonulceotide sequences) are attached to endogenous DNA fragments by ligation or primer extension, carried along through amplification and sequencing and finally identified from the reads. Sequencing errors can be corrected by majority vote within a UMI family, because reads sharing a common UMI and random fragmentation site should be identical except for rare collision events ( Liang  et al. , 2014 ) or errors within the UMI sequences. DNA polymerase errors occurring during DNA end repair and early PCR cycles (particularly the first cycle), however, cannot be corrected because all reads in the UMI would presumably carry the error. Although PCR error rates are low ( 10 − 4 − 10 − 6 , depending on the enzyme and types of substitution), they impose fundamental limits to UMI-based variant calling. A two-step UMI-based variant calling approach that first constructs a consensus read with tools like fgbio ( https://github.com/fulcrumgenomics/fgbio ) and then applies one of the conventional low-frequency variant callers ( Xu, 2018 ) to the consensus reads has been implemented in  Peng  et al.  (2015)  and  Blumenstiel  et al.  (2017) . In addition to the two-stage method, three UMI-based variant callers, DeepSNVMiner ( Andrews  et al. , 2016 ), smCounter ( Xu  et al. , 2017 ) and MAGERI ( Shugay  et al. , 2017 ), are publicly available. DeepSNVMiner relies on heuristic thresholds to draw consensus and call variants. By default, a UMI is defined as ‘supermutant’ if 40% of its reads support a variant and two supermutants are required to confirm the variant. smCounter was released in 2016 by our group and reported above 90% sensitivity at fewer than 20 false positives per megabase for 1% variants in coding regions. smCounter’s core algorithm consists of a joint probabilistic modeling of PCR and sequencing errors. MAGERI is a collection of tools for UMI-handling, read alignment, and variant calling. The core algorithm estimates the first-cycle PCR errors as a baseline and calls variants whose allele frequencies are higher than the baseline level. MAGERI reported 93% area under curve (AUC) on variants with about 0.1% allele frequencies. In this article, we present smCounter2, a single nucleotide variant (SNV) and short indel caller for UMI-based targeted sequencing data. smCounter2 offers significant upgrades from its predecessor (smCounter) in terms of algorithm, performance and usability. smCounter2 adopts the widely popular Beta distribution to model the background error rates and Beta-binomial distribution to model the number of non-reference UMIs. An important feature of smCounter2 is that the model parameters are dynamically adjusted for each input read set. In addition, smCounter2 uses a regression-based filter to reject artifacts in repetitive regions while retaining most of the real variants. The algorithm improvements help to push the detection limit down to 0.5% from the previously reported 1% and increase the sensitivity and specificity compared to other UMI-based methods (two-step consensus-read approach and smCounter), as shown in Section 3. For ease of use, smCounter2 has been released with a Docker container image that includes the complete read processing (using reads from a QIAGEN QIAseq DNA targeted enrichment kit as an example) and variant calling pipeline as well as all the supporting packages and dependencies. 2 Materials and methods 2.1 smCounter2 workflow smCounter2’s workflow ( Fig. 1 ) begins with read-processing steps that (i) remove the exogenous sequences such as PCR and sequencing adapters and UMI, (ii) identify the UMI sequence and append it to the read identifier for downstream analyses and (iii) remove short reads that lack enough endogenous sequence for mapping to the reference genome. The trimmed reads are mapped to the reference genome with BWA-MEM, followed by filtering of poorly mapped reads and soft-clipping of gene-specific primers. A UMI with much smaller read count is combined with a much larger read family if their UMIs are within edit distance of 1 and the corresponding 5' positions of aligned R2 reads are within 5 bp (i.e. at the random fragmentation site). After UMI clustering, the aligned reads (BAM format) are sent for variant calling.
 Fig. 1. smCounter2 workflow. Rectangular boxes represent the data files and elliptical boxes represent steps of the pipeline. Users can choose to run the whole pipeline from FASTQ to VCF or run the variant calling part only from BAM to VCF Like many variant callers, smCounter2 walks through the region of interest and processes each position independently. At each position, the covering reads go through several quality filters and the remaining high-quality reads are grouped by putative input molecule (as determined by both the clustered UMI sequence and the random fragmentation site). A consensus base call (including indels) is drawn within a UMI if  ≥ 80 %  of its reads agree. The core variant calling algorithm is built on the estimation of background error rates, i.e. the baseline noise level for the data. A potential variant is identified only if the signal is well above that level (Sections 2.2 and 2.3). The potential variants are subject to post-filters, including both traditional filters such as strand bias and novel model-based, UMI-specific repetitive region filters (Section 2.4). Finally, the variants are annotated with SnpEff ( Cingolani  et al. , 2012a ) and SnpSift ( Cingolani  et al. , 2012b ) and output in VCF format. For better flexibility, users can choose to run the variant calling part only. smCounter2 accepts both raw UMI-tagged BAM file and consensused BAM file (e.g. generated by fgbio) as input. In addition, smCounter2 can be used to verify a list of pre-called variants if a VCF file is provided. 2.2 Estimation of background error rates Estimating the background error rates is one of the commonly used strategies in somatic variant calling. EBCall ( Shiraishi  et al. , 2013 ) and shearwater ( Gerstung  et al. , 2014 ) assume that each site has a distinctive error rate (predominantly sequencing errors) that follows a Beta distribution. LoLoPicker ( Carrot-Zhang and Majewski, 2017 ) estimates site-specific sequencing error rates as fixed values. For UMI-tagged data, background errors can come from base mis-incorporation by DNA polymerase during end repair and the first-cycle PCR reaction, oxidation damage to DNA bases during sonication shearing and probe hybridization ( Newman  et al. , 2016 ;  Park  et al. , 2017 ), UMI mis-assignment, misalignment, and polymerase slippage (often in repetitive sequences), etc. iDES ( Newman  et al. , 2016 ) characterizes the site-specific background error rates in duplex-sequencing data using Normal or Weibull distributions. The limitation of these algorithms is the requirement of many control samples for the site-specific error modeling. As an alternative, MAGERI ( Shugay  et al. , 2017 ) assumes a universal Beta distribution for all sites, which may result in lower accuracy compared to site-specific error modeling, but as a trade-off requires only one control sample, if the UMI coverage is high enough to observe the background errors and enough sites are covered to reveal the full distribution of error rates. smCounter2 takes similar experimental and modeling approaches as MAGERI with important modifications. To obtain high-depth data for error profiling, we sequenced 300 ng of NA12878 DNA within a 17 kbp region using a custom QIAseq DNA panel. After excluding the known SNPs [Genome in a Bottle Consortium ( Zook  et al. , 2014 )], we calculated the error rates by base substitution at each site assuming any non-reference UMIs are background errors. The calculation process is explained in  Supplementary Material , Section 2. We observed notable variation across different base substitutions and that transitions were more error-prone than transversions ( Fig. 2a ). We used the Beta distribution to fit the observed error rates [ R fitdistrplus  ( Delignette-Muller and Dutang, 2015 ),  Fig. 2b ]. The quantile plot indicates good fit in general and under-estimation of the tail, possibly due to outliers ( Fig. 2c ). We prepared two versions of error models, one excluding singletons (UMIs with only one read pair) and the other including singletons, to accommodate deep and shallow sequencing depths. For read sets with mean read pair per UMI (rpu)  ≥ 3 , smCounter2 drops singletons to reduce errors and uses the error model without singletons. For read sets with rpu &lt; 3, smCounter2 keeps some or all singletons ( Supplementary Material , Section 4) to avoid losing too many UMIs, and uses the error model with singletons.
 Fig. 2. Underlying model of smCounter2. ( a ) Background error rates for each type of base change, averaged across the panel of M0466. ( b ) Modeling of the background error rates using the Beta distribution. The histogram shows the frequency of observed  G &gt; A  error rates in M0466. The dashed curve is the density of the fitted Beta distribution. ( c ) Quantile plot to check the goodness-of-fit of the  G &gt; A  error rate modeling. The observed and fitted quantiles form a 45° line in most places, indicating perfect fit. The tail skews towards ‘observed’, indicating under-estimation of the extremely high error rates. This may simply be explained by outliers, or suggests that a distribution (or mixed distributions) with heavier tail is needed. ( d ) A real example of parameter adjustment. The dashed curve is the originally fitted Beta distribution. The dotdashed curve with higher peak is the adjusted error model with the mean of the input data (N13532) and the original variance. ( e ) Illustration of the variant calling  P -value. The density curve is a hypothesized Beta-binomial distribution. The vertical line indicates the observed non-reference UMI counts. The area of the shaded region is the  P -value. ( f ) Detection limit prediction and confirmation. The top and bottom curves are the predicted site-wise detection limit for Ti and Tv/indels, respectively. The dots are the true variants in N13532 (outliers with extremely low UMI depth or high allele frequency excluded). For the dots, the  y -axis represents the observed allele frequencies. Round dots are the variants detected and triangle dots are the ones not detected, concentrated in the low enrichment regions As a distinctive feature of smCounter2, the Beta distribution parameters are adjusted for each dataset to account for the run-to-run variation. Because the true variants are unknown in the application dataset, we conservatively assumed that all non-reference alleles with VAF below 0.01 are background errors. The low DNA input in most applications impose another challenge in that few of the applications generate enough site-wise UMI coverage for any meaningful update of the error rate distribution. Fortunately, sufficient UMIs can usually be obtained by aggregating the target sites to accurately estimate the mean. Therefore, we only adjust the mean of the Beta distribution to equate the panel-wise mean and leave the dispersion unchanged ( Fig. 2d ). In specific, the adjusted Beta parameters are
 (1) a * = μ * ( μ * ( 1 − μ * ) σ 2 − 1 ) (2) b * = ( 1 − μ * ) ( μ * ( 1 − μ * ) σ 2 − 1 ) , 
where  μ *  is the mean error rate of the current data and  σ 2  is the variance of the error rate from our control sample. The adjusted distribution  B e t a ( a * , b * )  has a mean of  μ *  and variance of  σ 2 . Background errors are sensitive to enrichment chemistry and DNA polymerase. The error pattern we observed in QIAseq DNA panels agrees with that in other PCR enrichment studies ( Potapov and Ong, 2017 ;  Shagin  et al. , 2017 ) but differs from hybridization capture studies ( Newman  et al. , 2016 ;  Park  et al. , 2017 ) where  A  &gt;  C  and  G  &gt;  T  errors are dominant. Also, certain high-fidelity DNA polymerases have been shown to generate tens- or hundreds-fold lower error rates ( Potapov and Ong, 2017 ). Therefore, we did not attempt to build a universal error model by pooling data from multiple experiments with different polymerases as MAGERI did, but instead suggest users who run hybridization capture protocols or use non-QIAseq enrichment chemistry to build their own error profile. This can be done using a script provided in the Github repository. Limited by sequencing resources, we were unable to obtain adequate site-wise UMI depth to model base substitutions with low error rates, including all transversions and some transitions. This deficit had several impacts on our modeling procedure. First, we had to assume that all transitions followed the distribution of  G  &gt;  A  (second highest) and all transversions followed the distribution of  C  &gt;  T  (higher than all transversions). This conservative configuration ensured that the error rates were not under-estimated, but also prevented us from reaching the theoretical detection limit. Second, we were unable to model the indel error rates because (i) indel polymerase errors occur more frequently in repetitive regions, and our panel did not include enough such regions, (ii) there are countless types of indels and we cannot model the errors by each type and (iii) indel polymerase error rates are on average lower than base substitution and we lacked the UMI depth to observe enough of them. Again, we conservatively assumed that indel error rates followed the distribution of  G  &gt;  A . Third, because the error rates are very low, zero non-reference UMIs were observed at some sites, especially in low enrichment regions. Depending on the percentage of such sites, we either imputed the zeros with small values or used a zero-inflated Beta distribution (a mixture of Beta distribution and a spike of zeros) instead of Beta. 2.3 Statistical model for variant calling and detection limit prediction We treated variant calling as a hypothesis testing problem, where the null hypothesis ( H 0 ) is that all non-reference UMIs are from background errors and the alternative hypothesis ( H a ) is that the non-reference UMIs are from the real variant. We assume that there are  n  UMIs covering a site and  k  of them have the same non-reference allele. Under  H 0 ,  k  follows a Binomial distribution  Bin ( n ,  p ) where  p  is the background error rate. If  p  follows the Beta distribution with the adjusted parameter  B e t a ( a * , b * ) , the marginal distribution of  k  given  n , a * , b *  is Beta-binomial. If a zero-inflated Beta distribution is used,  k  has a non-standard marginal distribution. To compute the  P -value, we first simulated random samples of  { p i , i = 1 , … , I }  according to the distribution being used. Then for each  p i  we computed  P B i n ( K ≥ k | n , p i )  based on the Binomial distribution. The  P -value represents the probability of observing  ≥ k  non-reference UMIs at a wild-type site ( Fig. 2e ) and is approximated by
 (4) P = I − 1 ∑ i = 1 I P B i n ( K ≥ k | n , p i ) ≈ ∫ 0 1 P B i n ( K ≥ k | n , p ) f ( p | a * , b * ) d p (3) = P B e t a − b i n ( K ≥ k | n , a * , b * ) . To avoid extremely small fractions, smCounter2 reports  Q = min ( 200 , −   log   10 P )  as the variant quality score. The choice of variant calling threshold depends on the tolerance of false positive rate because if the model fits perfectly, the specificity would equal to 1 minus the  P -value threshold. By default, smCounter2 aims for  ≤ 1  false positives per megabase, which is equivalent to a threshold of  P ≤ 10 − 6  or  Q ≥ 6 . We will show in Section 3 and  Supplementary Material  that this threshold works well for datasets with deep and shallow UMI coverage and for variants with a range of VAFs (0.5, 1, 5% and germlines). The only exception is that, if 0.5–1% indels are of interest, we recommend lowering the Q-threshold to 2.5 to account for the overestimation of indel error rates. Under this framework, the site-specific detection limit (sDL, the minimum allele frequency to exceed the  P -value threshold) is a decreasing function of the UMI depth. It also depends on the type of variant because transitions have higher background error rates than transversions and indels. We estimate that the sDL of transitions is higher than transversions and indels on by about 0.001, or 0.1% in allele frequency. We denote  P ( n , k , t )  as the  P -value given UMI depth  n , non-reference UMI count  k  and the type of variant  ∈ { Ti ,   Tv + indel } .  P ( n , k , t )  can be computed by  Equation (3) . The sDL is denoted as arg min  k { P ( n ,  k ,  t ) &lt; threshold}∕ n  and can be computed numerically. Importantly, the predicted sDL is the  observed  allele frequency that often deviates from the true allele frequency in the sample due to random enrichment bias. If we loosely define the overall detection limit as the minimum  true  allele frequency that the variant caller can detect with good sensitivity and specificity, the overall detection limit is usually higher than sDL. Based on our calculation, the theoretical detection limit of a QIAseq DNA panel is around 0.5% when UMI depth is between 2000 and 4000. This detection limit was confirmed experimentally by sequencing a sample with known 0.5% variants ( Fig. 2f ). 2.4 Repetitive region filters based on UMI efficiency Repetitive regions such as homopolymers and microsatellites are enriched in non-coding regions where variants can have important functions from regulating gene expression to promoting diseases ( Khurana  et al. , 2016 ). Unfortunately, these regions are a major source of false variant calls due to increased polymerase and mapping errors. For instance, polymerase slippage (one or more bases of the template are skipped over during base extension) occurs more frequently at homopolymers and results in false deletion calls. Reads may be incorrectly mapped to similar regions or mis-aligned if they do not span the whole repetitive sequence, both causing false variant calls. Conventional variant callers apply heuristic filters to remove false calls. For example, Strelka ( Saunders  et al. , 2012 ) rejects somatic indels at homopolymers with  ≥ 8 nt or di-nucleotide repeats with  ≥ 16 nt. Recent haplotype-based variant callers such as GATK HaplotypeCaller ( DePristo  et al. , 2011 ) perform local  de novo  assembly to avoid mapping/alignment errors in repetitive regions. However, these methods were developed for non-UMI data.smCounter2 includes a set of repetitive region filters that are specifically designed for UMI data. The filters were inspired by the observations that (i) UMIs of the false variants tend to have lower read counts and more heterogeneous reads compared to UMIs of real variants, and (ii) reads of the false variants are more likely to contradict with their UMIs’ consensus allele (usually wild-type), whereas reads of the real variants are likely to agree with their UMIs. We used the term ‘UMI efficiency’ to describe these distinctions ( Fig. 3a ) and quantified the UMI efficiency with four variables: (i)  vafToVmfRatio , the ratio of allele frequencies based on reads and UMIs; (ii)  umiEff , the proportion of reads that are concordant with their respective UMI consensus; (iii)  rpuDiff , difference of read counts between variant UMIs and wild-type UMIs, adjusted by the standard deviations and (iv)  varRpu , mean read fragments per variant UMI.
 Fig. 3. Training and testing of the homopolymer indel filter. ( a ) Illustration of UMI efficiency. The UMI on the left has perfect efficiency because all reads contributed to the consensus. The UMI on the right has low efficiency because two reads in red disagree with the majority and thus are wasted. smCounter2 requires 80% agreement to reach a consensus, so the entire UMI would be dropped and the other three reads would be wasted as well. ( b ) Relative importance of each predictor ranked by the explained variation minus the degree of freedom. The read pairs per variant UMI ( varRpu ) and the ratio between allele frequencies by read and by UMI ( vafToVmfRatio ) are the two variables with the most predictive power. The plot is generated with R rms package. ( c ) ROC curves of the logistic regression classifier. The black curve is for the training data that combined all true and false homopolymer indels in N0030, N0015, N11582 and N0164. The blue and red curves are for two test datasets N13532 and N0261, respectively. The dots represent the actual sensitivity and specificity at the cutoff, which is consistent in all three datasets We trained and validated a logistic regression model to distinguish real homopolymers indels from artifacts. We focused our resources on this repetitive region subtype because during development, we observed that homopolymer indels were the main contributor of false positives. We combined data from several UMI-based sequencing experiments to assemble a training set with 255 GIAB high-confidence homopolymer indels with allele frequencies from 1 to 100% and 386 false positives that would otherwise be called without the filters. In addition to the UMI efficiency variables, we included  sVMF  (VAF based on UMI) and  hpLen8  (binary variable indicating whether the repeat length  ≥ 8 ) as predictors. We found that  varRpb  and  vafToVmfRatio  were the two most important predictors in terms of explained log-likelihood ( Fig. 3b ). We chose the cutoff on the linear predictors to target on the highest sensitivity while maintaining 99% specificity using the R package OptimalCutpoints ( López-Ratón  et al. , 2014 ). The model and cutoff were applied to two independent datasets N13532 and N0261, both containing 0.5% variants. N13532 had 41 real homopolymer indels and 122 false positives with  Q ≥ 2.5 . The predictive model achieved 39.0% sensitivity, 96.7% specificity, and 0.868 AUC. N0261 had 39 real homopolymer indels and 42 false positives with  Q ≥ 2.5 . The predictive model achieved 71.8% sensitivity, 95.2% specificity, and 0.910 AUC ( Fig. 3c ). For other subtypes of variants and repetitive regions, we used heuristic thresholds as filters due to lack of training data. The model parameters and default thresholds are presented in the  Supplementary Material . 3 Results 3.1 Training and validation datasets To develop the statistical model and fine-tune the parameters, we did multiple sequencing runs using reference materials NA12878 and NA24385, both of which have high-confidence variants released by GIAB (v3.3.2 used for this study). We mixed small amounts of NA12878 DNA into NA24385 based on the amount of amplifiable DNA measured by QIAseq DNA QuantiMIZE assay to simulate low-frequency variants. The modeling of background error rates was based on M0466, a high-input, deep-sequencing run that reached over 45 000 UMI coverage per site. The selection of variant calling threshold and refinement of filter parameters were based on four datasets: N0030, N0015, N11582 and N0164. After development, we tested smCounter2 on three independent datasets: N13532, N0261 and M0253 without any modification to the algorithm and parameters. The datasets involved in this study are summarized in  Table 1 . A more detailed description of these datasets is provided in the  Supplementary Material .
 Table 1. Key statistics of the datasets used for training and testing of smCounter2 Dataset Purpose Sample Target region (bp) Mean UMI depth Mean read pairs per UMI VAF (%) SNVs Indels M0466 Training 0.2% NA12878 17 859 45 335 3.2 0.1 87 0 N0030 Training 2% NA12878 1 032 301 3612 8.6 1 363 56 N0015 Training 10% NA12878 406 846 4825 8.5 5 4412 369 N11582 Training 100% NA24385 1 094 204 479 2.6 50 or 100 729 49 N0164 Training 1–20% NA12878 66 661 3692 11.5 0.5–10 237 177 N13532 Test 1% NA12878 928 315 4040 7.6 0.5 293 164 N0261 Test 1% NA12878 45 299 3384 13.8 0.5 5 269 M0253 Test 50% HDx Tru-Q 7 38 370 4980 13.0 ≥ 0.5 36 (with MNPs) 1 3.2 Benchmarking 0.5% variant calling performance using mixed GIAB samples We benchmarked smCounter2 against six state-of-the-art UMI variant calling algorithms (fgbio+MuTect, fgbio+MuTect2, fgbio+VarDict, MAGERI, DeepSNVMiner and smCounter) on N13532, which contained 0.5% NA12878 variants. The first three algorithms represent the two-step approach discussed in Section 1. We first constructed consensus reads from the aligned reads (BAM file) using fgbio’s  CallMolecularConsensusReads and FilterConsensusReads  functions and then applied three popular low-frequency variant callers, MuTect, MuTect2 ( Cibulskis  et al. , 2013 , and VarDict ( Lai  et al. , 2016 ), on the consensus reads. MAGERI, DeepSNVMiner and smCounter are representative UMI-aware variant callers. The results ( Fig. 4 ), stratified by type of variant (SNV and indel) and genomic region (all, coding and non-coding), were measured by sensitivity and false positives per megabase (FP/Mbp, or  10 6 ( 1 − specificity ) ) at several thresholds. smCounter2 outperformed the other methods in all categories. In coding regions, smCounter2 achieved 92.4% sensitivity at 12 FP/Mbp for SNVs and 84.4% sensitivity at 7 FP/Mbp for indels ( Table 2 ). In non-coding regions, smCounter2 was able to maintain comparable accuracy for SNVs (83.3% sensitivity at 4 FP/Mbp), but produced lower sensitivity (56.8%) and higher false positive rate (42 FP/Mbp) for indels. In the indel-enriched dataset N0261, smCounter2 produced consistent sensitivity (81.4% in coding and 61.3% in non-coding) and seemingly higher FP/Mbp (0 in coding and 114 in non-coding). However, FP/Mbp in N0261 was based on a very small target region (45 kbp) and therefore provides a less accurate specificity estimate.
 Table 2. smCounter2 performance in detecting 0.5, 1, 5 and 50–100% variants, stratified by type of variant (SNV and indel) and genomic region (coding and non-coding) Dataset Region Type TP FP FN TPR (%) FP/Mbp PPV (%) HC size (bp) N13532 Coding SNV 171 7 14 92.4 12 96.1 591 154 (0.5%, test) Indel 38 4 7 84.4 7 90.5 591 154 Non-coding SNV 90 1 18 83.3 4 98.9 259 162 Indel 67 11 51 56.8 42 85.9 259 162 N0261 Coding Indel 35 0 8 81.4 0 100.0 6119 (0.5%, test) Non-coding Indel 138 4 87 61.3 114 97.2 35 172 M0253 All SNV/MNV 32 — 4 88.9 — — 38 370 (0.5–30%, test) Indel 0 — 1 0.0 — — 38 370 N0030 Coding SNV 214 5 4 98.2 7 97.7 694 189 (1%, training) Indel 36 1 3 92.3 1 97.3 694 189 Non-coding SNV 137 3 8 94.5 13 97.9 236 687 Indel 12 3 5 70.6 13 80.0 236 687 N0015 Coding SNV 528 0 4 99.2 0 100.0 35 718 (5%, training) Indel 9 0 1 90.0 0 100.0 35 718 Non-coding SNV 3851 7 29 99.3 24 99.8 297 805 Indel 285 13 74 79.4 44 95.6 297 805 N11582 Coding SNV 421 2 0 100.0 3 99.5 682 483 (50–100%, Indel 4 0 0 100.0 0 100.0 682 483 training) Non-coding SNV 301 1 7 97.7 4 99.7 269 761 Indel 34 1 11 75.6 4 97.1 269 761 Notes : The metrics were generated with the default thresholds ( Q ≥ 2.5  for indels in N13532, N0261, M0253 and  Q ≥ 6  for all other cases). The allele frequency and the purpose of the dataset are displayed under the dataset name. All performance metrics are measured on GIAB high-confidence regions only, the sizes of which are presented in the last column. Fig. 4. Benchmarking smCounter2, smCounter, fgbio+MuTect, fgbio+VarDict and fgbio+MuTect2 on 0.5% variants in N13532. The performance is measured by false positives per megabase ( x -axis) and sensitivity ( y -axis), stratified by type of variant (SNV and indel) and region (coding, non-coding, and all). The ROC curves are generated by varying the threshold for each method:  Q -score for smCounter2, prediction index for smCounter, likelihood ratio for MuTect and MuTect2 and minimum allele frequency for VarDict. MuTect does not detect indels so is not included in the indel comparison We did not show DeepSNVMiner and MAGERI’s performance in  Figure 4 . DeepSNVMiner generated 7654 FP/Mbp to achieve 86% sensitivity for SNVs at the default setting. Similar or worse performance was achieved at other settings that we tested. Because this level of false positive rate is much higher than other methods (&lt;200FP/Mbp at similar sensitivity), it would be hard to put the ROC curves in the same figure. For MAGERI, it is unfair to compare its performance with smCounter2 using QIAseq data. MAGERI’s error model is based only on primer extension assays from a mix of DNA polymerases including several high-fidelity enzymes ( Shagin  et al. , 2017 ), while smCounter2’s error model is specific to the entire QIAseq targeted DNA panel workflow, including DNA fragmentation, end repair and PCR enrichment steps. Because the MAGERI error model does not include errors introduced at the typical DNA fragmentation and end repair process (their assays do not have those steps), MAGERI’s background error rates are lower than those in smCounter2. For example, the mean error rate of  A  &gt;  G  and  T  &gt;  C  used by MAGERI is  6.3 × 10 − 5  per base ( https://github.com/mikessh/mageri-paper/blob/master/error_model/basic_error_model.pdf ) and about  3 × 10 − 4  per base for smCounter2 ( Fig. 2a ). Therefore, with QIAseq data, MAGERI will produce more false positives due to under-estimation of the error rate. We included MAGERI’s ROC curve in the  Supplementary Figure S2  to illustrate the point that the error models are specific to each NGS workflow and need to be empirically established for different workflows. We applied smCounter2 on the same fgbio consensus reads that were used with MuTect/MuTect2 and VarDict. As expected, fgbio+smCounter2_consensus achieved lower sensitivity and specificity than smCounter2 on the raw reads ( Supplementary Fig. S2 ). One reason is that many smCounter2-specific filters cannot be used in this case because the UMI efficiency metrics are not computed by fgbio and therefore lost after consensus. We had to use smCounter’s filters for the fgbio consensus reads. However, despite having the same filters and a better statistical model, fgbio+smCounter2_consensus was still outperformed by smCounter. This can possibly be changed by further fine-tuning the parameters of fgbio and smCounter2. But on the other hand, it illustrates the challenge of the two-stage approaches for UMI-based variant calling, which is harmonizing the consensus and variant calling algorithms, as pointed out in  Xu  et al.  (2017)  and  Shugay  et al.  (2017) . We used the default setting for smCounter and adjusted the parameters of fgbio, MuTect and VarDict based on our experience of working with them. However, given the infinite parameter space, we cannot claim that the results reported here reflect their optimal performance. Several variant calling thresholds were used to investigate the sensitivity-specificity trade-off and draw the ROC curves. For fgbio+MuTect/MuTect2, we used MuTect and MuTect2’s likelihood ratio score as threshold. For fgbio+VarDict, we set VarDict’s minimum allele frequency (− f ). For MAGERI, we did not use the seemingly obvious threshold ‘Q-score’ because they were not allowed to exceed 100 for computational reasons, and even a  Q -score of 100 was overly sensitive and generated too many false calls. Instead, we held  Q -score constant at 100 and varied the number of reads in a UMI  (-defaultOverseq ). The parameters and thresholds used in this study are listed in the  Supplementary Material , Section 4. 3.3 Detecting  ≥ 1 %  variants in (possibly) shallow sequencing runs smCounter2 achieved good sensitivity on 1, 5, 50 and 100% variants as well ( Table 2 , datasets N0030, N0015, N11582). The biggest advantage for smCounter2 was in non-coding regions due to the repetitive region filters. Compared to smCounter, for 1% non-coding variants, smCounter2’s sensitivity increased from 75.2 to 94.5% for SNVs and from 23.5 to 70.6% for indels ( Supplementary Fig. S3 ). For 5% non-coding variants, smCounter2’s sensitivity increased from 95.1 to 99.3% for SNVs and from 58.2 to 79.4% for indels ( Supplementary Fig. S4 ). For 50 and 100% non-coding variants, smCounter2’s sensitivity increased from 89.0 to 97.7% for SNVs and from 42.2 to 75.6% for indels ( Supplementary Fig. S5 ). Both smCounter2 and smCounter outperformed fgbio+MuTect and fgbio+VarDict on 1 and 5% variants in all categories. For germline variants, however, smCounter2 had lower sensitivity for non-coding indels compared to fgbio+HaplotypeCaller (75.6% versus 88.9%). This demonstrated the advantage of a haplotype-based strategy in difficult regions. Other than for non-coding indels, the two methods achieved comparable accuracy in other categories. To test smCounter2’s robustness under low sequencing capacity, we  in silico  downsampled N0030 to 80, 60, 40, 20 and 10% of reads to mimic a range of sequencing and UMI depths. smCounter2 outperformed other methods in all sub-samples (Supplementary Figs S6–S10). The downsample series also demonstrated that smCounter2’s constant threshold can maintain consistently low false positive rates for SNV across a range of UMI depths ( Fig. 5 ). In contrast, smCounter’s default threshold must move linearly with the UMI depth to maintain a certain level of false positive rate. Similarly, MuTect’s threshold based on the likelihood ratio needs to be adjusted for datasets with varying read depth. smCounter2’s invariant threshold allows users to apply the default setting to a wide range of sequencing and sample input conditions.
 Fig. 5. Default thresholds of smCounter and smCounter2 at different UMI depths and associated false positive rates based on the downsample series of N0030. smCounter’s threshold moves linearly with the UMI depth and is determined using an empirical formula  y = 14 + 0.012 x . smCounter2’s threshold is constant at 6. The false positive rates for SNV are well controlled (between 5 and 13 FP/Mbp, represented by the point size) using both methods It is important to note that the results described in Sections 3.2 and 3.3 are measured over GIAB high-confidence region. smCounter2’s performance in GIAB-difficult regions is unknown, both absolutely and in comparison to other variant callers. We also note that the results in Section 3.3 are based on training datasets only. We have not tested smCounter2 on independent 1% or above variants. 3.4 Detecting complex cancer mutations using horizon Tru-Q samples The performance data described so far were based on diluted NA12878 or pure NA24385, all of which contained germline variants. To test smCounter2 on low-frequency cancer mutations, we sequenced the Tru-Q 7 reference standard (Horizon Dx) that contained verified 1.0% (and above) onco-specific mutations. The sample was diluted 1:1 in Tru-Q 0 (wild-type, Horizon Dx) to simulate 0.5% variants. For this dataset (M0253), smCounter2 detected 32 out of 36 SNV/MNVs (88.9%) and narrowly missed the only deletion ( Q  = 2.49 for threshold of 2.5). Because not all variants in the Tru-Q samples are known, we cannot evaluate specificity using this dataset. The list of variants in this dataset, along with the observed VAF and smCounter2 results, can be found in Supplementary File  M0253.HDx.Q7.vcf . The Tru-Q sample contains some complex multi-allelic variants that are challenging for variant callers that are not haplotype-aware. For example, there are four variants  A  &gt;  C ,  A  &gt;  T , AC &gt; CT and AC &gt; TT at one position (chr7: 140453136, GRCh37) and a  C  &gt;  T  point mutation at the next position. smCounter2 detected the three SNVs but failed to recognize the two MNVs. 4 Discussion 4.1 Improvement over smCounter In this paper, we described smCounter2, the next version of our UMI-based variant caller. Compared to the previous version of smCounter, smCounter2 features lower detection limit, higher accuracy, consistent threshold and better usability. smCounter2 pushed the detection limit of QIAseq targeted DNA panels from smCounter’s 1% down to 0.5%. smCounter2 achieved a lower detection limit because the background error rates were accurately estimated for specific base incorporation errors. The statistical model allows smCounter2 to quantify the deviation from real variants to the background errors using  P -values. Therefore, the ambiguous variants whose allele frequencies are close to the background error rates can be called by smCounter2 with reasonable confidence. Importantly, 0.5% is a not an algorithm limit, but rather a chemistry limit. We believe that smCounter2 can achieve even lower detection limits for other chemistry with lower background error rate. smCounter2 has higher accuracy than its predecessor for both SNVs and indels, in both coding and non-coding regions, for both deep and shallow sequencing runs, and for both low-frequency ( ≥ 0.5 % ) and germline variants ( Fig. 4  and Supplementary Figs S2–S10). In particular, for 0.5% coding region mutations, smCounter2 achieved over 92% sensitivity for SNVs and 84% for indels in coding regions at the cost of about 10 false positives per megabase, a significant improvement compared to smCounter’s 82% sensitivity for SNVs and 27% for indels at similar false positive rate. The accuracy improvement is due to the modeling of background error rates and, particularly in non-coding regions, UMI-based repetitive region filters. The filters catch false positives in the repetitive regions that pass the  P -value threshold but have low ‘UMI efficiency’, a novel concept that we have proved to be useful in distinguishing real variants from artifacts. Particularly for indels in homopolymers, smCounter2 employs a logistic regression classifier that was trained and validated with separate datasets. smCounter2 has a more consistent variant calling threshold ( Q ≥ 2.5  for 0.5–1% indels and  Q ≥ 6  for other cases) that is independent from the UMI depth, unlike smCounter or MuTect whose optimal threshold must move with the UMI or read depth. This is because smCounter evaluates potential variants by the  number  of non-reference UMIs, while smCounter2 evaluates potential variants by the  proportion  of non-reference UMIs. Moreover, because smCounter2 performs a statistical test at each site, UMI depth has already been accounted for in the  P -value. A higher UMI depth will result in better power of detection without raising the threshold. The consistent threshold makes it easier to benchmark smCounter2 with independent datasets. As pointed out by  Xu (2018) , benchmarking studies face the challenge of tuning the variant callers for different datasets. smCounter2 is also easier to use than smCounter. The read-processing code has been released together with the variant caller, making smCounter2 a complete pipeline from FASTQ to VCF. Some users may prefer to use their own read-processing script because read structures may differ from protocol to protocol. These users can run the variant caller only with the BAM file as input, if UMIs are properly tagged in the BAM. In addition, smCounter2 accepts UMI-consensused BAM files or pre-called variants in VCF format as input. Last but not least, smCounter2 is released as a Docker container image so that users do not need to install the dependencies manually. 4.2 Comparison with other UMI-based variant callers smCounter2 achieved better accuracy over other UMI-based variant callers in most of our benchmarking datasets ( Fig. 4  and Supplementary Figs S2–S4, S6–S10) except for non-coding germline indels where smCounter2 was outperformed by fgbio+HaplotypeCaller ( Supplementary Fig. S5 ). Compared to the two-stage approach, smCounter2 requires less tuning and achieves better detection accuracy with low-frequency variants. In contrast to MAGERI’s strategy of pooling data from several polymerases, smCounter2’s error model is developed using a single dataset with very deep coverage. Library preparation method and DNA polymerase have a large impact on the background error rates. Therefore we believe that profiling the errors per individual polymerase and protocol is a better approach. Furthermore, smCounter2 adjusts the error model for each individual dataset, making it a Bayesian-like procedure where the final error model is determined by both the prior knowledge and the data. 4.3 Limitations smCounter2 has several limitations. First, the error model is specific to the QIAseq targeted panel sequencing protocol, which uses integrated DNA fragmentation plus end repair process and single primer PCR enrichment. Without further tests, we are less certain if the error model holds for other types of library preparation and enrichment protocols. We are more certain, however, that our error model would not fit the data generated by hybridization capture enrichment due to distinct base errors from hybridization chemistry. We have released the modeling code and encourage users, who want to use smCounter2 on non-QIAseq panel data, to re-estimate the background error rates if datasets with sufficient UMI depth are available. Second, limited by resources, we were not able to generate data with enough UMI depth to accurately estimate the transversion and indel error rates. This deficit prevented the variant caller from reaching the assay’s theoretical detection limit. However, as we continue to generate data, we will update the error models with more precise parameters. Third, the germline indel calling accuracy, especially in non-coding regions, is lower than the two-step approach of fgbio+HaplotypeCaller. Although smCounter2 has very efficient repetitive region filters, it still adopts a base-by-base variant calling strategy and relies on the mapping, which is error-prone in repetitive regions. Haplotype-aware variant callers such as HaplotypeCaller are more effective in repetitive and variant-dense regions because they perform local assembly and no longer rely on the local reference genome alignment information. Fourth, smCounter2 has difficulty in handling very complex variants. For example, it failed to report all minor alleles of the complex, multi-allelic variant in Section 3.4. This can potentially be solved by including haplotype-aware features. We have not tested smCounter2’s reliability in detecting variants with three or more minor alleles, partly because these variants are not observed frequently. By default, smCounter2 reports bi- and tri-allelic variants only. Fifth, the benchmarking study was based on reference standards. We have not demonstrated smCounter2’s performance using real tumor samples and therefore cannot claim clinical utility. We hope smCounter2 will be used in both translational and clinical studies and look forward to feedback from users. Additional files and availability of data The high-confidence heterozygous NA12878-not-NA24385 variants (GIAB v3.3.2) in N13532, N0261, N0030, N0015, high-confidence NA24385 variants in N11582 and verified Tru-Q 7 variants in M0253 are available in VCF format. N0015 and N0030 reads have been published in  Xu  et al.  (2017)  and are available in Sequence Read Archive (SRA) under accession number SRX1742693. M0253, N13532, N0261 and N11582 are available in SRA under study number SRP153933. Supplementary Material Supplementary Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>ASAP: a web-based platform for the analysis and interactive visualization of single-cell RNA-seq data</Title>
    <Doi>10.1093/bioinformatics/btx337</Doi>
    <Authors>Gardeux Vincent, David Fabrice P A, Shajkofci Adrian, Schwalie Petra C, Deplancke Bart, Stegle Oliver</Authors>
    <Abstract/>
    <Body>1 Introduction Several bioinformatic platforms have been developed that aim to lower the entry point to ‘-omic’ type of analyses ( Afgan  et al. , 2016 ;  Reich  et al. , 2006 ). The latter include pipelines dedicated to single-cell analyses such as SINCERA ( Guo  et al. , 2015 ), SEURAT ( Satija  et al. , 2015 ), MAST ( Finak  et al. , 2015 ), PAGODA ( Fan  et al. , 2016 ) or SC3 ( Kiselev  et al. , 2016 ). However, these pipelines are embedded in R which makes them still computationally complex. For example, SC3 has preprocessing abilities using the scater package ( McCarthy  et al. , 2017 ) but has an interactive component that focuses mainly on the clustering part. Indeed, most of the other available pipelines lack an interactive visualization component as well as integration of a broad range of available single-cell data processing algorithms. In response, several valuable platforms have recently been developed that integrate graphics components. These include SCell ( Diaz  et al. , 2016 ), Sincell ( Julia  et al. , 2015 ), Fastproject ( DeTomaso and Yosef, 2016 ), or START ( Nelson  et al. , 2016 ). These tools are embedded in stand-alone applications and cover more comprehensively the whole RNA-seq analysis pipeline, yet, they still lack key features. For example, FastProject performs filtering and visualization but no further analysis. SCell implements RUVg normalization only, and visualization is limited to PCA. Moreover, SCell lacks marker gene identification (based on differential gene expression analysis) or functional gene set enrichment capacities. Finally, these pipelines require local installation of the software, which can be time-consuming or even daunting. To alleviate these constraints, we developed ASAP, a fully integrated, web-based pipeline aimed at the complete analysis of scRNA-seq data post genome alignment. Our choice of rendering ASAP completely web-based was motivated by the fact that fewer users are inclined to install and update manually their tools, which is no longer required with web 2.0 software. ASAP allows the user to easily select and compare common, as well as single-cell specific algorithms, and provides an interactive visualization of the results. ASAP supports users in the data interpretation process by its fast speed, running the whole analysis pipeline in minutes, and by providing on-the-go visualization, clustering, differential gene expression analysis, and enrichment functionality. ASAP, to our knowledge, is currently the only tool that combines in-depth analysis features and sophisticated visualization for single-cell data in one unique platform. 2 Materials and methods ASAP is a web-based application written in Ruby on Rails. The core structure is completely independent from any currently hosted web application (which are mostly coded in R/Shiny). This effectively makes the platform autonomous and allows the implementation of any tool independent of its source language. Currently, the server runs codes in R, Python and Java, and this process is invisible for the user, who only requires a web browser without prior installation of any development tool. The current list of methods that is included in ASAP is shown in  Figure 1  and detailed in  Supplementary Table S1 . Current and past versions are visible in the ‘Help’ page of the website (ASAP is versioned according to tool versions).
 Fig. 1 ASAP pipeline. The figure depicts the complete pipeline, including tools, that is implemented in ASAP. The user starts by uploading a count matrix (or a normalized matrix) of gene expression after which either the default pipeline or different filtering algorithms can be selected. After the normalization step, the user can apply different dimensionality reduction methods to visualize the data in 2D or 3D. The user can interactively select samples, or run clustering algorithms to perform differential gene expression analysis. Finally, the selected gene list can be analyzed for enrichment in biological modules or pathways such as the Gene Ontology or KEGG. All tools are referenced in  Supplementary Table S1 The current implementation of ASAP relies on the  delayed::job  framework which automatically creates and queues jobs when the user asks to run a particular method. This allows the application to be perfectly scalable to any IT architecture and prevents major slowdown of the website. Of course, the job execution time scales with the number of users and the host’s computational power. But this will be mainly dependent on the available cores/RAM on the server that hosts ASAP. ASAP has also full compatibility with the last versions of Chrome, Mozilla and Safari. The uploaded user data is protected by an anonymous registration system which keeps the user data private. A sandbox also allows any user to analyze the example project or upload his own data without prior registration. However, the data is destroyed when the user’s session ends. 3 Results As a proof-of-concept, we re-analyzed data from ( Dueck  et al. , 2015 ), in which scRNA-seq was used to study gene expression variation across five mouse cell types, involving 91 cells. We demonstrate that ASAP is capable of replicating the main findings of this study in minutes in straightforward fashion (Supplementary Figs S1–S14). We also made these data available as a demo study on the ASAP front page, which is available without registration. It is important to note that, despite the fact that ASAP is primarily dedicated to single-cell analysis, most of the tools can be employed for bulk RNA-seq analysis as well, which makes the pipeline more versatile and universal. ASAP will be further developed as we commit to adding more functionalities and species handling on a continuous basis. We also plan to add an automatic report generation functionality, aiming to summarize the employed methods together with figures, version, citation and parameters. The database for functional enrichment analysis will remain automatically updated through a CRON job, and more databases will be added to cover links to oncogenes, drugs, as well as additional species. Funding This work has been supported by funds from the Swiss National Science Foundation (#31003A_162735 and #IZLIZ3_156815) and by Institutional support from the EPFL and Human Frontier Science Program LT001032/2013 (to PCS). 
 Conflict of Interest : none declared. Supplementary Material Supplementary Tables Click here for additional data file. Supplementary Figures Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>BioBloom tools: fast, accurate and memory-efficient host species sequence screening using bloom filters</Title>
    <Doi>10.1093/bioinformatics/btu558</Doi>
    <Authors>Chu Justin, Sadeghi Sara, Raymond Anthony, Jackman Shaun D., Nip Ka Ming, Mar Richard, Mohamadi Hamid, Butterfield Yaron S., Robertson A. Gordon, Birol Inanç</Authors>
    <Abstract>Large datasets can be screened for sequences from a specific organism, quickly and with low memory requirements, by a data structure that supports time- and memory-efficient set membership queries. Bloom filters offer such queries but require that false positives be controlled. We present BioBloom Tools, a Bloom filter-based sequence-screening tool that is faster than BWA, Bowtie 2 (popular alignment algorithms) and FACS (a membership query algorithm). It delivers accuracies comparable with these tools, controls false positives and has low memory requirements.</Abstract>
    <Body>1 INTRODUCTION Pipelines that detect pathogens and contamination screen for host sequences so they do not interfere with downstream analysis ( Castellarin  et al. , 2012 ;  Kostic  et al. , 2011 ;  Tang  et al. , 2013 ;  Xu  et al. , 2014 ). The alignment-based algorithms that these pipelines use provide mapping locations that are irrelevant for classification, and thus perform more computation than is needed. To address this, we have developed BioBloom Tools (BBT). BBT uses Bloom filters—probabilistic, constant time access data structures that identify whether elements belong to a set ( Bloom, 1970 ). Bloom filters are similar to hash tables but do not store the elements themselves; instead, they store a fixed number of bits for every element into a common bit array. Thus, they use less memory, but queries to the filter may return false membership (hits) because of hash collisions in the common bit array. The false-positive rate (FPR) resulting from these false hits can be managed by increasing the size of the filter ( Supplementary Material ). Using Bloom filters for sequence categorization was pioneered by the program FACS ( Stranneheim  et al. , 2010 ). Here, we describe a Bloom filter implementation that includes heuristics to control false positives and increase speed. 2 METHODS We first build filters from a set of reference sequences by dividing the sequences into all possible  k -mers (substrings of length  k ). We compare the forward and reverse complement of every  k -mer, and include the alphanumerically smaller sequence in the filter. We calculate the bit signature of a  k -mer by mapping the sequence to a set of integer values using a fixed number of hash functions ( Supplementary Materials ) ( Broder and Mitzenmacher, 2004 ). The bitwise union of the signatures of all the  k -mers constitutes a Bloom filter for the corresponding reference sequences. To test whether a query sequence of length  l  is present in the target reference(s), we use a sliding window of  k -mers. Starting at one end of the query sequence, and shifting one base pair at a time along this sequence, we check each  k -mer against each reference’s Bloom filter. When a  k -mer matches a filter, we incrementally calculate a score:
 s = ∑ i = 1 c ∑ j = 1 a i 1 − 1 ( j + 1 ) l − k 
where  c  is the number of contiguous stretches of adjacent filter-matching  k -mers until the current position in the query, and  a i  is the length of the  i- th stretch. This heuristic penalizes likely false-positive hits. We evaluate  k -mers this way until we reach either a specified score threshold ( s* ) or the end of the query sequence. If at any point we reach  s* , we categorize the query as belonging to the reference, and terminate the process for that query. Further, we use a jumping  k -mer heuristic that skips  k k -mers when a miss is detected after a long series of adjacent hits. This efficiently handles cases in which the query has a single (or a few) base mismatch(es) with the target. 3 BENCHMARKING We compared BBT against two widely used Burrows–Wheeler transform-based alignment tools that have low memory usage and high accuracy—BWA ( Li and Durbin, 2003 ) and Bowtie 2 (BT2; Langmead and Salzberg, 2012)—and against the C++ implementation of FACS ( https://github.com/SciLifeLab/facs ). Tool versions and other details are provided in the  Supplementary Materials . 3.1 Benchmarking on simulated data We used dwgsim ( https://github.com/nh13/DWGSIM ) to generate simulated Illumina reads from human, mouse and  Escherichia coli  reference genomes. For each genome, we generated 1 million 2 × 150 bp paired-end (PE) reads and 1 million 100 bp single-end (SE) reads. We used  E. coli  because it is a common contaminant and is genetically distant from human. With mouse, which is commonly used in xenograft studies, we tested categorization accuracy for species that are closely related genetically. Because FACS does not support PE reads, we used the 100 bp SE reads to compare the false- and true-positive rates (FPR and TPR, respectively) of BBT and FACS. We tested a range of scoring thresholds for both tools. Using a  k -mer size of 25 bp, BBT generally matched or outperformed FACS ( Fig. 1 A and B). We note that, for shorter  k -mers, performance of BBT and FACS algorithms would deteriorate, especially in distinguishing sequences from closely related references. For both tools, longer  k -mers gave lower FPR but also lower maximum TPR ( Supplementary Figs S1 and S2 ), with BBT performing increasingly better than FACS for longer  k -mers.
 Fig. 1. Performance comparisons of BBT against FACS, BWA and BT2. Receiver operator characteristic curves of BBT and FACS using simulated 100 bp SE reads from  Homo sapiens  mixed with ( A )  E.coli  and ( B )  Mus musculus  filtered against an  H.sapiens  Bloom filter using a  k -mer size of 25 bp; ( C ) CPU time benchmark comparing BT2 (for a range of built-in settings), BWA (using aln and mem settings), FACS and BBT, on one lane of human 2 × 150 bp PE Illumina HiSeq 2500 reads To compare BBT and FACS to BWA and BT2, we used 2 × 150 bp PE reads. In our tests, overall, BBT performed comparably with the aligners and outperformed ‘fast’ and ‘very fast’ settings of BT2 in both false-negative rate (FNR) and false-discovery rate (FDR;  Table 1 ).
 Table 1. Benchmarking results using simulated paired end 2 × 150 bp reads Tool and Settings FNR FDR FDR ( H.sapiens) ( M.musculus ) ( E.coli) BT2 very sensitive 1.40 × 10 −5 2.03 × 10 −2 0 BT2 sensitive 7.52 × 10 −4 9.08 × 10 −3 0 BT2 fast 1.26 × 10 −2 5.90 × 10 −3 0 BT2 very fast 1.34 × 10 −2 5.65 × 10 −3 0 BWA aln 3.26 × 10 −3 8.14 × 10 −4 0 BWA mem 0 1.92 × 10 −1 1.00 × 10 −4 FACS 1.22 × 10 −1 9.88 × 10 −3 0 BBT ( s * = 0.1) 8.42 × 10 −3 3.78 × 10 −3 0 Note : All reads were treated as SE reads for FACS. 3.2 Benchmarking on experimental data We used a single lane of 2 × 150 bp PE human DNA reads ( https://basespace.illumina.com/run/716717/2x150-HiSeq-2500-demo-NA12878 ) generated with an Illumina HiSeq 2500 sequencer to benchmark computational performance. For a controlled comparison, we ran at least eight replicates for each tool, and we measured CPU time, with all applications using a single thread. We ran BBT with  s * = 0.1 and compared it with FACS, BWA and BT2, using a range of run modes for the latter two tools. BBT was faster than the fastest aligner/settings combination (BT2 very fast) by at least an order of magnitude ( Fig. 1 C). The mapping rates (categorization rates for BBT and FACS) of each tool were comparable, at 96.69 (BT2 very sensitive), 96.57 (BT2 sensitive), 96.18 (BT2 fast), 95.97 (BT2 very fast), 99.76 (BWA mem), 95.12 (BWA aln), 95.81 (FACS) and 97.27% (BBT). 3.3 Memory usage For categorization, using the human reference and simulated reads, the peak memory usage (GB) for each tool was 3.8 (BBT), 4.8 (FACS), 3.1 (BWA aln), 5.2 (BWA mem) and 3.4 (BT2). These figures are for categorization only and do not include the memory usage for creating the FM-indexes or Bloom filters. Unless slower disk-based methods are used, creating an FM-index takes at least O( n log( n )) bits of memory, where  n  is the size of the reference sequence ( Ferragina  et al. , 2012 ). In contrast, Bloom filter memory usage is the same for the creation and categorization stages, and takes O(- n log( f )) bits of memory, where  f  is the FPR and  n  is the number of input sequences. We created filters using 3.2 GB of memory for both FACS and BBT. Assuming optimal numbers of hash functions are used, filters with the same size should have similar FPRs. However, in practice, we had to use different FPR settings in creating these filters (FPR of 0.5% for FACS and 0.75% for BBT). We note that the tools would differ from theoretical estimates because of implementation-specific calculation differences. Finally, to demonstrate the scalability of BBT, we built a filter for 5182 bacterial sequences (representing 6 × 10 10  unique 25-mers), using 6.8 GB of memory, corresponding to an FPR of 0.75%. 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Modelling haplotypes with respect to reference cohort variation graphs</Title>
    <Doi>10.1093/bioinformatics/btx236</Doi>
    <Authors>Rosen Yohei, Eizenga Jordan, Paten Benedict</Authors>
    <Abstract/>
    <Body>1 Background Statistical modelling of individual haplotypes within population distributions of genetic variation dates back to the  Kingman (1982) n-coalescent . In general, the coalescent and other models describe haplotypes as generated from some structured state space via recombination and mutation events. Although coalescent models are powerful generative tools, their computational complexity is unsuited to inference on chromosome length haplotypes. Therefore, the dominant haplotype likelihood model used for statistical inference is the  Li and Stephens (2003)  model (LS) and its various modifications. LS closely approximates the more exact coalescent models but admits implementations with rapid runtime. Orthogonal to statistical models, another important frontier in genomics is the development of the variation graph, as described in  Paten  et al.  (2014) . This is a structure which encodes the wide variety of variation found in the population, including many types of variation which cannot be represented by conventional models. Variation graphs are a natural structure to represent reference cohorts of haplotypes since they encode haplotypes in a canonical manner: as node sequences embedded in the graph (see Novak  et al. , 2016). 
 Dilthey  et al.  (2015)  demonstrate the benefit of incorporating a graph representation of population information into a model for genome inference. However, their model does not account for haplotype phasing. In this paper, we present the first statistical model for haplotype modelling with respect to graph-embedded populations. We also describe an efficient algorithm for calculating haplotype likelihoods with respect to large reference panels. The algorithm makes significant use of the graph positional Burrows-Wheeler transform (gPBWT) index of haplotypes described by Novak  et al.  (2016). 2 Materials and methods 2.1 Encoding the full set of human variation Haplotypes in the  Kingman (1982) n -coalescent and  Li and Stephens (2003)  models are represented as sequences of values at linearly ordered, non-overlapping binary loci. Some authors model multiallelic loci (for example, single base positions taking on values of  A, C, T, G or gap ) as in  Lunter (2016) , but all assume that the entirety of genetic variation can be expressed by values at linearly ordered loci. However, many types of genetic variation cannot be represented in this manner. Copy number variations, inversions or transpositions of sequence create cyclic paths which cannot be totally ordered. Large population cohorts such as the 1000 Genomes Project Consortium  et al.  (2015) project data contain simple insertions, deletions and substitution at a sufficient density that these variants sometimes overlap or nest into structures not representable by linearly ordered sites. Two examples of this phenomenon from 1000 Genomes data [Phase 3 Variant Call Format file (VCF)] for chromosome 22 are pictured in  Figure 1 . Fig. 1 Two examples of non-linearly orderable loci in a graph of  1000 Genomes  variation data for chromosome 22 which form overlapping or nested sites In order to represent these more challenging types of variation, we use a  variation graph . This is a type of  sequence graph —a mathematical graph in which nodes represent elements of sequence, augmented with 5′ and 3′sides, and edges are drawn between sides if the adjacency of sequence is observed in the population cohort (see  Paten  et al. , 2017 ). Haplotypes are embedded as paths through oriented nodes in the graph. We are able to represent novel recombinations, deletions, copy number variations or other structural events by adding paths with new edges to the graph, and novel inserted sequence by paths through new nodes. 2.2 Adapting the recombination component of LS to graphs The  Li and Stephens (2003)  model (LS) can be described by an HMM with a state space consisting of previously observed haplotypes and observations consisting of the haplotypes’ alleles at loci. Recombinations correspond to transitions between states and mutations are modelled within the emission probabilities. Since variation graphs encode full nucleic acid sequences rather than lists of sites we extend the model to allow recombinations at base-pair resolution rather than just between loci. Let  G  denote a variation graph. Let  S ( G )  be the set of all possible finite paths visiting oriented nodes of  G . A path  h  in  S ( G )  encodes a potential  haplotype . A variation graph posesses an embedded  population reference cohort H  which is a multiset of haplotypes  p ∈ S ( G ) . Given a pair  ( G , H ) , we seek the likelihood  P ( h | G , H )  that  h  arose from haplotypes in  H  via recombinations. Recall that every oriented node of  G  is labelled with a nucleic acid sequence. Therefore, every path  h ∈ S  corresponds to a nucleic acid sequence  s e q ( h )  formed by concatenation of its node labels. We represent recombinations between haplotypes by assembling subsequences of these sequences  s e q ( h )  for  h ∈ H . We call a concatenation of such subsequences a  recombination mosaic . This is pictured in  Figure 2 . Fig. 2 The labelled path shows the recombination mosaic  x  superimposed on the embedded haplotypes  H  in our  1000 Genomes project  chr 22 graph; below,  x  is mapped onto its nucleic acid sequence Fig. 3 A sketch of the flow of information in the likelihood calculation algorithm described. Blue arrows a represent the  rectangular decomposition ,  R · ( · )  are prefix likelihoods We can assign a likelihood to a mosaic  x  by analogy with the recombination model from LS. Assume that nucleotide in  x  has precisely one successor in each  p ∈ H  to which it could recombine. Then, between each base pair, we assign a probability  π r  of recombining to a given other  p ∈ H , and therefore a probability  ( 1 − ( | H | − 1 ) π r )  of not recombining. Write  π c  for  ( 1 − ( | H | − 1 ) π r ) . By the same argument underlying the LS recombination model, we then we have a probability of a given mosaic having arisen from  ( G , H )  through recombinations:
 (1) P ( x | G , H ) = π r R ( x ) π c | x | − R ( x ) 
where  | x |  is the length of  x  in base pairs and  R ( x )  the number of recombinations in  x . We will use this to determine the probability  P ( h | G , H )  for a given  h ∈ S ( G ) , noting that multiple mosaics  x  can correspond to the same node path  h ∈ S ( G ) . Given a haplotype  h ∈ S ( G ) , let  χ ( h )  be the set of all mosaics involving the same path through the graph as  h . The law of total probability gives
 (2) P ( h | G , H ) = ∑ x ∈ χ ( h ) P ( x | G , H ) (3) = ∑ x ∈ χ ( h ) π c | h | − R ( x ) π r R ( x ) = π c | h | ∑ x ∈ χ ( h ) ( π r π c ) R ( x ) 
Let  ρ : = π r π c ; then  P ( h | G , H )  is proportional to a  ρ R ( x ) -weighted enumeration of  x ∈ χ ( h ) . We can extend this model by allowing recombination rate  π ( n )  and effective population size  | H | eff ( n )  to vary across the genome according to node  n ∈ G  in the graph. Varying the effective population size allows the model to remain sensible in regions traversed multiple times by cycle-containing haplotypes. In our basic implementation we will assume that  π ( n )  is constant and  | H | eff ( n ) = | H | ; however varying these parameters does not add to the computational complexity of the model. 2.3 A linear-time dynamic programming for likelihood calculation We wish to calculate the sum  ∑ x ∈ χ ( h ) ρ R ( x )  efficiently. (See (3) above) We will achieve this by traversing the node sequence  h  left-to-right, computing the sum for all prefixes of  h . Write  h b  for the prefix of  h  ending with node  b . 
 Definition  1.  A  subinterval s  of a haplotype  h  is a contiguous subpath of  h . Two subintervals  s 1 , s 2  of haplotypes  h 1 , h 2  are  consistent  if  s 1 = s 2  as paths, however we distinguish them as separate objects. 
 Definition  2.  Given a indices  a , b  of nodes of a haplotype  h ,  S b a  is the set of subintervals  s *  of  p ∈ H  such that there exists a subinterval  s  of  h  which begins with  a , ends with  b  and is consistent with  s * there exists no such subinterval of  p  which begins with  a − 1 , the node before  a  in  h (left-maximality) 
 Definition  3.  For a given prefix  h b  of  h  and a subinterval  s *  of a haplotype  p ∈ H , define the subset  χ ( h ) s * ⊆ χ ( h )  as the set of all mosaics whose rightmost segment arose as a subsequence of  s * . The following result is key to being able to efficiently enumerate mosaics: 
 Claim  1.  If  s 1 , s 2 ∈ S b a  for some  a , then there exists a recombination-count preserving bijection between  χ ( h b ) s 1  and  χ ( h b ) s 2 . 
 Proof . See  Supplementary Material . 
 Corollary  1.  If we define
 (4) R b ( s i ) : = ∑ x ∈ χ ( h b ) s i ρ R ( x ) 
then  R b ( s 1 ) = R b ( s 2 )  if  s 1 , s 2 ∈ S b a  for some  a . Call this shared value  R b ( a ) . 
 Definition  4. 
 A b  is the set of all nodes  a ∈ G  such that  S b a  is nonempty. Using these results, the likelihood  P ( h b | G , H )  of the prefix  h b  ending at index  b  can be written as
 (5) P ( h b | G , H ) = π c | h b | ∑ s i R b ( s i ) = π c | h b | ∑ a ∈ A b | S b a R b ( a ) Let  b − 1  represent the node preceding  b  in  h ; we wish to show that if we know  R b − 1 ( a )  for all  a ∈ A b − 1 , we can calculate  R b ( a )  for all  a ∈ A b  in constant time with respect to  | h | . This can be recognized by inspection of the following linear transformation:
 R b ( a ) = ρ f s ( w , ℓ ) ( A + B ) + 1 a ≠ b ( 1 − ρ ) ( f t ( ℓ ) R b − 1 ( a ) + (6) f s ( w , ℓ ) + f t ( ℓ ) w A ) 
where  w = ∑ a | S b a | ,  f s ( w , ℓ ) : = ( 1 + ( w − 1 ) ρ ) ℓ − 1 ,  f t ( ℓ ) : = ( 1 − ρ ) ℓ − 1 , and  A , B  are the  | A b − 1 | -element sums
 (7) A : = ∑ a ∈ A b − 1 | S b a | R b − 1 ( a ) , (8) B   : = ∑ a ∈ A b − 1 [ | S b − 1 a | − | S b a | ] R b − 1 ( a ) 
Proof that (6) computes  R b ( · )  from  R b − 1 ( · )  is straightforward but lengthy and therefore deferred to the  Supplementary Material . If we assume memoization of the polynomials  f s ( h , ℓ ) , f t ( ℓ ) , and knowledge of  w , ℓ  and all  | S b a | ’s, then all  R b ( a ) ’ s can be calculated together in two shared  | A b − 1 | -element sums (to calculate  A  and  A + B ) followed by a single sum per  R b ( a ) . Therefore, by computing increasing prefixes  h b  of  h , we can compute  P ( h | G , H )  in time complexity which is  O ( n · m )  in  n = | h | , and  m = max b | A b | . The latter quantity is bounded by  | H |  in the worst theoretical case; we will show experimentally that runtime is asymptotically sublinear in  | H | . 2.4 Using the gPBWT to enumerate equivalence classes in linear time The gPBWT index described by Novak  et al.  (2016) is a succinct data structure which allows for linear-time subpath search in a variation graph. This is graph analogue of the positional Burrows Wheeler transform by  Durbin (2014)  which is used in the  Lunter (2016)  fast implementation of the Viterbi algorithm in the LS model. Like other Burrows-Wheeler transform variants, the gPBWT possesses a subsequence search function which returns intervals in a sorted path index. 
 Novak  et al.  (2016)  prove that the gPBWT allows  O ( n )  query of the number of subintervals from a set of graph-embedded paths containing a sequence of length  n . Therefore, for any indices  a , b  in a path  h  we can compute the following quantity in  O ( b − a )  time. 
 Definition  5 .  J b a : =  the number of subpaths in  H  matching  h  between nodes  a  and  b . Since we can cache the search interval used to compute  J b a  from the gPBWT, we can also calculate  J b a  in  O ( 1 )  time given that we have already computed  J b − 1 a . This is important because 
 Claim  2 .  | S b a | = J b a − J b a − 1 Proof. By straightforward manipulation of definitions 2 and 5. And therefore, if we have already calculated  { | S b − 1 a | : a ∈ A b − 1 } , then in order to compute  { | S b a | : a ∈ A b } , we need only perform  | A b − 1 | O ( 1 )  extensions of the gPBWT search intervals used to compute the  | S b − 1 a | ’s and one additional  O ( 1 )  query to compute  | S b b | . Therefore, we can compute all nonzero values  | S b a | , for indices  a ≤ b  of  h , using  | A b − 1 | + 1 O ( 1 )  gPBWT search interval extensions for each node  b ∈ h . This makes the calculation of all such nonzero  | S b a | ’s calculable in  O ( n · m )  time overall, where  n = | h |  and  m = max b | A b | . This result, combined with the results of Section 2.3, show that we can calculate  P ( h | G , H )  in  O ( n · m )  time, for  n = | h |  and  m = max b | A b | . 2.5 Modelling mutations We can assign to two haplotypes  h , h ′  the probability  P m ( h | h ′ )  that  h  arose from  h ′  through a mutation event. As in LS model, we can assume conditional independence properties such that
 (9) P tot ( h | G , H ) = ∑ h ′ ∈ s e q ( G ) P m ( h | h ′ ) P r ( h ′ | G , H ) 
It is reasonable to make the simplifying assumption that  P m ( h | h ′ ) = 0  unless  h ′  differs from  h  exclusively at short, non-overlapping substitutions, indels and cycles since more dramatic mutation events are vanishingly rare. This assumption is implicitly contained in the  n -coalescent and LS models by their inability to model more complex mutations. Detection of all simple sites in the graph traversed by  h  can be achieved in linear time with respect to the length of  h . The number of such paths remains exponential in the number of simple sites. However, our model allows us to perform branch-and-bound type approaches to exploring these paths. This is possible since we can calculate upper bounds for likelihood from either a prefix, or from interval censored haplotypes where we do not specify variants within encapsulated regions in the middle of the path. Furthermore, it is evident from our algorithm that if two paths share the same prefix, then we can reuse the calculation over this prefix. If two paths share the same suffix, in general we only need to recompute the  | S b a |  values for a small number of nodes. This is demonstrated in Section 4.2. 3 Implementation We implemented the algorithms described in C ++, building on the variation graph toolkit  vg  by  Garrison (2016) . This is found in the ‘ haplotypes ’ branch at  https://github.com/yoheirosen/vg . No comparable graph-based haplotype models exist, so we could not provide comparative performance data; absolute performance on a single machine is presented instead. 4 Results 4.1 Runtime for individual haplotype queries We assessed time complexity of our likelihood algorithm using the implementation described above. Tests were run on single threads of an Intel Xeon X7560 running at 2.27 GHz. To assess for time dependence on haplotype length, we measured runtime for queries against a 5008 haplotype graph of human chromosome 22 built from the 1000 Genomes Phase 3 VCF on the hg19 assembly created using  vg  and 1000 Genomes Project Consortium  et al.  (2015) project data. Starting nodes and haplotypes at these nodes were randomly selected, then walked out to specific lengths. In our graph, 1 million nodes correspond, on average, to 16.6 million base pairs. Reported runtimes are for performing both the rectangular decomposition and likelihood calculation steps ( Fig. 4 ). The observed relationship (see  Fig. 4 ) of runtime to haplotype length is consistent with  O ( n )  time complexity with respect to  n = | h | . Fig. 4 Runtime (s) versus haplotype length (nodes) for Chr 22 1000 Genomes data. Line with slope  1.01  and  R 2 = 0.972  was fitted to samples with length &gt;50 000 nodes in the log-log plot. This supports a  O ( n )  time complexity with respect to haplotype length We also assessed the effect of reference cohort size on runtime. Random subsets of the  1000 Genomes  data were made using  vcftools  ( Danecek  et al. , 2011 ) and our graph-building process was repeated. Five replicate subset graphs were made per population size with the exception of the full population graph of 2504 individuals. We observe (see  Fig. 5 ) an asymptotically sublinear relationship between runtime and reference cohort size. Fig. 5 Runtime (s) versus reference cohort size (diploid individuals) for chromosome 22 1000 Genomes data. Line with slope  0.27  and  R 2 = 0.888  was fitted to samples with population size &gt;300 individuals in the log-log plot. This supports an asymptotically sublinear time complexity with respect to reference cohort size 4.2 Time needed to compute the rectangular decomposition of a haplotype formed by a recombination of two previously queried haplotypes The assessments described above are for computing the likelihood of a single haplotype in isolation. However, haplotypes are generally similar along most of their length. It is straightforward to generate rectangular decompositions for all haplotypes  h ∈ H  in the population reference cohort by a branching process, where rectangular decompositions for shared prefixes are calculated only once. This will capture all variants observed in the reference cohort. Haplotypes not in the reference cohort can then be generated through recombinations between the  h ∈ H . If this produces another haplotype also in  H , it suffices to recognize this fact. If not, then given that  h  is formed by a recombination of  h 1  and  h 2 , then  h  must contain some sequence of nodes  c → j  contained in neither  h 1  nor  h 2 . We only need to recalculate  S b a  for  a ≤ j ≤ b . We have implemented methods to recognize these nodes and perform the necessary gPBWT queries to build the rectangular decomposition for  h . The distribution of time taken (in milliseconds) to generate this new rectangular decomposition for randomly chosen  h 1 , h 2  and recombination point is shown in  Figure 6 . Fig. 6 Distribution of times (in milliseconds) required to recompute the rectangular decomposition of a haplotye given that it was formed by recombination of two haplotypes for which rectangular decompositions have been constructed. This graph omits 0.6% of observations which are outliers beyond 1 s of time Mean time is 141 ms, median time 34 ms, first quartile time 12 ms and third quartile time 99 ms. To compute a rectangular decomposition from scratch mean time is 71 160 ms, first quartile time 68 690 ms and third quartile time 73 590 ms. This rapid calculation of rectangular decompositions formed by recombinations of already-queried haplotypes is promising for the feasibility of a mutation model or of sampling the likelihoods of large numbers of haplotypes. Similar methods for the likelihood computation using this rectangular decomposition are a subject of our current research. 4.3 Qualitative assessment of the likelihood function’s ability to reflect rare-in-reference features in reads We used vg to map the 1000 Genomes low coverage read set for individual NA12878 on chromosome 22 against the variation graph described previously. 1 476 977 reads were mapped. Read likelihoods were computed by treating each read as a short haplotype. These likelihoods were normalized to ‘relative log-likelihoods’ by computing their log-ratio against the maximum theoretical likelihood of a sequence of the same length. An arbitrary value of  10 − 9  was used for  π recomb . We define a read to contain  n  ‘novel recombinations’ if it is a subsequence of no haplotype in the reference, but it could be made into one using a minimum of  n  recombination events. We define the prevalence of the rarest variant of a read to be the lowest percentage of haplotypes in the index which pass through any node in the read’s sequence. We segregated our set of mapped reads according to these features. We make three qualitative observations, which can be observed in ( Fig. 7 ). First, the likelihood of a read containing a novel recombination is lower than one without any novel recombinations. Second, this likelihood decreases as novel recombinations increase. Third, the likelihood of a read decreases with decreasing prevalence of its rarest variant. Fig. 7 Left: density plot of relative log-likelihood of reads not containing variants below 5% prevalence or novel recombinations (black line) versus reads containing novel recombinations. Right, density plot of relative log-likelihood of reads not containing variants below 5% prevalence or novel recombinations (black line) versus reads containing variants present at under 5% prevalence and under 1% prevalence A further comparison ( Fig. 8 ) of these same mapped reads against reads which were randomly simulated without regard to haplotype structure shows that the majority of mapped reads from NA12878 score are assigned higher relative log-likelihoods than the majority of randomly simulated reads. Fig. 8 Density plot of relative log-likelihood of mapped reads versus randomly generated simulated haplotypes 5 Conclusions We have introduced a method of describing a haplotype with respect to the sequence it shares with a variation graph-encoded reference cohort. We have extended this into an efficient algorithm for haplotype likelihood calculation based on the gPBWT described by Novak  et al.  (2016). We applied this method to a full-chromosome graph consisting of 5008 haplotypes from the 1000 Genomes data set to show that this algorithm can efficiently model recombination with respect to both long sequences and large reference cohorts. This is an important proof of concept for translating haplotype modelling to the breadth of genetic variant types and structures representable on variation graphs. Our basic algorithm does not directly model mutation, however we describe an extension which does. Making this extension computationally tractable will depend on being able to very rapidly compute likelihoods of sets of similar haplotypes. We demonstrate that our algorithm can be modified to compute rectangular decompositions for haplotypes related by a recombination event in millisecond-range times. We have also devised mathematical methods for recomputing likelihoods of similar haplotypes which take advantage of analogous redundancy properties; however, they have yet to be implemented and tested. However, we anticipate that we will be able to compute likelihoods of large sets of related haplotypes on a time scale which makes modelling mutation feasible. Supplementary Material Supplementary Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Powerful fusion: PSI-BLAST and consensus sequences</Title>
    <Doi>10.1093/bioinformatics/btn384</Doi>
    <Authors>Przybylski Dariusz, Rost Burkhard</Authors>
    <Abstract>Motivation: A typical PSI-BLAST search consists of iterative scanning and alignment of a large sequence database during which a scoring profile is progressively built and refined. Such a profile can also be stored and used to search against a different database of sequences. Using it to search against a database of consensus rather than native sequences is a simple add-on that boosts performance surprisingly well. The improvement comes at a price: we hypothesized that random alignment score statistics would differ between native and consensus sequences. Thus PSI-BLAST-based profile searches against consensus sequences might incorrectly estimate statistical significance of alignment scores. In addition, iterative searches against consensus databases may fail. Here, we addressed these challenges in an attempt to harness the full power of the combination of PSI-BLAST and consensus sequences.</Abstract>
    <Body>1 INTRODUCTION PSI-BLAST achieves a remarkable compromise between speed and quality . Ideally, an alignment method should accurately identify related sequences in today's rapidly growing databases within the shortest possible time. While we want to simultaneously optimize speed and reliability, in practice there is a tradeoff: very accurate alignment methods are relatively slow (e.g. profile–profile alignment algorithms), while very fast methods are far less sensitive than we might wish (e.g. BLAST; Altschul  et al. ,  1990 ). PSI-BLAST (Altschul  et al. ,  1997 ) strikes an excellent compromise between speed and sensitivity. Consensus sequences improve PSI-BLAST performance . Consensus sequences were used early on to improve alignments (Patthy,  1987 ). The initial approaches mimicked profile-sequence alignments (Henikoff and Henikoff,  1997 ; Sonnhammer and Kahn,  1994 ). Many improvements followed (Finn  et al. ,  2006 ; Kahsay  et al. ,  2005 ; Letunic  et al. ,  2006 ; Marchler-Bauer  et al. ,  2002 ; Merkeev and Mironov,  2006 ; Schaffer  et al. ,  1999 ; Schultz  et al. ,  1998 ; Servant  et al. ,  2002 ; Thelen  et al. ,  1999 ). However, none of those methods approached the success of PSI-BLAST. We have recently proposed a simple add-on to PSI-BLAST that substantially improves its performance (Przybylski and Rost,  2007 ). The add-on did not require any code change in PSI-BLAST. It consisted of adding a final step of ‘freezing’ the profile after the standard, iterative search against native sequences and then using it to search a database with the native sequences replaced by their consensus counterparts. This simple add-on improves the performance throughout the entire sensitivity curve. However, it is not clear how the underlying residue composition of database sequences affects the statistics of alignment scores. This is an important issue because users rely on the estimates of statistical significance to judge retrieved alignments. In addition, incorrect scoring might invalidate iterative searches against consensus sequences; a single false alignment in one of the intermediate searches might pollute a scoring profile and thereby all subsequent searches. This study was motivated by the following three assumptions: (1) For a given residue substitution scoring matrix, the statistical significance of alignment scores depends on the residue compositions of aligned sequences. Assume that a particular scoring matrix highly rewards the alignment of tryptophan. This implies that sequences rich in tryptophan will likely generate higher alignment scores than those with average tryptophan content. (2) In general, the composition of consensus sequences differs from that of native sequences. Therefore, the distribution of alignment scores is likely different for consensus and native sequences, at least when using the same scoring matrix for both [such as BLOSUM62 (Henikoff and Henikoff,  1992 ) or the corresponding position-specific scoring matrices]. (3) PSI-BLAST is very popular, well-maintained, and has a great impact on the community of scientists that use sequence alignments. Therefore, it is desirable to improve PSI-BLAST performance without changing its alignment parameters (including scoring matrices and gap scores) with which the community is already familiar. In order to accomplish this, we have asked the following questions: how much do the parameters of alignment score distribution change for various types of consensus sequences? Can PSI-BLAST compensate for compositional variations through its internal composition-based adjustments (Schaffer  et al. ,  2001 )? Or, can we build consensus sequences in a way that renders statistical significance reported by PSI-BLAST as valid? Finally, can we apply PSI-BLAST to iteratively search consensus sequence databases? 2 METHODS 2.1 Generation of consensus sequences We derived the consensus sequences from position-specific scoring matrices (PSSM, also known as scoring profiles) generated by iterative PSI-BLAST (‘blastpgp’) (Altschul  et al. ,  1997 ) searches of the redundancy-reduced UniProt (Apweiler  et al. ,  2004 ) database containing about 1.5 million sequences. The sequence redundancy was reduced with CD-HIT (Li  et al. ,  2001 ) such that pairs of sequences had &lt;80% identical residues (globally). We allowed up to five PSI-BLAST iterations, i.e. the  frozen  profile was computed based on the fourth iteration or the next to the last one for early converging queries. The  E -value threshold for inclusion in PSSMs was set to 0.001 and we increased the maximum number of aligned sequences to 2000 [blastpgp options ‘-j 5 -h 0.001 –v 2000 –b 2000 -Q PSSM(ASCII)’]. Other options were left unchanged, including the default compositional adjustment of alignment score statistics and gap scores of −(11+ k ) for gaps of length  k . The determination of consensus sequences was based on ASCII PSSMs. For a given sequence and a residue position, we looked at the corresponding column of its PSSM and/or the frequency profile also present in the PSI-BLAST output. We explored three alternative ways for computing consensus residues at a given position  i  of a sequence: (1)  MF :  maximal frequency —the consensus residue  j  had the highest occurrence frequency  f ij  in the profile column, (2)  MET: maximal relative entropy term —we chose the residue  j  with the highest relative entropy term  f ij ln ( f ij / b j ) with respect to the background frequency  b j , (3)  MR: maximal ratio of frequencies —we chose the residue with the highest frequency ratio  f ij / b j . In addition, we studied full ( MF-full ,  MET-full ,  MR-full ) and partial ( MF-partial ,  MET-partial ,  MR-partial ) versions of consensus sequences. For the ( 1 ) full consensus sequences, we computed the consensus residue at each sequence position, and for the (2) partial consensus, we computed the consensus in a constrained way, e.g. only for the more  informative  positions. The more informative positions were those having profile frequency columns with the relative entropy equal or above 0.6 (as reported in the PSI-BLAST output). 2.2 Alignments All of the alignments (except those used to estimate the asymptotic values of the alignment score distribution parameters) were generated using PSI-BLAST version 2.2.15. The  frozen  scoring profiles (PSSMs) for the non-iterative profile-sequence alignments were generated in the same way as those used for generation of consensus sequences, except that a file containing the binary version of a PSSM was also stored [blastpgp option ‘-C PSSM(binary)’]. Those binary PSSMs were used for a final (non-iterative) PSI-BLAST search against the appropriate consensus or native sequence databases [blastpgp options: ‘-j 1 -R PSSM(binary)’]. For the non-profile-based sequence–sequence alignments the default BLOSUM62 (Henikoff and Henikoff,  1992 ) scoring matrix was used (blastpgp options: ‘-j 1’). When studying iterative searches against consensus sequence databases, we compared the performance for various number of iterations. The consensus version of the redundancy-reduced UniProt database used in iterative consensus searches was computed over a period of a few months using spare CPUs of a large computing cluster. 2.3 Evaluation of similarity search capability We evaluated the ability to identify remotely related proteins using SCOP (Murzin  et al. ,  1995 ) (release 1.69). We used the usual, descending hierarchy levels of ‘fold’, ‘superfamily’ and ‘family’ to define true and false relationships. Our positives consisted of pairs of protein domains from the same SCOP superfamily, but different SCOP families (i.e. the relatively easy pairs from the same family were not counted). However, for the more sensitive iterative searches against consensus sequences, we also counted pairs from the same SCOP-fold as positives. The negatives belonged to different SCOP-folds. We removed domains with: discontinuous sequences, missing coordinates in their three-dimensional structures, NMR and low-resolution structures (&gt;2.5 Å), and the short ones (&lt;50 residues). Next, we reduced the sequence redundancy of the set so that no pair of sequences could be aligned by BLAST with  E -values better than 10 −3  (when computed on UniProt database of ∼2 000 000 sequences), or at levels of sequence identity and alignment length that corresponded to homology-derived structures of proteins (HSSP)-values above 0 (Rost,  1999 ; Sander and Schneider,  1991 ) (whichever of the two criteria applied). This yielded a dataset of 2476 sequences for which we applied the all-against-all test. 2.4 Score statistics PSI-BLAST provides statistical significance of alignment scores in terms of expectation values ( E -values) that are given by:
 (1) 
where  m  and  n  are the effective lengths (Altschul and Gish,  1996 ) of aligned sequences (query and database),  score  is a raw alignment score (as given by the values in scoring matrix and gap penalties), and  K  and λ are the parameters of the score distribution that depend on a scoring system and the residue composition of aligned sequences. Note that the computation of the  E -value primarily depends upon a proper estimate of λ and much less so on that for  K . 2.5 Determining parameters of alignment score distributions The problem of estimating the statistical significance of alignment scores has been studied extensively (Altschul and Gish,  1996 ; Karlin and Altschul,  1990 ; Mott,  1992 ; Waterman and Vingron,  1994 ). We computed λ and  K  parameters [Equation ( 1 )] with our implementation of the ‘island’ approach (Altschul  et al. ,  2001 ; Olsen  et al. ,  1999 ) for a case of scoring profiles. This approach is appropriate as the primary methods studied in this article rely on searching databases of consensus sequences with precomputed PSSMs. We have also estimated the score distribution parameters for profile-based searches against native sequences to relate our results to the earlier studies. First, we obtained the initial PSSMs for hundreds of thousands of randomly selected UniProt sequences. Most of them were too short to study the score distribution in the asymptotic limit of very long sequences. Therefore, we concatenated them in random order and then cut them into final long PSSMs, each composed of 7000 columns. We ended up with 75 000 of such long PSSMs. To generate corresponding long random consensus sequences, we first computed consensus sequences from each of the long PSSMs and used them to compute consensus residue background frequencies. Those background frequencies were used to generate random sequences used for studying asymptotic alignment score distribution parameters. For partial consensus sequences, we computed two separate sets of backgrounds—inside and outside of consensus regions and used them accordingly for generation of random partial consensus sequences (with the informative positions indicated by the original PSSM relative entropy values at each sequence position). 2.6 Studying the compositional adjustment of alignment score statistic in PSI-BLAST The newer versions of PSI-BLAST can adjust alignment score statistics based on varying residue compositions of query and database sequences (‘-t’ option in PSI-BLAST). In particular, we looked at the performance of the default adjustment implemented in the 2.2.15 version of the software. We have generated random sequence databases based on the native and consensus background residue frequencies. The numbers of random sequences and their sizes were the same as those found in the non-redundant UniProt database. We queried those databases with about 20 000 randomly chosen native sequences and the corresponding PSI-BLAST profiles (PSSMs). We recorded the average cumulative numbers of alignments per query that had  E -values better than a given threshold value. 3 RESULTS AND DISCUSSION 3.1 Alignment score parameters depended on consensus type The variation of λ with the alignment score [Equation ( 1 )] for gapped alignments has been described before (Altschul  et al. ,  2001 ). Low-scoring alignments usually have fewer gaps and their score distribution differs from those obtained for high-scoring alignments with gaps. Here, we have focused mostly on asymptotic values of λ for high scores because they correspond to statistically significant alignments originating from searches of large sequence databases. In particular, we looked at λ for PSSMs generated with five iterations of PSI-BLAST. We observed that λ depended on the sequence types ( Fig. 1 ). Computing consensus residues for the full sequence produced largest changes in λ (open symbols in  Fig. 1 , i.e.  MR-full ,  MET-full  and  MF-full ). For each one of them, the asymptotic value of λ was less than 0.2 (more data points would be needed to establish a precise limit). The value of λ for the profile-sequence alignments of the native sequences was about 0.255 ( Fig. 1 ; green squares). This is rather close to a value of 0.267 previously established for the sequence-sequence alignments with the BLOSUM62 scoring matrix (Altschul  et al. ,  2001 ). For the partial consensus sequences, λ appeared to follow the value obtained for the native sequences (filled symbols in  Fig. 1 , i.e.  MF-partial  and  MET-partial ). To some extent this result is not surprising because partial consensus substitutions are more restricted than the full ones, i.e. change fewer residues ( Table 1 ). As a result, we established that one could use PSI-BLAST without any modifications to perform profile-based search against partial consensus sequence databases and maintain proper estimates of  E -values.
 Fig. 1. Estimating λ. Score distribution parameter λ [Equation ( 1 ),  y -axis] varies with alignment scores ( x -axis). In practice, we are interested in the asymptotic value of λ for higher scores. Full consensus sequences affected λ significantly (open symbols) when compared to native sequences (green squares). In contrast, partial consensus did not significantly affect λ (filled black and blue symbols). Red error bars estimate the SD (for clarity only shown for native sequences). Note that high alignment scores were attained by few alignments. 
 Table 1. Pairwise residue identities of native and consensus sequences Native Full consensus Partial consensus native MR MF MET MR MF MET Native native 100 Full consensus MR 65 100 MF 54 76 100 MET 51 80 90 100 Partial consensus MR 86 79 64 63 100 MF 83 72 71 67 93 100 MET 82 73 69 69 94 98 100 Shown are average percentages of pairwise residue identities between different types of sequences of a test set. 
 We have also estimated the location parameter  K  used for computing  E -values [Equation ( 1 )]. For example, we found it to be ∼0.015 for the full consensus sequences ( MF-full ), 0.030 for the partial consensus sequences  (MF-partial ) and 0.032 for the native sequences. 3.2 Search performance similar for all consensus types Do some types of consensus sequences retrieve related sequences from a database better than others? For each type of consensus, we ordered all query alignments by PSI-BLAST  E -values. Next, we computed the cumulative numbers of true positive relations (same SCOP superfamily but different family) for increasing cumulative numbers of false positive pairs (different SCOP-folds). For any number of false positives (i.e. at any error rate), the profile-sequence searches against the databases of full consensus sequences yielded most true positives ( Fig 2 ; top three curves:  MET-full ,  MF-full ,  MR-full ). Interestingly, it did not matter much how we compiled the full consensus (three top lines with open symbols in  Fig. 2  are almost indistinguishable). The profile-based searches against partial consensus sequences (only most informative positions replaced by consensus) were somewhat less efficient, especially when more false hits were allowed ( Fig 2 ;  MET-partial ). Nevertheless, they were significantly better than standard profile-sequence searches of PSI-BLAST ( Fig 2 ;  native ). For comparison, we also included the performance of sequence–sequence searches with pairwise BLAST against the native and consensus sequences ( Fig. 2 ;  MET-full-1, MET-partial-1, native-1 ). As expected, pairwise searches fared much worse than profile-sequence searches. The relative performance difference between the full and partial consensus sequences appeared larger for the sequence–sequence ( Fig. 2 ;  MET-full-1, MET-partial-1 ) than for profile-sequence searches.
 Fig. 2. Comparison of search performance. All-against-all alignments of the test set sequences were ordered by their PSI-BLAST  E -values. The cumulative numbers of non-trivial true relations (same SCOP superfamily but different SCOP family) were plotted against the cumulative numbers of false positives (different SCOP-folds). The profile-sequence searches against the full consensus sequences performed best (top three curves:  MET-full ,  MF-full, MR-full ). Profile-sequence searches against partial consensus sequences were slightly less efficient ( MET-partial ), but they were still significantly better than standard profile-sequence ( native ). Sequence–sequence searches (one cycle of PSI-BLAST with BLOSUM62 matrix) were clearly inferior ( MET-full-1, MET-partial-1, native-1 ). 3.3 Composition of consensus sequences varied The search performance appeared not to differ between various types of full consensus sequences, although their average residue compositions were quite different ( Fig. 3 A). The consensus based on the maximum ratio of target and background frequencies ( MR-full ) weighed more heavily rare residues such as tryptophane (W). The consensus based on the most frequent residue ( MF-full ) weighed more heavily the more ubiquitous ones such as leucine (L). Finally, the consensus based on relative entropy ( MET-full ) produced the composition that appeared to be more balanced ( Fig. 3 A, blue bars). The average percent differences in residue identity (and SDs) between native and full consensus sequences were: 65 (±14) for  MR-full , 54 (±16) for  MF-full  and 51 (±17) for  MET-full  consensus sequences. The partial consensus calculations resulted in average compositions that were much closer to the native ones ( Fig. 3 B). The corresponding residue identities with respect to native sequences were: 86 (±7)%, 83 (±8)% and 82 (±8)%. Thus, the consensus calculation (MR) that changed sequences the least in terms of the average residue identity has changed the score distribution parameters the most. Other pairwise residue identities are given in  Table 1 . All calculations were performed on our non-redundant SCOP test set.
 Fig. 3. Comparison of residue compositions. We computed the background residue compositions for consensus and native sequences in our test set. Full consensus sequences ( A ) differed more from native than partial consensus sequences ( B ). Choosing the consensus residue corresponding to the highest relative entropy term (blue bars) resulted, on average in smaller deviations from the native composition. 3.4 PSI-BLAST compositional adjustments were partially successful When compositions of aligned sequences differ from a standard one, PSI-BLAST can attempt to correct the estimates of statistical significance accordingly (Schaffer  et al. ,  2001 ; Yu and Altschul,  2005 ). We studied how well the default adjustments perform on consensus sequences (non-default adjustments are not available for profile-based searches). Using PSI-BLAST profiles we searched against the consensus and native sequence databases ( Section 2 ). For the comparison, we also searched with the BLOSUM62 substitution matrix (standard, non-profile BLAST search). In the latter case, the estimates of statistical significance were not very sensitive to compositional differences and the statistic adjustments worked well ( Table 2 , observed and expected counts similar; adjustments were conservative). However, for the profile-based searches the compositional differences played a significant role, particularly for the full consensus sequences (especially pronounced for  MR-full ,  Table 3 ). The compositional adjustment of scores attempted by PSI-BLAST (-t option set to 1) failed to satisfactorily correct for the differences. In contrast, the  E -value estimates were good for partial consensus sequences. For both native and partial consensus sequences, the compositional score adjustment sometimes resulted in slightly increased numbers of random alignments with significant  E -values.
 Table 2. Accuracy of BLAST  E -values a Observed Native Full consensus Partial consensus Expected native native-adj. MR MR-adj. MF MF-adj. MET MET-adj. MR MR-adj. MF MF-adj. MET MET-adj. 0.001 0.0014 0.0010 0.0010 0.0002 0.0007 0.0006 0.0009 0.0003 0.0014 0.0008 0.0018 0.0008 0.0012 0.0009 0.01 0.010 0.007 0.006 0.004 0.006 0.005 0.008 0.004 0.011 0.006 0.012 0.005 0.011 0.006 0.1 0.09 0.07 0.07 0.04 0.03 0.06 0.08 0.06 0.09 0.07 0.11 0.07 0.10 0.07 1 0.9 0.7 0.7 0.5 0.2 0.7 0.9 0.6 0.9 0.7 1.1 0.7 1.0 0.7 10 9 7 7 6 18 7 9 7 9 8 11 8 10 8 a Shown are the expected and observed numbers of random alignment scores per query for ∼20 000 sequence queries on randomly generated databases (of UniProt size) of native and consensus sequences. Appendix ‘-adj.’ indicates results obtained with the use of compositional adjustment of  E -values with BLAST option ‘-t’ set to 1. 
 Table 3. Accuracy of PSI-BLAST a   E -values b Observed Native Full consensus Partial consensus Expected native native-adj. MR MR-adj. MF MF-adj. MET MET-adj. MR MR-adj. MF MF-adj. MET MET-adj. 0.001 0.0013 0.0033 66.4912 1.6637 0.0024 0.0008 0.0215 0.0093 0.0053 0.0040 0.0014 0.0028 0.0013 0.0038 0.01 0.008 0.020 98.880 4.3162 0.018 0.028 0.086 0.036 0.022 0.026 0.010 0.021 0.009 0.022 0.1 0.08 0.18 159.83 12.83 0.170 0.22 0.51 0.26 0.17 0.17 0.10 0.20 0.10 0.20 1 0.8 1.6 259.1 34.9 1.6 1.8 3.2 2.1 1.4 1.4 1.0 1.7 1.0 1.7 10 8 13 405 102 14 14 22 16 12 12 9 13 10 14 a Shown are the expected and observed numbers of random alignment scores per query for a set of about 20 000 profile (PSSM) queries on randomly generated databases (of UniProt size) of native and consensus sequences. Appendix ‘-adj.’ indicates results obtained with a use of compositional adjustment of E-values with PSI-BLAST option ‘-t’ set to 1. b PSI-BLAST search was restarted from a stored profile. 
 3.5 Little additional CPU needed for add-on In this study, we used separate databases for the iterative derivation of PSSMs (non-redundant UniProt) and for the final search and alignment against consensus sequences. On average, the entire iterative PSI-BLAST search took about 10 min per query (about 2 min per iteration on a single 3.2 GHz CPU with 2 GB of RAM using query sequences with average length of 415 residues). The additional time consumed by the add-on to search against a consensus sequence database of the same size depended on the sequence types. It took about 7 min to search  MR-full  and 4.5 min for  MF-full  consensus sequence databases. In the case of partial consensus it took about 2.5 min to search the  MR-partial  and about 2.2 min for  MF-partial  (compared to about 2 min needed to search native one with PSI-BLAST profile). 3.6 Iterative searches against consensus sequences yielded further improvements We made the first attempt at analyzing iterative PSI-BLAST searches against consensus sequence databases. For this analysis, we pushed the envelope by running up to 20 iterations. We counted hits belonging to the same SCOP-fold but to different families as positives to reach deeper into remote protein–domain relationships. The iterative PSI-BLAST searches against the native sequence database resulted in near saturation of performance at about 10 iterations. Only a small improvement was observed in the subsequent 10 iterations ( Fig. 4 ; top two green lines). The iterative searches against consensus sequences ( MF-full ) produced significantly more true hits with just three iterations. Five consensus iterations produced almost twice as many true hits as the native PSI-BLAST search produced with 20. For comparison, we showed the results of the profile-sequence search (profile obtained from 10 iterations of PSI-BLAST on a native database) against a final database of consensus sequences ( Fig. 4 ; blue line, mixed). These results remain to be compared to the performance of profile–profile methods (Bujnicki  et al. ,  2001 ; Fischer  et al. ,  2003 ).
 Fig. 4. Iterative PSI-BLAST searches against native and consensus sequences. Iterative PSI-BLAST searches and PSSM refinements on native sequence database (green lines) resulted in near saturation of performance at about 10 iterations (top two green lines). The corresponding searches on the database of consensus sequences (black lines) found significantly more true hits (same SCOP-fold but different family) with just three iterations (black triangles), while five iterations (black circles) retrieved almost twice as many true hits as the maximum for the native PSI-BLAST. For comparison, a result of the  frozen  profile-based search against a final database of consensus sequences ( MF-full ) is presented (blue line). 4 CONCLUSIONS PSI-BLAST is an excellent, well-known, well-maintained and trusted resource for searching and aligning sequence databases. A simple add-on consisting of searching with a PSI-BLAST generated scoring profile against a database of consensus sequences significantly improved the performance in finding related sequences. Here, we specified in detail how different strategies of compiling consensus residues affected the estimates of statistical significance and performance. Profile-based PSI-BLAST searches against full consensus sequences improved the most over searches against native sequences. However, they sometimes suffered from problems in the estimates of statistical significance. The partial consensus sequences improved significantly over native sequences without sacrificing estimates of statistical significance. Our initial results for iterative searches against consensus sequences were very promising: a lower number of iterations used less CPU overall and yielded about twice as many correct hits at the same error rates as standard PSI-BLAST searches did. Hence, the fusion of PSI-BLAST and consensus sequences promises another leap in database searches. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>A new statistical framework to assess structural alignment quality using information compression</Title>
    <Doi>10.1093/bioinformatics/btu460</Doi>
    <Authors>Collier James H., Allison Lloyd, Lesk Arthur M., Garcia de la Banda Maria, Konagurthu Arun S.</Authors>
    <Abstract>Motivation: Progress in protein biology depends on the reliability of results from a handful of computational techniques, structural alignments being one. Recent reviews have highlighted substantial inconsistencies and differences between alignment results generated by the ever-growing stock of structural alignment programs. The lack of consensus on how the quality of structural alignments must be assessed has been identified as the main cause for the observed differences. Current methods assess structural alignment quality by constructing a scoring function that attempts to balance conflicting criteria, mainly alignment coverage and fidelity of structures under superposition. This traditional approach to measuring alignment quality, the subject of considerable literature, has failed to solve the problem. Further development along the same lines is unlikely to rectify the current deficiencies in the field.</Abstract>
    <Body>1 INTRODUCTION A protein structural alignment is an assignment of residue–residue correspondences between the amino acids of two or more proteins, based on their 3D structure. Protein structural alignments support basic and applied research in molecular biology. For example, they reveal how protein families evolve, identify patterns of conservation in amino acid sequences that fold into similar structures, facilitate comparative modelling of structures from sequence and guide experimental solutions to structures using crystallographic molecular replacement ( Konagurthu  et al. , 2006 ). The last four decades have seen the development of many methods aimed at generating biologically meaningful structural alignments. While the number of new methods is estimated to be doubling roughly every five years ( Hasegawa and Holm, 2009 ), several comparative studies have observed many inconsistencies and paradoxes when comparing the alignments generated by existing methods. Noteworthy among these studies are those by Michael Levitt ( Kolodny  et al. , 2005 ), Liisa Holm ( Hasegawa and Holm, 2009 ) and Manfred Sippl ( Sippl and Wiederstein, 2008 ;  Slater  et al. , 2013 ) and colleagues. A common theme emerging from all these studies is the need for a systematic framework to asses the quality of structural alignments. While a handful of quantitatively rigorous statistical models for structure comparison have been proposed for this, there is no consensus regarding their usefulness. This is in stark contrast to the state-of-the-art in the closely related problem of aligning protein  sequences , where many rigorous statistical models have been proposed to quantitatively assess sequence alignment quality ( Allison  et al. , 1992 ;  Altschul, 1991 ;  Karlin and Altschul, 1990 ). This has, in turn, helped standardize the task of measuring sequence alignment quality and, thus, the task of generating meaningful sequence alignments. In this work, we begin by examining the foundations of how structural alignments are currently assessed. Guided by good biological insights, current structural aligners use a  scoring function  to  quantify  the structural alignment quality. This has traditionally been achieved by combining the contributions of a small number of important criteria into an easy-to-compute scoring function. [For a comprehensive list of commonly used scoring functions, see  Hasegawa and Holm (2009) ]. Overwhelmingly, the two key criteria that various current measures use are  coverage  and  fidelity . Typically, coverage measures the number of correspondences (or equivalences) in an alignment and, in some cases, also considers the number of gaps. Fidelity, measures how similarly positioned the aligned residues are. This is commonly (but not always) based on the root-mean-square deviation (RMSD) computed after the best rigid-body transformation of corresponding residues is found. To search for the  best  structural alignment, the goal of the aligners is to simultaneously maximize coverage and fidelity. However, these two objectives are in direct conflict with each other. We observe that most of the current proliferation of structural alignment scoring functions arise from attempts to reconcile this conflict, that is, existing scoring functions differ mainly in how they combine these two criteria. As the reviews show, existing scoring functions do not generate consistent results, even when aligning structures that have only moderately diverged in evolution ( Hasegawa and Holm, 2009 ;  Kolodny  et al. , 2005 ;  Slater  et al. , 2013 ). Because this traditional approach of  formulating  a scoring function has been explored extensively over the last four decades, further development along the same lines is unlikely to provide any major breakthrough. Therefore, this field will stand to benefit by departing from the traditional approaches and exploring radically new ones. This paper is a step in this direction. Structural alignment as an inductive inference problem.  The goal of inductive inference is to propose a  theory  (or  hypothesis ) that is able to best explain the observed data. Structural alignment can thus be seen an instance of the general class of inference problems. In this context, an alignment (i.e. residue–residue correspondence) is a hypothesis that attempts to  explain  the residue–residue relationships between two protein structures, whose observed data is the ( x ,  y ,  z ) coordinates of the structures. In general, any hypothesis has a certain (descriptive)  complexity . A complex hypothesis with more free parameters can predict (or  fit ,  explain ) a greater variety of observed data than a simpler hypothesis. Therefore, in order to choose the best hypothesis for any inference problem, one is confronted with a  trade-off  between hypothesis complexity and its fit with the observations. For structural alignments, this trade-off is related to the conflict between coverage and fidelity. Coverage (in various forms handled in the current scoring functions) is a crude  approximation  of the (alignment) hypothesis complexity. Similarly, the fidelity (or goodness of fit with the observed data) of a structural alignment is  approximated  using RMSD of superposition or using some distance measure. These rudimentary approximations cause the existing scoring functions to introduce several tunable parameters in an attempt to balance the contributions between coverage and fidelity of structural alignments. This has been a major source of the inconsistencies observed in alignments. The field of statistical learning and inference provides rigorous approaches to address this trade-off systematically. In the early 1960s, several landmark papers proposed links between inductive inference and information theory ( Kolmogorov, 1965 ;  Solomonoff, 1960 ;  Wallace and Boulton, 1968 ). The Minimum Message Length (MML) principle ( Wallace and Boulton, 1968 ) provided the first practical information-theoretic criterion for hypothesis selection based on observations. It is used here to rigorously assess structural alignment quality and reliably differentiate between competing alignments. Structural alignment quality and lossless information compression.  The pioneering work of Claude E. Shannon ( Shannon, 1948 ) provides the means to quantify information: the length of the  shortest code  required to transmit,  losslessly , an observed event. This can be understood as the length of the shortest message needed to communicate the event losslessly between an imaginary  sender  (Alice) and  receiver  (Bob). In this context, the structural alignment problem can be rationalized as a communication process between Alice and Bob, where Alice has access to the ( x ,  y ,  z ) coordinates of two protein structures and she wants to encode and transmit this information to Bob losslessly. Two possible scenarios then arise: (i) If the two are  unrelated  to each other structurally, Alice cannot do better than to encode and transmit the information of the two structures  independently , one after another. That is, knowledge of one structure (called  reference , or  S ) does not provide information about the other (called  target , or  T ) and, thus, knowledge of  S  cannot be used to compress  T . This form of independent transmission is termed here as the  null model  message. (ii) On the other hand, if the two structures are structurally  related  (i.e. there is a meaningful alignment between the two), knowledge of  S  reveals information about  T . The more similar the structures, the more information one reveals about the other. Alice can use this similarity to  compress  and transmit the information of the target structure using the information of the reference. For Bob to decode the information of the target losslessly (i.e. to the precision with which Alice sees it), he will require the structural information of the reference structure  plus  the information of its proposed relationship (i.e. the structural alignment) with the target. This will allow Alice to encode the target more concisely than stating the target structure using a null model. We call this form of transmission, the  alignment model  message (to contrast it with the null model message, where the structures are transmitted independently). We note that this information-theoretic framework for structural alignment is  intuitive . If the proposed alignment relationship is a poor one, then the encoded  alignment model  message will be inefficient (i.e. long). Alternatively, if the alignment relationship is a good one, then the transmission of the target becomes efficient (i.e. short). Therefore, the total message length of the lossless transmission of coordinate information (using an alignment hypothesis) forms an excellent measure to assess structural alignment quality. It follows that  the best alignment is the one with the shortest total message length of lossless transmission . While we have intuitively rationalized this framework as a communication process, this message paradigm is also backed by mathematical rigour. Formally, let  A  denote some alignment between structural coordinates  S  and  T . Using the product rule of probability over three events  A ,  S  and  T  we have:
 (1) P ( A &amp; S &amp; T ) = P ( A ) × P ( S | A ) × P ( T | S &amp; A ) = P ( A ) × P ( S ) × P ( T | S &amp; A ) 
where  P ( A &amp; S &amp; T )  gives the joint probability of alignment  A  for structures  S  and  T ,  P ( A )  the prior probability of the alignment,  P ( T | S &amp; A )  the likelihood of  T  given  S  and  A . Note,  P ( S | A )  is  P ( S ) because  S  and  A  are assumed to be independent. Shannon’s mathematical theory of communication ( Shannon, 1948 ) gives the relationship between the shortest message length  I ( E ) to communicate losslessly any observation  E , and its probability  P ( E ) as  I ( E ) = − log ⁡ ( P ( E ) ) . Technically,  I ( E ) denotes the  Shannon information content  of  E . Restating  equation 1  in terms of information content, we obtain:
 (2) I ( A &amp; S &amp; T ) = I ( A ) + I ( S ) + I ( T | S &amp; A ) 
where transmitting the information of the reference structure  S  takes  I ( S ) bits, transmitting the alignment information takes  I ( A )  bits and transmitting the information of the target structure  T  using  A  and  S  takes  I ( T | S &amp; A )  bits. Our message length measure has the following three key properties, which are not achieved by previous scoring functions:
 The difference between the lengths of the messages needed to transmit the structures  S  and  T  using any two alignments, gives their log-odds posterior ratio.
 (3) I ( A 1 
 &amp; S 
 &amp; T ) − I ( A 2 
 &amp; S 
 &amp; T ) = log ⁡ ( P ( A 2 
 &amp; S 
 &amp; T ) P ( A 1 
 &amp; S 
 &amp; T ) ) = log ⁡ ( P ( S 
 &amp; T ) P ( A 2 | S 
 &amp; T ) P ( S 
 &amp; T ) P ( A 1 | S 
 &amp; T ) ) = log ⁡ ( P ( A 2 | S 
 &amp; T ) P ( A 1 | S 
 &amp; T ) ) 
As a result, any two competing alignment hypotheses  A 1  and  A 2  can now be compared based on their message lengths. Therefore, the best alignment hypothesis  A *  is the one that results in the shortest message length value of  I ( A * &amp; S &amp; T ). Our measure permits a  natural null hypothesis test  where the statistical significance of any proposed alignment hypothesis can be estimated. Any alignment hypothesis  A  whose message length  I ( A &amp; S &amp; T )  is worse (longer) than that of the null model message,  I ( S &amp; T ) = I ( S ) + I ( T ) ,  must be rejected . This measure provides an objective, formal trade-off between the complexity of the alignment ( I ( A ) ) and the fidelity of the structures given the proposed alignment ( I ( T | S &amp; A ) ). Unlike previous attempts, these terms are not  ad hoc  approximations, as they represent rigorous estimations of Shannon information content based on lossless encoding and compression. 
 2 METHODS 2.1 Computation of the null model message length The null model message corresponds to the transmission of protein coordinates without an alignment hypothesis. (In this work, we consider only the C α  coordinates.) We have previously defined [for a completely different problem ( Konagurthu  et al. , 2012 )] a null model encoding of coordinates along a protein chain. We will briefly summarize this approach, as elements of this encoding are used and developed further in our current work. The null model encoding relies on the observation that the distance between successive  C α  atoms in a protein chain is highly constrained to  3.8 ± 0.2  (s.d.) Å. For a chain of coordinates  { p 1 , p 2 , … , p n } , any coordinate  p i + 1  can be transmitted given the previous  p i , by first transmitting the distance  r i  between  p i  and  p i + 1  using a normal distribution  N ( r ; μ , σ )  stated to  ϵ = 0.001  Å accuracy, with  μ = 3.8  Å and  σ = ± 0.2  Å. ( ϵ = 0.001  reflects the precision of  statement  of coordinate data, which is three places after the decimal point as reported in the protein data bank.). We represent the length of this encoding as  I ( r i ) . With this information transmitted, Bob now knows that  p i + 1  lies on a sphere of radius  r i  centred at  p i , but does not yet know where exactly on the sphere it is. Assuming that  p i + 1  is distributed uniformly over the surface of the sphere, transmitter Alice can discretize the sphere’s surface into cells, each of area  ϵ 2 . Using this discretization,  p i + 1  can be transmitted as cell number  c i + 1 . The numbering convention of the discretization is in the shared codebook. With the knowledge of  p i ,  r i  and  c i , Bob can reconstruct  p i + 1  to the stated accuracy. Stating the cell number takes  I ( c i ) = − log ⁡ 2 ( ϵ 2 4 π r i 2 ) = log ⁡ 2 ( 4 π r i 2 ) − 2 log ⁡ 2 ϵ  bits. When sending a chain of  C α  coordinates  { p 1 , p 2 , … , p n }  over a null model message, we assume that  p 1  is the origin and, hence, does not need to be transmitted as part of the message. Even if  p 1  is not assumed to be the origin, its encoding will add a fixed one-time cost to the message length. Thus, to transmit the chain of points  p 1 , p 2 , … , p n  over the null model, Alice needs to send to Bob the number  n  of  C α  atoms in the chain, followed by incrementally transmitting (using the method above)  p 2  given  p 1 ,  p 3  given  p 2  and so on, until all coordinates are transmitted. Alice can transmit the number  n  over a integer distribution.  Wallace and Patrick (1993)  gave an efficient code ( I integer ( n ) ) for transmitting any positive integer  n  &gt; 0. Therefore, the total message length required to send all the coordinates over the null model message takes  I null ( p 1 , … , p n ) = I integer ( n ) + ∑ i = 1 n − 1 ( I ( r i ) + I ( c i ) )  bits. Using the above, the coordinates of structures  S = { S 1 , S 2 , … , S | S | }  and  T = { T 1 , T 2 , … , T | T | }  are sent as independent chains of coordinates over the null model, taking  I null ( S &amp; T ) = I null ( S ) + I null ( T )  bits. 2.2 Computation of the alignment model message length Equation 2  gives the amount of information required to transmit the coordinates of structures  S  and  T  using the alignment hypothesis  A . To estimate this, the explanation message involves transmitting: the coordinates of  S , the residue–residue correspondences proposed by the alignment in  A  and, finally, the coordinates of  T  using the information of  S  and  A . 2.2.1 Transmitting the coordinates of S This is achieved by sending the coordinates of  S = { S 1 , S 2 , … , S | S | }  over the null model. Therefore,  I ( S ) = I null ( S 1 , S 2 , … , S | S | )  bits. 2.2.2 Transmitting the correspondences in  A Any alignment can be described as a string switching between three states:  match  (‘m’),  insertion  (‘i’) and  deletion  (‘d’) states. This alignment string can be transmitted losslessly using a first-order Markov model. To transmit an alignment over a 3-state Markov chain, we use an approach similar to the  adaptive encoding  method used by  Wallace and Boulton (1969)  over a multinomial (n-state) distribution. The adaptive encoding here requires maintaining nine running counters, one for each possible transition probability, all initialized to 1. Traversing the alignment string left to right, for every observed transition,  Alice  estimates its probability by dividing the current value of the corresponding transition counter by the sum of all counters from previous to any state. After the probability is estimated, Alice encodes the current alignment state using this probability and then increments the corresponding counter by 1. The code length to encode each state is the negative logarithm of its estimated probability. Summing each transition over the entire alignment gives the code length,  I ( A ) . 2.2.3 Transmitting the coordinates of T given S and  A With the information of  S  and  A  known to  Bob ,  Alice  can now use that information to encode the coordinate information of  T . Intuitively, our encoding is based on the fact that when scanning  A  from left to right,  Alice  views  T  as runs of coordinates that alternate between blocks of insertions and matches with respect to  S  and the stated alignment. Note that all deletion blocks (with respect to  S ) in  T  are ignored as they contain no information to be transmitted about  T . More formally, let  A  yield  { I 1 , … , I m }  insertion blocks, where any  I k  represents a consecutive stretch of coordinates that are inserted in  T  (with respect to  S ). Each insertion block is transmitted as a null message taking  I ins ( T | S &amp; A ) = ∑ k = 1 m I null ( I k )   bits . What remains to be sent to  Bob  are the coordinates in  T  aligned to corresponding coordinates in  S , that is, the matches. Let  { S i 1 , S i 2 , … , S i n }  and  { T j 1 , T j 2 , … , T j n }  where  1 ≤ i 1 &lt; … i n ≤ | S |  and  1 ≤ j 1 &lt; … j n ≤ | T | , denote the ordered set of corresponding coordinates in  S  and  T , respectively.  Bob  already knows  S  and the alignment. From the alignment information he can infer the indexes of the aligned residue–residue correspondences: ( i 1 ,  j 1 ), ( i 2 ,  j 2 ),  … ,  ( i n ,  j n ) between  S  and  T . Thus, Alice can use the following procedure to transmit the aligned coordinates in  T . To start the procedure, the first three matched coordinates of  { T j 1 , T j 2 , T j 3 }  are sent over the null model message taking:  I startup ( T | S &amp; A ) = I null ( T j 1 , T j 2 , T j 3 )   bits .  Alice then  incrementally  sends the remaining aligned coordinates of  T  as follows. To transmit the current aligned coordinate  T j k + 1 , Alice considers only the set of (previous plus current) aligned coordinates  { T j 1 , 
 T j 2 , 
 … , 
 T j k , 
 T j k + 1 } . This set is orthogonally transformed to the set  { T ¯ j 1 , 
 T ¯ j 2 , 
 … , 
 T ¯ j k , 
 T ¯ j k + 1 } , such that it minimizes the least-square error between  { S i 1 , 
 … , 
 S i k }  and  { T j 1 , 
 … , 
 T j k } . Using this setup, Alice can transmit  T ¯ j k + 1  over a directional distribution on a sphere. This is achieved by first transmitting the radius  r k = | | T ¯ j k + 1 − T ¯ j k | |  over a normal distribution with the same procedure described for formulating a null model message. This allows Alice to state  T ¯ j k + 1  as a point on a sphere with radius  r k  centred at  T ¯ j k . However, we do not state it over a uniform distribution (which would make it a null model description), as the knowledge of correspondence of  T ¯ j k + 1  with  S i k + 1  gives clues about its position on the sphere (provided the assigned correspondence is a ‘good’ one). Because Bob already knows the corresponding point  S i k + 1 , after transmitting  r k , Alice can use a directional probability distribution to state  T ¯ j k + 1  more concisely. In directional statistics, the von Mises–Fisher distribution gives the probability density function (PDF) on the surface of any sphere in  p -dimensions. In three dimensions, the PDF on the surface of a unit sphere is ( Fisher, 1953 ;  Mardia and Jupp, 1999 ):  V ( x ^ ; μ ^ , κ ) = κ 2 π ( e κ − e − κ ) e κ μ ^ · x ^ Using this distribution to transmit  T ¯ j k + 1 , we compute  x ^ k + 1  as the direction cosines of the vector  T ¯ j k + 1 − T ¯ j k , and  μ ^ k + 1  as the direction cosines of the vector  S i k + 1 − T ¯ j k . The probability of stating  T ¯ j k + 1  to the required precision (that is,  ϵ = 0.001  Å precision on each component) using von Mises–Fisher distribution over the surface of a 3D sphere of unit radius is then given by:  P ( x ^ ) = ϵ ′ 2 κ 2 π ( e κ − e − k ) e κ μ ^ · x ^  where  ϵ ′ 2 = ϵ 2 r i 2 , accounting for the scaling of the sphere of radius  r i  to a unit sphere. Transmission of each  T j k + 1  requires the concentration parameter  κ . We use the  maximum-likelihood  estimator based on the available superposition [see  Mardia and Jupp (1999) ]. Therefore, the code length to state  x  using von Mises–Fisher is:  I vmf ( x ^ ) = − log ⁡ ( P ( x ^ ) )   bits . Each  T ¯ j k + 1  is transmitted iteratively over this procedure, which we term  adaptive  superposition. Thus, the message length required to transmit the matched points in  T  with respect to their corresponding point in  S  is  I match ( T | S &amp; A ) = I startup ( T | S &amp; A ) + ∑ i = 4 n I vmf ( x ^ i )   bits . Combining the message lengths of transmitting coordinates in the insertion and matched blocks gives  I ( T | S &amp; A ) = I ins ( T | S &amp; A ) + I match ( T | S &amp; A )   bits .  An illustration of this procedure is shown in  Figure 1 .
 Fig. 1. An idealized example of the adaptive superposition used to send the matched residues in  T  (in blue) incrementally given the knowledge of  S  (in black). Both structures have 8 points and are assumed here to be in one-to-one correspondence. Assume that Bob already knows the first 3 points of  T . Alice sends the fourth point in  T  by superposing all previously matched points between the two structures. (Green crosshairs shows the rotational centre of superposition.) This orients the fourth point (in red) in  T  [or, more generally,  T j k + 1 , whose deviation from its corresponding  S i k + 1  can be encoded over a von Mises–Fisher spherical distribution (see main text)] 2.3 Measure of alignment quality I ( A &amp; S &amp; T )  is used as the measure of alignment quality. We call this measure  I -value, indicating a value measuring the information content in the structural coordinates of  S  and  T , given the structural alignment  A  as a model of compression. The smaller the  I -value, the better the alignment. It follows that for competing alignments  A 1  and  A 2 , if the  I -value of  A 1  is smaller than that of  A 2  by, for example, 15 bits, then  A 1  is 2 15  times more likely than  A 2  (see property 1 of this measure shown in  Equation 3 ). Further, any alignment  A  for which  I ( S &amp; T &amp; A ) &gt; 
 I null ( S &amp; T ) = I null ( S ) + I null ( T )  can be rejected (see property 2 of this measure under ‘Structural alignment quality and lossless information compression’). Handling shifts and rotations.  So far we estimated the  I -value under the rigid model of structural alignment. This model can be generalised to handle plastic deformations commonly observed in protein evolution, such as hinge rotations and shifts. Handling these deformations requires a modification in the way  I ( T | S &amp; A )  is estimated under a flexible model of transmission. Without loss of generality, assume that  T  contains a certain number of shifts and rotations, with respect to  S , associated with its residues. In computing  I ( T | S &amp; A ) , alignment  A  is partitioned at the residues in  T  about which the shifts and rotations are defined. For example, consider below an alignment containing a hinge rotation about residue 10 of  T  (marked by *). Then, the alignment can be partitioned into two separate parts as follows: Let these partial alignments be denoted as  A ( T 1 , … , T 10 )  and  A ( T 10 , … , T 15 ) , identifying the start and end residue indexes in  T  about which the partition is defined. Then  I ( T | S &amp; A )  is computed as  I ( T | S &amp; A ( T 1 , … , T 10 ) ) + I ( T | S &amp; A ( T 10 , … , T 15 ) )  using the procedure described earlier. More generally, if there are  k  residues in  T  about which shifts/rotations are defined, the full alignment  A  is partitioned into  k  + 1 partial alignments:  A ( T 1 , … , T i 1 ) , 
 A ( T i 1 , … , T i 2 ) ,  … , 
 A ( T i k , … , T | T | ) , where  i 1 &lt; i 2 &lt; … &lt; i k &lt; | T | . Given these partitions,  I ( T | S &amp; A )  can be computed as  I ( T | S &amp; A ( T 1 , … , T i 1 ) ) 
 + ⋯ + I ( T | S &amp; A ( i k , … , | T | ) ) .  This immediately poses another inference question: Given an alignment  A  of  S  and  T , how many shifted/rotated residues does it contain? We note that adding a shift/hinge has an overhead which must pay for itself with a better fit if it is to be accepted. Inference of shifted/rotated residues:  A dynamic programming algorithm is used to optimally partition  A  minimizing  I ( T | S &amp; A ) . The algorithm first constructs a matrix  M  of size  | T | × | T |  such that each cell  M ( i ,  j ) ( 1 ≤ i &lt; j ≤ | T | ) stores the value  I ( T | S &amp; A ( i , … , j ) ) . The best partition of  A  is then computed using the following dynamic programming recurrence relationship:
 (4) P ( 1 , … , j ) = min ⁡ i = 1 j − 1 { M ( 1 , j ) , P ( 1 , … , i ) + M ( i , j ) ∀ 1 ≤ j ≤ | T | 
where any  P ( 1 , … , i )  gives the optimal partitioning up to the  i th residue in  T ,  1 ≤ i ≤ | T | . At the end of this procedure the value  P ( 1 , … , | T | )  gives the component message length  I ( T | S &amp; A )  of  Equation 2 , in a way that handles shift and hinge rotations. 2.4 The time complexity of computing  I -value Using the rigid model of transmission (i.e.without handling the hinge-rotations and shift), the computation of  I ( A &amp; S &amp; T )  is linear in the size of the alignment, as the computation of  I ( S ),  I ( A )  and  I ( T | S &amp; A )  are all linear. While the linearity of the first two is clear, that of  I ( T | S &amp; A )  is not, as it requires repeated adaptive superpositions. However, we have recently proved sufficient statistics for the orthogonal superposition problem that allows each updated superposition to be computed as a constant-time update over the previous ones ( Konagurthu  et al. , 2014 ), making the computation of  I ( T | S &amp; A ) , and  I -value under rigid superposition, linear. On the other hand, using the flexible model which allows for hinge rotations and shifts, the computation of  I ( A &amp; S &amp; T )  is quadratic, as it is dictated by the complexity of the dynamic program given by  Equation 4 . 3 RESULTS AND DISCUSSION We have compared the quality of our  I -value measure (using the flexible model described in sections 2.3) with popular scoring functions DALI, TM-Score, MI, SI, STRUCTAL, LGA_S3, GDT_TS, SAS and GSAS, using a large data set of alignments produced by the popular structural alignment methods DALI, TM-Align, LGA, CE and FATCAT. [Refer to  Hasegawa and Holm (2009)  for references of these scores/aligners.] Due to lack of space, we restrict our results herein to those obtained when comparing the DALI Score, TM-Score, and  I -value measures, using TM-Align and DALI as the structural alignment generators. We refer to our online  supplementary material  for the remaining results. Our first experiment tests the ability of the scoring functions to differentiate between pairs of structural domains that vary along the hierarchical groups defined by SCOP ( Lo Conte  et al. , 2000 )—Class, Fold, Superfamily and Family. To do this, we randomly selected a set of 500 ‘pivot’ domains from SCOP and, for each of these pivots, we randomly selected five other domains whose relationship with the pivot varies progressively: (i) Same-Family, (ii) Same-Superfamily (but not Family), (iii) Same-Fold (but not Superfamily and Family), (iv) Same-Class (but different below this level) and (v) Decoy (or different-Class). This results in a collection of 500 × 5 SCOP domains. We then aligned each pivot with each of its five counterparts, generating a total of 2500 alignments per alignment program. Finally, we assessed all these alignments using the selected scoring functions. Figure 2  shows (some of) the box-whisker plots resulting from these comparisons. (As mentioned earlier, more results are given in  Supplementary Table S1  of the online  supplementary material .) Rows in this table denote the alignment method (DALI/TM-Align) used to generate the 2500 alignments in our collection. Columns denote the scoring function (DALI, TM-Score,  I -value) used to compute the alignment score. Each cell in the table is a box-whisker plot that displays the numerical scores (as quartile marks) produced by each [alignment method, scoring function] pair, over the five groups of (500) alignments each. Note that for  I -value, we show the compression gained (in bits) over the null model message length, that is, the (Null– I -value) message lengths. Thus, the greater the compression, the better the alignment. (In contrast, when using raw  I -values rather than compression with respect to Null, the smaller the  I -value the better the alignment.)
 Fig. 2. Table comparing the value of the DALI, TM-Score and  I -value scoring functions (Columns) over 5 SCOP groups (see main text) containing 500 alignments each, generated by (rows) DALI and TM-Align programs. Note that the  Y-axis uses different scales , as the range of values differ between scoring functions. Therefore, the absolute heights of the boxes cannot be compared between the box-whisker plots. However, their performance can be compared by the relative overlaps of the various quartile levels for each group with respect to others  within  the same box-whisker plot A cursory inspection of these box-whisker plots indicates that,  for the given alignments  (which might not be the best/optimal ones), all scoring functions consistently differentiate between the SCOP groups to some extent. However, none of the scoring functions can be said to separate the SCOP groups cleanly nor to be clearly better than the others. This reflects partly the fuzzy classification boundaries of SCOP, and partly the quality of the (sub-optimal) alignments of domain pairs generated by popular alignment methods. For example, for TM-Score it has been claimed that the numerical score of &lt; 0.5 corresponds to alignments not being in the same fold. However, we observe from the box-whisker plots in the second column of the table (the Fold group), that &gt;3 quartiles of the alignments have a TM-score of &lt;0.5. Inspecting the box-whisker plots at the same-Fold group, shows that a very large majority of alignments produced by DALI and TM-Align and scored using  I -value are seen to be statistically significant (i.e. with Null– I -value message length &gt; 0), with DALI alignments being more reliable than those generated by TM-Align, judging by the compression gain at the level of their respective medians. Interestingly,  I -value seems to provide the smallest of variations when comparing the results obtained for the alignments generated by DALI to those generated by TM-Align. Upon closer inspection, the results of our experiment indicate a significant degree of disagreement between the respective scores: only in 28% of the 2500 pivot-versus-counterpart alignments, all three scores agree on whether the alignment produced by DALI or by TM-Align is the best. This disagreement highlights the need for a generally accepted, rigorous alignment score.  Supplementary Table S2  in the online  supplementary material  shows the full list of disagreeing pairs. Our second experiment uses the set of ambiguous alignments described by  Zu-Kang and Sippl (1996) , as case studies for the various scoring functions ability to differentiate between very closely competing alignments. These alignments are indistinguishable in RMSD and number of equivalences (NEquiv).  Table 1  compares the three scoring functions across five pairs of ambiguous structural alignments. For each scoring function, the  better  score of the two alignments for each pair is highlighted in bold. We again observe disagreement in two out of the five pairs between the scoring functions in their ability to decide which of the two alignments is the  best . We emphasize that this discrimination is crucial for any scoring function to be useful as the basis of a search method looking for the optimal alignment.
 Table 1. Comparison between DALI score, TM-Score and  I -value on ambiguous alignments reported by  Zu-Kang and Sippl (1996) Structures Residues Alignment  A 1 Alignment  A 2 S  v.  T | S | | T | DALI Score TM-Score I ( A 1 , S , T ) Dali Score TM-Score I ( A 2 , S , T ) 2TMVP v. 256BA 154 106 262.8 0.4871 9611.2  bits 242.6 0.4744 9614.0 bits 1TNFA v. 1BMV1 152 185 265.1 0.3815 12577.1 bits 307.3 0.3947 12463.7  bits 1UBQ v. 1FRD 76 98 161.9 0.4790 6384.8  bits 146.1 0.4518 6409.8 bits 2RSLC v. 3CHY 119 128 182.6 0.3773 9159.8 bits 206.1 0.3768 9143.5  bits 3CHY v. 1RCF 128 169 377.5 0.4960 10983.0 bits 336.4 0.4855 10961.8  bits Note : For DALI and TM-score, the higher the score the better the alignment. For I-value, the smaller the value (or message length), the better is the alignment. Bold numbers in each row indicate the better of the two competing alignments under each of the scoring measures. Let us illustrate the problems in using some of these scoring functions for an optimal alignment search method, using the case study of pair  3CHY v. 1RCF . To do so we ‘sieve’ each of the two ambiguous alignments using the following procedure, similar to the one described in  Irving  et al.  (2001) , to generate a number of competing alignments at varying levels of NEquiv and RMSD: (i) Compute the [NEquiv, RMSD] and corresponding DALI, TM-Score and  I -value of the current alignment. (ii) Delete the worst-fitting aligned pair that appears at the end of a ‘matched block’ in the alignment, and force the deleted pair of residues to be unaligned. (iii) Repeat from Step 1 until the RMSD falls below a threshold value. Studying the three sieving plots corresponding to DALI, TM-Score and  I -value in  Figure 3 , it is immediately clear that TM-Score ( Fig. 3 b) does not produce any clear optima for this set of competing alignments to choose from. In fact, TM-Score monotonically increases towards the TM-scores of the unsieved alignments. On the other hand,  I -value and the DALI score produce clear, though conflicting optima.  I -value points to an optima for the sieved Alignment 1 at [NEquiv,RMSD] of [101,3.7 Å], whereas DALI points at [94, 3.17 Å]. (Superpositions based on these two alignments can be found in the online  supplementary material .) This reflects the standard dilemma human experts and scoring functions face in choosing between two conflicting objectives. However,  I -value objectively discriminates between the two competing alignments in terms of the lossless compression achieved. We emphasize that compression takes into account  all  aspects of the trade-off—descriptive complexity of the alignment hypothesis versus the quality of fit of the alignment to the structural data—that manifests in the structural alignment problem.
 Fig. 3. Sieving of the two ambiguous alignments reported by  Zu-Kang and Sippl (1996)  for the pair 1RCF and 3CHY. The X-axis gives the RMSD of each sieved alignment, while the Y-axis gives the scoring function used: ( a ) DALI, ( b ) TM-Score and ( c )  I -value. The labels in the figures correspond to the NEquiv values during sieving. For the TM-Score plot (b), the horizontal dotted line is the threshold for fold-level relationship. For the  I -value plot (c), the horizontal line corresponds to the NULL model message length. The sieved alignments below this line are statistically significant Finally, as a proof of concept, we have also developed a  quadratic-time  dynamic programming  heuristic  to search for the optimum  I -value alignment. Currently this alignment heuristic is restricted to the  rigid  (and not flexible) model of computing the component message length  I ( T | S , A ) . Details of the heuristic search are beyond the scope of the current article, given the page limitations. Our implementation of this alignment heuristic based on  I -value is available for use from our website. 4 CONCLUSIONS The importance of finding biologically meaningful structural alignments has led to the intensive development of methods for generating alignments and evaluating their quality. However, these methods produce conflicting results and none has been generally accepted as clearly superior. Here we have described a measure of alignment quality,  I -value, that uses the information content of messages that losslessly compress the C α  atoms of a pair of protein structures, given a proposed alignment. A lower  I -value signifies a superior alignment. The method contains  no  adjustable parameters, as it is built on a formal Bayesian principle of minimum message length inference. Examination of competing alignments over many pairs of protein structures demonstrates that  I -value can accurately distinguish between competing structural alignments in cases in which other methods either cannot significantly distinguish the quality of these possibilities, or do not agree in the selection of a best one. Funding : J.H.C. is supported by  Australian Postgraduate Award (APA)  and  NICTA PhD scholarship . Conflict of interest : none declared. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>MRMAssayDB: an integrated resource for validated targeted proteomics assays</Title>
    <Doi>10.1093/bioinformatics/bty385</Doi>
    <Authors>Bhowmick Pallab, Mohammed Yassene, Borchers Christoph H, Wren Jonathan</Authors>
    <Abstract/>
    <Body>1 Introduction Targeted proteomics has become the method of choice for protein quantification which can be used to reproducibly quantitate large sets of proteins in a high-throughput manner, up to hundred proteins in 45 min ( Keshishian  et al. , 2009 ;  Percy  et al. , 2014 ;  Picotti  et al. , 2013 ). Targeted proteomics methods are typically performed using triple quadrupole in the Multiple Reaction Monitoring (MRM) mode, or using Orbitrap mass spectrometers in the Parallel Reaction Monitoring (PRM) mode. In an MRM experiment, the first quadrupole is used to isolate a particular precursor peptide ion, the second quadrupole is used for collision-induced dissociation, and the third one is used to isolate a characteristic fragment ion. In PRM, only the last step differs in that all ions are monitored. In scheduled LC/MRM-MS analysis, specific precursor/product ion pairs are monitored while the peptide is eluting from the liquid chromatography system, which allows quantitation of a large number of peptides and, by inference, the corresponding proteins ( Mohammed  et al. , 2014 ). Designing and validating a new MRM/PRM assay is a laborious process. It is a multistep workflow that, in addition to performing wet-laboratory experiments, requires integration of prior knowledge with experimental data. This information includes for example the uniqueness of the proxy peptide (surrogate for a protein of interest) within a particular proteome, its retention time under specific LC conditions, the corresponding precursor/fragment ion pairs and more. The very first step in the assay design workflow is a good example of the challenges that one can face. Selecting the most suitable MRM peptide from a target protein includes enforcing almost 30 rules ( Mohammed  et al. , 2014 ), where an important aspect is whether a peptide has been previously observed in MS/MS analyses and thus is known to be detectable—and this type of data can be found in several online public repositories ( Liebler and Zimmerman, 2013 ). There is no doubt that sharing already designed and validated targeted proteomics assays can save a lot of work for scientists and researchers planning to perform MRM or PRM experiments. Well-established resources for MRM-based targeted proteomics data include: PeptideAtlas SRM Experiment Library (PASSEL) ( Farrah  et al. , 2012 ), NCI’s Clinical Proteomic Tumor Analysis Consortium (CPTAC) ( Whiteaker  et al. , 2014 ), PanoramaWeb ( Sharma  et al. , 2014 ), SRMAtlas ( Kusebauch  et al. , 2016 ) and PeptideTracker ( Mohammed  et al. , 2016 ). New entries are added constantly to these repositories by different groups, whereas some repositories are specific like CPTAC which hosts mainly tumor assays, others are more generic for any targeted proteomics experiment like PASSEL. Some repositories store the raw data like PanoramaWeb, while others put emphasis on including the full sample preparation protocols like CPTAC and PeptideTracker. Also, for some protein entries, PeptideTracker lists the determined protein concentration ranges in samples, as measured by MRM. PASSEL is a generic data repository from the Institute of System Biology. New MRM experimental results and the corresponding raw data can be submitted by the user ( Farrah  et al. , 2012 ), ( www.peptideatlas.org/passel/ ), which—along with the corresponding internally-processed results—are made available to the research community. The assay portal of CPTAC ( Whiteaker  et al. , 2014 ) from the National Cancer Institute (NCI), ( http://assays.cancer.gov ), serves as an open-source repository of well-characterized targeted proteomic assays. Its main goals are to enable robust quantification of all human proteins and to standardize the quantification of targeted MS-based assays ( Whiteaker  et al. , 2016 ). Submission to the portal is done by the consortium partners. PanoramaWeb ( Sharma  et al. , 2014 ) is an online resource for storing, sharing and analyzing targeted proteomic experiment processed by Skyline software ( MacLean  et al. , 2010 ). The PanoramaWeb repository ( https://panoramaweb.org ) provides a detailed view for the peptide, including chromatograms for the precursors in of all the replicates and also plots the peak area integrations. PeptideTracker ( Mohammed  et al. , 2016 ) ( http://peptidetracker.proteincentre.com ) was introduced as a knowledge base that is designed for collecting and storing information on protein concentration ranges in biological tissues along with the detailed description of the assays that were used. The information in PeptideTracker has been compiled from different experiments, along with the acquisition protocols and conditions used. While submission of new data is possible, the listed assay entries are mainly for human and mouse tissues due to the work history of the group behind PeptideTracker. Because these data repositories and knowledge bases on targeted proteomics assays were introduced with different goals in mind, it is understandable that the information hosted is heterogeneous. In this context, MRMAssayDB provides the first tool to obtain comprehensive information on all available targeted proteomics assays in all of these community-wide online resources. 2 Materials and methods 2.1 MRMAssayDB data collection Currently, MRMAssayDB is populated with MRM-based targeted proteomics assay data from PASSEL ( Farrah  et al. , 2012 ), CPTAC ( Whiteaker  et al. , 2014 ), PanoramaWeb ( Sharma  et al. , 2014 ), SRMAtlas ( Kusebauch  et al. , 2016 ) and PeptideTracker ( Mohammed  et al. , 2016 ). Ultimately, the goal is to combine data that resides in different targeted proteomics repositories, and provide users with a unified view of available targeted proteomics assays. MRMAssayDB keeps various information on the protein assays including UniProtKB accession number ( UniProt_Consortium 2011 ), protein name, gene name, organisms, peptide sequence, uniqueness in the proteome, peptide presence of isoforms, labeled internal standard used and, importantly, a hyperlink of each entry to the relevant targeted proteomics resources from which the information was obtained ( Supplementary Fig. S1 ). The datatables in MRMAssayDB are updated periodically. For this, new data from all five repositories is retrieved, matched with UniProtKB ( UniProt_Consortium 2011 ) and other databases, and compiled automatically on a monthly basis. The uniqueness of the peptide to the target protein is re-assessed in each compilation as it may change with the discovery of new proteins. Here, a ‘unique peptide’ means that its sequence is present only in the specific protein and/or its isoforms, and both these levels are annotated. This means that users have available to them information on whether a peptide assay can be used to quantify a specific isoform, or whether it can be used to test if a gene model with any splice isoform is expressed at the protein level. 2.2 Search functionality Because of the richness of the collected data, having a good search algorithm is essential. MRMAssayDB allows searching the assays and filtering as well as downloading the search results in various ways, based on the user’s interest. The assays can be searched based on partial protein name, protein accession, partial peptide sequence, biological pathway involvement, or involvement in disease ( Fig. 1 ). After the search results are present on the screen, additional filters can be applied to each column, columns can be blended and viewed, and a final report of the search results can be downloaded. MRMAssayDB also provides an advanced search option where multiple search terms specific to different data types can be combined, e.g. protein name and subcellular location, or gene name, organism and biological pathway. It is recommended that users use the advanced search function whenever possible because combined queries limit the volume of data that has to be loaded, and thereby improve responsiveness.
 Fig. 1. MRMAssayDB homepage with simple and advanced search functionalities, along with some statistics on its content 2.3 Domain, post-translational modifications and disease-causing mutations Mutations and post-translational modifications (PTMs) are important in the study of health and disease, and more data is continuously becoming available on the positions of disease-related mutations and PTMs on the protein ( Lam  et al. , 2016 ;  Nehrt  et al. , 2012 ). While the majority of MRM/PRM targeted proteomics assays are based on non-modified peptides, it is important to have updated information on whether the proxy peptide selected carries a mutation or PTM, or not. We have combined data on known disease-causing mutations and PTMs, and mapped it to the assays in MRMAssayDB. To visualize the variations, PTMs and important UniProtKB features, we have added a recently-released visualization tool, ProtVista ( Watkins  et al. , 2017 ), that shows the proxy peptide, protein sequence features in the UniProtKB, experimental proteomics and variation public datasets ( Fig. 2 ). The functional, positional and variant information was obtained from the EBI Protein API ( Nightingale  et al. , 2017 ).
 Fig. 2. A visual representation of a protein and its MRM peptide assays along with other annotations displayed in MRMAssayDB. The example in the figure is from cellular tumor antigen p53 protein (P04637) showing UniProtKB features, experimental proteomics and variants obtained from public datasets using the EBI ProtVista tool after extending it with MRMAssayDB data 2.4 Structural data In targeted proteomics methods that are based on surrogate or proxy peptides, it is sometime of interest to know the exact position of the peptide in the 3D structure of the protein. Therefore, 3D protein struc-tures in the unbound (free) and bound (complex) states that include the particular peptide have been extracted from Protein Databank (PDB) ( Berman  et al. , 2002 ). PDB structures were selected using two criteria: (i) the constructs used for the structure determination experiment contained the peptide sequence of interest, and (ii) there is visible electron density for the peptide. IUPred ( Dosztányi  et al. , 2005 ) software was used to predict the structural disorder, and IUPred outputs scores (ranging between 0 and 1) are included for each residue; with scores that are &gt;0.5 indicating disordered residues (the ‘short’ mode of IUPred was used) ( Supplementary Fig. S2A–E ). 2.5 Biological pathways MRMAssayDB maps all available assays to the entries in the KEGG pathway database ( Kanehisa and Goto, 2000 ), and extracts and displays the entire network of which the assay is a part. The genes or proteins in the pathway maps are color coded, with violet indicating the assay that the user searched for in MRMAssayDB, and maroon and orange indicating that there is an assay available for a protein or gene, while rectangular green boxes represent genes or proteins in the pathway without assays in MRMAssayDB. All color-coded boxes are linked to the data in MRMAssayDB via hyperlinks. With one single interactive plot, the user can see the coverage of the biological pathway using available assays ( Fig. 3A ).
 Fig. 3. Pathway and PPI Viewer in MRMAssayDB. ( A ) An example of Complement and Coagulation cascades for tissue factor protein (gene name F3) from the KEGG database. ( B ) Protein–protein interaction network of the F3 gene. All nodes in (A) and (B) can be searched for targeted assays in MRMAssayDB via the web interface, using a single click 2.6 Protein–protein interactions For each assay, MRMAssayDB extracts the known protein–protein interactions from the STRING database ( Szklarczyk  et al. , 2015 ). The same color-coding scheme for pathway analysis was used when mapping the genes or proteins onto the protein–protein-interaction network, while white boxes represent non-mapped genes in the interactome ( Fig. 3B ). 2.7 Functional data MRMAssayDB links the targeted proteomics protein assays to the functional data annotations for the protein as represented in the Gene Ontology (GO) annotations from the QuickGO ( Binns  et al. , 2009 ) web-based portal ( Supplementary Fig. S3 ). This functional data includes information on the three most general GO terms: Biological Process, Molecular Function and Cellular Component. Briefly, Molecular Function explains what a gene product does at the biochemical level. Biological Process explains a broad biological objective. Cellular Component defines the location of a gene product, within cellular structures and within macromolecular complexes ( Gene_Ontology_Consortium 2011 ). 2.8 Implementation All the routines in MRMAssayDB are written in the Python 2.7 programming language ( www.python.org ). The user web interface was developed using the Django 1.8 framework ( https://djangoproject.com ), and displays plots are generated on the fly using JavaScript. JSmol is used to visualize the 3D protein structures interactively ( http://wiki.jmol.org/index.php/JSmol#JSmol ). We preferred JSmol over Jmol because JSmol is a JavaScript applet and does not require any Java (popup window) authorization. Jvenn ( Bardou  et al. , 2014 ) and Cytoscape.js ( Franz  et al. , 2016 ) were used to plot the interactive Euclidean diagrams ( Fig. 1 ) and PPI networks ( Fig. 3B ), respectively. The metadata from PASSEL ( Farrah  et al. , 2012 ), CPTAC ( Whiteaker  et al. , 2014 ), PanoramaWeb ( Sharma  et al. , 2014 ), SRMAtlas (Kusebauch  et al. ), PeptideTracker ( Mohammed  et al. , 2016 ), UniProt ( UniProt_Consortium 2011 ), PDB ( Berman  et al. , 2002 ), KEGG ( Kanehisa and Goto, 2000 ), STRING ( Szklarczyk  et al. , 2015 ) and QuickGO ( Binns  et al. , 2009 ) are automatically retrieved using routines written in Java ( www.oracle.com/java/index.html ), Python and Selenium Webdriver ( http://www.seleniumhq.org ). 3 Results MRMAssayDB captures various types of data and information from MRM/PRM experiment repositories as well as proteomic and pathway information from targeted proteomics assays, and makes all linked information accessible through a single web interface. Our system provides up-to-date information on thousands of targeted proteomics assays. Currently, entries for 168 404 peptide assays are included, covering more than 34 740 proteins from 63 species ( Table 1 ). These proteins correspond to 13 102 biological processes, 4886 molecular functions, and 1929 cellular components.  Supplementary Figure S4  shows this trend exemplified in the top GO terms for human proteins. As of March 2018, PanoramaWeb ( Sharma  et al. , 2014 ), CPTAC ( Whiteaker  et al. , 2014 ), SRMAtlas (Kusebauch  et al. ) and PASSEL ( Farrah  et al. , 2012 ), and PeptideTracker ( Mohammed  et al. , 2016 ) contain information on thousands of assays with transitions for the labeled and label-free forms of the peptide ( Table 2 ). While each of the five resources was developed for a specific goal, the information hosted by them is complementary ( Fig. 1 ), and together they form a valuable foundation for MRM based targeted proteomics.
 Table 1. The five organisms with the highest number of entries in MRMAssayDB (March 2018) Organism name Number of unique peptide assay entries Homo sapiens  (human) 101 613 (covering 19 777 proteins) Saccharomyces cerevisiae  (yeast) 47 946 (covering 6478 proteins) Escherichia coli 3420 (covering 2363 proteins) Mus musculus  (mouse) 5327 (covering 2304 proteins) Mycobacterium tuberculosis 5892 (covering 1681 proteins) Others 4211 (covering 2137 proteins) Total 168409 (covering 34740 proteins) Table 2. Assays in the targeted proteomics repositories Database name Number of unique peptide assay entries CPTAC 1405 assay covering 808 proteins PanoramaWeb 41 998 assay covering 8625 proteins PASSEL 20 496 assay covering 7553 proteins PeptideTracker 5640 assay covering 3281 proteins SRMAtlas 118 098 assay covering 25914 proteins The information on these 168K proteotypic peptide assays is based on the 177 327 MRM assays currently available in the targeted proteomics community repositories. The difference in the numbers is due to assays that are not unique to the proteome of a specific organism (using UniProtKB as the reference). A user can nevertheless use the advanced search function to upload his own protein sequences in FASTA format, and all 177K assays will be searched and suitable proteotypic peptide assays will be reported in the results. Because MRMAssayDB uses an integrative approach and maps data from multiple repositories, it allows a new view of every assay and shows how many data repository a particular assay is in. For example one protein may have multiple proteotypic peptide assays already developed, validated, and in used. The proteotypic peptide which has been successfully used by the largest number of users would probably be a good choice if one is interested in simple quantification of the protein. In addition, MRMAssayDB lists the most common transitions for each peptide MRM assay. Keeping in mind that all transitions originate from actual validated assays used in one or more laboratory, the most commonly used transitions are those ions which appearing most frequently in the assays for a specific surrogate peptide. Although there may be various ways of determining the ‘best’ transitions—e.g. those with the highest intensity, best peak shape, or fewer interferences, we assume that if various scientists have followed various methods to determine the transitions and validate them in actual experiments, then the best transitions will be those that appear most frequently in various experiments. Keeping in mind that the user can sort and filter any column in the MRMAssayDB results datatable, the user can, for example, identify the most commonly used assay for a specific protein and its best transitions with only few clicks. Not all of the assays in the online repositories are based on unique peptides in the proteome. Therefore, for all peptides that are included in the MRMAssayDB, a uniqueness test is performed and the results are reported next to each peptide. Currently 93% of all assays are based on unique peptides ( Table 3 ). Using the EBI standard tool for the graphical representation of protein sequence features—ProtVista tool ( Watkins  et al. , 2017 ), the proxy peptide of the target protein can be compared to the various protein-sequence features in UniProtKB, and experimental proteomics and sequence variation data. As example of the human protein P53_HUMAN which functions as a tumor suppressor (25) is shown in  Figure 2 , and one can see that the proxy peptide (residues 182–196) contain a modified residue and two disease-causing mutations.
 Table 3. Assay specificity to protein isoform based on repository data Database name Peptide uniqueness in proteome (%) Peptide is specific to one protein isoform (%) CPTAC 97.30 48.93 PanoramaWeb 95.65 95.41 PASSEL 96.38 82.15 PeptideTracker 95.65 67.12 SRMAtlas 78.21 60.23 Overall 92.64 70.77 The protein structure viewer facilitates the exploration and visualization of 3D protein structures and allows the user to visualize the region of origin of the proxy peptide. This allows the user to visually explore whether the surrogate peptide originated from the surface or was embedded deeper in the folded protein, or whether the peptide formed part of an alpha helix or a beta sheet. In case of multiple peptide assays for one protein, the user can choose an assay based on the specific region of the proxy peptide in the 3D structure of the protein. An example showing ubiquitin-like ISG15 protein and its assay using the surrogate peptide LAVHPSGVALQDR is presented in  Supplementary Figure S2 . From all entries in MRMAssayDB, we were able to map a total of 13 568 proteins to 384 KEGG master biological pathways.  Figure 4  shows a few examples of these pathways, such as the P13K-AKT signalling pathway in humans which includes 524 proteins of which 342 are present with assays in MRMAssayDB, the KEGG pathways in cancer for which there are 522 assays available for human ( Fig. 4 ).  Figure 4  provides an insight into how the integrative approach of combining the five major targeted-proteomics assay repositories can provide improved coverage compared to an individual repository. Approximately only two-third of all human proteins are currently associated with metabolic pathways (based on entries retrieved from KEGG and UniProtKB in March 2018). Using the information in MRMAassayDB on the protein in volvement in biological pathways, assays can be chosen and used to further characterize existing pathways, or—as not all proteins are present in KEGG—an assay can be chosen to find new undocumented involvement of a protein in a pathway. For pathway visualization, all nodes (genes/proteins/enzymes) can be interactively used, as they are hyperlinked to each corresponding entry and are color-coded and highlighted on the pathway graphs ( Fig. 3A ).
 Fig. 4. Top 10 KEGG pathways with targeted proteomics assays in humans. Above each bar is the number of proteins with assays in MRMAssayDB, the total number of proteins corresponding to that pathway, and—in parentheses—the corresponding percent coverage As the amount of available protein–protein interaction data increases, it is common for researchers to analyze their proteins of interest as a part of a system of dynamic interactions. The protein–protein interaction network of each assay in MRMAssayDB is displayed using the Cytoscape Web application ( Franz  et al. , 2016 ). The network can be obtained from the search-results table ( Supplementary Fig. S1 ). The color-coded network (See Section 2) ( Fig. 3B ) provides helpful insights into the available targeted proteomics assay for which interaction. MRMAssayDB also maps all entries to their Gene Ontology annotation. The current assays map to 13102 biological processes, 4886 molecular functions, and 1929 cellular components. Most of the available assays for human samples are associated with G-protein coupled receptor signaling pathway (99%) and signal transduction (99.5%). On one hand, this indicates the possibility of using the assays collected in MRMAssayDB to characterize diverse biological process ( Supplementary Fig. S4 ), and, on the other hand, it shows the preferences in the targeted proteomics community toward developing assays to characterize biological processes. The majority of the molecular functions and cellular components were associated with protein binding and cytosol ( Supplementary Fig. S4 ). 4 Conclusions MRMAssayDB is a freely available web-based resource for accumulating available targeted proteomics assays in the community. It retrieves information from five major MRM and PRM targeted-proteomics assay portals, and the data repositories: PASSEL, PanoramaWeb, CPTAC assay portal, SRMAtlas and PeptideTracker. The collected information is integrated and annotated with additional information on the involvement of biological pathways, protein–protein interaction, Gene Ontology terms, known PTMs and disease-associated mutations, disease involvement, etc. MRMAssayDB will help scientists and researchers attempting to perform a targeted proteomics experiment to easily find the suitable assay and will accelerate sharing the assays between scientists. MRMAssayDB shows how many data repositories reported each assay, allowing results to be sorted and enabling the user to immediately see which assay was used successfully by most users. Furthermore, for each MRM peptide assay, the common transitions appearing most frequently in all available assays from different repositories and instruments are listed. Users can also upload own protein sequences to search for suitable existing assays. The application combines the strengths of Java, Python, Ajax, JavaScript, Selenium Webdriver and Django with a suite of integrated bioinformatics tools. Ultimately, MRMAssayDB supports basic and advanced search queries, as well as the possibility of filtering and sorting the search results. The software has also an advanced Application Program Interface allowing a query to be sent in an automated manner. As new datasets become available, we intend to continue to integrate them into the database. The data entries are updated automatically at the beginning of each month. MRMAssayDB is unique in both its integrative approach and the comprehensiveness of information provided on the available targeted-proteomics assays in the community–wide online resources. The type of empirical, detailed information included in this tool for each peptide is required and essential for the planning of MRM experiments. Access to the software is available free of charge, at  http://MRMAssayDB.proteincentre.com/ . Supplementary Material Supplementary Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Bayesian survival analysis in genetic association studies</Title>
    <Doi>10.1093/bioinformatics/btn351</Doi>
    <Authors>Tachmazidou Ioanna, Andrew Toby, Verzilli Claudio J., Johnson Michael R., De Iorio Maria</Authors>
    <Abstract>Motivation: Large-scale genetic association studies are carried out with the hope of discovering single nucleotide polymorphisms involved in the etiology of complex diseases. There are several existing methods in the literature for performing this kind of analysis for case-control studies, but less work has been done for prospective cohort studies. We present a Bayesian method for linking markers to censored survival outcome by clustering haplotypes using gene trees. Coalescent-based approaches are promising for LD mapping, as the coalescent offers a good approximation to the evolutionary history of mutations.</Abstract>
    <Body>1 INTRODUCTION Much of the current focus in human genetics is on disentangling the genetic contribution to complex diseases via genetic association studies. Numerous methods have been proposed for the analysis of genetic data from case-control studies, but very little is available for the analysis of time-to-event outcomes, such as patients’ overall survival time or time to cancer recurrence. The most popular approach to modelling survival data is the Cox proportional-hazards regression (Cox,  1972 ). However, in the context of genetic association studies, Cox regression faces the same problems as common regression which are related mainly to the size of datasets currently being collected and the collinearity between markers that may exist due to linkage disequilibrium (LD). The simplest approach would be to use univariate Cox models to assess the association between each marker and outcome separately. However, univariate analyses can be inefficient as they do not account for the aforementioned statistical correlation or LD between markers, as opposed to multi-marker approaches. In this article, we propose to tackle these problems (high-dimensionality and multi-collinearity) by clustering haplotypes with similar hazard risks. The proposed method is an extension of approach described in Tachmazidou  et al.  ( 2007 ), which deals with case-control data. Here, we assume a parametric model for survival times and search for genetic variants, mostly single nucleotide polymorphisms (SNPs), that show important associations with the survival times. In particular, we scan the chromosomal region of interest for sub-regions of no obligate recombination, or parallel and back mutations. Each sub-region can be represented by a unique evolutionary tree, called gene tree or perfect phylogeny (PP) (Griffiths,  2001 ), whose topology approximates the mutational history of the haplotypes therein. Coalescent approaches are promising for LD mapping, as the coalescent is likely to provide a better approximation to the evolutionary history of mutations compared to empirical clustering methods. We use a Markov chain Monte Carlo (MCMC) algorithm to iteratively sample from the PPs that make up our genetic region, and we cluster the haplotypes according to the relative ages of the markers in the sampled PP. The main idea behind our clustering metric is that ancestrally similar haplotypes are likely to have similar hazard risks. After convergence, we obtain the posterior probability of each SNP being a cluster centre, and we treat this as the posterior density of the location of a putative causal variant, since high values correspond to markers where haplotypes are best separated, suggesting the presence in the region of a variant influencing the risk of the clinical event. The proposed method is fast and can handle large datasets with many markers and/or patients. Its performance is compared in simulation studies to the univariate Cox regression, and to the dimension reduction methods of Li and Gui ( 2004 ), implemented in the software PCRCox, and Bair and Tibshirani ( 2004 ) and Bair  et al.  ( 2006 ), implemented in the software SUPERPC. Li and Gui ( 2004 ) propose a partial Cox regression (PCR) method that constructs uncorrelated components via repeated least square fitting of residuals and Cox regression fitting. From the resulting PCR components, the first  k  most important are determined by univariate Cox regression. Li and Gui ( 2004 ) also suggest that using PC analysis to find the non-trivial principle components and then fitting only these using their method, results in a more parsimonious model. Bair and Tibshirani ( 2004 ); Bair  et al.  ( 2006 ) propose a semi-supervised form of PC analysis, called Supervised Principle Components (SPC). SPC initially computes univariate Cox regression coefficients, and retains those variables whose coefficients exceed in absolute value some threshold, estimated by cross-validation. Using the reduced dataset, it computes the first few principle components and provides important scores for the initial variables. In our simulation studies, we consider different scenarios varying in genetic relative risk, minor allele frequency of the causal allele, sample size and censoring. The proposed method yields similar localization performance to the other methods considered, while showing a clear advantage in terms of false positive associations. We applied our approach to data from the SANAD (a study of Standard and New Antiepileptic Drugs) UK prospective study ( www.liv.ac.uk/neuroscience/sanad ) and found potential associations between candidate ATP-binding cassette (ABC) transporter genes and epilepsy treatment outcomes. 2 METHODS 2.1 Perfect phylogenies Over genomic regions characterized by strong LD, where there is no evidence of recombination and recurrent point mutations, haplotypes are said to have evolved according to a PP. Haplotypes within these regions can be represented by a unique gene tree that describes their mutational history. For example,  Table 1  represents the incidence matrix for a set of 4000 haplotypes. Columns correspond to six diallelic SNPs and rows are the unique haplotypes in the dataset. Alleles are coded as 0 for the major allele and 1 for the minor allele. A rooted PP assumption poses the constraint that, for any two SNPs in the incidence matrix, not all three combinations (01, 10, 11) exist. In contrast, recombination and back or parallel mutation lead to the possible existence of all three combinations.
 Table 1. Incidence matrix for six distinct haplotypes and their frequencies, consisting of six SNPs ( S 1 – S 6 ) Frequency S 1 S 2 S 3 S 4 S 5 S 6 204 1 1 0 0 0 0 932 1 1 0 0 1 1 233 1 1 0 0 1 0 2064 0 0 0 0 0 0 565 0 1 1 1 0 0 2 0 1 1 0 0 0 
 When the PP assumption is valid, we can use Gusfield's algorithm (Gusfield,  1991 ) to construct the gene tree compatible with the data.  Figure 1  shows the gene tree for the haplotypes in  Table 1 . The nodes in the tree correspond to mutations and the gene tree is rooted at the haplotype with all major alleles. Mutations are ordered on the tree according to their relative age. If the causal mutation is embedded between SNPs 2 and 3 say, all descendant haplotypes will inherit it and will therefore have a more recent shared ancestry than the other haplotypes. Thus, the survival associated with the 567 haplotypes that correspond to the last two branches of the tree will tend to be similar. However, the phenotype–haplotype relationship may become more fuzzy due to dominance, epistasis or the effect of environmental factors.
 Fig. 1. The gene tree consistent with the haplotypes in  Table 1 . Labels 1–6 refer to mutations  S 1 – S 6 . At the bottom of each branch, we report the multiplicity of each observed haplotype in the sample. Therefore, the use of the PP model implies little or no recombination in genomic segments and the ‘infinitely many sites’ assumption from population genetics. We propose to split the chromosomal region of interest into consecutive PPs with window boundaries deterministically defined by the locations where the PP assumption breaks. Details of how this is implemented are given in Tachmazidou  et al.  ( 2007 ). Once the set of windows and corresponding trees have been identified, we use a Bayesian partition model to search through trees to identify those, if any, where the corresponding set of haplotypes appear to form clusters that discriminate high from low-hazard risk, thus possibly harbouring a causal variant. 2.2 Haplotype clustering We use a Bayesian partition model to split the haplotype space into an a priori unknown number of disjoint clusters on the basis of haplotype similarity. Our similarity measure has an evolutionary interpretation, with sequences sharing a cluster depending on the time to their most recent common mutation. In other words, our distance metric is based on the order with which the mutations have arisen in the haplotype sample. This information is provided by the topology of the gene tree. Therefore, any SNP set selected as cluster centres can be time ordered and we assign haplotypes to clusters according to the relative ages of these centres. Suppose, for example that SNPs 1, 3 and 5 of  Figure 1  are selected as cluster centres. SNP 5 is younger than SNP 1, and SNP 3 is on a different branch, implying that a haplotype carrying mutation 3 cannot carry mutation 1 or 5. Starting with SNP 5, we assign the haplotypes that correspond to the third and fourth branch of the tree as members of this cluster. Only the second haplotype is assigned to the cluster with SNP 1 as centre because, although the third and fourth haplotypes carry mutation 1, they have been already allocated to a cluster. The last two haplotypes are allocated to a separate cluster with centre SNP 3, and the remaining haplotype is assigned to a hypothetical ‘null’ cluster, which can be interpreted as a baseline risk group. Therefore, every haplotype is deterministically allocated to the cluster with the closest centre. Haplotypes within each cluster are assumed to have similar survival probabilities and risks. 2.3 Modelling approach Let us assume that the haplotype data can be split into  n tr  PP or gene trees, that tree  T  is selected as harbouring the causal mutation and that the haplotype space is currently partitioned into  n c = n clust +1 clusters ( n c  includes the ‘null’ cluster, while  n clust  is the number of SNPs selected as cluster centres). Conditionally on tree  T , an indicator vector  γ  represents the partition, with  γ =(γ 1 ,…,γ n SNP T ), γ k ∈(0,1),  k =1,…, n SNP T , such that γ k =1 if the  k -th SNP is selected as cluster centre and γ k =0 otherwise, where  n SNP T  is the number of SNPs in  T . For a failure time  X , right-censored data can be represented by pairs of random variables ( t ,δ). When the lifetime  X  is observed, δ=1 and  t  is equal to  X , whilst for right-censored data, δ=0 and  t  is equal to the censoring time  C r , i.e.  t =min( X , C r ). Therefore, δ ij ∈{0,1} is the censoring status indicator of survival time  t ij  for haplotype  i =1,…, n j  in cluster  j =1,…, n c . The vector of responses for cluster  j  is denoted by  D j ={( t 1 j ,δ 1 j ),…,( t n j j ,δ n j j )}′ and let  D ={ D j , j =1,…, n c }. Each  t ij  is assumed to have an exponential distribution with cluster-specific parameter θ j . Thus, the likelihood of the data is:
 
where ∑ i =1 n j   t ij  is the total time in the study for all  n j  haplotypes in cluster  j , and ∑ i =1 n j  δ ij  is the observed number of events (e.g. deaths) in cluster  j , with  j =1,…, n c . Parameters θ j  are given independent Gamma(ν 0 , ν 1 ) priors. The marginal probability of the data is available analytically and given by
 (1) 
where Γ denotes the Gamma function. A priori each tree is equally likely to contain the putative mutation. Conditionally on tree  T , we impose a binomial prior on the number of cluster centres  n clust . Given  n clust , any cluster configuration is equally likely a priori. Then the joint prior distribution of tree  T  and partition  γ  is given by
 (2) 
where  q  is the success probability of the Binomial distribution, and can be chosen to penalize or favour big number of clusters in such a way to reflect the investigator's prior beliefs. The posterior conditional distribution of θ j  is also available in closed form
 Upon convergence, we obtain a posterior sample of partitions and an estimate of the posterior probability that the causal mutation is embedded within each of the trees. We then calculate the Bayes factor in favour of association at each marker site as the ratio of the posterior to prior odds (Kass and Raftery,  1995 ), where the prior odds of each SNP being a cluster centre is evaluated by simulation using Equation ( 2 ). 2.4 Details of the MCMC algorithm We use an MCMC algorithm to sample from the posterior distribution over the space of possible partitions. At each MCMC iteration, we perform two M-H steps:
 Change partition: sample a new partition from the posterior distribution of the cluster centres without changing the current gene tree. Update tree: sample a new tree and a new partition from their joint posterior distribution. In particular, we use a Metropolis–Hastings (M–H) step to sample from the conditional distribution of partition  γ  given the data and tree  T . We consider two possible moves in the partition space: adding (birth) or deleting (death) a cluster centre. At each move, we randomly select SNP  i  and we propose γ ⋆ i =1, if the current γ i =0 or γ ⋆ i =0 otherwise. Thus, the proposal distribution  q ( γ ⋆ | γ ) is simply 1/ n SNP T . Given the cluster centres, the observed haplotypes are deterministically allocated to the haplotype clusters according to our similarity metric. The logarithm of the acceptance probability for the first M-H sampler ( a 1 ) simplifies to the logarithm of the Bayes factor (BF) in favour of  γ ⋆  over  γ , i.e.  p ( D | γ ⋆ )/ p ( D | γ ) where the marginal probability is calculated using Equation ( 1 ) ± log((1− q )/ q ) depending if the death or birth of a cluster centre is proposed respectively, i.e.
 
We use a second M-H step to sample a tree from the  n tr  possible trees with probability 1/ n tr  and a new partition given the tree. Each SNP in the tree has a 0.5 probability of being proposed as a cluster centre. Therefore, the proposal probability of the tree and the partition space is equal to 1/( n tr  × 2 n SNP T ), and the joint prior distribution of a gene tree  T  and a partition  γ  is given by Equation ( 2 ). The logarithm of the acceptance probability for the second M-H sampler ( a 2 ) simplifies to the logarithm of the Bayes factor in favour of the proposed partition over the current partition plus the logarithm of the prior ratio and the logarithm of the proposal ratio, i.e.
 
  where   and   is the number of SNPs in the proposed and current tree respectively, and  n clust prop  and  n clust curr  is the number of clusters in the proposed and current tree, respectively. 3 RESULTS 3.1 Simulation study The performance of the proposed method was compared to that of the univariate Cox regression, the PCRCox and SUPERPC software. PCRCox and SUPERPC were mainly developed for analysing gene expression data. Here, we explore their performance when searching for marker-survival phenotype associations. We simulated a population of 20 000 haplotypes over 1 Mb chromosomal region with recombination hotspots using the FREGENE software (Hoggart  et al. ,  2007 ) under the default parameters. From the simulated data, we retained markers with minor allele frequency (MAF) ≥1%. From these markers, we selected 1000 SNPs with probability proportional to  p (1− p ), where  p  is the allele frequency of a marker in the sample. In this way, we obtain an average spacing of 1 kb. We then selected at random a SNP with allele frequency between  p −0.005 and  p +0.005, where  p  was in a range between 0.02 and 0.3, to be the causal locus. We sampled with replacement pairs of haplotypes to form an individual's genotype and generated their survival times from a Gompertz(α,β) distribution (with α=1,β=1). The Gompertz distribution (Gompertz,  1825 ) is a popular probability model for human mortality and it is a proportional hazards model like the Cox model. Assuming additive genotype effect, the hazard function is given by  h ( t | G i )=βe α t e ρ G i , where  G i  is the number of copies of the causal allele,  G i =0,1,2, and ρ is the logarithm of the hazard relative risk of the heterozygote, HRR(Aa). We varied the relative risk between 1.2 and 3. The censoring times were generated from an exponential distribution with mean equal to  t ⋆ =1/λ. To evaluate  t ⋆ , we assumed that 5% of the whole population experiences the disease by time  t ⋆ , i.e.  P ( t ≤ t ⋆ )= P ( t ≤ t ⋆ | G 0 )(1− p ) 2 + P ( t ≤ t ⋆ | G 1 )2 p (1− p )+ P ( t ≤ t ⋆ | G 2 ) p 2 =5%, where  P ( t ≤ t ⋆ ) is the cumulative distribution function of the survival times, and  p  is the allele frequency of the causal variant. We considered a censoring level of 30, 50, 70 and 90%, and sample sizes of 400, 1000, 2000 and 4000 individuals, which at the 90% censoring level yielded 40, 100, 200 and 400 disease cases, respectively. In all analyses, we removed the causal allele from the dataset and, using the technique described in the ‘Perfect phylogenies’ section, we constructed the perfect phylogenies in the dataset. The average number of gene trees was 200 and the average number of SNPs in a gene tree was 4. The MCMC algorithm was run for 100 000 iterations with a burn-in of 10 000 iterations for 50 datasets under different combinations of the simulation parameters. For a dataset of 1000 markers and 4000 haplotypes the proposed method took 14 min to construct the phylogenies and 13 min to run the algorithm on an Intel Xeon 3.40 GHz processor with 2 Gb of memory. The computing time for PCRCox and SUPERPC was 4 and 59 min respectively. An R ( www.r-project.org ) package called BETA-Surv (Bayesian Evolutionary Tree-based Association analysis for Survival) implementing the proposed method is available on request from the first author. In the simulated and real data examples, we set the prior hyperparameters as follows: we assume a Gamma(0.1,0.1) prior on θ j ,  j =1,…, n c , and a Binomial( n SNP T , p =0.98) for  n clust  given tree  T . Such a high success probability of the Binomial reflects an a priori belief that each haplotype in the tree has its own risk. 3.2 Model performance We investigate the performance of BETA-Surv, univariate Cox, PCRCox and SUPERPC in terms of localization, power and false positive rates. To run PCRCox, we first performed PC analysis and we only used the significant principle components (i.e. those whose SD is bigger than 10 −10  in absolute value), as suggested in the software documentation. We were also advised (Jiang Gui, personal communication) that the first five principle components were usually sufficient. From these, we estimated regression coefficients of the original variables. SUPERPC returns importance scores for each of the significant variables, which we use to estimate associations. In order to estimate the best threshold, SUPERPC computes a cross-validated likelihood ratio (LR) statistic using the first, the first two or the first three principle components. Because for most of the simulated datasets none of the three LR tests were significant, we calculated scores using three different thresholds (i.e. using the first, the first two or the first three principle components), and we took the mean value of the scores for each significant SNP. Results reported for each simulation scenario are averages over 50 replicates. However, SUPERPC spuriously failed to produce results on approximately half of the datasets under each scenario. Therefore, for this method results are over approximately 25 replicates. To measure each method's accuracy in estimating the position of an untyped causal allele, we report the probability that the identified location is within some distance from the true location. The position of the susceptibility variant is estimated by the physical location of the SNP with the maximum Bayes factor for the proposed approach and with the minimum  P -value for univariate Cox regression. For PCRCox and SUPERPC the causal location is estimated by the SNP with the highest absolute regression coefficient or score, respectively. To determine power, we define a window of 100 kb either side of the causative allele and calculate the proportion of the 50 replicates having a significant association within the window. The significance of a signal is assessed via the following rules. For BETA-Surv, a SNP is considered to have positive signal when its Bayes factor in favour of association is ≤3 (Kass and Raftery,  1995 ). For univariate Cox regression, we use two different significance thresholds, i.e. a  P -value ≤0.05, and the Bonferroni-adjusted value. For PCRCox and SUPERPC, a SNP is regarded as a positive hit if its regression coefficient or score, in absolute value, is larger than or equal to the upper 90% quantile of the absolute value of the regression coefficients or scores, respectively. We also define as false positives those markers with smaller  P -values, larger Bayes factors, or smaller absolute regression coefficients or scores lying outside a window of 100 kb either side of the causal variant. 3.3 Results across a range of simulation scenarios Figure 2  shows the localization performance of the different methods. Results are averages over 50 simulated datasets under a default scenario, with 2000 sample size, 1.6 HRR(Aa), MAF of causal allele 5% and 50% censoring. Overall, BETA-Surv and univariate Cox perform better than PCRCox and SUPERPC. For BETA-Surv and univariate Cox, there were no significant differences in the distribution of distances for reasonable location errors, with univariate Cox showing advantage for distances &gt;100 kb.
 Fig. 2. Distribution of distances between the association peak and the causal SNP. Analysis of 50 datasets simulated with sample size equal to 2000, 1.6 HRR(Aa), MAF of causal allele 5%, and 50% censoring. Figure 3  reports the power and false positive rates of the methods for the 50 datasets simulated under the default scenario. Univariate Cox and PCRCox have the highest power, but the worst performance in false positives. BETA-Surv and SUPERPC have similar power, but BETA-Surv has the lowest false positive rate, which is almost as low as Bonferroni-corrected univariate Cox.
 Fig. 3. Probability of detecting true associations ( A ) and false positive rate ( B ). True positives are significant SNPs within some physical distance with the causal allele, otherwise they are classified as false positives. Analysis of 50 datasets simulated with sample size equal to 2000, 1.6 HRR(Aa), MAF of causal allele 5%, and 50% censoring. In  Supplementary Figure 1 , we plot the power and false positive rates of the different methods across the various simulation scenarios and over the 50 replicates. In each plot, we vary a simulation parameter along the  x -axis whilst assuming default values for the remaining ones. Uncorrected single locus test and PCRCox are the most powerful approaches, having however the worst performance in terms of false positives. The false positive rates for BETA-Surv are very low under all simulation scenarios. The same conclusions apply when we vary the censoring level. The choice of a 100 kb window is sensible but arbitrary; a 200 kb window was also investigated and did not alter conclusions about both power or false positives. Also, using the upper 95% quantile of the absolute standardized regression coefficients or scores for PCRCox and SUPERPC, instead of the 90%, results in lower power and lower false positive rates for these methods, but BETA-Surv has still by far the best performance in terms of false positives. To show the trade-off between power and false positive rate for the different methods using varying significance thresholds, we plot ROC curves for the different simulation scenarios ( Fig. 4  and  Supplementary Figs 2–5 ). The threshold for univariate Cox regression ranges between Bonferroni correction and 0.05, for BETA-Surv from 1.1 to 10, and for PCRCox and SUPERPC from 60% to 100%. The graphs show that BETA-Surv and univariate Cox regression are the best performing methods. Compared to univariate Cox regression, BETA-Surv has a small advantage in power when the association signal is weak, while it performs similarly for stronger signals. However, it has lower false positive rate in every simulation scenario. Overall, SUPERPC has the worst performance in power, whereas PCRCox has the worst performance in false positive associations.
 Fig. 4. Power versus false positive rate for different significance thresholds. Analysis of 50 datasets simulated with sample size equal to 2000, 1.6 HRR(Aa), MAF of causal allele 5%, and 50% censoring. Finally, we constructed 50 datasets under a null model of no disease association and we calculated the false positive rate. For the univariate analysis, this was 5.39% ( P -value ≤0.05) and 0 when using Bonferroni correction, while BETA-Surv resulted in a false positive rate of 0.634% (Bayes factor ≥3). PCRCox and SUPERPC had a 10% and 4.80% false positive rate using the upper 90% quantile, and 5% and 2.46% using the upper 95% quantile, respectively. 3.4 Prospective epilepsy study The ABC proteins are a superfamily of efflux pumps that extract several classes of drugs from the eukaryotic cell. The ABC transporters are currently the focus of a major effort to determine their role in mediating drug resistance in a variety of human diseases including cancer (Ambudkar  et al. ,  2003 ), HIV (Sankatsing  et al. ,  2004 ) and epilepsy (Schmidt and Loscher,  2005 ). Retrospective studies have reported associations between epilepsy treatment outcome and drug transporter genes (Siddiqui  et al. ,  2003 ; Zimprich  et al. ,  2004 ), using individual SNPs or 3-SNP haplotypes. However, results have been contradictory (Sills  et al. ,  2005 ; Tan  et al. ,  2004 ). Leschziner  et al.  ( 2006b ) analyzed data from the UK SANAD prospective study for the gene complex ABCB1/ABCB4. They used single SNP log-rank tests, 3-SNP haplotype analyses and Cox multiple regression with stepwise selection on a subset of the genotypes (due to the problems of SNP collinearity and over-fitting), and observed no significant genetic association. Here, we use our proposed method to simultaneously analyze five drug transporters genes. For a prospective cohort of 503 epilepsy patients from the SANAD study, 500 potential SNPs were genotyped across five ABC transporter genes (ABCB1/ABCB4, ABCC1, ABCC2 and ABCC5 located in 7q21.12, 16p13.1, 10q24 and 3q27 respectively). Details of genotyping, SNP identification and LD structure are given in Leschziner  et al.  ( 2006a ). Of the 500 loci identified, only 317 were polymorphic with ∼60% of SNPs with MAF ≥5%. SNPs with ≤1% MAF, showing evidence of Hardy Weinberg disequilibrium (HWD χ 2 1 ≥ 12) or with ≥10% missing per SNP were discarded, leaving a total of 235 SNPs for analysis ( Table 2 ). Treatment outcomes were prospectively recorded for patients commencing anti-epileptic drug therapy. Here we concentrate on two outcomes: time to 12 month remission (512 observed and 494 censored events), and time to withdrawal from drug due to unacceptable adverse side-effects (194 observed and 812 censored events).
 Table 2. Summary of ABC genotype SANAD data Gene size (kb) SNPs Mono MAF&lt;1% HWD Miss&gt;10% Used ABCB1/4 283 186 73 22 3 5 85 ABCC1 193 162 50 12 5 3 92 ABCC2 69 76 32 19 2 3 24 ABCC5 98 76 28 8 4 2 34 Total 643 500 183 61 14 13 235 We used PHASE (Stephens  et al. ,  2001 ) to phase the genotypes. Each haplotype pair was chosen at random according to its posterior probability. To account for phase uncertainty, we repeated the above procedure 10 times and we analyzed each of the 10 datasets separately. Gene region ABCB1/ABCB4 consists of 54 trees, and genes ABCC1, ABCC2 and ABCC5 of 67, 14 and 27 trees, respectively averaged over the 10 datasets. For time to 12 month remission, the proposed method yielded no evidence of association for ABCB1/ABCB4 and ABCC5 regions for any of the datasets. For gene ABCC1, markerwise Bayes factors from BETA-Surv and (−log) P -values from univariate Cox regression, averaged over the 10 datasets, are given in  Figure 5 . Positive hits from BETA-Surv were observed at 7495.248, 7497.311 and 7536.426 kb with average Bayes factors 3.68, 7.09 and 8.84, respectively. For gene ABCC2, both BETA-Surv and Cox regression identified the variant at position 20312.532 kb (average Bayes factor 4.08 and  P -value 0.03).
 Fig. 5. Results for gene ABCC1 from BETA-Surv and univariate Cox regression for the SANAD epilepsy cohort with outcome ‘time to 12 month remission’. Bayes factor in favour of association at each marker from BETA-Surv ( A ), and  P -values from Cox regression ( B ) averaged over the 10 datasets. For time to withdrawal due to adverse side-effects, the proposed method yielded no evidence of association for ABCB1/ABCB4 and ABCC2 regions for any of the datasets. For gene ABCC1, BETA-Surv found positive association at 7548.444 kb with average Bayes factor 3.36, whereas Cox regression reported the SNP at 7549.729 kb with average  P -value 0.026. For gene ABCC5, both BETA-Surv and Cox regression identified the variant at position 90194.67 kb in all 10 datasets (average Bayes factor 5.1 and  P -value 0.03). Generally, BETA-Surv and Cox regression yielded similar results. There is positive but not strong evidence of association between some ABC transporter genes and epilepsy treatment outcomes. To the best of our knowledge, there are no other reported analyses or associations between genes ABCC1, ABCC2 or ABCC5 and epilepsy. 4 DISCUSSION We have presented a method to analyze genetic association studies with time-to-event outcomes. Cohort studies are a useful and increasingly common study design, but there is a noticeable lack of statistical methods for their analysis. The method presented here is best suited for densely genotyped candidate gene regions and can easily handle large number of individuals and markers. Compared to univariate Cox regression and multi-marker dimension reduction techniques, our method performs similarly in terms of localization, while offering clear advantages in terms of false positive associations. Moreover, it runs fast and it offers computational advantages especially over methods that rely on cross-validation to determine model parameters. Here, we assume that survival times can be modelled parametrically by the exponential distribution within each cluster. The use of such a simple distribution may seem restrictive, but offers computational advantages over more complicated models. We have also used the Weibull distribution to model the survival outcome, which in simulation studies offered no significant additional advantages, while increasing the running times of the method. The incorporation of environmental covariates in the model is an issue that has not been investigated in this work. One possible way of dealing with this, is by fitting a cluster-specific survival regression using the exponential distribution. Finally, phase uncertainty in haplotype reconstruction from genotype data could be incorporated in the analysis by adding another step in the MCMC algorithm and sampling from the different haplotype reconstructions at each MCMC step before performing the rest of the analysis for the chosen phase. However, this approach is likely to add significant computational burden. A simpler approach, and one we adopt in the application to the real data, consists in repeating the analysis for a number of different haplotype reconstructions and average the results. 
          Supplementary Material 
          
             
                [Supplementary Data] 
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
          
       </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>MFCompress: a compression tool for FASTA and multi-FASTA data</Title>
    <Doi>10.1093/bioinformatics/btt594</Doi>
    <Authors>Pinho Armando J., Pratas Diogo</Authors>
    <Abstract>Motivation: The data deluge phenomenon is becoming a serious problem in most genomic centers. To alleviate it, general purpose tools, such as gzip, are used to compress the data. However, although pervasive and easy to use, these tools fall short when the intention is to reduce as much as possible the data, for example, for medium- and long-term storage. A number of algorithms have been proposed for the compression of genomics data, but unfortunately only a few of them have been made available as usable and reliable compression tools.</Abstract>
    <Body>1 INTRODUCTION Saying that the volume of genomic data produced every day is large is clearly a euphemism. With the dramatic drop in price of the sequencing machines (the $1000 limit for sequencing a human genome will be history shortly), virtually everyone will want to sequence everything. Unfortunately, the pace at which storage and communication resources are evolving is not enough, and the genomic data centers are being flooded with data. It is a data deluge ( Berger  et al. , 2013 ). The interest for DNA compression was started with the Biocompress algorithm of  Grumbach and Tahi (1993) . The subsequent two decades have seen the publication of a considerable number of algorithms for compressing DNA sequences and several other forms of genomic data (e.g.  Bonfield and Mahoney,2013 ;  Cao  et al. , 2007 ;  Cox  et al. , 2012 ;  Hach  et al. , 2012 ;  Jones  et al. , 2012 ;  Korodi and Tabus,2007 ;  Matos  et al. , 2013 ;  Pinho  et al. , 2011 , 2012 ;  Popitsch and Haeseler, 2013 ), although usually aiming more at proving the concept than at providing usable compression tools. For example, the majority of these algorithms assume data drawn from the four-letter alphabet, ACGT, ignoring other letters that can be found in DNA data sequences. No doubt that for the purpose of showing the potentialities of the algorithms, this is often a fair approach, because those letters outside the main alphabet are usually rare and, therefore, represent only a small fraction of the bits required to represent the sequence. However, a usable lossless compression tool needs to be capable of handling every letter that it finds in the file and to reproduce it exactly during decompression. Moreover, genomic files rarely contain only sequence information. Usually, they also include additional data, such as headers, quality scores and alignment information. Therefore, the compression tools need to compress these data as well in an efficient way. In this article, we describe MFCompress, a tool for compressing FASTA and multi- FASTA files. Recently,  Mohammed  et al.  (2012)  proposed DELIMINATE, also a compression method for FASTA and multi-FASTA files, which relies on a preprocessing stage, where header and sequence data are separated and transformed, followed by a general purpose compressor (7-Zip). The MFCompress tool described here provides better compression than DELIMINATE for the large majority of the files used in the benchmarking dataset, at a similar compression and decompression time. This dataset is an extended version of the benchmarking dataset used by  Mohammed  et al.  (2012) , to which we added some larger files, due to its increasing importance and commonness. MFCompress relies on multiple competing finite-context models and arithmetic coding, a powerful approach for DNA data compression ( Pinho  et al. , 2011 ). 2 METHODS The compression tool that we describe in this article relies on probabilistic models (finite-context models) that comply to the Markov property, i.e. that estimate the probability of the next symbol of the information source using the   immediate past symbols (order- k  context) to select the probability distribution. MFCompress uses single finite-context models for encoding the header text, as well as multiple competing finite-context models for encoding the main stream of the DNA sequences ( Pinho  et al. , 2011 ). The compression algorithm divides the data source into two separate sub-sources: one containing the headers of the FASTA records, the other one the sequences. The sub-source that deals with the sequences may be further divided into two or three streams ( Supplementary Fig. S4  of the  Supplementary Material ): the main stream, the extra stream and the case stream. The  main stream  is a four-symbol information source, conveying most of the information of the four DNA bases. Both upper and lower case characters representing the four DNA bases are converted to this four-symbol alphabet. If characters other than the four DNA bases are also present, they are all mapped to the ‘0’ symbol in the main stream. When the sequences contain other characters besides the DNA bases, another coding stream must be present to disambiguate the occurrences of the ‘0’ symbol in the main stream. This  extra stream  is responsible for representing all non-acgt/ACGT characters that have been found in the sequences, as well as to indicate when the ‘0’ in the main stream is an a/A DNA base. If the sequences contain both DNA bases in upper and lower case, an additional binary symbol is associated to each symbol in the main stream, indicating the respective case type (the  case stream ). A more detailed description of the methods is provided in the  Supplementary Material . 3 RESULTS AND DISCUSSION In the  Supplementary Material , we provide compression results obtained using several popular general purpose compression methods, namely gzip, bzip2, ppmd and lzma (the last two using the versions implemented in the 7z archiver), as well as by the recent special purpose compressor DELIMINATE ( Mohammed  et al. , 2012 ) and by the compressor that we describe in this article. Supplementary Table S1  in the  Supplementary Material  shows the total compressed file size, in bytes, obtained with gzip in the FFN and FNA datasets (composed of all bacteria in the NCBI), as well as the compressing gains attained by the other methods in relation to gzip. We can see that MFCompress provides a compression gain of ∼3.5% in relation to DELIMINATE for the FFN dataset and of ∼4% for the FNA dataset. Compared with gzip, the compression gain of MFCompress is ∼25%. In  Supplementary Table S2  of the  Supplementary Material , we present the compression results for the human genome dataset (HG19). The gain of the default mode of MFCompress is marginal in comparison with DELIMINATE (only 1.8%). For the more complex coding mode, the gain is ∼3.3%. In relation to gzip, the gain is &gt;34%. Regarding the CAMERA dataset, in  Supplementary Table S4  of the  Supplementary Material , we provide compression results regarding the 26 files that have been used in this dataset, showing significant gains over DELIMINATE for most of them. To give a wide range of examples, we chose files with sizes from ∼  to &gt;  characters, i.e. covering five orders of magnitude. For three of these files, DELIMINATE was not able to provide reliable results: in two cases the decoded file was different from the original file and in one case the encoder crashed. In relation to gzip, the size was reduced to almost half. In  Supplementary Table S5  of the  Supplementary Material , we show the compression results of two highly redundant datasets (all  Escherichia  and  Salmonella  genomes of the FNA dataset). In this case, MFCompress attained an 8-fold file size reduction over gzip and &gt;44% gain in relation to DELIMINATE. 4 CONCLUSION For daily use, general purpose compression tools, such as gzip, may continue to play an important role in the context of genomic data processing, mainly due to its pervasiveness and relatively good speed. However, as shown in this article, special purpose compression tools can sometimes attain additional file reductions as large as 50% or even more, in relation to gzip. In our opinion, the possibility to virtually double the amount of sequence data that can be stored in a given space, exclusively by means of software compression tools, is an opportunity worthy of consideration by the genomic laboratories. Higher compression can only be obtained using more complex algorithms, often requiring some more time and memory to run. However, these additional requirements are compensated by the relief attained in terms of storage requirements. In conclusion, we believe that the compression tool reported in this article is a relevant contribution to slow down the negative impact of the data deluge that we are facing nowadays. Funding :  European Fund for Regional Development  ( FEDER ) through the  Operational Program Competitiveness Factors (COMPETE)  and by the  Portuguese Foundation for Science and Technology  ( FCT ), in the context of projects  FCOMP-01-0124-FEDER-022682  (FCT reference PEst-C/EEI/UI0127/2011) and  Incentivo/EEI/UI0127/2013 . Conflict of Interest : none declared. 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>New algorithms to represent complex pseudoknotted RNA structures in dot-bracket notation</Title>
    <Doi>10.1093/bioinformatics/btx783</Doi>
    <Authors>Antczak Maciej, Popenda Mariusz, Zok Tomasz, Zurkowski Michal, Adamiak Ryszard W, Szachniuk Marta, Valencia Alfonso</Authors>
    <Abstract/>
    <Body>1 Introduction Understanding the RNA structure is crucial for learning the principles of RNA folding, its regulatory impact on transcription and translation, catalytic properties, specificity of RNA-protein interactions and viral infectivity ( Mortimer  et al. , 2014 ). The RNA folding process has been shown to follow a hierarchical pathway in which domains are assembled sequentially ( Batey  et al. , 1999 ;  Brion and Westhof, 1997 ;  Mustoe  et al. , 2014 ;  Fig. 1 ). At first, upon folding of RNA strand, its selected nucleotide residues interact through base-pairing to form diverse secondary structure motifs like hairpin apical loops, bulges, internal and n-way junction loops, separated by stems that consist of stacked Watson-Crick and GU wobble base pairs mostly. By that means, the secondary structure is established at the molten globule state ( Brion and Westhof, 1997 ). Subsequently, intramolecular tertiary interactions position the secondary structure elements with respect to each other, often bringing nucleotide residues from distant molecule parts to a close contact and initiating formation of structure motifs called pseudoknots. This generates the conformational space of RNA three-dimensional (3D) structures, to be related to their biological functions ( Guo and Cech, 2002 ;  Woodson, 2002 ).
 Fig. 1. Subsequent tiers of cyanocobalamin aptamer (1DDY, chain A) folding pathway from ( a ) single-stranded form, through creation of ( b ) a hairpin and ( c ) first order pseudoknot of H-type, to ( d ) the final structure with second order pseudoknot of L-type Large RNAs often contain pseudoknots, classified both on secondary and tertiary structure level. They occur when loop- or bulge-involved nucleotides pair with a single-stranded region outside to form a double helical segment. In general, four basic types of pseudoknots have been distinguished: H-type (loop—single-stranded region outside of the loop), K-type (loop—loop interaction), L-type and M-type (being more complex pseudoknots) ( Kucharík  et al. , 2015 ). As the name suggest, pseudoknots are not real knots. Although pulling 5’ and 3’ ends of the RNA strand, pseudoknot yields a fully stretched chain, whereas physical knot tightens. Formation of pseudoknots makes RNA structures more compact and is often linked to biological function(s) attributed to that particular motif ( Cho  et al. , 2009 ). As the first observation of pseudoknot in turnip yellow mosaic virus structure ( Rietveld  et al. , 1982 ), this motif and its biological functions were intensively studied ( Staple and Butcher, 2005 ). It was shown ( Antczak  et al. , 2014 ;  Chiu and Chen, 2012 ) that over 53% of RNA structures in Protein Data Bank ( Berman  et al. , 2000 ) contain pseudoknots. Among them 20% has simple H-type pseudoknots formed by two conflicting regions only, the remaining RNAs include more complex pseudoknots. Structure complexity in this sense corresponds to the tiers in RNA folding hierarchy ( Mustoe  et al. , 2014 ). Simple pseudoknots appear first, followed by formation of more complicated ones when folding process advances. Such hierarchy is exemplified in  Figure 1  which illustrates consecutive steps of cyanocobalamin aptamer folding along two alternative pathways. At first, the basic secondary structure including hairpin apical loop (left pathway) or hairpin and internal loops (right pathway) is established from a single-stranded form ( Fig. 1b ). Next, simple H-type pseudoknot is formed ( Fig. 1c ). In the final step, more complex L-type pseudoknot is created ( Fig. 1d ). The secondary structure on each level of  Figure 1  has been encoded in dot-bracket notation and visualized by PseudoViewer ( Byun and Han, 2009 ). The 3D structures in  Figure 1(a–c)  have been generated by RNAComposer ( Popenda  et al. , 2012 ), while  Figure 1d  displays the X-ray structure of cyanocobalamin aptamer ( Sussman  et al. , 2000 ). The tertiary structures have been visualized in PyMOL ( DeLano, 2002 ) using the rainbow scale to label consecutive residues from 5’- (blue) to 3’-end (red). Despite the considerable accumulation of experimental data and bioinformatics studies addressing pseudoknot problems, there are still some unresolved issues. For example, difficulties and ambiguity in pseudoknot encoding resulted in the fact that many computational methods cannot reliably handle them. Therefore, several algorithms have been developed to extract the core structure including nested base pairs only, by removing pseudoknots ( Chiu and Chen, 2015 ;  Smit  et al. , 2008 ). However, identifying which of two conflicted helical regions is responsible for a pseudoknot formation, and, thus, should be removed, is not an obvious procedure. For example, extraction of a nested structure for RNA shown in  Figure 1d  may end up in obtaining one of two structures displayed in  Figure 1b  which vary significantly. It has been agreed that an important decisive factor––although not always the only one––is the region’s length (number of base pairs in the region). That is, if we consider two conflicted double-stranded regions, the shorter one is regarded to have initiated pseudoknot formation, while the longer region is within the basic structure. Such simple rule has been followed i.a. in ( Antczak  et al. , 2014 ;  Ponty, 2006 ;  Popenda  et al. , 2008 ;  Rybarczyk  et al. , 2015 ;  Smit  et al. , 2008 ), mostly applying fast greedy procedures sufficient to solve optimally not complicated structures (with H- and K-type pseudoknots). The problem becomes harder if conflicted regions have the same length and when pseudoknot involves more than two regions, like it is observed in L- and M-type pseudoknots. For such cases, greedy algorithms do not guarantee the optimal solution. Highly conflicted sub-structures have a significant impact on structure-based analysis, especially if this is made by automated, computational approaches. For many years, also text representation of their topology has been ambiguous. Conventional parentheses notation allowed to encode nothing more besides a nested RNA structure topology, and the first version of extended dot-bracket notation allowed to handle simple pseudoknots only ( Byun and Han, 2009 ;  Hofacker  et al. , 1994 ). This situation resulted in slower than expected progress in the field of secondary-structure-based 3D structure prediction of pseudoknotted RNAs as well as in their annotation from 3D data ( Miao  et al. , 2017 ;  Purzycka  et al. , 2015 ). To advance studies in these directions, it is necessary to have an access to a reliable representation of RNA secondary structure with complex pseudoknots. Here, we propose new algorithms that can handle such RNAs and process them on the secondary structure level. Our methods are based on exhaustive search approach and provide exact (optimum) solution. They operate on BPSEQ-formatted data, handling all base pairs listed in the input file regardless of their types. They identify, count and classify pseudoknots and encode them in dot-bracket notation which reflects the RNA structure topology and hierarchy of the folding process. These new algorithms have been implemented within RNApdbee web server ( http://rnapdbee.cs.put.poznan.pl ), where they support the route from RNA 3D structure to secondary structure. They can be also run separately to allow the user for conversion of BPSEQ data to dot-bracket notation and graphical view of the secondary structure. 2 Materials and methods The basic way of describing the RNA secondary structure is to list base pairs (e.g. in BPSEQ format), which are formed during the molecule folding to stabilize the structure. Usually, such base pairs are formed surrounded by other pairs, thus, creating longer double-stranded regions. Occasionally, single isolated base pairs occur in RNA structures. A double-stranded (paired) region contains only nested base pairs. Two base pairs ( i, i’ ) and ( j, j’ ) are nested if  i &lt; j&lt;j’&lt;i’ . However, sometimes we can find base pairs––we will call them crossed or conflicted––which form pseudoknot(s). A pseudoknot occurs if for any pair ( i, i’ ) there exists another one, ( j, j’ ), such that and  i &lt; j&lt;i’&lt;j’  ( Studnicka  et al. , 1978 ). It is believed that in the process of RNA hierarchical folding nested base pairs are formed at first, while pseudoknotted ones bind in the next steps. For many years, complete, unambiguous representation of pseudoknots in text and graphical form has been a non-trivial problem, especially in the case of highly conflicted structures. In ( Popenda  et al. , 2008 ), we have introduced our first method for encoding pseudoknotted RNA structure in dot-bracket notation extended to dot-bracket-letter (DBL) ( Table 1 ). DBL allowed to encode various pseudoknots, e.g. H-type:  (   [   )   ] , K-type:  (   [   )   (   ]   ) , L-type:  (   [   {   )   ]   }  and M-type pseudoknot:  (   [   {   )   (   ]   }   ) . The new method aimed to generate and clearly present DBL representation of the secondary structure topology based on the input BPSEQ data. All canonical and non-canonical base pairs listed in the input BPSEQ file were handled similarly. This First-Come-First-Served (FCFS) algorithm ( Algorithm 1 ) was applied within the workflow of RNA FRABASE 1.0 ( Popenda  et al. , 2008 ). Each double-stranded region was handled in the order determined by its first residue number, due to the residue arrangement in RNA sequence from 5’ to 3’-end. Every region was assigned an order ( regorder ) that translated into characters used to represent all of its base pairs in DBL notation ( Table 1 ). Starting from the first in line region, which was assigned  regorder  =   0, the succeeding regions were pushed onto order-labelled stack(s) in order of their appearance in RNA sequence. If the newly pushed region was conflicting with anything already on current stack, a global order value was increased by 1, order-labelled stack (if non-existent) was created and current region pushed onto it with  regorder  =   global order assigned. Once the closing of a region was found, the region was popped from its stack.
 Table 1. Base pair encoding in DBL notation Region order ( regorder ): 0 1 2 3 4 5 6 7 8 Base pair representation: () [] {}  &lt; &gt; A a B b C c D d E e Algorithm 1  FCFS algorithm from RNA FRABASE 1.0 Input: ssin  – RNA secondary structure in  BPSEQ  format Output: ssout  – RNA secondary structure in  DBL  notation 1:  function  FCFS( ssin ) 2:   regs ← findAllPairedRegions ( ssin ) 3:   n  ← | regs |           ▹ count paired regions 4:   sortRegionsByStartPoint(regs) 5:   setRegionOrders ( regs ,  n )   ▹ assign orders to regions 6:   ssout ← encodeBasePairs ( regs )  ▹ encode solution in  DBL 7:   return ssout 8:  end function 9: 10:  procedure  setRegionOrders( regs ,  n ) 11:   r e g s [ 1 ] . o r d  ← 0 12:   for i ← 2   to   n do 13:    order ←  0 14:    for j ← 1   to   i − 1 do 15:     if r e g s [ j ] . o r d = o r d e r  AND 16:      c o n f l i c t e d ( r e g s [ j ] , r e g s [ i ] ) then 17:      order ← order  + 1 18:     end if 19:    end for 20    r e g s [ i ] . o r d  ←  order 21:   end for 22:  end procedure In ( Antczak  et al. , 2014 ), we have introduced a concept of a pseudoknot order and we have applied it in RNApdbee tool to compute orders of pseudoknot-forming regions. Following the approach presented in ( Smit  et al. , 2008 ), we have defined the pseudoknot order as a minimum number of base pair set decompositions resulting in a nested structure. Thus, for example, if there is a pseudoknot structure involving three conflicted double-stranded regions,  A ,  B ,  C  and a decomposition of one region (preferably one including the least number of base pairs)––e.g.  B ––leads to a structure without conflicts, then the pseudoknot has an order equal to 1 ( psorder  =   1). Based on that, we can assign region orders in the following way. Region  B  selected for decomposition has  regorder  =   1 (the same as pseudoknot order), and the remaining regions,  A  and  C , have  regorder  =   0 (since after decomposition they are not crossed). In general, a pseudoknot with  psorder  =  k  consists of regions with  regorder  =   0… k . The maximum order among regions involved in a pseudoknot is a pseudoknot order. Thus, H- and K-type pseudoknots are topologically simple with  psorder  =   1, while more complex L- and M-type pseudoknots have  psorder  =   2. To compute pseudoknot orders and region orders (for the purpose of their further encoding and visualization), a modified version of Elimination Gain (EG) heuristics introduced in ( Smit  et al. , 2008 ) was incorporated into RNApdbee. EG application results in obtaining RNA secondary structure topology which maximizes the length of double-stranded regions with small order value. However, since EG is based on a greedy approach, it does not guarantee finding an optimal solution. Thus, we have developed a Dynamic Programming (DP) algorithm applying the same criterion and we have compared its performance with EG heuristics. Further study of RNApdbee-annotated secondary structures has led us to consider alternative criterion of optimality, which is a minimum pseudoknot order throughout the whole structure. Hence, we have proposed a new criterion function and we designed new algorithms to encode pseudoknotted RNA structures in DBL notation. The detailed description of our new algorithms is provided in the next section. 3 Algorithms The presented algorithms apply different approaches to solve the problem of pseudoknot identification and classification, and pseudoknotted RNA secondary structure encoding. The Hybrid algorithm (HYB) combines heuristic and exact procedures. DP finds the solution by treating succeeding sub-problems. Each method optimizes solution with reference to own criterion function. The function used in DP (Section 3.1) aims to maximize the number of non-conflicted base pairs at each computational step. Function in HYB (Section 3.2) combines maximization of nested base pair number with minimization of the highest pseudoknot order for the entire structure. 3.1 Criterion function I All existing heuristics for pseudoknot identification and removal [EG, Elimination Conflict (EC), etc.] that we have tested follow the same criterion to evaluate representation  R ( S ) of RNA secondary structure  S . It is defined by function  fscoreI :
 (1) f s c o r e I ( R ) = ∑ 1 ≤ i ≤ n , o r d e r ( r e g i ) = 0 l e n g t h ( r e g i ) 
where  length ( reg i ) denotes a length (number of base pairs) of the  i -th region, order( reg i ) stands for the  i -th region order and  n  is a number of paired regions in structure  S . The function (Formula 1) sums up lengths of all non-conflicted double-stranded regions in  S . The representation  R ( S ) with a maximum value of  fscoreI  wins. When the above-mentioned methods are used to determine pseudoknot orders, we run them iteratively. In every  j -th iteration ( j  =   0, 1, 2,…),  fscoreI  is applied to select the maximum nested sub-structure. All non-conflicted regions in the best solution are assigned  regorder  =  j  and removed from structure  S . Next iteration is processed with the reduced representation of  S  to identify regions with  regorder  =  j  +   1, etc. 
 fscoreI  has been also applied to optimally evaluate partial solutions in newly developed DP algorithm (Section 3.3). 3.2 Criterion function II An analysis of the results obtained by methods applying  fscoreI  for complex RNA structures made us propose a modified version of the criterion function. It is defined as two-element vector function  fscoreII , where each element is a weighted sum of lengths of particular double-stranded regions in  S :
 (2) f s c o r e I I ( R ) = [ ∑ i = 1 n ( 1 − x i ) · l e n g t h ( r e g i ) − 1 · ∑ i = 1 n o r d e r ( r e g i ) · l e n g t h ( r e g i ) ] where   x i = { 0 ,   if   o r d e r ( r e g i ) = 0 1 ,   otherwise 
The components of  fscoreII  are defined as follows:  n  denotes a number of paired regions in structure  S ;  length ( reg i ) stands for a length of the  i -th region; order( reg i ) is the  i -th region order (represented by appropriate character in DBL);  x i  is an auxiliary variable. In practice, the first element of the vector is the same as criterion function defined by Formula 1, i.e.  fscoreII [1] =  fscoreI . It sums up the lengths of all non-conflicted regions (with  regorder  = 0). The second element,  fscoreII [2], is to sum up the lengths of conflicting regions multiplied by their orders. As, we aim to penalize  R ( S ) for high order regions, the sum in  fscoreII [2] is taken with a negative value. Looking for a representation of structure  S , we maximize values of both vector elements. Having two representations,  R 1( S ) and  R 2( S ), we define the following domination rule to decide which one is better:
 (3)   if   f s c o r e I I [ 1 ] ( R 1 ) &gt; f s c o r e I I [ 1 ] ( R 2 )   or   ( f s c o r e I I [ 1 ] ( R 1 ) = f s c o r e I I [ 1 ] ( R 2 )       and   f s c o r e I I [ 2 ] ( R 1 ) &gt; f s c o r e I I [ 2 ] ( R 2 ) )   then   R 1 ( S )   dominates   over   R 2 ( S ) 
Two example representations of the secondary structure of cyanocobalamin (vitamin B12) aptamer (1DDY, chain A),  R 1 provided by HYB and  R 2 output by FCFS, are shown in  Figure 2 . As it can be seen from  fscoreII  values,  R 1 is better than  R 2, since it dominates on both vector elements (although, according to Formula 3, a domination on  fscoreII [1] is sufficient for  R 1 to be the winner). The difference between  R 1 and  R 2 is already in location of zero-order regions. If these results were considered by the procedure aimed to obtain the nested structure by pseudoknot removal, we would observe significant differences at the level of both the secondary and the 3D structure.  Figure 1b  shows nested structure of cyanocobalamin aptamer that can be obtained by removal of pseudoknots identified by FCFS (left) and HYB (right).
 Fig. 2. DBL representations of cyanocobalamin aptamer (1DDY, chain A) secondary structure encoded by ( a ) HYB and ( b ) FCFS, the corresponding arc diagrams and  fscoreII  values 3.3 DP algorithm DP method ( Algorithm 2 ) follows the optimality principle formulated by Richard Bellman ( Bellman, 1952 ). The problem is broken into time separable sub-problems and the solution is accomplished by recursively solving Bellman’s equations. In our case, the sub-problem lies in the classification of a single base pair. The main DP procedure iteratively runs four operations: (i) find a set of nested base pairs in the input set, (ii) associate found base pairs with current order (initially set to 0) and add them to the solution, (iii) increase current order and (iv) remove obtained base pairs from the input set. In each iteration, an optimum subset of nested base pairs is found according to criterion function  fscoreI  (Formula 1). The algorithm stops when all base pairs are moved from the input set to the solution. The first step is a key part of the algorithm. It starts from reading current input set ( ssin ) in BPSEQ format and preparing the data ( treatBasePairSet ). The latter includes: (i) base pair renumeration, (ii) addition of virtual edge pair and (iii) sorting base pairs with respect to the distance between indexes of paired residues (firstly) and first residue index (secondly) ( Supplementary Fig. S1 ). Next, two DP matrices are allocated and filled recursively with numerical weights. We use  scoreMtx  matrix to store the ratings of consecutive optimum solutions (nested sets). A cell in  indexMtx  matrix keeps an index of the closing residue of outermost base pair in currently analyzed nested set. A change in  scoreMtx  initiates the corresponding modification in  indexMtx . There are three operations followed in filling the cells of  scoreMtx :
 O1: s c o r e M t x [ i +1 ] [ j ] =max { s c o r e M t x [ i +1 ] [ j ] , s c o r e M t x [ i +1 ] [ j -1 ] } O2: s c o r e M t x [ i +1 ] [ j ] =max { s c o r e M t x [ i +1 ] [ j ] , s c o r e M t x [ i +1 ] [ j ′ -1 ] + s c o r e M t x [ j ′ ] [ j ] } O3: s c o r e M t x [ i ] [ i ′ ] =1+ s c o r e M t x [ i +1 ] [ i ′ -1 ] Their application depends on mutual position of considered base pairs, i.e. for every two base pairs  ( i , i ′ ) ,   ( j , j ′ ) ∈ s   sin ,   i + 1 &lt; j &lt; i ′ − 1 :
 if  ( j , j ′ )  is nested and  i &lt; j &lt; j ′ &lt; i ′  ( Supplementary Fig. S2a ), or  ( i , i ′ ) ,   ( j , j ′ )  are in conflict ( Supplementary Fig. S2b ) we perform operation  O 1, if  ( j , j ′ )  is nested,  j ′ &lt; j  ( Supplementary Fig. S2c ), and  s c o r e M t x [ i +1 ] [ j ′ –1 ] + s c o r e M t x [ j ′ ] [ j ] &gt; ( s c o r e M t x [ i +1 ] [ j –1 ]  and  s c o r e M t x [ i +1 ] [ j ] ) we apply operation  O 2, if  ( j , j ′ )  is nested,  j ′ &lt; j  ( Supplementary Fig. S2c ), and  s c o r e M t x [ i +1 ] [ j ′ –1 ] + s c o r e M t x [ j ′ ] [ j ]  &lt; = ( s c o r e M t x [ i +1 ] [ j –1 ]  or  s c o r e M t x [ i +1 ] [ j ] ) we apply operation  O 1. Finally, for every  ( i , i ′ )  the algorithm performs operation  O 3. After filling the matrices, optimum solution (nested set) is back-tracked from  indexMtx . Starting from  i  =   1,  i ′ = n , if  i n d e x M t x [ i ] [ i ′ ] = k ′  and  k ′ ≠ N N , then  k ′  is the closing residue number of base pair  ( k , k ′ )  in the solution. The opening residue,  k , is gained from raw Bps . Next, the procedure continues recursively into  i n d e x M t x [ k +1 ] [ k ′ –1 ]  and  i n d e x M t x [ i ] [ k –1 ] , until stepping into not set cell ( NN ) ( Supplementary Fig. S1 ). 3.4 Hybrid algorithm HYB that we introduced ( Algorithm 3 ), combines two procedures, exhaustive search ( exSearch ) and random walk ( randWalk ), run depending on the number of conflicting regions in the pseudoknot structure. In the pre-processing stage, it identifies all regions which are not pseudoknot-involved. They obtain a zero order and are disregarded in further steps. Next, the algorithm finds disjoint pseudoknots. Two pseudoknots,  P 1 and  P 2, are disjoint if no region involved in the formation of  P 1 is in conflict with any region in  P 2. Disjoint pseudoknots are processed separately. All conflicting regions which form one pseudoknot are stored in single container. In detail, one container is a vector of region identifiers and corresponds to a chain of regions’ decompositions leading to a nested structure. A single container is processed iteratively to find the best decomposition chain. In every iteration, the vector is permuted by either  exSearch  or  randWalk . Next,  setRegionOrders  ( Algorithm 1 ) assigns orders to regions, and the solution is scored using  fscoreII  (Formula 2). The best permutation for the container is selected due to the domination rule (Formula 3). Thus, for each container one permutation is obtained. They are merged to create final solution for the input structure. If the container includes up to eight conflicted regions, the  exSearch  procedure is used to produce succeeding solutions and to find the exact one, being the global optimum. That is, all permutations of regions within container are generated, order-assigned and scored. Then, the best one is selected as an optimum solution. At most, if the container stores eight regions,  exSearch  has to handle 40 320 solutions. Otherwise,  randWalk  is launched. This method generates and scores MAX_ITERATIONS = 10 000 random permutations, and provides the user with sub-optimal solution ( Supplementary Fig. S3 ).
 Algorithm 2  DP algorithm Input: ssin  – RNA secondary structure in  BPSEQ  format Output: ssout  – RNA secondary structure in  DBL  notation 1:  function  DynamicProgramming( ssin ) 2:   solution ← ∅ 3:   order ←  0 4:   do 5:     bps ← findNestedBasePairs ( ssin ) 6:    setBasePairOrders ( bps ,  order ) 7:    solution ← s o l u t i o n ∪ b p s 8:    order ← order  + 1 9:    ssin ← s   sin   ∖ b p s 10:   while b p s ≠ ∅ 11:   ssout ← encodeBasePairs ( solution ) 12:   return ssout 13:  end function 14: 15:  function  findNestedBasePairs( ssin ) 16:   rawBps ← getAllBasePairs ( ssin ) 17:   n ← | r a w B p s | 18:   bpSet ← treatBasePairSet  ( raw Bps ) 19:   scoreMtx  ←  c r e a t e M a t r i x ( n + 2 , 0 ) 20:   indexMtx ← c r e a t e M a t r i x ( n + 2 , N N ) 21:   for each b a s e   p a i r   ( i , i ′ ) ∈ b p S e t do 22:    for j ← i + 2   to   i ′ − 1 do 23:     j ′ ←  getPairedBase ( j ,  bpSet ) 24:     if ( j ′ &lt; i  OR  j ′ &gt; i ′  OR  j ′ &gt; j ) then 25:      u p d a t e M t x ( i , j , s c o r e M t x , i n d e x M t x )    ▹ O1 26:     else if ( j ′ &lt; j ) then 27:      nsc ← s c o r e M t x [ i + 1 ] [ j ′ − 1 ] + s c o r e M t x [ j ′ ] [ j ] 28:      if ( s c o r e M t x [ i + 1 ] [ j − 1 ] &lt; n s c  AND 29:       s c o r e M t x [ i + 1 ] [ j ] &lt; n s c ) then 30:       s c o r e M t x [ i + 1 ] [ j ]  ←  nsc        ▹ O2 31:       i n d e x M t x [ i + 1 ] [ j ]  ←  j 32:      else 33:       u p d a t e M t x ( i , j , s c o r e M t x , i n d e x M t x )   ▹ O1 34:      end if 35:     end if 36:    end for 37:    s c o r e M t x [ i ] [ i ′ ] ← 1 + s c o r e M t x [ i + 1 ] [ i ′ − 1 ]   ▹ O3 38:    i n d e x M t x [ i ] [ i ′ ] ← i ′ 39:   end for 40:   nestedBps ← ∅ 41:   a d d N e s t e d ( 1 , n , i n d e x M t x , r a w B p s , n e s t e d B p s ) 42:   return nestedBps 43:  end function 44: 45:  procedure  updateMtx( i , j , s c o r e M t x , i n d e x M t x ) 46:   if ( s c o r e M t x [ i + 1 ] [ j ] &lt; s c o r e M t x [ i + 1 ] [ j − 1 ] ) then 47:    s c o r e M t x [ i + 1 ] [ j ] ← s c o r e M t x [ i + 1 ] [ j − 1 ] 48:    i n d e x M t x [ i + 1 ] [ j ] ← i n d e x M t x [ i + 1 ] [ j − 1 ] 49:   end if 50:  end procedure 51: 52:  procedure  addNested( i , i ′ , i n d e x M t x , r a w B p s , n e s t e d B p s ) 53:   if ( i &lt; i ′  AND  i n d e x M t x [ i ] [ i ′ ] ≠ N N ) then 54:    j ′  ←  i n d e x M t x [ i ] [ i ′ ] 55:    j ← g e t P a i r e d B a s e ( j ′ , r a w B p s ) 56:    nestedBps ← n e s t e d B p s ∪ ( j , j ′ ) 57:    a d d N e s t e d ( j + 1 , j ′ − 1 , i n d e x M t x , r a w B p s , n e s t e d B p s ) 58:    a d d N e s t e d ( i , j − 1 , i n d e x M t x , r a w B p s , n e s t e d B p s ) 59:   end if 60:  end procedure Algorithm 3  Hybrid algorithm  Input: ssin  – RNA secondary structure in  BPSEQ  format Output: ssout  – RNA secondary structure in  DBL  notation 1:  function  Hybrid( ssin ) 2:   regs ← findAllPairedRegions(ssin) 3:   ncfregs ← findNonConflictedRegions(regs) 4:   s e t R e g i o n O r d e r s ( n c f r e g s , | n c f r e g s | ) 5:   cfregs ← r e g s ∖ n c f r e g s 6:   containerSet ← splitRegionsToContainers ( cfregs ) 7:   for each c o n t a i n e r ∈ c o n t a i n e r S e t do 8:    c o n t a i n e r . b e s t S o l u t i o n  ←  ∅ 9:    c o n t a i n e r . b e s t S c o r e  ← {0, 0} 10:    m ← | c o n t a i n e r | 11:    if ( m ≤ 8 ) then 12:     exSearch(container, m) 13:    else 14:     randWalk(container, m) 15:    end if 16:   end for 17:   solution ← mergeBestSolutions ( containerSet ) 18:   ssout ← encodeBasePairs ( solution ) 19:   return ssout 20:  end function 21: 22:  procedure  exSearch( container ,  m ) 23:   for k ← 1   to   m ! do 24:    currSol ← generateNextSolution ( container ) 25:    setRegionOrders ( currSol, m ) 26:    updateBest ( container, currSol ) 27:   end for 28:  end procedure 29: 30:  procedure  randWalk( container ,  m ) 31:   for k ← 1   to   M A X _ I T E R A T I O N S do 32:    currSol ← shuffleRegions ( container ) 33:    setRegionOrders ( currSol, m ) 34:    updateBest ( container, currSol ) 35:   end for 36:  end procedure 37: 38:  procedure  updateBest( container, currSol ) 39:   currScore ← fScore(currSol) 40:   if ( c u r r S c o r e [ 1 ] &gt; c o n t a i n e r . b e s t S c o r e [ 1 ]  OR 41:     ( c u r r S c o r e [ 1 ] = c o n t a i n e r . b e s t S c o r e [ 1 ]  AND 42:     c u r r S c o r e [ 2 ] &gt; c o n t a i n e r . b e s t S c o r e [ 2 ] ) ) then 43:    c o n t a i n e r . b e s t S c o r e  ←  currScore 44:    c o n t a i n e r . b e s t S o l u t i o n  ←  currSol 45:   end if 46:  end procedure 4 Results and discussion In computational experiments, we have analyzed the performance of five algorithms: First-Come-First-Serve (FCFS) method ( Popenda  et al. , 2008 ), EG and EC heuristics ( Smit  et al. , 2008 ) and the new ones, Hybrid (HYB) and DP algorithms. All of them were implemented in  Java  (including EG and EC, originally developed in  Python ) and are available through RNApdbee web server ( Antczak  et al. , 2014 ). Quantitative experiments aimed to compare algorithms’ efficiency in solving (annotating and representing) secondary structures of complex pseudoknotted RNAs. The test set was built based on representative, non-redundant RNA 3D structure repository ( Leontis and Zirbel, 2012 ). Initial set consisted of 1272 entries. In the preparation step, all of them were processed using 3DNA/DSSR running in two modes, without (mode I) and with helices’ analysis (mode II) ( Lu and Olson, 2008 ). This resulted in obtaining base pair list for every RNA 3D structure. Next, RNAs without pseudoknots were removed to obtain two datasets,  DS 1 containing 209 structures (mode I) and  DS 2 with 283 structures (mode II). We processed every structure in  DS 1 and  DS 2 to find how many disjoint pseudoknots it included. It appeared that the majority, i.e. 154 structures in  DS 1 and 221 in  DS 2, had only one pseudoknot, but single structures contained up to 13 pseudoknots per structure ( Table 2 ). All together, there were 466 pseudoknots identified in  DS 1 and 547 in  DS 2.
 Table 2. A number of instances in  DS 1 and  DS 2 which include  k  =   1…13 disjoint pseudoknots per one structure # Pseudoknots per str. 1 2 3 4 5 6 7 8 9 10 11 12 13 # Structures in  DS 1 154 18 8 1 0 5 6 1 4 4 5 2 1 # Structures in  DS 2 221 25 8 1 0 5 6 1 4 4 5 2 1 Data in both sets were managed by all considered algorithms. Many structures included only first-order pseudoknots. In these, and a few other cases, all algorithms returned the same results, as expected. Solutions were different for 80 structures in  DS 1, and 172 structures in  DS 2. Our further analysis covered these cases only, i.e. subsets  D S 1 ′  (80 structures) and  D S 2 ′  (172 structures), respectively.  D S 1 ′  included structures with pseudoknots of up to the fifth order,  D S 2 ′ ––up to the eighth. Subsets processed equally by all algorithms, i.e.  D S 1 ″ = D S 1 ∖ D S 1 ′  and  D S 2 ″ = D S 2 ∖ D S 2 ′ , included structures with pseudoknots of up to the second and the third order, respectively. A distribution of structures with the  i -th highest pseudoknot order is summarized in  Table 3 . For example, [FCFS, 3] = 7 in the table’s part (a) means that in subset  D S 1 ′ , FCFS algorithm found seven structures with pseudoknots of order  ≤ 3.
 Table 3. A number of structures with pseudoknot order  psorder  =   1…8, found in particular datasets (a) D S 1 ′ dataset (b) D S 2 ′ dataset Pseudoknot order 1 2 3 4 5 1 2 3 4 5 6 7 8 FCFS 162 36 7 3 1 138 52 59 16 9 3 5 1 EG 160 32 12 4 1 133 61 57 15 6 5 5 1 EC 160 35 9 4 1 132 63 55 15 6 7 4 1 DP 159 33 11 5 1 132 62 56 16 6 6 5 0 HYB 161 33 9 5 1 137 58 58 14 8 5 3 0 (c)  D S 1 ″  dataset (d)  D S 2 ″  dataset All algorithms 123 6 0 0 0 87 17 7 0 0 0 0 0 In the first experiment, solutions provided by particular algorithms were evaluated using multi-criterion function  fscoreII  (Formula 2). For each input RNA structure  S , provided in BPSEQ format, we obtained five structure representations  R 1 ( S ) – R 5 ( S )  encoded in DBL notation and we made their all-against-all comparison. For every pair of representations, we picked the winner applying Formula 3. This way, we counted how many times each algorithm won/lost a duel with every other one (draws were not considered). We also identified cases, in which one method dominated over all the others (won the battle) or lost with all the remaining algorithms (lost the battle;  Tables 4  and  5 ).
 Table 4. All-against-all algorithm comparison for  D S 1 ′  dataset upon  fscoreII FCFS EG EC DP HYB # Duels won # Battles won FCFS – 0 1 0 0 1 0 EG 75 – 22 1 2 100 1 EC 75 6 – 0 2 83 0 DP 79 6 22 – 1 108 0 HYB 78 12 23 7 – 120 7 # Duels lost 307 24 68 8 5 – – # Battles lost 71 0 1 0 0 – – Table 5. All-against-all algorithm comparison for  D S 2 ′  dataset upon  fscoreII FCFS EG EC DP HYB # Duels won # Battles won FCFS – 0 15 0 0 15 0 EG 169 – 94 1 5 269 0 EC 108 6 – 0 5 119 0 DP 170 18 95 – 5 288 1 HYB 167 28 98 25 – 318 25 # Duels lost 614 52 302 26 15 – – # Battles lost 107 0 15 0 0 – – It can be easily noticed that one method stands out among all. HYB algorithm is the one to have dominated in overwhelming part of duels. It has won 15% of battles for  D S 2 ′ , and 9% of battles for  D S 1 ′  (compared to other methods winning 0 or 1 battle only), which means that for that percentage of instances it has generated the best solution and outperformed all other algorithms. The second place belongs to DP, and the third one is occupied by EG heuristics. The same relationship between the algorithms emerges from the analysis of lost duels and battles. HYB did not lose a single battle. The FCFS method, historically the first and the simplest of all, proved to be the least successful. In consequence, we decided to update RNA FRABASE by exchanging FCFS to HYB within its workflow. The experiment with multi-objective  fscoreII  function was followed by a study of Pareto frontier. We have applied Pareto-based multi-objective algorithm to identify the front of non-dominated solutions. This experiment was run separately for each RNA structure from dataset  DS 1 (mode I, 209 instances) and from  DS 2 (mode II, 283 instances). For every instance, we obtained five solutions and we analyzed which ones were Pareto optimal. In two cases, two incomparable solutions were found for the instance: 4GMA–Z ∈ D S 1  (Pareto front: [49,–10]; [48,–9]) and 4WCE–X ∈ D S 2  (Pareto front: [951,–143]; [950,–142]). For every other RNA, single non-dominated solution was identified. For every algorithm, we have investigated for what fraction of  DS 1 or  DS 2 dataset it found Pareto optimal solution (i.e. included in the Pareto frontier). These results are provided in  Table 6 . For some instances (i.e. 8 instances from  DS 1, 25 instances from  DS 2) only one solution, obtained by exactly one method, has constituted Pareto frontier. In all but one of these cases, the Pareto optimal solution was found by HYB only.
 Table 6. Percentage of instances from dataset  DS 1 and  DS 2 for which Pareto optimal solution was found by the algorithm Dataset FCFS EG EC DP HYB DS 1 62.68% 94.25% 88.52% 96.17% 99.04% DS 2 39.58% 89.75% 63.60% 91.17% 98.59% Next experiment was performed to examine algorithms’ performance with respect to the first criterion function, i.e.  fscoreI  (Formula 1). The experiment followed the same pattern as in the previous case, i.e. each RNA structure  S  was processed by five algorithms that provided various structure representations,  R 1 ( S ) – R 5 ( S ) . They were compared against one another upon their evaluation with  fscoreI . The results ( Supplementary Table S1 and S2 ) show the advantage of DP algorithm over others. DP is the only method that has not lost any duel. It has also won most duels with other algorithms. The second place in  fscoreI -based ranking belongs to HYB, and the third one to EG heuristic. Similarly as in the experiment based on  fscoreII -ranking, FCFS method is the least successful of all. Finally, we have analyzed a single case experiment performed with all algorithms that were applied to process example RNA molecule. For this experiment, we have selected RNA from ribosomal subunit from human mitochondria, 3J7Y, chain A, ( Brown  et al. , 2014 ), being one of the largest and most complex structures in our dataset. This RNA is composed of 1473 residues and includes 7 disjoint pseudoknots. In our experiment, base pair list for 3J7Y_A was obtained by 3DNA/DSSR in mode II ( Lu and Olson, 2008 ). Next, different methods were used to annotate pseudoknots and determine their orders. One hundred double-stranded regions were found to form pseudoknots. The minimum highest pseudoknot order determined by HYB algorithm was 4, FCFS–5 and the remaining methods (EG, EG and DP)–6. From  Table 7 , we can read how many regions of the  i -th order ( i  =   0…6) have been annotated by particular algorithms in this 100, in 3J7Y_A structure. Every method found 24 regions with non-zero order ( Fig. 3 ). Fourteen regions were encoded differently by various algorithms. These differences can be spotted in DBL encoding provided in  Figure 3 . They are observed mainly in single base pair regions (i.e. isolated base pairs forming pseudoknots). To complete the view of 3J7Y_A pseudoknots we provide  Figure 4  prepared using R-CHIE ( Lai  et al. , 2012 ). It displays two arc diagrams, HYB- and FCFS-based, with all pseudoknot-involved base pairs, included in 100 mentioned regions. Diagrams resulting from other algorithms are shown in  Supplementary Material  ( Supplementary Figs. S4 and S5 ). Each of these figures enables pairwise comparison of two representations returned by different methods.
 Table 7. A number of the  i -th order regions identified in pseudoknots of RNA from ribosomal subunit from human mitochondria (3J7Y, chain A) Region order ( i ) 0 1 2 3 4 5 6 # FCFS-identified  i -th order regions 76 12 8 2 1 1 0 # EG/EC/DP-identified  i -th order regions 76 13 6 2 1 1 1 # HYB-identified  i -th order regions 76 13 7 3 1 0 0 Fig. 3. A distribution of regions with non-zero order in the structure of RNA from ribosomal subunit from human mitochondria (3J7Y, chain A) and their encoding by considered algorithms Fig. 4. Arc diagrams of pseudoknot-involved regions in RNA from ribosomal subunit from human mitochondria (3J7Y, chain A) corresponding to HYB (top) and FCFS (bottom) results  5 Conclusion RNA pseudoknots draw wide interest of researchers studying the RNA structure. But still, due to topological complexity, their classification and machine representation remain a challenge. In our work, we have addressed the problem of determination and assignment of pseudoknot order and encoding the pseudoknotted RNA structures in DBL notation. We have introduced new algorithms, Hybrid and DP, to handle this problem and we have compared them with already existing approaches, FCFS ( Popenda  et al. , 2008 ), EG and ECs ( Smit  et al. , 2008 ). We have proposed the new scoring function to better evaluate the solutions. The methods were tested using the representative set of 1272 non-redundant RNA 3D structures ( Leontis and Zirbel, 2012 ). The computational experiment has identified HYB as the best one in the ranking made according to  fscoreII . It finds machine representation of the secondary structure maximizing the number of non-conflicting base pairs and minimizing the highest pseudoknot order. If the first criterion ( fscoreI ) is considered, DP beats the other methods and HYB is just behind. All considered algorithms have been made available within  RNApdbee  web server ( http://rnapdbee.cs.put.poznan.pl ) and are ready to be used and investigated in further experiments. We hope they will open new opportunity in modelling more accurate 3D structures of pseudoknotted RNAs ( Miao  et al. , 2017 ), in particular in the case of secondary structure-based prediction ( Antczak  et al. , 2016 ;  Martinez  et al. , 2008 ;  Parisien and Major, 2008 ;  Popenda  et al. , 2012 ). They should facilitate an access to a proper secondary structure for those who annotate it from the tertiary data. They also allow to apply the preferable optimization criterion, based on  fscoreI  or  fscoreII , depending on the user expectations. We believe that an admittance to properly represented RNA secondary structure can contribute to explain the folding process and explore the RNA fragmentation pattern ( Rybarczyk  et al. , 2016 ). The algorithms can be also useful in comparison and evaluation of predicted 3D models via their back-translation to the secondary structure level ( Lukasiak  et al. , 2015 ;  Wiedemann  et al. , 2017 ;  Zok et al., 2014 ). Finally, they can cast a new light on the study of relationships between the sequence, secondary and tertiary structure of RNAs ( Wiedemann and Milostan, 2016 ), as well as an investigation of structure-function relationship. Supplementary Material Supplementary Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Improved quality control processing of peptide-centric LC-MS proteomics data</Title>
    <Doi>10.1093/bioinformatics/btr479</Doi>
    <Authors>Matzke Melissa M., Waters Katrina M., Metz Thomas O., Jacobs Jon M., Sims Amy C., Baric Ralph S., Pounds Joel G., Webb-Robertson Bobbie-Jo M.</Authors>
    <Abstract>Motivation: In the analysis of differential peptide peak intensities (i.e. abundance measures), LC-MS analyses with poor quality peptide abundance data can bias downstream statistical analyses and hence the biological interpretation for an otherwise high-quality dataset. Although considerable effort has been placed on assuring the quality of the peptide identification with respect to spectral processing, to date quality assessment of the subsequent peptide abundance data matrix has been limited to a subjective visual inspection of run-by-run correlation or individual peptide components. Identifying statistical outliers is a critical step in the processing of proteomics data as many of the downstream statistical analyses [e.g. analysis of variance (ANOVA)] rely upon accurate estimates of sample variance, and their results are influenced by extreme values.</Abstract>
    <Body>1 INTRODUCTION The majority of statistical strategies to assess peptide/protein differential abundances from liquid chromatography-mass spectrometry (LC-MS) proteomic experiments are based on analysis of variance (ANOVA) methodologies applied to peak intensities (i.e. abundance measures) of proteolytic peptides ( Bukhman  et al. , 2008 ;  Daly  et al. , 2008 ;  Karpievitch  et al. , 2009 ;  Oberg and Vitek, 2009 ;  Oberg  et al. , 2008 ). However, the ANOVA approach relies upon accurate estimates of sample variance, and proteomics studies not only have inherent variability associated with the biological samples, but potentially diverse process-based sources of variability. That is, the accurate estimate of sample variances is often difficult to obtain. For example, sample preparation protocols and instrument variations associated with the LC column (particularly important for multi-column platforms) and mass spectrometer can cause variations in peak intensities, as well as peptides identified across MS analyses within an experiment. Data quality is especially important when the number of biological samples is small, often the case in proteomics experiments, and extreme values can negatively influence all subsequent data analysis outcomes. Identification of statistical outliers in univariate data is an established but highly debated statistical topic ( Barnett and Lewis, 1994 ;  Hawkins, 1980 ). There are many consecutive outlier procedures, focusing on one suspect value at a time, that have been proposed and implemented across many fields of application, such as Grubbs' test and Dixon's Q-test ( Dixon, 1950 ;  Grubbs, 1950 ). Because these methods iteratively remove outlier points, the false positive rate (i.e. Type 1 error) is inflated ( Jain, 2010 ). In contrast, recursive outlier detection procedures detect the presence of any number of outliers and control Type 1 errors. For example,  Jain (2010 ) presents a recursive version of Grubbs' test and  Caroni and Prescott (1992 ) derived a sequential application of Wilks's multivariate outlier test. There are downfalls to these recursive procedures: (i) they are designed for univariate data, and if applied to multivariate data, will likely fail to detect statistically influential extreme values, and (ii) they are negatively affected by masking (i.e. the inability to detect an outlier in the presence of another outlier) and swamping (i.e. identify non-outliers as outliers) effects. The identification of statistical outliers in multivariate data, such as microarray and proteomic data, is non-trivial. The multiple dimensions of the data often subject outliers to masking ( Filzmoser  et al. , 2008 ). The microarray community, however, has made considerable progress in applying statistical metrics to assess the quality of microarray data ( Kauffmann  et al. , 2009 ;  Kemmeren  et al. , 2005 ;  Lee  et al. , 2006 ;  Wilson and Miller, 2005 ). Of particular applicability to proteomics data are the ideas presented by Kauffmann  et al.  They note that a poor quality array will impede the statistical and biological significance of the analysis due to the added noise. This is also true for proteomics data. That is, poor quality peptide abundance data will hinder downstream statistical analysis, including normalization, and subsequent biological interpretations. For proteomics data, a routine but non-probabilistic approach used for the identification of outlier LC-MS analyses (i.e. runs) during data preprocessing is through a correlation matrix plot ( Metz  et al. , 2008 ). The sample correlation coefficient is calculated among technical replicates and biological replicates. Those runs with a relatively low correlation are removed from the dataset. The determination of ‘low’ correlation is subjective, and varies across analysts, experiments and time. Correlation may be examined via a heat map in which a color palette represents the numeric value, the color palette choice as well as the range of correlation values it covers can be highly subjective and extremely influential on the selection of which runs should be removed from the dataset. In addition, the sample correlation coefficient can only be computed across peptides with common identifications between runs (i.e. it does not account for missing data), it does not account for the multivariate nature of LC-MS runs, nor is there any statistical certainty associated with the exclusion of a run. Advanced statistical approaches to outlier detection in proteomics data have focused either on the identification of outlier spectra maps ( Rudnick  et al. , 2010 ;  Schulz-Trieglaff  et al. , 2009 ) or on peptide/protein abundances independent of LC-MS run behavior ( Cho  et al. , 2008 ;  MacCoss  et al. , 2003 ;  Xia  et al. , 2006 ).  Rudnick  et al.  (2010 ) described a large set of metrics for the quantitative assessment of system performance and evaluation of technical variability among inter- and intra-laboratory LC-MS/MS proteomics experiments. However, the use of these metrics to assess the quality of an individual LC-MS/MS run is not addressed.  Schulz-Trieglaff  et al.  (2009 ) applied a multivariate method to perform a quality assessment of raw LC-MS maps using 20 quality descriptors. The goal of their approach was to identify and remove outlier runs using unprocessed spectra before noise filtering, peak detection or centroiding was performed.  Cho  et al.  (2008 ) presented a peptide outlier detection method using quantile regression to account for the heterogeneity of variance between replicate LC-MS/MS runs. Peptide intensity ratios were plotted on an  MA  plot, where  M  is the difference in peptide abundance values and  A  is the average peptide intensity value.  MacCoss  et al.  (2003 ) developed a correlation algorithm to detect outlier peptides using fractional changes between sample and reference intensities.  Xia  et al.  (2006 ) proposed a two-stage method, combining Dixon's Q-test and a median absolute deviation (MAD) modified  z -score test, for outlier detection of peptide ratios. These latter methods focus on assessing individual peptides for extreme behavior rather than the distribution of peptide abundance values for an entire LC-MS run. Our goal is to statistically identify runs that exhibit extreme peptide abundance distribution properties, and thus will likely impact downstream statistical analyses. Consequently, we are not focused on outliers specific to the spectral properties. We describe a statistical strategy to identify and remove extreme LC-MS runs with a high level of statistical certainty, thus removing subjectivity from the filtering process. The approach, based on a robust Mahalanobis distance (rMd), assesses the reproducibility of the distribution of peptide abundance values across replicate runs of the same biological sample as well across related biological samples. Statistical methods, which limit the influence of extreme observations, are applied to obviate assumptions about underlying probabilistic models ( Hoaglin  et al. , 2000 ). We demonstrate the approach by applying it to simulated and real LC-MS datasets. 2 METHODS Our approach to detect and ascertain if an individual LC-MS run within an experiment, is a statistical outlier with a four-step process. The algorithm was implemented in MATLAB (version 7.10.0.499, R2010a, The MathWorks Inc.: Natwick, MA, USA). 2.1 Summarize each LC-MS run as five metrics Five statistical metrics were chosen to describe the distribution of observed peptide abundance values in a single LC-MS run. These metrics described below capture selected aspects of the peptide abundance distribution such as shape and scatter. The location of each distribution is not directly considered since it could potentially be a false indicator of outlingness. In addition, location can easily be corrected by a simple overall normalization factor. The metrics are vectorized for each run, represented as  ; initially reducing the dimension of each run from  p  peptides to  q  metrics with the resulting dataset dimensionality of ( n × q ) where  n  is the number of LC-MS runs. 2.1.1 Metric 1: correlation coefficient The sample correlation coefficient,  r ij , is calculated for peptide abundance values between all LC-MS runs ( i =1,…,  n ;  j =1,…,  n ) resulting in an  n × n  matrix. The correlation coefficient metric for the  i -th run,  R i , which is used for the robust principal component analysis, is the average correlation within a common grouping (e.g. treatment group, G), and has dimension ( n ×1). For the  i -th run this is computed as,
 (1) 
where  N G ( i )  is the total number of runs in the group associated with run  i . The average correlation among biological replicates, rather than among technical replicates, is used due the small number of technical replicates, if any at all, observed in a typical LC-MS experiment. 2.1.2 Metric 2: fraction of missing peptide abundance data The fraction of missing abundance data in the  i -th (1,…,  n ) LC-MS run is defined as,
 (2) 
where  a ij =1 if the  j -th peptide abundance is absent for the  i -th run; otherwise,  a ij =0. 2.1.3 Metric 3: median absolute deviation of peptides within a LC-MS run The MAD ( Hoaglin  et al. , 2000 ) is a robust measure of the spread of the data, and is used as an estimate of the sample standard deviation if scaled by a factor of 1.483. The MAD of the  i -th LC-MS run is defined as,
 (3) 
That is, within a run, each abundance value for peptide  j  is compared with the median peptide abundance values of the run  i . 2.1.4 Metric 4: skew The asymmetry of a distribution is described by skew. In our application to the  i -th (1,…,  n ) LC-MS run,  p  is the number of peptides observed in the  i -th run,  is the average peptide abundance value of all peptides observed in the  i -th run and  S  is the sample standard deviation of the  i -th run.
 (4) 2.1.5 Metric 5: kurtosis The peakedness, or ‘heavy-tailedness’, of a distribution is described by kurtosis. The same parameters are used as skew. Kurtosis is calculated as,
 (5) 2.2 Obtain a robust estimate of the covariance matrix The purpose of robust principal component analysis (rPCA) in our method is to obtain the eigenvalues and eigenvectors to calculate a robust covariance matrix, which will be used in the calculation of the rMds. We employ a rPCA algorithm developed by Croux  et al.  that is based on the projection-pursuit approach to estimate the eigenvalues, and subsequent scores obtained from the projections of the metrics on the eigenvectors ( Croux and Ruiz-Gazen, 2005 ;  Li and Chen, 1985 ). The robust covariance estimate is defined as,
 (6) 
for which  S n  is the robust scale estimator used by the projection-pursuit index λ Sn , k  is the  k -th eigenvalue and ν Sn , k  is the  k -th eigenvector ( Croux and Ruiz-Gazen, 2005 ). The rPCA algorithm uses the L 1 -median value to center the data, and (MAD*1.483) as the robust scale estimate. 2.3 Identify outlier LC-MS run(s) using the rMd A widely accepted measure of distance in multivariate data is the Mahalanobis distance because it accounts for not only the average value, but also the covariance structure of the measured variables ( Mahalanobis, 1936 ). The distance of an individual LC-MS run from the center of the data is measured by a rMd. For a  q -dimensional multivariate vector   for  i =1,…,  n , the rMd is defined as,
 (7) 
where  C Sn , a robust estimate of the covariance matrix, is obtained from the robust principal component analysis of the  n × q  quality matrix, and   is a vector of medians of the five metrics. 2.4 Statistical assessment of the rMds The rMd squared values associated with the peptide abundances vector (rMd-PAV) is the score used to assess whether an individual LC-MS run is an outlier. The rMd-PAV scores are approximately chi-square distributed with  q  degrees of freedom (χ q 2 ). Therefore, outlier LC-MS runs are defined by a large rMd-PAV score such that the calculated squared distance exceeds a critical value of the χ q 2  distribution specified a priori. 2.5 Proteomics data processing We present two independent real datasets to demonstrate the application of this outlier discovery strategy to LC-MS proteomics data. Human cell culture samples were analyzed with an Exactive mass spectrometer (Thermo Electron Corp.), and mouse plasma samples were analyzed with an LTQ-Orbitrap mass spectrometer (Thermo Electron Corp.). Nanoelectrospray ionization was used in the analysis of all samples. Spectra were collected at 400–2000 m/z with a resolution of 100 k and analyzed using the accurate mass and elution time (AMT) tag approach ( Smith  et al. , 2002 ). The mass de-isotoping process was performed using Decon2LS ( Jaitly  et al. , 2009 ), and the matching process was performed using VIPER ( Monroe  et al. , 2007 ). Features from the LC-MS analyses were matched to AMT tags to identify peptides, using an initial tolerance of ±3 p.p.m. for mass and 2.5% for the LC normalized elution time (NET). The human cell culture peptide datasets were further processed to remove peptides identified with low confidence, using the uniqueness filter Statistical Likelihood Confidence (SLiC) ( Anderson  et al. , 2006 ) score of 0.35 and a DelSLiC of 0.2. In circumstances where a peptide was identified in some LC-MS analyses, but not others, the missing data were coded as ‘NaN’. All peptide abundance values were transformed to the log10 scale. Minimum occurrence data filters were used to identify those peptides for which the amount of data present was not adequate for differential abundance analysis ( Webb-Robertson  et al. , 2010 ). The sample complexity of the sham controls (SCs) in each of the designed experiments is the same with respect to original biological material. 3 RESULTS Simulations of size 500 based on the  p -variate standard normal distribution  Np ( 0,I ), and an empirically influenced  p -variate normal distribution  Np (μ, Σ) were performed to examine a range of outlier configurations. In addition, we assessed the performance of the multi-dimensional outlier detection method against the conventional method of using a Pearson's correlation coefficient [previously described in  Section 2.1  as metric 1—Equation ( 1 )] to ascertain whether a LC-MS run is an outlier. Simulation is useful to investigate the properties of rMd-PAV, however; since simulation of expected distribution parameters in real proteomics data is not well understood, these results are presented in  Supplementary Material  ( Rocke  et al. , 2009 ). The results of the multi-dimensional outlier detection analysis are displayed in a simple yet effective graphic in which rMd-PAV scores are plotted for each LC-MS run and compared with a reference line representing the χ 2  critical value. For improved visualization, the rMd-PAV scores and the χ 2  critical value are transformed to the log 2 scale. The red horizontal line represents the log 2 (χ 2 0.9999,5 ) critical value. That is, at a significance level of 0.0001, a LC-MS run may be classified as a statistical outlier if the calculated test statistic ≥χ 2 0.9999,5  critical value, or equivalently, the χ 2   P ≤0.0001. LC-MS runs with log 2 (rMd-PAV) scores above the red horizontal line are suspect and should be removed from the dataset. 3.1 Real data benchmark—expert identified outlier runs Calu-3 cells, a human lung adenocarcinoma cell line, were infected with the severe acute respiratory syndrome coronona virus (SARS-CoV) at a multiplicity of infection of 5. Cell monolayers were inoculated with SARS for 40 min at 37 ○ C, and sham-infected controls were inoculated with medium only. Following inoculation, monolayers were rinsed and incubated for times 0, 3, 7, 12, 24, 30, 36 and 48 h. At the indicated times post-infection, wells were washed three times with ice cold 150 mM ammonium bicarbonate buffer and cells lyzed for 5 min in ice cold 8 M urea. Samples were frozen at −80 ○ C until assayed. Samples were analyzed in triplicate, except where noted in  Supplementary Table S2 , and the minimum occurrence filter returned a total of 26 776 peptides ( Webb-Robertson  et al. , 2010 ). This study included three biological replicates per time point as well as a large number of LC-MS runs ( n =141), thus the removal of runs with poor quality abundance data is essential to maintain statistical power in downstream analyses. An LC-MS expert at Pacific Northwest National Laboratory upon reviewing the chromatography maps for this study was able to designate 28 out of 141 (~20%) LC-MS analyses as suspect due to various reasons (e.g. electrospray instability, elution time, sample prep/collection problem). We performed the rMd-PAV analysis, and compared its performance with  t  correlation alone to identify statistical outliers (runs at the peptide abundance level) via a receiver operating characteristic (ROC) curve analysis. The rMd-PAV approach identified 12 out of the 28 expert-designated suspect runs as statistical outliers at the 0.0001 significance level ( Fig. 1 a). Electrospray issues represent almost half (13/28) of the expert identified runs, while the statistical algorithm identified three of these runs. It is the most likely technical issue to occur and the most difficult to detect. One reason could be that the electrospray issue does not translate to a poor peptide abundance distribution, and thus an outlier. The other 15 runs identified by the MS expert are due to elution time (5/28; 4/5 identified by algorithm), chromatography (3/28; 1/3 identified by algorithm) and sample prep/collection (7/28; 4/7 identified by algorithm).
 Fig. 1. Calu-3 cell-line experiment. ( a ) The rMd-PAV plot of the LC-MS runs. Runs identified as outliers (blue downward triangles) sit above the red horizontal line which represents the log 2  (χ 0.9999,5 2 ) critical value (i.e.  P =0.0001). The empty upward triangles below the red horizontal line represent runs identified as suspect by the MS expert that were not identified as statistical extreme. ( b ) The correlation plot of the LC-MS runs. LC-MS runs that were expert designated as suspect, but did not exhibit different peptide abundance distributions from those runs that were not designated as suspect are identified in  Figure 2 a as unfilled triangles. Although the MS expert identified these runs as suspicious, the peptide abundance distributions are indistinguishable from those runs that were not designated as suspect.
 Fig. 2. The ROC curves from the rMd-PAV and correlation alone outlier analyses of the calu-3 cell-line experiment. In addition, we reviewed the sample correlation coefficient between all the study runs ( Fig. 1 b). Based on a subjective visual inspection of this graph, 6 out of the 28 expert-designated suspect LC-MS runs (#6, 25, 67, 78, 131 and 132) would have been dropped from the dataset. The rMd-PAV scores identified six additional runs as statistical outliers. This method did not identify any of the extreme runs due to electrospray issues; it did identify 3/5 runs labeled as suspect due to elution time, 1/3 suspect runs due to chromatography and 2/7 runs due to sample prep/collection issues. A ROC analysis was completed to compare all levels of sensitivity and specificity. A comparison of the ROC curves for the rMd-PAV scores and the correlation metric alone by a Wilcoxon signed rank test results in statistically significant differences between the curves in favor of rMd-PAV ( P &lt;0.0001,  Fig. 2 ). Therefore, for this benchmark dataset we observe that rMd-PAV scores are superior to correlation alone for the identification of statistical outlier runs in LC-MS peptide abundance data. 3.2 Case study—cigarette smoke exposure data Groups ( N =8 biological replicates) of regular weight (RW) and diet-induced obese (OB) C57BL/6 mice (15 weeks old) were exposed to either filtered air (SCs), mainstream (MS) or side stream (SS) cigarette smoke by nose-only inhalation exposure for 5 h/day for 8 days. Target cigarette smoke exposure concentrations were 250 μg wet-weight total particulate matter (WTPM)/L of air for the MS exposures and 85 μg WTPM/L for the SS exposures. RW mice are defined as those mice fed a regular diet (PMI 5002 Rodent Diet ® , Richmond, IN, USA; ~5kal% fat) throughout the study. DIO mice were fed a high-calorie/high-fat diet (D12492 Rodent Diet, Research Diets Inc., New Brunswick, NJ, USA; 60kal% fat) starting at 6 weeks of age and continued throughout the study. Immediately following the last exposure, each animal was removed from the exposure unit and anesthetized. Blood was collected into tubes containing potassium ethylenediaminetetraacetic acid (EDTA) (Tyco Healthcare Group LP, Mansfield, MA, USA) and centrifuged to obtain plasma for analysis by LC-MS/MS. Samples were analyzed in duplicate, except where noted in  Supplementary Table S3  and a minimum occurrence filter returned a total of 3655 peptides ( Webb-Robertson  et al. , 2010 ). As in any data analysis problem, visual inspection of complex data before statistical analysis is vital. Box plots are a simple and statistically robust techniques that are informative concerning distributional properties (e.g. skew and kurtosis), and provide visual guidance when interpreting analysis results. Peptide abundance data for each example has been displayed versus a LC-MS run order identification number (not true LC run order) using a box plot. The box plot of the mouse plasma data ( Fig. 3 a) shows a fair amount of variability from run to run making visual determination of statistical outlier runs difficult.
 Fig. 3. Cigarette smoke exposure experiment. ( a ) Box plots of peptide abundance values observed in LC-MS runs ( n =98) for the mouse plasma dataset. The color indicates experimental group membership. ( b ) The rMd-PAV plot of the LC-MS runs. Those runs identified as outliers sit above the red horizontal line which represents the log 2  (χ 0.9999,5 2 ) critical value (i.e.  P ≤0.0001). The downward triangles represent outlier runs—red represents all technical replicates from a biological sample, and blue represents individual technical replicates within a sample. ( c ) The run-by-run ( r ij ) correlation plot of the LC-MS runs. The rMd-PAV approach identified 6 out of the 98 LC-MS runs as statistical outliers at the 0.0001 confidence level ( Fig. 3 b). Singleton technical replicates were removed (run id #23 and 96), in addition to two complete biological samples (run id #11 and 12—obese SC sample; run id #27 and 28—obese MS inhalation sample). Of the six runs identified as statistical outliers, it is unlikely any would have been removed using run-by-run correlation coefficient,  r ij , as the median correlation of all runs is 0.86 ( Fig. 3 c), and ranging from 0.72 to 0.87 across the pool of identified outlier runs. Using a more reflective score of correlation,  R i , which for the  i -th run is the average correlation among the biological replicates within a group, the rMd-PAV identified runs would not have been removed from the dataset as the median correlation is 0.88 ranging from 0.73 to 0.85 across the identified outlier runs. An additional benefit of the rPCA is the ability to explore the behavior of the metrics (e.g. skew, kurtosis, fraction missing, etc.) used to describe the peptide abundance distributions for the LC-MS runs within an experiment. Explaining high-dimensional data in two or three latent variables (i.e. principal components) is highly desirable. With only a few latent variables, data can be graphically displayed and the key contributing attributes to the total explained variation is easily interpreted. The relationship among the five metrics for peptide abundance data can be understood by examining the score plots of the latent variables. In addition, the behavior of the outlier runs can be understood relative to the non-outlier runs (i.e. average). The most dominant manner in which these runs deviate is  Kurtosis, Skew  and  Fraction Missing Data , as observed in the score plot associated with the rPCA ( Fig. 4 ). The score plot is unique to an experiment, and thus is an excellent tool to further understand statistical differences in the peptides distributions among the LC-MS runs. The first score plot to consider is a comparison of the first two rPCA components (i.e. latent variables). In combination they account for &gt;88% of the total variation in the data, and suggest differences among kurtosis, skew and fraction of missing abundance data explain most of the variation in the data. The plot shows the rMd-PAV identified runs located at the extreme ends of the observed data with respect to the first and second latent variables. Using the angle between vectors as a visual guide, for this data, it can be deduced the  Fraction Missing Data  and  Skew  of the peptide abundance distribution are correlated. In total, the first three components account for ~95% of the variation observed in the data. While a two-dimensional view of the data is helpful in understanding relationships among variables, outliers and non-outliers, it is the relationship among the data under the full dimensionality that is the basis for the evidence of outlier runs.
 Fig. 4. Cigarette smoke exposure experiment. The score plot of the first two latent variables resulting from the rPCA of the data. It suggests the runs labeled on the plot are outliers due to the fraction of missing peptide abundance values, and the skewness and kurtosis of the peptide abundance distribution within a run. 4 DISCUSSION Outlier detection in multivariate data is a non-trivial statistical task often subject to the masking effect ( Filzmoser  et al. , 2008 ;  Rocke and Woodruff, 1996 ). Caution should always be taken when removing data from any dataset, large or small, and data should not be removed solely on the grounds of a statistical outlier test. Rather, the results of any statistical outlier algorithm used should always be reviewed in the context of the research goal and the experiment. Often the extreme data values are of interest and may explain technical difficulties in the process (e.g. sample preparation issues, technical difficulties with instrumentation and a mislabeling of samples). However, as with any statistical analysis and especially those dealing with small sample sizes, reviewing the outcome of the analysis is imperative. Specifically, graphical methods allow the analyst to review the analysis in a stepwise manner. For example, as our first step, we first plot the peptide abundances observed in the experiment for each LC-MS run using a box plot. Then to understand how the abundance distributions vary across the LC-MS runs we examine the scores plot resulting from the robust PCA. 5 CONCLUSION We have presented a novel approach to the identification of statistical outliers in LC-MS proteomics peptide abundance data. The value of the multivariate outlier discovery strategy utilizing rMd-PAV scores is the use of an objective probabilistic model to assess statistical certainty of the exclusion of runs within an experiment in the context of the complete dataset. Proteomics has placed considerable effort on assuring the quality of the peptide identification with respect to spectral processing ( Piening  et al. , 2006 ;  Rudnick  et al. , 2010 ;  Schulz-Trieglaff  et al. , 2009 ;  Stead  et al. , 2008 ); however, quality assessment of the subsequent data matrix has focused on subjective visual inspection of run-by-run correlation, or individual peptide components. The quality of the LC-MS peptide abundance data matrix is essential to the identification of robust biomarkers. Moreover, statistical evaluation of the data relies upon tools often based on linear models, such as ANOVA which require accurate estimates of variance ( Bukhman  et al. , 2008 ;  Daly  et al. , 2008 ;  Karpievitch  et al. , 2009 ;  Oberg  et al. , 2008 ). Without proper identification of statistical outlier runs the estimates of variance will be inflated, which may have a considerable effect on the identification of significant peptides and proteins. 
          Supplementary Material 
          
             
                Supplementary Data 
             
             
             
          
       </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Environment specific substitution tables improve membrane protein alignment</Title>
    <Doi>10.1093/bioinformatics/btr230</Doi>
    <Authors>Hill Jamie R., Kelm Sebastian, Shi Jiye, Deane Charlotte M.</Authors>
    <Abstract>Motivation: Membrane proteins are both abundant and important in cells, but the small number of solved structures restricts our understanding of them. Here we consider whether membrane proteins undergo different substitutions from their soluble counterparts and whether these can be used to improve membrane protein alignments, and therefore improve prediction of their structure.</Abstract>
    <Body>1 INTRODUCTION Membrane proteins constitute ~30% of human proteins ( Almén  et al. , 2009 ), and are important drug targets. Unfortunately, the structures of these proteins are hard to determine. As of January 2011, there are ~70 000 structures in the Protein Data Bank (PDB) ( Berman  et al. , 2000 ), of which fewer than 1500 are recognized as membrane proteins by the PDB_TM database ( Tusnády  et al. , 2005 ). Even these available structures are highly redundant. Thus, for most membrane proteins only a sequence is known: structural information must be inferred by modelling. Structure modelling can broadly be divided into template-free and template based methods ( Moult  et al. , 2009 ). Template based modelling makes use of ‘homologous’ proteins that are identified as having similar structures to that of the target sequence. Proteins are normally considered homologous if they share significant sequence similarity. Once a homologous protein with a known structure (the ‘template’) has been identified, it is aligned to the target sequence. This alignment between two sequences can be improved by the use of structural information from the template. This sequence-to-structure alignment forms the blueprint which coordinate generation programs such as MODELLER for soluble proteins ( Sali, 1993 ), or MEDELLER for membrane proteins ( Kelm  et al. , 2010 ) use to build a model. The accuracy of a model is primarily determined by the quality of the initial alignment ( Sánchez, 1997 ). The membrane is a radically different environment from the aqueous environment of soluble proteins. Most membrane bilayers are composed of phospholipids with hydrophobic tail groups and charged head groups. This suggests for example that substitutions from charged residues to hydrophobic ones are unlikely in head-contacting regions, and conversely that substitutions from hydrophobic residues to charged ones are unlikely in tail-contacting regions. Thus, membrane proteins will have unique patterns of substitutions ( Mokrab  et al. , 2010 ). However, due to lack of data, the alignment of membrane proteins is typically performed with substitution tables optimized for soluble proteins. Identifying appropriate tables for membrane environments is expected to improve methods that depend on them, particularly for sequence-to-structure alignment ( Mokrab and Mizuguchi, 2005 ). Environment-specific substitution tables (ESSTs) are one of the methods used to align target sequences with known structures. A substitution table tabulates the chances that an amino-acid in one protein is replaced by another in a second protein: for example that a glycine residue is replaced by an alanine. ESSTs are a set of substitution tables, each of which is to be used in a different environment. Environments are defined by features such as secondary structure and accessibility. ESSTs are used in sequence-to-structure alignment as the environments for each residue can be determined from the structure. To create an ESST, substitutions in each environment must be counted between a number of related proteins. In early studies tables were constructed by counting substitutions between homologous proteins of known structure ( Shi  et al. , 2001 ). More recently, tables have been made by counting substitutions between one structure and many sequences ( Mizuguchi  et al. , 2007 ). The latter method is used here. Environment independent substitution tables have previously been made for transmembrane regions. The JTT table ( Jones  et al. , 1994 ) appears to be the earliest example, with the PHAT ( Ng  et al. , 2000 ) and SLIM ( Müller  et al. , 2001 ) tables following. These tables were intended to be used in conjunction with a non-membrane table, such as BLOSUM62 ( Henikoff and Henikoff, 1992 ). This ‘bipartite’ scheme requires a separate algorithm to decide where to use each table. Sequence alignment has been attempted using PHAT, with both bad ( Forrest  et al. , 2006 ), and good ( Pirovano  et al. , 2008 ) results when compared with alignments using only BLOSUM62. The SLIM table is optimized for homology detection: its authors explicitly caution against using it for alignment. Two problems complicate the construction of ESSTs for membrane proteins: Firstly, it is difficult to determine the structural environment of residues even in known membrane structures—although secondary structure and accessible surface area can be determined as for soluble proteins, the location of a residue within the membrane bilayer cannot be inferred from the solved structure alone. Here we use the annotation program iMembrane ( Kelm  et al. , 2009 ) to determine these contacts. Secondly, once an ESST is created, it is difficult to assess if it is representative of its environment across all membrane proteins. How can we tell if we have made a ‘good’ table? We describe a metric of ESST quality that is robust against perturbations in the observed frequencies of individual substitutions. We create ‘good’ membrane ESSTs and analogous soluble ESSTs and make global comparisons between them. A dendrogram illustrates inter-table distances, and a principal component analysis is used to detect the dependence of substitution patterns on environment type. Membrane environments are found to be far more diverse than soluble protein environments. For example, any pair of lipid tail layer environments are more dissimilar than the corresponding pair of soluble environments. FUGUE ( Shi  et al. , 2001 ) is a commonly used program to produce sequence-to-structure alignments with ESSTs. We compare several methods for sequence-to-structure alignment generation: default FUGUE, our membrane-based FUGUE, a bipartite PHAT/BLOSUM62 scheme, and the sequence-to-sequence alignment program MUSCLE ( Edgar, 2004 ). Our membrane-specific ESSTs consistently improve alignment quality, especially at low sequence identity. In the 10–25% sequence identity range, compared with the next best method (the default FUGUE tables) 54 alignments are improved by at least 10 residues, whereas only 6 alignments are worsened by the same amount. In this range the average improvement per alignment is 28 more residues aligned correctly. These alignment improvements are found to lead to corresponding improvements in structure prediction. 2 METHODS 2.1 Environment descriptors Secondary structure and accessible surface area are annotated using JOY ( Mizuguchi  et al. , 1998b ). We use ‘a’ to label inaccessible residues and ‘A’ to label accessible ones. By convention, a residue is deemed accessible when more than 7% of its surface area is exposed ( Hubbard and Blundell, 1987 ). The secondary structure types used are helix (H), β-strand (E), +ve ϕ angle (P), and coil (C). In the case of membrane proteins, environments can also be defined depending on the part of the membrane a residue is contacting. iMembrane uses coarse-grained molecular dynamics simulation data from the CGDB database ( Scott  et al. , 2008 ) to annotate three contact environments. Residues that are in contact with the membrane for &lt;10% of the time are in the ‘n’ environment; of the remaining residues, those that spend more time in contact with the lipid heads are in the ‘h’ environment, and those that spend more time in contact with the lipid tails are in the ‘t’ environment. By taking a consensus of these contact annotations, a membrane protein can be divided into layers corresponding to the lipid heads (H), lipid tails (T), and not-in-membrane (N) regions. There are thus 72=2×4×3×3 distinct possible environments. Larger numbers of environments lead to more specific substitution tables, at the cost of each table being constructed with less data. It is desirable to combine environments so as to find a minimal set that encompasses as much variation in substitution patterns as possible. There are many possible valid minimal environment sets. Here we ignore the contact annotation, with two exceptions. Accessible residues that lie in the tail layer but rarely contact the membrane can be identified as residues that line a pore (P). Accessible residues that are annotated with head contacts but are in the tail layer, or with tail contacts but are in the head layer, define an interface region (I) spanning the hydrophilic and hydrophobic parts of the membrane. We add these to the existing layer types to give five labels: H(ead), N(ot in membrane), T(ail), P(ore) and I(nterface) regions ( Figure 1 ).
 Fig. 1. A schematic slice through a membrane protein (1YEW) in the membrane indicating the layer types used. ‘N’ is the region outside the membrane, ‘T’ and ‘H’ span the tail and head groups of the membrane lipids respectively, ‘P’ is the area lining the pore, and ‘I’ is the interface region between the tail and head groups. It is convenient to refer to environments using a letter code e.g. ‘IEA’ = interface layer, β-strand, accessible residues. Letter codes will always be built in the order Layer:Secondary Structure:Accessibility. An asterisk ‘*’ will be used when the exact letter does not matter. Under this system ‘I**’ refers to all interface layer environments, whereas ‘I*A’ refers to accessible interface layer environments. 2.2 Alignments for table generation Transmembrane protein structures were identified from the PDB_TM database ( Tusnády  et al. , 2005 ) and downloaded from the PDB ( Berman  et al. , 2000 ). Each was then split into its component protein chains. Redundant chains—those with &gt;80% sequence similarity—were removed using Cd-hit ( Li and Godzik, 2006 ). Chains without iMembrane search hits were also removed, leaving 328 chains. For each chain, related sequences were obtained from 5 iterations of PSI-BLAST ( Altschul  et al. , 1997 ) using an  E -value threshold of 1×10 −3  for keeping a hit, and a threshold of 1×10 −5  for including a hit in the sequence profile of the next iteration. PSI-BLAST searches were made against the NCBI nr database. These sequences were then aligned to their corresponding structures with MUSCLE ( Edgar, 2004 ), and the alignments used to generate the membrane substitution tables. Soluble tables were generated from four different alignment sets. The first of these was generated as above—that is, by aligning multiple homologous sequences with each structure. The structures were obtained by taking the first structure from each family in the HOMSTRAD database ( Mizuguchi  et al. , 1998a ), and the sequences were found by searching the nr database. After filtering, this yielded 423 soluble chains which were used to produce our standard soluble tables. The other three alignment sets (SUB177, SUB371 and HOMSTRAD) are structure-to-structure alignments used only to validate our standard soluble tables. SUB177 and SUB371 are described in the original FUGUE paper ( Shi  et al. , 2001 ). SUB177 is a set of 177 protein families comprising 706 structures used to build the default tables of FUGUE. SUB371 is a set of 371 protein families comprising 1357 structures used to test the stability of the SUB177-derived tables. The HOMSTRAD set comprises 1032 families and more than 3000 structures. 2.3 Table construction Membrane ESSTs are constructed as follows. The number of times a particular substitution is observed in an environment is tabulated in an environment specific counts matrix  A E  (where  E  labels the environment). Environments are determined by the annotations from iMembrane and JOY. For each structure in our set of 328 membrane protein alignments, every time a structure residue ‘ a ’ in environment  E  has a corresponding residue ‘ b ’ in one of the aligned sequences, the matrix element  A E ba  is increased by unity. The entries of the ESST  S E  are obtained from the following formula:
 (1) Given that the structure has a residue  a , the numerator of the logarithm is the probability of a substitution  a → b  in the matched sequence. The denominator is the probability that any substitution in any environment will go to  b  rather than another residue. The prefactors (and the taking of the logarithm itself) are a standard rescaling. ESSTs are generally asymmetric ( S E ba ≠ S E ab ), and are rounded to the nearest integer. The program JSUBST (a java derivative of SUBST available at  http://www.stats.ox.ac.uk/proteins/resources ) was used to construct the counts matrices  A E . Counts were made between clusters of similar sequences (60% sequence identity) and the cluster containing the structure. Each cluster was weighted by the number of sequences it contained as described in  Shi  et al.  (2001 ). Substitutions to and from gaps were not counted, but all columns in the alignments were included when constructing the matrices. A constant of 1/100 of a count was added to each entry  A E ba  to prevent  S E ba  evaluating to −∞ in rare cases. All sequences in the same cluster as the structure were annotated with its structural annotation for the purposes of matrix construction. Soluble tables were built in an analogous manner for each of the four sets of soluble alignments. 2.4 Identifying consistent tables How can we identify substitution tables that are unrepresentative of their environments? A crude method is to label as unrepresentative all those tables with fewer than a minimum number of counts. However, this method can run into problems—a rare environment might be extremely consistent in the substitutions it allows, such that the number of counts is small, but the data is representative. Here we use a combination of a count threshold and a ‘self-consistency’ score. The latter is obtained as follows. By normalizing the columns of a counts matrix  A E ba , we can interpret each entry as the probability that  a → b  in environment  E . When a vector of amino acid counts is multiplied by this matrix, it changes according to the mutation probabilities encoded in the matrix. After a large number of rounds of mutation (matrix multiplications), the resulting vector of amino acid counts is invariant under mutation. Mathematically, this vector is the eigenvector of the matrix with eigenvalue +1. It is assumed that the distribution of amino acids in a given environment, averaged over all proteins, is stable over time. Thus, a representative table should have a limiting distribution of amino acids that is close to the distribution observed in the alignments used to construct it. The self-consistency score, ‘ Q ’ is calculated according to Equation  2 :
 (2) 
where  v  is the eigenvector of the probability matrix with eigenvalue +1, and  w  is a normalized vector of the observed amino acid frequencies, which can be estimated as shown. This has the desirable property of taking values between 0 (totally inconsistent) and 1 (identical). A simple interpretation of this score exists. It is the maximum fraction of residues that could remain the same if substitutions occurred according to the probabilities encoded in the counts matrix over many iterations. The self-consistency score is scale-invariant, so it provides a measure of table quality that is independent of the number of counts.  Figure 2  shows a useful scheme for visually identifying poor tables. The fraction of the total number of counts and  Q  are plotted for each table with increasingly large subsets of the data. A stable counts matrix should tend to a stable level of  Q  as more data is included.
 Fig. 2. A high-quality table (IHA,  a ) and low-quality table (TPa,  b ). Each point is the fraction of total counts and consistency of a table when constructed with 20 more alignments than the preceding point. Some points are superimposed. 2.5 Table analysis and visualization The relative similarity of tables was visualized in two ways. Firstly a dendrogram was constructed based on the Euclidean distance between ESSTs. The dendrogram was built using single linkage clustering—meaning that new branches join existing clades based on the smallest distance between a member of the clade and the new branch. This linkage has the advantage that the dendrogram does not change under a rescaling of the data. Secondly, following the example of  Gong  et al.  (2009 ), a principal component analysis ( Hotelling, 1933 ) in multi-dimensional ‘substitution space’ was performed. This selects a set of 2 or 3 orthogonal axes that explain the greatest amount of variation in the data, and thus projects substitution space down into 2D or 3D with minimal distortion. 2.6 Sequence-to-structure alignment To test sequence-to-structure alignment, we take two homologous proteins of known structure and align the sequence of one (the target) to the structure of the other (the template). The alignments were made using FUGUE with the default tables, the PHAT/BLOSUM62 tables, and our membrane tables. The annotations from iMembrane ( Kelm  et al. , 2009 ) and JOY ( Mizuguchi  et al. , 1998b ) determined where each table was to be applied. Pairwise alignments were also made using the sequence-to-sequence alignment program MUSCLE. The quality of these alignments was assessed against the implicit sequence alignments generated by the structure-to-structure alignment program TM-align ( Zhang and Skolnick, 2005 ). The MEDELLER test-set ( Kelm  et al. , 2010 ) consists of pairs of homologous membrane proteins of known structure. We use one element of each pair as the target sequence, and the other as the template structure. We filtered the set such that no two templates, and no two target sequences, had more than 80% sequence identity. This left 408 pairs of proteins ranging from 0 to 100% identity, with a median sequence identity of 14%. The alignment of each template residue in the structure-to-structure alignment produced by TM-align was compared with the alignment of the same residue produced by one of the methods. A schematic of this procedure is given below. In this example, 9 residues are correctly aligned over a total alignment length of 10 residues. 3 RESULTS 3.1 Validation of substitution tables The tables used in FUGUE were obtained by counting substitutions between homologous structures. Due to the scarcity of membrane protein structures, we count substitutions between a structure and related sequences, following a similar method to that of  Mizuguchi  et al.  (2007 ). To assess the validity of this procedure we compared eight soluble ESSTs generated by this method with those derived from the SUB177, SUB371 and HOMSTRAD structure sets ( Table 2 ). From here onwards, soluble environments are labelled by a leading ‘s’.
 Table 1. A glossary of membrane protein environments First letter (layer) Second letter (secondary structure) Third letter (accessibility) H – lipid head H – helix A – accessible N – not in membrane E – beta strand a – inaccessible T – lipid tail C – coil P – pore-lining P – +ve ϕ I – interface region All combinations of letters are possible. The exception being that Pore-lining and Interface-region layers cannot be inaccessible as the definition of these layers requires them to have contacts with either the membrane or solvent. Positive ϕ environments will later be merged into just two environments labelled ‘NPA’ and ‘NPa’ (see  Section 3.2 ). 
 Table 2. The number of differences between soluble structure/sequence derived tables and their structure/structure derived counterparts Dataset sCa sCA sEa sEA sHa sHA sPa sPA SUB177 98 12 119 25 77 10 156 103 SUB371 57 7 78 19 41 1 138 99 HOMSTRAD 32 5 75 5 31 0 140 78 Number of table entries (out of a possible 400) that differ by more than 2 log-odds units from our soluble structure/sequence tables. This 2 unit threshold is chosen to be a reasonable measure of dissimilarity. Differences between structure/sequence and structure/structure derived tables decrease as the number of families included in the structure/structure set increases. Of the eight tables, larger differences are seen in the sEa (soluble, β-strand, inaccessible), sPa (soluble, +ve ϕ, inaccessible), and sPA (soluble, +ve ϕ, accessible) environments due to the greater number of rare substitutions in these environments. As the scores are logarithmic, small variations in the number of rare substitutions lead to disproportionately large effects on their log-odds scores. The small number of differences between the structure/structure and structure/sequence derived tables, particularly when larger numbers of structures are used, suggests that structure/sequence derived tables are representative of substitution preferences. Below, only the structure/sequence derived tables are compared. 3.2 Membrane environment selection Many +ve ϕ angle tables (*P*) suffer from low self-consistency scores, low count numbers, and poor stability. For example, the TPa environment of  Figure 2  has a  Q  score of 0.64 from 10 060 counts. Low self-consistency scores are to be expected: the majority of substitutions in these environments involve glycine, and other substitutions may be too rare to be representative. To increase table quality, the +ve ϕ environments were merged into an accessible +ve ϕ environment (NPA) and an inaccessible +ve ϕ environment (NPa). These are labelled ‘N’ layer so as to maintain a consistent notation. Self-consistency scores and total numbers of substitutions for each table in the resulting environment set are shown in  Table 3 . Each of our membrane tables has a three letter code of the form layer (H,N,T,P,I) : secondary structure (H,E,P,C) : accessibility (a,A).
 Table 3. Self-consistency scores and number of counts for each membrane environment specific substitution table ESST Q Counts ESST Q Counts NPa 0.72 36 437 PCA 0.96 45 654 NPA 0.88 146 253 PHA 0.96 96 771 TCa 0.90 46 512 HHA 0.97 147 118 NEa 0.92 137 556 THA 0.97 265 566 NHa 0.92 118 306 HEA 0.97 109 238 PEA 0.92 60 677 THa 0.97 171 736 TCA 0.92 44 326 HCA 0.98 228 302 HCa 0.93 65 124 TEA 0.98 211 862 HHa 0.93 63 665 NCA 0.98 350 138 NCa 0.93 113 341 IHA 0.98 56 569 ICA 0.95 44 362 IEA 0.98 34 660 HEa 0.95 48 262 NEA 0.98 148 662 TEa 0.95 102 801 NHA 0.98 253 458 Environment labels are described in  Section 2.1 . Accessible environment tables (**A) tend to have higher self-consistencies than inaccessible environment tables (**a). 3.3 Clustering of tables The Euclidean distance between the log-odds tables is used to create a ‘family-tree’ of the different environments ( Figure 3 ). Tables for soluble proteins, labelled with a leading ‘s’, are included for comparison. When calculating the distance, each substitution is normalized by its standard deviation across all the tables. This prevents the distance measure being dominated by a handful of extreme substitution changes.
 Fig. 3. Dendrogram of ESSTs. A split is seen between accessible (red) and inaccessible (blue) environments. Tail-layer environments (T**) appear not to cluster. Note that here, as elsewhere, ‘NPa’ and ‘NPA’ refer to combined +ve ϕ environments that include residues in the transmembrane regions (see  Section 3.2 ). It has been suggested that loops of membrane proteins that extend above and below the bilayer behave similarly to loops in soluble proteins (e.g.  Tastan  et al. , 2009 ). We also see this in our results, where each table of the form NC* clusters with its sC* counterpart. As might be expected, the not-in-membrane tables (N**) are most similar to their soluble equivalents (the notable exception being that sHA clusters with HHA rather than NHA). The tail-contacting environments are clear outliers, and do not cluster. Environments of the form T*A are dissimilar to both s*a and s*A. This is consistent with a number of other studies e.g.( Stevens and Arkin, 1999 ) that have found little evidence for the early ‘inside-out’ hypothesis of membrane protein structure ( Engelman and Zaccai, 1980 ). Additional outliers are +ve ϕ environments (*P*), and some β-strand environments (*E*). This lasts may be because much of our β-strand data comes from outer-membrane porins of Gram-negative bacteria. In Gram-negative bacteria, the outer membrane is asymmetric: the inner leaflet is composed of phospholipids whereas the outer leaflet is composed of lipopolysaccharides. Additionally, inward-facing solvent-exposed residues are in contact with the periplasm rather than the cytosol. Evidence for the uniqueness of the β-strand environments can also be seen in their composition. Accessible β-strands within the membrane rarely contain cysteine, and the TEA environment is abundant in tyrosine. The remaining environments separate by accessibility. Surprisingly, within the inaccessible clade ( Figure 3 , blue), the soluble secondary structure environments are more similar to each other than to their membrane equivalents. An accessible clade in  Figure 3  is coloured red from the same level as the inaccessible clade. The pore-lining and interface environments lie just beyond these clades, suggesting that these environments have distinct properties, and therefore that their use is sensible. A PCA plot allows patterns in substitutions to be discerned.  Figure 4  accounts for 48% of the variation in the data with 3 principal components.  Figure 4 c shows that the differences between accessible and inaccessible environments cause most of the variation between tables—they are largely separated along the first principal component (the main exceptions being accessible tail-layer tables, T*A). This first component can broadly be identified as a measure of ‘hydrophobicity’. Looking at the labelled points in  Figure 4 a, as the first principal component increases we move from tail layer to interface layer to head layer accessible environments, corresponding to decreasing hydrophobicity.
 Fig. 4. Principal component analysis of ESSTs. The top row and the bottom row are views of the same data along different principal components. The columns colour-code the data-points by layer type, secondary structure and accessibility, respectively. This allows the three-letter table code of each point to be read off from left to right. The labelled tables are ordered by secondary structure in the second principal component—reading panel ( e ) from left to right we first encounter TCa, then TEa, then THa. A similar ordering holds for other layer and accessibility types. The second principal component appears to relate to secondary structure. Moving from left to right in  Figure 4 e, we encounter the labelled points in the order TCa, TEa, THa as the second component increases. The same ordering is found for other layer types within the membrane. However, for soluble and not-in-membrane environments the order instead runs coil tables, helix tables, β-strand tables (e.g. sCA, sHa, sEa). The bottom row of plots shows that different secondary structure environments cluster in the second and third components. The third principal component appears to be dominated by the differences between β-strand environments. 3.4 Sequence-to-structure alignment The previous section discussed the variations in substitution preferences in different environments. Now we demonstrate that a knowledge of these differences improves sequence-to-structure alignment. Alignments were made with the sequence-to-sequence alignment program MUSCLE, and the sequence-to-structure alignment program FUGUE. Three different sets of substitution tables were used in the case of FUGUE (i) the default soluble tables, (ii) our membrane tables and (iii) the PHAT/BLOSUM62 tables in a bipartite scheme. In this last case, PHAT was applied to residues with a ‘T’ layer annotation (including pore-lining residues), and BLOSUM62 was used elsewhere. As the same program, FUGUE, was used with each set of tables, fair comparisons can be made between them. Gap penalties were determined separately for each set of tables. 3.5 Gap penalty determination In the case of FUGUE, the optimal alignment is that which maximizes the sum of the table entries  S E ba  for each pair of aligned residues. Not all residues will align, even between very similar proteins, and penalties to the alignment score must be determined for introducing gaps into the alignment. FUGUE distinguishes between several types of gaps (see  Shi  et al. , 2001  for details). Gaps are penalized in order of severity as follows:
 Gap within a secondary structure element (H) Gap at the end of a secondary structure element (L) Gap in a loop region (VL) Gap at a terminus (VVL) There are actually 8 types of gap penalty: each of the above categories can initiate a gap or be an extension of an existing gap. Initiating a gap results in a larger penalty than continuing an existing gap: the alignment is thus biased to a small number of large insertion/deletion events rather than a larger number of smaller events. A subset of 72 protein pairs was selected at random from the 408 pairs of proteins in the alignment dataset (see  Section 2.6 ), and alignments made with perturbations of the default FUGUE gap penalties. Perturbations were made such that gap opening penalties were at least as large as gap extension penalties, and such that more ‘severe’ gaps had penalties at least as large as less ‘severe’ ones. The size of the perturbation steps ranged from 1 to 5 units and depended on the size of the default penalties. The alignment quality with the default FUGUE tables differed little as the penalties were changed. In view of this, and as most users are unlikely to change the gap penalties, the default penalties were kept. For the membrane tables, only two gap initiation penalties were found to substantially differ from the default values ( Table 4 ). The increase of penalties in these cases is to be expected as the sizes of transmembrane secondary structure elements are constrained by the membrane thickness. The subsequent analysis uses these revised penalties, but membrane tables with the default gap penalties lead to similar results. The PHAT/BLOSUM62 tables are scaled differently from the others, so their penalties are not directly comparable.
 Table 4. Gap penalties for each set of tables used with FUGUE Tables Initiation Extension H L VL VVL H L VL VVL Default 28 20 20 8 4 4 2 2 Membrane 35 25 20 8 4 4 2 2 PHAT/BLOSUM62 26 24 16 8 6 4 2 2 3.6 Alignment accuracy Alignments were made for the remaining 336 pairs of proteins in the alignment dataset. None of the methods performed well in the 0–10% sequence identity range, but beyond this the membrane tables gave a consistent alignment advantage. At &gt;35% sequence identity, the alignments show few differences.  Figure 5  compares the alignments of membrane specific tables to common alternatives. PHAT/BLOSUM62 is omitted for clarity—its performance is comparable to that of default FUGUE.
 Fig. 5. Box plots of the fraction of residues aligned correctly as sequence identity increases. There are three boxes at each sequence identity, from left to right corresponding to membrane FUGUE (black), default FUGUE (blue) and MUSCLE (red). The green bars show the number of alignments divided by 100. For example, there are 78 alignments in the 5–10% sequence identity range. Consideration of the outliers in  Figure 5  is informative. In the 30–35% sequence identity range, the three methods in the figure appear to have correctly aligned only ~30% of the residues in one sequence-structure pair [PDB codes 1SU4A (sequence), 1XP5A (structure)]. In fact, these proteins are an identical rabbit Ca 2+ ATPase in different conformations. In this case the structure-to-structure alignment (by rigid-body superposition) from TM-align does not capture local similarities, leading to the 30–35% sequence identity figure, and the low assessment of performance of the other alignment methods. Outliers in the 0–10% sequence identity range are mostly due to short alignment lengths.  Figure 5  gives a broad picture of performance differences, but does not distinguish between a small alignment improvement on a short protein and a much larger improvement on a bigger protein.  Table 5  lists the number of times that membrane FUGUE correctly aligns at least 10 residues more (Win) or fewer (Loss) than another method.
 Table 5. Alignment quality of membrane tables versus other methods Identity (%) Number of alignments Membrane FUGUE versus Default FUGUE MUSCLE PHAT/BLOSUM62 Win Loss Win Loss Win Loss 0–5 53 12 5 14 4 11 3 5–10 78 32 10 29 8 30 6 10–15 49 36 4 43 1 33 6 15–20 20 10 2 14 0 11 2 20–25 30 8 0 18 0 12 3 25–30 26 4 1 14 0 6 1 30–35 14 1 0 6 0 3 1 &gt;35 66 1 2 3 1 1 0 Total 336 104 24 141 14 107 22 For each sequence identity range, the number of alignments where membrane FUGUE correctly aligns at least 10 more (Win) or 10 fewer (Loss) residues than the named alternative method. For example, in the 10–15% sequence identity range membrane FUGUE correctly aligns at least 10 more residues in 36 out of 49 alignments. Membrane FUGUE often improved alignment by more than 10 residues.  Table 6  gives the number of correctly aligned residues across all the alignments in each sequence identity bracket. Membrane FUGUE outperforms all other methods in all brackets, except at &gt;35% sequence identity where the differences between the methods are marginal.
 Table 6. Number of correctly aligned residues for each set of tables Identity (%) Number of residues Membrane FUGUE Default FUGUE MUSCLE PHAT/BLOSUM62 0–5 12 915 1132 872 910 1007 5–10 22 734 3571 2555 3001 2721 10–15 24 349 12 915 10 819 9893 10 427 15–20 7576 5042 4697 4145 4467 20–25 9156 6900 6607 6145 6565 25–30 5644 4608 4522 4300 4479 30–35 4792 3448 3403 3274 3402 &gt;35 18 881 17 578 17 586 17 547 17 545 Total 106 047 55 194 51 061 49 215 50 613 The highest number of aligned residues for each sequence identity range is shown in bold. If the alignment set is divided into α and β proteins the same trends in accuracy are seen for both, with membrane FUGUE outperforming the other methods. The principal difference is the scarcity of β-type alignment pairs at higher sequence identities. 3.7 Structure prediction Models were built with MEDELLER for each of the 336 default FUGUE, and membrane FUGUE alignments. Models were also built for the implicit sequence alignments from TM-align. MEDELLER provides different model-building options that prioritize accuracy or coverage. However, the relative quality of the models produced by different alignment methods showed little sensitivity to the model-building details. Results described below are for the default ‘high-accuracy’ models, but results for the ‘naive’ and complete models are similar. Reasonable alignments are only achieved from 15% sequence identity upwards ( Figure 5 ), and above 35% alignments differ little between methods. In the 15−35% sequence identity range the average RMSDs between the model and the native structure are: 3.4 Å (membrane FUGUE), 4.1 Å (default FUGUE), 2.0 Å (TM-align). The mean sequence identity is 24%. 4 DISCUSSION We have constructed substitution tables for membrane proteins by aligning single structures to multiple homologous sequences. This method, already used in the literature, allows a small number of structures to be leveraged to build tables at the cost of increased error in table construction. To address this problem, we suggest a method of assessing the quality of tables constructed in this way which allows us to build tables that are stable and consistent with the data used to construct them. A principal component analysis of the individual tables revealed that residues in contact with lipid-tails have some substitution preferences typical of hydrophobic regions. However, the differences in other substitution preferences mean that membrane proteins are not simply ‘inside out’. Globally, it appears that accessibility is the primary determinant of membrane substitution preferences, followed by secondary structure. Position within the membrane has a less clearly-defined, but substantial effect. For example, membrane tables showed greater variability than their soluble equivalents. This suggests that an environment-specific approach to membrane protein modelling will yield greater improvements than did the environment-specific approach to soluble protein modelling. Evidence for this supposition was found in a set of 336 alignments made by MUSCLE and FUGUE. MUSCLE is designed for sequence-to-sequence alignment, and so makes no use of structural information. It is unsurprising therefore that it performed worst at sequence-to-structure alignment. The default FUGUE tables and the bipartite PHAT/BLOSUM62 alignments performed better than MUSCLE and comparably to each other. Each makes use of different structural information—the default tables take into account the accessibility, secondary structure and hydrogen-bonding of a residue; whereas PHAT/BLOSUM62 distinguishes between residues inside and outside the membrane. Conflicting accounts of the performance of PHAT have previously been reported. It has been suggested that this is due to bad alignments when PHAT is applied to non-transmembrane residues ( Pirovano  et al. , 2008 ). The good alignments here can most likely be attributed to the quality of the transmembrane annotation from iMembrane. Our membrane tables distinguish between both membrane location, and secondary structure and accessibility. Compared with the best performing alternative tables, the use of the membrane tables led to 104 of the 336 alignments having at least 10 more correctly aligned residues, with only 24 alignments being worse by the same margin. These improved alignments translate into predicted structures with a lower average RMSD (3.4 Å membrane FUGUE, 4.1 Å default FUGUE) within the 15–35% sequence identity range. These results represent only a proof-of-principle for this approach. Here, to demonstrate the method we have considered only pairwise alignment, but multiple sequence information should further improve results. Alignment quality might also be enhanced by changes to the definition of when a residue is in contact with the head or tail layers of a membrane, or from the introduction of a bipartite scheme of gap penalties in which insertions and deletions are punished more severely in the transmembrane region. More radically, alignment might be improved by an iterative approach to table construction. The tables presented here were generated by counting substitutions between homologous sequences aligned to a single structure by MUSCLE. Instead, these alignments could be made by FUGUE using the membrane tables. The resulting improved substitution tables could then be used to realign the sequences. This procedure could be iterated until convergence. Our substitution tables, which take into account the environments of residues in membrane proteins, substantially improve alignments between membrane protein sequences and structures. In turn, these improved alignments lead to better structural models of membrane proteins. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>KEA: kinase enrichment analysis</Title>
    <Doi>10.1093/bioinformatics/btp026</Doi>
    <Authors>Lachmann Alexander, Ma'ayan Avi</Authors>
    <Abstract>Motivation: Multivariate experiments applied to mammalian cells often produce lists of proteins/genes altered under treatment versus control conditions. Such lists can be projected onto prior knowledge of kinase–substrate interactions to infer the list of kinases associated with a specific protein list. By computing how the proportion of kinases, associated with a specific list of proteins/genes, deviates from an expected distribution, we can rank kinases and kinase families based on the likelihood that these kinases are functionally associated with regulating the cell under specific experimental conditions. Such analysis can assist in producing hypotheses that can explain how the kinome is involved in the maintenance of different cellular states and can be manipulated to modulate cells towards a desired phenotype.</Abstract>
    <Body>1 INTRODUCTION Protein phosphorylation causes the addition of a phosphate group onto serine, threonine or tyrosine amino-acid residues of proteins. Phosphorylations are precise reversible changes that are used to regulate intracellular events such as protein complex formation, cell signaling, cytoskeleton remodeling and cell cycle control. Consequently, protein kinases, which are responsible for the phosphorylations, play an important role in controlling protein function, cellular machine regulation and information transfer through cell signaling pathways. Kinase activities therefore have definitive regulatory effects on a broad variety of biological processes, in which activated kinases typically target a large number of different substrate proteins. There are over 500 protein kinases encoded in the human genome, and it is approximated that 40% of all proteins are phosphorylated at some stage in different cell types and at different cell states (Manning  et al. ,  2002 ). Furthermore, kinases regulate each other through phosphorylation, resulting in a complex web of regulatory relations (Ma'ayan  et al. ,  2005 ). High-throughput techniques such as stable isotope labeling coupled with affinity purification and mass-spectrometry proteomics are now able to identify phosphorylation sites on multiple proteins under different experimental conditions. Databases that integrate the results from such studies are emerging, e.g. phosphosite (Hornbeck  et al. ,  2004 ). However, such data does not provide the kinases responsible for the phosphorylation. Several resources are available to link identified phosphorylation sites to the kinases that are most likely responsible for protein phosphorylations (Huang  et al. ,  2005 ; Linding  et al. ,  2008 ). For example, NetworKIN (Linding  et al. ,  2007 ; Linding  et al. ,  2008 ) uses an algorithm to predict the most probable kinase that is responsible for phosphorylating an identified phosphosite. The NetworKIN algorithm is accompanied with a database containing ∼1450 predicted mammalian substrates that are mapped to 73 upstream protein kinases belonging to 21 kinase families. Although useful, the coverage of this dataset is not comprehensive enough for kinase statistical enrichment analysis. To achieve more comprehensive prior knowledge kinase–substrate dataset, large enough for statistical enrichment analysis, we merged interactions from several other online sources reporting mammalian kinase–substrate relations. Additionally, we included binary protein—-protein interactions involving kinases from protein–protein interaction databases as these were recently proposed to be highly enriched in kinase–substrate relations: in a recent study that identified ∼14 000 phosphosites at different stages of the cell cycle in Hela cells (Dephoure  et al. ,  2008 ) it was shown that many phosphosites experimentally identified using phosphoproteomics can be associated with four known kinases (CDC2, PLK1, Aurora-B and Aurora-A) using the literature-based protein–protein interactions from the HPRD database (Mishra  et al. ,  2008 ). Hence, having a large background knowledge dataset of kinase–substrate interactions and protein–protein interactions that involve kinases, we can associate large lists of proteins/genes with many kinases that phosphorylate them. This allows the computation of statistical enrichment which can be used to suggest the kinases that are most likely to be involved in regulating the proteins/genes from a list generated under specific experimental conditions. 2 IMPLEMENTATION We first constructed a database that consolidates kinase–substrate interactions from multiple online sources. We integrated data describing kinase–substrate interactions from NetworKIN (Linding  et al. ,  2008 ), Phospho.ELM (Diella  et al. ,  2004 ), MINT (Chatr-aryamontri  et al. ,  2007 ), HPRD (Mishra  et al. ,  2008 ), PhosphoPoint (Yang  et al. ,  2008 ) and Swiss-Prot (Quintaje and Orchard,  2008 ) as well as phosphorylation interactions we manually previously extracted from literature (Ma'ayan  et al. ,  2005 ). The NetworKIN database contains 3847 kinase–substrate unique pairs made of 73 kinases (21 families) linked to 1452 substrates. HPRD contains 1794 kinase–substrate pairs made of 229 kinases linked to 864 substrates. Phospho.Elm has 1451 interactions between 225 kinases and 784 substrates. MINT has 269 interactions between 145 kinases and 184 substrates. In phosphoPoint there are 436 kinases, 3076 substrates, 9251 kinase–substrate relations from which only 1587 are unique in this dataset, while the rest overlaps with the other databases. In Ma'ayan  et al. , there are 66 interactions between 19 kinases and 43 substrates. There is some overlap among these sources such that the number of unique kinase–substrate relations totals 6414 links between 352 kinases and 2014 substrates in the combined dataset. We consolidated interactions from mouse and rat into human by converting all protein/gene IDs to human Entrez gene symbols. Each kinase–substrate data record is associated with a specific kinase, kinase family and kinase subfamily. To group kinases into families, we used the kinome tree from Manning  et al.  ( 2002 ) where kinases are classified into 10 major classes and 119 families. To further increase the size of our background dataset, we included all direct protein–protein interactions involving kinases from HPRD (Mishra  et al. ,  2008 ) and MINT (Chatr-aryamontri  et al. ,  2007 ). By this expansion the current dataset contains a total of 11 923 interactions between 445 kinases having 3995 substrates. The analysis begins with an input list of gene symbols entered by the user for kinase enrichment analysis (KEA). Before performing the KEA, we remove all input entries that do not match a substrate in the consolidated background kinase–substrate dataset. This step is necessary for achieving proportional comparison. The expected value for a randomly generated list of kinase–substrates can be found by determining the cardinality of the set of substrates that are targeted by specific kinases (or family of kinases) dividing such number by the total number of substrates in the background dataset. In order to detect statistical significant deviations from this expected value, we use the Fisher Exact Test (Fisher,  1922 ). The  P -value can be used to distinguish specific kinases among the large number of kinases appearing in the output table. To implement the web-based system we use Java Server Pages (JSP) and MySQL database running on a Tomcat server. All reported results can be exported to Excel via CSV files. Additionally, users can mouse over on the number of targets for each kinase, kinase family or class to see the list of substrates and view a connectivity diagram that visualizes known protein–protein interactions within the substrates using a database of protein–protein interactions we previously published (Berger  et al. ,  2007 ). The map is dynamic where users can move nodes around and click on nodes for more detail ( Fig. 1 ). The visualization of these connectivity diagrams was achieved using Adobe Flash CS4 with ActionScript. Such subgraphs can be used to link kinase specific substrates to pathways and complexes.
 Fig. 1. Screenshot of the KEA user interface. Users can paste lists of Entrez gene symbols, representing human proteins; select the level of analysis: kinase-class, kinase-family or kinase and then the program outputs a list of ranked kinase-classes, kinase-families or kinases based on specificity of phosphorylating substrates from the input list. Substrates can be then connected based on their known protein–protein interaction using an original network viewer developed using Adobe Flash CS4. 
 As prior knowledge is increasingly used to interpret high-throughput results, e.g. Balazsi  et al.  ( 2008 ), we anticipate that KEA is going to be especially useful for the analysis of proteomics and phosphoproteomics data. KEA can be used for analyzing multivariate datasets collected on a time-course to observe trends in kinase activity overtime. Results that show changes in kinase enrichment under different conditions can be due to one of the following reasons: change in kinase enzymatic activity, change in kinase subcellular localization or changes in kinase concentration. Furthermore, KEA can help researchers understand how they can perturb cellular systems toward a desired phenotype by targeting a kinase or group of kinases with pharmacological or gene silencing means. Kinase signaling is well-established to be disturbed in many disease states, especially in cancer (Blume-Jensen and Hunter,  2001 ), while it is apparent that phenotypic integrity is controlled by the activity of the regulated behavior of multiple kinases. Hence, mapping kinase activation patterns based on different experimental conditions and time points when measuring many genes/proteins at once in diseased/perturbed versus normal/control may directly suggest combinations of kinase inhibitors that would shift the cellular state towards a desired phenotype. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Modelling BioNano optical data and simulation study of genome map assembly</Title>
    <Doi>10.1093/bioinformatics/bty456</Doi>
    <Authors>Chen Ping, Jing Xinyun, Ren Jian, Cao Han, Hao Pei, Li Xuan, Hancock John</Authors>
    <Abstract/>
    <Body>1 Introduction Genome physical mapping technologies have received increasing attention in recent years because of their ability to effectively complement the shortfalls of short-read sequencing technologies. Whereas the early physical mapping methods, such as OpGen optical mapping, were often expensive and labour intensive( Paux  et al. , 2008 ), the newly developed BioNano Irys system greatly enhances the throughput and performance of physical mapping by leveraging advances in nanoscale material engineering, fluorescent labelling of DNA molecules and imaging processing algorithms ( Lam  et al. , 2012 ). BioNano optical mapping technology rapidly expanded its applications in many aspects of genome research, e.g. assessing and guiding sequence assembly ( Chen  et al. , 2017 ;  Zhihai  et al. , 2016 ), identification of satellite repeated sequences ( Dong  et al. , 2016 ), resolving haplotypes ( Pendleton  et al. , 2015 ) and the detection of structural variations ( Cao  et al. , 2014 ). As for popular sequencing technologies, data biases/errors are inevitable in the BioNano optical mapping system. BioNano optical mapping experiments are vulnerable to variations and perturbations of internal and external sources, creating data biases and variations that have profound implications for downstream applications. A non-random molecule distribution can hamper the haplotype resolution in regions of low coverage depth. Labelling errors on DNA molecules, i.e. BioNano molecules, can lead to false signals and incorrect variant calls. However, very little is known about the properties, biases and error rates of BioNano data or the factors that induce or contribute to them. The purpose of this study is threefold. First, it was designed to investigate the properties such as molecule length, false labelling signal, variation in optical resolution, coverage distribution bias and the data variations due to factors such as chimeric molecules, fragile sites and DNA molecule stretching, which have great impact on common applications of BioNano optical mapping technology. Based on our analyses, we developed statistical models to model the properties, biases and error profiles of BioNano molecule data and provided guidelines for filtering BioNano data to mitigate their impact on downstream applications. Second, we developed a novel optical data simulation program, BioNano Molecule SIMulator (BMSIM), for the generation of simulated BioNano molecule data. With available models, BMSIM can also be extended to simulate other optical mapping experiments. Third, we employed BMSIM to evaluate the impact of variable factors on the outcome of whole-genome optical map assembly. Specifically, the impacts of the coverage depth, molecule length, false signal rates, nicking enzyme and chimeric molecule were evaluated. We summarize the general rules for the design of whole-genome optical map projects and present guidelines for data processing and gauging the parameters for whole-genome optical map assembly. 2 Materials and methods 2.1 Data source and basic processing We generated BioNano molecule data from eight organisms ( Supplementary Table S1 ; and  Supplementary Methods  Section S1.1). Molecule data were then aligned and compared to their reference genomes, which were converted  in silico  into a restriction map according to the recognition signature of the nicking enzyme Nt.BspQI. Alignment was performed using RefAligner. Custom scripts (in Perl) were developed to process outputs from RefAligner and to extract information, e.g. aligning the positions of molecules and signals of labelling sites, etc. for further analysis. Statistical analysis and plotting were performed using R and MATLAB (R2016a). 2.2 Modelling the distribution of BioNano molecule length To model the BioNano molecule length in the BioNano data simulator BMSIM, a molecule (with a size of  ι ) was first randomly selected from a distribution of molecules whose sizes are described by an exponential distribution  ι   ∼     exp   o n e n t i a l ( λ )  with the average size  λ . Next, the molecule with size  ι  was randomly sliced from one chromosome of the reference genome. If the molecule end exceeded the end of the chromosome, the shorter molecule was also retained. 2.3 Modelling for false-positive (FP) and false-negative (FN) labelling sites It was reported ( Valouev, 2006 ) that the false-positive sites resulting from random DNA nicks or the nonspecific action of endonuclease have no preference for regions of DNA molecules. Thus, the number of false positives,  R F P , per fixed length of DNA ( s Kb), obeys a Poisson distribution with intensity parameter  λ  (nick/Kb), i.e.  R F P ∼ P o i s s o n ( λ s ) . The FPs were modelled as a homogeneous Poisson process with the rate  λ  in BMSIM. It was also reported ( Das  et al. , 2010 ) that the labelling of each site is independent with an efficiency of  p . Thus, the probability  P  of molecules with  m  sites labelled from a total of  n  sites observed is  n ! m ! ( n - m ) ! P m ( 1 - P ) n - m . Thus, whether true nick sites are labelled can be modelled as independent Bernoulli trials. Therefore, to model missing label events (FNs), we treated the nicking of each site as a Bernoulli event with the probability of success  P  in BMSIM. 2.4 Modelling for DNA molecule stretch variation in the BioNano system DNA stretching is a complex interaction of several stochastic variables, such as the Brownian motion of DNA ( Tegenfeldt  et al. , 2004 ), salt concentration ( Jo  et al. , 2007 ) and flow voltage changes ( Reccius  et al. , 2008 ), many of which have unknown distribution functions. It was reported ( Chan, 2004 ) that the stretching of DNA molecules with free termini is not homogeneous. We devised the ‘stretch variation factor’  R  as a measurement of stretch variation for each DNA fragment between neighbouring sites in the BioNano system. Stretch variation factor  R  was defined as  R = l e n m e a s u r e d / l e n r e f e r e n c e , where the reference length,  l e n r e f e r e n c e , represents the number of DNA base between neighbouring sites, as obtained from the reference genome sequence aligned to the DNA fragment, and the measured length,  l e n m e a s u r e d , was computed for the DNA fragment between neighbouring sites by converting the pixels into a base number with a constant predefined for the Irys system (e.g. a constant of 500 bases per pixel, corresponding to 85% of the theoretical maximal DNA stretching;  Lam  et al. , 2012 ;  Shelton  et al. , 2015 ). According to the central-limit theorem ( Dedecker, 1998 ), the distribution of the stretch variation factor  R  for each region of DNA between neighbouring sites in the BioNano system was assumed to be Gaussian,  R ∼ N ( μ , σ 2 ) . 2.5 Modelling for the observed variation in optical resolution Under ideal conditions, the likelihood of resolving two particles,  p r e s o l v i n g  was defined as a unit step function  H ( x ) :
 H ( x ) = { 0 , x &lt; t 1 , x &gt; t 
which is a discontinuous function whose value is 0 when the distance between neighbouring sites  x  is smaller than a resolution threshold  t , and 1 when the distance between neighbouring site  x  is larger than a threshold  t . However, we found that the optical resolution of the BioNano system is influenced by variations in the Irys machine conditions (e.g. temperature, flow-rate, emitted fluorescence light, vibration, sample density and sample/solution purity) and sample preparations (DNA quality, nicking and labelling reactions and fluorescent dye quality/quantity). The combined function of these variables led to the variation in optical resolution, which we observed when operating the BioNano system. According to the Central Limit Theorem ( Dedecker, 1998 ), the observed variation in optical resolution tends to have the effect of convolution with a Gaussian kernel. Under the net effect of convolving with a Gaussian kernel, the step function of the likelihood of resolving the neighbouring site  p r e s o l v i n g  was expected as a cumulative Gaussian kernel, i.e. the convolution of the unit step function  H ( x )  with a Gaussian function  G ( x )  ( Broadbent, 1954 ;  Hirschman and Vernon, 1955 ;  Wang  et al. , 2005 ), which is defined as
 
where  x  is the distance between two neighbouring sites,  G ( v )  is the Gaussian function  G ( v ) = 1 σ 2 π exp ( − ( v − μ ) 2 2 σ 2 ) . Model parameter estimation was based on non-linear regression using the curve fitting tool from MATLAB (R2016a). Model parameters were estimated with a mean of  μ = 1.2 , and a standard deviation of  σ = 0.9 . Accordingly, two neighbouring sites 1.2 Kb apart have a 50% chance of being resolved, and two neighbouring sites 1.2 + 2 * 0.9 = 3 Kb apart have an approximately 97.5% probability of being resolved. 2.6 Modelling for fragile sites A fragile site occurs when two nicking sites are located on opposite strands in close proximity. Its occurrence probability can be modelled as a function of the distance between two nicking sites on opposite strands. It should follow the exponential model that was previously used to estimate the breakage probability related to length ( Griebel  et al. , 2012 ;  Iyengar, 1981 ). The curve of the likelihood of breakage against the distance between nicking sites on opposite strands was found to fit that of exponential decay. Therefore, the model of fragile site formation, i.e. the likelihood of breakage  p breakage  against the distance ( dis ) between nicking sites on opposite strands, was defined by the exponential model:  p breakage = a *   exp ( b * dis ) , where  dis  is the distance between nicking sites on opposite strands,  a  and  b  are two empirical parameters (a = 0.7758, b = −0.006984) that are estimated from empirical data using cftool. In simulation experiments using the BioNano molecule simulator BMSIM, a Bernoulli trial was performed on  p breakage  to determine whether a break occurred at a fragile site of a simulated BioNano molecule. 2.7 Simulation for chimeric molecules Chimerism resulted primarily from the concatenation of unrelated molecules in a nanochannel during imaging. Types of chimera included ‘Bimera’, ‘Trimera’ or higher-order chimeras. To simulate random concatenation events in BMSIM, chimeric molecules were generated in the following steps: i) assigning a target proportion of overall chimeric molecules; ii) determining the ratio of Bimera, Trimera and higher-order chimeras; iii) randomly selecting parent molecules and joining them with random orientations (to simplify simulation, we assumed randomly occurring overlapping regions between parent molecules); and iv) repeating step iii until the target properties are attained. 3 Results 3.1 Characterizing properties of BioNano genome mapping data 3.1.1 Distribution of the BioNano molecule length The molecule length is an essential property of BioNano data. Because genomic DNA is randomly broken by a shearing force, BioNano molecules should be uniform in distribution along a genome. The shearing of DNA molecules follows a homogeneous Poisson process; thus, the molecule lengths, as independent and identically distributed exponential variates, should fit an exponential model ( Sarkar, 2006 ). Our results confirmed that the length distribution was consistent with an exponential model among the eight datasets ( Supplementary Fig. S1 ), which was also validated by quantile plot analysis ( Supplementary Fig. S2 ). Notably, the length of the BioNano molecules is often benchmarked with the N50 values, which varied between 133 and 246 Kb for the eight datasets ( Supplementary Table S1 ). 3.1.2 Distribution of false-positive and false-negative sites The enzymatic process for DNA nicking and labelling is prone to errors. However, the distribution of false signals has not been fully investigated for BioNano genome mapping technology. The two types of errors were gauged for BioNano molecules: i) false-positive (FP) signals observed at non-restriction sites and ii) false-negative (FN) signals at restriction sites with missed labels. FP errors were mainly attributed to naturally occurring nicks on DNA molecules ( Jo  et al. , 2007 ;  Neely  et al. , 2011 ;  Xiao  et al. , 2007 ;  Zohar and Muller, 2011 ), non-specific digestion by enzymes ( Lam  et al. , 2012 ) or noise generated during the imaging process, e.g. shot noise ( Thompson  et al. , 2002 ). FN errors, in contrast, were due to incomplete nicking or labelling ( Das  et al. , 2010 ;  Xiao  et al. , 2007 ). We characterized the profile of FP and FN errors using the eight datasets by aligning and comparing them to their reference genomes. The FP error rates were found to vary from 2.2 to 9.04%, with an average of 5.2%, whereas the FN error rates were much higher on average: 12.2% ranging from 5.25 to 15.9% ( Supplementary Table S1 ). We further looked at whether FP and FN errors were biased along DNA molecules. DNA molecules were first divided into five equally sized intervals. The FP rates had a uniform distribution along DNA molecules ( Fig. 1 , left panel), which appeared to have no bias towards any regions of DNA molecules (Kruskal-Wallis rank sum test,  P  = 0.978), thus supporting the random assumption. In contrast, the FN error rates were significantly elevated in the middle intervals of DNA molecules (Kruskal-Wallis rank-sum test:  P  &lt; 0.05) ( Fig. 1 , right panel).
 Fig. 1. Distribution of false-positive (FP) and false-negative (FN) signals along whole molecules. BioNano molecules are divided into five equal-sized intervals, 0–20, 20–40, 40–60, 60–80 and 80–100%. For FN rates, pairwise comparison between three middle interval groups (20–40, 40–60, 60–80%) and two outside groups (0–20, 80–100%) showed the three middle interval groups significantly differed from the two outside groups ( t -test:  P  &lt; 0.05) 3.1.3 Data variations due to the stretching of DNA molecules Stretching of DNA molecules under various conditions, e.g. salt concentration, temperature, electrical field strength, nano-channel pore size, etc. may cause changes in the measured length of BioNano molecules, thus affecting downstream applications such as mapping or the assembly of BioNano molecules. We investigated the changes in the stretch variation factor  (Section 2 ) between nanochips, between flow cells of the same chips, between runs for the same flow cells, and between scans within a run. First, although no difference in the stretch variation factor was detected between flow cells within the same nanochips ( Supplementary Fig. S3 , second row, Chip2 Flowcell1 versus Chip2 Flowcell2: Kruskal-Wallis rank-sum test,  P  &gt; 0.05), a significant difference in the stretch variation factor was observed between different nanochips ( Supplementary Fig. S3 , first row, Chip1 versus Chip2; Kruskal-Wallis rank-sum test,  P  &lt; 0.05). Second, in practice, multiple runs (with the same sample preparation) were often applied to the same flow cells, for which a new run was often started with the addition of a new sample volume. Changes in the stretch variation factor between different runs of the same flow cells can occasionally be significant ( Supplementary Fig. S3 , third row; and  Supplementary Fig. S4G ). Changes in the stretch variation factor between different runs were found to be larger than within the same run. Third, considering the scans within runs, we found that the stretch variation factor decreased during a single run ( Supplementary Fig. S3 , fourth row). Reduction in stretch variation factor were the result of the increased salt concentration ( Kim  et al. , 2011 ). We noticed that higher flow rates and a short run time reduced the impact of salt concentration on the stretching variation ( Supplementary Fig. S4F ). We further considered the distribution of the stretch variation factor for molecules of the same run. They were found to approximate a Gaussian distribution ( Supplementary Fig. S5 ; Section 2.4), which was used to model the DNA molecule stretch variation in the BioNano molecule simulator BMSIM. 3.1.4 Variation in the optical resolution of the BioNano system and its modelling We investigated the resolution within our BioNano datasets that was likely a combined function of many variables. By aligning BioNano molecules to their reference sequences, we analyzed the distribution of neighbouring labelling sites, and their relationship to distance on the reference genome. We found that neighbouring sites within 1–3 Kb of each other were only partially resolved by BioNano with resolution limit approaching 1 Kb ( Fig. 2 ). The likelihood of two labelling sites being distinguished decreased as the distance between them was reduced from 3 to 1 Kb, which was modelled as a cumulative Gaussian distribution (Section 2.5). The fitness of the model to the observed data was confirmed using a curve fitting test ( Fig. 2 ). Notably, the eight BioNano datasets displayed a high consistency in curve fitting. In addition to the optical resolution that determines the minimum detectable unit, the presence of restriction sites in a region is required to map structural features by BioNano. Thus, the frequency of restriction sites is another important factor limiting the detectability of the BioNano genome mapping system. Based on the profile, the variation in optical resolution for BioNano molecules was modelled as a cumulative Gaussian distribution in the BioNano molecule simulator BMSIM (Section 2.5).
 Fig. 2. Variation in the optical resolution of the BioNano system, as revealed by the ratio distribution of resolved neighbouring sites in BioNano molecules from eight organisms. ‘Resolved neighbouring site ratio’ represents the ratio of the resolved neighbouring sites to the total neighbouring sites on BioNano molecules at the same distance. Dots are the distribution for BioNano molecule data, and lines represent the fitting curves of the cumulative Gaussian distribution. R2 (coefficient of determination) and RMSE (root mean squared error) were computed for each dataset 3.1.5 Formation of chimeric molecules The formation of chimeric molecules from the joining of multiple unrelated genomic regions has not been systematically characterized for the BioNano genome mapping system to date. Because the BioNano experiments did not involve PCR or DNA-ligation steps, the chimerism most likely resulted from the concatenation of random DNA molecules in nanochannels during imaging. By analyzing the BioNano molecules that were aligned to two or more regions in the reference genome, we identified the joining of two, three or more BioNano molecules from unrelated genomic regions ( Fig. 3A ). Among the eight BioNano datasets, on average, ∼15% of all molecules were chimeric ( Fig. 3B and C ) and shared a similar profile of chimerism. ‘Bimera’ was the most common type, accounting for ∼87% of all chimeric instances. Higher-order chimeras, such as ‘Trimera’ and ‘Quadramera’, occurred at a much lower frequency, accounting for ∼12 and ∼1%, respectively. Considering the frequency of chimeric events for molecules of different length, not surprisingly, we found that longer molecules generally had a higher frequency of chimeric events ( Fig. 3D ). For example, molecules longer than 300 Kb were found to have an approximately 30% chance to be chimeric.
 Fig. 3. Formation of chimeric BioNano molecules. ( A ) Examples of chimeric molecules identified in our  E.coli  sample sets. To identify a chimaera, some  more stringent  thresholds were used, i.e. mapping confidence score &gt;8 (for anchoring molecule regions to reference genome) and the ratio of the anchored regions to the total molecule length &gt;70%. The lines represent the boundary for aligned regions between chimeric molecules and reference genomes. ( B ) Chimera fraction detected for each BioNano molecule dataset. ( C ) Proportion of Bimera, Trimera and Quadramera among all chimeric molecules for each BioNano molecule dataset. ( D ) Longer molecules having a higher rate of chimeric events 3.1.6 Coverage distribution bias and fragile sites BioNano molecules should distribute uniformly over genomes because of the shotgun process. Ignoring the edge effect, it was an equivalent of a homogeneous Poisson process ( Sarkar, 2006 ). To examine the coverage uniformity, we aligned the BioNano molecule data to those of reference genomes and tabulated the coverage depth per labelling site for each genome to produce coverage density plots ( Fig. 4A ). The coverage distributions of  P.pudita  and  D.melanogaster  were found to deviate from theoretical Poisson distribution. To identify the cause of deviation, we looked more closely at the  P.Putida  dataset, which had over 1000-fold average coverage and severe deviation ( Fig. 4A ). Eight spots with dramatically lower coverage were found to correspond to so-called fragile sites in the  P.putida  genome ( Fig. 4B , dots).
 Fig. 4. Coverage distribution bias and fragile sites of BioNano molecules. ( A ) Density plots of the coverage depth per labelling site for eight organisms. The solid (BioNano) and dashed (Poisson) lines represent the distributions of real molecules and the theoretical Poisson model with the same mean, respectively. Note that the gap regions and mapped outliers (count greater than two times the median count) along the chromosome are excluded. ( B ) Coverage depth and fragile sites along the  P.putida  genome. The dots represent the predicted fragile sites. ( C ) Curve fitting of the breakage likelihood versus deduced neighbouring site distance. The distance between two neighbouring sites was deduced based on their locations on the reference genome. The breakage likelihood of fragile sites is designated 1 minus the relative coverage (RCOV) at potential fragile sites. The relative coverage (RCOV) is the ratio of the coverage depth at a fragile site to the average coverage depth of the whole genome. The inner panel illustrates the distribution of RCOV versus the binned neighbouring sites distance. When the distance between neighbouring sites increases to ≥400 bp, RCOV is close to 1, indicating a low (near 0) likelihood of breakage. SSE, the sum of squares due to error; R-square, coefficient of determination; Adjusted R-square, degree-of-freedom adjusted coefficient of determination; RMSE, root mean squared error To determine the distance range between sites in a fragile site, we plotted the likelihood of breakage against the distance between nicking sites on opposite strands ( Fig. 4C  inner panel). The likelihood of breakage was estimated using the relative coverage of aligned molecules, i.e. a lower coverage representing higher breakage likelihood. We concluded that a fragile site was likely to form when two nicking sites on opposite strands were within 400 bp of each other ( Fig. 4C  inner panel, dash line). The potential fragile sites for the eight datasets in our study were calculated ( Supplementary Table S2 ). The likelihood of breakage increased as the two nicking sites on opposite strands getting closer to each other, following a curve approximating exponential decay ( R 2  = 0.7615;  Fig. 4C ) (Section 2.6). Uneven labelling density along a genome was another factor that might contribute to coverage bias. By analyzing the relationship between the uniformity of the labelling site distribution and coverage depth, we found a correlation (averaged  R  = 0.41) between the local label density (labelling sites/100 Kb) and the relative coverage depth, indicating a biased coverage against genome regions with sparse labelling sites. The existence of large sparse labelling regions in the  P.pudita  genome ( R  = 0.63), and  D.melanogaster  genome ( R  = 0.76), may explain their derivations from theoretical Poisson distribution in coverage depth. Apart from fragile sites and uneven labelling site distribution, molecule length, alignment parameters ( Supplementary Figs S6 and S10 ) as well as repetitive elements, gaps and ambiguous sequences in reference genomes, DNA stretching variations, FN or FP errors and chimeric molecules may play a role in making coverage distribution over-dispersed and/or skewed. 3.2 BioNano molecule simulator (BMSIM) Based on the properties and statistical models of the BioNano molecule data (detail information of the models can be found in Section 2 and  Supplementary Methods ), we developed a program, BioNano Molecule SIMulator (BMSIM), to generate BioNano molecules data  in silico . BMSIM comprises five major steps:  in silico  fragmentation, nicking site labelling, error modelling, SNR score assigning and coverage depth iteration ( Fig. 5 ). To evaluate the performance of BMSIM, we generated simulated BioNano molecule data for four organisms ( E.coli  PL,  P.putida ,  S.pombe  and  O.sativa ). The synthetic datasets were found to closely match the measured properties of the experimental data ( Supplementary Table S3 ). Remarkably, BMSIM reproduced coverage bias due to fragile sites. When the function for simulating fragile sites was disabled for BMSIM, the signature of fragile site bias, i.e. reduced coverage depth at fragile sites disappeared ( Supplementary Fig. S7 ).
 Fig. 5. Scheme diagram of BioNano Molecule SIMulator (BMSIM). BMSIM comprises of five major steps:  in silico  fragmentation, nicking sites labelling, error modelling, SNR score assigning and coverage depth iteration (see  Supplementary Methods  Section S1.3 for details) 3.3 Evaluation of factors impacting whole-genome optical map assembly using simulated BioNano data The assembly of a whole-genome optical map is an essential application of BioNano genome mapping technology, which can facilitate the construction of a reference genome, resolving highly repetitive sequences and correcting of some gnome assembly errors. The factors that impact whole-genome optical map assembly have not been systematically investigated, and their effects have not been quantitatively evaluated. Thus, it is imperative to investigate how whole-genome optical map assembly is affected by the biases and errors of BioNano molecule data and to seek the optimal conditions/parameters under which the best practice of BioNano experiments should be carried out. To perform the study, we used synthetic datasets generated with BMSIM, which simulated broad data features/conditions that were not possible without the BioNano data simulator. The simulated datasets were assembled using the BioNano in-house Assembler Version 3370 from the IryView package. The analysis was designed to investigate the effects that factors such as coverage depth, molecule length, FN/FP errors, chimerism, enzyme selection and nicking/labelling site density had on the outcome of whole-genome optical map assembly. 3.3.1 Coverage depth of BioNano molecule data Coverage depth is a critical factor for the design of BioNano experiments. To estimate the sensitivity and specificity at various coverage depths, we generated simulated BioNano data for four organisms of various genome sizes:  S.coelicolor, H.sapiens ,  D.melanogaster  and  O.sativa.  Although the other factors, e.g. average length, FN/FP, nicking site density and chimeric molecules, remained constant among the four datasets, each genome was assembled with BioNano molecule data of various coverage depths between 10× and 100× ( Fig. 6A ). The results of assemblies were evaluated with assembly completeness, contig N50 and contig number. While a high assembly completeness (95%) was achieved at 40× coverage depth for a smaller genome, larger genomes required a much higher coverage depth (100×) to reach the same level. However, contig N50 behaved similarly among the genomes below 80× coverage depth and started to differentiate above that point ( Fig. 6A  middle panel). Notably, for large genomes, e.g.  D.melanogaster  and  O.sativa , a coverage depth of 80× was required to have a contig N50 larger than 1 Mb. For assembly fragmentation, we observed that the number of contigs increased initially before it was reduced ( Fig. 6A  right panel). The turning point was intrinsic to each genome, probably closely related to its genome size.
 Fig. 6. Simulation study of whole-genome optical map assembly using synthetic BioNano molecule data by BMSIM. ( A ) Effect of coverage depth on results of whole-genome optical map assembly. Assembly completeness is the ratio of assembled map size to reference genome size. ( B ) Effect of BioNano molecule length (N50) on results of whole-genome optical map assembly. The genome of  O.sativa , IRGSP1.0, was used in the simulation analysis. ( C ) Impact of FP/FN error rates on results of whole-genome optical map assembly. The genome of  O.sativa , IRGSP1.0, was used in the simulation analysis. The variable error rate of 0.05, 0.10, 0.15 and 0.20 was simulated with the other one fixed at 0.1, and coverage depth at 40, 60, 80 and 100×. The result plane of variable FN rates has a steeper slope than that of variable FP rates. ( D ) Effect of different chimera fractions on results of whole-genome optical map assembly. The genome of  O.sativa , IRGSP1.0, was used in the simulation analysis. ( E ) Effect of nicking enzyme selection and nick site density on results of whole-genome optical map assembly. Synthetic BioNano molecule data were generated  in silico  with sampled genomes of the eight organisms with different GC contents ( Supplementary Table S4 ) using four nicking enzyme, Nt.BspQI, Nb.BbvCI, Nb.Bsml and Nb.BsrDI. Labelling ratio, the ratio of labelling density to nick density; valid molecule length ratio, the ratio of valid molecule (&gt;100 Kb) N50 with fragile sites to valid molecule (&gt;100 Kb) N50 without fragile sites; valid molecule quantity ratio, the ratio of valid molecule (&gt;100 Kb) quantity with fragile sites to valid molecule quantity (&gt;100 Kb) without fragile sites; assembly completeness, the ratio of assembled map size to reference genome size 3.3.2 Molecule length of BioNano molecule data The length of BioNano molecules can vary greatly depending on the organism, experimental protocol for DNA isolation, and often the skill of the researcher. Thus, we determined whether the length of the BioNano molecules (assuming the same standard distribution as described above) impacts the outcome of map assembly. We generated synthetic BioNano molecule data of the  O.sativa  genome (∼380 Mb), with N50 lengths at 140, 170, 200 and 230 Kb and coverage depths fixed at 40, 60, 80 and 100×. Our results showed that, at a lower coverage depth, the molecule length had a greater impact on the assembly completeness ( Fig. 6B , left panel). However, a larger molecule length needed only a low coverage depth to achieve a high completeness rate. As the molecule length increased, contig N50 became larger, accompanied by the decrease in the contig number ( Fig. 6B  middle and right panels). 3.3.3 FP/FN errors of BioNano molecule data The FN and FP rates of the BioNano data varied depending on the quality of DNA samples and conditions of the nicking/labelling reactions. It was unclear how FN and FP errors impacted the outcome of whole-genome optical map assembly. Using BMSIM, we generated simulated BioNano data for  O.sativa  (∼380 Mb) with combinations of different FN and FP rates (5, 10, 15 and 20%). For this analysis, the coverage depth was fixed at 40, 60, 80 and 100×. The result plane of variable FN rates had a steeper slope than that of variable FP rates ( Fig. 6C ), suggesting the FN rate had a greater influence on whole-genome optical map assembly completeness than the FP rate. The assembly completeness dropped sharply from 99 to 0.04, 0.07, 0.5 and 2% as the FN rate increased from 5 to 20% for the coverage depths of 40, 60, 80 and 100×, respectively ( Fig. 6C ). On the other hand, the assembly completeness decreased from 99 to 7, 20, 36 and 49% as the FP rate increased to the same extent. However, at high error rates, e.g. 20%, of either FP or FN, the result of whole-genome optical map assembly became unacceptable. An increased coverage depth may have been able to compensate for the slightly elevated error rates of FP or FN. For example, with an FN rate of 15% (FP = 10%), the coverage depths of 80× and 100× led to assembly completeness values of 50 and 95%, respectively ( Fig. 6C ). With an FP rate of 15% (FN = 10%), the coverage depths of 80× and 100× resulted in assembly completeness values of 67 and 81%, respectively. 3.3.4 Chimerism of BioNano molecule data Molecule chimerism is a common issue for BioNano molecule data and might cause assembly errors using overlapping graphs. To evaluate the impact of chimeric molecules on whole-genome optical map assembly, simulated BioNano molecule data (with coverage depths of 40× and 80×) were generated for  O.sativa  with chimera fractions at 0, 10, 25 and 40%. Whereas chimeric molecule artificially increased the valid molecule quantity (&gt;100 kb), the molecule length, label density, FP rate and FN rate changed little ( Fig. 6D ). In contrast, as the chimera fraction increased, the molecule mapping rate, the molecule usage, the assembly completeness and contig N50 were reduced. Further, the assembly with lower coverage molecule data had a much greater impact in the presence of chimeric molecules ( Fig. 6D ). Notably, the experimentally generated BioNano datasets had an average chimera fraction between 8.7 and 26.4% ( Fig. 3B ). Thus, molecule chimerism is an important factor when considering BioNano experiments for whole-genome optical map assembly. To minimize the chimeric rate, we may reduce the DNA concentration when running DNA samples through nanochips. 3.3.5 Choice of nicking enzyme and nick site density The choice of nicking enzyme is genome specific and critical to produce informative data for whole-genome optical map assembly. BioNano currently offers the choice of four nicking enzymes for labelling, Nt.BspQI, Nb.BbvCI, Nb.Bsml and Nb.BsrDI. To understand how different enzymes and their nicking patterns affect the outcome of whole-genome optical map assembly, we designed and performed a comprehensive analysis using sample genomes of organisms with different GC content,  D.discoideum  (GC% = 23),  S.ratti  (GC% = 22),  A.thaliana  (GC% = 37),  H.sapiens  (GC% = 41),  D.melanogaster  (GC% = 43),  O.sativa  (GC% = 44),  P.putida  (GC% = 63) and  S.coelicolor  (GC% = 73) ( Supplementary Table S4 ). BioNano molecule data were generated  in silico  with the four available nick enzymes using BMSIM, producing a spectrum of nick site densities ranging from 1.0 to 82.2/100 Kb that were translated to labelling densities ranging from 2.0 to 34.3/100 Kb due to the collapse of nearby sites ( Fig. 6E , and  Supplementary Table S4 ). The properties and quality of simulated BioNano molecule data were evaluated using the labelling ratio (ratio of the labelling density to the nicking density), valid molecule (&gt;100 Kb) length ratio (ratio of the valid molecule length with fragile sites to that without fragile sites) and valid molecule (&gt;100 Kb) quantity ratio (ratio of the valid molecule quantity with fragile sites to that without fragile sites). As expected, assuming an optimized nicking reaction protocol, we observed a decrease in the labelling ratio, valid molecule length ratio and valid molecule quantity ratio with the increase in nick site density ( Fig. 6E ) because a higher nick density would result in more fragile sites, thus making the molecule length shorter. For example, with a nick site density at 23.85/100 Kb, the labelling density was 15.8/100 Kb, a ∼34% drop in the labelling ratio, a ∼20% drop in the valid molecule length ratio and a ∼22% drop in the valid molecule quantity ratio. In the extreme case of a nick site density at 62.6/100 Kb, the labelling density would be 28.4/100 Kb, with a 70% decrease in the valid molecule quantity ratio. Thus, for enzymes that produced a high nick site density, the reduction of the labelling ratio and valid molecule became significant. Next, we evaluated the effects of nicking enzymes and the nicking site density on the outcome of map assembly. We observed that although the labelling density ratio remained high at low nick site density (&lt;8.9/100 Kb; labelling density &lt; 7.1/100 Kb), the assembly completeness was poor ( Fig. 6E , dash line). In contrast, at a high nick site density (&gt;42.0/100 Kb; labelling density &gt; 20.2/100 Kb), a 50% drop in the labelling density led to the assembly completeness dropping significantly to below 80%. Thus, decreasing the labelling ratio is an important signal to monitor, and new experiments may be warranted if the ratio becomes too low. Considering the results, it was suggested that enzymes with a nick site density above 40/100 Kb or below 9.0/100 Kb should be avoided. For enzymes with a nick site density in the range of 20–40/100 Kb, the labelling ratio, valid molecule length ratio, valid molecule quantity ratio and assembly completeness declined substantially. Hence, a nick site density between 9 and 20/100 Kb (labelling site density between 7 and 14/100 Kb) would be optimal to guarantee high-quality molecule data and complete map assembly at an acceptable cost ( Fig. 6E , shaded regions). 4 Discussion To investigate the property, bias and error profile of BioNano data, we generated BioNano molecule data and exploited organisms with varying genome sizes. Although we revealed many common descriptive properties for physical mapping data, some pertain only to the BioNano system. Summary of the properties of BioNano data and the observation of these properties are listed in  Supplementary Table S6 . The FP and FN signals are distributed differently for BioNano molecules. While FP signals are random events with a physically uniform distribution along DNA molecules, FN signals are elevated in the middle intervals of DNA molecules, attributed to the tertiary structure of DNA molecules in nicking and labelling reaction solutions that limit the access of enzymes. DNA molecule stretching varies between different nanochips, runs and scans and is likely affected by factors such as nano-channel size, labelling reagent and salt concentration. Optical resolution of the BioNano system varied with the low boundary approaching ∼1 Kb, whereas neighboring sites within 1 Kb of each other may be detected but un-reliable. The likelihood of two neighbouring sites within 1–3 Kb being resolved follows a cumulative Gaussian distribution. Chimeric molecules, on average, account for ∼15% of total molecules and have a substantially higher frequency for longer DNA molecules (&gt;300 Kb). A good option is to reduce chimeric rates of long molecules, by removing long molecules with a backbone intensity greater than a certain threshold that are likely chimeric. Additional work is needed to explore the method to detect the cases of multi-molecule pileups in nano-channel, and exam its relationship with occupancy. The coverage distribution of BioNano molecules is found to be deviated from homogeneous Poisson, for which fragile sites, sparse labelling, molecule length, repetitive elements, DNA molecule stretching variation, chimerism and reference genome quality, among others, are likely contributing factors. For whole-genome optical map assembly, low coverage due to sparse labelling can be remedied by increasing the overall molecule length or total BioNano data. However, higher coverage depth is often useless for fragile sites. For example, although up to 1783× BioNano molecule data were generated for  P.putida , its whole-genome optical map assembly was still fragmented due to break-up at four fragile sites. For such cases, we propose that a ‘stitching’ procedure utilizing rare molecules covering fragile sites or a combination of BioNano maps with different nicking enzymes can be explored. In addition, algorithmic enhancements to BioNano pipeline, for example, normalization for DNA molecule stretching, chimeric molecule analysis with split/partial alignments or detection/filtering of false signals, may be pursued to improve optical data mapping and genome map assembly. Coverage depth and molecule length are two essential variables for BioNano experiments, and it is important to put them in perspective for whole-genome optical map assembly. It is apparent that as genomes become larger, coverage depth becomes more critical for the three measurements of whole-genome optical map assembly: the assembly completeness, contig N50 and contig number ( Fig. 6A ). Under normal experimental conditions, a 40× to 60× coverage depth is sufficient for the map assembly of bacterial genomes, but an 80× to 100× coverage depth is required for eukaryotic genomes over 100 Mb in size. When the assembly completeness approaches 100%, the contig N50 and contig number for whole-genome optical map assembly continue to improve with higher coverage. Thus, we recommend even higher coverage when designing BioNano experiments for applications that desire long continuity of whole-genome optical maps. Increasing the length of BioNano molecule data has a significant effect (somewhat surprising) on the quality of whole-genome optical map assembly, as evidenced by the near linear correlation between molecule length and completeness of assembly, contig N50 and contig number ( Fig. 6B ). Although these results demonstrate the benefit of a large molecule length in whole-genome optical map assembly, more often in practice, the benefit of a large molecule length is not achievable due to the technical difficulty to preserve long DNA through isolation and labelling steps. FP/FN signals and chimeric molecules are two intrinsic factors that may not be easy to control or manipulate. However, it is critically important to understand the impact of their variations on whole-genome optical map assembly. FP/FN signals are found to have an uneven effect on whole-genome optical map assembly, with an increase in the FN rate having a greater influence than that in the FP rate ( Fig. 6C ). We suggest using a sufficient amount of enzyme and possibly a longer reaction time in the nicking/labelling steps to keep the FN rate low. Additionally, based on simulation results, the quality filter parameter ‘snrFilter’ in BioNano assembly pipeline can be relaxed to avoid missing true positive signals with the cost of a slightly higher FP rate. Furthermore, an increased coverage depth may be recommended to mitigate the effect of elevated FN and FP rates for whole-genome optical map assembly. Note that, among the eight BioNano datasets generated experimentally, the FP and FN rates were found to be no greater than 9.04 and 15.9%, respectively ( Supplementary Table S1 ). Thus, for genomes over 100 Mb in size, a coverage depth of 80× to 100× should be at least considered for whole-genome optical map assembly. The effect of chimeric molecules on whole-genome optical map assembly was found to be more severe at a low coverage depth than at high coverage ones ( Fig. 6D ). However, at a higher coverage depth (e.g. ≥80×) the impact of chimeric molecules on whole-genome optical map assembly was minimized. Nicking enzyme selection is often the first decision to make in BioNano experiments. Researchers often rely on general suggestions from BioNano to perform test runs for an unknown genome. By simulating the whole spectrum of nicking and labelling densities with combinations of enzymes and synthetic datasets of organisms with variable GC content, we revealed a complete picture of variable nicking/labelling densities with the outcome of whole-genome optical map assembly. With higher nicking densities, increases in fragile sites and FN rates severely reduce the labelling ratio, valid molecule length ratio and valid molecule quantity ratio. In particular, the decrease in the labelling ratio is the benchmark signal to monitor. With the labelling ratio decreasing to 50%, whole-genome optical map assembly collapses and becomes unacceptable in terms of completeness and fragmentation. In such a scenario, new experiments with different nicking enzymes should be pursued. Our results demonstrate that a nick site density between 9 and 20/100 Kb (labelling site density between 7.0 and 14/100 Kb) is the optimal range for BioNano experiments. For a new organism, these parameters may be estimated using a closely related genome or Illumina sequencing data. Further, to develop a guideline on how to choose appropriate nicking enzymes for organisms of various GC contents, we obtained a total of 128 genomes covering broad phylogenetic systems ( Supplementary Table S7 ). Basing on their nicking/labelling density predicted using recognition sequences of nicking enzymes, we recommend the nicking enzymes Nb.Bsml and NbBsrDl for genomes with a low GC content (&lt;25%), Nt.BspQI for those with a medium GC content (25–40%), and both Nt.BspQI and Nb.BbvCI for those with a higher GC content (&gt;40%) ( Supplementary Fig. S11 ). BMSIM simulator can be extended to other applications. For example, it can help investigate on the robustness of haplotype-sensitive assembly for various BioNano data. By simulating haplotype blocks of various length for diploid genomes, the accuracy and sensitivity of haplotype-sensitive assembly can be evaluated. Funding This work was supported in part by grants from the Ministry of Agriculture of China (2016ZX08010-002), the National Science and Technology Major Projects (2018ZX09711003) and the National Natural Science Foundation of China (Nos. 31401128, 31571310, 31771412) and by Special Fund for Strategic Pilot Technology Chinese Academy of Sciences (XDA08020104). 
 Conflict of Interest : none declared. Supplementary Material Supplementary Information Click here for additional data file. Supplementary Table S1 Click here for additional data file. Supplementary Table S2 Click here for additional data file. Supplementary Table S3 Click here for additional data file. Supplementary Table S4 Click here for additional data file. Supplementary Table S5 Click here for additional data file. Supplementary Table S6 Click here for additional data file. Supplementary Table S7 Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Drug repositioning based on bounded nuclear norm regularization</Title>
    <Doi>10.1093/bioinformatics/btz331</Doi>
    <Authors>Yang Mengyun, Luo Huimin, Li Yaohang, Wang Jianxin</Authors>
    <Abstract/>
    <Body>1 Introduction The process of new drug discovery is time-consuming and tremendously expensive ( Chong  et al. , 2007 ). It has been showed that the average time of developing a new drug is more than 13.5 years and the cost exceeds $1.8 billion dollars ( Paul  et al. , 2010 ). Discovering new and reliable indications for commercialized drugs allows the pharmaceutical industry and the research community to reduce time and costs, because the existing commercialized drugs have already owned safety, efficacy and toleration data after various tests and clinical trials. The process of identifying new applications for existing drugs is known as drug repositioning. In fact, some successfully repositioned drugs, such as sildenafil, raloxifene and thalidomide, have generated generous revenues for their patent holders or companies. Therefore, drug repositioning is an effective strategy for developing new drugs. Computational drug repositioning has attracted increasing attention, since manual investigation is time-consuming. With the development of high throughput technology and continuously updating databases, quite a few computational approaches have been proposed, including network-based analysis, machine learning, text mining and semantic inference approaches. The network-based methods are popular and fundamental for drug repositioning. Based on a network of drugs, diseases and targets (proteins),  Martinez  et al.  (2015)  proposed an approach named DrugNet to predict new use for existing drugs. DrugNet can perform both drug–disease and disease–drug prioritization by propagating information in the heterogeneous network.  Gottlieb  et al.  (2011)  integrated drug similarities and disease similarities to obtain primary features to support a computational approach called PREDICT to identify unknown drug–disease associations.  Wang  et al.  (2013)  constructed a heterogeneous drug–target graph, which contains intra-similarity information and drug–target association information. Based on the guilt-by-association principle, heterogeneous graph based inference (HGBI) algorithm ( Wang  et al. , 2013 ) was proposed to predict new drug–target associations. HGBI is also used for predicting drug–disease associations ( Wang  et al. , 2014 ).  Luo  et al.  (2016)  exploited the available information of drug–disease associations to enhance drug similarity and disease similarity. The MBiRW algorithm, which used some comprehensive similarity measures and Bi-Random Walk (BiRW) algorithm, is implemented on the drug–disease heterogeneous network to predict potential drug–disease associations. Matrix factorization and matrix completion techniques have been applied to drug repositioning in recent years.  Dai  et al.  (2015)  incorporated the interaction network of genes and developed a matrix factorization model. Taking advantage of the information in genes network, the association between drug and disease can be predicted and new indications for known drugs can be obtained.  Luo  et al.  (2018)  constructed a heterogeneous network by integrating drug–drug network, disease–disease network and drug–disease association network, and then R 4 SVD ( Li and Yu, 2017 ) was employed to efficiently compute the dominant singular values and the corresponding singular vectors of the association matrix. Based on the Singular Value Thresholding (SVT) algorithm ( Cai  et al. , 2010 ), a Drug Repositioning Recommendation System (DRRS) has been proposed to rank the potential associations between drugs and diseases by completing the drug–disease association matrix. In fact, the methods based on random walks are equivalent to certain special cases of those using matrix completions. For example, MBiRW is equivalent to finding the eigenvector with respect to the largest eigenvalue of the association matrix. However, the above matrix completion algorithms are operated in a noiseless setting, assuming that the drug–disease associations are correctly derived and the disease–disease as well as drug–drug similarities are accurately measured. But in reality, drugs and diseases vary in many aspects and it is difficult to construct a single measure to precisely describe the similarity relationship among drugs or diseases. Occasionally, such similarity is misleading. For example, a disease caused by bacteria may have highly similar symptoms as one caused by virus, which should be treated by completely different drugs. Moreover, in the matrix completions algorithms, typically, 1’s in the drug–disease association matrix denote known drug–disease associations while 0’s represent the unknowns. The predicted values are expected to be within the range of [0, 1], indicating the likeliness of the predicted associations. However, the above matrix factorization and completion approaches are unable to avoid the situations that the predicted values fall out of the [0, 1] range, which brings difficulty in biological interpretation. In this study, assuming that similar drugs share the similar molecular pathway to treat similar diseases, we consider the prediction of drug–disease association as a noisy matrix completion problem and develop a bounded nuclear norm regularization (BNNR) method to address this problem. First of all, we construct a heterogeneous drug–disease network, which is composed of drug–drug, drug–disease and disease–disease sub-networks. Then, BNNR is implemented to recover the missing entries in the adjacency matrix of this heterogeneous network while tolerating the potential noise in drug–drug and disease–disease similarities calculations. Finally, we evaluate the performance of BNNR on various datasets and compare it with several state-of-the-art methods. Our results show that our approach has superior capability of predicting hidden drug–disease associations. The main contributions of our BNNR model include:
 BNNR performs noisy matrix completion by incorporating nuclear norm regularization, which effectively addresses overfitting and leads to better improved accuracy as shown in our results; Our BNNR model incorporates a range constraint, which enforces all predicted matrix entry values within the specific interval; Our BNNR model is able to deal with noisy data efficiently; and An efficient iterative scheme is designed to numerically solve the BNNR model. 2 Materials and methods In this section, we describe the BNNR model to predict the potential indications for existing drugs, which is organized as follows. First, we describe the datasets used in this study. Then, we depict the construction of the drug–disease heterogeneous network and its adjacency matrix to be completed. Finally, we present the BNNR model, solved by alternating direction method of multipliers (ADMM), to fill out the unknown associations between drugs and diseases. The overall workflow of BNNR is illustrated in  Figure 1 .
 Fig. 1. The overall workflow of BNNR. ( a ) Drug–drug network and its similarity matrix. ( b ) Disease–disease network and its similarity matrix. ( c ) Drug–disease association network and its association matrix. ( d ) The heterogeneous drug–disease network and its adjacency matrix. ( e ) The model of BNNR 2.1 Datasets We use the gold standard dataset to predict new drug indications, which is obtained from ( Gottlieb  et al. , 2011 ) collecting comprehensive associations from multiple data sources. There are 593 drugs, 313 diseases and 1933 validated drug–disease associations. Drugs are collected from the DrugBank database ( Wishart  et al. , 2006 ) and diseases are extracted from the Online Mendelian Inheritance in Man (OMIM) dataset ( Ada  et al. , 2002 ). The similarities between drugs are calculated by the Chemical Development Kit (CDK) ( Steinbeck  et al. , 2003 ) according to the chemical structures of all drug compounds in the Canonical Simplified Molecular Input Line-Entry System (SMILES) ( Weininger, 1988 ). We firstly download the Canonical SMILES format of all drugs from DrugBank. Then, we utilize CDK to calculate a binary fingerprint for each drug. Finally, the Tanimoto score ( Tanimoto, 1958 ) measuring the similarity of pairwise drugs is calculated with respect to their chemical fingerprints, which is in the range of [0, 1]. Disease–disease similarities are obtained from MimMiner ( Van Driel  et al ., 2006 ), which measure the number of appearance of MeSH (medical subject headings vocabulary) terms of two diseases in the medical descriptions obtained from the OMIM database. 2.2 Construction of the heterogeneous network We construct a heterogeneous drug–disease network, which integrates the drug–drug, disease–disease and drug–disease association networks. Let  R = { r 1 , r 2 , . . . , r m }  and  D = { d 1 , d 2 , . . . , d n }  denote a set of  m  drugs and  n  diseases, respectively. For the drug–drug network, the edge between two drugs is weighted by the pairwise drug similarity value. Similarly, the edge between two diseases is weighted by the pairwise disease similarity value. Then, the drug–disease association network is treated as a bipartite graph  G ( R ,  D ,  E ), where  E ( G ) = { e ij } ⊆ R × D  contains edges representing known associations between drug  r i  and disease  d j . In this heterogeneous drug–disease network, drug–drug network and disease–disease network are connected by drug–disease associations.  Figure 1a–d  illustrates the construction of the heterogeneous network. The adjacency matrix of the drug–disease heterogeneous network is then defined as:
 M = A RR A DR T A DR A DD , 
where the sub-matrices  A RR  and  A DD  denote the adjacency matrices of drug network and disease network and their weights are set as the pairwise drug and disease similarities, respectively, in range [0, 1].  A RR  and  A DD  are dense which include rich correlation information among drugs and diseases. In contrast, due to the fact that drug–disease associations are rare,  A DR  is usually extremely sparse, where 1’s denote known drug–disease associations and 0’s correspond to the unknowns. After all, our goal is to fill out the unknown elements in  A DR  as the predicted scores of potential associations between drugs and diseases. 2.3 BNNR for predicting drug–disease associations Assuming a low-rank structure, the general matrix completion problem ( Ramlatchan  et al. , 2018 ) to fill out the missing entries is formulated as:
 min X ⁡   rank ( X ) s . t . P Ω ( X ) = P Ω ( M ) , 
where  M ∈ R ( m + n ) × ( m + n )  is the given incomplete matrix,  rank (.) denotes the rank function, Ω is a set containing index pairs ( i ,  j ) of all known entries in  M  and  P Ω  is the projection operator onto Ω.
 ( P Ω ( X ) ) ij = X ij ,   ( i , j ) ∈ Ω   0 ,     ( i , j ) ∉ Ω . Unfortunately, the rank minimization problem is known to be NP-hard. The rank minimization in the above matrix completion model is often relaxed to a nuclear norm minimization problem such that:
 (1) min X ⁡ ‖ X ‖ * s . t . P Ω X = P Ω M , 
where  ‖ X ‖ *  denotes the nuclear norm of  X , which is defined as the sum of all singular values of  X . The nuclear norm minimization model is a convex optimization problem. Many algorithms have been designed to provide numerical solutions for the above model or alternative forms, including the fixed point continuation with approximate SVD (FPCA) ( Ma  et al. , 2011 ), the accelerated proximal gradient algorithm (APG) ( Toh  et al. , 2010 ), the SVT algorithm ( Cai  et al. , 2010 ) and the ADMM ( Boyd  et al. , 2011 ;  Chen  et al. , 2012 ;  Wen  et al. , 2010 ).  Candes  et al.  (2013)  showed that the solution obtained by optimizing the nuclear norm is equivalent to the one by rank minimization under certain conditions, minimizing the nuclear norm. For predicting drug–disease associations, the elements in the drug similarity matrix  A RR  and disease similarity matrix  A DD  are within the interval of [0, 1]. The elements in the association matrix  A RD  are either 0 or 1. As a result, the predicted values in the unknown entries are expected to be in the interval of [0, 1], where a predicted value closer to 1 indicates that this is likely to be an indication and vice versa. Nevertheless, in the above matrix completion models (1), the entries in the completed matrix can be any real value in (− ∞ , + ∞ ). A predicted value out of the interval [0, 1] is meaningless in the application context. Hence, it is important to add a bound constraint to the matrix completion model to ensure that the uncovered missing elements are within the interval of [0, 1]. Moreover, since there may be a lot ‘noise’ in the drug and disease data, particularly when measuring the drug–drug and disease–disease similarities, the drug repositioning model should effective tolerate the potential noise. A matrix completion model to tolerate noise is:
 min X ⁡ ‖ X ‖ * s . t . | | P Ω ( X ) - P Ω ( M ) | | F ≤ ϵ , 
where ϵ measures the noise level. However, for this model with the inequality constraint, choosing the appropriate parameter is challenging, because the noise level is not explicitly known. Moreover, it is not straightforward to come up with an efficient solver for this model. Therefore, we relax the constraint satisfaction model into a regularization model. Introducing the soft regularization term not only enables tolerance to the unknown noise ( Chen  et al. , 2012 ;  Hu  et al. , 2013 ;  Ma  et al. , 2011 ;  Toh  et al. , 2010 ), but also provides computational convenience. Putting all pieces together, we propose a BNNR method, which minimizes the nuclear norm as the regularization term and ensures the recovered matrix elements within a specific interval. The BNNR model is described as follows:
 (2) min X ‖ X ‖ * + α 2 ‖ P Ω ( X ) - P Ω ( M ) ‖ F 2 s . t .   0 ≤ X ≤ 1 , 
where α is parameter balancing the nuclear norm and the error term. Note that we use  0 ≤ X ≤ 1  to denote  0 ≤ X ij ≤ 1  for all elements in  X  throughout this paper. We derive a simple but effective numerical scheme using ADMM to solve (2). Model (2) is solved by an iterative method. Starting from the initial solution   X 1 = P Ω ( M ) . It is important to notice that the objective function in (2) is convex. By introducing an auxiliary matrix  W , (2) can be optimized using the ADMM framework in the following equivalent form.
 (3) min X ⁡ X * + α 2 P Ω ( W ) - P Ω ( M ) F 2 s . t .     X = W ,     0 ≤ W ≤ 1.   Accordingly, the augmented Lagrangian function becomes
 (4) L ( W , X , Y , α , β ) = ‖ X ‖ * + α 2 ‖ P Ω ( W ) - P Ω ( M ) ‖ F 2 + Tr ( Y T ( X - W ) ) + β 2 ‖ X - W ‖ F 2 , 
where  Y  is the Lagrange multiplier and β &gt; 0 is the penalty parameter. At the  k- th iteration, BNNR requires alternatively computing   W k + 1 ,   X k + 1  and   Y k + 1 . 
 Compute 
   W k + 1 : We fix  X k  and  Y k  to minimize  L ( W , X k , Y k , α , β )  for   W k + 1 . We hereby take full advantage of the inverse operator to obtain an exact and closed-form solution.
 (5) W k + 1 = arg   min 0 ≤ W ≤ 1   L ( W , X k , Y k , α , β ) = arg   min   0 ≤ W ≤ 1 α 2 ‖ P Ω ( W ) - P Ω ( M ) ‖ F 2   + Tr ( Y k T ( X k - W ) ) + β 2 ‖ X k - W ‖ F 2 . Here,  W * is the optimal solution of  arg   min W   L ( W , X k , Y k , α , β ) , if and only if
 (6) α P Ω * ( P Ω ( W * ) - P Ω ( M ) ) - Y k - β ( X k - W * ) = 0 
holds, where  P Ω *  denotes the adjoint operator of   P Ω . Then, a closed-form solution becomes
 (7) W * = ( I + α β P Ω * P Ω ) - 1 ( 1 β Y k + α β P Ω * P Ω ( M ) + X k ) = ( I - α α + β P Ω * P Ω ) ( 1 β Y k + α β P Ω * P Ω ( M ) + X k ) = ( 1 β Y k + α β P Ω ( M ) + X k ) - α α + β P Ω ( 1 β Y k + α β P Ω ( M ) + X k ) , 
where  I  is the identity operator.  ( I + α β P Ω * P Ω ) - 1  denotes the inverse operator of  ( I + α β P Ω * P Ω )  and is equal to  I - α α + β P Ω * P Ω  ( Yang and Yuan, 2012 ). It’s worth noting that   P Ω * P Ω = P Ω . Considering the interval  [ 0,1 ]  constraint, we limit the range of the elements of  W k + 1  to [0, 1] such that
 (8) W k + 1 = Q [ 0,1 ] ( W * ) , 
where  Q [ 0,1 ]  is the projection operator defined as
 ( Q [ 0,1 ] ( W * ) ) ij = 1 ,     W ij * &gt; 1 W ij * ,   0 ≤ W ij * ≤ 1 0 ,     W ij * &lt; 0 . Compute   X k + 1 : Alternatively, we fix  W k + 1  and  Y k  to compute   X k + 1 .
 (9) X k + 1 = argmin X   L ( W k + 1 , X , Y k , α , β ) = argmin X ‖ X ‖ * + Tr ( Y k T Y k T ( X - W k + 1 ) ) + β 2 ‖ X - W k + 1 ‖ F 2 = argmin X ‖ X ‖ * + β 2 ‖ X - ( W k + 1 - 1 β Y k ) ‖ F 2 = D 1 β ( W k + 1 - 1 β Y k ) , where  D τ ( X )  is the singular value shrinkage operator ( Cai  et al. , 2010 ;  Ma  et al. , 2011 ) defined as
 D τ ( X ) = ∑ i = 1 σ i ≥ τ ( σ i - τ ) u i v i T , 
where σ i  is the singular values of  X  which is larger than τ, while  u i  and  v i  are the left and right singular vectors corresponding to σ i , respectively. 
 Compute 
   Y k + 1 : Finally,  Y k + 1  is calculated as
 (10) Y k + 1 = Y k + γ β ( X k + 1 - W k + 1 ) ,   γ ∈ ( 0 , 5 + 1 2 ) , 
where γ is the learning rate, which is set to 1 in this study for simplicity ( Hu  et al. , 2013 ). Putting all pieces together, Algorithm 1 presents an iterative BNNR scheme for solving (2). Based on the assumption that similar diseases tend to be treated by similar drugs, because of the common molecular pathways, there exist certain low-rank structures governing drug–disease associations. Minimizing the nuclear norm of the target matrix, BNNR reveals the low-rank structures and provides a way to recover the missing entries. After supplying the adjacency matrix of the drug–disease heterogeneous network to BNNR, we can obtain an updated drug–disease association matrix   A DR * , where the unknown entries in  A DR  are filled up. The entries in  A DR *  with predicted values (scores) close to 1 indicate the potential drug–disease associations. 
 Algorithm 1 BNNR Algorithm 
 Input : The drug similarity matrix  A RR ∈ R m × m , the disease similarity matrix   A DD ∈ R n × n , the drug–disease association matrix   A DR ∈ R n × m , parameters α and β. 
 Output : Predicted association matrix   A DR * . 
 M ← A RR A DR T A DR A DD ;   
 initialize  X 1 = P Ω ( M ) , W 1 = X 1 , Y 1 = X 1 , γ = 1 ;  //Ω is a set of indices of all known entries in  M . 
 k ← 1 ; 
 repeat 
 
 W k + 1 ← Q [ 0,1 ] ( W * ) ; 
 
 X k + 1 ← D 1 β ( W k + 1 - 1 β Y k ) ; 
 
 Y k + 1 ← Y k + γ β ( X k + 1 - W k + 1 ) ; 
 
 k ← k + 1 ; 
 until convergence 
 
 [ A R R * A D R * T A D R * A D D * ] ← W k ; 
 
 return 
 A DR * . 
 
 3 Results and discussion 3.1 Evaluation metrics To evaluate the performance of BNNR, a 10-fold cross-validation is conducted to verify the candidate diseases for given drugs. All known drug–disease associations are randomly divided into 10 exclusive subsets of approximately equal size. Each subset is treated as the testing set in turn, while the remaining nine subsets are used as the training set. The 10-fold cross-validation is repeated 10 times with random subset division and the average accuracy values are showed as the final results. After the association matrix of the drug–disease heterogeneous network is completed, the predicted scores of all drug–disease associations are obtained. For each drug, the predicted scores of its associations with the diseases are ranked in descending order. The score of the candidate association exceeding a given threshold is considered as a positive prediction; otherwise, negative. For increasing threshold values, true positive rate (TPR) and false positive rate (FPR) will be calculated to generate the receiver-operating characteristic (ROC) curve. Precision and recall (equivalent to TPR) are obtained to plot the precision–recall (PR) curve ( Davis  et al. , 2006 ). Meanwhile, due to the fact that the top-ranked results are of most interest, the number of correctly identified drug–disease associations using different thresholds will be illustrated. The area under the ROC curve (AUC) and top-ranked results are presented to compare the overall performance of BNNR with a variety of existing methods in this study. 3.2 Parameter setting In BNNR algorithm, there are two parameters needed to be determined, including α and β. For the parameters α and β, we perform cross-validation on the training dataset to determine, which are determined from {0.1, 1, 10, 100}.  Table 1  reports AUC values calculated by BNNR when α and β are ranging from {0.1, 1, 10, 100} in 10-fold cross-validation, where the best AUC values are displayed in bold. One can find that BNNR achieves the best performance when α = 1 and β = 10. Table 1. The AUC values using different α and β values in 10-fold cross-validation on the gold standard dataset α\β 0.1 1 10 100 0.1 0.757 0.785 0.879 0.888 1 0.863 0.921 
 0.933 
 0.899 10 0.854 0.921 0.926 0.890 100 0.862 0.919 0.925 0.889 Meanwhile, we terminate the BNNR algorithm when the following stopping criterions are satisfied:
 (11) f k ≤ tol 1 , | f k + 1 - f k | max { 1 , | f k | } ≤ tol 2 , 
where   f k = ‖ X k + 1 - X k ‖ F ‖ X k ‖ F ,  tol 1 and  tol 2 are the given tolerances, which are set as 2 × 10 − 3  and 10 − 5  in BNNR algorithm, respectively. 3.3 Compare with other methods BNNR is compared with four latest methods for drug repositioning: HGBI ( Wang  et al. , 2013 ), DrugNet ( Martinez  et al. , 2015 ), MBiRW ( Luo  et al. , 2016 ) and DRRS ( Luo  et al. , 2018 ). Based on the guilt-by-association principle and the interpretation of information flow, HGBI is designed for predicting disease-associated drugs. DrugNet is based on propagation flow algorithm, which can perform both drug–disease and disease–drug prioritization. MBiRW and DRRS are our previous works, MBiRW uses comprehensive similarity measures and BiRW algorithm to infer drug–disease association. DRRS constructs a heterogeneous drug–disease network and conducts prediction based on the matrix completion of SVT algorithm to predict potential indications for drugs. Although DRRS and BNNR are based on the same heterogeneous drug–disease network, BNNR can exploit more accuracy association information due to better robustness. BNNR has several distinct advantages compared with DRRS: First, BNNR could fit the whole network better. Since the values of similarity matrices computed  in silico  may include noisy information, BNNR has a relaxed penalty function to cope with noisy entries, while DRRS attempts to fit all entries. Second, BNNR has more interpretable predicted values. The bounded constraint ensures that all predicted associations are within [0, 1]. In contrast, the predicted association scores may be negative or &gt;1 in DRRS. Third, the regularization term based on nuclear norm is able to address overfitting effectively. This enables us to design an appropriate stop criterion for BNNR to directly obtain the optimal solution without the need of designating a part of known drug–disease associations as the validation set to identify the optimal rank. To ensure a fair comparison, the parameters in the compared approaches are set to the default values according to the authors’ recommendation (HGBI: α = 0.4; MBiRW: α = 0.3,  l     =    2,  r     =    2; DRRS: τ and δ are two adaptive parameters) and cross-validation (DrugNet: α is chosen from {0.1, 0.2, …, 0.9}). The overall results of 10-fold cross-validation for all methods are depicted by ROC curve, PR curve and top-ranked results in  Figure 2 . As shown in  Figure 2 , the BNNR method outperforms the other methods in terms of AUC values of the ROC curves, precisions and top-ranked indications. Specifically, BNNR reports AUC value of 0.932, while HGBI, DrugNet, MBiRW and DRRS have 0.829, 0.868, 0.917 and 0.930, respectively. The more significant gains are in precision. BNNR obtains prediction precision of 0.440, which is significantly higher than HGBI (0.130), DrugNet (0.192), MBiRW (0.304) and DRRS (0.375). It is important to note that BNNR can successfully rank 44.0% true drug–disease associations at top 1, which is 13.6 and 6.5% higher than MBiRW and DRRS, respectively. One true drug–disease association is treated as a retrieved association when its predicted rank is higher than the specified top rank threshold. These approaches identify different numbers of true drug–disease associations with respect to different rank cutoffs, which are presented in  Figure 2c . For instance, among the 1933 true drug–disease associations, 1333 associations are identified at top 5 by BNNR, while in comparison, only 561, 738, 1044 and 1251 associations are predicted by HGBI, DrugNet, MBiRW and DRRS, respectively. In practice, precision is a more important measure of the drug–disease association prediction performance, because a more precise prediction provides correct indication for existing drugs with higher probability, which can lead to budget and time reduction.
 Fig. 2. The performance of all methods in predicting drug–disease association for 10-fold cross-validation. ( a ) ROC curve of prediction results. ( b ) PR curve of predicting candidate diseases for drugs. ( c ) The number of correctly retrieved drug–disease associations for various rank thresholds 3.4 Predicting indications for new drugs To assess the capability of BNNR in predicting potential indications for new drugs, we choose these drugs which have only one known drug–disease association to conduct a  de novo  test. For each of these drugs, the known disease association is removed in turn as the test sample and other existing associations are used as training sample. For a new drug without any known drug–disease association, BNNR is able to predict its drug–disease associations by taking advantage of the similarity information of the novel drug in adjacency matrix. Also, due to the fact that there is no drug–disease association information for the novel drug, the similarity information is more important than the existing drug–disease association information for the other drugs, which should be given heavier weights. Equivalently, association matrix is multiplied by a weight coefficient 0.7 in this study. As shown in  Figure 3  for the  de novo  test, BNNR achieves AUC value of 0.830, while HGBI, DrugNet, MBiRW and DRRS have inferior results with 0.746, 0.782, 0.818 and 0.824, respectively. For top-ranked results, BNNR outperforms all methods at top 5, 10 and 50, except for being inferior to DRRS at top 1.
 Fig. 3. The performance of all methods in predicting potential diseases for new drugs. ( a ) PR curve of prediction results. ( b ) The number of retrieved drugs for various rank thresholds 3.5 Case studies In these case studies, we apply BNNR to predict new uses for already approved drugs in practical applications. In the process of identifying novel drug–disease associations, we treat all known drug–disease associations in the gold standard dataset as the training set and regard the missing drug–disease pairs as the candidate set. After the prediction scores of all candidate pairs are computed by BNNR, we rank the candidate diseases by the predicted scores for each drug. In order to confirm whether the predicted diseases are true or not, we choose Levodopa, Doxorubicin, Amantadine and Flecainide as the representative drugs to validate their potential diseases predicted by BNNR and then list the confirmed information of top-5 candidate diseases for them. We confirm the potential diseases associated with the given drug by authoritative public databases, such as DrugBank, CTD ( Davis  et al. , 2013 ) and KEGG ( Kanehisa  et al. , 2014 ). The predicted results and the supporting evidences are summarized in  Table 2 . For each representative drug, more than three new drug–disease associations on top-5 have been reported in the public databases. It demonstrates the effectiveness of BNNR in predicting novel indications for drugs in practical use. Table 2. The top five candidate diseases for Levodopa, Doxorubicin, Amantadine and Flecainide Drugs (DrugBank IDs) Top five candidate diseases (OMIM IDs) Evidences Levodopa (DB01235) Parkinson disease (168600) KEGG/DB/CTD Dementia (125320) DB/CTD Multiple sclerosis (126200) CTD Pheochromocytoma (171300) CTD Hyperplastic myelinopathy (147530) Doxorubicin (DB00997) Small cell cancer of the lung (182280) CTD Dohle bodies (223350) Testicular germ cell tumor (273300) CTD Reticulum cell sarcoma (267730) CTD Leukemia (109543) KEGG/DB/CTD Amantadine (DB00915) Parkinson disease (168600) KEGG/DB/CTD Dementia (125320) DB/CTD Restless legs syndrome (102300) Alzheimer disease (104300) CTD Malignant hyperthermia (217150) Flecainide (DB01195) Atrial fibrillation (608583) CTD Cardiac arrhythmia (115000) DB/CTD Diastolic hypertension (608622) CTD Nephropathy-hypertension (161900) Hyperplastic myelinopathy (147530) Furthermore, BNNR identifies other new indications including: Levodopa for hyperplastic myelinopathy; Doxorubicin for dohle bodies; Amantadine for restless legs syndrome and malignant hyperthermia; Flecainide for nephropathy-hypertension and hyperplastic myelinopathy. These predicted associations are not yet reported in current literature, but may have a greater likelihood of existing. There are great opportunities to research and validate these associations for medical researchers and pharmaceutical companies. 3.6 The effects of bounded constrain and regularization model of BNNR on performance In order to evaluate the effectiveness of bounded constraint [0, 1] and regularization model, we compare BNNR with two models in 10-fold cross-validation. The first model is BNNR without bounded constraint [0, 1] (referred to as NNR), while the other one is BNNR without regularization term (referred to as BNN). Specifically, NNR is defined as:
 (12) min X ‖ X ‖ * + α 2 ‖ P Ω ( X ) − P Ω ( M ) ‖ F 2 , 
and BNN is defined as:
 (13) min X ‖ X ‖ * s . t . P Ω ( X ) = P Ω ( M ) 0 ≤ X ≤ 1. One can find that incorporating the regularization term leads to more robust prediction results compared to simply minimizing the nuclear norm, where the noise in similarity measures is tolerated. Moreover, constraining the predicted association values within [0, 1] further improves the prediction accuracy. This is shown in the 10-fold cross-validation results illustrated in  Figure 4 .
 Fig. 4. Performance comparison of BNNR, NNR and BNN in 10-fold cross-validation in terms of AUC values To further verify the robustness of BNNR, we increasingly add random noises to the drug–drug and disease–disease similarity matrices. The noise entries are drawn independently from  N ( 0,1 / 20 )  and noise rate is the proportion of the contaminated entries with respect to all components of similarity matrix. We set the noise rate in [0, 0.3] with an increase step size of 0.06. BNNR and BNN are compared in 10-fold cross-validation in terms of AUC values. Without a surprise, as shown in  Figure 5 , the AUC values decrease gradually as the noise rate increases in both BNNR and BNN. However, the decrease of BNNR is much slower compared to BNN, indicating that BNNR is able to better tolerate noisy similarity computations. This also explains why BNNR leads to better prediction accuracy when the nuclear norm regularization term is incorporated.
 Fig. 5. Performance comparison of BNNR and BNN under different noise rates in terms of AUC values 3.7 Experiments on the other datasets In order to illustrate the adaptability of BNNR in different datasets, we perform BNNR on the two other datasets including Cdataset and DNdataset, which are used in our previous work ( Luo  et al. , 2016 ,  2018 ). Cdataset ( Luo  et al. , 2016 ) contains 663 drugs collected in DrugBank, 409 diseases obtained in OMIM database and 2352 known drug–disease associations. DNdataset ( Martinez  et al. , 2015 ) includes 1490 drugs registered in DrugBank, 4516 diseases annotated by Disease Ontology (DO) terms and 1008 known drug–disease associations. We evaluate the robustness of our method on these two datasets by performing 10-fold cross-validation and the  de novo  test. The parameters of BNNR for Cdataset and DNdataset are set as Section 3.2. (For Cdataset, α = 1 and β = 10. For DNdataset, α = 1 and β = 1.) For Cdataset, as shown in  Figure 6 , BNNR obtains AUC value of 0.948 in 10-fold cross-validation, while HGBI, DrugNet, MBiRW and DRRS have 0.858, 0.903, 0.933 and 0.947, respectively. The PR curves illustrate that BNNR obtains the best precision with 0.471, while HGBI, DrugNet, MBiRW and DRRS have 0.168, 0.239, 0.351 and 0.403, respectively. Meanwhile, BNNR outperforms the other methods on top rank results. More specifically, at top-5 rank, 1855 associations out of 2532 are identified by BNNR, while only 796, 1193, 1481 and 1753 associations are predicted by HGBI, DrugNet, MBiRW and DRRS, respectively. In the  de novo  test, PR curve and top rank results are illustrated in  Figure 7 . BNNR obtains AUC value of 0.812, while HGBI, DrugNet, MBiRW and DRRS have 0.732, 0.785, 0.804 and 0.819, respectively. DRRS achieves slightly better performance than BNNR. In addition, BNNR outperforms the other methods with respect to different top-ranked thresholds. Specifically, for 177 drug associations, BNNR retrieves 87(49.2%) drugs at top 10 rank, while HGBI, DrugNet, MBiRW and DRRS have 48(27.1%), 61(34.5%), 80(45.2%) and 78(44.0%), respectively.
 Fig. 6. The performance of all methods in predicting drug–disease associations for 10-fold cross-validation on Cdataset. ( a ) ROC curve of prediction results. ( b ) PR curve of predicting candidate diseases for drugs. ( c ) The number of correctly retrieved drug–disease associations for various rank threshold Fig. 7. The performance of all methods in predicting potential diseases for new drugs on Cdataset. ( a ) PR curve of prediction results. ( b ) The number of retrieved drugs for various rank thresholds For DNdataset, as shown in  Figure 8 , BNNR obtains AUC value of 0.955 in 10-fold cross-validation, while HGBI, DrugNet, MBiRW and DRRS have 0.921, 0.950, 0.956 and 0.934, respectively. The PR curves show that BNNR obtains the best precision with 0.347, while HGBI, DrugNet, MBiRW and DRRS have 0.204, 0.150, 0.321 and 0.346, respectively. It is a noteworthy fact that BNNR has better AUC value and precision compared to other methods. Meanwhile, BNNR outperforms the other methods on top rank results from four different thresholds. In  de novo  test, PR curve and top rank results of  de novo  test are illustrated in  Figure 9 . BNNR obtains AUC value of 0.956, which is slightly worse than DrugNet and MBiRW, while HGBI and DRRS have 0.928 and 0.946, respectively. BNNR surpasses the other methods on top rank results: for 347 test drug associations, BNNR retrieves 145 drugs at top 1 rank, while HGBI, DrugNet, MBiRW and DRRS have 111, 84, 136 and 134, respectively.
 Fig. 8. The performance of all methods in predicting drug–disease association for 10-fold cross-validation on DNdataset. ( a ) ROC curve of prediction results. ( b ) PR curve of predicting candidate diseases for drugs. ( c ) The number of correctly retrieved drug–disease associations for various rank threshold Fig. 9. The performance of all methods in predicting potential diseases for new drugs on DNdataset. ( a ) PR curve of prediction results. ( b ) The number of retrieved drugs for various rank thresholds 3.8 Computation time comparisons In order to compare the computational efficiency of different methods, we have conducted a 10-fold cross-validation on the gold standard dataset, Cdataset and DNdataset. The running times of these methods were obtained on a Linux server with CPU 2.30 GHz and 128 GB memory, which are shown in  Supplementary Table S1 . The average running time of BNNR is more than HGBI and DrugNet but less than MBiRW and DRRS on the gold standard dataset. Although HGBI is much faster than the others, it yields the lowest precision and AUC values. Moreover, compared to DrugNet on a bigger dataset such as DNdataset, BNNR is more computationally efficient. 4 Conclusions This study has developed a novel method named BNNR for drug repositioning. BNNR not only can restrict all predicted matrix entry values within a specific interval, but also exhibit robustness to tolerate potentially noisy similarity calculations. The results of cross-validation and  de novo  experiments have demonstrated that BNNR is an effective prediction approach. Especially, comparing with the existing drug repositioning methods, BNNR yields both the best AUC value and the best precision in most measures. Our case studies have confirmed the reliability of the identified new drug–disease associations. In the future, we plan to integrate drug–target information into the existing heterogeneous networks to further improve the prediction ability of BNNR. Funding This work was supported by the National Natural Science Foundation of China under Grant number (61732009, 61622213, 61772552 and 61420106009). 
 Conflict of Interest : none declared. Supplementary Material btz331_Supplementary_Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>RSEQtools: a modular framework to analyze RNA-Seq data using compact, anonymized data summaries</Title>
    <Doi>10.1093/bioinformatics/btq643</Doi>
    <Authors>Habegger Lukas, Sboner Andrea, Gianoulis Tara A., Rozowsky Joel, Agarwal Ashish, Snyder Michael, Gerstein Mark</Authors>
    <Abstract>Summary: The advent of next-generation sequencing for functional genomics has given rise to quantities of sequence information that are often so large that they are difficult to handle. Moreover, sequence reads from a specific individual can contain sufficient information to potentially identify and genetically characterize that person, raising privacy concerns. In order to address these issues, we have developed the Mapped Read Format (MRF), a compact data summary format for both short and long read alignments that enables the anonymization of confidential sequence information, while allowing one to still carry out many functional genomics studies. We have developed a suite of tools (RSEQtools) that use this format for the analysis of RNA-Seq experiments. These tools consist of a set of modules that perform common tasks such as calculating gene expression values, generating signal tracks of mapped reads and segmenting that signal into actively transcribed regions. Moreover, the tools can readily be used to build customizable RNA-Seq workflows. In addition to the anonymization afforded by MRF, this format also facilitates the decoupling of the alignment of reads from downstream analyses.</Abstract>
    <Body>1 INTRODUCTION The advent of next-generation sequencing technologies has revolutionized the study of genomes and transcriptomes. In particular, the application of deep sequencing approaches to transcriptome profiling (RNA-Seq) is increasingly becoming the method of choice for studying the transcriptional landscape of cells ( Hillier  et al. , 2009 ;  Mortazavi  et al. , 2008 ;  Wang  et al. , 2009 ). Typically, the first step in this analysis is the alignment of the sequence reads to a reference sequence set. Recently, a number of different alignment tools have been developed to map short reads in an efficient manner ( Trapnell and Salzberg, 2009 ). While much progress has been made on this front, there is still a great need for a set of software tools that facilitate the downstream analysis of mapped RNA-Seq reads. Further, two other issues remain to be addressed. First, the immense file size of next-generation sequencing data poses many challenges in terms of data processing, storage and sharing. Secondly, mechanisms to protect personal confidential genetic information need to be established. With the birth of personal genomics, sequencing data stems fundamentally from individuals, and this type of data cannot be distributed as easily because significant privacy concerns arise with sharing all the sequence variations of a particular individual ( Greenbaum  et al. , 2008 ;  Lowrance and Collins, 2007 ). One critical challenge for genomics, then, is to devise new data summaries that allow the sharing of large amounts of information from sequencing experiments without exposing the genotypic information of the underlying individual (Supplementary Material). Although many data formats have been developed such as SAM ( Li  et al. , 2009 ), there is no practical solution yet that addresses the privacy concerns when sharing large sequence alignment files. Addressing this challenge is precisely what we have endeavored to do in putting together the Mapped Read Format (MRF), a format that allows data summaries to be exchanged, enabling many aspects of the RNA-Seq calculation to be performed such as expression measurements, but that also detaches the actual sequence variation in a person into separate files. Further, it provides a very clear way of linking these two pieces of information so that the data summaries can be subsequently conjoined back to the original sequences for more in-depth analyses with potentially confidential data. Here, we present an overview of a flexible suite of tools (RSEQtools) that are designed to facilitate easily customizable workflows and efficient pipeline building for the analysis of RNA-Seq experiments using this compact format ( Fig. 1 ). Briefly, we first convert the aligned reads into MRF and thus decouple the alignment step from the downstream analyses. RSEQtools implements several modules using this standardized format for performing common RNA-Seq analyses, such as expression quantification, discovery of transcribed regions, coverage computations annotation manipulation, etc.
 Fig. 1. Schematic overview of RSEQtools. Mapped reads are first converted into MRF from common alignment tool output formats, including SAM. The resulting MRF files can be divided in two files: one with the alignment only and another with the corresponding sequence reads. The read identifiers provide a mapping between the two files. Then, several modules perform the downstream analyses independently from the mapping step, such as expression quantification, visualization of the mapped read and the calculation of annotation statistics, etc. Other tools have been developed based on this framework to perform more sophisticated analyses such as transcript assembly, isoform quantification (IQSeq,  http://rnaseq.gersteinlab.org/IQSeq ), fusion transcript identification (FusionSeq,  http://rnaseq.gersteinlab.org/fusionseq ), as well as aggregation and correlation of signal tracks (ACT,  http://act.gersteinlab.org ). 2 FEATURES AND METHODS 2.1 MRF and converters MRF only stores a minimal set of information, i.e. information that cannot be derived from the MRF data itself. This has the advantage of keeping the format succinct, while still capturing the relevant information for most analyses. MRF consists of three components: comment lines (optional) denoted by a leading ‘#’ sign, a header line and the mapped reads. The header line specifies the data type of each column: AlignmentBlocks, Sequences, QualityScores and QueryID. The column type AlignmentBlocks is required and represents the mapped reads. Each alignment block contains the coordinates with respect to the reference genome to which the read aligns as well as the read coordinates. A read spanning multiple regions, e.g. multiple exons, is denoted by multiple alignment blocks that are separated by a comma. Paired-end reads can be represented by using a set of alignment blocks for each end, which are separated by the ‘|’ symbol. By using this format, it is straightforward to specify both gapped and paired-end alignments. The RSEQtools package includes various utilities to convert the output of several mapping tools into MRF. A converter for the commonly used SAM format is included as well. The first example below represents two paired-end reads where one end is spliced, whereas the second example shows two un-spliced single-end reads with their associated QueryIDs: The optional types Sequences, QualityScores and QueryID provide additional information. In particular, the confidentiality issues can be addressed by generating two files: one including the alignments and a second one containing the sequences such as a FASTQ file. The former is useful for most analyses and can be publicly shared because it does not contain confidential information, whereas the latter can be subjected to a higher level of security and control. The two files can be conjoined, if necessary, by using the common QueryID as shown in  Figure 1 . 2.2 RNA-Seq analysis with RSEQtools The RSEQtools suite contains a set of modules to perform a large variety of tasks including the quantification of expression values, manipulation of gene annotation sets, visualization of the mapped reads, generation of signal tracks, the identification of transcriptional active regions and several auxiliary utilities (Supplementary Table S1). Genome annotation tools : to generate a splice junction library from any annotation set, we extract the genomic sequences of all the exons and synthetically create all splice junctions specified in the annotation set. This splice junction library can be used in combination with the reference sequences. A second tool is particularly useful when estimating expression levels. In order to capture the information of the various transcript isoforms, a ‘gene model’ is required. The module  mergeTranscripts  collapses the transcript isoforms into a single gene model by either taking the union or intersection of the exonic nucleotides. Quantification of gene expression:  one of the key features of RNA-Seq is the quantification of expression at different levels. Hence, a key module calculates the gene expression values for a given annotation set and a collection of mapped reads in MRF format. The annotation set specifies which ‘elements’ will be quantified. The program  mrfQuantifier  calculates RPKM (reads per kilobase per million mapped reads) values at the nucleotide level ( Mortazavi  et al. , 2008 ). Briefly, for a given entry in the annotation set (typically an exon or gene model), the number of nucleotides from all the reads that overlap with this annotation entry are added up and then this count is normalized by sequence length of the annotation entry (per killobase) and by the total number of mapped nucleotides (per million). This calculation is not performed at the transcript level, which requires a more sophisticated analysis ( Guttman  et al. , 2010 ;  Trapnell  et al. , 2010 ). Visualization of mapped reads:  the RSEQtools package also contains various tools for visualizing the results in genome browsers, by means of wiggle (WIG) and bedGraph files, which are commonly used to represent a signal track of mapped reads. Also, a GFF file can be generated from MRF files to visualize splice junction reads (example in  Fig. 1 ). Identification of transcriptionally active regions (TARs) : transcribed regions can be identified  de novo  by performing a maxGap/minRun segmentation ( Kampa  et al. , 2004 ;  Royce  et al. , 2005 ) from the signal files using the  wigSegmenter  program. Briefly, the signal is first thresholded to identify transcribed elements. Contiguous elements whose distance is less than ‘maxGap’ are joined together and then filtered if the final size is less than ‘minRun’. This type of analysis is particularly useful in discovering novel TARs such as small RNAs, etc. MRF selection and auxiliary utilities : lastly, RSEQtools includes a set of utilities to easily manipulate MRF files and a collection of format conversion tools allowing for rapid pipeline development. Implementation and run time : the modules of the RSEQtools suite were implemented in C and the code was optimized in order to efficiently handle large datasets. The importance of code scalability cannot be overemphasized in a time where datasets become increasingly large and easily exceed several gigabytes. For example, the conversion of an ELAND export file (uncompressed file size: ∼4 GB; total number of reads: ∼20 million; number of mapped reads: ∼12 million) to MRF takes ∼2 min and the resulting MRF file is significantly smaller (∼400 MB uncompressed, ∼130 MB compressed with gzip). Converting the same ELAND export file to SAM generates a file of ∼3.1 GB (uncompressed) and the corresponding BAM file has a size of ∼1.2 GB. The subsequent quantification of gene expression using  mrfQuantifier  requires 45 s to calculate estimates for about 20 000 genes. In addition, the modularity of RSEQtools also enables the development of additional programs in any programming language and their seamless integration into this framework. Finally, most modules use STDIN and STDOUT to process the data, making them suitable to be integrated into an automated pipeline. 3 CONCLUSIONS In summary, RSEQtools contains a number of useful and highly specific modules that can rapidly analyze RNA-Seq data. The MRF format has two major features: it allows the decoupling of downstream analysis from the mapping strategy and addresses the issue of confidentiality that is intrinsic in any sequencing experiments involving human subjects. By separating the actual sequencing reads from the alignments, MRF provides a mechanism to protect the private genotypic information of the underlying individual. Although this approach removes the most obvious genotypic features, other distinctive attributes do remain. First of all, the information in a MRF file is at least equivalent to that in traditional expression array, which can potentially identify the underlying individual. Secondly, some information about structural variants may be contained in the MRF file of an RNA-Seq experiment. However, it is not obvious how to extract genotypic information from a subset of structural variations just affecting genes. In addition, inferring structural variations from RNA-Seq data as opposed to DNA sequencing would be more complicated due to the presence of alternative splicing. Another advantage of storing the alignments without the underlying sequences is that it saves space, especially as reads become longer. Moreover, a possible future extension is the development of a specific compression schema that could further reduce the size of the files. In addition, this data format could be easily applied to sequence alignments obtained from other high-throughput functional genomic assays such as ChIP-Seq or chromosome conformation capture (3C). 
          Supplementary Material 
          
             
                Supplementary Data 
             
             
             
          
       </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>RADIS: analysis of RAD-seq data for interspecific phylogeny</Title>
    <Doi>10.1093/bioinformatics/btw352</Doi>
    <Authors>Cruaud Astrid, Gautier Mathieu, Rossi Jean-Pierre, Rasplus Jean-Yves, Gouzy Jérôme</Authors>
    <Abstract>In an attempt to make the processing of RAD-seq data easier and allow rapid and automated exploration of parameters/data for phylogenetic inference, we introduce the perl pipeline RADIS. Users of RADIS can let their raw Illumina data be processed up to phylogenetic tree inference, or stop (and restart) the process at some point. Different values for key parameters can be explored in a single analysis (e.g. loci building, sample/loci selection), making possible a thorough exploration of data. RADIS relies on Stacks for demultiplexing of data, removing PCR duplicates and building individual and catalog loci. Scripts have been specifically written for trimming of reads and loci/sample selection. Finally, RAxML is used for phylogenetic inferences, though other software may be utilized.</Abstract>
    <Body>1 Introduction Restriction site Associated DNA sequencing (RAD-seq,  Baird  et al. , 2008 ;  Miller  et al. , 2007 ) is a promising tool to confidently resolve phylogenetic relationships among eukaryote species and genera (e.g.  Cruaud  et al. , 2014 ;  Eaton and Ree, 2013 ;  Hipp  et al. , 2014 ;  McCluskey and Postlethwait, 2015 ). However, analyzing RAD-seq data to infer phylogenies remains challenging as it requires many steps and decisions to process raw data into a format ready for analysis. Some steps can be achieved using a collection of well-packaged software, but others require bioinformatic skills. An examination of data is required to better analyze their quality and impact on topology/branch lengths. Assessment of the robustness of the resulting trees to the parameters chosen for loci building and loci/sample selection is required ( Leaché  et al. , 2015 ) but represents a tedious and error-prone task when done manually. In an attempt to standardize processing of RAD-seq data for phylogenetic inference, allow fast and automated exploration of key options, and facilitate comparison among clustering methods to form sets of loci (e.g. Stacks  Catchen  et al. , 2011 ;  Catchen  et al. , 2013 
 versus PyRAD ,  Eaton 2014 ), we designed the user-friendly perl pipeline  RADIS. 2 Description Processing of raw data has been split up into two steps: data cleaning and data analysis ( Supplementary Fig. S1 ). Example datasets are provided with the package (processing time &lt;1 min on 8-cores of a 16-core Linux, 2.9 GHz, 64 GB RAM computer). Users can choose to (i) process their raw Illumina data up to phylogenetic tree inference ( RADIS.pl ); (ii) perform only data cleaning ( RADIS_step1_data_cleaning.pl ) or (iii) perform only data analysis ( RADIS_step2_data_analysis.pl ). RADIS  operates on the basis of a configuration file (RADIS.cfg) in which users provide parameters values to be tested and paths to external software. The software comes with a fully annotated configuration file to facilitate the initialization. A correspondence between barcodes and sample codes can be provided to allow file renaming (barcodes_lib_names.txt). Progress of the analysis can be followed (stdout/stderr files). Output files and necessary subdirectories are automatically created in a directory specified by the user. Data cleaning — Reads that do not pass Illumina’s filtering are discarded.  RADIS  relies on  process_radtags  from the software pipeline  Stacks  to demultiplex data. Users can choose to remove nucleotides from the 5′ and 3′ ends of forward and reverse reads (e.g. to remove enzyme cut sites or bad quality nucleotides). If barcodes of different sizes are used, reads are automatically trimmed to the same length. To remove PCR duplicates,  RADIS  then uses  clone_filter  ( Stacks ). Finally, sequence files are renamed after the sample codes. At each step of the process, files are created that provide summary statistics on the number of reads removed/kept Data analysis —‘Purified read 1’ outputs from the first step are processed individually and a set of loci is produced for each sample using  ustacks  ( Stacks ). Users can provide a list of values to be tested for M, the maximum number of nucleotides that may be different between stacks (assembly of exactly matching reads) to be merged into a single locus ( http://creskolab.uoregon.edu/stacks/param_tut.php ). Individual loci are then merged into a catalog of loci with  cstacks  ( Stacks ). Users can provide a list of values for the parameter  n , the number of mismatches allowed between individual loci when generating the catalog. Exploring alternate parameterization allows the user to find a good compromise for merging orthologous loci from distant species, whilst ensuring that paralogs and non-homologous loci are not merged. Users can then perform loci and sample selection to build datasets that fit with their prior knowledge of the studied species. They can fix a minimum number of loci required for a sample to be kept in the analysis, or retain only loci for which at least a given number of samples have sequences (a list of values can be provided). Users can also choose to remove loci in which paralogs and non-homologous sequences are probably merged together. Phylip-formatted files that meet the selection criteria are produced by  RADIS . Finally, combined datasets (concatenation of the full sequence of each locus) are analyzed using  RAxML  ( Stamatakis, 2006a , b ) to produce phylogenetic trees. Users can delay implementation of  RAxML  analyses in order to increase the number of cpus to be used. Explicit names are used for output directories and files making the results obtained with different sets of parameters easily distinguished and compared (e.g. stacks_M2n4S12L10000.sel.phy is the phylip-formatted combined dataset obtained when individual loci are built using  M  = 2 (M2,  ustacks ), the catalog of loci is built using  n  = 4 (n4,  cstacks ), only sample with at least 10 000 loci (L10000) and loci for which at least 12 samples have sequences (S12) are selected (.sel). It is noteworthy that  RADIS  can process data from as many RAD libraries as needed. 3 Evaluation using empirical data To test the program, we reanalyzed the raw data from  Cruaud  et al.  (2014) . Experimental design was as follows: DNA from 31 samples was first digested with  PstI  and P1 adaptors containing 5 or 6 bp barcodes were then ligated. Paired-end sequencing of the library (2 * 100 nt) was performed on a single lane of a HiSeq 2000. Raw Illumina data were processed with  RADIS.pl  using  Stacks-1.32  and  RAxML 8.2.0-ALPHA  on 8-cores of a 16-core Linux, 2.9 GHz, 64 GB RAM computer. In the R ADIS.cfg file, radis_nttrim_read1_5p was set to 5 (to remove the overhang of the restriction site), radis_nttrim_read2_3p was set to 5 (to remove low quality bases) while radis_nttrim_read1_3p and radis_nttrim_read2_5p were set to 0.  M  was set to 2 and 4 values of  n  were tested (4, 6, 8, 10). We only retained samples with more than 10 000 loci, and loci for which at least 12 or all samples were represented. Phylogenetic analyses were performed without partitioning. A GTR + Γ model with a discrete gamma approximation (4 categories,  Yang, 1994 ) was used for the ML analysis and a GTRCAT approximation of models was used for bootstrapping (1000 replicates). Results (identical to published ones) were obtained within 4 h ( Supplementary Fig. S2 ). 4 Conclusion By facilitating testing the impact of different parameter combinations, the pipeline  RADIS  automates and standardizes the analyses of RAD-seq data for phylogenetic inference. The program may prove useful to evaluate the robustness of the results to the options chosen to process real RAD-seq data, or to carry out simulation studies. Most importantly,  RADIS  could also help to assess how different clustering methods may impact tree topology (e.g. Stacks versus  PyRAD ). 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Adjusting scoring matrices to correct overextended alignments</Title>
    <Doi>10.1093/bioinformatics/btt517</Doi>
    <Authors>Mills Lauren J., Pearson William R.</Authors>
    <Abstract>Motivation: Sequence similarity searches performed with BLAST, SSEARCH and FASTA achieve high sensitivity by using scoring matrices (e.g. BLOSUM62) that target low identity (&lt;33%) alignments. Although such scoring matrices can effectively identify distant homologs, they can also produce local alignments that extend beyond the homologous regions.</Abstract>
    <Body>1 INTRODUCTION Sequence similarity search algorithms are used to identify evolutionary homologs and to generate hypotheses for the function of unknown proteins. These algorithms assign homology between sequences achieving statistically significant similarity scores with high fidelity, even between highly divergent sequences sharing low similarity ( Brenner  et al. , 1998 ;  Pearson, 1995 ;  Pearson and Sierk, 2005 ). However, the same methodology that provides for the sensitive identification of homology at low identity can also lead to alignments that include non-homologous sequence adjacent to, or between, higher identity homologous sequences ( Gonzalez and Pearson, 2010a ). Homologous overextension was first identified as a source of error during iterative similarity searches ( Gonzalez and Pearson, 2010a ). Overextension occurs when alignments extend past the boundaries of the homologous region in the library, query or both sequences, leading to the inclusion of non-homologous sequence in an alignment ( Fig. 1 ). The inclusion of non-homologous sequence has been identified in alignments between highly identical DNA sequences ( Chao  et al. , 1993 ) and has been termed the ‘mosaic effect’ ( Arslan  et al. , 2001 ).
 Fig. 1. Homologous overextension. BLASTP with BLOSUM62 was used to create an alignment between a RPD2 query and a homologous sequence from the RPD2 library. The raw BLASTP output and a schematic of the sequences are shown. Homologous domains in the query (top) and subject (bottom) sequence are represented by black boxes. Light gray boxes in the library sequence indicate other domains. The embedded domain in this query is from B0TE74_HELMI and the sequence in the query outside of the embedded domain is shuffled. Black numbers show the homologous domain boundaries in both the schematic and raw BLASTP output (in boxes); gray numbers indicate the boundaries of the neighboring domain. The boundaries of the alignment are given by the open box, whereas the correct alignment is represented by the dark gray box in the schematic Overextension occurs because local sequence alignment boundaries depend on the scoring matrix. The popular BLASTP ( Altschul  et al. , 1997 ) tool, along with other sequence alignment tools (e.g. SSEARCH and FASTA;  Pearson, 2000 ), creates local alignments between similar sequences using scoring matrices. Scoring matrices assign a similarity score to each pair of aligned amino acids based on the probability that the amino acid transition has occurred more often through evolution than by chance. Amino acid replacements that are common through evolution are assigned high similarity scores, whereas rare replacements are assigned negative scores. Scoring matrices have an implicit evolutionary model, which allows different matrices to target different evolutionary distances ( Altschul, 1991 ;  Dayhoff  et al. , 1978 ;  Henikoff and Henikoff, 1992 ;  Muller  et al. , 2002 ). Scoring matrices that target long evolutionary times (deep scoring matrices) allow more amino acid substitutions and gaps, whereas shallower matrices favor higher sequence identity and have higher gap penalties. The scoring matrix dictates the local alignment boundaries; increasing or decreasing the length of the optimal local alignment reduces the total alignment score. Likewise, changing the scoring matrix can result in a different alignment. Ideally, a local alignment of homologous domains in different sequence contexts will align every residue in the homologous region, and no residues outside the domain boundaries, so that the alignment boundaries reflect the domain boundaries. Overextended alignments include additional sequence from outside the homologous domain boundaries. For example, in  Figure 1  artificial, randomly shuffled, sequence from the query appears to be homologous to a real protein. In this article, we show that scoring matrices have preferred alignment identities and alignment lengths, and that BLOSUM62 can produce overextended alignments, most often between sequences with &gt;33% identity. We also show that using the correct scoring matrix can produce more accurate alignment boundaries. Finally, we show that we can produce more accurate alignment boundaries, even without true domain boundary knowledge, by using the initial BLOSUM62 alignment identity to specify a more appropriate scoring matrix. 2 METHODS 2.1 Construction of the RefProtDom 2 (RPD2) dataset 2.1.1 Selecting families for RPD2 For this study, we built an updated version of the RPD protein database ( Gonzalez and Pearson, 2010b ) initially used to characterize alignment overextension with PSI-BLAST, using protein domains and sequences annotated in Pfam version 26 ( Punta  et al. , 2012 ). From 13 672 initial Pfam version 26 families, 136 families were selected that met the following criteria: (i) model length (&gt;200 residues); (ii) available structure; (iii) family size (&gt;100 members); and (iv) taxonomic diversity (presence in two of three kingdoms of life with the second most abundant kingdom having at least 15% as many the members as the most abundant). Although most Pfam domain families can be represented by a single hidden Markov model (HMM), some diverse families require multiple HMMs. When this occurs, the related domain families are grouped into Pfam clans. Protein domains belonging to the same Pfam family or Pfam clan are homologous to each other. Only a single family from any one clan was included and then only if the family model lengths of the HMMs in the clan differed by &lt;2-fold. Of the 136 families selected, 56 were members of clans. Four RNA polymerase families were excluded because they have a complex and inconsistent domain organization. 2.1.2 Selecting sequences for the RPD2 library For each of the RPD2 families, up to 5000 non-viral full-length (&gt;80% of Pfam model length) domains were randomly selected. The unique protein sequences from which the domains came were then identified and included in the RPD2 library. Low complexity regions were lowercase masked by pseg and stored in FASTA format. Because many of these sequences contained domains other than the identifying domain, the final RPD2 library contains 1837 families ranging in membership from 7063 examples of the domain to 1. In total, the RPD2 library contains 499 058 domains from 282 742 different protein sequences. 2.1.3 Creating query sets for RPD2 For each RPD2 family, 10 non-viral full-length examples of the domain were randomly selected. These domain sequences were used as queries against the RPD2 library. Searches were performed with SSEARCH version 36.3.6. The example of the domain that was able to find the largest number of the RPD2 library domains with an  E ()-score   was selected to be that family’s query sequence. Each selected domain was embedded in the center of shuffled sequence with the same length and amino acid composition as the original domain. 2.2 Database searches and scoring matrices Searches were performed using BLASTP version 2.2.27+ ( Camacho  et al. , 2009 ) or SSEARCH version 36.3.6 ( Pearson, 2000 ). A SSEARCH comparison of 136 query sequences against the 282 742 sequence RPD2 library took about 2 min on a 48 core machine. Bit scores, sequence identity, expectation values and alignments were calculated by the search algorithm. All alignments had an  E ()-score   with a domain originally annotated by Pfam. Two types of scoring matrices were evaluated: the BLOSUM62 routinely used with BLASTP and the VTML matrices (VT) described by  Muller  et al.  (2002) . For the VT, the gap penalties described by  Reese and Pearson (2002)  were adjusted to produce a smooth mean identity transition. The gap penalties used for each matrix are shown in  Table 1 .
 Table 1. Scoring matrices, gap penalties and mean identity, entropy and alignment length Matrix Open Extend Identity a Entropy a Length a BLOSUM50 −10 −2 26% 0.24 178 BLOSUM62 −11 −1 30% 0.45 95 VT160 −12 −2 25% 0.28 155 VT140 −10 −1 31% 0.51 88 VT120 −11 −1 34% 0.63 67 VT100 −10 −1 40% 0.80 54 VT80 −11 −1 41% 0.82 54 VT40 −12 −1 65% 2.0 20 VT20 −15 −2 85% 3.3 11 VT10 −16 −2 93% 3.8 10 a Means measured from 136 random sequence searches ( Fig. 3 ). 2.3 Boundary accuracy Boundaries for each alignment were known because the query domain was embedded in shuffled sequence. Alignments that extend outside of the embedded domains into the shuffled sequence are overextended. Alignments that fail to extend to the domain boundaries are incomplete. Alignment boundaries within ±10 residues of the embedded domain boundary are considered correct. The beginning and end of the alignments were evaluated independently, and the difference between the alignment boundaries and the embedded domain boundaries was calculated in number of residues. Incomplete alignments had negative boundary errors and overextended alignments had positive boundary errors. 2.4 Sub-alignment scoring SSEARCH from FASTA version 36.3.6 can provide location, identity and score values for non-overlapping subsections of any alignment. In this study, we annotated the embedded domain and non-domain regions in each query, which provided the score and identity for the homologous correct alignment, even if the alignment was overextended. For overextended alignments, the identity and score of the shuffled sequence that was included in the alignment was also calculated. 2.5 Scoring matrix adjustment Alignments with greater than 36% identity were realigned using a series of VTML matrices. The new matrix was selected based on the BLOSUM62 identity given in  Table 2 . 3 RESULTS 3.1 Homologous overextension Deep scoring matrices can produce inaccurate alignment boundaries.  Figure 1  shows an example of an overextended alignment created by BLASTP. The query was constructed using an E1-E2 ATPase (PF00122) domain from B0TE74_HELMI surrounded by shuffled sequence (dashed lines). This domain is homologous to the E1-E2 ATPase domain, also labeled PF00122, in the library sequence. The PF00122 domain extends from position 113 to 335 in the query. Any alignment that includes sequence from the query outside of the embedded domain includes shuffled sequence that is not homologous to the library sequence. In this example, the alignment extends from position 84 to 415 in the query, incorporating 109 residues of shuffled sequence or 33% of the total alignment length. The library sequence, like many proteins, consists of multiple domains. The alignment between these two sequences falsely indicates that shuffled sequence in the query is homologous to a neighboring Hydrolase (PF00702) domain in the library. BLASTP reports that the aligned sequences are 50% identical, but the homologous region is 64.1% identical, whereas the non-homologous flanking regions are 23% identical. The homologous region contributes 83% of the bit score (248.2 bits) and the non-homologous region only contributes 17%. This imbalance in the contributions of homologous compared with non-homologous regions to both alignment identity and score is a hallmark of overextended alignments. 3.2 Overextension occurs more frequently in alignments with higher sequence identity To understand how often incorrect alignment boundaries occur, searches were performed with both BLASTP and SSEARCH, using BLOSUM62 (BL62) with the RPD2 query set and library. Each alignment boundary was measured and the results were divided into seven bins ranging from extremely incomplete (&lt;−40 residues, i.e. &gt;40 residues missing) to extremely overextended (&gt;40 residues added;  Fig. 2 A). Although most of the alignment boundaries were within 10 residues of the embedded domain boundaries (71% BLASTP, 75% SSEARCH), BLASTP and SSEARCH also created incorrect alignment boundaries. Of the boundaries measured, 22% of BLASTP boundaries were incomplete and 7% were overextended, aligning random sequence with real protein residues. Seventeen percent of the SSEARCH boundaries were incomplete and 8% were overextended. Alignment identity was divided into quartiles. Each identity quartile shows similar representation within the group of ‘correct’ alignment boundaries (within ±10 residues of the embedded domain). In contrast, incomplete alignment boundaries are more common in low identity alignments, whereas overextended alignment boundaries are more common in high identity alignments. Most incomplete alignment boundaries (73% for BLASTP, 76% for SSEARCH) were from alignments in the lowest two identity quartiles. The opposite is true for overextended alignments, where most had identities in the top two quartiles (52% for BLASTP, 54% for SSEARCH). When incorrect alignments are examined independently, the percentage of the boundaries that is overextended increases with identity ( Fig. 2 B).
 Fig. 2. Boundary accuracy and sequence identity. Using the RPD2 embedded domain queries and sequence library, pairwise protein sequence alignments were calculated with BLASTP (B) and SSEARCH (S) using BLOSUM62. Boundary accuracy was measured for both the beginning and end of alignments between known homologs with  E ()-score   as detailed in Methods. Alignment inaccuracy of less than −10 residues indicates an incomplete alignment; &gt;10 residues is considered overextension. In panel ( A ), alignment identities were divided into quartiles. The data from the searches were binned by boundary accuracy (top) and sequence identity (color). In panel ( B ), incorrect alignment boundaries were isolated and alignments were divided into six identity bins. The boundary accuracy is given by the color of the bar. Identity bins are inclusive at the maximum Figure 2  reports incomplete and overextended alignment boundaries for the 397 123 homologs that were identified by BLASTP and SSEARCH. Because RPD2 was built from diverse domain families, most of these homologs are distant, with a median identity of 33%. In practice, one rarely examines every significant match, so we also counted incomplete and overextended boundaries for the top 100 significant hits with each query. For the top 100 hits, the median alignment identity increases to 52%. In this more closely related set, the percentage of overextended alignments increases to 8% for BLASTP and 10% for SSEARCH and incomplete alignment decreases to 8% and 5%, respectively.  Incomplete alignments can occur when homologous domains are evolutionarily distant, so that the alignment captures only the most conserved regions of the homology. This contrasts with traditional false negatives, where the homology is missed altogether. In the traditional case, the reduced sensitivity of pairwise sequence comparisons compared with model-based (PSI-BLAST, PSI-SEARCH, HMMER) or structure-based methods is well recognized ( Pearson and Sierk, 2005 ). Incomplete alignments are another example of inadequate alignment sensitivity. Overextension, while recognized in pairwise genomic alignments ( Chao  et al. , 1993 ), had not been systematically measured in pairwise protein alignments. Missed homologs can be identified using transitive homology, protein family models or structures. But strategies for removing non-homologous sequence from pairwise protein alignments have not been described. 3.3 Scoring matrices, identity and alignment length Alignment overextension often results from a mismatch between the evolutionary distance between the homologous sequences and the target identity of the scoring matrix used in the alignment. Unlike global sequence alignments, which use the full length of each sequence, the scoring matrix determines local alignment boundaries. To understand how different scoring matrices produce different alignment boundaries, we used shuffled sequences as queries against the RPD2 library. ‘Deeper’ scoring matrices (scoring matrices targeted to more evolutionary change) produce longer less identical alignments by chance, whereas ‘shallower’ scoring matrices produce shorter higher identity alignments ( Fig. 3 ). Here, the same 136 shuffled queries were used with each matrix, so the resulting trends in identity and alignment length reflect the average properties of the matrices themselves. The target identities with gaps are lower, and the alignment lengths longer, than the values estimated from the scoring matrix alone. Remarkably, the entropies calculated analytically from the scoring matrix alone track closely between the gapped and ungapped empirical mean entropies. Including gaps (black boxes) makes scoring matrices ‘deeper’, thus lowering identity and increasing alignment length compared with the same matrix without gaps (gray circles). Different scoring matrices can produce different alignment boundaries.
 Fig. 3. Scoring matrix target identity, entropy and alignment length. Queries were constructed from 136 shuffled protein domains. SSEARCH was used to search against the RPD2 library with these shuffled queries using either the gap penalties given in  Table 1  (black squares) or gap penalties of −1000/−1000 for open/extend (gray circles), which effectively creates alignments with no gaps. The identity and alignment length from the highest scoring alignment was selected from each query. The ( A ) mean identity, ( B ) mean entropy and ( C ) mean alignment length is given by the point, and the standard deviation is indicated by the error bars for each scoring matrix. The analytical entropy calculated from the scoring matrix is shown as open triangles in panel (B) 3.4 Selecting the correct scoring matrix gives correct domain boundaries To illustrate how ‘correct’ scoring matrices—scoring matrices with target identities that match the evolutionary distance of the homologous domains—improve accuracy, we examined alignment boundary changes with different scoring matrices. Beginning with 16 640 overextended alignments, we tracked the boundary accuracy produced by six VT with increasing target identity ( Fig. 4 ). The alignment with the smallest cumulative difference between the embedded domain boundaries and the alignment boundaries was identified, and 10 realignments from each of the VT scoring matrices were randomly selected. The maximum boundary errors for both the initial BLOSUM62 and final best alignment are shown in  Figure 4 . All of the realignments corrected the overextended boundary to within ±10 residues of the embedded domain, producing alignments with higher identities. As the identity of the initial alignment increases, the target identity of the matrix that produces the corrected alignment also increases. However, the matrix required did not correlate with the amount of overextension in the original BLOSUM62 alignment in this dataset. Nor was there any correlation in alignments that used alternate shuffling strategies for the embedded domains.
 Fig. 4. Selecting the scoring matrix that creates the best alignment. ( A ) Sequence pairs with &gt;33% identity and overextended alignment boundaries were selected from the results of the similarity search performed using SSEARCH with BLOSUM62. Each sequence pair was realigned using VT120, 100, 80, 40, 20 and 10 ( Table 1 ). Boundary accuracy was calculated for each alignment and the alignment with the smallest cumulative difference between the embedded domain boundaries and the alignment boundaries was selected. Symbol shape and color (black, open) indicate the scoring matrix used for the alignment; lines connect alignments between the same sequence pairs. (B) Maximum boundary inaccuracy across every scoring matrix for two sequence pairs in (A) is shown. The rounded dashed line to the left in panel (A) and the higher line between VT120 and VT80 in panel ( B ) show a low identity alignment corrected by VT40; the square dash-dot line to the right in panel (A) and flat between VT120 and VT10 in panel (B) shows a high identity alignment corrected by VT120 In general, lower target identity matrices (VT120, VT100, VT80) correct lower identity alignments (the filled symbols tend to be on the left of the final distribution) and higher target identity matrices (VT40, VT20, VT10) correct higher identity alignments (the open symbols tend to be on the right). But this is not always the case; sometimes a high identity alignment is corrected by a distant matrix (dash-dot line) and vice versa (rounded-dash line). Anomalous matrices can correct overextension because alignment boundary correction is robust to matrix selection.  Figure 4 B shows two extreme examples, a deep matrix (VT120) correcting a high identity alignment (dash-dot line) and a shallow matrix (VT40) correcting a low identity alignment (rounded-dash line). In both cases, a wide range of scoring matrices correct the alignment, including a matrix at the predicted target identity (for the red low identity alignment, VT120, VT100 and VT80 produce an alignment that is off by two residues, whereas VT40 is perfect). The robustness of boundary correction to scoring matrix choice allows us to approximate the ‘correct’ alignment identity from the initial (possibly overextended) BLOSUM62 identity. Because high identity alignments tend to be corrected by shallow scoring matrices while lower identity alignments can be corrected by less shallow scoring matrices ( Fig. 4 ), we attempted to correct BLOSUM62 alignments using the scoring matrices and thresholds shown in  Table 2 .
 Table 2. Identity required to realign using each scoring matrix Matrix Identity range VT120 36–50% VT100 50–60% VT80 60–70% VT40 70–80% VT20 80–85% VT10 &gt;85% Note : Values are inclusive at the maximum for each matrix. Forty-seven percent of overextended boundaries came from alignments with &gt;36% identity and, therefore, were candidates for the realignment algorithm. Of the overextended boundaries that could be realigned, 97% had reduced overextension with 86% of the overextended boundaries moving within ±10 residues of the embedded domain boundaries. Overall, including overextended alignments that were not realigned, the total amount of overextension was reduced from 8 to 5%. Although the scoring matrix identity thresholds in  Table 2  dramatically decrease overextension errors, they can also produce incomplete alignments ( Fig. 5 ). In contrast to  Figure 4 , where we selected the most accurate alignment,  Figure 5  shows the results of realignment based solely on the identity of the initial BLOSUM62 alignment (the thresholds in  Table 2 ). Looking at all alignments with &gt;36% identity, 16 411 alignment boundaries changed accuracy bins. Of the alignment boundaries that changed accuracy bins, 68% moved from being overextended (&gt;10 residues, blue colors) to within ±10 residues, whereas 20% moved from being within ±10 residues or overextended to incomplete. Most (73%) of the realigned incomplete alignment boundaries fall into the   bin (orange). The most overextended alignments (&gt;40 residues,  Fig. 2 ) decreased by 2217 alignment boundaries, whereas the most incomplete alignments increased by 399 boundaries. The final distribution of all alignment boundaries had 7863 more boundaries within 10 residues of the embedded domain boundary and 3189 additional incomplete boundaries, or 2.5 additional boundaries within ±10 residues for each additional incomplete boundary.
 Fig. 5. Realignment algorithm results only results from sequence pairs that were realigned by the algorithm are shown. Gray dashed bars indicate the initial boundary accuracy before realignment; colored bars indicate the final distribution of alignment boundary errors. The colors in the final distribution bars show the original accuracy before realignment Alignment boundary correction is much more effective when applied to the highest scoring alignments. Focusing on the top 100 alignments from each query, 83% of the overextended boundaries were from alignments with &gt;36% identity of which 90% moved within 10 residues of the embedded domain boundaries reducing the amount of overextension from 10 to 3%. The top 100 alignments produced many fewer incomplete alignments; 1645 boundaries moved to within 10 residues of the embedded domain, whereas only 233 boundaries became worse than less than −10 residues incomplete, a ratio of 7 corrected boundaries for each additional incomplete boundary. 4 DISCUSSION Mismatches between the sequence identity of aligned homologous domains and the target identity of the scoring matrix used to produce the local sequence alignment can lead to overextended alignments ( Figs. 1  and  2 ). Similarity scoring matrices have preferred alignment lengths and identity. Deep scoring matrices create longer alignments and have lower target identity compared with shallower matrices ( Fig. 3 ). Alignments created by BLOSUM62, most often between sequences with higher identity (&gt;33%), can extend past the boundaries of the homologous domain to include non-homologous sequence ( Fig. 2 ). Using a shallower scoring matrix that targets the correct sequence identity can correct overextension ( Fig. 4 ). Predicting the scoring matrix that will lead to a better alignment, using initial (possibly overextended) identity given by BLOSUM62, can correct overextended alignments. In our RPD2 database, 37% of overextended alignments were corrected to within   residues, or 86% of the alignments with high enough identity (&gt;36%) to be considered for realignment. However, realignment has a cost; a fraction of correctly aligned domains are incompletely realigned. The observation that ‘deep’ scoring matrices produce overextended alignments between domains that are less evolutionarily distant (have higher identity) than the scoring matrix target identity is not surprising, though the relationship between alignment boundaries (in contrast to internal alignment accuracy) and scoring matrices has not been extensively studied. Traditional internal alignment accuracy decreases as evolutionary distance increases; different sequences are difficult to align accurately. In contrast, alignment overextension occurs most often when closely related sequences are aligned, and thus becomes more frequent as sequence databases grow. As log-odds matrices, every scoring matrix has a target evolutionary distance, or percent identity, which can be approximated from the homologous replacement frequencies that are the numerator of the log-odds ratio ( Altschul, 1991 ). As evolutionary distance and the number of replacements increase, the replacement frequencies for identities decrease and the non-identical replacement frequencies increase, which reduces the target identity of the matrix when aligning random sequences ( Fig. 3 ). Overextension occurs when a scoring matrix that models a longer period of evolution (a deeper scoring matrix) by allowing more mutations is used to align sequences with less evolutionary change. A deep scoring matrix produces a less identical alignment because it accepts more amino acid replacements. Gap penalties also modify alignment length and identity; increased gap penalties produce shorter higher identity alignments, whereas lower gap penalties produce longer lower identity alignments ( Fig. 3 ). Lower mismatch penalties and lower gap penalties in deep matrices allow the local alignment algorithm to add additional identities that are occurring by chance from non-homologous sequence for the sake of modest increases in score. Thus, in  Figure 1 , 83% of the score was produced by 67% of the alignment. Overextended alignments are locally optimal, but they are not biologically correct. RPD2 was designed to simulate the most common similarity search—searches against full-length proteins in a comprehensive sequence database. RPD2 sequences were selected from the set of sequences annotated by Pfam release 26, which samples both SwissProt and Trembl protein sequences. The RPD2 library is large (528 742 sequences) and diverse. Queries were engineered from long (&gt;200 residues) protein domains, allowing BLOSUM62 searches to identify distant homologs. These domains are surrounded by shuffled protein sequence, providing known alignment boundaries. Alignments that extend into the flanking random sequence are thus guaranteed to be non-homologous. Our initial searches with 136 independent embedded domain queries produced both incomplete alignments (22% BLASTP, 17% SSEARCH, both with BLOSUM62) and overextended alignments (7% BLASTP, 8% SSEARCH). Incomplete alignments reflect the reduced sensitivity of pairwise alignment compared with the HMM based methods used to annotate the Pfam domains in RPD2, and the fact that in the diverse set of homologous RPD2 domains, half of the detectable homologs share &lt;33% sequence identity. In characterizing &gt;2 × 200 000 alignment boundaries in the 136 query domain searches, we consider far more distant alignments than would typically be examined during the genome annotation process, where sequences sharing at least 40% identity might be used to transfer annotation. Restricting the analysis to the top 100 significant hits for each query increases the median alignment identity to 53%, which in turn decreases incomplete alignments to 5%, and increases the overextension to 10%. Restricting the analysis to the top 25 homologs further decreases incomplete alignment to 2%, whereas increasing overextension to 11%. When the thresholds in  Table 2  are used to correct the top 100 alignments for each query, overextension is corrected 73% of the time, whereas incomplete alignments are produced only about 10% of the time. For the top 25, overextension is corrected 86% of the time, whereas alignments become incomplete only 1% of the time. We believe our estimates of alignment overextension (7–10% of alignments) are conservative, both because sequence databases are growing, allowing similarity searches to identify closer homologs, and because many proteins comprise multiple domains. In this study, we examine alignment overextension from a single domain. Many proteins contain multiple domains separated by non-homologous regions; proteins that contain multiple widely dispersed common domains, like Ankyrin, fn3 or SH3 domains, will have many more chances to overextend across non-homologous regions. Although we understand why high identity alignments might overextend when aligned with low target–identity scoring matrices like BLOSUM62, matrix/target–identity mismatch only accounts for about half of the overextensions we observed. In our diverse sequence set, 53% of overextensions occur in alignments that are &lt;36% identical. Unfortunately, we cannot predict which lower identity alignments will overextend. The amount of overextension does not correlate well with the difference between alignment identity and scoring matrix target identity. Likewise, overextension does not occur significantly more often in domains that have more identity at their ends. The increased frequency of overextended boundaries in alignments between high identity sequences ( Fig. 2 B) is the only meaningful trend that we identified. In contrast to high identity overextension, where the difference in target–identity between the homologous region and the scoring matrix can explain overextension, low identity overextension may simply reflect the propensity of deep matrices to produce long alignments, even between unrelated sequences ( Fig. 3 ). The long alignments in  Figure 3  are not statistically significant, but when they occur by chance near a (low identity) homologous domain, they can contribute to overextension. Overextension occurs more frequently in higher identity alignments because of target–identity mismatch, but the majority of overextension we measured occurs by chance in low identity alignments, because most of our alignments are low identity. In this study, we have focused on overextension in pairwise alignments because pairwise similarity searches are widely used to annotate newly sequenced genomes. Alignment overextension also occurs with model-based searches like PSI-BLAST; we initially identified overextension as the major cause of model contamination with PSI-BLAST ( Gonzalez and Pearson, 2010a ). Our strategy for reducing overextension—re-alignment with a more correct scoring matrix—is most easily applied to pairwise alignment because a traditional non–position-specific scoring matrix like BLOSUM62 or VT120 has an easily characterized target identity and the alignment between two sequences has a natural evolutionary distance. It is more difficult to interpret the ‘distance’ between a sequence and a position-specific scoring matrix or HMM, and it is unclear how such models might be scaled to reduce overextension. The expansion of modern protein databases has led to an increase in the identification of higher identity homologs. Accurate function prediction requires a higher level of sequence identity and an accurate alignment, two factors that are at odds with deep scoring matrices. With modern comprehensive databases, it is common to identify many homologs that are &gt;40% identical. In our diverse RPD2 protein set, the median sequence identity for the top 100 homologs was 53%, much higher than the target identity range for BLOSUM62. With more high identity homologs and increased sequence and structural annotation, pairwise alignments can provide essential insights to the function of novel proteins, but only if the alignment boundaries are accurate. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Multitask learning for host–pathogen protein interactions</Title>
    <Doi>10.1093/bioinformatics/btt245</Doi>
    <Authors>Kshirsagar Meghana, Carbonell Jaime, Klein-Seetharaman Judith</Authors>
    <Abstract>Motivation: An important aspect of infectious disease research involves understanding the differences and commonalities in the infection mechanisms underlying various diseases. Systems biology-based approaches study infectious diseases by analyzing the interactions between the host species and the pathogen organisms. This work aims to combine the knowledge from experimental studies of host–pathogen interactions in several diseases to build stronger predictive models. Our approach is based on a formalism from machine learning called ‘multitask learning’, which considers the problem of building models across tasks that are related to each other. A ‘task’ in our scenario is the set of host–pathogen protein interactions involved in one disease. To integrate interactions from several tasks (i.e. diseases), our method exploits the similarity in the infection process across the diseases. In particular, we use the biological hypothesis that similar pathogens target the same critical biological processes in the host, in defining a common structure across the tasks.</Abstract>
    <Body>1 INTRODUCTION Infectious diseases are a major health concern worldwide, causing millions of illnesses and deaths each year. Newly emerging viral diseases, such as swine H1N1 influenza, severe acute respiratory syndrome (SARS) and bacterial infections, such as the recurrent  Salmonella  and  Escherichia coli  outbreaks not only lead to wide-spread loss of life and health, but also result in heavy economic losses. To better navigate this landscape of infectious diseases, it is important to not only understand the mechanisms of individual diseases, but also the commonalities between them. Combining knowledge from related diseases will give us deeper insights into infection and host immune response, will enhance our ability to comprehend new diseases and lead to efficient development of therapeutics. Key to the infection process are host–pathogen interactions at the molecular level, where pathogen proteins physically bind with human proteins. Via these protein interactions, the pathogen manipulates important biological processes in the host cell, evades host immune response and multiplies within the host. Interactions between host and pathogen proteins can be studied using small-scale biochemical, biophysical and genetic experiments or large-scale high-throughput screening methods like yeast two-hybrid (Y2H) assays. Databases like PHI-base ( Winnenburg  et al. , 2008 ), PIG ( Driscoll  et al. , 2009 ), HPIDB ( Kumar and Nanduri, 2010 ), PHISTO ( Tekir  et al. , 2012 ) aggregate host–pathogen protein interactions from several small-scale and high-throughput experiments via manual literature curation. These databases are valuable sources of information for developing models of the modus operandi of pathogens. However, interaction datasets from these databases are not only small but are available for only a few well-studied pathogens. For example, the PHI-base ( Winnenburg  et al. , 2008 ) database covers 64 diseases but has only 1335 interactions, PIG ( Driscoll  et al. , 2009 ) covers only 12 pathogens. Computationally, this calls for techniques that combine datasets and build joint models across several pathogens, which can then be used to analyze the commonalities in the pathogens and also to predict plausible interactions that are biased by this joint understanding. In our work, we study host–pathogen protein–protein interaction (PPI) where the host is fixed and the pathogens are various bacterial species ( Fig. 1 A). The host species we consider is human and the bacterial species are  Yersinia pestis ,  Francisella tularensis ,  Salmonella  and  Bacillus anthracis , which cause the diseases bubonic plague, acute pneumonia, typhoid and anthrax, respectively.
 Fig. 1. ( A ) Host–pathogen PPI prediction where the host is human and the pathogens are bacteria. ( B ) An example depicting the commonality in the bacterial attack of human proteins. Pathway-1 and pathway-3 (highlighted) represent critical processes targeted by all bacterial species Some recent work on infectious diseases has alluded to the hypothesis that  different pathogens target essentially the same critical biological processes in the human body . The analysis by  Chen  et al.  (2012)  suggests that HIV infection shares common molecular mechanisms with certain signaling pathways and cancers.  Dyer  et al.  (2008)  study bacterial and viral interactions with human genes and find infection mechanisms common to multiple pathogens. Experiments by  Jubelin  et al.  (2010)  show how various bacterial cyclomodulins target the host cell cycle. The study by  Mukhtar  et al.  (2011)  on plant pathogens, in particular, Arabidopsis concludes that pathogens from different kingdoms deploy independently evolved virulence proteins that interact with a limited set of highly connected cellular hubs to facilitate their diverse life cycle strategies.  Figure 1 B illustrates an example depicting the commonality in various bacterial species, where they are targeting the same biological pathways in their human host. This biological hypothesis, which we henceforth call the  commonality hypothesis , is exploited here to jointly learn PPI models for multiple bacterial species. We translate the hypothesis into a prior that will bias the learned models. We use a multitask learning–based approach, where each ‘task’ represents the interaction of one bacterial species with human. The prior is represented in the form of a regularizer that penalizes models to the degree that the above hypothesis is violated. 2 BACKGROUND The most reliable experimental methods for studying PPI are often time-consuming and expensive, making it hard to investigate the prohibitively large set of possible host–pathogen interactions—for example, the bacterium  B.anthracis , which causes anthrax, has about 2321 proteins, which when coupled with the 25 000 or so human proteins gives approximately 60 million protein pairs to test, experimentally. Computational techniques complement laboratory-based methods by predicting highly probable PPIs—thereby enabling experimental biologists to focus on fewer interactions and ruling out the vast majority of unlikely ones. In particular, supervised machine learning–based methods use the few experimentally discovered interactions as training data and formulate the interaction prediction problem in a classification setting, with target classes: ‘interacting’ or ‘non-interacting’. Features are derived for each host–pathogen protein pair using various attributes of the two proteins such as protein sequence, gene expression, gene ontology (GO) etc. The general outline of the supervised PPI prediction procedure is illustrated in  Supplementary Figure S1 . Most of the prior work in PPI prediction has focussed on building models separately for individual organisms ( Chen and Liu, 2005 ;  Qi  et al. , 2006 ;  Singh  et al. , 2006 ;  Wu  et al. , 2006 ) or on building a model specific to a disease in the case of host–pathogen PPI prediction ( Dyer  et al. , 2007 ;  Kshirsagar  et al. , 2012 ;  Qi  et al. , 2009 ;  Tastan  et al. , 2009 ). The use of PPI data from several organisms has predominantly been in the form of (i) features derived from various PPI datasets, (ii) use of common structural properties of proteins across organisms ( Wang  et al. , 2007 ) or (iii) methods that narrow down predicted interactions in the organism of interest ( Garcia  et al. , 2010 ). Some of these methods use the concepts of ‘homologs’, ‘orthologs’ and ‘interologs’ to define a similarity measure between PPIs from various organisms ( Garcia  et al. , 2010 ). There has been little work on combining PPI datasets with the goal of improving prediction performance for multiple organisms.  Qi  et al.  (2010)  proposed a semi-supervised multitask framework to predict PPIs from partially labeled reference sets. The basic idea is to perform multitask learning on a supervised classification task and a semi-supervised auxiliary task via a regularization term. Another line of work in PPI prediction ( Xu  et al. , 2010 ) uses the Collective Matrix Factorization (CMF) approach proposed by  Singh and Gordon (2008) . The CMF method learns models for multiple networks by simultaneously factorizing several adjacency matrices and sharing parameters amongst the factors.  Xu  et al.  (2010)  use these ideas in their transfer learning setting, where the source network is a relatively dense interaction network of proteins and the objective is to infer PPI edges in a relatively sparse target network. To compute similarities between the nodes in the source and target networks, they use protein sequences and the topological structures of the interaction networks. 3 APPROACH Multitask learning is a family of machine learning methods that addresses the issue of building models using data from multiple problem domains (i.e. ‘tasks’) by exploiting the similarity between them. The goal is to achieve performance benefits for all the tasks involved. This paradigm of building joint models has been applied successfully in many areas including text mining, computer vision, etc. Because bioinformatics datasets often represent an organism, a natural notion of a ‘task’ is an ‘organism’—for example, the work by  Widmer  et al.  (2010)  uses a multitask learning approach for splice-site prediction across many organisms. They use phylogenetic trees to incorporate similarity between organisms (i.e. tasks). For a survey of multitask learning in computational biology, see  Xu and Yang (2011) . Our multitask learning method is based on the task regularization framework, which formulates the multitask learning problem as an objective function with two terms: an empirical loss term on the training data of all tasks, and a regularization term that encodes the relationships between tasks.  Equation (1)  shows the general form of such an objective, the term  R  being the regularizer raised to the power  p  and with a  q -norm. The work by  Evgeniou and Pontil (2004)  is one of the early few to develop this general approach. The function in  Equation (1)  represents a simple multitask objective with a single regularizer  R ; many of the formulations often involve a summation over multiple terms.
 (1) 
 We optimize this function by modifying the regularizer  R  to encode the biological hypothesis. Our approach differs greatly from prior work because we propose a technique to translate a problem-relevant biological hypothesis into a task regularization–based approach rather than applying existing general formalisms on a dataset. Our tasks try to capture a naturally occurring phenomenon. While our framework is developed in the context of a specific hypothesis, we also illustrate the incorporation of other hypotheses with an example. The key contributions of our work are as follows:
 we present a novel way of combining experimental PPI data coming from several organisms we incorporate domain knowledge in designing a prior that causes the learned models to exhibit the requisite common structure across the tasks to optimize the resulting non-convex objective function, we implement a concave convex procedure (CCCP)-based method 
 In the Methods section ( Section 4 ), we describe details of the PPI datasets and our multitask learning framework. The evaluation metrics and description of experiments is in  Section 6 , results and analysis in  Section 7 . 4 METHODS 4.1 Multitask pathway–based learning In this section, we describe how we incorporate the commonality hypothesis into our multitask classification framework formulating it as an optimization problem. We consider each human-bacteria PPI prediction problem as one task. The predicton problem is posed as a binary classification task, with each instance   being a pair of proteins  , where one protein is the bacterial protein ‘ b ’ (e.g.  Y. pestis ) and the other ‘ h ’ is the host protein (i.e. human). The class-label   represents interacting and non-interacting proteins respectively. Features are defined for every protein-pair using various properties of the individual proteins and combining them all into a single feature vector. The positive class in our training data comprises the known human-bacterial PPI which are obtained from databases like PHISTO ( Tekir  et al. , 2012 ). The construction of the negative-class data is explained in  Section 5 . Our objective is to minimize the empirical error on the training data while favoring models that are biased toward the commonality hypothesis. To achieve this, we use a bias term in the form of a regularizer in our objective function. For brevity and without loss of generality, we will henceforth refer to each human–bacteria PPI prediction problem as a ‘task’ (We will also refer to a task by the name of the bacterial species only, as the host species, i.e. human, is common across all tasks). Our method first combines all tasks in a pairwise manner, and finally aggregates the output from the pairwise models. Let   be the set of tasks to be combined, where  m  is the number of tasks. Consider two tasks   and  . Let the training data for the task   be   where each example  . Similarly, the training data for   is   where  .  n s  and  n t  are the number of training examples and  d s  and  d t  denote the number of features in the two tasks. Let   represent the parameter vectors, i.e. the models for the two tasks. We now describe how we combine these two tasks.  Section 4.3  will show how such pairwise models are aggregated. The pathway-based objective Biologists often represent the set of human proteins involved in a particular biological process by a graph called a ‘biological pathway’. One such example, the ‘glucose transport pathway’ in human is shown in the Supplementary. To use this pathway construct, we revise our hypothesis to ‘proteins from different bacterial species are likely to interact with human proteins from the same biological pathway’.  Figure 1 B illustrates an example where this hypothesis holds. The pathway information for each human protein can be obtained from pathway databases like Reactome ( Matthews  et al. , 2009 ) and PID ( Schaefer  et al. , 2009 ). While pathways are generally represented as graphs, for our current work we do not use the edges. We treat a pathway as a  set  of proteins—a human protein  h  can be a member of several pathways depending on the biological processes it is involved in. Let  N  be the total number of pathways in human. For a protein pair  , let   be the binary ‘pathway vector’ indicating the pathway membership of  h . The commonality hypothesis suggests that the pathway memberships of human proteins from interactions should be similar across tasks. We define a pathway-summary function  S , which aggregates all pathway vectors for a given task  . Because our hypothesis is about  interactions , we only consider pathway vectors of  positive  examples. Let   represent the set of positive examples from tasks   and  ; let   be their sizes. In  Figure 2 , we depict the aggregation done by  S . Mathematically, we have
 (2) 
where   is the pathway vector for example  i , and  .  S  sums up the pathway vectors of examples predicted to be positive. We normalize using   to compensate for the different dataset sizes across tasks.
 Fig. 2. A schematic illustrating the pathway summarizing function  S  for a task  . On the left are the examples from the input predicted to be positive, indicated by  . The matrix  P  has the pathway vectors for each example in  . The summary function aggregates the pathway vectors to get the distribution Let   be a matrix containing all pathway vectors for positive examples from task  . Analogously,   is a matrix for the positive examples from task  . Matrices  P s  and  P t  are constant matrices and are known a priori. Let   and   be the pathway summaries of the tasks. We want to penalize the dissimilarity between these summaries. Our objective function thus has the following general form:
 (3) 
 Here   and   can be any convex loss functions computed over the two tasks. We use logistic loss in our work based on prior experience with PPI datasets. The last two   norms over the parameter vectors   and   control overfitting. The parameters λ and σ take positive values. The indicator function  I pos  is non-differentiable. So we approximate  I pos  with the exponential function, which is a convex upper bound of the indicator function and will make optimization easier. Let  , where  C  is a positive constant. This function, for various values of  C  has been plot in  Figure 3 . Small positive values of   indicate positive-class predictions that are closer to the decision boundary of the classifier. Examples predicted to be positive with a high confidence have a large  z . With varying values of  C , the function   gives varying importance to predictions based on their classifier confidence ‘ z ’. Negative values of  z , which correspond to examples predicted to be negative, are given close to zero importance by  . The choice of an appropriate  C  is important so as to ensure the proper behavior of the summary function  S . A steeply increasing curve ( C  = 1) is undesirable as it will assign too much weight to the summary of only some of the examples. We chose a moderate value of  C  = 30 for our experiments.
 Fig. 3. The exponential function   for different values of  C Replacing  I pos  by   in  Equation (2) , our summary function  S  becomes  . Putting everything together, our objective with the logistic loss terms, the pathway summary function and the   regularizer terms has the following form:
 (3) 
where  The objective in  Equation (3)  is non-convex, and with some algebraic simplifications we can reduce it to a difference of convex functions (DC). To optimize this function, we implement the CCCP algorithm, which was originally introduced by  Yuille and Rangarajan (2003) . 4.2 Solving the optimization problem The objective in  Equation (3)  is non-convex in the shown form. We tried to optimize it directly using L-BFGS, but found that the objective does not decrease consistently. Below, we show that ( 3 ) is a DC functions. The first two log-loss terms [we abbreviate them henceforth as  ] and the last   term are all convex and do not pose any problem with optimization.
 P roposition  1 The objective ( 3 ) is a DC functions.
 (4) 
 
 P roof Expanding the pathway vectors   and   and rewriting  Equation (3)  we get
 (5) 
 Note that  f k  and  g k  are non-negative convex functions. This follows because   is a positive convex function and the matrices  P s  and  P t  are non-negative by construction.  f k  and  g k  are both thus positive linear combinations of convex functions and hence convex. We now decompose the squared term in  Equation (5)  as follows.
 (6) 
 We further observe that   is convex. To derive this, we use the following proposition: a composition of a monotonically increasing convex function and a convex function is still convex. The square function   is a monotonically increasing function for  , thus the composition with  f k  (i.e.  ) is also convex by the positivity of  f k . Analogously,   is also convex. Further,   is also convex by the same argument. Substituting ( 6 ) back into  Equation (5)  we get our result.
 (7) 
 To optimize this function, we use a CCCP algorithm ( Yuille and Rangarajan, 2003 ). Our approach is inspired by the work from  Yu and Joachims (2009)  on learning structural SVMs. The idea is to compute a local upper bound on the concave function ( ) and instead of optimizing  L  from  Equation (4)  directly, use an approximation based on the upper bound of  .  Equation (7)  shows this function  L approx . Let  w  represent the concatenation of the two parameter vectors   and  . Let   be the  k -th iterate. We have from Taylor’s first order approximation that   for all  w . This allows us to obtain the following approximation, which we get by substituting the above bound in place of   in  Equation (4) :
 (7) 
 since  
 is a constant. The optimization problem in  Equation (7)  is now convex and can be solved using conventional techniques like L-BFGS, conjugate gradient, etc. The outline of our CCCP-based procedure is shown in Listing 1. 
 Algorithm 1  CCCP procedure 1: Initialize  2:  repeat 3: Compute   using  4: Compute current value  L approx 5: Solve  6: Set  7: Compute new value  8:  9:  until 
 Yuille and Rangarajan (2003)  show that such a CCCP-based algorithm is guaranteed to decrease the objective function at every iteration and to converge to a local minimum or saddle point. We observe a similar behavior in our experiments. Computationally, this algorithm is efficient because the regularizer works on a subset of the data—only the positive examples, which are a small fraction of the complete training data. Stopping criteria The convergence criterion for algorithm 1 is  , where τ is a threshold. We used   in our experiments. Smaller values required a long time to convergence. The inner optimization (line # 5), which uses L-BFGS, had a convergence threshold of 0.0001. This step took more iterations initially and fewer iterations getting closer to convergence. 4.3 Combining pairwise models In the previous sections, we described how we combine two tasks. In particular,  Equation (3)  involves pairwise learning, which results in two models   and  . Because our current framework can combine only two tasks at a time, for  m  tasks we perform   pairwise learning experiments and then combine their outputs. Each task will thus have  m  − 1 models as a result of pairing up with each of the other tasks. Let the set of models for task   be  . We treat   as an ensemble of models for this task and aggregate the output labels from all models to get the final labels on the test data. Let the output labels from each model for a given test instance  x  be  . Then the final output label  y  is computed by taking a vote and checking if it crosses a threshold:
 (8) 
where  v  is a vote threshold that should be crossed in order for the label to be positive. In our experiments, we found that the predictions for   from all models in   overlapped greatly. Hence, we used  v  = 1, which implies that  x  is an interaction if any one of our four tasks labels it as such. 5 DATASET AND FEATURES For  Salmonella typhi , we used the list of 62 interacting protein pairs reported in  Schleker  et al.  (2012) , which were obtained by the authors by manual literature curation. These interactions come from small-scale experiments. The other three PPI interaction datasets were obtained from the PHISTO database. Most of the reported interactions for these three bacterial species come from a single high-throughput experimental study reported in  Dyer  et al.  (2010) . While  F.tularensis ,  S.typhi  and  Y.pestis  are gram-negative gamma-protobacteria,  B.anthracis  is a gram-positive bacteria. The number of unique proteins in each bacterial species, the sizes of all datasets and the number of all possible host–pathogen protein pairs are listed in  Table 1 .
 Table 1. Characteristics of all four interaction datasets used B.anthracis F.tularensis Y.pestis S.typhi Total no. of bacterial proteins (‘reviewed’ protein set from UniprotKB) 2321 1086 4600 3592 Total no. of human–bacteria protein pairs 59.4 M 27.8 M 117.7 M 87.7 M No. of known interactions 3073 1383 4059 62 No. of interactions with no missing features 655 491 839 62 Size of training data with 1:100 class ratio 66 155 49 591 84 739 6262 No. of unique features in the training data 69 4715 468 955 886 480 349 155 Note:  Total no. of human proteins: 25 596; M, million. For each host–pathogen PPI dataset, the number of pathogen proteins, the size of the dataset and other such statistics are shown. 5.1 Feature set For each protein pair, we compute features similar to the work in  Kshirsagar  et al.  (2012) . Some features use both proteins in the pair, while some others are based on either the host protein or the pathogen protein. While the features used for  S.typhi  were obtained directly from the authors, those for the other three datasets were derived from the following attributes of proteins available in public databases: protein sequences from Uniprot ( UniProt Consortium, 2011 ), gene ontology from GO database ( Ashburner  et al. , 2000 ), gene expression from GEO ( Barrett  et al. , 2011 ), properties of human proteins in the human PPI network. Owing to the lack of space, we briefly mention only some of the prominent features here, and encourage the readers to refer to the supplementary for details. The sequence features count the frequency of amino acid–based  n -grams or  n -mers (for  n  = 2, 3, 4, 5) in the protein sequence. The GO features count the co-occurrence of host–pathogen GO term combinations. The human PPI network-based features compute various graph properties like node-degree, betweenness-centrality, clustering coefficient of the human protein. Our features define a high-dimensional and sparse space (the number of features is listed in  Table 1 ). Because our features are derived by integrating several databases, some of which are not complete, there are many examples and features with missing values. In our current work, we eliminate all examples with &gt;10% missing features. For the rest, we use mean value–based feature imputation. Handling missing data effectively is an important aspect of the PPI prediction problem; however, it is not the focus of this work. The remaining examples after elimination and imputation are also shown in  Table 1 . 5.2 Negative class examples The interactions listed in the table form the positive class. Because there is no experimental evidence about proteins that do not interact, we construct the ‘non-interacting’ (i.e. negative) class using a technique commonly used in PPI prediction literature. We use random pairs of proteins sampled from the set of all possible bacteria–human protein pairs. The number of random pairs chosen as the negative class is decided by what we expect the interaction ratio to be. We chose a ratio of 1:100 meaning that we expect 1 in every 100 random bacteria–human protein pairs to interact with each other. In general, there is no basis for choosing a more meaningful ratio, as there are few known interactions. We rely on previous work on better-studied organisms, where a ratio of 1:100 was used, based on the number of known interactions. Further, prior studies ( Dyer  et al. , 2007 ;  Tastan  et al. , 2009 ) also use a similar ratio. This random selection strategy is likely to introduce ∼1% false negatives into the training set. 5.3 Analyzing the known interactions We analyze the known host–pathogen interactions from our datasets. This analysis also motivates our choice of a multitask approach that uses a pathway-based similarity across tasks. The known PPIs are compared across datasets in two ways: (i) pathway enrichment and (ii) presence of interologs.
 The human proteins involved in each interaction dataset are used to obtain the human pathways that are enriched. We use Fisher’s test (based on the hypergeometric distribution) to compute the  P -value of each pathway. We plot these  P -values for each pathway, and for each dataset in the form of a heatmap shown in  Figure 4 . The heatmap shows how there are several commonly enriched pathways across the datasets (the black vertical lines spanning all four rows). It also shows the difference in the enrichment for the  S.typhi  dataset, which comes from small-scale PPI experiments. We analyze the similarity between the PPIs from various datasets. A natural way to determine similarity is to check if proteins known to interact in one dataset have homologous proteins that are also interacting in another dataset. Such pairs of proteins, also called ‘interologs’, are defined as a quadruple of proteins  , where   (interaction) and  . Further,  A ,   are homologs and  B ,   are also homologs. The number of such interologs existing between the four datasets is shown in  Table 2 . To compute homologs of a protein, we used BLASTP sequence alignment with an e-value cutoff of 0.1. As evident from  Table 2 , there are few interologs across the bacterial PPIs. None of the high-throughput datasets have an interolog in the small-scale  S.typhi  dataset. This seems to indicate that interolog-based approaches to compute task similarity are not relevant here. The phenomenon governing the similarity of these host–pathogen interactions is probably at a much higher level, rather than at the level of individual proteins. We explore one such possibility—the ‘commonality hypothesis’. 
 Fig. 4. Heatmap showing pathways enriched in each bacterial–human PPI interactions dataset. The horizontal axis represents the pathways (about 2100 of them) and the vertical axis represents the four datasets. Each entry in the heatmap represents the  P -value of a pathway w.r.t one dataset. Darker values represent more enrichment. The black columns that span across all four rows show the commonly enriched pathways 
 Table 2. Conserved interactions in the form of interologs across the various host–bacterial datasets Human–bacteria PPI datasets compared H-B versus H-F H-B versus H-Y H-B versus H-S H-F versus H-Y H-F versus H-S H-Y versus H-S Number of Interologs 2 3 0 3 0 0 Note : H-X: stands for human–pathogen where the pathogen ‘X’ can be B, F, Y and S referring to  B.anthracis ,  F.tularensis ,  Y.pestis  and  S.typhi. , respectively. The non-zero entry ‘2’ for ‘H-B versus H-F’ means there are two PPIs in the H-B dataset that have interologs in the H-F dataset. 6 EXPERIMENTS We use 10-fold cross validation (CV) to evaluate the performance of all algorithms. Our evaluation criteria do not use accuracy, which measures performance on both the classes. Because our datasets are highly imbalanced with a large number of negative samples, a naïve classifier that always says ‘no’ would still have a high accuracy. We instead use precision and recall computed on the interacting pairs (positive class) because they can deal with class imbalance.
 
 The baselines that we compare against are briefly described below. Independent models (Indep.):  We train models independently on each task using two standard classifiers: Support Vector Machines and Logistic regression with   and   regularization. We used LibLinear ( Fan  et al. , 2008 ) for these experiments and found that logistic regression with   regularization performs the best across all tasks. For conciseness, we report only the best model’s performance. Coupled models:  This baseline was implemented so as to couple the regularizer parameter across two tasks, thereby keeping the basic framework similar to that in our technique. To achieve this we optimize the function in  Equation (9)  and use the L-BFGS implementation from Mallet. Note that the previous baseline has separate regularization parameters for each task.
 (9) 
 Independent models with pathway features (Indep. Path.):  This baseline incorporates the pathway information from the pathway vectors   as features. For each example  i , the feature vector is appended by the pathway vector  . While our method uses the pathway vectors only for the positive class examples (via the matrices  P s  and  P t ), this baseline uses the pathway information for all examples via features. The performance of this baseline will indicate whether using raw pathway information without incorporating any biologically relevant coupling does well. We learn independent models for each task as before, and find that logistic regression with   regularization does the best (only these results are reported). Mean Multi-task Learning (Mean MTL):  This is a logistic regression-based implementation of the multitask SVM model proposed by ( Evgeniou and Pontil, 2004 ). The important feature of this work is the use of a regularizer that penalizes the difference between a model and the ‘mean’ model formed by averaging over models from all  m  tasks. In the original paper, the loss functions   were all hinge loss. Because we find that logistic regression does better on our datasets, we replaced the original hinge loss function by logistic loss. The objective we use is shown in  Equation (10) .
 (10) 
 Multitask pathway-based learning:  This refers to our technique, which minimizes the sum of logistic loss over the two tasks with an   regularization penalizing the difference between the pathway summaries. We train two tasks at a time and compute the performance for each task. Because we have four tasks, there are six such pairwise learning experiments in all. While evaluating performance during 10-fold CV, we obtain the F1 on 1-fold of a task   by averaging the F1 across all pairwise learning experiments that involve   (see  Section 4.3  for details). The final CV performance reported in our results is an average over 10-folds. 6.1 Parameter tuning We followed an identical procedure for all algorithms. For the 10-fold CV experiments we train on 8-folds, use 1-fold as held-out and another as test. The optimal parameters (i.e. the best model) were obtained by parameter tuning on the held-out fold. The test fold was used to evaluate this best model—these results are reported in  Section 7 . The range of values we tried during the tuning of the regularization parameter (λ) were 150–10 −4 . For σ, the parameter controlling overfitting in multitask pathway–based learning (MTPL), we used a fixed value of  . For Mean MTL, we tune both λ and σ. To handle the high-class imbalance in our data, we used a weight-parameter  W pos  to increase the weight of the positive examples in the logistic loss terms of our function. We tried three values and found   performed the best on training data. 7 RESULTS AND DISCUSSION 7.1 Overall performance Table 3  reports for each bacterial species, the average F1 along with the standard deviation for the 10-fold CV experiments. The performance of all baselines is similar, and our method outperforms the best of the baselines by a margin of 4 points for  B.anthracis , 3.4 points for  F.tularensis  and 3.2 points for  Y.pestis  and 3.3 for  S.typhi . The overall performance of all methods on this dataset is twice as good as that on the others. We believe that the difference in the nature of the datasets might explain the above observations. While the  S.typhi  dataset comprises small-scale interaction studies, the other datasets come from high-throughput experiments. Owing to its smaller size, it has less variance making it an easier task. This dataset is also likely to be a biased sample of interactions, as it comes from focussed studies targeting select proteins.
 Table 3. Averaged 10-fold CV performance for all methods for a positive:negative class ratio of 1:100 Method B.anthracis F.tularensis Y.pestis S.typhi Indep. 27.8 ± 4 25.7 ± 5.4 28.8 ± 4 72.5 ± 11.4 Coupled 27 ± 3.9 25.5 ± 5 27.9 ± 3.4 69.8 ± 12.4 Indep. Path. 26.5 ± 4.7 26.1 ± 6.9 26.7 ± 4.3 69.1 ± 12.7 Mean MTL 25.2 ± 4.9 26.7 ± 4 27.5 ± 6.3 69.4 ± 12.1 MTPL 31.8  ± 3.9 30.1  ± 5.8 32.1  ± 2.5 75.8  ± 12.1 Note : Accuracy is reported as the F1 measure computed on the positive class. The standard deviation over the 10-folds is also reported. Bold values indicate the highest F1 value for each column (i.e. for that PPI dataset). The coupled learner (Coupled) performs slightly worse than Indep. This is explained by the fact that Indep. has more flexibility in setting the regularization parameter for each task separately, which is not the case in Coupled. It is interesting to note that the independent models that use the pathway matrices  P s  and  P t  as features (i.e. Indep-Path) show a slightly worse performance than the Indep. models that do not use them. This seems to suggest that the cross-task pathway similarity structure that we enforce using our regularizer has more information than simply the pathway membership of proteins used as features.  Precision-Recall curves:  We also plot the P-R curves for MTPL. Please see the  Supplementary Section 3 . 7.2 Paired  t -tests for statistical significance Given two paired sets of  k  measured values, the paired  t -test determines whether they differ from each other in a significant way. We compare MTPL with Indep.—the best baseline from the 10-fold CV results. Because the 10-fold CV results from the previous section give insufficient samples (i.e. only 10 samples), we instead use 50 bootstrap sampling experiments and use the results to compute the  P -values. Each bootstrap sampling experiment consists of the following procedure: we first make two random splits of 80 and 20% of the data, such that the class ratio of 1:100 is maintained in both. The training set is then constructed using a bootstrap sample from the 80% split and the test data from the 20% split. A total of 50 models are thus trained and evaluated. We do not tune parameters again for each model and instead use the optimal setting of parameter values from our 10-fold CV experiments. The F1 is computed for each experiment thereby giving us 50 values, which will be our samples for the hypothesis test. Because  t -tests assume a normal distribution of the samples, we first did a normality test on each set of 50 F1 values. We performed the Shapiro–Wilk test with a significance level of   and found that our samples satisfy normality. The averaged F1 over the 50 bootstrap experiments for the four tasks with MTPL and Indep. has been tabulated in the Supplementary Table S1. We observe that MTPL does better than Indep. for the three high-throughput datasets and marginally underperforms for the  S.typhi.  dataset.  Table 4  shows the  P -values on applying the paired  t -tests to the 50 F1 values. For three of the four tasks, the performance improvement by MTPL is clearly statistically significant. For the fourth task, which involves  S.typhi. , the baseline has a slightly better averaged performance but the  P -value does not indicate statistical significance. Hence we can say that the performance of MTPL and Indep. is similar for this task.
 Table 4. P -values from pairwise  t -tests of statistical significance B.anthracis F.tularensis Y.pestis S.typhi P -value 4.1e-04 a 9.1e-04 a 2.2e-07 a 0.1 b Note : We compare MTPL with the best baseline ‘Indep.’, using results from 50 bootstrap sampling experiments. The null hypothesis is ‘there is no significant difference between the performance of MTPL and Indep.’. Null hypothesis: MTPL = Indep.  a Alt. hypothesis: MTPL &gt; Indep.  b Alt. hypothesis: MTPL &lt; Indep. 7.3 Pairwise performance of tasks in MTPL The previous section gave a summary of the aggregated performance of MTPL for every task. Here we present the performance of every pairwise learning experiment of MTPL in  Table 5 . This gives an idea of how various tasks benefit from being paired up with other tasks. For each task, we check the task-pairing that gave the best performance (best F1 for each task is shown in bold). For instance, the best F1 of 32.3 for  Y.pestis  was obtained in the pairwise model learned with  S.typhi . It is evident that coupling a model with one additional task seems to improve the performance over the baseline.
 Table 5. Pairwise model performance of MTPL Pairwise tasks F1 Task-1, Task-2 Task-1 Task-2 B.anthracis ,  F.tularensis 31.4 30.1 B.anthracis ,  S.typhi 32 76.3 B. anthracis ,  Y.pestis 31.6 32 F.tularensis ,  S.typhi 30.3 73 F.tularensis ,  Y.pestis 30 32.1 S.typhi ,  Y.pestis 74.2 32.3 Note : F1 computed during 10-fold CV of various pairwise models from MTPL. Positive: negative class ratio was 1:100. The best F1 achieved for each task (i.e. for each bacterial species) is shown in bold. For example,  B.anthracis  has the best performance of 32 when it is coupled with  S.typhi . 7.4 Feature importance across tasks To get an understanding of inter-task model similarity, we compared the parameter vectors ‘ w ’ of all tasks with each other (each  w  was learned on the entire training data). Because the number of features is large, we computed the cosine similarity between them. Note that we only use features that are common across tasks for this comparison. Gene expression features for instance were not used as they vary with regard to the number of expression time points, the experiment protocol, etc. We found that the feature weights vary greatly across models—the cosine similarity ranges between 0.1 and 0.13. We also analyzed which features had the highest absolute weight. We found that the node-degree feature (computed using the human PPI graph) has a high positive weight across all tasks. Gene expression features have large negative weights across all tasks. In general, the GO and protein sequence–based  n -gram features have different weights across tasks. This seems to imply that having similar parameter values across models is not particularly important for this multitask problem. This explains why one of our baselines: the Mean MTL method, which penalizes differences between parameter vectors, does not perform well. Instead, regularization using the pathway summaries seems key in giving a better performance. Sparsity of weights: We use   regularization in our optimization function, which does not produce sparse weight vectors. We observe that ∼50% of the features have 0 weight in all tasks. About 75–80% of the features have small weights in the range of (0.001 to −0.001). 7.5 Analysis of predictions The F1 measure gave us a quantitative idea of the performance of each method on training data. In this section, we present a qualitative analysis of the new interactions that our models predict. We first construct, for each task ‘ ’, a random set  R t  of protein pairs that is disjoint from the training dataset. We train the pairwise models on the training data and obtain predictions on  R t . The method described in  Section 4.3  is used to aggregate predictions from all pairwise models. The subset of  R t  labeled as ‘positive’ is used for the analysis described below. Enriched human pathways We perform enrichment analysis on the human pathways from the positive predictions of MTPL. We use Fisher’s exact test with the hypergeometric distribution. We intersect the top enriched pathways that satisfy  P -value  e-07 from each task to get the commonly enriched pathways. The sizes of the various intersections are shown in  Figure 5 . Seventeen pathways are commonly enriched across all four tasks. One hundred four pathways are enriched across the three high-throughput datasets, which is a significant fraction of the total number of pathways considered. This result indicates that the bias produced by our regularizer does produce predictions satisfying the commonality hypothesis.
 Fig. 5. The intersection of enriched human pathways from predicted interactions. The total number of enriched pathways for each bacterial species are  B.anthracis : 250,  F.tularensis : 164,  Y.pestis : 400 and  S.typhi : 40. The size of the intersection between all tasks’ enriched pathways is 17. The size of this intersection for the high-throughput datasets (excluding  S.typhi ) is much larger: 104 Table 6  shows some of the common enriched pathways. The ‘Integrin alpha IIb beta3 (αIIb  ) signaling’ pathway is enriched only in  B.anthracis  and  Y.pestis  in the training data. However, in the predictions it is enriched in all four bacterial datasets. Integrin-αIIb   is a transmembrane receptor expressed in mast cells and plays an important role in innate immune responses against pathogens.
 Table 6. Five of the 17 commonly enriched pathways in the predicted interactions from MTPL Platelet activation, signaling and aggregation Integrin alpha IIb beta3 signaling Stabilization &amp; expansion of E-cadherin adherens junction Post-translational regulation of adherens junction stability &amp; disassembly Signaling by NGF We also analyze the overlap between the pathways enriched in the gold-standard positives and those enriched in the predictions. Please see the  Supplementary Section 5  for details. 7.6 Incorporating other biological hypotheses The regularizer in  Equation (3)  uses the pathway information matrix to enforce pathway-level similarity. The matrix can be used to represent any other common structure. For example, consider the hypothesis ‘ all pathogens target hub proteins in the host ’, which implies that bacterial proteins are often found to interact with host proteins that have a high node degree in the PPI network of the host. We tried two variants to incorporate this hypothesis—(i) we identify ‘hubs’ in the human PPI graph and use the binary vectors   as an indicator of the ‘hub’ protein targeted by the bacterial protein, (ii) instead of a discrete ‘hub’/‘not hub’ indicator we use   to represent the node degree [each component of   represents one node-degree bin say (10–20)]. We found that using (i) gives us an improvement of upto 2.5 F points over the baseline methods. 8 CONCLUSION We presented a method that uses biological knowledge in jointly learning multiple PPI prediction tasks. Using a task regularization–based multitask learning technique, we were able to encode a biological hypothesis into the optimization framework effectively, thus enabling the commonality hypothesis to be tested. Our results indicate that the tasks benefit from this joint learning and we see an improvement of 4 F points over the baseline methods. While our current results were presented on four bacterial species, we plan to extend our analysis to several other pathogens. Another direction to explore is the case where the pathogen is fixed and the hosts are different. Our current approach integrates multiple tasks in a pairwise manner, which is inefficient because it does not scale well while integrating several PPI datasets. The most straightforward way of extending  Equation (3)  to learning  m  tasks simultaneously involves loss terms for each of the tasks and   pairwise regularization terms, which unfortunately makes the optimization problem more complex and inefficient. A more promising and efficient direction would be to consider model selection at the task level where only the most relevant and useful tasks are used for multitask learning. 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>HypercubeME: two hundred million combinatorially complete datasets from a single experiment</Title>
    <Doi>10.1093/bioinformatics/btz841</Doi>
    <Authors>Esteban Laura A, Lonishin Lyubov R, Bobrovskiy Daniil M, Leleytner Gregory, Bogatyreva Natalya S, Kondrashov Fyodor A, Ivankov Dmitry N, Schwartz Russell</Authors>
    <Abstract/>
    <Body>
 01  Input : list of genotypes 02  Output : list of found hypercubes 03  FOR  each  Genotype  from  Input : 04   HCube.Diagonal  &lt;- empty list 05   HCube.First  &lt;-  Genotype 06   HCube.Last  &lt;-  Genotype 07   ADD   HCube  to  ListHCubes[0] 08  N  &lt;- 0     #  current dimensionality 09  REPEAT : 10  FOR  each  Group  from  ListHCubes[N]  with same  Diagonal : 11    FOR  each pair of hypercubes  HCube1 ,  HCube2  from the  Group : 12     IF  mutation from  HCube1.First  to  HCube2.First  is single: 13      Forward  &lt;- mutation from  HCube1.First  to  HCube2.First 14     Reverse  &lt;- mutation from  HCube2.First  to  HCube1.First 15       IF   Forward  is alphabetically less than  Reverse : 16        NewDiagonal  &lt;- list( Diagonal ,  Forward ) 17     HCube  &lt;- ( NewDiagonal ,  HCube1.First ,  HCube2.Last ) 18       ELSE : 19        NewDiagonal  &lt;- list( Diagonal ,  Reverse ) 20      HCube  &lt;- ( NewDiagonal ,  HCube2.First ,  HCube1.Last ) 21       IF   Mutations  in  NewDiagonal  are alphabetically ordered: 22        ADD   HCube  to  ListHCubes[N + 1] 23   SORT   ListHCubes[N + 1]  by  Diagonal 24   N  &lt;-  N  +   1 25  UNTIL  no new hypercubes are found 
 1 Introduction Epistasis, the dependence of the impact of a mutation on the genetic context, is abundant and important phenomenon in molecular evolution ( Breen  et al. , 2012 ). Formally, epistasis is characterized by coefficients  α  having two or more indices in the following representation of fitness  f  as a function of a genotype  g  (assuming, for simplicity, that maximum of one mutation is allowed at any position):
 f ( g ) = const + ∑ i = 1 N α i δ i + ∑ i = 1 N ∑ j &gt; i N α ij δ i δ j + ∑ i = 1 N ∑ j &gt; i N ∑ k &gt; j N α ijk δ i δ j δ k + ⋯ where sums are taken over  N  considered positions,  δ i  = 1 if  i -th position is mutated in genotype  g ; otherwise  δ i  = 0 or  δ i  = –1, depending on the formalism of epistasis description ( Poelwijk  et al. , 2016 ). Coefficients  α i  correspond to a single effect of the mutation in the  i -th position. Coefficients  α ij , having two indices, represent the pairwise epistasis between positions  i  and  j , while coefficients  α  having three or more indices correspond to ‘higher-order epistasis’ ( de Araujo and Guimaraes, 2016 ;  Otwinowski  et al. , 2018 ;  Poelwijk  et al. , 2016 ;  Sailer and Harms, 2017a ,  b ,  c ;  Tuo, 2018 ;  Weinreich  et al. , 2013 ,  2018 ). To detect epistatic terms of the order  n  by means of Walsh transform ( Weinreich  et al. , 2013 ), one has to measure phenotypes of all 2 n  genotypes forming an  n -dimensional hypercube in genotype space. Such experimental datasets are called ‘combinatorially complete datasets’ ( Weinreich  et al. , 2013 ). Up to now, higher-order epistasis was studied using only a handful of examples carefully designed to have all 2 n  combinations ( Weinreich  et al. , 2013 ). On the other hand, high-throughput experiments using (quasi-)random mutagenesis have produced vast amounts of data: 51 715 measured genotypes for GFP ( Sarkisyan  et al. , 2016 ), over 65 000 for arginine tRNA ( Li  et al. , 2016 ), 956 648 for HIS3 protein ( Pokusaeva  et al. , 2019 ), to name a few. These experiments may contain a number of combinatorially complete datasets as subsets of a general dataset. However, the extraction of such hypercubes from a large dataset is not straightforward, and may not be feasible to do in a brute-force manner. 2 Algorithm The algorithm uses the fact that any  n -dimensional hypercube contains two opposite hyperfacets, which are parallel to each other. Those hyperfacets are hypercubes of dimensionality ( n —1), which, in turn, are built from parallel hypercubes of dimensionality ( n —2), etc. down to hypercubes of 0-th dimensionality (which are simply points in genotype space, i.e. genotypes). The algorithm consists of repeating steps. At each step, all possible  n -dimensional hypercubes are generated from the set of ( n ––1)-dimensional hypercubes. Informally, at each step, the algorithm takes all pairs of existing parallel hypercubes and if the distance between the hypercubes in the pair is one, the pair composes the hypercube of a higher dimensionality ( Fig. 1 ). We need to define the diagonal of a combinatorially complete dataset as a list of mutations transforming a genotype of the dataset to the most distant genotype of the same dataset. An  n -dimensional combinatorially complete dataset therefore contains 2 n – 1  diagonals, each of which (if not empty) can be written in the forward and reverse direction. Fig. 1. Example illustrating the work of the algorithm. ( A ) A graph corresponding to a five-dimensional hypercube is given, where measured genotypes are drawn in yellow-green and non-measured genotypes are drawn in grey. Vertices are given in binary notation, where each digit corresponds to one of five substitution sites. The digit is zero if the corresponding substitution site contains an amino acid of the reference ‘00000’ genotype; otherwise, it is one. Two vertices are connected if they differ by only one digit. The graph is drawn so that: (1) all vertices are visible (that is, no vertex is shaded by another one); (2) all vertices having the same number of zeros belong to the same vertical line; (3) the edges parallel in five-dimensional cube are drawn parallel to each other, and in different colors, for convenience. Blue, black, red, pink and green edges correspond to substitutions in the fifth, fourth, third, second and first sites, respectively. ( B ) The algorithm is applied to an example of random mutagenesis data from the panel (A). The diagonal for each group of hypercubes is given in bold Formally, the algorithm consists of the following steps: Each step of the algorithm can be easily parallelized. The multi-processor version can be found at  https://github.com/ivankovlab/HypercubeME.git . 3 Results We have applied the algorithm to the recently published fitness landscape for HIS3 protein ( Pokusaeva  et al. , 2019 ), the biggest fitness landscape published so far. The HIS3 protein was divided into 12 segments, and quasi-random mutagenesis has been done in each segment separately. We had to exclude indels and mutations outside the segment, so the number of considered experimentally measured genotypes ranged from 16 182 for segment S7 to 82 081 for segment S2, overall summing up to 721 791 genotypes ( Supplementary Table S1 ). We have found all 199 847 053 hypercubes having dimensionality from 2 to 12. The single-processor working time ranged from 2 h for S7 to almost 10 days for S5. Among the found hypercubes, the percentage of squares was 12%, while the remaining 88% had dimensionality 3 and higher and, thus, can be used for exploring higher-order epistasis. The number of found hypercubes throughout segments is given in  Supplementary Table S2 . Supplementary Material btz841_Supplementary_Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Differentially conserved amino acid positions may reflect differences in SARS-CoV-2 and SARS-CoV behaviour</Title>
    <Doi>10.1093/bioinformatics/btab094</Doi>
    <Authors>Bojkova Denisa, McGreig Jake E, McLaughlin Katie-May, Masterson Stuart G, Antczak Magdalena, Widera Marek, Krähling Verena, Ciesek Sandra, Wass Mark N, Michaelis Martin, Cinatl Jindrich, Kelso Janet</Authors>
    <Abstract/>
    <Body>1 Introduction In December 2019, severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), a novel betacoronavirus, was identified that causes a respiratory disease and pneumonia called coronavirus disease 19 (COVID-19) ( Coronaviridae Study Group of the International Committee on Taxonomy of Viruses, 2020 ;  Zhu et al., 2020 ). As of 22nd of December 2020, 77 801 721 confirmed COVID-19 cases and 1 713 109 COVID-19 deaths have been reported ( Dong  et al. , 2020 ). Since 2002, SARS-CoV-2 is the third betacoronavirus, after severe acute respiratory syndrome coronavirus (SARS-CoV) and Middle East respiratory syndrome coronavirus (MERS-CoV), that has caused a substantial outbreak associated with significant mortality ( Wu  et al. , 2020 ). SARS-CoV-2 is closely related to SARS-CoV ( Coronaviridae Study Group of the International Committee on Taxonomy of Viruses, 2020 ;  Wu  et al. , 2020 ). Entry of both viruses is mediated via interaction of the viral Spike (S) protein with the cellular receptor ACE2, and both viruses depend on S activation by cellular proteases, in particular by TMPRSS2 ( Cui  et al. , 2019 ;  Hoffmann  et al. , 2020a ;  Walls  et al. , 2020 ;  Wan et al., 2020 ;  Wrappet al., 2020 ;  Wu  et al. , 2020 ;  Yan et al., 2020 ). Despite these similarities, the diseases caused by SARS-CoV-2 (COVID-19) and SARS-CoV (SARS) differ. According to WHO, the SARS-CoV outbreak resulted in 8098 confirmed and suspected cases and 774 deaths, equalling a mortality rate of 9.6% ( www.who.int ). Estimated mortality rates for SARS-CoV-2 are below 1% ( Borges do Nascimento, 2020 ). SARS-CoV was only spread by symptomatic patients with severe disease ( Cheng  et al. , 2013 ). In contrast, SARS-CoV-2 has been reported to be transmitted by individuals who are asymptomatic during the incubation period or who do not develop symptoms at all ( Rivett et al. , 2020 ). We have developed an approach to identify sequence-associated phenotypic differences between related viruses based on the identification of differentially conserved amino acid sequence positions (DCPs) and in silicomodelling of protein structures ( Martell  et al. , 2019 ;  Pappalardo et al. , 2016 ). Conserved amino acid positions are likely to be of functional relevance, and differential conservation may indicate functional differences and they have been widely used for the analysis of protein families ( Rausell et al. , 2010 ,  Das  et al ., 2015 ). Here, we used this method to identify differentially conserved positions that may explain phenotypic differences between SARS-CoV-2 and SARS-CoV. These data were combined with data derived from virus-infected cells. 2 Materials and methods 2.1 Structural analysis Sequences for each of the SARS-CoV-2 proteins were obtained from the GISAID resource. The protein sequences were then filtered for sequences from human hosts with high coverage, and sequences with spans of X’s were removed. The number of sequences retained after filtering for each protein is shown in  Supplementary Table S4 . Fifty-three SARS-CoV genome sequences derived from human hosts were downloaded from VIPR ( Pickett  et al. , 2012a , b ). Open Reading Frames (ORFs) were extracted using EMBOSS getorf ( Rice  et al. , 2000 ) and matched to known proteins using BLAST. Fragments and mismatches were discarded. To match the ORF1ab non-structural proteins, a BLAST database of the sequences from the SARS non-structural proteins was generated and the SARS-CoV-2 ORF1ab searched against it. The sequences for each protein were then aligned using ClustalO ( Sievers et al. , 2011 ) with default settings. Conserved positions were identified by calculating the Jensen-Shannon divergence score ( Capra &amp; Singh, 2007 ) for each position in the multiple sequence alignment in virus. Differing alignment positions with conservation score &gt;0.8 for both species were considered as differentially conserved positions (DCPs). SARS-CoV-2 and SARS-CoV protein structures were downloaded from the Protein Databank (PDB;  Supplementary Table S1 ) ( Armstrong  et al. , 2020 ). Where structures were not available, they were modelled using Phyre2 ( Kelley  et al. , 2015 ;  Supplementary Table S2 ). Where Phyre2 did not generate a confident model, structural models from AlphaFold were used ( Senior  et al. , 2020 ). Ligand binding sites were modelled using 3DLigandSite ( Wasset al., 2010 ). DCPs were mapped onto protein structures using PyMOL. Exposed (solvent-accessible) and buried (solvent-inaccessible) residues were identified using Python module  findSurfaceResidues  with default parameters. Amino acid changes at DCPs were manually analysed for their potential impact on protein structure and function based on the presence or absence of hydrogen bonding, changes in hydrogen bonding capacity and changes in charge in SARS-CoV compared with SARS-CoV-2 proteins. Where models were unavailable, mutagenesis was performed within PyMOL to assess the potential impact of the amino acid changes. The structural analysis grouped DCPs into six different categories based on the effect that they were proposed to have. These include ‘unlikely’, ‘possible’ and ‘likely’. The possible and likely categories were split into three and two subgroups respectively depending on the type of effect ( Supplementary Table S3 ). 2.2 Cell culture The Caco2 cell line was obtained from DSMZ (Braunschweig, Germany). The cells were grown at 37°C in minimal essential medium (MEM) supplemented with 10% foetal bovine serum (FBS), 100 IU/ml penicillin, and 100 μg/mL of streptomycin. 293 cells (PD-02-01; MicrobixBisosystems Inc.) and 293/ACE2 cells ( Kamitani  e t al. , 2006 ) (kindly provided by Shinji Makino, UTMB, Galveston, Texas) were cultured in Dulbecco’s modified Eagle medium (DMEM) supplemented with 10% FBS, 50 IU/mL penicillin and 50 µg/mL streptomycin. Selection of 293/ACE2 cells constitutively expressing human angiotensin-converting enzyme 2 (ACE2) was performed by addition of 12 µg/mL blasticidin. All culture reagents were purchased from Sigma (Munich, Germany). Cells were regularly authenticated by short tandem repeat (STR) analysis and tested for mycoplasma contamination. 2.3 Virus infection The isolate SARS-CoV-2/1/Human/2020/Frankfurt ( Hoehl et al. , 2020 ) was cultivated in Caco2 cells as previously described for SARS-CoV strain FFM-1 ( Cinatl et al. , 2004 ). Virus titres were determined as TCID50/ml in confluent cells in 96-well microtitre plates ( Cinatl et al. , 2003 ;  2005 ). 2.4 Western blot Western blotting was performed as previously described ( Schneider  et al.  2017 ). Briefly, cells were lysed using Triton-X-100 sample buffer, and proteins were separated by SDS-PAGE. Proteins were blotted on a nitrocellulose membrane (Thermo Scientific). Detection occurred by using specific antibodies against β-actin (1:2500 dilution, Sigma-Aldrich, Munich, Germany), ACE2 and TMPRSS2 (both 1:1000 dilution, abcam, Cambridge, UK) followed by incubation with IRDye-labeled secondary antibodies (LI-COR Biotechnology, IRDye ® 800CW Goat anti-Rabbit, 926-32211, 1:40 000) according to the manufacturer’s instructions. Protein bands were visualized by laser-induced fluorescence using infrared scanner for protein quantification (Odyssey, Li-Cor Biosciences, Lincoln, NE, USA). 2.5 Receptor blocking experiments SARS-CoV/SARS-CoV-2 receptor blocking experiments were adapted from  Cinatl et al  (2004) . Caco2 cells were pre-treated for 30 min at 37°C with goat antibodies directed against the human ACE2 or DDP4 ectodomain (R&amp;D Systems, Wiesbaden-Nordenstadt, Germany). Then, cells were washed three times with PBS and infected with SARS-CoV-2 at MOI 0.01. Cytopathogenic effects were monitored 48 h post-infection. Cytopathogenic effect (CPE) was assessed visually by light microscopy by two independent laboratory technicians 48 h after infection ( Cinatl et al. , 2003 ). 2.6 Antiviral assay Confluent cell cultures were infected with SARS-CoV-2 or SARS-CoV in 96-well plates at MOI 0.01 in the absence or presence of drug. Cytopathogenic effect (CPE) was assessed visually by light microscopy by two independent investigators 48 h post-infection ( Cinatl et al. , 2003 ). 2.7 Viability assay Cell viability was determined by 3-(4,5-dimethylthiazol-2-yl)-2,5-diphenyltetrazolium bromide (MTT) assay modified after Mosmann ( Mosmann, 1983 ), as previously described ( Onafuye et al. , 2019 ). 2.8 Qpcr SARS-CoV-2 and SARS-CoV RNA was isolated from cell culture supernatants using AVL buffer and the QIAamp Viral RNA Kit (Qiagen) according to the manufacturer’s instructions. RNA was subjected to OneStepqRT-PCR analysis using the SYBR green based Luna Universal One-Step RT-qPCR Kit (New England Biolabs) and a CFX96 Real-Time System, C1000 Touch Thermal Cycler. Primers were adapted from the WHO protocol ( Corman et al. , 2020 ) targeting the open reading frame for RNA-dependent RNA polymerase (RdRp) of both SARS-CoV-2 and SARS-CoV: RdRP_SARSr-F2 (GTGARATGGTCATGTGTGGCGG) and RdRP_SARSr-R1 (CARATGTTAAASACACTATTAGCATA) using 0.4 μM per reaction. RNA copies/ml were determined by standard curves which were using plasmid DNA (pEX-A128-RdRP) harbouring the corresponding amplicon regions for SARS-CoV-2 RdRP target sequence (GenBank Accession number NC_045512 ). For each condition, three biological replicates were used. Mean and standard deviation were calculated for each group. 3 Results 3.1 Determination of differentially conserved positions (DCPs) Coronavirus genomes harbour single-stranded positive sense RNA (+ssRNA) of about 30 kilobases in length, which contain six or more open reading frames (ORFs) ( Cui  et al. , 2019 ;  Wu  et al. , 2020 ). The SARS-CoV-2 genome has a size of approximately 29.8 kilobases and was annotated to encode 14 ORFs and 27 proteins ( Wu  et al. , 2020 ). Two ORFs at the 5’-terminus (ORF1a, ORF1ab) encode the polyproteins pp1a and pp1b, which comprise 15 non-structural proteins (nsps), the nsps 1 to 10 and 12–16 ( Wu  et al. , 2020 ). Additionally, SARS-CoV-2 encodes four structural proteins (S, E, M, N) and eight accessory proteins (3a, 3b, p6, 7a, 7b, 8b, 9b, orf14) ( Wu  et al. , 2020 ). This set-up resembles that of SARS-CoV. The 8a protein in SARS-CoV is absent in SARS-CoV-2. 8 b is longer in SARS-CoV-2 (121 amino acids) than in SARS-CoV (84 amino acids), while 3 b is shorter in SARS-CoV-2 (22 amino acids) than in SARS-CoV (154 amino acids) ( Wu  et al. , 2020 ). To identify genomic differences between SARS-CoV-2 and SARS-CoV that may affect the structure and function of the encoded virus proteins, we identified differentially conserved amino acid positions (DCPs) ( Rausell et al. , 2010 ) and determined their potential impact by in silicomodelling ( Martell  et al. , 2019 ;  Pappalardo et al. , 2016 ). In the reference sequences of the 22 SARS-CoV-2 virus proteins that could be compared with SARS-CoV, 1393 positions encoded different amino acids. 891 (64%, 9% of all SARS-CoV-2 genome residues) of these positions were DCPs ( Supplementary Table S2 ). Most of the amino acid substitutions at DCPs appear to be fairly conservative as demonstrated by the average BLOSUM substitution score of 0.32 (median 0; Supplementary Fig. S1) and with 69% of them having a score of 0 or greater (the higher the score the more frequently such amino acid substitutions are observed naturally in evolution). 46% of DCPs represent conservative changes where amino acid properties are retained (e.g. change between two hydrophobic amino acids), 18% represented polar—hydrophobic substitutions, and &lt;10% were changes between charged amino acids ( Supplementary Table S3 ). Six of the SARS-CoV-2 proteins have a higher proportion of DCPs, S, 3a, p6, nsp2, nsp3 (papain-like protease), and nsp4 with 14.82%, 11.68%, 9.52%, 21.38%, 17.9% and 10.8% of their residues being DCPs, respectively ( Supplementary Table S4 ). Very few DCPs were observed in the envelope (E) protein and most of remaining non-structural proteins encoded by ORF1ab. For example, no residues in the helicase and &lt;4% of residues in the RNA-directed RNA polymerase, 2’-O-Methyltransferase, nsp8 and nsp9 are DCPs ( Supplementary Table S1 ). We were able to map 572 DCPs onto protein structures (Supplementary Fig. S2,  Supplementary Table S5  and S6). Nearly all of the mapped DCPs occur on the protein surface (86%), with only 34 DCPs buried within the protein, primarily in S and the papain-like protease (nsp3) ( Supplementary Table S3 ). We propose that 49 DCPs are likely to result in structural/functional differences between SARS-CoV and SARS-CoV-2 proteins. A further 259 could result in some change. The remaining 264 DCPs seem unlikely to have a substantial functional impact ( Supplementary Table S3 ). 3.2 Differentially conserved positions (DCPs) in interferon antagonists At least 10 SARS-CoV proteins have roles in interferon antagonism ( Totura and Baric, 2012 ). Two of these proteins, p6 and the papain-like protease (nsp3), contain many DCPs, two have very few DCPs (nsp7 and nsp16), five have intermediate numbers of DCPs (nsp14, nsp1, nsp15, N and M), while p3b is not encoded by SARS-CoV-2. Initial studies have identified a difference in the interferon inhibition between SARS-CoV and SARS-CoV-2 ( Lokugamage et al. , 2020 ). Thus, it is possible that especially the DCPs in p6 and the papain-like protease may have an effect on interferon inhibition. 3.3 Differences in cell tropism between SARS-CoV-2 and SARS Next, we elucidated whether the substantial number of DCPs results in different phenotypes in cell culture, using the cell lines Caco2, CL14 (susceptible to SARS-CoV infection), HT-29 and DLD-1 (non-susceptible) ( Cinatl et al. , 2004 ). Analogously to SARS-CoV infection, SARS-CoV-2 replication was detected in Caco2 and CL14 cells, but not in HT-29 or DLD-1 cells, as shown by cytopathogenic effects (CPE) ( Fig. 1A ),staining for double-stranded RNA (Supplementary Fig. S3A) and viral genomic RNA levels (Supplementary Fig. S3B). Fig 1 SARS-CoV-2 and SARS-CoV replication in cell culture. ( A ) Cytopathogenic effect (CPE) formation 48 h post-infection in MOI 0.01-infected Caco2, CL14, DLD-1 and HT29 cells. Representative images showing immunostaining for double-stranded RNA (indicates virus replication) and quantification of virus genomes by qPCR are presented in  Supplementary Figure S3 . ( B ) CPE formation in SARS-CoV and SARS-CoV-2 (MOI 0.01)-infected ACE2-negative 293 cells and 293 cells stably expressing ACE2 cells (293/ACE2) 48 h post-infection. Immunostaining for double-stranded RNA and quantification of virus genomes by qPCR is shown in  Supplementary Figure S4 . ( C ) Western blots indicating cellular ACE2 and TMPRSS2 protein levels in uninfected cells. Uncropped blots are provided in  Supplementary Figure S5 . ( D ) A sequence view of the DCPs in the vicinity of the S two cleavage sites and an image of the R815 cleavage site and closely located DCPs. S is cleaved and activated by TMPRSS2. ( E ) Concentration-dependent effects of the TMPRSS2 inhibitors camostat and nafamostat on SARS-CoV-2- and SARS-CoV-induced cytopathogenic effect (CPE) formation determined 48 h post-infection in Caco2 infected at an MOI of 0.01 using a phase contrast microscope. Similar effects were observed in CL14 cells (Supplementary Fig.S6). Values are presented as means ± S.D. (n = 3) However, ACE2-expressing 293 cells differed in their susceptibility to SARS-CoV-2 and SARS-CoV ( Fig. 1B , Supplementary Fig. S4). ACE2 has been identified as a cellular receptor for both SARS-CoV-2 and SARS-CoV( Cui  et al. , 2019 ;  Hoffmann  et al. , 2020a ;  Walls  et al. , 2020 ;  Wan  et al. , 2020 ;  Wrapp et al. , 2020 ;  Wu  et al. , 2020 ;  Yan  et al. , 2020 ). Unmodified 293 cells are not susceptible to SARS-CoV infection due to a lack of ACE2 expression. However, 293 cells that stably express ACE2 (293/ACE2) support SARS-CoV infection (Kamitani et al. , 2006). As expected, infection of 293 cells with SARS-CoV or SARS-CoV-2 did not result in detectable cytopathogenic effect (CPE) ( Fig. 1B ), but a SARS-CoV-induced CPE was detected in 293/ACE2 cells ( Fig. 1B ). In contrast, 293/ACE2 cells displayed limited permissiveness to SARS-CoV-2 infection ( Fig. 1B ). Staining for double-stranded RNA (Supplementary Fig.S4A) and detection of viral genomic RNA copies (Supplementary Fig.S4B) confirmed these findings. Hence, the ACE2 status does not reliably predict cell sensitivity to SARS-CoV-2. Indeed, CL-14 was characterized by lower ACE2 levels than DLD-1 and HT29 ( Fig. 1C ). SARS-CoV-2 and SARS-CoV cell entry depends on S cleavage by transmembrane serine protease 2 (TMPRSS2) ( Hoffmann  et al. , 2020a , b ;  Zhou et al., 2015 ). However, the non-SARS-CoV-2 susceptible and susceptible cell lines displayed similar TMPRSS2 levels ( Fig. 1C ). Thus, cellular TMPRSS2 levels do also not reliable predict cell susceptibility to SARS-CoV-2. 3.4 Differences between SARS-CoV-2 and SARS-CoV S (Spike) protein cleavage sites and sensitivity to protease inhibitors R667 and R797 are the critical cleavage sites in SARS-CoV S that are recognized by TMPRSS2 ( Simmons  et al. , 2013 ;  Zhou  et al. , 2015 ). These cleavage sites are conserved in SARS-CoV-2 (R685 and R815) ( Fig. 1D ). However, there is a four amino acid insertion in SARS-CoV-2 S prior to R685 and many of the residues close to R685 are DCPs (V663 = Q677, S664 = T678, T669 = V687, Q671 = S689, K672 = Q690 DCPs are represented by the SARS-CoV residue followed by the SARS-CoV-2 residue) ( Fig. 1D ). The R815 cleavage site has two DCPs in close proximity (L792 = S810, T795 = S813) ( Fig. 1D ). Around the R685 cleavage site two DCPs retain polar side chains (S664 = T678, Q671 = S689), while the others represent larger changes between hydrophobic and polar side chains (V663 = Q677, T669 = V687) and one changes from a positive charge to a polar side chain (K672 = Q690). While around the R815 cleavage site, one substitution is conservative (T795 = S813) and the other is a hydrophobic to polar change (L792 = S810). These changes are likely to impact on TMPRSS2-mediated S cleavage. Indeed, SARS-CoV-2 was more sensitive than SARS-CoV to inhibition by the serine protease inhibitors camostat and nafamostat ( Fig. 1E , Supplementary Fig. S6), which are known to inhibit TMPRSS2-mediated S cleavage and virus entry ( Hoffmann  et al. , 2020a , b ;  Zhou  et al. , 2015 ). This confirms that the observed differences in the amino acid sequence of S have functional consequences. 3.5 Differences between SARS-CoV-2 and SARS-CoV S interaction with ACE2 Our computational analysis detected further interesting changes in the S protein. SARS-CoV-2 S is 77.46% sequence identical to the SARS-CoV S and many of the remaining positions are DCPs (186 residues) ( Supplementary Table S1 ). The SARS-CoV S receptor binding domain (residues 306-527, equivalent to 328-550 in SARS-CoV-2) is enriched in DCPs, containing 43 DCPs (19% of residues). Nine of the 24 SARS-CoV S residues in direct contact with ACE2 were DCPs ( Fig. 2A ,  Supplementary Table S4 ). Five of these DCPs represent conservative substitutions in amino acid (hydrophobic—hydrophobic or polar-polar), two hydrophobic -polar substitutions, one positive charge to polar change, while the ninth is substitution between a hydrophobic and positively charged amino acid ( Supplementary Table S5 ). Fig 2 SARS-CoV-2 and SARS-CoV S interaction with ACE2. ( A–D )Differentially conserved positions in the Spike protein. (A) A sequence view of the DCPs present in the Spike protein, with an inset showing the receptor binding domain. (B) The S interface with ACE2 (cyan). The ACE2 interface is shown in blue spheres, DCPs in red. (C) The V404 = K417 DCP. (D) The R426 = N439 DCP, the left image shows SARS-CoV S R426, the image on the right show the equivalent N439 in SARS-CoV-2 S. ( E ) SARS-CoV residues associated with altering ACE2 affinity and the residues at these positions in SARS-CoV-2 S. ( F ) Cytopathogenic effect (CPE) formation in SARS-CoV-2 and SARS-CoV (MOI 0.01)-infected Caco2 cells in the presence of antibodies directed against ACE2 or DPP4 (MERS-CoV receptor) 48 h post-infection Analysis of the DCPs using the SARS-CoV and SARS-CoV-2 S protein complexes with ACE2 ( Song  et al. , 2018 ;  Yan  et al. , 2020 ) identified runs of DCPs (A430-T433, F460-A471) in surface loops forming part of the S-ACE2 interface and resulted in different conformations in SARS-CoV-2 S compared to SARS-CoV S ( Figure 2A, 2B ). Two DCPs remove intramolecular hydrogen bonding within the spike protein in SARS-CoV-2 ( Supplementary Table S4 ) and three DCPs (R426 = N439, N479 = QQ493, Y484 = Q498) are residues that form hydrogen bonds with ACE2. For two of these positions, hydrogen bonding with ACE2 is present with both S proteins, but for R426 = N439 hydrogen bonding with ACE2 is only observed with SARS-CoV S. N439 in SARS-CoV-2 S is not present in the interface and the sidechain points away from the interface. Further, analysis of the SARS-CoV-2 S-ACE2 complex highlighted important roles of the V404 = K417 DCP, where K417 in SARS-CoV-2 S is able to form a salt bridge with ACE2 D30 ( Figure 2C, 2D ) ( Yan  et al. , 2020 ). Alanine scanning ( Chakraborti et al. , 2005 ) and adaptation experiments ( Wan  et al. , 2020 ) have identified 16 SARS-CoV S residues impacting on the binding affinity with ACE2. For all five residues identified from adaptation studies and four of the 11 identified by alanine scanning experiments, different amino acids are present in SARS-CoV-2 S ( Fig. 2E ), highlighting the difference in the interaction with ACE2. In agreement with our structural analysis, we detected differences in the effects of an anti-ACE2 antibody on SARS-CoV-2 and SARS-CoV infection. Antibodies directed against ACE2 were previously shown to inhibit SARS-CoV replication ( Li  et al. , 2003 ). In line with this, an anti-ACE2 antibody inhibited SARS-CoV infection in Caco2 cells ( Fig. 2F ). In contrast, the anti-ACE2 antibody displayed limited activity against SARS-CoV-2 infection ( Fig. 2F ). This shows that it is more difficult to antagonize SARS-CoV-2 infection with anti-ACE2 antibodies and supports previous findings indicating a stronger binding affinity of SARS-CoV-2 S to ACE2 compared to SARS-CoV S ( Walls  et al. , 2020 ;  Wrapp et al. , 2020 ). As anticipated, antibodies directed against DPP4, the MERS-CoV receptor ( Cui  et al. , 2019 ;  de Wit  et al. , 2016 ), did not interfere with SARS-CoV or SARS-CoV-2 infection ( Fig.2F ). 4 Discussion Here, we performed an in-silico analysis of the effects of differentially conserved amino acid positions (DCPs) between SARS-CoV-2 and SARS-CoV proteins on virus protein structure and function in combination with a comparison of wild-type SARS-CoV-2 and SARS-CoV in cell culture. We identified 891 DCPs, which represents 64% of the amino acid positions that differ between SARS-CoV-2 and SARS-CoV and nearly 9% of all residues encoded by the SARS-CoV genome. 49 of these DCPs are likely to have a structural and functional impact. The DCPs are not equally distributed between the proteins. DCPs are enriched in S, 3a, p6, nsp2, papain-like protease and nsp4, but very few DCPs are present in the envelope (E) protein and most of the remaining non-structural proteins encoded by ORF1ab. This indicates that the individual proteins differ in their tolerance to sequence changes and/or their exposure to selection pressure exerted by the host environment. The large proportion of DCPs reflects the differences in the clinical behaviour of SARS-CoV-2 and SARS-CoV. Mortality associated with SARS-CoV is higher than that associated with SARS-CoV-2 ( Borges do Nascimento, 2020 ; Cui  et al. , 2019). SARS-CoV causes a disease of the lower respiratory tract. Infected individuals are only contagious when they experience symptoms ( de Wit  et al. , 2016 ). SARS-CoV-2 is present in the upper respiratory tract and can be readily transmitted prior to the onset of symptoms. Mild but infectious cases may substantially contribute to its spread ( Rivett et al. , 2020 ). The large proportion of DCPs reflects the differences in the clinical behaviour of SARS-CoV-2 and SARS-CoV. Mortality associated with SARS-CoV is higher than that associated with SARS-CoV-2 ( Borges do Nascimento, 2020 ; Cui  et al. , 2019). SARS-CoV causes a disease of the lower respiratory tract. Infected individuals are only contagious when they experience symptoms ( de Wit  et al. , 2016 ). SARS-CoV-2 is present in the upper respiratory tract and can be readily transmitted prior to the onset of symptoms. Mild but infectious cases may substantially contribute to its spread ( Rivett et al. , 2020 ). Although further research will be required to elucidate in detail, which DCPs are responsible for which differences in virus behaviour, our analysis has already provided important clues. Both viruses use ACE2 as a receptor and are activated by the transmembrane serine protease TMPRSS2 (Cui  et al. , 2019;  Hoffmann  et al. , 2020a ;  Li  et al. , 2003 ; Walls et al. , 2020;  Wan  et al. , 2020 ;  Wrapp et al. , 2020 ;  Yan  et al. , 2020 ). Our results show, however, that the ACE2 and the TMPRSS2 status are not sufficient to predict cells susceptibility to SARS-CoV-2 or SARS-CoV. The cell line CL14 supported SARS-CoV-2 replication, although it displayed lower ACE2 levels and similar TMPRSS2 levels to non-susceptible DLD-1 and HT29 cells. Thus, attempts to identify SARS-CoV-2 target cells based on the ACE2 status ( Luan  et al. , 2020 ;  Qiu et al. , 2020 ;  Xuet al., 2020 ) need to be considered with caution. As previously described (Kamitani et al. , 2006), ACE2 expression rendered SARS-CoV non-permissive 293 cells susceptible to SARS-CoV. However, ACE2 expression had a substantially lower impact on SARS-CoV-2 infection. This suggests the presence of further host cell factors that determine SARS-CoV-2 susceptibility. Based on our sequence analysis, DCPs in the viral interferon antagonists may contribute to the differences observed in the cellular tropism of SARS-CoV-2 and SARS-CoV. Our computational analysis detected DCPs in the ACE2-binding domain of S, which are likely to impact S-ACE2 binding. In agreement, an anti-ACE2 antibody displayed higher efficacy against SARS-CoV than against SARS-CoV-2, illustrating the differences between SARS-CoV-2 S and SARS-CoV S interaction with ACE2. This probably reflects an increased SARS-CoV-2 S affinity to ACE2 compared to SARS-CoV S ( Wrapp et al. , 2020 ), which may be more difficult to antagonize. To mediate virus entry, S needs to be cleaved by host cell proteases, in particular by TMPRSS2 ( Hoffmann  et al. , 2020a , b ;  Zhou  et al. , 2015 ). The S cleavage sites are conserved between SARS-CoV-2 and SARS-CoV. However, we found DCPs in close vicinity to the S cleavage sites, which are likely to affect S cleavage by host cell enzymes and/or the activity of protease inhibitors on S cleavage. Indeed, the serine protease inhibitors camostat and nafamostat, which interfere with S cleavage ( Hoffmann  et al. , 2020a , b ), displayed increased activity against SARS-CoV-2 infection than against SARS-CoV infection, confirming the functional relevance of the DCPs. In conclusion, our in-silico study revealed a substantial number of differentially conserved amino acid positions in the SARS-CoV-2 and SARS-CoV proteins. In agreement, cell culture experiments indicated differences in the cell tropism of these two viruses and showed that cellular ACE2 and TMPRSS2 levels do not reliably indicate cell susceptibility to SARS-CoV-2. Moreover, we identified DCPs in S that are associated with differences in the interaction with ACE2 and increased SARS-CoV-2 sensitivity to the protease inhibitors camostat and nafamostat relative to SARS-CoV. Supplementary Material btab094_Supplementary_Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>The Cardiac Atlas Project—an imaging database for computational modeling and statistical atlases of the heart</Title>
    <Doi>10.1093/bioinformatics/btr360</Doi>
    <Authors>Fonseca Carissa G., Backhaus Michael, Bluemke David A., Britten Randall D., Chung Jae Do, Cowan Brett R., Dinov Ivo D., Finn J. Paul, Hunter Peter J., Kadish Alan H., Lee Daniel C., Lima Joao A. C., Medrano−Gracia Pau, Shivkumar Kalyanam, Suinesiaputra Avan, Tao Wenchao, Young Alistair A.</Authors>
    <Abstract>Motivation: Integrative mathematical and statistical models of cardiac anatomy and physiology can play a vital role in understanding cardiac disease phenotype and planning therapeutic strategies. However, the accuracy and predictive power of such models is dependent upon the breadth and depth of noninvasive imaging datasets. The Cardiac Atlas Project (CAP) has established a large-scale database of cardiac imaging examinations and associated clinical data in order to develop a shareable, web-accessible, structural and functional atlas of the normal and pathological heart for clinical, research and educational purposes. A goal of CAP is to facilitate collaborative statistical analysis of regional heart shape and wall motion and characterize cardiac function among and within population groups.</Abstract>
    <Body>1 INTRODUCTION In both the clinical and research settings, a variety of techniques can be used to quantify cardiac performance at various structural and functional levels. Electrocardiography allows monitoring of the electrical activity of the heart to diagnose pathology such as rhythm disturbances, myocardial infarction and hypertrophy. Blood pressure measurements enable detection of hypertension. Cardiovascular imaging studies provide quantification of cardiac mass and volumes, as well as assessment of regional heart wall motion. The ability to integrate these multi-source, multivariate data has enormous implications for the diagnosis and clinical care of patients ( Kohane, 2009 ). Mathematical and computational models can be used to integrate data in a standardized way, providing both a ‘big picture’ population map of the various factors that determine cardiac function as well as highly detailed information which can be used to characterize function in an individual patient (patient specific models). These models can elucidate the complex interaction of electrical, anatomical and functional data to provide insight into the processes underlying the normal or pathological function of the heart. Furthermore, models derived from large populations of patients can provide a range of reference values against which individual patients' data can be compared. A number of biomedical initiatives use computational modeling to integrate multi-scale anatomical, functional and clinical data from diverse sources. These include the Physiome Project ( Hunter and Borg, 2003 ), the International Consortium for Brain Mapping ( Mazziotta  et al. , 2001 ), Informatics for Integrating Biology and the Bedside (i2b2) ( Murphy  et al. , 2006 ) and the Cardiac Gene Expression database ( Bober  et al. , 2002 ) among others. These projects depend on large population databases for the development and validation of physiological models. The Cardiac Atlas Project (CAP) is an international collaboration to establish a large-scale standardized database of cardiac imaging examinations and derived functional analyses. The aim is to develop a computational, structural and functional atlas of the normal and pathological heart. These atlases can be defined as a set of maps which relate scientific information to spatial coordinates at a series of scales, from genotype to phenotype ( Thompson  et al. , 2000 ;  Toga  et al. , 1996 ). The research objectives of CAP are to:
 Establish a database of cardiac imaging examinations consisting of de-identified image files together with associated clinical data. Develop open source software for the analysis of cardiac morphology including (i) visualization of images in 3D over time, and (ii) interactive construction of mathematical cardiac models from the images. Develop standardized protocols for the contribution, curation, archival, classification and sharing of cardiac image data and derived analyses, including labeling of images and models in the CAP database with ontological terms. 
Here, we describe the design of the computational and informatics infrastructure together with procedures for contribution of data to the CAP, de-identification, standardization and sharing of data and software tools, and policies for the protection of the rights of participants, contributors and users of the database. The article is organized as follows:  Section 2  outlines the requirements, design and implementation of CAP components, accompanied by an overview of the issues underlying regulatory requirements and stake-holder rights.  Section 3  gives details of the major contributing studies, the results of the validation procedures performed, current database and client functionality, XML schema and policies regarding access to the database. 2 METHODS 2.1 Stake-holders The CAP is an international collaboration funded by the National Institutes of Health, USA. The host institutions are the University of Auckland (New Zealand) and the University of California Los Angeles (USA) with databases maintained and mirrored in both institutions. Where possible, infrastructure for data sharing has been adapted and re-used from the UCLA Center for Computational Biology and Laboratory of Neurological Imaging (LONI), which has considerable experience in the area of computational brain atlases ( Mazziotta  et al. , 2001 ). Stake-holders in CAP include the following parties:
 Participants: people who have participated in a research study contributing data to CAP, or have otherwise provided informed consent to contribute data to CAP. Contributors: research study investigators who originally acquired the data and shared it by contributing it to CAP. Decisions regarding data use are typically made by the contributing study steering committee. Users: third party researchers who access CAP data in order to undertake research into cardiovascular function and disease. 
Cardiac imaging examinations and associated clinical data are contributed from a number of sources, including research studies, clinical trials and clinical centers. 2.2 Imaging and clinical data Cardiovascular imaging provides an abundant source of detailed, quantitative data on heart structure and function. Common investigations include ultrasound, computed tomography, radionuclide imaging and MRI. Many research studies have employed MRI because it is noninvasive, well tolerated and safe (no ionizing radiation), has the ability to modulate contrast, and can provide high-quality functional information in any plane and direction ( Fig. 1 ).
 Fig. 1. Cine MRI short- (top) and long-axis (bottom) images, at end-diastole (end of ventricular filling, left), and end-systole (end of ejection, right). Contours show inner (green) and outer (blue) boundaries of the left ventricle, and the position of the mitral valve (red). The tomographic nature of MRI data lends itself to 3D atlas building techniques and to date, all CAP imaging data has come from MRI. These studies typically consist of 6–12 cine acquisitions in the short axis orientation, with 20–50 frames through the cardiac cycle and 1–2 mm pixel resolution. Imaging protocols include gradient recalled echo (GRE) ( Boxerman  et al. , 1998 ) and steady state free precession (SSFP) ( Thiele  et al. , 2001 ) techniques. Studies have also contributed core laboratory analyses of the image data, in the form of annotations and contouring ( Fig. 1 ) of the left ventricular boundaries at end-diastole (end of filling) and end-systole (end of ejection), and de-identified text data containing the clinical status and demographics of the participants. 2.3 Regulatory compliance Since CAP is an international, multi-institutional collaborative project, it must comply with a variety of legislative and local Institutional Review Board (IRB) requirements. Local Ethics Committee and IRB approvals were obtained for CAP at the two host institutions. In addition, CAP policy requires that all data must be obtained and contributed with the approval of a local IRB or Ethics Committee, and informed consent for data sharing must be obtained from each participant. The Health Insurance Portability and Accountability Act (HIPAA) Privacy and Security Rules (45 CFR Parts 160, 162 and 164, available at  http://www.hhs.gov/ocr/privacy/hipaa/administrative/privacyrule/index.html ) regulate the use and disclosure of research participant's protected health information (PHI) in the USA. PHI are any data that could be used to identify an individual, e.g. names, dates (except for year), social security or medical record numbers, locations or other unique identifiers. To protect the identity of participants, PHI must be replaced or removed before data can be shared, a process known as de-identification. Use of de-identified data is considered by many IRBs not to constitute human subjects research. 2.4 Data de-identification In CAP, all data must be de-identified by the contributors before upload into the database. The de-identification process removes PHI from both text and image data, and replaces subject identifiers such as the name of the individual, or the original study code, with an unrelated CAP code. Medical images contributed to CAP are stored as DICOM Objects ( DICOM-PS3.5, 2009 ). These objects contain data attributes including information on the scanner, the imaging protocol and the scanned object. The current DICOM standard (v2009) contains more than 2800 public DICOM attributes. In addition, many DICOM and PACS manufacturers include proprietary information within private DICOM attributes. We adapted the LONI Debabeler ( Neu  et al. , 2005 ), a HIPAA compliant software tool, for the de-identification of DICOM images. We created a CAP-specific Debabeler with rules to encrypt or replace DICOM attributes that could potentially contain PHI, while retaining essential information on the image acquisition. The output of the CAP Debabeler includes the key linking CAP codes to the original identifiers, which is then kept by the contributor. CAP personnel, and third party CAP users, do not have access to this key. The Debabeler rules are available from the CAP project site at SourceForge.net ( http://sourceforge.net/projects/cardiacatlas/ ). 2.5 Database design Software was designed to enable storage and retrieval of medical image and text data, ontological annotations and volumetric shape models ( Fig. 2 ). Browsing, searching, image preview and download functionality are included.
 Fig. 2. CAP workflow. Step 1: data ACQUISITION; Step 2: data processing; Step 3: data analysis and Step 4: public data access. The database was developed using a three-tier architecture (web-, application- and database server) including monitoring and secure authentication with access privileges based on user need. Due to its mature architecture and code base, active development, maintenance and support, DICOM compliance, compatibility with other Java APIs and other international research projects such as the Cancer Biomedical Informatics Grid ( Cimino  et al. , 2009 ) and the Cardiovascular Research Grid (CVRG;  http://www.cvrgrid.org ), we based the database on the open source image archive Dcm4chee, and extended its functionality. Dcm4chee is a clinical data manager system based on a J2EE ( Alur  et al. , 2003 ) and JMX ( Fleury and Lindfors, 2002 ) software architecture and is deployed within the JBoss Application Server. It provides a number of useful clinical interfaces, including:
 Ability to store, query, and retrieve any type of DICOM object; WADO (Web Access to DICOM Objects) and RID (IHE Retrieve Information for Display) interfaces to allow access from the web; a robust user interface which runs entirely in a web browser; and Audit Record Repository—IHE ATNA audit logging ( Gregg  et al. , 2006 ). The Dcm4chee application logic, database schema and web-application were extended to provide access to MRI specific attributes as defined in the DICOM Standard MR Image Module ( DICOM-PS3.3, 2009 ). These included vendor and model of the scanner used to acquire the images. The database fields are populated at image import using extended methods from the Dcm4chee Enterprise Java Beans (EJBs). The web-application extension allows all added attributes to be used as search options. A researcher might be interested in specific studies, cine series or individual images that satisfy specific search parameters. For this purpose, a search filter was added to generate result listings grouped by Patient, Study, Series or Image. To allow searching for arbitrary DICOM attributes, an XPath query (details of XPath are described at  http://www.w3.org/TR/xpath20/ ) was implemented. An XML tree ( Bray  et al. , 2006 ) representing the DICOM structure of the imported images is generated, stripped of binary and large values, and stored in the database. The download functionality of the Dcm4chee web-application was extended to allow the download of complete DICOM studies, series and volumetric models including referenced data. This is achieved by implementing a servlet that collects the requested data from the server and provides it as a compressed archive to the user ( Smart  et al. , 2005 ). 2.6 Parametric modeling of cardiac function Atlas-based methods are well established for the statistical classification and quantification of shape and wall motion characteristics of the heart ( Young and Frangi, 2009 ). These methods enable standardized analysis of statistical variations present within and among patient groups, and enable classification of individual phenotypes within known population distributions. In almost all cases contributed to the CAP, contours were contributed in association with the images and clinical information. These contours can be used as the input to a standardized model-based analysis to establish shape and motion with respect to a standard coordinate system, similar to the Talairach coordinate system used in the brain ( Mega 2005 ;  Tang, Hojatkashani  et al.  2010 ). Since shape and motion are mathematically mapped, statistical tools such as principal component analysis can be used to quantify the significant modes of variation present within a population. In CAP, the parametric shape descriptors lend themselves to finite element modeling, which can then enable biophysical simulation of physiological processes, including nonlinear mechanical properties and large deformations of the heart, and solve the biophysical conservation laws linking stress, strain and energy expenditure. By customizing mathematical models of the anatomy and function of the heart to individual cases, it is possible to construct parameter variation models describing the distribution of regional cardiac shape and function across patient subgroups. Homologous landmarks (i.e. the points that are aligned to match corresponding features in the shape) can be used to characterize shape and shape variations with the aid of a principal component analysis, or similar technique. Since mathematical models, represented by the model parameters, are a complete and efficient characterization of cardiac shape and motion, a statistical analysis of the variation inherent in the parametric shape and motion models can be formed ( Young and Frangi, 2009 ). 2.7 CAP client design An open-source client side software tool was developed for the visualization and analysis of the cardiac MRI images available in the database. The software allows visualization of the data in 3D + time and interactive fitting of a finite-element volumetric model to any given dataset. The CAP client has been designed with the following objectives in mind.
 Performance: to allow the user to view and manipulate large image datasets in 3D and fit models to them in an interactive manner. In order to meet the real-time 3D graphics and numerical computation requirements, the C++ programming language was chosen for its superior time efficiency and the availability of high-performance numerical libraries. The CAP client uses hardware-accelerated OpenGL API for graphics rendering and the GMM++ linear algebra library (available at  http://download.gna.org/getfem/html/homepage/gmm/index.html ) for model fitting. Ease of use: the CAP client is expected to be used by nontechnical users for educational and research purposes, therefore ease of use was an important design goal. All features of the software are accessible through an intuitive graphical user interface. Extensibility and maintainability: in order to encourage external developers to extend the CAP client to suit their needs, the source code is structured to accommodate such extensions. Various object-oriented techniques were adopted to increase the extensibility of the software. For example, the abstract factory design pattern and the adaptor design pattern ( Gamma  et al. , 1995 ) were used to ease the possible replacement of the linear algebra library. Portability: the CAP client was designed to be portable across different platforms and currently runs on Microsoft Windows, Mac OS X and Linux. This portability is achieved using cross-platform libraries such as wxWidgets ( Smart  et al. , 2005 ), boost (available at  http://www.boost.org ) and GMM++, as well as build and testing tools such as CMake and Google Test. 
The CAP client was built on the open source Cmgui library (available at  http://www.cmiss.org/cmgui ), an advanced visualization software library developed at the Auckland Bioengineering Institute for visualization and manipulation of general finite element models. This visualization library was employed because it has a large suite of tools available for parametric shape modeling of the heart, and is readily customizable for CAP purposes. 2.8 Semantic data model The CAP database contains more than 1.5 million MRI images from symptomatic and asymptomatic patients. To facilitate automated fitting of volumetric models and searching for specific characteristics, CAP is labeling images using a controlled vocabulary. SNOMED-CT ( Ryan  et al. , 2007 ) and the Foundational Model of Anatomy (FMA) ( Rosse and Mejino, 2003 ) are established ontologies providing clinical and anatomical concepts. RadLex ( Kundu  et al. , 2009 ) unifies and supplements these standards and provides a single source of radiology terms. From these resources, CAP has selected cardiac and MRI terms for classification of images and annotation of anatomical landmarks. Where appropriate, we have provided feedback to the resources to improve the terminology for cardiac labeling. 2.9 Policy and rights To ensure that all data provided to CAP are managed according to well-defined principles, and in accordance with the regulatory and ethical requirements associated with de-identified human image and clinical data, policies and procedures related to data access, control and sharing have been developed. These policies apply to (i) participants from whom the data was obtained, (ii) contributors who originally collected and have contributed the data, (iii) the CAP investigators and, (iv) third-party users who wish to access CAP data. CAP contributors have made substantial monetary, intellectual and time investments for the collection of the data in a well-controlled manner (viz. original study design, recruitment, quality control, analysis, etc.), which represents a valuable scientific resource. The conditions under which data are originally acquired may vary substantially among contributors, for example ranging from public good government funded studies to privately funded trials with a commercially sensitive outcome. The terms and conditions under which data can be shared consequently vary substantially among contributors. CAP has therefore adopted a ‘bundle of rights’ approach (see  www.cardiacatlas.org  for further discussion) reflecting the goal of providing access as openly and widely as possible, consistent with contributor and participant consent. Data can be contributed as public domain provided the informed consent is compatible with open unrestricted access. It should be noted that an explicit objective of CAP is to provide a flexible mechanism by which data that would otherwise be inaccessible (for example, data generated by privately funded clinical trials or ongoing longitudinal studies) can now be accessed by researchers for a variety of diverse investigations. CAP has successfully achieved this aim by developing policies that not only protect the original data contributors (sponsors and investigators of privately funded or ongoing studies), but also allow third party investigators an avenue of access that would not be possible via other means. By negotiating and working through these policies with the original investigators, CAP has paved the way for future third-party access to the data. 3 RESULTS 3.1 Database Two main studies comprise the current CAP database. The Multi Ethnic Study of Atherosclerosis (MESA;  Bild  et al. , 2002 ) has contributed 2864 asymptomatic volunteers to date. MESA is investigating subclinical cardiovascular disease and the progression to clinically overt disease in a diverse, population-based sample of asymptomatic men and women aged 45–84 years. Participants with no history of cardiovascular disease were recruited from six field centers across the United States. Approximately 38% of the study's participants were white, 28% African-American, 22% Hispanic and 12% Asian, predominantly of Chinese descent. The Defibrillators to Reduce Risk by Magnetic Resonance Imaging Evaluation (DETERMINE) trial ( Kadish  et al. , 2009 ) has contributed 470 datasets from patients with myocardial infarction to date. New studies are being contributed on an ongoing basis. DETERMINE is a multicenter, randomized, clinical trial in patients with coronary artery disease (CAD) and mild-to-moderate LV dysfunction. The trial investigated whether patients with an infarct size greater than or equal to 10% of left ventricular mass, randomized to receive an implantable defibrillator plus appropriate medical therapy will have improved survival compared with patients randomized to medical therapy alone. In MESA, MRI data were acquired on Siemens and GE 1.5T MRI scanners only. The images included cine (using the GRE pulse sequence) in short-axis planes covering from the base of the heart to the apex and in three long-axis planes. Images were analyzed using MASS 4.0 (Medis, The Netherlands) by the MESA MRI core laboratory at Johns Hopkins University School of Medicine, and ventricular contours contributed to CAP. In the DETERMINE trial, MRI data were acquired on any of Siemens, GE and Philips 1.5T or 3.0T MRI scanners. The imaging protocol included cine images (using the SSFP pulse sequence) acquired in short-axis planes from the base of the heart to the apex and in three long-axis planes, as well as delayed enhancement viability ( Kim  et al. , 1999 ) used for detection and quantification of myocardial infarction, acquired in the same planes as the cine images. Images were analyzed using QMASS MR 7.2 (Medis, The Netherlands) by the DETERMINE MRI core laboratory at Northwestern University Feinberg School of Medicine, and contours contributed to CAP. Both studies also contributed limited clinical information including: age (years), gender (M/F), height (cm), weight (kg), systolic and diastolic blood pressure (mmHg), hypertension (Y/N), heart rate (bpm), race/ethnicity and classifications for hypertension, diabetes, smoking (Y/N), alcohol (Y/N), angina (Y/N), ECG and NYHA classification. DETERMINE included an IRB approved specific section in the participant information and consent forms to contribute de-identified data to CAP. Participants could choose to give or withhold this consent independent of their participation in DETERMINE. MESA included an IRB approved informed consent process compatible with data sharing and further IRB approval was obtained for the contribution of de-identified data to CAP. CAP has been endorsed by the Society for Cardiovascular Magnetic Resonance ( www.scmr.org ). Clinical cases with appropriate informed consent can also be de-identified and made publicly available in the database. 3.2 CAP client Given a set of cardiac MRI images from the CAP database, the CAP client software, (see  Fig. 3 ) can be used for:
 Visualization of MRI images, the mathematical model constructed from the images, and animation of the motion through time in the cardiac cycle. Customization of a finite element model of the left ventricle to the MRI images using guide point modeling ( Young, 2000 ). This process requires minimal human intervention, and results in a mathematical model of the heart shape and motion in 3D and time. The Client software also provides a means for users to interactively and graphically modify model parameters derived from other sources, such as semi-automatic ventricular analysis methods. 
Customized model parameters, along with images and contour information, are stored in an XML file format as described in  Section 3.4 .
 Fig. 3. Screenshot of the CAP Client running on Mac OS X. One short axis and one long axis MRI image are visible, togther with the inner and outer surfaces of the LV model (green and red lines, respectively). 3.3 Data analysis To demonstrate the utility of the database for the statistical characterization of heart shape and motion, the major modes of variation within a subset of the DETERMINE cohort were calculated using a principal component analysis. The first three modes associated with the greatest variation were found to correspond with size, sphericity and mitral valve geometry ( Fig. 4 ). These results are encouraging since each of these modes are known measures of adverse geometric remodeling following myocardial infarction. Projection of an individual's shape and motion onto these modes (e.g. sphericity) provides a standardized method for quantifying the amount of each mode present.
 Fig. 4. First three modes of shape variation in principal component analysis of a subset of the DETERMINE cohort ( n =200). Mode 1: size; Mode 2: sphericity; Mode 3: mitral geometry. 3.4 Semantic data model To store volumetric models ( Fig. 5 ) and  supplementary data  for images such as contours and cardiac annotations using ontological concepts, we have designed a standardized data structure using XML. Storing data in an XML format allows for simple conversions using the Extensible Stylesheet Language Transformations (XSLT; described at  http://www.w3.org/TR/xslt ) of (i) geometrical data into other languages such as FieldML ( Christie  et al. , 2009 ), and (ii) cardiac annotations into knowledge representation languages, e.g. OWL (Web Ontology Language),  http://www.w3.org/TR/owl-features/,  or RDF (Resource Description Framework),  http://www.w3.org/standards/techs/rdf . The XML files are stored using the XML database eXist ( Meier, 2003 ), which provides core database features, such as indexing and transaction recovery, enabling fast search and retrieval of model-related data. Import and export of XML data has been implemented by extending the Dcm4chee architecture (see  Fig. 6 ). The extended architecture provides a vehicle to store and retrieve image, model and derived data.
 Fig. 5. In order to store volumetric models generated with the CAP client application, an XML schema has been designed representing the elements associated with volumetric shape model creation and curation. This includes input parameters, such as images, contours and markers, calculated output parameters, mesh files representing the model geometry, and provenance information. 
 Fig. 6. Three-tier architecture of the CAP model implementation based on Dcm4chee. Blue boxes represent basic Dcm4chee classes, and yellow boxes represent CAP specific model extensions. 3.5 Policy and rights Standard operating procedures were developed to manage the logistics of data sharing, including data requests and data transfer, and to maintain the rights of the stake-holders. Participants must give informed consent compatible with data sharing to contribute their de-identified image and text data for cardiovascular research now and in the future. All data must be de-identified in a manner compatible with the HIPAA privacy rule, using the CAP de-identification process, and must occur at the Contributor's site before upload to the CAP data servers. CAP must not receive or retain the original identifiers. CAP investigators, and Users, must agree not to attempt to identify participants. The key linking CAP codes with original identifiers must be retained by the Contributor, so that investigators of the Contributing Study can link results from CAP back to the original study if desired. Participants can request withdrawal of their data from the database at any time by requesting removal either via the CAP or directly to the Contributor. In this case the Contributor must notify CAP of which CAP data must be deleted. Access to the data is unrestricted and open for those cases with informed consent compatible with unrestricted access. In many cases, however, the participant consent requires that access is approved by the contributor. The Contributor or Contributing Study Steering Committee then controls all data access through data distribution agreements, on a request basis. Users are required to submit a brief Research Proposal, outlining the rationale and goals of the project, timeline and data storage, to the CAP Steering Committee which includes CAP investigators from both of the partner institutions, The University of Auckland and the University of California Los Angeles, the NIH Program Officer for CAP, and investigator-representatives from each of the contributing studies. The purpose of the review is primarily to protect the rights of participants and contributors. If the proposal is within the remit of CAP, CAP will liaise with each of the Contributors whose data has been requested. Each Contributor (or nominee) must then review the proposal and, if approved, provide a Data Distribution (DDA) agreement. The User must then sign and abide by the DDAs for each Contributor. Separate DDA's are required for each Contributor because terms and conditions governing data use may differ depending on the circumstances under which the data were acquired. The DDA defines all the terms and conditions governing the use of the data, including publication policy, acknowledgements, data handling, and intellectual property. 3.6 CAP software license All software is freely available via the CAP website, to researchers and educators in the nonprofit sector, such as educational institutions, research institutes and government laboratories. Instructions for accessing CAP source code are available at  http://www.cardiacatlas.org/web/guest/tools . Developer access to CAP source projects is made available through a sourceforge Mecurial repository. CAP database and heart modeling tools, comprising database management, uploading and downloading of images, web browser interface, conversion of data formats, visualization and parametric modeling of shape and motion, are being made available using the Mozilla Public License Version 1.1. Dependent software such as Dcm4chee and Cmgui are compatible with this license. Commercialization of enhanced or customized versions of the software, or incorporation of the software or pieces of it into other software packages, is permitted subject to third party intellectual property claims. Researchers are permitted to modify the source code and are strongly encouraged to share modifications with other researchers as well as with CAP. 3.7 Use cases CAP data may be used for a variety of purposes. Some use cases are described below:
 Image analysis: a subset of the data is being provided for a left ventricular segmentation challenge at the Medical Image Computing and Computer Assisted Intervention (MICCAI) 2011 conference. This challenge enables researchers to test their automated segmentation algorithms on the same large cardiac MRI dataset, thereby facilitating comparative discussion as well as collaboration among peers on combining the results to find a better ground truth ( http://cilab2.upf.edu/stacom_cesc11/index.php ). Clinical evaluation: CAP data will be used to create a statistical atlas for clinical purposes. This would be used to determine whether a particular patient fits within the normal range or how many standard deviations the patient is from normal values. Pathological processes such as LV remodeling will be examined by comparison to pathology-matched statistically predicted parameters, based on an individual's known clinical characteristics and the CAP population subgroup he or she matches most closely. Clinical trials: CAP data will be used to test hypotheses comparing cohorts among studies in the database, or to perform metadata queries on several studies, utilizing mapping transforms to reduce bias due to study protocol. Education: CAP data could also play an important role in biomedical education programs. The collection of a well-curated and diverse population of cardiac data is an excellent platform from which to understand normal structure and function as well as to examine the statistical differences related to age, gender, height, weight and pathology. The value of the data is enhanced by the downloadable CAP Client software for visualization in 3D and over time. Students could use the software to fit mathematical models to data, and then use the models to better understand the effects of pathology on standard clinical measurements such as ejection fraction, volumes and wall thickening parameters. 4 FUTURE WORK In accordance with the goals of standardized classification and sharing of data and resources, CAP is developing and building upon currently available ontological schema to describe cardiac image data and derived annotations and models, with plans to federate these cardiovascular modeling software and data via the CVRG ( www.cvrgrid.org ). A SPARQL ( http://www.w3.org/TR/rdf-sparql-query/ ) interface may be built when there is a substantial amount of annotated data for this purpose, which requires semantic annotations in our XML format, conversion to RDF or OWL, and implementation of a semantic storage. The parametric modeling tools and associated ontological schema that are being developed by CAP will be expanded to facilitate data fusion between different imaging protocols and modalities as well as other data sources. Tools necessary for the statistical analysis of CAP data are also being developed and will be used for the generation of parametric distribution models. 5 CONCLUSIONS The CAP currently hosts approximately 3000 cardiac MRI studies, derived functional analyses and associated participant data that represents a substantial and valuable resource. Tools for the de-identification of data were developed, validated and successfully deployed by the contributing studies. The necessary IRB and Ethics Committee approvals were obtained and policies were developed to protect the rights of subject participants, contributors and users of the database. Applications to use the data can now be submitted to the CAP website. Upon completion of a Data Distribution Agreement, users can browse and query the database as well as view the images, and download the data. The CAP database is compliant to the DICOM standard and provides sophisticated image attribute search options. The CAP Client software, downloadable at the Project's website, allows the user to import images from the database and customize a finite element model to the image data. Volumetric shape models are stored in XML and are available to the research community via the CAP database. CAP procedures and tools are designed to facilitate a workflow from the acquisition of CMR images toward a statistical analysis of volumetric models. Funding :  National Heart, Lung and Blood Institute, USA  ( R01HL087773 ). Support from the  Center for Computational Biology (LONI)  was provided by  National Institutes of Health / National Center for Research Resources  ( U54 RR021813 ,  P41 RR013642 ). MESA was supported by the  National Heart, Lung and Blood Institute  ( N01-HC-95159  through  N01-HC-95169 ). A full list of participating MESA investigators and institutions can be found at  http://www.mesa-nhlbi.org . DETERMINE was supported by St Jude Medical, Inc; and the  National Heart, Lung and Blood Institute  ( R01HL91069 ). A list of participating DETERMINE investigators can be found at  http://www.clinicaltrials.gov . The content is solely the responsibility of the authors and does not necessarily represent the official views of the  National Heart, Lung and Blood Institute  or the  National Institutes of Health . Conflict of Interest : none declared </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Inferring gene targets of drugs and chemical compounds from gene expression profiles</Title>
    <Doi>10.1093/bioinformatics/btw148</Doi>
    <Authors>Noh Heeju, Gunawan Rudiyanto</Authors>
    <Abstract>Motivation: Finding genes which are directly perturbed or targeted by drugs is of great interest and importance in drug discovery. Several network filtering methods have been created to predict the gene targets of drugs from gene expression data based on an ordinary differential equation model of the gene regulatory network (GRN). A critical step in these methods involves inferring the GRN from the expression data, which is a very challenging problem on its own. In addition, existing network filtering methods require computationally intensive parameter tuning or expression data from experiments with known genetic perturbations or both.</Abstract>
    <Body>1 Introduction Knowing the molecular targets of a drug or chemical compound is crucial in the drug discovery research for, among other things, identifying therapeutic properties and side effects, understanding the mechanism of action of a drug, finding alternative compounds with similar or greater efficacy and exploring new applications of a drug for treatment of other diseases (drug repositioning). In this regard, advances in high-throughput omics technology have been playing a crucial role in providing the data for elucidating cellular entities which interact with drug and chemical compounds. Cellular-wide response such as whole-genome gene expression profile, to genetic perturbations and chemical compounds can now be measured easily and cheaply. Furthermore, large amount of omics data are available from the ever-growing public biological databases. Because such data are typically of high dimensionality, the use of computational methods has become necessary in their analysis, for example in the inference of gene regulatory networks ( Hurley  et al. , 2012 ). Computational systems biology has provided many tools to analyze gene expression profiles for drug target predictions. A summary of different methods in this topic can be found in a review article ( Chua and Roth, 2011 ). Briefly, there exist two main strategies: comparative analysis and network analysis. In comparative analysis, the gene targets are determined by comparing the gene expression profiles under drug treatments of interest with those from experiments with known genetic perturbations. This strategy generally involves gathering a compendium of expression profiles from genetic perturbations and chemical compound treatments with known mechanisms, followed by an association analysis of the expression profiles from drugs (e.g. using clustering, distance or connectivity score) ( Hughes  et al. , 2000 ;  Lamb  et al. , 2006 ;  Iorio  et al. , 2010 ). A strong degree of association suggests a high similarity between the molecular targets of a drug of interest and the known perturbations. In network analysis strategy, a model of the GRN is employed to predict GRN perturbations caused by drugs. The perturbation analysis requires constructing a network model of gene transcriptional regulation. One class of network analysis called network filtering is based on an ordinary differential equation (ODE) model of the gene transcription process. By taking the steady state assumption, the inference of GRN and drug targets reduces to solving a multivariate linear regression problem (see Methods and Materials for more details). Algorithms from this class of network analysis include network identification by multiple regression (NIR) ( Gardner  et al. , 2003 ), mode of action by network identification (MNI) ( di Bernardo  et al. , 2005 ) and sparse simultaneous equation model (SSEM) ( Cosgrove  et al. , 2008 ). Here, the gene target predictions are obtained by first inferring the GRN using a library of gene expression data from the same species or cell line. Subsequently, the inferred GRN is used to filter the expression data of drug treatments. More precisely, genes with expressions which could not be explained by the transcriptional activity of their regulators are scored more likely to be direct targets of the drug. Another type of network analysis methods rely on statistical test or enrichment analysis of the gene expression profiles to identify drug targets. One strategy called reverse causal reasoning uses literature-mined gene regulatory networks to generate hypotheses, which are subsequently scored against the gene expression profile ( Belcastro  et al. , 2013 ;  Chindelevitch  et al. , 2012 ;  Martin  et al. , 2012 ). Another set of methods employ a transcription factor (TF) enrichment analysis followed by an upstream analysis, which involves a search for proteins that are highly connected to enriched TFs in signal transduction or protein-protein interaction networks ( Chen  et al. , 2012 ;  Koschmann  et al. , 2015 ;  Lachmann and Ma’ayan, 2009 ;  Laenen  et al. , 2015 ). Meanwhile, a method called Master Regulatory Inference algorithm (MARINa) applies gene set enrichment analysis using a transcriptional regulatory network to identify TFs whose regulons are enriched for differentially expressed genes ( Lefebvre  et al. , 2010 ). Finally, a recent algorithm named Detecting Mechanism of Action by Network Dysregulation (DeMAND) uses an input GRN and expression data from control and drug treatments to identify target genes based on dysregulated gene interactions ( Woo  et al. , 2015 ). Despite the differences in how GRNs are used in network analysis methods, the accuracy of the target predictions naturally depend on the fidelity of the GRN model. Unfortunately, the inference of GRN is known to be very challenging as the problem has been shown to be underdetermined ( Szederkényi  et al. , 2011 ;  Ud-Dean and Gunawan, 2014 ). In this work, we developed a network analysis method called DeltaNet for predicting the genetic perturbations caused by a drug or chemical compound using gene expression profiles. DeltaNet is also based on an ODE model of the GRN, but does not require a separate step of GRN inference. Instead, the target predictions are obtained directly from the data, while the GRN is only inferred implicitly. DeltaNet relies on the least angle regression (LAR) ( Efron  et al. , 2004 ) and the LASSO regularization ( Tibshirani, 1996 ) to tackle the curse of dimensionality of the underlying regression problem. We demonstrated the advantages of DeltaNet over  z -scores and other network filtering methods, namely MNI and SSEM, using compendia of gene expression data from  Escherichia coli ,  Saccharomyces cerevisiae ,  Drosophila melanogaster  and  Homo sapiens . 2 Methods and materials 2.1 DeltaNet formulation DeltaNet is based on the following ODE model of gene transcriptional process ( Liao  et al. , 2003 ):
 (1) d r k d t = u k ∏ j = 1 n r j a k j − d k r k 
where  r k  denotes the mRNA concentration of gene  k ,  u k  and  d k  denote the mRNA transcription and degradation rate constants of gene  k , respectively,  a k j  denotes the regulatory control of gene  j  on gene  k , and  n  denotes the total number of genes. The sign of  a k j  describes the nature of the regulatory control, where a positive (negative) value represents activation (inhibition). Meanwhile, the magnitude of  a k j  corresponds to the strength of the regulation. We assume that  a k k = 0 , i.e. there exists no direct self-regulatory loop. While this assumption may appear limiting, the case studies showed that DeltaNet could accurately predict network perturbations across different species. Under the steady state assumption, the concentration change of mRNA over time  d r k / d t  can be set to 0, which simplifies the model above into
 (2) r k = u k d k ∏ j = 1 n r j a k j = g k ∏ j = 1 n r j a k j 
where  g k = u k / d k  is the ratio between mRNA transcriptional and degradation rate constants. Gene expression data of a treatment are typically reported as ratios with respect to readings from the corresponding control experiments. One can rewrite the model above for gene expression ratios (dividing both sides of  Eq. (2)  by the mRNA level in the control experiment), as follows:
 (3) r k i r k b i = ( g k i g k b i ) ∏ j = 1 n ( r j i r j b i ) a k j 
where  r k i  and  r k b i  denote the mRNA levels of gene  k  in treatment sample  i  and in the corresponding control experiment  b i , respectively. The model formulation in  Eq. (3)  relies on the implicit assumption that the drug treatment affects only mRNA transcriptional and/or degradation rate constants without causing any changes in the GRN. Therefore, some care should be taken when applying DeltaNet and related methods such as MNI and SSEM to any treatments that may rewire the GRN. Taking the logarithm of both sides of  Eq. (3)  leads to the following linear expression:
 (4) c k i = ∑ j = 1 n a k j c j i + p k i 
where  c k i = log ( r k i / r k b i )  denotes the log-fold change (logFC) of mRNA level of gene  k  and  p k i = log ( g k i / g k b i )  denotes the effects of treatment in sample  i . Typically, a base-2 logarithm is employed in the analysis of gene expression data ( Tarca  et al. , 2006 ). According to  Eq. (4) , the logFC of gene transcript  k  in a given sample comes from a contribution of two factors: (i) changes in the expression of genes that regulate gene  k  and (ii) a direct perturbation on the effective transcription (i.e. the ratio between transcription and degradation) of gene  k  by the treatment. A positive (negative) perturbation variable  p k i  indicates that the effective transcription of gene  k  is increased (decreased) by the treatment. Several network filtering methods have been formulated based on  Eq. (4) , such as NIR, MNI and SSEM ( Cosgrove  et al. , 2008 ;  di Bernardo  et al. , 2005 ;  Gardner  et al. , 2003 ). In these methods, the inference of gene targets of a treatment is performed in two steps. The first step involves the identification of GRN, i.e. the coefficients  a k j , using gene expression data from experiments with known genetic perturbations (e.g. gene knock-out or silencing) and/or data compiled from publicly available database. In the second step, the perturbations  p k i  are calculated for the treatment samples of interest by network filtering using the GRN identified in the first step. Consequently, the predictions of gene targets depend on the GRN inference, a problem that is known to be severely underdetermined ( Szederkényi  et al. , 2011 ;  Ud-Dean and Gunawan, 2014 ). In contrast, DeltaNet generates the target prediction in a single step based on rewriting  Eq. (4)  in a matrix-vector format, as follows:
 (5) C = A C + P = [ A P ] [ C I m ] 
where  C  is the  n × m  matrix of logFC gene expression data of  n  transcripts from  m  samples,  A  is the  n × n  matrix of the coefficients  a k j  (with zero diagonal entries),  P  is the  n × m  matrix of treatment effects or perturbations  p k i , and  I m  is the  m × m  identity matrix. Here, we estimate the coefficients of the matrices  A  and  P  simultaneously by solving the linear regression problem:
 (6) C T = [ C T I m ] [ A T P T ] 
Since the dimension of the unknowns is larger than the number of samples, the regression problem above is underdetermined. We employ two different strategies for solving  Eq. (6) . The first involves least angle regression, which is a particularly efficacious model variable selection algorithm for low-sample high-dimensional data ( Efron  et al. , 2004 ). In the second implementation, we use LASSO regularization by constraining the L 1 -norm of the solution ( Tibshirani, 1996 ). The details of DeltaNet implementations are given in the next section. 2.2 DeltaNet implementation In the implementation of DeltaNet, we treat  Eq. (6)  as a general linear regression problem:
 (7) Y = X B 
where  X = [ C T I m ] ,  Y = C T  and  B = [ A P ] T . The columns of  X  and  Y  are further centered to have zero mean, while those of  X  are also normalized to have a unit Eucledian norm. The matrix  B  could be solved one column at a time, i.e. the matrices  A  and  P  are obtained one gene at a time. Thereby, DeltaNet involves solving multiple independent linear regression problems of the type  y k = X β k , which can be easily parallelized for computational speed-up. In order to enforce  a kk  = 0, we set the ( k th) row of the data matrix  C  corresponding to gene  k  to zero when solving  β k .  The matrix  A , if desired, can be computed by rescaling the appropriate submatrix of  B . Meanwhile, the matrix  P  is taken from the solution for  B  without rescaling. Two versions of DeltaNet are available: DeltaNet-LAR and DeltaNet-LASSO. As the name suggests, DeltaNet-LAR uses the LAR algorithm to solve the underdetermined regression problem above. LAR is an algorithm developed for creating sparse linear models ( Efron  et al. , 2004 ). Like the traditional forward selection method, LAR starts with a zero vector as the initial solution (i.e. no active variables), and adds a new predictor variable (i.e. an active variable) at every step. LAR employs a less greedy algorithm than the forward selection method in calculating the coefficients of the active variables. Briefly, in the first iteration, we choose the predictor that correlates most with the data (i.e. one that forms the least angle with the residual vector) and add this variable to the active set. The solution is updated along the direction of equal angles with respect to all variables in the active set, until the residuals become equally correlated with another predictor which is outside the active set. In the next iteration, this predictor is added to the active set, and the process is repeated until completion or until a desired number of active variables is reached. We employ the LAR algorithm from the MATLAB toolbox SpaSM (Sparse Statistical Modeling) ( http://www2.imm.dtu.dk/projects/spasm/ ). In a typical scenario, LAR terminates after  m  or fewer steps, since the number of samples  m  is far fewer than the number of genes in the dataset. The output of LAR consists of a series of solution vectors  β k i ,  i = 1 ,   2 , ⋯ ,   I , where  I  is the total number of steps. In DeltaNet-LAR, the steps are carried out until the relative norm error  ‖ y k − X β k i ‖ / ‖ y ‖  falls below a user-defined stopping criterion  δ r . Setting  δ r  higher would lead to fewer steps taken in LAR and thus fewer non-zero coefficients in the solution vector  β k .  The case studies below showed that the accuracy of DeltaNet predictions does not depend strongly on  δ r  in the range of 1% ≤  δ r  ≤ 10%. A higher  δ r  has the benefit of reducing computational time at the trade-off of slightly reduced prediction accuracy (see Section 3). In DeltaNet-LASSO, we solve the following penalized minimization problem:
 min β k ‖ y k − X β k ‖ 2 subject to ‖ a k ‖ 1 ≤ T 
where  a k  is the  k th row vector of the  A  matrix. Here, we employ GLMNET ( Friedman  et al. , 2010 ) to generate a regularization path for the LASSO problem above. Briefly, GLMNET uses the cyclical coordinate descent algorithm, which successively minimizes the objective function one-parameter-at-a-time and cycles over the parameters until convergence. While LAR could also be modified to generate the regularization path for LASSO ( Efron  et al. , 2004 ), our experience showed that GLMNET could reduce the computational times by several folds. For DeltaNet-LASSO, we implemented GLMNET subroutines for MATLAB ( http://www.stanford.edu/∼hastie/glmnet_matlab /). We perform a  k -fold cross validation (CV) method to determine the optimal  T  value. Briefly, we randomly divide the data into  k  equal-sample parts. For each CV trial, we assign  k –  1 parts as the training set and the remaining part as the test set. We then generate the regularization path using GLMNET and evaluate the errors of predicting the test set data as a function of  T  by rescaling the appropriate subvector of  β k  to  a k . The optimal  T  corresponds to the minimum average test errors over  k  number of CV trials. Figure 1  illustrates the general workflow of DeltaNet. In the first case study, we evaluated the performance of DeltaNet in predicting known gene perturbations in  Escherichia coli ,  Saccharomyces cerevisiae  (yeast) and  Drosophila melanogaster  (fruit fly). In the second case study, we assessed enriched transcription factors among the top gene target predictions from chemical treatment samples in yeast and  Homo sapiens  (MCF7 human cell line) datasets. We compared the performance of DeltaNet with z-score analysis and two network filtering methods, MNI and SSEM.
 Fig. 1. Workflow of gene target prediction using DeltaNet. The performance of DeltaNet in predicting known gene perturbations was evaluated using gene expression data of  E.coli, S.cerevisiae  and  D.melanogaster 
 2.3 Gene expression data For the case studies, we gathered gene expression data from public databases. For  E. coli , we retrieved microarray data from Many Microbe Microarrays Database (M3D, as of 29th October 2007) ( http://m3d.mssm.edu ) ( Faith  et al. , 2008 ). More specifically, we obtained the robust multi-array average (RMA)-normalized dataset from the file E_coli_v4_Build_3_chips524probes4217.tab. As summarized in  Figure 1 , the data comprised 4217 genes and 524 samples with 319 samples from gene perturbation experiments, 12 samples from chemical treatments, 55 samples from wild-type control experiments and 138 samples from other conditions (e.g. different growth phases, nutrient feeding strategies). The logFC expression data were computed by subtracting the average of wild-type control experiments from the log-2 RMA intensity data. For  S. cerevisiae , we compiled gene expression data from ArrayExpress ( Parkinson  et al. , 2007 ) and Gene Expression Omnibus (GEO) ( Barrett  et al. , 2013 ). In order to avoid cross-platform variability, we only used data from  Affymetrix GeneChip Yeast Genome S98 . Among 9335 probe sets, we could match 5117 probe sets to gene symbols using  ygs98.db  package in Bioconductor (Saccharomyces Genome database as of 9th March 2014). As shown in  Figure 1 , the yeast dataset consisted of 566 samples, among which 383 samples were from gene perturbation experiments, 36 samples from chemical treatments, 140 samples from wild-type control experiments and 7 samples from other conditions. The raw data were RMA-normalized using  justRMA  function in the  affy  package of Bioconductor ( Gentleman  et al. , 2004 ), which provided log-2 normalized intensity. The logFC expression data were again calculated by subtracting the average of all wild-type control samples from the log-2 RMA intensity. For multicellular organisms, like  Drosophila  and human, the gene expression data should ideally come from the same cell lineage, as the GRN can vary across cell lines. For  D. melanogaster , we compiled 330 microarray samples of  Affymetrix GeneChip Drosophila Genome 2.0  from ArrayExpress and GEO, of which 80% came from experiments using Schneider 2 (S2) cells and the remaining came from whole-body homogenates. These data originated from five studies involving gene knock-down (KD) and overexpression experiments. In particular, 242 samples came from genetic perturbations and 88 samples were from wild-type control experiments. We mapped the probe sets to GenBank accession number using  drosophila2.db  in Bioconductor (Entrez Gene database as of 17th March 2015). The expression data were again pre-processed using  justRMA  to give log-2 normalized intensity. We noted that the RMA intensities of the controls differed significantly among publications. Therefore, we computed the logFC by subtracting the control data for each publication separately. In order to reduce computational complexity, we only used 6165 probe sets which showed significant differential expression (logFC ≥ 1). By using the median values for multiple probe sets that mapped to one gene, we further reduce the dimension to 5879 genes. Finally, for human dataset, we compiled 2537 samples of MCF-7 human breast cancer cells from the Connectivity Map (C-Map version 2) ( Lamb  et al. , 2006 ). The expression data were first pre-processed by using  justRMA , based on which we computed the logFC expression using mean-centering within batches, as recommended in a previous study ( Iskar  et al. , 2010 ). We then selected a subset of 569 samples corresponding to 142 different compound treatments with three or more replicates. The final dataset corresponded to the median logFC expressions among the respective replicates. We mapped 19 846 probe sets from  Affymetrix GeneChip HT Human Genome U133  to GenBank accession number using  hthgu133a.db  in Bioconductor (Entrez Gene database as of 17th March 2015). For computational speed-up, we performed the gene target analysis using only 3153 genes that showed significant differential expressions (|median logFC| ≥ 1) in at least one of the treatments. 3 Results 3.1 Predicting network perturbations In the first application, we used DeltaNet, MNI, SSEM and  z -scores to predict the targets of gene perturbation experiments in  E. coli , yeast, and  Drosophila  datasets. The experiments involved known gene knockouts, over-expression and mutations, which provided the gold standard data for comparing the methods. The  z -scores were computed according to:
 (8) z k i = c k i − c ¯ k σ k 
where  c k i  is the logFC for gene transcript  k  in sample  i ,  c ¯ k  and  σ k  are the average and standard deviation of transcript  k  across all samples, respectively. For DeltaNet-LASSO and SSEM, we employed a 10-fold cross validation to determine the optimal  T  as discussed in Methods and Materials. Meanwhile, for DeltaNet-LAR, we used a threshold criterion  δ r  of 10%. In the case of MNI, we performed a grid search optimization for each compendium to determine the parameters  Q  and  thP , which we found to influence the target predictions strongly. Here, we selected the parameter combination giving the minimum average rank error for samples with known perturbations by employing a 5-fold CV. Meanwhile, the parameter  KEEPFRAC  was set such that we retained &gt;200 genes after the last round of tournament (0.37 for  E. coli , 0.35 for yeast and 0.33 for  Drosophila ) following the published protocol ( Xing and Gardner, 2006 ). The remaining parameters ( NROUNDS  and  ITER ) were set to their default values. Finally, we used a unit standard deviation for all samples, as this setting gave much better performance than using the recommended sample standard deviation. The test samples of  E. coli  came from 85 experiments with known perturbations, while the test samples of yeast comprised 109 experiments (see  Supplementary Data ). For  Drosophila , the test set came from the study of cell cycle regulators using S2 cells ( Bonke  et al. , 2013 ), comprising 91 different perturbation experiments.  Figure 1  gives the numbers of the combined gene targets in the test samples of  E. coli , yeast, and  Drosophila  test samples, which were slightly higher than the number of samples since a few experiments involved more than one gene perturbation. Except for MNI, we analyzed each sample of experimental replications separately, and used the median value as the final prediction. In the analysis using MNI, we followed the published protocol and used the average gene expression values over replicates as the input data ( Xing and Gardner, 2006 ). From each method and each test dataset, we obtained a rank list of genes where we sorted the genes in decreasing magnitudes of the perturbation variables  p k i  (see  Supplementary Data ). The ranking reflects the confidence level that a gene is directly perturbed in the corresponding experiment, while the sign of  p k i  indicates the nature of the perturbation. In evaluating the performance of the methods, except for MNI, a true positive (TP) requires not only a high confidence prediction (i.e. large magnitude in  p k i ), but also the correct sign of perturbations (a positive sign for gene induction and a negative sign for gene repression). As MNI did not provide the sign of the perturbations, we only use the gene ranking in evaluating its performance. Figure 2  compares the true positive rate (TPR) as a function of the gene rank according to DeltaNet, SSEM, MNI and  z -scores. The TPR was computed as the fraction of the known gene perturbations that appear above a given rank (shown in the  x -axis).  Figure 2  shows that DeltaNet significantly outperformed SSEM, MNI and  z -scores for all three test datasets. DeltaNet-LAR and DeltaNet-LASSO gave relatively the same TPR. The top 10 genes from DeltaNet had on average 14% and as large as 19% (for  D. melanogaster ) higher TPR than the next best method SSEM. MNI gave the worst TPRs among the methods considered, which could be caused by suboptimal tuning of the parameters. The need to optimize the method parameters for different datasets is a known drawback of MNI, since the tuning of these parameters can be computationally demanding ( Cosgrove  et al. , 2008 ).
 Fig. 2. True positive rates of gene target predictions from DeltaNet, SSEM, MNI and z-scores. The results of DeltaNet-LAR came from analyses using a threshold  δ r  = 10% 
 As shown in  Table 1 , DeltaNet-LAR finished faster than DeltaNet-LASSO and SSEM. The computational time of DeltaNet-LAR decreased with increasing  δ r , as expected. Meanwhile the TPR of DeltaNet-LAR did not vary significantly for  δ r  between 1 to 10% (see Supplementary Fig. S1). DeltaNet-LASSO and SSEM had similar computational times since both methods used the same LASSO regularization with 10-fold CV. If the optimal method parameters were known beforehand, then MNI finished quicker than DeltaNet and SSEM. But, as mentioned above, the parameter tuning could lead to a high total computational requirement (see  Table 1 ). Finally,  Table 2  gives the area under precision-recall (AUPR) and receiver operating characteristic curve (AUROC) for each method, which further confirms the higher accuracy of DeltaNet predictions over those from the other strategies.
 Table 1 .  Computational times of DeltaNet, SSEM and MNI Computational times a  (h) E.coli Yeast Drosophila DeltaNet-LAR δ r = 20% 4.34 9.6 5.7 δ r = 10% 9.77 18.8 9.2 δ r  = 5% 13.76 24.6 11.4 δ r  = 1% 17.20 29.1 12.9 completion 17.90 29.9 13.2 DeltaNet-LASSO 30.5 43.8 42.9 SSEM 33.8 48.6 43.1 MNI Single run 0.16 0.19 0.14 Parameter tuning b 15.58 15.55 11.83 a Computational times were determined based on a single CPU run in a workstation with AMD Opteron 6282 SE processor and 256 GB RAM. b The parameter tuning for  E.coli , yeast and  Drosophila  was performed by a grid search using 99, 96 and 89 grid points, respectively. 
 Table 2. AUROC and AUPR of DeltaNet, SSEM, MNI and  z -scores AUROC AUPR E.coli DeltaNet-LAR a 0.951 0.694 DeltaNet-LASSO 0.942 0.717 SSEM 0.921 0.558 MNI 0.906 0.252 Z -scores 0.860 0.262 Yeast DeltaNet-LAR a 0.890 0.432 DeltaNet-LASSO 0.903 0.402 SSEM 0.893 0.360 MNI 0.876 0.085 Z -scores 0.897 0.233 Drosophila DeltaNet-LAR a 0.966 0.534 DeltaNet-LASSO 0.957 0.527 SSEM 0.882 0.352 MNI 0.846 0.224 Z -scores 0.95 0.243 a DeltaNet-LAR result was obtained using δr = 10%. 3.2 Predicting transcription factor targets of chemical compounds In the second application, we employed the predicted gene targets to identify transcription factors (TFs) which interact with drug and chemical compounds. We used the subset of yeast dataset corresponding to chemical treatments and the human MCF-7 dataset. For yeast dataset, we employed DeltaNet, SSEM, MNI and  z -scores analysis to rank gene targets. For DeltaNet-LAR, we used  δ r  = 1% in order to keep enough non-zero  p k i  coefficients. We performed a TF enrichment analysis using the top 100 genes for each chemical treatment sample using Yeastract ( Teixeira  et al. , 2014 ), and obtained the adjusted  p -values of the enriched TFs. We ranked the enriched TFs in increasing  p -values (i.e. TFs with lower  p - values are ranked higher). For evaluating the accuracy of the TF prediction, we used protein-chemical interaction database in Search Tool for Interactions of Chemicals (STITCH) ( http://stitch.embl.de ) ( Kuhn  et al. , 2014 ) as a reference. STITCH uses experiments, public database and literature mining as evidence to establish links between chemicals and proteins. In addition, we also used two publications to establish links between TFs and acetate ( Giannattasio  et al. , 2013 ), and between TFs and rapamycin ( Jacinto and Hall, 2003 ). Among the chemical treatment experiments in the yeast compendium, only five compounds have TF interactions in STITCH (with a confidence score &gt; 0.7).  Figure 3a  compares the rankings of known TF targets of these five chemical compounds, according to the adjusted  p - values from Yeastract for DeltaNet, SSEM, MNI and  z -scores predictions (see  Supplementary Table S1  for more details). Here, DeltaNet gave the best median ranking (69.5), followed by SSEM (85), MNI (92.5), and lastly  z -scores (127). However, differences among the methods were not statistically significant (see  Supplementary Table S2 ).
 Fig. 3. Rankings of known TF targets of chemical compounds based on TF enrichment analysis of DeltaNet, SSEM, MNI and z-scores predictions. The TFs are ranked according to (a) the adjusted  p -values of Yeastract for yeast dataset and (b) the combined enrichment scores of Enrichr for human MCF-7 dataset 
 For MCF7 dataset, we applied DeltaNet, SSEM and  z -scores to generate the gene target predictions. We could not perform MNI analysis as we had no known perturbations in the MCF7 dataset for parameter tuning. For the TF enrichment analysis, we employed Enrichr ( Chen  et al. , 2013 ) with the option of position weight matrices using the top 100 predicted gene targets in each sample. Among the drugs in the dataset, only 21 compounds have at least one reported TF target in DrugBank ( Wishart  et al. , 2006 ) and STITCH, which is also in Enrichr.  Figure 3b  compares the rankings of the known TF targets of these 21 compounds according to the combined enrichment scores from Enrichr for DeltaNet, SSEM and  z -scores predictions (see  Supplementary Table S3  for more details). Again, DeltaNet gave the best median ranking (65), followed by z-scores (83) and SSEM (105). Here, the difference in the median rankings between DeltaNet and SSEM was statistically significant (see  Supplementary Table S2 ). Taken together, the outcomes of TF enrichment analyses above demonstrated that DeltaNet could provide gene target predictions which agreed better with previously reported TFs, than the other methods. Unfortunately, we could not assess the gene target predictions for the chemical treatment samples from  E. coli  because the chemical compounds, namely ampicillin, kanamycin, norfloxacin and spectinomycin, do not have any TF interactions with high confidence (score &gt; 0.7) in STITCH. 4 Discussion We developed a method called DeltaNet for predicting the molecular targets of drugs or chemical compounds from gene expression data. Many applications of great interest and importance in drug discovery research, including elucidating the mode of action (MoA) of drugs and the mechanisms of diseases, fall within the problem that is addressed by this work. DeltaNet formulation uses an ODE model of gene transcription process under steady state condition. We employed two strategies for solving the resulting underdetermined linear regression problem: least angle regression (DeltaNet-LAR) and LASSO regularization (DeltaNet-LASSO). One can relax the assumption in DeltaNet which sets  a k k  to zero, by treating the predicted  p k i  as the sum between the self-regulatory feedback and the perturbations caused by the treatment. In such a case, instead of using the magnitude of the perturbation coefficients  p k i  to rank genes, we could use the  q -values of  p k i  ( Storey, 2002 ). However, for the case studies above, we did not observe any improvement in the prediction accuracy when using gene ranking according to the  q -values (see  Supplementary Material  and Supplementary Fig. S2). The output of DeltaNet comprises a ranked list of gene target predictions. Such a list is amenable for further enrichment analysis to identify other type of molecular targets, such as TFs in the second case study above. An upstream analysis can also be applied to find protein partners of enriched TFs, for example by using Expression2Kinase ( Chen  et al. , 2012 ) and Enrichr ( Chen  et al. , 2013 ). Beyond TF and protein targets, one can also apply functional enrichment analysis to obtain the functional relevance of the gene target predictions. Several web-server tools exist for such a purpose, notably ToppCluster analysis ( Kaimal  et al. , 2010 ) which provides 17 categories of human-ortholog gene annotations such as gene ontology, pathways, microRNAs and human phenotype. DeltaNet uses the same ODE model of the gene transcription process as the methods MNI and SSEM, but with a key difference in the manner by which the target predictions are inferred from the data. The first phase of MNI and SSEM involve the identification of the GRN matrix  A  using a compendium of gene expression data. The perturbation matrix  P  is subsequently obtained for the treatment samples of interest by network filtering, which in essence uses the difference  P = C − C A .  In MNI, the matrix  A  is estimated from training data together with the unknown matrix  P , using a procedure that resembles Expectation Maximization algorithm ( di Bernardo  et al. , 2005 ). The convergence of this procedure is however not guaranteed and the solution often varies with the initial starting guess. Also, the performance of MNI is known to sensitively depend on the tuning of method parameters which often leads to numerically intensive optimization ( Bevilacqua and Pannarale, 2013 ). Not to mention, MNI also requires data from known genetic perturbations for parameter tuning. In contrast, SSEM uses LASSO regularization to identify the matrix  A  using the complete gene expression data, where the perturbation matrix  P  is initially set to zero. By doing so, SSEM ignores the treatment or perturbation effects when inferring the GRNs. The matrix  P  is subsequently obtained from the residuals of the regression above. The LASSO regularization enforces a limit on the model complexity, an assumption which is based on the observed sparsity of GRNs ( Gardner  et al. , 2003 ;  Luscombe  et al. , 2004 ;  Tegner  et al. , 2003 ). The implementation of LASSO requires selecting the appropriate constraint parameter  T  for model complexity. As the optimal value is not known  a priori  and is also problem dependent, a cross-validation is often used for setting  T . As shown in  Table 1 , analyses using LASSO, including DeltaNet-LASSO and SSEM, were the slowest among the methods considered. Here, the majority of the computational time was contributed by the cross validation step. One can view DeltaNet as a hybrid between MNI and SSEM. The inference of the matrices  A  and  P  in DeltaNet is performed simultaneously, which resembles the first step of MNI. But, like SSEM, we employed a GRN sparsity assumption by way of LAR and LASSO regularization to tackle the underdetermined problem. Nevertheless, DeltaNet does not involve an explicit network filtering step. Instead, the perturbation matrix  P  for the treatment samples is obtained together with the other samples in the compendium. We could therefore fully use the information contained in the available data (training and treatment sets) in predicting the effects of a treatment. As demonstrated in the case studies, DeltaNet offers a significant improvement in the accuracy of target prediction over MNI and SSEM. Furthermore, DeltaNet-LAR has better numerical efficiency and robustness with respect to parameter tuning over DeltaNet-LASSO, MNI and SSEM. We therefore recommend DeltaNet-LAR using a threshold parameter  δ r  of 10%, since in our experience, this setting provides a good balance between target prediction accuracy and computational performance. While the difference between DeltaNet and the existing network filtering methods may appear slight, this deviation is nevertheless important and fundamental. There were two key factors motivating the single-step inference in DeltaNet. First, the inference of GRNs from the typical gene expression has been shown to be underdetermined ( Szederkényi  et al. , 2011 ;  Ud-Dean and Gunawan, 2014 ). Thus, any method relying on the solution of such an inference problem could be sensitive to the associated uncertainty. Second, despite the underdetermined issue, it is often possible to predict the effects of a network perturbations from existing gene expression data with reasonable accuracy ( Maathuis  et al. , 2010 ). We formulated DeltaNet based on the premise that the available gene expression data, while lacking information for the accurate inference of GRN, have enough information to identify the network perturbations caused by a treatment. The differences between the gene target predictions from DeltaNet-LASSO and SSEM are quite interesting, considering that both methods employ the same LASSO regularization. In the first case study, we noted that for yeast and  E. coli  datasets, DeltaNet-LASSO produced sparser GRNs than SSEM (see Supplementary Fig. S3a). This trend is expected since in comparison to SSEM, DeltaNet formulation has additional degrees of freedom that come from the perturbation vector. However, the network sparsity between DeltaNet and SSEM in the  Drosophila  dataset did not differ significantly. We further looked at the set of known gene targets among the top 10 predictions from DeltaNet-LASSO, but not from SSEM. For these gene targets ( n  = 10, 15 and 13 for  E. coli , yeast and  Drosophila , respectively), DeltaNet-LASSO clearly produced fewer non-zero coefficients than SSEM for all three datasets (see Supplementary Fig. S3b). The observations above indicated a possibility of overfitting in SSEM as the regression problem did not consider perturbations on the genes. Finally, time-series expression data have become routine and increasingly available in public databases. Applying DeltaNet and similar methods such as SSEM and MNI to time series data should be done with caution because of the underlying steady state assumption in the method formulation. In the case studies, we included time-series data as a part of the training dataset. Excluding time-series data however did not affect the accuracy of DeltaNet predictions significantly (see Supplementary Fig. S4). An extension of DeltaNet to take advantage of time-series data is currently being developed, the results of which will be reported in a separate publication. 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Path: a tool to facilitate pathway-based genetic association analysis</Title>
    <Doi>10.1093/bioinformatics/btp431</Doi>
    <Authors>Zamar David, Tripp Ben, Ellis George, Daley Denise</Authors>
    <Abstract>Summary: Traditional methods of genetic study design and analysis work well under the scenario that a handful of single nucleotide polymorphisms (SNPs) independently contribute to the risk of disease. For complex diseases, susceptibility may be determined not by a single SNP, but rather a complex interplay between SNPs. For large studies involving hundreds of thousands of SNPs, a brute force search of all possible combinations of SNPs associated with disease is not only inefficient, but also results in a multiple testing paradigm, whereby larger and larger sample sizes are needed to maintain statistical power. Pathway-based methods are an example of one of the many approaches in identifying a subset of SNPs to test for interaction. To help determine which SNP–SNP interactions to test, we developed Path, a software application designed to help researchers interface their data with biological information from several bioinformatics resources. To this end, our application brings together currently available information from nine online bioinformatics resources including the National Center for Biotechnology Information (NCBI), Online Mendelian Inheritance in Man (OMIM), Kyoto Encyclopedia of Genes and Genomes (KEGG), UCSC Genome Browser, Seattle SNPs, PharmGKB, Genetic Association Database, the Single Nucleotide Polymorphism database (dbSNP) and the Innate Immune Database (IIDB).</Abstract>
    <Body>1 INTRODUCTION The introduction of high-throughput single nucleotide polymorphism (SNP) genotyping methods has given rise to large-scale genome-wide association studies (GWAS) to identify common SNPs associated with complex traits. Until recently, the primary focus of most of these studies has been the discovery of single-SNP associations. However, single-SNP analyses are limited to identifying a subset of the most significant SNPs that account for only a small fraction of the total phenotypic variance. As the number of hypotheses that may be tested increases exponentially with the number of SNPs included in a study, biological information from the literature is commonly utilized in the development of hypotheses. For these kinds of large studies, the simple task of storing, retrieving and visualizing results of an analysis has become surprisingly challenging. Although several software applications, such as PLINK (Purcell  et al .,  2007 ), were designed to help analyze genetic association data and subsequently help to store and visualize results, none was designed to retrieve information from several bioinformatics resources and to conveniently integrate this knowledge with the results from a genetic association study. We were, therefore, motivated to develop Path, a software application designed to help researchers interface their data with biological information from several bioinformatics resources. This information may be used to help generate biologically plausible hypotheses for testing gene–gene interactions. The Path software is a first-step bioinformatics approach to investigate gene–gene interactions in genetic association studies. Examples of the type of information retrieved and the bioinformatics resources accessed by Path are shown in  Table 1 .
 Table 1. Bioinformatics resources accessed by Path Resource Name URL Description Extracted Information National Center for Biotechnology Information (NCBI) http://www.ncbi.nlm.nih.gov A resource for molecular biology information. The SNP function and gene it belongs to. Online Mendelian Inheritance in Man (OMIM) http://www.nslij-genetics.org/search_omim.html Archive of human genes and genetic phenotypes. List of known patterns of disease inheritance and genes with prior substantial evidence for association with disease. Kyoto Encyclopedia of Genes and Genomes (KEGG) http://www.genome.jp/kegg A collection of manually drawn pathway maps representing current knowledge concerning several networks of molecular interactions and reactions. Biological pathways and corresponding diagrams that each gene is involved in. UCSC Genome Browser http://genome.ucsc.edu Archive of reference sequences and working draft assemblies for a large collection of genomes. Genome browser page link for each gene. Seattle SNPs http://pga.gs.washington.edu SNP variation discovery resource. Links to the sequencing and genotyping information for each gene. PharmGKB http://www.pharmgkb.org Collection of relationships among drugs, diseases and genes, including their genetic variations and gene products. PharmGKB page link for each gene. Genetic Association Database http://geneticassociationdb.nih.gov Archive of composite information about genetic linkage data and genetic association data from published reports. Links to results from published association studies. The Single Nucleotide Polymorphism database (dbSNP) http://www.ncbi.nlm.nih.gov/projects/SNP A public-domain archive for a broad collection of SNPs dbSNP page link for each SNP. The Innate Immune Database (IIDB) http://db.systemsbiology.net/cgi-bin/GLUE/U54/IIDBHome.cgi A repository of genomic annotations and experimental data for over 2000 genes associated with immune response behavior in the mouse genome. IIDB page link for each gene. 2 FUNCTIONALITY As input, Path requires a dataset in the LINKAGE pre-makeped format (Terwilliger and Ott,  1994 ) and a data file in QTDT format (Abecasis  et al .,  2000 ). Additionally, one may supply a file with single-SNP association results. If association results are not supplied, the application initially performs a single-SNP analysis. Thereafter, a simple graphical user interface is used to explore the data along with the information retrieved from all nine bioinformatics resources and to conduct studies on the SNP–SNP interactions of the user's choice. Version 3.0.13 of the software application, UNPHASED (Dudbridge,  2003 ,  2006 ,  2008 ) is used for all analyses. The imported data and results of the analysis are stored in a local database. Analogous to PLINK, our software also provides the means to analyze and store genetic association data and to visualize results with charts, plots and summary tables. A summary page that can be easily accessed and queried through the user interface is provided for each SNP. Entries for each SNP include basic background information, such as function, gene, chromosome, etc., and a summary of the results of single-SNP associations. Each SNP entry also provides several links to other data, such as KEGG pathway (Kanehisa  et al .,  2006 ,  2008  and Kanehisa and Goto,  2000 ), and to previous association study results. To facilitate the selection of SNPs to test for gene-gene interactions, Path automates the SNP to gene annotation. This allows the user to easily visualize association results for SNPs and genes in the context of KEGG pathways. Path will display the selected KEGG pathway and highlight the genes with genotypes in the selected pathway. Path includes visualization tools that interface with KEGG pathways and the users genetic association results, facilitating the exploration of genetic associations in the context of genetic pathways. Path guides users with simple point and click interfaces in the selection of SNPs to test for gene-gene interactions. In addition, the linkage disequilibrium (LD) plot of the gene containing the specified SNP is provided in the SNP summary page. The LD plots are generated by using the Haploview (Barrett  et al .,  2005 ) software. The majority of the bioinformatics information provided by our application is accessed through external links; therefore, connection to the Internet is required. These links are not automatically generated when a dataset is imported, because they may already exist and take time to create (depending on the speed of your Internet connection). Instead, we have incorporated an update option that may be periodically run by the user to maintain an up-to-date database of links to external resources. Studies of gene–gene interactions are carried out by using a simple point-and-click interface. Results of analysis of single-SNP associations and of gene–gene interactions are calculated by using UNPHASED. After a gene–gene interaction has been submitted for testing, a link to the analysis results produced by the UNPHASED application is returned. Most GWAS typically include several hundred thousand SNPs, therefore, to help users single out SNPs that they want to include in a gene–gene interaction test, we have implemented a filtering option that enables the user to work with a subset of SNPs whose single-SNP association  P -values fall below a chosen threshold. It took 124 s to import a dataset of 10 000 SNPs on 162 individuals using an Intel Core 2 Duo 2.4 GHz CPU. 3 FUTURE DIRECTIONS We will extend our application to include information based on ontology and gene expression profiles. Due in part to the current partial identification and understanding of locus control regions, our software is limited in that it does not extract information on SNPs that may regulate a genetic pathway (i.e. promoters or locus control regions); thus our application, at present, does not account for such SNPs. To remedy this, we will include pathway information on SNPs that fall outside gene regions. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>hapConstructor: automatic construction and testing of haplotypes in a Monte Carlo framework</Title>
    <Doi>10.1093/bioinformatics/btn359</Doi>
    <Authors>Abo Ryan, Knight Stacey, Wong Jathine, Cox Angela, Camp Nicola J.</Authors>
    <Abstract>Summary: Haplotypes carry important information that can direct investigators towards underlying susceptibility variants, and hence multiple tagging single nucleotide polymorphisms (tSNPs) are usually studied in candidate gene association studies. However, it is often unknown which SNPs should be included in haplotype analyses, or which tests should be performed for maximum power. We have developed a program, hapConstructor, which automatically builds multi-locus SNP sets to test for association in a case-control framework. The multi-SNP sets considered need not be contiguous; they are built based on significance. An important feature is that the missing data imputation is carried out based on the full data, for maximal information and consistency. HapConstructor is implemented in a Monte Carlo framework and naturally extends to allow for significance testing and false discovery rates that account for the construction process and to related individuals. HapConstructor is a useful tool for exploring multi-locus associations in candidate genes and regions.</Abstract>
    <Body>1 INTRODUCTION Multiple tagging-SNPs (tSNPs) are widely used in candidate gene association studies. It has been shown that there is increased power to detect disease variants with low frequency by performing both haplotype and single-locus analyses even with the multiple testing correction (Becker and Knapp,  2004 ). In new studies, tSNPs are usually analyzed independently and in multi-SNP combinations. Even when associations are considered established (Cox  et al. ,  2007 ), comprehensive SNP-set haplotype analyses can be performed to more accurately define the haplotype/s on which the susceptibility variants lie. One avenue that may effectively guide such searches is a more systematic haplotype-mining analysis. Multi-locus analyses are of high dimension leading to reduced power when testing association. Haplotype similarity, cladistic and phylogenetic techniques can be used to reduce dimensionality (Bardel  et al. ,  2006 ; Camp  et al. ,  2005 ; Jannot  et al. ,  2004 ; Liu  et al. ,  2007 ; Molitor  et al. ,  2003 ; Tzeng and Zhang,  2007 ; Waldron  et al. ,  2006 ; Yu  et al. ,  2005 ). However, these methods require a priori determination of which SNPs to include; and there remains the question of whether to analyze monotype or diplotype data and the mode of expression. Studying SNP subsets may be optimal and reduce dimension. Sliding windows (Lin  et al. ,  2004 ) and haplotype clustering using variable-length Markov chain models (Browning and Browning,  2007 ; Browning,  2006 ) have been proposed for traditional case-control data and contiguous subsets of SNPs. An approach for non-contiguous SNP subsets exists; constructing haplotypes by starting from SNP pairs and iteratively adding SNPs based on significance and base pair distance (haploBuild; Laramie  et al. ,  2007 ). While this latter approach is flexible in haplotype construction, it is limited to transmission statistics in the FBAT software (Horvath  et al. ,  2001 ) and lacks a valid significance assessment that accounts for all the multiple testing inherent in a data-mining technique. We present hapConstructor, software to construct and test multi-locus data, allowing for non-contiguous SNP subsets. Tests for non-independence and effect size are incorporated. Monotype (alleles or haplotypes); diplotype (genotypes or haplotype pairs); and composite genotype (unphased genotypes across multiple loci) tests are included. Standard reductions of dimensionality are incorporated, such as specific haplotype tests for monotype data, and dominant, recessive and additive tests for specific haplotypes for diplotype data. Multi-locus SNP sets are constructed through a forward–backward stepwise process. HapConstructor operates in a Monte Carlo (MC) framework which offers two advantages. First, it naturally extends to testing related individuals. Second, the null distribution for the full SNP set is simulated once, and can be used to assess both empirical significance of individual tests and  construction-wide P -values and false discovery rates (FDRs) that account for the construction process. HapConstructor is a Java-based extension of Genie (Allen-Brady  et al. ,  2006 ). 2 METHODS The MC framework is provided by Genie, with imputation of missing data, estimation of population haplotype frequencies and maximum likelihood estimates (MLE) of individuals’ haplotype pairs provided by the hapMC component. First, all single SNPs {s1, s2,…,s n } are tested. In each forward step, a SNP is added to SNP sets whose  P -value surpassed the user-defined threshold at the previous step. The thresholds can be constant or may vary by step. For example, if s1 surpassed the first threshold, the next step would consider two-locus SNP sets {s1-s2, s1-s3,…,s1-s n }. An optional backward process starts at the third step and consists of testing all ( n −1)-locus subsets not previously considered. To maintain efficiency and speed and reduce redundancy, each subsequent step in the build process extends the haplotypes with the specific alleles that previously met the threshold at the prior step rather than considering all haplotypes spanning the new loci set. Test statistics available are χ 2 , χ 2 -trend and odds ratio. The data can be considered as diplotype or monotype or both. For diplotype data, haplotype and composite genotype tests are performed. Haplotype models are dominant, recessive and additive models for each haplotype. Composite genotypes include each of the dominant and recessive combinations across loci. For monotype data, each specific haplotype is compared to all others. Summaries for all tests performed are stored. A user interface allows these to be sorted by step, SNP, test-type and significance. If required, a  construction-wide  assessment that accounts for the building process can be made. A valid global  P -value and FDR is generated; the latter is more appropriate for data mining (Benjamini and Hochberg,  1995 ). These are achieved by reusing the null configurations generated for the MC procedure. Each null configuration is considered as the ‘observed data’ and the construction algorithm is used with significances determined from the remaining  N −1 null configurations. This is repeated to generate a set of null ‘constructions’ from which valid empirical  construction-wide P -values and FDRs are determined. 3 RESULTS We illustrate hapConstructor using a sample of 1128 independent breast cancer cases and 1149 independent controls from Sheffield, UK and 14 tSNPs in the CASP8 gene. Single SNP tests results yielded three SNPs with  P -values below 0.05 (0.010–0.047). The construction process continued to the fifth step (five-locus haplotypes). A four-locus haplotype was identified as the most significantly associated haplotype with an empirical  P -value of 8.0×10 −5  and a construction-wide FDR of 0.044, a result which is consistent with the established association between breast cancer and CASP8 (Cox  et al. ,  2007 ). This four-locus haplotype contained only one of the three SNPs that had obtained significant single test results. HapConstructor completed the building process for the real data in 96 h with 100 000 MC simulations, on a machine with an Intel Pentium core 2 duo with 3.0 Ghz per processor and 2 GB of memory. It required 7 days using 10 server nodes to complete 1000 simulated builds for the  construction-wide  significance assessment. To assess the potential value-added of the construction process in our illustrative example, we analyzed all 14-SNP haplotypes with frequencies over 1% and also performed exhaustive sliding window analyses for window sizes of 2- to 6-SNP haplotypes. Of the 15 14-SNP haplotypes analyzed, only one obtained nominal significance ( P =0.0357). For the sliding windows, 2351 tests were conducted and 314 were found to be nominally significant (0.0021–0.05, not accounting for multiple testing). The most significantly associated haplotypes were found in the four-, five- and six-locus window sizes. The results from both of these more standard approaches were inferior to the haplotype building in terms of significance and indicate that hapConstructor was a valuable approach and that exhaustive searches using contiguous multi-SNP sets are not the optimal solution in this situation. 4 CONCLUSIONS HapConstructor offers a data-mining approach to association analyses, allowing automatic and comprehensive construction of multi-locus SNP-set tests. It improves upon other methods in the variety of analyses and statistics performed, and the ability to appropriately assess global significance. Additional features are the immediate extension to mixtures of independent and related individuals, a virtue of the method being nested in Genie (Allen-Brady  et al. ,  2006 ), and the ability to impute missing data. It should be noted, however, that the extension to related individuals is limited to an assumption of no recombination, as only under these conditions are MLE haplotype estimates using relatives unbiased. A limitation of hapConstructor, and MC testing in general, is computational burden. This is dependent upon the number of simulations (especially  construction-wide  assessment), sample size, number of SNPs considered and threshold values. Depending on the dataset being analyzed, hapConstructor may require significant time and computational resources to complete both the build process and construction-wide assessment. Construction-wide assessment may be intractable for large datasets due to time or resources. Despite the computational intensity, hapConstructor is a useful tool for exploring multi-locus associations in candidate genes and regions, and fulfills a current need of many investigators. Our future work will include more sophisticated heuristics for the construction process and extensions to interaction models. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Fast-SNP: a fast matrix pre-processing algorithm for efficient loopless flux optimization of metabolic models</Title>
    <Doi>10.1093/bioinformatics/btw555</Doi>
    <Authors>Saa Pedro A., Nielsen Lars K.</Authors>
    <Abstract>Motivation: Computation of steady-state flux solutions in large metabolic models is routinely performed using flux balance analysis based on a simple LP (Linear Programming) formulation. A minimal requirement for thermodynamic feasibility of the flux solution is the absence of internal loops, which are enforced using ‘loopless constraints’. The resulting loopless flux problem is a substantially harder MILP (Mixed Integer Linear Programming) problem, which is computationally expensive for large metabolic models.</Abstract>
    <Body>1 Introduction Constrained-based methods are the most popular methods for exploring the capabilities of genome-scale metabolic models (GEMs) ( Lewis  et al. , 2012 ). GEMs have been reconstructed for many model organisms ( Dal’Molin  et al. , 2010a ;  Duarte  et al. , 2007 ;  Edwards and Palsson, 2000 ;  Forster  et al. , 2003 ), enabling comprehensive study of the genotype–phenotype relationship, cellular physiology, metabolic capabilities, among others ( Bordbar  et al. , 2014a ). More recently, these reconstructions have included larger and more complex networks, describing different cellular interactions ( Bordbar  et al. , 2010 ;  Lewis  et al. , 2010 ), microbial communities ( Shoaie  et al. , 2015 ;  Stolyar  et al. , 2007 ;  Zhuang  et al. , 2011 ) and multi-tissue organisms ( Bordbar  et al. , 2011 ;  Dal’Molin  et al. , 2010b , 2015). The increased complexity of current metabolic models demands efficient constrained-based methods to compute possible network states, especially thermodynamically feasible ones. Computation of thermodynamically feasible states is computationally hard ( Müller and Bockmayr, 2013 ), severely limiting its application to large-scale models. Efficient computational methods are thus needed to interrogate the consequences of thermodynamic feasibility on these models. GEMs are mathematically represented by the stoichiometric matrix  S ∈ R m × n , which encodes the mass balances for  m  internal metabolites involved in  n  reactions. The capacity of each reaction is phenomenologically constrained by thermodynamics and enzyme kinetics through the use of appropriate lower  lb  and upper  ub  bounds on the vector of reaction fluxes  v ∈ R n  ( Equation 1 ). Assuming vanishing accumulation of internal metabolites ( Equation 2 ), the space of feasible steady-state fluxes is defined by the following set of constraints (hereafter referred as mass balance constraints),
 (1) lb ≤ v ≤ ub 
 (2)     S ⋅ v = 0 
 This typically highly undetermined space is readily explored using Flux Balance Analysis (FBA). Given an optimality criterion, FBA uses linear optimization to compute a steady-state flux solution that maximizes a defined objective such as growth rate, ATP production, among others (Orth  et al. ,  2010 
 b ). The flexibility of the network under (sub)optimal conditions can be further assessed using Flux Variability analysis (FVA) ( Mahadevan and Schilling, 2003 ). FVA computes the minimum and maximum flux through each reaction capable of supporting a defined network state.  Equations (3)  and  (4)  describe the LP formulations of the above optimization problems,
 (3) FBA :  max v   c T ⋅ v                       s . t .   { mass   balance   constraints } 
 (4) FVA :  max v / min v   v i ,       i ∈ { 1 , … , n }                       s . t .   c T v ≥ α ⋅ z opt                                 { mass   balance   constraints } 
where  c  represents the objective vector,  z opt  denotes the optimal value found by FBA, and  α  is a parameter describing the degree of optimality w.r.t. FBA, i.e. for suboptimal analysis 0 ≤ α &lt; 1, whereas α = 1 for alternative optima analysis. Although FBA and FVA provide feasible steady-state flux solutions, they are not guaranteed to be thermodynamically feasible. Additional constraints on the flux vector  v  are needed to ensure this and a minimal criterion for thermodynamic feasibility is the absence of internal loops. Let us define the ‘loop-law’ matrix,  N int ∈ R n i × l , the matrix containing a null space basis of the stoichiometric matrix of internal reactions  S int ∈ R m i × n i , and,  v int  and Δ μ int  the internal reaction and the corresponding chemical potential vectors, respectively. A steady-state flux solution  v * is thermodynamically feasible if, (i) the global potential energy of the network is balanced, i.e.,  N int T ⋅ Δ μ int * = 0  (first law), and (ii) reactions proceed in the opposite direction of chemical potential change, i.e.  diag ( Δ μ int * ) ⋅ v int * ≤ 0  (second law). These two conditions hold simultaneously true if and only if the net flux around all closed loops is equal to zero, i.e.  v * is ‘loopless’ ( Beard  et al. , 2002 ). The loopless condition can be enforced by formulating mixed-integer linear constraints on  v int  (hereafter referred as loopless constraints) ( Schellenberger  et al. , 2011 ),
 (5) N int T ⋅ Δ μ int = 0 1 − ε int ⋅ ( K + 1 ) ≤ Δ μ int ≤ K − ε int ⋅ ( K + 1 ) − K ⋅ ( 1 − ε int ) ≤ v int ≤ K ⋅ ε int ε int , i ∈ { 0 , 1 }   i ∈ { 1 , … , n i } ,   Δ μ int ∈ R n i ,   K   large   ( e . g . ,   10 3 ) 
 Addition of  Equation (5)  to  Equations (3)  to  (4)  yields two MILP formulations commonly known as ll-FBA and ll-FVA (‘ll’ stands for loopless). Inclusion of the loopless constraints not only increases the size of the problem (decision variables are now  ε int ,  v , Δ µ int ), but also yields a harder optimization problem. Computationally, this problem has recently been addressed by recasting ll-FVA into a series of LP problems, in which steady-state flux solutions are computed by conventional FBA and corrected by iteratively blocking all infeasible cycles ( Desouki  et al. , 2015 ;  Müller and Bockmayr, 2013 ). Here, we proposed a different approach where loopless constraints are reduced to a sufficient set of feasible loop laws, enabling efficient computation of loopless flux optimization problems. In this way, once the sufficient set of loop laws has been found, any standard loopless optimization problem can be formulated and solved using a single optimization problem without post-processing. In this work, we present a pre-processing algorithm—Fast-SNP—for efficient formulation of loopless constraints. We note that it is always possible to pre-process the loop-law matrix  N int  constrained to known directionalities, so that the reduced set of constraints yields an easier-to-solve, yet equivalent MILP problem. By using a fast sparsification algorithm, Fast-SNP generates a reduced feasible loop-law basis  N ˜ int , accounting for these directionalities. Application of Fast-SNP considerably reduced computation times during loopless flux optimization compared with the traditional approach in different metabolic models. Furthermore, our approach identifies key directional constraints, which may enable elimination of infeasible cycles in the underlying GEM. Altogether, Fast-SNP enables efficient computation of thermodynamically feasible flux solutions in GEMs. 2 Methods 2.1 A useful matrix pre-processing algorithm for efficient loopless flux optimization The motivation of our method stems from the observation that not all internal loop laws are feasible given the directionalities defined by  Equation (1)  ( Saa and Nielsen, 2016 ). We previously exploited this fact to detect ‘potentially active or feasible loop-laws’ in metabolic models using the conventional representation of  N int  (e.g. reduced row echelon form of  N int T ), enabling faster discovery of infeasible random flux samples ( Saa and Nielsen, 2016 ). This reduced set of feasible loop-laws cannot immediately be used for loopless flux computation. However, if we are able to derive a reduced basis of  N int  containing a minimum set of feasible loop-laws, then the computed loopless flux solutions will be accurate (see  Supplementary Fig. S1  for an illustrative example). In addition to minimizing the number of loop-laws, we decided to simplify the laws through sparsification. We developed a fast matrix pre-processing algorithm motivated by recent progress in the matrix sparsification field ( Bian  et al. , 2015 ). As opposed to sparsification algorithms based on MILP formulations (Bordbar  et al. ,  2014b ), our approach relies on an LP formulation that is not guaranteed to find the ‘sparsest’ null space basis, i.e. least number of non-zero entries, but quickly generates a reduced feasible basis for efficient computation of thermodynamically feasible flux solutions. The general workflow for efficient loopless flux optimization is illustrated with a toy model ( Fig. 1 ). First, the network stoichiometry is formulated and the internal and exchange reaction matrices,  S int  and  S ext , are defined ( Fig. 1A ). Next, an initial basis  N int  is found using singular value decomposition (SVD). Conventional loopless optimization would use this matrix or a reduced row echelon form of  N int T  as constraint for loopless flux computation ( Equation 5 ). In this example, out of the three possible independent loop laws found using these methods, there are only two feasible loop laws—L 1  and L 2 —given the directionalities of  v 1  and  v 2 , i.e. L 3  cannot form a loop ( Fig. 1B ). Thus, a reduced basis  N ˜ int  can be constructed using just the first two loop laws. This basis is readily found using Fast-SNP by integrating the topology of  S int  and directionality constraints. Implementation of this pre-processing step significantly improves computational performance in a diverse family of loopless optimization problems ( Fig. 1C ). In the following, we present our algorithm for pre-processing the loop-law matrix  N int .
 Fig. 1. Illustration of the optimization workflow using Fast-SNP. 
 (A)  Network stoichiometry definition. The toy model consist of seven internal reactions ( v i ,  i  = 1,...,7), three exchange reactions ( b j ,  j  = 1,2,3) (nine reactions in total) and five metabolites. The stoichiometric matrix  S  is defined by two block matrices containing, respectively, the internal ( S int ) and exchange ( S ext ) reactions.  (B)  Pre-processing the loop-law matrix using known directionalities. An initial basis for the null space of internal reactions ( N int ) is for example computed using SVD. This matrix contains  r = n i  −  rank( S int ) = 3 loop laws or basis vectors accounting only for the information in  S int . However, given the directional constraints on N int , a minimal basis  N ˜ int  can be found by including directionality information using Fast-SNP. The resulting basis  N ˜ int  has  r*  = 2 ≤  r  feasible loop laws, enabling more efficient optimization of loopless problems.  (C)  Once a reduced basis for the loop-law matrix is found, a diversity of loopless optimization problems can be performed more efficiently 
 2.2 A fast matrix sparsification for efficient formulation of loop-law constraints The problem of finding the sparsest linear basis of the null space—or the Sparse Null-space Basis problem (SNB)—is motivated by its application to linear equality problems arising in constrained optimization problems ( Gottlieb and Neylon, 2010 ).  Coleman and Pothen (1986)  demonstrated that a greedy algorithm must find the SNB of a matrix  A m × n  in  r  =  n  −   rank( A ) steps, provided that at each step the subproblem of finding the sparsest null-space vector (SNV) can be solved. Since SNV is NP-complete, SNB is NP-hard ( Coleman and Pothen, 1986 ), and the use of approximate algorithms to solve SNB is justified. Recently,  Bian  et al.  (2015)  proposed a convex-relaxation of the SNB—referred to as the Sparse Null-space Pursuit (SNP)—where a sparse basis is computed in  r × n  LP optimization runs. While this formulation is in principle attractive, it does not consider any directional constraints on the basis vectors, which is a key feature of our problem. Moreover, the  n  LP optimizations per basis vector are excessive. We have developed a more efficient sparsification algorithm—Fast-SNP—inspired by the SNP formulation. Fast-SNP finds a minimal sparse representation of  N int  in at most 2 r  LP optimization runs. Briefly, starting from an empty null space basis, the SNV is solved by finding the minimum  l 1 -norm steady-state flux solution  v int, k  that, (i) is consistent with the defined directionalities, and (ii) is contained in the orthogonal space of  N int, k  derived from the previous ( k  − 1) iterations. The latter constraint ensures that the basis vector computed at iteration  k  is linearly independent from the previous ( k  − 1) vectors. This condition can be formulated as  P N int , k ⊥ ⋅ v int , k ≠ 0 , where  P N int , k ⊥  denotes the projection matrix onto null( N int, k ). As this above equation defines neither a convex nor a compact region, we replace it with two equivalent constraints, namely:
 (6) w T ⋅ P N int , k ⊥ ⋅ v int , k &gt; ζ     ∨     w T ⋅ P N int , k ⊥ ⋅ v int , k &lt; − ζ 
where  w  represents a vector of random weights and  ζ  is a small positive constant, e.g. 10 − 3 . Here we employed uniform random weights; however other choices can be also used yielding similar results ( Supplementary Table S1 ). As  w  is non-zero, the above constraints are only violated if  P N int , k ⊥ ⋅ v int , k ≠ 0 , which is the condition to be avoided. The resulting SNV optimization problem can be then formulated as follows,
 (7) SNV :  min ( x , v int , k )   ∑ i x i                       s . t .   S int ⋅ v int , k = 0                   v int , k ≤ x − v int , k ≤ x                                   l b int ≤ v int , k ≤ u b int w T ⋅ P N int , k ⊥ ⋅ v int , k &gt; ζ   ( positive   constraint )   ∨                                   w T ⋅ P N int , k ⊥ ⋅ v int , k &lt; − ζ   ( negative   constraint )                                   x ∈ R n i 
 In this way, two LP optimizations are required to determine the SNV at each iteration, and thus 2 r  runs are needed to complete the null space basis. Notably, after each iteration, the sparsest (i.e. the minimum  l 0 -norm) solution is kept for the next iteration. If at one iteration there are no feasible solutions for both SNV problems, then the algorithm terminates as there are no remaining non-zero basis vectors given the current  N int, k  and the directionality constraints. The pseudocode of our Fast-SNP is shown in Algorithm 1. 2.3 Detection and removal of infeasible loops Generation of a reduced loop-law matrix can accelerate other loopless flux optimization problems. Here, we evaluated two of such problems, namely: loop detection (LD) and loop removal (LR). The first problem determines whether a steady-state flux distribution is thermodynamically feasible (i.e. loopless), whereas the second computes the nearest feasible flux solution. Specifically, given a steady-state flux solution  v 0  with  v int,0  denoting its internal reactions, LD tests for thermodynamic feasibility by directly applying the loopless constraints using a LP formulation with a blank objective ( Schellenberger  et al. , 2011 ),
 (8) LD :  min Δ μ int , 0   0     s . t .   N int T ⋅ Δ μ int , 0 = 0 − K ⋅ ( 1 + sgn ( v int , 0 ) ) − sgn ( v int , 0 ) ≤ Δ μ int , 0                             Δ μ int , 0 ≤ K ⋅ ( 1 − sgn ( v int , 0 ) ) − sgn ( v int , 0 )                             K   large   ( e . g . ,   10 3 ) 
 v 0  is loopless if the previous LP problem admits a solution for Δ μ int,0 . If the flux solution is infeasible, LR finds the nearest feasible flux solution  v * employing the following MIQP formulation ( Schellenberger  et al. , 2011 ),
 (9) LR :  min ( δ , int * , v * , Δ μ int * )   ‖ δ ‖ 2     s . t .   v * − v 0 = δ           { mass   balance   constraints }                         { loopless   constraints }                           δ ∈ R n 
where  δ  denotes the distance vector from  v 0  to  v *. Thus, the closest feasible flux distribution to  v 0  is found by minimizing the Euclidean norm of  δ .
 Algorithm 1.  Fast SNP Inputs:  Stoichiometric matrix of internal reactions  S int , lower and upper bounds ( lb int , ub int ) Outputs:  Reduced feasible null space basis of the loop matrix  N int Set  k ← 0 ,   P N int , k ← 0 ,   N int , k ← ∅ w  ← GenerateRandomWeights while  k  &lt;  r  do    P N int , k ⊥ ← I n i × n i − P N int , k    v int , k + ← SolvePossitiveConstraintSNV ( P N int , k ⊥ , w )    v int , k − ← SolveNegativeConstraintSNV ( P N int , k ⊥ , w ) Set  v int , k ← argmin ( ‖ v int , k − ‖ 0 , ‖ v int , k + ‖ 0 ) if IsEmpty( v int, k ) then   break else    N int , k + 1 ← N int , k ⊕ v int , k    N ^ int , k + 1 ← GenerateOrthogonalBasisUsingSVD ( N int , k + 1 )    P N int , k + 1 ← N ^ int , k + 1 ⋅ N ^ int , k + 1 T end if Set  k  ←  k  + 1 end while return  N int, k 2.4 Computational implementation Fast-SNP and all the loopless flux formulations were coded and implemented in MATLAB 2015b (The MathWorks, Natick, MA). Our implementation relies on third-party solvers to solve LP, MILP and MIQP problems. Optimization runs were performed using Gurobi Optimizer 6.5 (Gurobi Optimization, Inc., Houston, TX) and IBM ILOG CPLEX 12.5.1 (IBM Corp., NY). All computations were run on a Dell Studio OptiPlex 9020 workstation (Intel Core i7-4790 processor, 16 GB ram memory, 64-bit architecture) running on Windows 7. 3 Results The computational performance of our strategy was compared with the traditional approach in different metabolic models covering a range of sizes. The models considered include the toy model shown in  Figure 1 , an  Escherichia 
 coli  core model (Orth  et al. ,  2010a ), and six GEMs, namely: iAF692 ( Feist  et al. , 2006 ), iJN661 ( Jamshidi and Palsson, 2007 ), iYL1228 ( Liao  et al. , 2011 ), Salmonella Typhimurium Model (STM) ( Thiele  et al. , 2011 ), iJO1366 ( Orth  et al. , 2011 ) and Yeast 6 ( Heavner  et al. , 2013 ). We have assumed growth in minimal medium and growth rate maximization as the objective function for each model. In the case of the toy model, the upper bound of  b 1  was set to 1 and reaction  b 3  was maximized (see  Fig. 1B ). The characteristics of each model after removal of blocked reactions are shown in  Table 1 . The models used in this study can be downloaded from the BiGG Models database at  http://bigg.ucsd.edu  ( King  et al. , 2016 ).
 Table 1. Characteristics of the metabolic models used in this study Model Organism Mets Rxns Rev. rxns Int. rev. rxns Toy model — 5 10 4 3 E.coli  core E.coli 68 87 15 13 iAF692 Methanosarcina barkeri 417 484 42 38 iNJ661 Mycobacterium tuberculosis 579 740 82 77 iYL1228 Klebsiella pneumoniae 830 1223 85 78 STM Salmonella Typhimurium LT2 1086 1597 83 79 iJO1366 E.coli 1136 1679 115 110 Yeast 6 Saccharomyces cerevisiae 756 1018 78 76 3.1 Fast-SNP significantly reduces loop-law constraints enabling efficient ll-FBA computation Fast computation of a reduced feasible loop-law matrix is critical for efficient loopless optimization. Even if a reduced basis accelerates optimization, if its computation is too expensive then the approach is simply not useful. Therefore, we first evaluated the computational performance of our sparsification algorithm in the studied models ( Table 2 ). The computation of a reduced basis was achieved in &lt;25 s for all models and Gurobi was ∼60% faster than CPLEX. The speed depends on both the total number and the number of feasible loop laws. For example, Yeast 6 has considerably less internal reactions and therefore total loop laws than iJO1366 (224 versus 484), but it has almost 3-fold more feasible loop laws than iJO1366 (110 versus 34). In this case, the pre-processing time was similar in these two models using both solvers, highlighting the dependence on both factors. We further compared the performance of Fast-SNP against a previous method for generation of a minimal null space basis ( Hay  et al. , 2014 ). This method is based on the identification of reactions involved in infeasible loops through FVA and extreme pathway analysis. As such, this method would require 2 n  LP iterations + 1 basis computation for the generation of  N int . In contrast, Fast-SNP requires 2 r  LP iterations +  r  basis computations (in the worst case). Considering the greedy nature of Fast-SNP, the high efficiency of SVD algorithms for orthogonal basis generation and that  r  &lt;  n  as  r  =  n  – rank( S int ), Fast-SNP produces a sparse null basis substantially faster (∼10 times) than the latter method ( Supplementary Fig. S2 ).
 Table 2. Pre-processing results for each metabolic model Model Feasible loop laws/ total loop laws No. non-zero entries Fast-SNP N int /no. non-zero entries  N int 
 a Pre-processing time b  (s) Gurobi CPLEX Toy model 2/3 7/10 3.6·10 −2  ± 9.0·10 −3 4.7·10 −2  ± 1.6·10 −3 E.coli  core 1/13 2/80 4.2·10 −2  ± 9.0·10 −3 4.2·10 −2  ± 9.0·10 −3 iAF692 12/64 38/409 5.5·10 −1  ± 9.7·10 −2 8.5·10 −1  ± 1.3·10 −1 iNJ661 21/149 63/1183 1.6 ± 1.7·10 −1 2.6 ± 1.1·10 −1 iYL1228 23/368 76/4635 4.3 ± 8.9·10 −2 8.9 ± 2.4·10 −1 STM 23/446 59/5453 6.5 ± 3.8·10 −1 1.4 ± 4.6·10 −1 iJO1366 34/484 96/6164 12.0 ± 1.2·10 −1 21.6 ± 3.3·10 −1 Yeast 6 110/224 1802/3825 15.14 ± 4.8·10 −1 23.63 ± 3.1·10 −1 a In order to ensure a fair comparison between the two matrices, the reduced row echelon form of  N int T  was employed. b Runtimes are shown as mean ± standard deviation calculated from three independent runs. Fast-SNP also yielded substantially reduced loop-law matrices with fewer feasible loop laws in all GEMs ( Table 2 ). Reduction of GEMs loop laws varied from 50.9% for Yeast 6 to 94.8% for STM, underscoring the impact of known directionalities on the number of feasible loop laws. In addition to shrinking the loop matrices, the algorithm created much sparser matrices. For instance, the computed loop-law matrix for iJO1366 contained 98.5% less non-zero entries compared with a reduced column echelon of the latter. The above results are encouraging for the potential increase in algorithmic efficiency of loopless optimization algorithms. Whether they actually translate into substantial speedups is evaluated next. Pre-processing the loop-law matrix significantly accelerated ll-FBA optimization ( Fig. 2 ). The runtime reductions increased with the size of the model and Gurobi was ∼3-fold faster than CPLEX ( Fig. 2 ). For the largest GEMs (STM, iJO1366 and Yeast 6) a runtime reduction of around 20-fold was observed. Importantly, in all the studied models there was a perfect agreement on the computed optimal values ( Supplementary Fig. S3 ).
 Fig. 2. Optimization performance with and without pre-processing the loop-law matrix. Comparison of ll-FBA runtimes with and without pre-processing using distinct solvers in different models. The error bars represent the standard deviation of three optimization runs. Pre-processing of the loop-law matrix substantially reduces the computation time in large models using both solvers 
 3.2 Reduced loop-law constraints improve computational performance of ll-FVA and related loopless optimization problems The computational improvement of Fast-SNP was further evaluated in ll-FVA setting α = 0.9, i.e. 90% of the optimal growth rate calculated previously. As with ll-FBA, our strategy displays considerably lower runtimes using either solver compared with the conventional approach ( Table 3 ). In general, the greater the reduction in the loop-law constraints, the greater the relative acceleration. For large GEMs, reductions in computation time varied from 7-fold in Yeast 6 using Gurobi (13-fold improvement with CPLEX) up to 50-fold in STM (31-fold improvement with CPLEX). Very promising speedups were also achieved in iJO1366; 41- and 47-fold improvements were obtained using CPLEX and Gurobi, respectively. In the case of medium-scale GEMs, speedups were moderate ranging from 2-fold runtime reduction for iAF692 using CPLEX, to ∼20-fold improvement for iYL1228 using Gurobi. Notably, flux ranges computed with Fast-SNP yielded almost identical results as in conventional ll-FVA with no statistical significant difference ( Fig. 3  and  Supplementary Table S4 ). We further compared the computational improvement of Fast-SNP against recent approaches for efficient loopless flux optimization, namely: fast-tFVA ( Müller and Bockmayr, 2013 ) and CycleFreeFlux ( Desouki  et al. , 2015 ). Overall, Fast-SNP yielded more modest improvements compared with the other approaches though magnitude varied dramatically between problems and solvers used ( Supplementary Tables S2 and S3 ).
 Fig. 3. Flux ranges computed using the reduced loop-law matrix are consistent with the ranges obtained from conventional ll-FVA in the studied models 
 Table 3. Runtime comparison of ll-FVA with and without reduction of loopless constraints Model ll-FVA without pre-processing a ll-FVA with pre-processing a Average runtime fold-change ( t conv ll-FVA / t Fast-SNP ll-FVA ) CPLEX Gurobi CLPEX Gurobi CPLEX Gurobi Toy model 4.7·10 −1  ± 3.0·10 −2 1.1·10 −1  ± 2.4·10 −2 3.1·10 −1  ± 1.0·10 −2 1.4·10 −1  ± 1.6·10 −2 1.7 0.8 E.coli  core 15.1 ± 1.9 3.3 ± 0.2 2.6 ± 3.3·10 −1 1.9 ± 1.3·10 −1 5.8 1.8 iAF692 4.4·10 2  ± 4.7 2.9·10 2  ± 6.3 2.1·10 ± 7.8 69.0 ± 5.6 2.1 4.2 iNJ661 2.9·10 3  ± 24.5 1.6·10 3  ± 40.1 6.4·10 2  ± 30.2 3.1·10 2  ± 5.6 4.5 5.1 iYL1228 2.9·10 4  ± 61.5 1.3·10 4  ± 40.6 1.7·10 3  ± 36.6 6.5·10 2  ± 36.1 17.5 19.3 STM 5.9·10 4  ± 4.9·10 2 2.8·10 4  ± 5.6·10 2 1.9·10 3  ± 41.9 5.7·10 2  ± 15.1 31.1 49.7 iJO1366 8.3·10 4  ± 2.3·10 2 4.2·10 4  ± 42.3 2.0·10 3  ± 7.65 9.0·10 2  ± 19.8 40.7 46.8 Yeast 6 9.2·10 4  ± 2.8·10 2 2.9·10 4  ± 91.0 7.2·10 3  ± 22.3 4.4·10 3  ± 1.9·10 2 12.7 6.5 a Runtimes are shown as mean ± SD calculated from three independent runs. We also evaluated the performance of Fast-SNP in the related loopless flux optimization problems, LD and LR (see ‘Methods’ section). These optimization problems have been respectively proposed for detection and correction of infeasible flux distributions generated using random sampling ( Schellenberger  et al. , 2011 ). Given the large size of samples required for the analysis of GEMs (typically 10 5 ), efficient detection and removal of infeasible cycles is fundamental ( Saa and Nielsen, 2016 ). As for ll-FBA and ll-FVA, pre-processing the loop-law matrix improves runtimes for detection and removal of loops in iJO1366 ( Fig. 4 ). Comparing the performance of LD optimizations with and without pre-processing of iJO1366, runtimes decreased 1.3- and 7.4-fold using Gurobi and CPLEX, respectively. Interestingly, on this occasion CPLEX was faster than Gurobi. The results are even more impressive for the case of LR. Neither Gurobi nor CPLEX were capable of finding a feasible solution within 6 h, when the loop-law matrix was not processed using Fast-SNP ( Fig. 4 ). With Fast-SNP, a solution was found within minutes using either Gurobi or CPLEX. This result highlights that an efficient formulation of the loop matrix might not only be a convenient strategy, but an absolute requirement to solve complex loopless formulations.
 Fig. 4. Reduced loopless constraints generated from Fast-SNP reduce optimization runtimes of other loopless problems in iJO1366. Computational performance in LD and LR problems using flux samples from iJO1366. Error bars represent the standard deviation of five optimization runs. Bars shown with (*) denote cases where it was not possible to compute a solution in &lt; 6 h 
 3.3 Analysis of the reduced loop-law matrix reveals key directional constraints required to avoid infeasible loops A reduced feasible loop-law matrix can also serve as a valuable source of information for unravelling infeasible cycles in genome-scale models. The set of vectors encoded in the rows of  N ˜ int  describes a minimal basis of feasible loop laws allowed by the directionalities in the model, and thus, they can yield useful insights about potential infeasibilities in the model. In the following, we analyzed the topology of these laws in iJO1366 after performing ll-FVA with α = 0.9 (see previous section). Application of ll-FVA yielded 13 blocked reactions and changed the directionality of 728 reactions in iJO1366. Importantly, the resulting model has 18 loop laws out of the initial 34, easing its topological analysis ( Fig. 5 ).
 Fig. 5. Topological exploration of the reduced loop-law matrix obtained from Fast-SNP enables rational elimination of infeasible loops in iJO1366.  (A)  Topological analysis of the reduced loop-law matrix. Reactions indicated with arrows can be either blocked or their directionalities fixed to eliminate infeasibilities.  (B)  Examples of different approaches to remove infeasible loops 
 
 Figure 5A  depicts the complete set of loop laws found in iJO1366 after definition of growth conditions (minimal medium), optimality criterion (at least 90% of optimal growth rate) and thermodynamic feasibility (loopless condition). The set of feasible loop laws covers 45 reactions (11 reversible) involved in ion and metabolite inter-compartment transport, alternate carbon, acetate and glycogen metabolism ( Supplementary Table S1 ). Reactions with the highest participation in loop laws are: NAt3pp (5), PPKr (5), CA2t3pp (4) and Cat6pp (4) ( Supplementary Fig. S4 ). Notably, the latter reaction is also flux-forcing, i.e. it has a strictly non-zero flux, and thus it cannot be removed without rendering the model infeasible. Next, our interest is to analyze the impact of blocking or fixing the directionalities of reactions involved in infeasible cycles. We have selected three illustrative cases to show different strategies for rational analysis and removal of loops ( Fig. 5B ). Case 1 illustrates a simple loop comprised of two duplicated reactions: ASPt2pp (irreversible) and ASPt2rpp (reversible). In this case, the most sensible action is to remove the irreversible reaction (ASPt2pp) and preserve the reversible one (ASPt2rpp). The resulting model has the same capabilities as the original without the artificial loop L 2 . Cases 2 and 3 are more subjective and illustrate different modelling rationales. In Case 2, if we are not interested in pyruvate alternate metabolism, reaction HPYRI can be removed from the model effectively eliminating L 1 . Case 3 illustrates that loops can be also eliminated by fixing the directionalities of reversible reactions rather than deleting a reaction. L 7  consists of a loop where the succinyl-CoA synthetase operates in reverse. However, under fast-growing glucose-limited aerobic conditions, this reaction proceeds in the forward direction in  E. coli  ( Ishii  et al. , 2007 ). By fixing its direction to forward, L 7  is eliminated as no other reactions in this cycle can be reversed ( Fig. 5B ). In summary, the topology encoded in the reduced loop-law matrix facilitates rational infeasibility analysis of large metabolic models. 4 Discussion Computation of loopless steady-state flux solutions is an NP-hard MILP problem ( Müller and Bockmayr, 2013 ). Recent approaches such as fast-tFVA ( Müller and Bockmayr, 2013 ) and CycleFreeFlux ( Desouki  et al. , 2015 ) have exploited particular features of the problem and used LP formulations following alternative approaches to solve different loopless flux optimization problems. Although these approaches enabled fast computation of feasible fluxes (acceleration up to two orders of magnitude), they rely on post-processing methods for blocking infeasible cycles after steady-state flux computation. In the current study, we developed a pre-processing strategy where the conventional MILP problem is reformulated using a minimal set of loop-law constraints, enabling efficient computation of loopless flux solutions as well as topological analysis of infeasible cycles in large metabolic models. All modern MILP solvers use pre-processing to recast MILP formulations into easier-to-solve, yet equivalent representations ( Savelsbergh, 1994 ). It is often possible to develop superior pre-processing strategies using insights into the specific MILP problem. We have developed a fast pre-processing algorithm, Fast-SNP, which finds a minimal set of feasible loop laws taking into consideration the directionalities of the model. Once the reduced loop-law matrix is found, a smaller loopless MILP problem can be formulated and solved efficiently. Speedups up to 50-fold were achieved with pre-processing across diverse types of loopless flux optimization problems ( Tables 2  and  3 ,  Figs 2 and 4 ). Although the computational improvement of Fast-SNP is more modest than previous approaches ( Supplementary Tables S2 and S3 ), it is the only strategy capable of efficiently solving loopless related problems (i.e. ll-FBA, ll-FVA, LD and LR) using a single optimization problem based on suitable LP, MILP or MIQP formulations subject to the loopless constraints ( Equation 5 ). Importantly, such constraints have recently been algebraically proven to always yield thermodynamically feasible flux solutions ( Noor  et al. , 2012 ). Finally, we highlight additional and distinctive features of Fast-SNP for the analysis of metabolic models. As shown in the previous section, this approach enables a priori assessment of model complexity in the ‘loop law sense’, as it explicitly computes a minimal basis describing all the potentially infeasible cycles in the studied model. Formally, these loop laws represent elementary flux modes that do not exchange mass with the surroundings ( Schuster  et al. , 2000 ). These internal cycles have previously been studied in the context of oriented matroid theory ( Beard  et al. , 2004 ;  Oliveira  et al. , 2001 ) and more recently, infeasible cycles have been enumerated in GEMs using sampling approaches ( De Martino  et al. , 2013 ). However, efficient assessment of the impact of directionality constraints as well as topological analysis of these loop laws for their potential removal, has been lacking in the literature. Fast-SNP aids comprehensive assessment, unravelling simple strategies for removing infeasible loops based on thermodynamic or practical considerations (see  Fig. 5 ). Additionally, we note that the proposed matrix sparsification algorithm in Fast-SNP can also be employed for other useful analyses. For example, this algorithm can readily be modified to explore alternative solutions by incorporating suitable additional constraints (e.g.,  v i  ≥  z *) using an appropriate objective (e.g.  l 1 -norm minimization). The resulting greedy algorithm would generate a set of linearly independent solutions or pathways (i.e. a basis) consistent with the previous constraints by solving only LP problems. This avoids the need of computing expensive MILP formulations. In conclusion, Fast-SNP complements existing methods in the constrained-based modelling toolbox for exploration and topological analysis of infeasible cycles and metabolic pathways. 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Exploring the structure and function of temporal networks with dynamic graphlets</Title>
    <Doi>10.1093/bioinformatics/btw310</Doi>
    <Authors>Hulovatyy Y., Chen H., Milenković T.</Authors>
    <Abstract/>
    <Body>Bioinformatics , (2015) 31(12):  i171–i180  doi: 10.1093/bioinformatics/btv227 The authors wish to correct the following error in the above article: in the legend of figure 1, a sentence in the legend reads, ‘But there are two orbits in graphlet  G 2 , as the two end nodes are topologically identical to each other but not to the middle node (and vice versa)’, this should be corrected to, ‘But there are two orbits in graphlet  G 1 , as the two end nodes are topologically identical to each other but not to the middle node (and vice versa)’. The authors apologize for this error. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Utilizing de Bruijn graph of metagenome assembly for metatranscriptome analysis</Title>
    <Doi>10.1093/bioinformatics/btv510</Doi>
    <Authors>Ye Yuzhen, Tang Haixu</Authors>
    <Abstract>Motivation: Metagenomics research has accelerated the studies of microbial organisms, providing insights into the composition and potential functionality of various microbial communities. Metatranscriptomics (studies of the transcripts from a mixture of microbial species) and other meta-omics approaches hold even greater promise for providing additional insights into functional and regulatory characteristics of the microbial communities. Current metatranscriptomics projects are often carried out without matched metagenomic datasets (of the same microbial communities). For the projects that produce both metatranscriptomic and metagenomic datasets, their analyses are often not integrated. Metagenome assemblies are far from perfect, partially explaining why metagenome assemblies are not used for the analysis of metatranscriptomic datasets.</Abstract>
    <Body>1 Introduction Metagenomes are being generated at an accelerating pace, revealing important properties of microbiomes. Other meta-omic (e.g. metatranscriptomic and metaproteomic) techniques can provide additional insights, in particular into functional characteristics of microbial communities, such as gene activities and their regulatory mechanisms. Bacteria have low inventories of short-lived mRNAs; as such, fluctuations in their mRNA pools provide a highly sensitive bioassay for environmental signals [e.g. the concentrations of dissolved organic carbon ( Shi  et 
 al. , 2012 ) and pollutant concentrations ( de Menezes  et 
 al. , 2012 ) relevant to microbes ( Moran  et 
 al. , 2013 )]. The acquisition of meta-omics data on human microbiomes will enable us to refine the annotations of the metagenomes [the ENCODE ( Dunham  et 
 al. , 2012 ) and modENCODE ( Roy  et 
 al. , 2010 ) projects are great exemplars], and more importantly to study gene activity and its regulation ( Maurice  et 
 al. , 2013 ) in complex microbial communities in order to understand how microbial organisms work as a community in response to changes in their environment, e.g. health conditions of their human hosts ( Jorth  et 
 al. , 2014 ). A recent metatranscriptomic study of the human oral microbiome using patient-matched healthy and diseased (periodontal) samples revealed that health- and disease-associated communities exhibit defined differences in metabolism that are conserved between patients while the metabolic gene expression of individual species was highly variable between patients ( Jorth  et 
 al. , 2014 ). In a metatranscriptomic RNA-seq study, total RNA is first isolated from the sample (with rRNAs removed to enrich for mRNA), which is then reverse transcribed into cDNA, and subjected to sequencing using next-generation sequencing platforms ( Gosalbes  et 
 al. , 2011 ). Unlike metagenomics, which reveals potential activity (as reflected in genes or pathways that can be coded for by metagenomic sequences), metatranscriptomic data indicate which of the genes/metabolic pathways are actually active (and the level of their activities) on the basis of their transcription within the community.  Giannoukos  et al.  (2012)  presented a protocol for metatranscriptomic analysis of bacterial communities that accommodates both intact and fragmented RNA and combines efficient rRNA removal with strand-specific RNA-seq. Currently, only a handful of metatranscriptomic datasets are available (and metaproteomic datasets are even scarcer), but we envision a flood of metatranscriptomic data in the near future, as experimental techniques mature ( Franzosa  et al. , 2014 ;  Giannoukos  et 
 al. , 2012 ). Metatranscriptome analyses typically include the assignment of the predicted function and taxonomic origin of RNA-seq reads, by directly searching metatranscriptomic sequences (bags of reads) against prokaryotic genomes (the reference genomes) ( Leimena  et 
 al. , 2013 ) or known protein sequences ( Franzosa  et 
 al. , 2014 ). This way, tools and pipelines—including MG-RAST ( Meyer  et 
 al. , 2008 ), MEGAN ( Huson  et 
 al. , 2011 ) and HUMAnN ( Abubucker  et 
 al. , 2012 )—that have been developed for metagenome data analysis can be utilized for analyzing metatranscriptomic datasets. For example,  Franzosa  et al .  (2014)  analysed metagenomic and metatranscriptomic datasets of human gut microbiomes using the HUMAnN pipeline, revealing that metatranscriptional profiles were significantly more individualized than DNA-level functional profiles. One potential pitfall of such approaches is that they cannot identify transcripts of new genes, which however may be better annotated using assembly approaches ( de novo  or reference based). A recent study ( Celaj  et 
 al. , 2014 ) compared the performances of currently employed transcriptome assemblers—including Trinity ( Grabherr  et 
 al. , 2011 ), Oases ( Schulz  et 
 al. , 2012a ), Metavelvet ( Namiki  et 
 al. , 2012 ) and IDBA-MT ( Leung  et 
 al. , 2013 )—and showed that assembly helps to improve the rate of functional annotation for metatranscriptomic datasets. A matched metagenome can be helpful for the analysis of metatranscriptomic dataset. Metagenomes are often represented as contigs and scaffolds (although de Bruijn graphs are often the underlying data structure of the assemblers that were used), and are fragmented, limiting the utilization of metagenome for metatranscriptome analysis. There are pros and cons with the contig (and scaffold) representations of metagenomes. Most existing computational tools for sequence analysis work with linear representations of assemblies, so these tools (or modified versions) can be employed to analyse these representations of metagenomes. However, metagenomes are often very fragmented, and the connections between contigs or scaffolds are not captured in linear representations, which otherwise could be utilized later. For example, after we assembled two metagenomic datasets of stool samples from the Human Microbiome Project ( Huttenhower  et 
 al. , 2012 ), the total lengths of scaffolds and contigs (≥300 bp) reported by SOAPdenovo2 ( Luo  et 
 al. , 2012 ) were about 85 and 90 Mb, respectively, whereas the total length of the edge sequences in the de Bruijn graph from the same assembly was 150 Mb for each. This comparison indicates that the de Bruijn graph representation of the assembly contains 50% more sequences than scaffolds reported from the assembler: most of these extra sequences are relatively fragmented sequences connecting long contigs. Furthermore, many short contigs contain only gene fragments; even long contigs contain broken genes at their ends due to the complexity of metagenome assembly ( Wu  et 
 al. , 2012b ). Here, we propose a novel application of de Bruijn graphs for metatranscriptomic data analyses, taking advantage of the fact that de Bruijn graph representations of metagenome assemblies contain more information than the contigs and scaffolds reported by assemblers. The de Bruijn graph was first proposed for  de novo  genome assembly in EULER, replacing the traversal of Hamiltonian paths in the overlap graph by the traversal of Eulerian paths ( Pevzner  et 
 al. , 2001 ), and is now employed as an efficient data structure in most short-read assemblers [e.g. Velvet ( Zerbino and Birney, 2008 ), ALLPATHS-LG ( Gnerre  et 
 al. , 2011 ), SOAPdenovo ( Li  et 
 al. , 2010 ) and IDBA-UD ( Peng  et 
 al. , 2012 )] for single genomes and metagenomes. Our approaches based on de Bruijn graph representation of metagenomes provide a natural way of compressing the data, and, more importantly, allow direct utilization of the graphs. We note that we have developed several applications previously, based on de Bruijn graph representation of genomes and metagenomes, for mining of functional elements ( Wu  et 
 al. , 2012a ) and reads mapping ( Wang  et 
 al. , 2012 ), demonstrating the utility of direct computation on de Bruijn graphs. Application of our method to simulated and real metatranscriptomic datasets showed that our approach can significantly improve the assembly of metatranscriptomic datasets, resulting in substantially more transcripts that otherwise would have been missed or truncated because of the fragmented nature of the reference metagenome. 2 Methods In this article, we propose a novel algorithm (i.e.  read2graph ) for aligning short reads from RNA-seq experiments to de Bruijn graphs of assemblies. We note in this article we focused on de Bruijn graphs of metagenome assemblies, but the mapping algorithm can be applied to mapping short reads to any de Bruijn graph of assembly. We also developed an application of the mapping algorithm for metatranscriptome assembly using matched metagenomes as the reference. Based on reads mapping results, we will derive putative transcripts (encoding a single bacterial gene or multiple genes within an operon), using paired-end RNA-seq reads to traverse the de Bruijn graph. We named our transcript assembly approach TAG, in which TA stands for Transcript Assembly, and G is used to emphasize the fact that our approach utilizes the graph of metagenome assembly instead of the linear sequences. We note that our method is different from the  de novo  approaches to transcriptome assembly, including Trinity ( Grabherr  et 
 al. , 2011 ), IDBA-MT ( Leung  et 
 al. , 2013 ) [and also a hybrid approach ( Leung  et 
 al. , 2014 ) that utilizes known protein sequences], and that it is different from the traditional reference-based assembly approaches. In our method, metatranscriptomic sequences are mapped onto matched metagenomes represented as de Bruijn graphs. So our method represents a new variant of the reference-based approaches, which uses the de Bruijn graph of matched metagenome, instead of a genome (or a collection of genomes), as the reference. 2.1 Fast reads mapping onto de Bruijn graph using a hash table of  k -mers spanning branching structure in the graph Given a de Bruijn graph, more exactly, a  contracted  de Bruijn graph ( Cazaux  et 
 al. , 2014 ;  Chang  et 
 al. , 2015 ), in which each edge represents an assembled unique sequence from metagenomic reads, and a set of short reads from an RNA-seq experiment, the goal of our read2graph algorithm is to find the location of each read on the graph. Because bacterial genes do not have split gene structure, we can assume each read should be contained in the graph as a whole; equivalently, each read, if its location in the graph is known, can be represented as a path (i.e. sequence of edges) in the graph. The reads, therefore, can be classified into two groups depending on the path length: some reads are located within a single edge, whereas many others may cross one or more vertices in the graph. The first class of reads can be mapped to the graph using conventional fast reads mapping algorithms by using all edge sequences longer than the read length as the target sequences. In this article, we used Bowtie 2 ( Langmead and Salzberg, 2012 ) for this purpose; but other mapping algorithms including BWA ( Li and Durbin, 2009 ) can be used. Here, we focus on the methods for mapping reads spanning multiple edges (i.e.  multi-edge-spanning reads ; see  Fig. 1 ), which cannot be mapped using conventional mapping algorithms. A substantial number of reads may belong to this class, due to the incompleteness of metagenome assembly.
 Fig. 1. A schematic illustration of the algorithm for mapping reads onto de Bruijn graphs. ( a ) A toy example showing four reads spanning  junction k-mers  in the graph (shown as the vertices). ( b ) Using a hash table of junction  k -mers, candidates of reads that span multiple edges can be retrieved by looking up in the table. ( c ) For each candidate, a matched  k -mer determines a unique putative location of the read in the graph (i.e. a seed match). The seed match will then be used to constrain the alignment between the read and the graph by a dynamic programming algorithm 
 Recall that each vertex in the de Bruijn graph represents a  k -mer in metagenomic reads [typically  k  = 23–31 for metagenome assembly ( Huttenhower  et 
 al. , 2012 ;  Qin  et 
 al. , 2010 )]. Therefore, as illustrated in  Figure 1 , each multi-edge-spanning read contains one or more junction  k -mers (i.e. corresponding to vertices with either indegree or outdegree &gt;1): reads A and D span three edges in the graph, and thus each contains two such  k -mers, whereas reads B and C span two edges, and thus each contains one such  k -mer ( Fig. 2 a). Hence, we can build a hash table for all  junction k-mers  that span branching structures in the de Bruijn graph assembly and then search for their exact occurrences in each putative multi-edge-spanning read (i.e., those that cannot be mapped to the edge sequences) with the assistance of the hash table ( Fig. 1 b). Because each  k -mer in the de Bruijn graph is unique ( Pevzner  et 
 al. , 2001 ), every  k -mer in a read matches at most one  k -mer stored in the hash table. Each matched  k -mer determines a unique putative location of the multi-edge-spanning read in the graph (i.e. a seed match between the read and the graph), and simultaneously breaks the read into two or more segments ( Fig. 1 c). The seed match will then be used to constrain the alignment between the read and the graph, starting from the seed match, going in opposite directions, using a constrained dynamic programming algorithm allowing only a small number of indels and mismatches. The bandwidth for constrained alignment is set to 7 by default for metatranscriptome assembly using matched metagenome as the reference, and this parameter can be changed by users for other purposes.
 Fig. 2. A schematic example illustrating the induced transcript graph derived from four reads (A–D) mapped to a de Bruijn graph of metagenome assembly 
 The mapping of multi-edge-spanning reads should run fast and consume reasonable memory because usually there are only hundreds of thousands of junction  k -mers in a typical metagenome assembly in practice. We note that the multi-edge-spanning reads considered here are different from the split reads considered in transcript assembly for eukaryotes ( Grabherr  et 
 al. , 2011 ), and in rare cases for archaeal species (due to tRNA splicing and self-splicing introns) ( Doose  et 
 al. , 2013 ). Since strand-specific RNA-seq protocols are often used in metatranscriptome analysis ( Giannoukos  et 
 al. , 2012 ), our algorithm can consider the strand information and map reads to one appropriate strand in the de Bruijn graph that contains sequences from both DNA strands (and thus is symmetric). 2.2 Construction of transcripts from mapped reads Once all RNA-seq reads including multi-edge-spanning reads are mapped to the graph, each read can be represented by a path (referred to as the  read path ) traversing the graph  &lt; e 1 , e 2 , … , e l &gt;  ( e 1 , e 2 , … , e l  are edges; for non-multi-edge-spanning reads, path length  l  = 1) as well as two offset values representing the locations of the read in the first and last edges in the graph. Furthermore, in most cases, two paired-end reads can also be represented as a path (i.e. the  read-pair path ) if there exists a unique path in the graph whose length is consistent with the expected insert size. As a result, the assembly of RNA-seq reads is equivalent to the superpath problem, which attempts to find a minimal set of superpaths (each corresponding to a transcript) that covers a given set of paths in a de Bruijn graph ( Nagarajan and Pop, 2009 ). Although this problem is generally hard, we can represent the solutions of the problem in a much simpler subgraph (the transcript graph) that contains only the edges present in at least one of the read paths or read-pair paths.  Figure 2  shows such an example: assuming four read paths (A, B, C and D) are derived from multi-edge-spanning reads, we will induce the transcript graph by retaining all edges in these paths, and then contracting all vertices with both indegree and outdegree of 1. We note that many read paths may contract into a single edge in the transcript graph if they are not tangled with reads from another transcript (e.g. k1 and k4 in  Fig. 2  can be contracted because there are no conflicting transcript reads traversing through these nodes); as a result, the corresponding transcript sequences can be retrieved as a subsequence of an edge sequence in the transcript graph. In other cases, read paths remain spanning multiple edges in the transcript graph. These read paths sometimes can be used to further simplify transcript graph, as illustrated in the heuristic algorithms in genome assembly ( Pevzner  et al. , 2001 ;  Zerbino and Birney, 2008 ). For instance, in the example shown in  Figure 2 , if we have two read-pair paths spanning AC and BD, respectively, we can obtain two resolved transcripts from the graph. Otherwise, we can only obtain partial transcript sequences. We note that, even if the transcripts cannot be fully resolved, the transcript graph is still useful for inferring the abundances of putative transcripts in a metatranscriptome sample based on the counts of reads on the edges in the transcript graph, a problem similar to the inference of splicing variants in eukaryotic RNA-seq experiments (see  Pachter, 2011  for a review). 2.3 Metatranscriptome assembly using metagenome assembly graph as the reference Our approach for metatranscriptome assembly (called TAG) is based on the read2graph mapping algorithm and the transcript construction approach as described above. Given a metatranscriptomic dataset and a matched metagenomic dataset, SOAPdenovo2 ( Luo  et 
 al. , 2012 ), one commonly used assembler in metagenomic shotgun sequencing, is used to assemble the metagenomic dataset. Notably, SOAPdenovo2 is a de Bruijn graph-based assembler, and in its final output, both the de Bruijn assembly graph and the contig sequences (representing the  edges  in the graph) are produced. The mapping of metatranscriptomic sequences to the de Bruijn graph is conducted in two consecutive steps: (1) all reads are first mapped to the edges (i.e. contigs) in the de Bruijn graph using Bowtie 2 (version 2.2.3) ( Langmead and Salzberg, 2012 ), and then, (2) the un-mapped reads in the previous step are further mapped to the graph based on the matching with junction  k -mers. Next, TAG traverses the de Bruijn graph along with the mapped metatranscriptomic reads, and reports the transcripts that may span multiple edges in the assembly graph. To use the strand-specific information, the mapping of a metatranscriptomic sequence is only considered for the strand of the read that represents the transcript (i.e. the forward strand of R2 reads and the reverse-complement strand of R1 reads for the datasets we have tested). We note that other short read assemblers [such as IDBA ( Peng  et 
 al. , 2012 )] and mapping tools [such as BWA ( Li and Durbin, 2009 )] can be utilized for generating the inputs (i.e. the de Bruijn assembly graph and the mapping of metatranscriptomic reads to contigs) for TAG. For the rest of this article, we will focus on the utility of TAG on improving the assembly of transcripts, which will be demonstrated by using the SOAPdenovo2 and Bowtie2 tools. The construction of an optimal pipeline (in particular the selection of upstream software tools) utilizing TAG is beyond the scope of this article. 3 Results We tested our tool (TAG) on two metatranscriptomic datasets ( Giannoukos  et 
 al. , 2012 ): one derived from a mock microbial community consisting of three bacterial species, and the other derived from a real microbiome sample in human stool. Results showed that our graph-based reads mapping algorithm (read2graph) is efficient, and TAG, which is based on the mapping algorithm, significantly improves the assembly of metatranscriptomes by considering reads mapping to branching structures in de Bruijn graphs of matched metagenomes. 3.1 Evaluation of assembly accuracy on a mock dataset We first tested TAG using a metranscriptomic data from the mock bacterial community of three species ( Giannoukos  et 
 al. , 2012 ). The ‘matched’ metagenomic dataset used in TAG were simulated from the reference genomes of these bacteria [ Escherichia 
 coli  (GenBank: NC_000913.3),  Perkinsus  marinus  (GenBank: NC_005072.1) and  Rhodobacter  sphaeroides  (GenBank: NC_007493.2)] using NeSSM ( Jia  et 
 al. , 2013 ) with the Illumina error model. We used this  hybrid  approach here because (1) there is currently no metatranscriptomic dataset from a mock community with a matched metagenomic dataset available, and (2) there is no proper software tool for simulating metatranscriptomic dataset. (Flex Simulator is a tool for simulating RNAseq data for single species, and it has been mainly used for eukaryotic species. Bacteria have complicated transcription regulation mechanisms, which are not completely understood.) In total, 1 M paired-end reads of length 101 bp (i.e. ∼20 × coverage) were simulated from the three species with equal abundances. SOAPdenov2 (version 2.04-r240) ( k  = 31; see below for the choice of  k -mer size) was used to assemble the simulated reads, and the assembly results (including the contigs and the de Bruijn assembly graph) were then used as the inputs to TAG. Because this is a simple community with bacterial species that are phylogenetically distant ( Giannoukos  et 
 al. , 2012 ), the assembly graph of the metagenome is not very tangled, and thus we do not anticipate that many transcripts reported by TAG will span multiple edges (referred to as the  multi-edge transcripts ) in the assembly graph. In fact, TAG reported a total of 9428 transcripts (of ≥100 bp), among which only 138 are multi-edge spanning transcripts. 3.1.1 Accuracy evaluation for the TAG transcripts We blasted transcripts assembled by TAG against the three reference genomes to evaluate the accuracy of metatranscriptome assembly. Our results showed that only 16 out of 9428 (0.17%) transcripts cannot be perfectly aligned back to the reference genomes: among the 16 transcripts, 14 can be aligned with minor differences, and only two contain potentially serious problems (see  Table 1 ). We note that there are two types of potential errors in the transcripts assembled by TAG: the errors introduced by TAG, and the errors inherited from the metagenome assembly (i.e. the mis-assemblies present in the metagenome assembly that propagates into the transcript). One of the problematic transcript is single-edge transcript, suggesting that this assembly error was propagated from the metagenome assembly. The other problematic transcript (of 390 bp) is a multi-edge spanning transcript, and the error was introduced by TAG (as no matching sequence can be found in the metagenome assembly). Our results suggest that TAG achieves high assembly accuracy overall with an error rate of &lt;&lt;1%. If we only focused on multi-edge spanning transcripts (which are more difficult to assemble than transcripts contained within edges and therefore more error prone), the assembly error rate is still very low: only one out of 138 multi-edge transcripts contains such large assembly problem (the error rate is 0.7%).
 Table 1. Performance comparison of TAG and other assemblers on the mock dataset Oases Trinity TAG No. of transcripts a 12598 24804 b 9428 Perfectly aligned transcripts (percentage) c 5483 (43.5%) 12392 (50.0%) 9412 (99.8%) Transcripts with minor problems (percentage) c 2724 (21.6%) 2725 (11.0%) 14 (0.15%) Problematic transcripts (percentage) c 4391 (34.9%) d 9687 (39.1%) d 2 (0.02%) Total length of the transcripts 6860841 bp 7428187 bp 7020975 bp Total length of perfectly aligned transcripts 2265224 bp 3858486 bp 7002290 bp Total length of good transcripts 4076481 bp 5025072 bp 7020484 bp a Only transcripts of at least 100 bp were considered for all programs. b Trinity has many more transcripts, but their total length is comparable to the other methods. c A transcript that is perfectly aligned to one of the reference genomes (with an alignment covering the entire transcript at 100% sequence identity) is considered to be correctly assembled. We consider the problem of a transcript is ‘minor’ if its longest alignment with the reference genomes is not 10 nt shorter than the transcript and the alignment has 95% sequence identity or better. Other transcripts that do not meet these criteria are considered to be problematic. d A large fraction of the problematic transcripts for Oases and Trinity are likely caused by the presence of contaminated sequences or other artifacts so should not be considered as mis-assemblies. For example, 3494 (out of 4391) Oases transcripts have no significant alignments with the reference genomes with  E -values better than 1e − 4, and therefore are unlikely transcripts from the reference genomes. 3.1.2 Comparison with de novo assembly We further compared the performance of TAG with Oases (version 1.2.10) ( Schulz  et 
 al.,  2012a ) and Trinity (release 2014-07-17) ( Grabherr  et 
 al.,  2011 ),  de novo  assemblers for transcriptomic sequences. ( Trinity has been applied to analyse metatranscriptomic datasets ( Celaj  et al. , 2014 ), although the program was developed targeting splicing isoforms in Eukaryotes. ) For Oases, we used merged results from assemblies using  k -mer sizes ranging from 19 to 31.  Table 1  summarizes the comparison results. Although Oases and Trinity produced larger numbers of transcripts than TAG, the total bases in the transcripts assembled by these three methods are comparable (i.e. TAG assembled longer transcripts). If we considered only the ‘good’ transcripts by excluding the transcripts that cannot be aligned well to the reference genomes [which are likely misassembles, or assemblies from contaminated sequences or other artifacts commonly found in RNA-seq experiments ( Lahens  et 
 al. , 2014 )], the difference in the total lengths of transcripts is even more significant. TAG produced a total of 9426 good transcripts with a total of 7020484 bp, while Oases and Trinity assembled transcripts of 407648 and 5025072 total bases, respectively. This result shows that using reference genomes for metatranscriptome assembly helps to improve the coverage and quality of the assemblies. We ran CD-HIT-EST (version 4.6) ( Li and Godzik, 2006 ) to cluster the good transcripts from all programs at 95% sequence identity cutoff (−c 0.95), resulting in 10  944 clusters: only a modest number of clusters (2309) are shared by all methods, 2965 clusters are shared by two methods (1399 shared Trinity and Oases; 1369 by Trinity and TAG; and 116 by TAG and Oases), and the remaining clusters are unique to one method (TAG: 2571, Trinity: 2983, and Oases: 197). We quantified the abundances of the transcripts by mapping metatranscriptomic sequences onto the transcripts using Bowtie2. The transcripts that are shared by all methods are highly abundant with an average coverage of 28.2 (i.e. on average, each position is covered by 28.2 reads). In contrast, the average abundances of the transcripts that can be assembled by Oases, Trinity and TAG are 11.4, 8.0 and 6.4, respectively. This result suggests that  de novo  assembly and reference-based approaches can complement each other: transcripts of highly expressed genes in rare species (and therefore less well represented in metagenomes) may be assembled by  de novo  assembly, while transcripts of low expression level can be better identified using reference-based approaches. 3.2 Application of TAG to a real metatranscriptomic dataset We applied TAG to analysing a metatranscriptomic dataset derived from a human stool sample, using its matched metagenomic dataset as the reference ( Giannoukos  et 
 al. , 2012 ). (We combined the metatranscriptomic reads from four fractions of sequencing of the same sample, downloaded from SRA (SRX130930, SRX130937, SRX130922 and SRX130928), and the metagenomic reads from four fractions of sequencing, also downloaded from SRA (SRX130930, SRX130954, SRX130936 and SRX130949). Note that we used the metatranscriptomic dataset sequenced on 5 µg RNA extracted from an individual’s stool microbiome, which was shown to yield the best sequencing results ( Giannoukos  et al. , 2012 ).) As described above, the metagenomic sequences were first assembled using SOAPdenovo2, and the metagenome assembly was then used as the reference for the metatranscriptome assembly by TAG. 3.2.1 Time and memory cost of the reads mapping to the de Bruijn graph Metatranscriptome assembly by TAG (including reads mapping onto the graph and the transcript inference afterwards) for this dataset takes about 7 min to complete on a Linux computer with Intel(R) Xeon(R) CPU E5-2680 v2 @ 2.80 GHz (using single processor). The actual reads mapping step takes about 1 min to complete—the remaining 6 min were spent on other I/O steps including processing the input SAM alignment file (from Bowtie2) and reads files. This indicates that our graph-based reads mapping algorithm (read2graph) is efficient. TAG adds only a small amount of computational time to the whole pipeline for the metatranscriptome analysis—SOAPdenovo2 takes several hours to assemble the metagenome, and mapping metatranscriptomic reads onto the metagenome contigs by Bowtie2 takes about 1700 CPU minutes (the actual job was done in parallel using 32 processors). The memory consumption by TAG is bounded by the size of the input metagenome assembly (which is used as the reference). TAG consumed &lt;2 GB RAM for the stool dataset. It shows another advantage of using metagenome assembly as the reference for metatranscriptome analysis, since de Bruijn graph provides a compact representation of the metagenome assembly (but still keeps the uncertainties of the assembly in the graph for future applications). 3.2.2 Exploiting tangles in de Bruijn graph to improve metatranscriptome assembly We tested the performance of TAG using reference metagenomes assembled with different  k -mer sizes, considering that the choice of  k -mer size is important for the metagenome assembly ( Li  et 
 al.,  2010 ;  Zerbino and Birney, 2008 ) and therefore metatranscriptome assembly. As shown in  Figure 3 , when a relatively small  k -mer (e.g. 25) was used, the metagenome assemblies are more tangled, and as a result, fewer transcripts can be assembled using the contigs as the reference. This pitfall, however, can be alleviated by retaining the tangled structure (i.e. the ambiguous connection caused by short repeats) in the metagenome assembly in the de Bruijn graph, which can be exploited by TAG to connect metatranscriptomic reads into complete transcripts, resulting in improved assembly of metatranscriptome.
 Fig. 3. The impact of  k -mer size on the performance of TAG. When the  k -mer size increases from 25 to 31 in SOAPdenovo2 assembly, the performance of TAG remains the same: a substantial fraction of multi-edge transcripts can be assembled by TAG. However, when further increasing the  k -mer size to 35, most transcripts assembled by TAG are single-edge transcripts, indicating the TAG algorithm is not effective when a large  k -mer is used. This is probably because, in this case, the metagenome assembly is fragmented rather than tangled, and as a result the total length of the transcript also decreases. Therefore, in the experiments of this article, we choose  k  = 31 in SOAPdenovo2 assembly, which seems to yield the best results here 
 As shown in  Figure 3 , the total length of the assembled transcripts by TAG decreases slowly when  k -mer size increases from 25 to 31. Considering that most transcripts are longer at  k -mer = 31 as compared with smaller  k -mers (e.g. average lengths of the transcripts are 264 and 273 for  k -mer = 25 and 31, respectively), we selected  k -mer = 31 to demonstrate the improvement of metatranscriptome assembly by using TAG.  Figure 4  shows the distribution of the path lengths (i.e. the number of edges that are traversed in the de Bruijn graph to form a transcript by TAG) of the transcripts assembled by TAG: most of the multi-edge transcripts span two edges (contigs), although a single transcript may span as many as seven edges.
 Fig. 4. The path length distribution for  multi-edge-spanning reads  that span two or more edges when mapped to the de Bruijn graph by TAG. The  X -axis represents the length of multi-edge-spanning read paths (i.e. the number of edges that the multi-edge-spanning reads span) and the  Y -axis represents the total number of multi-edge-spanning reads spanning the paths of certain lengths. Paths of length 1 represent the cases when the seed extension in one direction resulted in an alignment of at most 7 bp, and thus were considered insignificant and discarded 
 Table 2  summarizes the metatranscriptome assembly results by TAG. A majority of the metagenomic reads can be mapped to metagenomic assembly: for 68.8% of read pairs, both reads can be mapped to contigs by Bowtie2, whereas an additional 13.6% reads can be mapped to contigs although their mate-pairs cannot be mapped. Among the ≈9.8 M  remaining unmapped reads, ≈1.9 M  (18.9%) can be mapped to multiple edges (i.e. through one or more junction  k -mers in the de Bruijn graph) by TAG. Thanks to these reads, TAG was able to improve the metagenomic assembly significantly. In total, TAG assembled about 177K transcripts, among which about 21K (15.7%) are multi-edge transcripts. These multi-edge transcripts cannot be fully assembled if only those reads mapped to contigs are considered in the metatranscriptome assembly; instead, they are likely to be broken into  partial  transcripts, each contained in a separate contig (i.e. the edge in the de Bruijn graph). We note that TAG did not resolve all transcripts. A small fraction of TAG-assembled transcripts are  partial  transcripts, each of which represents a unique edge in the tangled  transcript graph , formed by two or more transcripts sharing some common segments (see Section 2 for details) that cannot be resolved without additional information. About 2.6% (552 out of 21455) of the multi-edge transcripts were not fully resolved by TAG and remained as partial transcripts. Similarly, 2.2% (2573 out of 115100) of the single-edge transcripts are also partial transcript as some multi-edge-spanning reads connect them with other partial transcripts, although their actual connections remain ambiguous. We note that these two numbers increase substantially (to 21.1 and 8.1%, respectively) when there is no minimum length applied for output transcripts.
 Table 2. Some statistics of TAG assembly on the human stool metatranscriptomics dataset Total number of reads 27962127 × 2 (paired) Number of reads mapped to contigs 19233474 × 2 + 7645742 (single) Number of multi-edge-spanning reads 1893157 Number of  resolved a  single-edge transcripts (length) 112527 (32216351 bp) Number of  partial 
 a  single-edge transcripts (length) 2573 (340276 bp) Total number of  single-edge b  transcripts (length) 115100 (32556627 bp) Number of  resolved  of multi-edge transcripts (length) 20903 (4596622 bp) Number of  partial  multi-edge transcripts (length) 552 (110063 bp) Total number of  multi-edge b  (length) transcripts (length) 21455 (4706685 bp) Total number of transcripts (length) 177463 (40456052 bp) Proportion of multi-edge transcripts (in length) 15.7% (11.6%) Only transcripts of at least 100 bp were considered in this summary. a Partial transcripts: the transcripts that are not fully resolved by TAG (i.e. the edge sequences); Resolved transcripts: the transcripts that are resolved by TAG and therefore likely represent full-length transcripts. b Single-edge transcripts: the transcripts reported by TAG that are fully contained within edges (contig) in the de Bruijn graph of the metagenome assembly (they can be considered as the results of a baseline reference-based metatranscriptome assembly approach that uses the contigs as the reference); Multi-edge transcripts: the transcripts reported by TAG that span multiple edges in the de Bruijn graph. 
 We also compared the TAG assemblies with the  de novo  transcript assemblies from Trinity. We note that this is a real metatranscriptomic dataset, so that we cannot compare the results in terms of the accuracy of the assembly as we did for the mock dataset (but we have shown using the mock dataset that  de novo  assembly tends to produce more problematic transcripts). In total, TAG produced 136 555 transcripts with a total of 37.4 Mb, whereas Trinity generated 207697 transcripts with a total of 44.8 Mb. Similar to the results on the mock dataset, TAG transcripts are longer than Trinity transcripts: the average lengths of the transcripts are 273 and 216 bp, for TAG and Trinity, respectively. Combining the transcripts from both assemblers (and removing redundant transcripts at 95% sequence identity by CD-HIT-EST) resulted in 233  201 transcripts with a total of ∼55.8 Mb, again demonstrating that reference-based and  de novo  approaches can complement with each other to improve the coverage of transcript assembly. 4 Discussion Even though thousands of complete prokaryotic genomes and many more draft genomes are available, metagenomes are constantly found to contain many new species and new genes ( Huttenhower  et 
 al. , 2012 ;  Qin  et al. , 2010 ;  Vital  et 
 al. , 2014 ). It is therefore important to develop methodologies for metatranscriptome data analysis that are not constrained by the sequenced genomes. With ‘matched’ metagenomic and metatranscriptomic datasets, we believe that proper utilization of the metagenome data will help greatly the analysis of metatranscriptomic data (and  vice versa ). The eventual integration of these datasets (as well as other meta-omic datasets) will provide new insights on the composition, function and regulation of microbiomes. Well-assembled transcripts are important for the function annotation of the metatranscriptome, and also for inferring gene regulatory mechanisms such as the operons. We developed a novel reads mapping algorithm (read2graph) that allows fast mapping of short reads from transcriptome sequencing onto the assembly graphs of reference genomes. We applied this mapping algorithm for metatranscriptome assembly, showing the utility of the de Bruijn assembly graph of the metagenome in downstream applications such as the metatranscriptome analysis. Our mapping tool is fast and can be applied to other applications, for example, mapping metagenomic sequencing reads onto the de Bruijn graph of closely related species for estimating the relative abundances of these species ( Wang  et 
 al. , 2012 ). We have shown in a related research that genes are often broken into fragments in metagenome assembly, and multi-edge-spanning reads can stitch them together ( Wu  et 
 al. , 2012b ). The mapping of multi-edge-spanning reads will also improve quantification of gene expression based on read counts, in particular for genes (from the same or different organisms) sharing highly similar sequences. In reality, however, we may still miss the mapping of a small fraction of multi-edge-spanning reads: if a read contains a sequencing error in the occurrence of a branching  k -mer, we cannot find its location in the graph. Because of the low error rate (&lt;1%) in Illumina reads, we believe this fraction of reads is indeed negligible in metatranscriptomic data analysis. We note that de Bruijn graphs will naturally capture the genomic variations of the metagenomes in the graphs [e.g. the single-nucleotide variations are represented as bulges ( Nijkamp  et 
 al. , 2013 ), the variations in tandem repeats are represented as wheels, and structural variations are represented long loops ( Pevzner  et 
 al. , 2001 )], which is yet another advantage of using graphs instead of contigs to represent metagenomes. Genomic variations in metagenomes are naturally handled by our graph-centric mapping approach. We expect that a combination of different approaches (reference-based and  de novo ) need to be applied to accomplish the comprehensive metatranscriptome analysis. As the references for metatranscriptome analysis, the matched metagenome will never be perfect, due to biological (rare species may be poorly sampled), experimental (some genomic regions may not be covered well) and computational (assemblers are not perfect) reasons. Integration of known reference genomes, matched metagenomes and even non-matched metagenomes can maximize the coverage of references for reference-based approaches. On the other hand, if a microbial community contains new, rare but highly expressed microbial species, their transcripts can only be revealed by  de novo  metatranscriptome assembly ( Schulz  et 
 al. , 2012b ) but not by the reference-based approaches such as the one presented in this article. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Domain-oriented edge-based alignment of protein interaction networks</Title>
    <Doi>10.1093/bioinformatics/btp202</Doi>
    <Authors>Guo Xin, Hartemink Alexander J.</Authors>
    <Abstract>Motivation: Recent advances in high-throughput experimental techniques have yielded a large amount of data on protein–protein interactions (PPIs). Since these interactions can be organized into networks, and since separate PPI networks can be constructed for different species, a natural research direction is the comparative analysis of such networks across species in order to detect conserved functional modules. This is the task of network alignment.</Abstract>
    <Body>1 INTRODUCTION Understanding complicated networks of interacting proteins is a major challenge in systems biology. Recently, with the rapid progress of high-throughput experimental techniques, protein–protein interaction (PPI) databases have rapidly increased in size, allowing for comparative analysis of PPI networks from which conserved modules can be identified across PPI networks of different species (Sharan and Ideker,  2006 ; Srinivasan  et al. ,  2007 ). By analogy to sequence alignment, this problem is called PPI network alignment. Typically, PPI network alignment algorithms compare PPI networks of two or more species and identify conserved modules, e.g. pathways or protein complexes. Often a PPI network is represented as an undirected graph in which nodes indicate proteins and edges indicate interactions. Hence, the network alignment problem can also be viewed as a graph isomorphism problem. Many network alignment algorithms have been proposed in recent years and most of them focus on the pairwise alignment of PPI networks. As an early approach, PathBLAST (Kelley  et al. ,  2003 ) proposed a likelihood-based scoring scheme to search for conserved pathways. Sharan  et al.  ( 2005a ) extended PathBLAST to employ a greedy heuristic to detect conserved protein complexes across species. NetworkBLAST-E (Hirsh and Sharan,  2007 ) introduced an evolutionary model of networks into the alignment scoring function to extract conserved complexes. MaWISh (Koyutürk  et al. ,  2006 ) merged pairwise interaction networks into a single alignment graph and treated network alignment as a maximum weight induced subgraph problem. MNAligner (Li  et al. ,  2007 ) described an integer quadratic programming (IQP) model to identify conserved substructures. Recently, several network alignment algorithms have been developed that can align more than two species. Graemlin (Flannick  et al. ,  2006 ) is capable of aligning at least 10 microbial networks at once. NetworkBLAST (Sharan  et al. ,  2005b ), another extension of PathBLAST, can align networks of up to three species, and its later version, NetworkBLAST-M (Kalaev  et al. ,  2008 ), can align 10 networks with tens of thousands of proteins in minutes. In addition, Singh  et al.  ( 2008 ) described a method inspired by Google's PageRank to detect global alignments from five eukaryotic PPI networks. All these network alignment algorithms follow a  node-then-edge-alignment  paradigm. That is, they generally first need to identify homologous proteins across species before they can exploit protein interaction and network topology information to detect conserved subnetworks. The node alignment step essentially acts as a filter, artificially constraining the search space of conserved modules to putatively homologous protein pairs. However, proteins rarely act alone. They interact with each other to carry out their activities, and these interacting proteins are likely to evolve with high correlation during the evolution of species (Goh  et al. ,  2000 ; Mintseris and Weng,  2005 ; Pazos  et al. ,  1997 ). Furthermore, it has been shown recently that such co-evolution is more evident if we focus our attention on interacting domains that are responsible for PPIs (Itzhaki  et al. ,  2006 ; Jothi  et al. ,  2006 ; Schuster-Böckler and Bateman,  2007 ). Based on these observations, we present DOMAIN, an algorithm for  dom ain-oriented  a lignment of  i nteraction  n etworks, that follows an alternative  direct-edge-alignment  paradigm. DOMAIN does not explicitly restrict its attention to putatively homologous proteins. Instead, it directly aligns PPIs across species by decomposing PPIs in terms of their constituent domain–domain interactions (DDIs) and looking for conservation of these DDIs. We apply DOMAIN to detect conserved protein complexes in yeast–fly and yeast–worm PPI networks, and demonstrate that it achieves better results than two previous techniques in most performance metrics. The article is organized as follows:  Section 2  presents the details of DOMAIN.  Section 3  describes the quality assessment measures, as well as the experimental results of DOMAIN compared with two extant methods. In  Section 4 , we discuss implications of the results, along with further directions. 2 METHODS As illustrated in  Figure 1 , DOMAIN consists of three stages: (1) it constructs a complete set of alignable pairs of edges (APEs); (2) it builds an APE graph; and (3) it employs a heuristic search to identify conserved protein complexes across species. The three subsections that follow elaborate upon these three stages.
 Fig. 1. Method overview. (1) Constructing APEs. The input of DOMAIN includes two PPI networks and the constituent domains of the proteins. Using this information, DOMAIN calculates species-specific DDI probabilities, and then identifies a set of APEs across networks. (2) Building an APE graph. An APE graph is a merged representation of the PPI networks, in which each node represents an APE and each edge represents one of four network connectivities connecting two APEs: (a) alignment extension, (b) node duplication, (c) edge indel (insertion/deletion), or (d) edge jump. The details of these connectivities are given in  Section 2.2 . (3) Searching for high-scoring non-redundant subgraphs within the APE graph. We use a greedy heuristic to carry out this task. 
 2.1 Constructing and scoring APEs Domains are structural and functional units of proteins. Many studies (Bernard  et al. ,  2007 ; Deng  et al. ,  2002 ; Riley  et al. ,  2005 ) have revealed that direct PPIs are often mediated by interactions between the constituent domains of the two interacting proteins. These studies have made two particular assumptions that we adopt as well: (1) DDIs are independent of each other, and (2) two proteins interact if at least one pair of domains from two proteins interact. These assumptions allow us to formulate the probability of an interaction between two proteins in terms of a ‘noisy-or’ over the DDIs that might possibly mediate the interaction between those two proteins. In our network alignment scenario where we seek to align edges directly, we additionally assume that a pair of cross-species PPIs can be aligned to one other only if they are plausibly mediated by at least one common DDI. We represent the input PPI networks from two species as undirected graphs  G 1 ( V 1 ,  E 1 ) and  G 2 ( V 2 ,  E 2 ), where nodes indicate proteins and edges indicate the observed PPIs. We first wish to construct a complete set of APEs. We say that a pair of edges,  e 1 ∈ E 1  and  e 2 ∈ E 2 , is  alignable  if there exists a DDI that can plausibly mediate the two PPIs represented by that pair of edges. We say that a DDI can  plausibly mediate  a PPI if the corresponding interaction probability between the two domains is above some value ϵ&gt;0. Using a non−zero value for ϵ allows us to filter out domains between which there is negligible evidence of a DDI. For an edge  e ∈ E 1  or  E 2 , we define 𝒟( e ) to be all the possible interactions between the constituent domains of the two proteins. Given the species-specific probabilities of DDIs that mediate PPIs, we can then write the score of an APE  c =( e 1 ,  e 2 ) using a ‘noisy-or’ formulation:
 
where  d α,β  denotes an interaction between domains α and β, and θ α,β =Pr( d α,β ), and Θ={θ α,β }. The function  g (θ α,β 1 , θ α,β 2 ) measures the probability of aligning the PPI  e 1  to the PPI  e 2  mediated by interactions between domains α and β. In this work, we have chosen to set  g (θ α,β 1 , θ α,β 2 )=(θ α,β 1 ·θ α,β 2 ) 1/2 . As previous authors have also done, to estimate the species-specific DDI probabilities Θ, we applied the EM (expectation–maximization) algorithm of Deng  et al.  ( 2002 ) for each given network. 2.2 Building an APE graph The APE graph is motivated by the evolutionary model of PPI networks suggested by Berg  et al.  ( 2004 ). The model indicates that PPI networks are shaped primarily by two kinds of evolutionary events,  link dynamics  and  gene duplication . Link dynamics events are primarily caused by sequence mutations of a gene and affect the connectivities of the protein whose coding sequence undergoes mutations. Gene duplication, the second kind of evolutionary event, is often followed by either silencing of one of the duplicated genes or by functional divergence of the duplicates. From the perspective of protein domains, a link dynamics event may result from switching a constituent domain of a protein to another, or a change in a domain's interaction partners; a gene duplication event consists of duplication of one protein, followed by a domain switching or being removed in one or both of the duplicates, or followed by progressive small changes from point mutations that cause a change in domain interaction partners. With this motivation in place, we define an APE graph to be an undirected weighted graph, where nodes correspond to the APEs identified above, and edges correspond to one of four evolutionary relationships that we consider between two APEs, as illustrated in  Figure 2  and as listed below:
 Alignment extension: two APEs are connected if they share two proteins, one per species. Node duplication: two APEs are connected if they share a protein in one species and a PPI in the other. Edge indel (insertion/deletion): two APEs are connected if they share a protein in one species and the graph distance between the two PPIs in the other network is 1. Edge jump: in this case, all proteins within the two APEs are distinct, but for each species, the graph distance between the two PPIs in their corresponding network is 1. We consider this case because our current knowledge of both PPIs and DDIs is noisy and incomplete. Thus, if there exists a pair of PPIs that can make two APEs connected in each network, we treat the pair as a potential APE. Note that some insignificant DDIs (probabilities of DDIs &lt;ϵ) are shared in such potential APEs. 
 Fig. 2. Four connectivities in an APE graph. The details of these connectivities are given in the text, and the legend is the same as in  Figure 1 . 
Given this definition of an APE graph, we note that every subgraph in an APE graph corresponds to a network alignment. Each node in an APE graph contributes the score  f ( c ) of its corresponding APE, and each edge is scored by a positive number according to its connection relationship. Using these edge scores, we want to reward alignment extension and penalize both node duplication and edge indel. Let γ a , γ b , γ c  and γ d  be the edge scores of alignment extension, node duplication, edge indel and edge jump, respectively. We thus need to assign γ a &gt;1 and γ b , γ c &lt;1. Because we neither wish to reward nor penalize an edge jump, we simply assign γ d =1. For a subgraph  G s ( V s ,  E s ) in an APE graph, the overall score for its corresponding network alignment is calculated as
 
where γ( e ) is the edge score for  e ∈ E s , and  f ( c ) is the score of the APE  c ∈ V s . 2.3 Detecting protein complexes Network alignment methods generally require a search algorithm to detect high-scoring subgraphs from a single or several weighted graphs. Such tasks are computationally difficult, so a number of search heuristics have been proposed: for example, PathBLAST uses randomized dynamic programming to search for conserved pathways across networks, while NetworkBLAST-E implements a greedy heuristic to search for conserved protein complexes. As many pairwise network methods aim to identify conserved protein complexes, for comparative purposes, we devise a greedy heuristic for finding conserved protein complexes across species. The heuristic aims to identify high-scoring non-redundant subgraphs from the resultant APE graph. Specifically, exhaustively starting from each APE, we iteratively expand the subgraph by introducing a new APE that increases the alignment score the most, until any of the following empirical stopping conditions occur: (i) the number of proteins in either species exceeds an upper limit (we used 15); (ii) the score of the next expanding APE is smaller than a threshold (we used 10 −2 ); (iii) the overall alignment score of the subgraph is smaller than a threshold (we used 10 −3 ); or (iv) the graph distance of the next expanding APE exceeds an upper limit (we used 4). At the end, small and redundant subgraphs are removed if the number of proteins in a subgraph is less than four, or if there exists a higher scoring subgraph overlapping &gt;80% of proteins in either species. 3 RESULTS 3.1 Experimental setup We compare our method to two extant pairwise network alignment algorithms, NetworkBLAST and MaWISh. We do not include NetworkBLAST-M and Graemlin in our comparisons because they mainly focus on alignment of multiple networks, and because Graemlin requires the unavailable in-house SRINI algorithm (Srinivasan  et al. ,  2006 ) to assign weights to PPIs. The ISOrank algorithm aims at resolving a different problem of aligning networks globally, while NetworkBLAST-E performs similarly to NetworkBLAST and is not available online. We thus exclude these methods from the comparisons as well. We apply DOMAIN on yeast–fly and yeast–worm PPI networks taken from DIP (Database of Interacting Proteins, Oct 2008) (Xenarios  et al. ,  2002 ), as they were widely used in pairwise network alignment studies as benchmarks. The protein-to-domain mappings are taken from Pfam (Pfam 23.0) (Finn  et al. ,  2008 ), and we only consider high-quality Pfam-A entries. Because not all proteins contain significant Pfam domains, we generate a so-called ‘backbone’ network, a subnetwork of DIP in which all proteins contain at least one Pfam-A domain. As summarized in  Table 1 , 78.2% of MIPS annotated proteins and over 70% of GO annotated proteins are contained in backbone networks. To simplify the setting of the four γ parameters, we reduced the parameter space to one dimension by insisting that γ a = k , γ b =γ c =1/ k  and γ d =1, for some value of  k &gt;1. We found that DOMAIN was not sensitive to changes in  k . In the results that follow, we used  k =10.
 Table 1. Summary of backbone networks DIP Backbone DIP Yeast Fly Worm Yeast Fly Worm Number of PPIs 17 528 22 381 4038 11 426 11 013 2213 Number of proteins 4928 7446 2644 3300 4500 1620 Number of GO annotated proteins a 4625 4477 1566 3280 3253 1145 Number of MIPS annotated proteins b 1100 – – 860 – – a With respect to the biological process annotation of Gene Ontology. b Excluding MIPS category 550. 
 3.2 Experimental results We employ three measures to evaluate the biological significance of the alignments: sensitivity/specificity, MIPS purity and GO enrichment. These measures are also suggested in several other network alignment studies (Dutkowski and Tiuryn,  2007 ; Hirsh and Sharan,  2007 ; Kalaev  et al. ,  2008 ). The first two measures use the known yeast protein complexes cataloged in MIPS (May 2006) (Mewes  et al. ,  2002 ) as a gold standard. We exclude category 550 (obtained from high-throughput experiments) and only use complexes at level 3 or lower. In consequence, there exist 122 MIPS complexes spanning 519 yeast proteins in the yeast backbone network, 62 of which contain at least three proteins and span 438 proteins. For each identified yeast alignment, we try to find a complex from MIPS that maximizes the hypergeometric score and calculate an empirical enrichment  P -value. The significance level is obtained from sampling 10 000 random sets of proteins of the same size, and the  P -values are corrected for multiple testing using the false discovery rate (FDR) (Benjamini and Hochberg,  1995 ). Then, the specificity is defined as the percent of yeast alignments that have a significant match in MIPS ( P &lt;0.05), and the sensitivity is defined as the percent of MIPS alignments that have significant matches in the resulting alignments. Moreover, an alignment is called a pure alignment if it satisfies two conditions: (i) it contains at least three MIPS annotated proteins and (ii) there exists a complex in MIPS that covers &gt;75% of its MIPS annotated proteins. We report purity, calculated by the number of pure alignments divided by the total number of alignments with at least three MIPS annotated proteins, as an alternative measure of the sensitive identification of specific complexes. GO enrichment measures the functional coherence of the proteins in an identified alignment with respect to the biological process annotation of GO, for each species separately. We use the tool GO TermFinder (Boyle  et al. ,  2004 ) to compute empirical enrichment  P -values, and correct for multiple testing using FDR. For each species, we report the fraction of process-coherent alignments with  P -value &lt;0.05 (considering only the alignments with at least one GO annotated protein). We chose to set the probability threshold of DDIs ϵ to the low but non−zero value of 10 −20  so as to take into account as much DDI information as possible. For yeast–fly alignment, DOMAIN generated an APE graph consisting of 6918 APEs with 47 964 alignment extension links, 24 549 node duplication links, 5573 edge indel links and 1149 edge jump links; for yeast–worm alignment, it returned a 1410-node APE graph with 4230 alignment extension links, 4087 node duplication links, 140 edge indel links and 37 edge jump links. For accurate comparison, we applied NetworkBLAST and MaWISh on backbone networks with their suggested parameter settings [see Koyutürk  et al.  ( 2006 ) and Sharan  et al.  ( 2005b ) for details]. As summarized in  Tables 2  and  3 , DOMAIN identified more significant non-redundant alignments than NetworkBLAST and MaWISh in both alignments—explaining the good scores on the sensitivity metric—but also managed to outperform the other methods on the specificity and purity metrics. Indeed, it achieved the highest performance on almost every evaluation metric, and in the instances in which it was bested, the difference is slight.
 Table 2. Performance comparisons of DOMAIN with NetworkBLAST and MaWISh on yeast–fly backbone networks Method No. of complexes No. of proteins Specificity (%) Sensitivity (%) MIPS purity (%) GO enrichment Yeast Fly Yeast (%) Fly (%) DOMAIN 100 338 313 34.0 9.0 66.7 89.0 78.0 NetworkBLAST 82 299 213 31.7 7.4 40.6 87.8 79.3 MaWISh 54 193 142 18.5 4.1 30.0 75.9 66.7 The largest value in each column is indicated in bold. 
 Table 3. Performance comparisons of DOMAIN with NetworkBLAST and MaWISh on yeast–worm backbone networks Method No. of complexes No. of proteins Specificity Sensitivity(%) MIPS purity GO enrichment Yeast Worm Yeast (%) Worm (%) DOMAIN 21 84 63 36.4 3.3 75.0 90.5 9.5 NetworkBLAST 19 82 51 7.7 0.8 60.0 89.5 10.5 MaWISh 11 42 32 11.1 1.6 42.8 63.6 9.1 The largest value in each column is indicated in bold. 
 The running time of DOMAIN is comparable with MaWISh and NetworkBLAST. DOMAIN is currently implemented in Perl, and its running time on yeast–fly and yeast–worm backbone networks is &lt;1 min (Intel Core 2 CPU 6600@2.4 GHz, 2 GB RAM). Because the running time is so small, we were able to exhaustively expand from all APEs. If for some reason we needed to further reduce computational complexity, we could instead consider an alternative expansion strategy where we would expand only from ‘seed’ APEs. The idea would be that if a protein complex is conserved in many species, the PPIs in this complex are likely to be conserved as well, and therefore the corresponding subgraph in the APE graph should contain many alignment extension links. With this in mind, we could rank the APEs by counting the number of their surrounding alignment extension links and select, say, the top 25% as seeds for expansion. We tested this, and the results were nearly identical to those listed in  Tables 2  and  3 , but the running time for yeast–fly and yeast–worm alignments reduces to 30 and 15 s, respectively. In our case, the running time was not a problem, but it is reassuring that a seed-based expansion strategy seems to be effective at reducing the running time without affecting the results. 3.3 Case studies DOMAIN is sensitive at detecting small network alignments that might be deemed by other algorithms to be topologically insignificant. For example, DOMAIN reported a network alignment between the yeast NEF1 complex and the fly proteins mei-9, Ercc1 and Xpac with high confidence ( Fig. 3 ). The GO process coherence of these three fly proteins is significant: nucleotide excision repair ( P ≃10 −8 ), DNA repair ( P ≃10 −6 ), cellular response to DNA damage stimulus ( P ≃10 −6 ), etc. However, neither MaWISh nor NetworkBLAST reports any alignment involving the yeast NEF1 complex. They are likely to miss such alignments because (i) the sequence similarity between Rad10 and Ercc1 is insignificant (BLAST  E -value≃10 −8 ) and may be ignored if using a restrictive BLAST  E -value threshold [e.g. 10 −10  suggested in Hirsh and Sharan ( 2007 )], and (ii) this alignment consists of only three matched proteins and two conserved interactions, so it may not be sufficiently topologically significant for some aligners to detect. On the other hand, the DDIs within this alignment are well-conserved across species (the DDI probabilities of  ERCC4-Rad10  are 1.00 in both species; the DDI probabilities of  Rad10-XPA_C  are 1.00 and 0.54 in yeast and fly, respectively).
 Fig. 3. DOMAIN reports a network alignment between the yeast NEF1 complex (MIPS category 510.180.10.10) and the fly proteins mei-9, Ercc1 and Xpac. The object to the right of the double arrow depicts the corresponding subgraph of this alignment in the APE graph. 
 Another advantage of DOMAIN is that often it provides a more comprehensive means of interpreting the identified network alignments, because protein domains are directly relevant to function in many cases. For instance, Rad14 and Xpac may play a similar role in the biological process of nucleotide excision repair, as they share a common  XPA_C  domain. Furthermore, although the  XPA_N  domain is not reported as a significant domain for Rad14 in Pfam ( E -value =0.023), the alignment of yeast Rad14 to fly Xpac suggests that  XPA_N  is potentially an important functional domain in Rad14. Identifying conserved biological pathways across species is another important application of network alignment.  Figure 4 a demonstrates an example of alignment reported by DOMAIN between 10 yeast proteins and three worm proteins, in which nine of the yeast proteins (all except Nyv1) and all three worm proteins are known to be involved in the pathway of SNARE interactions in vesicular transport in KEGG (Kanehisa and Goto,  2000 ).
 Fig. 4. ( a ) DOMAIN reports an alignment between 10 yeast proteins and 3 worm proteins that significantly matches the pathway of SNARE interactions in vesicular transport in KEGG. ( b ) An example of improving network alignment by combining several cross-species pairwise alignments. (Green, yeast proteins; blue, fly proteins; orange, worm proteins.) 
 Alignment performance may further be improved by combining several cross-species pairwise network alignments.  Figure 4 b shows an example of combining three alignments taken from yeast–fly, yeast–worm and fly–worm network alignments, respectively. By aligning yeast and fly networks, DOMAIN detects an alignment between three fly proteins (CG8142, RfC3, and RfC40) and seven yeast proteins, and four of them (Rfc1-4) are involved in the replication factor C complex (MIPS: 410.40.30). As the yeast replication factor C complex contains five proteins (Rfc1-5), the  F -score 1  is 0.67. Further, we see that two worm proteins (F44B9.8 and Rfc-2) are aligned to all these three fly proteins in fly–worm alignment and three of these seven yeast proteins (Rfc2-4) in yeast–worm alignment. This three-way alignment suggests that the alignment between fly proteins CG8142, RfC3 and RfC40 and yeast proteins Rfc2-4 are of high confidence, and the  F -score is increased to 0.75. 4 CONCLUSIONS In this study, we described DOMAIN, a domain-oriented pairwise network alignment framework. To our knowledge, DOMAIN is the first algorithm to introduce protein domains into the network alignment problem. Also, DOMAIN uses a novel  direct-edge-alignment  paradigm to directly detect equivalent PPI pairs across species and suggests a new graph representation to merge these equivalent PPI pairs and their network evolutionary-based relationships into one graph. We tested DOMAIN to identify conserved protein complexes in the yeast–fly and yeast–worm protein interaction networks, and the experimental results show that DOMAIN exhibits better performance than two recent pairwise network alignment methods in most performance metrics. Although DOMAIN can be applied only to the subset of proteins with domain mappings, we notice that most functionally annotated proteins contain domain structures and remain in this subset. To overcome this restriction, we may employ a larger domain database, e.g. CDD (Marchler-Bauer  et al. ,  2007 ), or combine DOMAIN with other network aligners. In addition, as the set of defined domains expands and is refined over time, this will gradually become less of a restriction. Further directions for research include extending this approach to multiple network alignment and to network querying. Since multiple network alignment requires more than two networks by definition, we would simply need to devise an appropriate scoring scheme that can handle more than a pair of alignable PPIs at once, and then extend the notion of the APE graph accordingly. The goal of network querying is to identify subnetworks in a given network that are similar to the query. Typically, the query is a hypothetical or known functional module. We may simply treat the query as a small input network and apply our DOMAIN method directly on it. A more sophisticated approach would be to devise a sequence-profile-like structure to describe the DDI contents of the network query, as well as perhaps constructing such structures for the full network as a one-time expense for many successive queries. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Detection of microRNAs in color space</Title>
    <Doi>10.1093/bioinformatics/btr686</Doi>
    <Authors>Marco Antonio, Griffiths-Jones Sam</Authors>
    <Abstract>Motivation: Deep sequencing provides inexpensive opportunities to characterize the transcriptional diversity of known genomes. The AB SOLiD technology generates millions of short sequencing reads in color-space; that is, the raw data is a sequence of colors, where each color represents 2 nt and each nucleotide is represented by two consecutive colors. This strategy is purported to have several advantages, including increased ability to distinguish sequencing errors from polymorphisms. Several programs have been developed to map short reads to genomes in color space. However, a number of previously unexplored technical issues arise when using SOLiD technology to characterize microRNAs.</Abstract>
    <Body>1 INTRODUCTION So-called next-generation sequencing technologies, or deep sequencing, permit the fast and comprehensive analysis of genomes and transcriptomes ( Mardis, 2008 ). The short length of the sequence reads produced is compensated by the capacity to produce millions of reads in a single run. Thus, new strategies to align and assemble highly redundant short sequence reads have been developed over the last few years ( Flicek and Birney, 2009 ;  Li and Homer, 2010 ). MicroRNAs are endogenous RNA molecules, ~22 nt in length that repress gene translation ( Bartel, 2004 ). In the past 4 years, the overwhelming majority of novel microRNAs have been identified by deep sequencing. Reads from deep sequencing experiments may contain sequences of the short DNA adapters (termed ‘linkers’ here) used in the sequencing reaction. Characterization of small RNAs from deep sequencing datasets requires the removal of these linker sequences from the 3 ′  ends of reads. Illumina/Solexa sequencing has been extensively used to detect microRNAs ( Berezikov  et al. , 2011 ;  Morin  et al. , 2008 ;  Ruby  et al. , 2007 ) and the available data suggest that the 3′ linker sequences are easily detected and removed. For instance,  Ruby  et al.  (2007 ) detected 3′ linker fragments in &gt;82% of the sequenced reads by string matching. The use of AB SOLiD sequencing to characterize microRNAs is on the rise ( Cai  et al. , 2010 ;  Chen  et al. , 2010 ;  Goff  et al. , 2009 ;  Li  et al. , 2010 ;  Marco  et al. , 2010 ). Unlike other technologies, SOLiD machines produce sequences in color space, each color representing a dinucleotide. The rationale behind color space is that, since colors are produced for overlapping dinucleotides ( Fig. 1 A), each nucleotide is read twice. This is purported to reduce sequencing errors, and to permit better distinction between sequencing errors and polymorphisms ( Applied Biosystems, 2008 ). The characterization of microRNAs in color space produces two specific issues that have so far been overlooked, one associated with each end of the read. First, the read length is longer than the biological sequence, such that the 3 ′  end of every read derived from a microRNA contains linker sequence that must be removed. However, detecting and removing adapter sequences in color space is not as straightforward as in base space, as we explore in this work. Second, the first color of each read represents the last base of the adapter and the first of the target sequence. The treatment of this color is controversial since different programs keep or remove it. Removal of the first color causes the first base to be lost, whereas retaining the first color may reduce the proportion of reads that map to the genome. The loss of the 5 ′  nt has critical consequences in the characterization of microRNAs. In this article, we address the issues of using color space sequences to characterize microRNAs and other small RNAs, and provide a simple strategy to easily map color space reads from small RNA libraries to whole genomes.
 Fig. 1. Effects of color space encoding in the first nucleotide of sequenced reads. ( A ) cDNA sequences are linked to a P1 adapter. The first color produced is determined by the first base of the read and the last of the adapter. If we remove the first color, the first base is lost during the color-to-base decoding. This first base is kept if we do not remove the first color. ( B ) Effect of missing the first nucleotide (red) in sequencing long (fragmented) sequences. ( C ) Effect of missing the first nucleotide in sequencing microRNAs. 2 METHODS 2.1 Linker fragments detection To analyze the nature of the contaminant sequences within reads, we explore the presence of linker fragments at 3 ′  ends in a  Tribolium  adult library (GEO accession number: GSM639446). We used the cutadapt tool ( http://code.google.com/p/cutadapt/ ) to remove the linker sequences used during the sequencing reaction (5 ′  linker: CCACTAC GCCTCCGCTTTCCTCTCTATGGGCAGTCGGTGAT; 3 ′  linker: CTGCC CCGGGTTCCTCATTCTCTCATCGGCTGCTGTACGGCCAAGGCG). We also mapped all adjacent 20 color length overlapping fragments within the last 30 colors of each read against these linker sequences using Bowtie ( Langmead  et al. , 2009 ) allowing from 0 to 3 color mismatches. 2.2 Sequential trimming and mapping of reads in color space Sequence datasets were obtained from GEO ( http://www.ncbi.nlm.nih.gov/geo/ ) and SRA ( http://www.ncbi.nlm.nih.gov/sra ). For each dataset (GEO:GSM639446; GEO:GSM639447; SRA:SRR039230), we mapped the full-length reads, and then sequentially trimmed the last color of the reads and repeated the mapping procedure for all previously unmapped reads, to a minimum read length of 19 colors. At each mapping step, we first removed reads that match a library of known rRNAs and tRNAs for the target species. Ribosomal RNAs (rRNAs) in each target species were extracted from the SILVA database ( http://www.arb-silva.de/ , release 108). Transfer RNAs were detected in honeybee and beetle genomes using tRNAscan-SE with default parameters ( Lowe and Eddy, 1997 ). The remaining reads were mapped against the reference genome sequence ( Tribolium castaneum  version 3.0 and  Apis mellifera  version 4.0) with Bowtie ( Langmead  et al. , 2009 ) allowing two color mismatches (-v 2) and retaining all best matches (-a --best --strata). We modified the input files to force Bowtie to keep the first color of the reads by removing the first letter of each sequence in the input file (B.Langmead, personal communication). 2.3 Detection of microRNAs Reads mapped at color length 20–26 were used for microRNA detection. Reads that map to &gt;5 positions in the genome were removed. We grouped overlapping reads and retrieved flanking regions (−50 to +100 and −100 to +50) from the target genomes. We scanned these genomic fragments for hairpins using RNAfold ( Hofacker  et al. , 1994 ). We applied a series of filters to the resulting hairpins to detect putative microRNAs based on  Marco  et al.  (2010 ), with minor modifications to increase the stringency of microRNA calls: (i) reads that were sequenced only once were removed prior to microRNA detection; we subsequently require that (ii) at least 10 reads map to the putative arms of the predicted hairpins; (iii) the hairpin folding energy is below −20 kcal/mol; (iv) at least 50% of the nucleotides of one arm of the hairpin pair with nucleotides from the other arm; (v) at least 70% of a minimum of five reads from one arm have the same 5 ′  end; (vi) putative mature sequences from both arms (so-called miR and miR* sequences) are supported by reads, and the most abundant reads from each arm pair in the predicted hairpin structure across at least 70% of their length; (vii) where a read maps to multiple genomic loci, all loci are annotated as microRNA candidates by criteria 1–6. MicroRNA candidates were manually inspected and compared with previously annotated microRNAs using BLAST ( Altschul  et al. , 1997 ). 3 RESULTS AND DISCUSSION 3.1 Linker fragments in color space reads Sequence reads often contain, in their 3 ′  end, fragments of the linker used during the sequencing reaction. Moreover, when the biological sequences of interest are shorter than the read length, we expect that most reads contain linker sequences. Linkers are typically removed by filtering out 3 ′  ends that match with known linker sequences. However, in the particular case of SOLiD reads this is problematic. AB SOLiD machines (versions 3 and 4) produce 50 nt long reads. The sequencing error rate rapidly increases toward the 3 ′  end of the read ( Sasson and Michael, 2010 ). We can estimate the proportion of linker sequences at the 3 ′  end of the read that contain no sequencing errors. Let  e i  be the color call error rate at position  i  of the read. Then, the proportion of reads with no errors at position i is (1 −  e i ). Hence, the proportion ( P ) of reads with no errors at their 3 ′  end is:
 (1) 
where  R  is the length of the read and  r  the length of the 3 ′  end fragment we are testing. Using the error rates described for  Escherichia coli  re-sequencing data in  Sasson and Michael (2010 ), and assuming that the linker accounts for half of the read (25 nt), we estimate that the percentage of reads with no errors in the linker sequence is ~43%. That means that we could not detect linker fragments in more than half of the reads by simple sequence matching. Moreover, linker fragments can have multiple sequencing errors. Assuming a fixed error rate of 0.033 [average for the last 25 nt of the reads according to  Sasson and Michael (2010 )], and using the binomial distribution, the expected percentage of sequences with at least 2 errors is 20%, and for 3 or more errors is ~5%. Additionally, &gt;90% of the mature microRNAs registered in miRBase ( Kozomara and Griffiths-Jones, 2011 ) are &lt;25 nt. Hence, these values are likely to be underestimates of the real impact of errors in the 3 ′  ends of the SOLiD small RNA sequencing reads. We further explored whether the 3 ′  end sequences are actually linkers in SOLiD small RNA datasets. We scanned sequence reads from a small RNA library from  T.castaneum  for known 3 ′  linkers used during the sequencing process using Bowtie (see  Section 2.1  for details). We find that ~23% of reads contain linker fragments with zero mismatches. As we increase the number of color mismatches to 3, we detect linker sequences in up to 57% of the reads ( Table 1 ). As a control, we also mapped against the 5 ′  linker (P1 adapter) used during the sequencing reaction. As expected, we find virtually no 5 ′  linker fragments at the 3 ′  ends of the reads ( Table 1 ). These data suggest that the number of errors in the 3′ linker sequence is higher than that predicted by our model, such that we miss  bona fide  linker fragments. Another possibility is that many reads are chimeric artifacts of small RNAs and other fragments of transcripts. As expected, when we directly remove linkers using the cutadapt tool (see  Section 2.1 ), only 2741 linker sequences are removed, and the number of reads that can be subsequently mapped to the genome is very low ( Supplementary Material ). Altogether, we conclude that directly filtering for linker sequence is not productive for SOLiD data.
 Table 1. Linker fragments detected at 3 ′  end of sequenced reads using Bowtie Color mismatches Reads matching 3′ linkers (%) Reads matching 5′ linkers (%) 0 15 540 742 (23.17) 1827 (0.00) 1 24 626 724 (36.72) 2745 (0.00) 2 31 983 586 (47.69) 3815 (0.01) 3 38 392 776 (57.24) 5775 (0.01) 
 To deal with 3 ′  end linker sequences of variable length, we propose a strategy based on sequential trimming and mapping. This type of strategy is appropriate for contaminant fragments of unknown length and undetectable origin ( Cloonan  et al. , 2009 ;  Marco  et al. , 2010 ). The procedure is as follows. First, we map all reads to a reference genome in color space using Bowtie ( Langmead  et al. , 2009 ). We trim the last color of only the unmapped reads and repeat the mapping. We sequentially trim one color and re-map, to a minimum read length in our case of 19 colors. Bowtie is particularly amenable to this approach for two main reasons: its speed allows multiple rounds of mapping, and the ‘--un’ option allows easy access to the unmapped reads at each stage. Bowtie is, however, less sensitive to reads with multiple mismatches ( David  et al. , 2011 ), although this is not a major consideration for microRNA analysis. Bowtie decodes colors to nucleotides using the dynamic programming approach described in ( Li and Durbin, 2009 ); as a consequence, the length of the decoded sequence will be 1 nt shorter than the length in colors of the read. That is, if we trim sequences to a minimum of 20 colors, the minimum nucleotide length will be 19. We note that RNA2MAP ( Applied Biosystems, 2009 ), the program provided by AB SOLiD, deals with the linker issue in a different way, but using the same principle that we cannot directly remove linker fragments in color space. RNA2MAP maps the reads from 5 ′  to 3 ′  by extending an initial aligned seed. During the process, the reads are mapped against ‘hypothetical reads’ (concatenating genome fragments and linker sequences) in order to detect contaminants ( Applied Biosystems, 2009 ). We compare the two approaches in a later section. RNA2MAP does not prefilter sequences based on quality values. The rationale behind this is that mapping in color space allows the easy identification of errors in color calls. Likewise, we did not prefilter the datasets analyzed in this work. However, we note that a prefiltering step significantly reduces the computational complexity of mapping, with a small impact on sensitivity for microRNA detection ( Supplementary Material ). This confirms that our mapping approach is also robust to low-quality sequences. 3.2 Mapping the first color of the read The first color of a read from the SOLiD output is determined by the last nucleotide of the P1 adapter linker and the first nucleotide of the actual read ( Fig. 1 A). Some mapping algorithms, including Bowtie, remove the first color before mapping, as only half of the information in the first color derives from the sequenced molecule. In this case, during the color-to-base decoding step we lose the first nucleotide of the actual read ( Fig. 1 A). This may have little or no effect when mapping overlapping reads from a longer transcript ( Fig. 1 B), but when we map small processed RNAs, we will wrongly detect the beginning of the functional sequences ( Fig. 1 C). Indeed, the correct definition of the 5′ end of the microRNA is critical for functional analysis. Other programs, such as SHRIMP ( Rumble  et al. , 2009 ) and BFAST ( Homer  et al. , 2009 ), keep the first color of the read during the mapping process. We propose keeping this first color but allowing an extra color mismatch to account for the 0.75 probability that the first base encoded in the color (the last base of the 5′ linker) does not match the 5′ flanking base in the genome. Keeping this first color has an important consequence in the mapping of reads. Since we retrieve all ‘best’ mapping positions, reads mapping equally well to multiple genome sites may be artificially differentiated by score based on the matching of the 5′ end color. For instance, sequence 0132012 will map better to 0132012 than to 1132012. However, both sequences may map equally well to both positions in base space. On the other hand, if we remove the first color, an alternative problem arises: a read that is unique in the genome may map to many places because the first nucleotide is not taken into account. Bowtie has a new native option to keep the first and last color of the reads but ignore color mismatches in those positions. We reanalyzed the honeybee data using this option. We find that the number of matches per read increases, suggesting a reduction in mapping specificity. The number of microRNAs detected by the downstream analysis falls slightly, suggesting our strategy outperforms the Bowtie option. We, therefore, recommend that the first color is retained for the purpose of microRNA detection. We also suggest that candidate microRNAs detected by deep sequencing (which will likely number only in the hundreds) should be independently mapped to the reference genome, using for example BLAST ( Altschul  et al. , 1997 ), in order to detect potential copies that escaped our mapping procedure. However, in our experience, this bias is negligible. 3.3 Efficient mapping of color space reads We implemented our integrated sequential trimming and mapping strategy in a simple script (see  Section 2.2 ). This script maps reads using Bowtie, but it modifies the input file so that Bowtie keeps the first color of the read. We have used this approach to reanalyse three published SOLiD datasets. We first consider two  T.castaneum  RNA libraries from adult and embryos, sequenced in our laboratory ( Marco  et al. , 2010 ). In our previous analyses, we first converted colors to bases directly, and mapped the sequences in base space. We obtained mapping rates to the reference genome of only ~13% of the adult reads and &lt;4% of the embryonic reads ( Marco  et al. , 2010 ). When we applied the strategy described here, we mapped ~68 and 58% for adults and embryos, respectively ( Table 2 ). The direct conversion from color to base space produces a high proportion of artifactual sequences (because a single color mismatch causes errors in every downstream base of the converted sequence). This is the primary explanation for the low number of reads mapped in our previous analyses. We also analyzed a third-party RNA library from honeybee ( Chen  et al. , 2010 ). In this case, we successfully mapped 79% of the reads. Since our interest is in detecting microRNAs, we consider further only reads mapped at color length 20–26. These sequences account for a large proportion of the total (~41%).
 Table 2. Reads mapped with the sequential trimming and mapping strategy Experiment Description Total reads Mapped reads (%) Small RNA set (19–25 nt) (%) Expected FP in small RNAs (%) GEO:GSM639446 Tribolium  adult RNA library 67 070 132 45 838 144 (68.34) 21 547 990 (32.13) 39 267 (0.06) GEO:GSM639447 Tribolium  embryo RNA library 52 620 004 30 382 986 (57.74) 14 120 332 (26.83) 35 162 (0.07) SRA:SRR039230 Apis  RNA library 36 796 459 28 909 134 (78.57) 15 204 339 (41.32) 19 059 (0.05) 
 We estimated the number of expected false positive mappings that passed our criteria. Calculating the frequency of a given word in a genome is a known problem that often requires the use of distribution approximations ( Robin and Schbath, 2001 ). We used a simpler approach to estimate the number of reads that map by chance to the genome, assuming no sequence bias composition and no effect of overlapping words. Consider the probability that a read maps at exactly one position in the genome by chance, that is a function of the number of mapping positions (approximately the length of the genome,  L , multiplied by the number of colors) and the number of potential different sequences (4 l , where  l  is the read length). Hence, the average number of sequences mapped by chance to the genome in 5 or fewer sites (since we discarded reads mapping to &gt;5 positions in our analysis),  M , is given by:
 (2) 
where  N  is the number of sequences to be mapped in each step. As shown in  Table 2 , the number of expected false positives (expected number of sequences mapped by chance) is, in all cases, &lt;0.1%. Chen  et al.  (2010 ) used RNA2MAP to analyze their honeybee dataset. RNA2MAP first maps the reads to known microRNAs at miRBase, and the remaining reads are mapped against the reference genome. In order to compare our results with those published previously, we followed a similar approach with our pipeline. When mapping to previously known microRNAs, we observed that both mapping strategies yielded similar results ( Fig. 2 A). However, we observed that for some microRNAs we map many more sequences than RNA2MAP. For example, the authors detected 546 reads that map to mir-1, while our strategy successfully mapped &gt;300 000 sequences. We note that there are two identical copies of mir-1 in the honeybee genome: ame-mir-1-1 and ame-mir-1-2. We also map many more reads than the previous study for other multiple copy sequences, for example mir-92b and mir-87 ( Fig. 2 A). It is important to keep reads with multiple matches in the genome, since they may map to real microRNAs. Additionally, our sequential trimming strategy clearly outperforms RNA2MAP-based mapping when mapping into the reference genome ( Fig. 2 B). We conclude that the ability of Bowtie to deal with multiple mapped reads outperforms that of RNA2MAP, and our sequential trimming strategy permits the removal of highly degraded linkers that escape RNA2MAP.
 Fig. 2. Comparison of reads mapped with RNA2MAP and a sequential trimming strategy. ( A ) Reads mapped using both strategies to known microRNAs in miRBase. ( B ) Reads mapped for both strategies to newly discovered microRNAs by  Chen  et al.  (2010 ). The inset in both graphs shows a zoomed view of the shaded area. Reads mapped to the genome by our sequential trimming approach can be used to detect microRNAs with in-house built tools, or can easily be converted to input files for popular microRNA detection tools such as miRDeep ( Friedlander  et al. , 2008 ) for animal microRNAs or miRCat ( Moxon  et al. , 2008 ) for plants. Using the mapping results of our sequential trimming procedure, we reanalyze the honeybee small RNA dataset for new microRNAs. Using strict detection criteria (see  Section 2.3 ), we detected eight new microRNAs, one of them (ame-mir-2765) with known homologs in other species ( Table 3 ). All but one have a relatively low number of reads ( Table 3 ). We also reanalyzed our own  T.castaneum  datasets ( Marco  et al. , 2010 ). We detected 14 additional new microRNAs that escaped our earlier annotation ( Table 3 ,  Supplementary File 1  in  Supplementary Material ). Four out of the 14 new microRNAs in  Tribolium  (tca-mir-6007, tca-mir-6016, tca-mir-927b, tca-mir-9e) were detected because of our improved strategy to characterize microRNAs (as described in  Section 2.3 ). However, the other 10 detections were only possible in color-space, mostly because low abundance reads from one of the arms did not map in base space due to sequencing errors.
 Table 3. Novel microRNAs discovered in  Apis mellifera  and  T.castaneum Name Chr Str Start End Reads ame-mir-6000 LG11 − 1 144 1637 11 441 734 48 ame-mir-6001 LG13 − 2 650 488 2 650 555 2973 ame-mir-6002 LG16 − 2 944 352 2 944 431 15 ame-mir-6003 LG2 − 705 902 7 059 571 35 ame-mir-6004 LG5 + 7 573 377 7 573 464 26 ame-mir-6005 LG6 − 6 509 945 6 510 107 180 ame-mir-6006 LG9 − 62 686 62 773 19 ame-mir-2765 LG9 + 5 203 815 5 203 903 162 tca-mir-6007 CHR1 + 8 492 377 8 492 463 867 tca-mir-6008 CHR2 + 14 782 252 14 782 347 27 tca-mir-6009 CHR2 + 186 871 186 960 44 tca-mir-6010 CHR2 + 11 831 158 11 831 242 33 tca-mir-6011 CHR3 − 31 219 647 31 219 723 41 tca-mir-6012 CHR3 − 9 600 253 9 600 425 258 tca-mir-6013 CHR4 + 11 124 335 11 124 402 17 tca-mir-6014 CHR4 + 3 369 897 3 369 978 46 tca-mir-6015 CHR4 + 11 485 945 1 1486 036 23 tca-mir-6016 CHR7 − 17 010 368 17 010 442 57 tca-mir-6017 CHR7 − 10 450 348 10 450 502 252 tca-mir-6018 CHR8 − 247 709 247 801 42 tca-mir-927b CHR9 − 16 099 288 16 099 383 224 tca-mir-9e CHR9 − 11 062 11 172 3696 Chr, Chromosome/linkage group; Str, strand; start, first nucleotide position; end, last nucleotide position; reads: total number of reads. 
 4 CONCLUSIONS Our exploration of the consequences of detecting microRNAs in color space can be summarized in two recommendations, independent of the particular software used for small RNA mapping. First, be aware of how your program of choice is dealing with the first color of the read. Second, do not rely on simple pattern match approaches to remove linker sequences. The sequential trimming approach discussed here, or seed mapping and extension, are likely to result in significantly higher mapping rates. We do not recommend cropping the read sequences to fixed length since this will create both false positives and false negatives. For example, a 50 nt read that maps to the genome is very unlikely to be a microRNA, yet cropping to 25 nt before mapping could cause an erroneous microRNA annotation. Deep sequencing was originally devised for high coverage of relatively long sequences. The application of such techniques to the detection of small RNAs highlights new issues and biases. Characterizing these issues is crucial for the development of more efficient and biologically congruent mapping tools. 
          Supplementary Material 
          
             
                Supplementary Data 
             
             
             
             
          
       </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Affy exon tissues: exon levels in normal tissues in human, mouse and rat</Title>
    <Doi>10.1093/bioinformatics/btp414</Doi>
    <Authors>Pohl Andrew A., Sugnet Charles W., Clark Tyson A., Smith Kayla, Fujita Pauline A., Cline Melissa S.</Authors>
    <Abstract>Summary: Most genes in human, mouse and rat produce more than one transcript isoform. The Affymetrix Exon Array is a tool for studying the many processes that regulate RNA production, with separate probesets measuring RNA levels at known and putative exons. For insights on how exons levels vary between normal tissues, we constructed the Affy Exon Tissues track from tissue data published by Affymetrix. This track reports exon probeset intensities as log ratios relative to median values across the dataset and renders them as colored heat maps, to yield quick visual identification of exons with intensities that vary between normal tissues.</Abstract>
    <Body>1 INTRODUCTION The mammalian transcriptome is complex. By recent estimates, as many as 94% of human genes undergo alternative splicing (Wang,  et al .,  2008 ). Alternative splicing is consequential as well as frequent, with effects ranging from altering protein structure to targeting mRNA for early decay (Hartmann and Valcarcel,  2009 ). Furthermore, mammalian genomes contain an abundance of non-coding RNA genes (Chu and Rana,  2007 ). In short, to understand the consequences of transcription, one must look beyond overall expression levels of known genes. Affymetrix exon arrays facilitate transcriptome analysis with probesets that measure RNA abundance for individual exons, conserved genomic regions, and blocks from syntenic alignments (see  http://www.affymetrix.com/support/technical/technotes/exon_array_design_technote.pdf ). These arrays have offered new insights on how transcript isoforms may be influenced by a myriad of factors including tissue type (Clark  et al .,  2007 ), genetic variation (Kwan  et al .,  2007 ; Zhang  et al .,  2009 ), differentiation (Yeo  et al .,  2007 ), and disease (Soreq  et al .,  2008 ; Thorsen  et al .,  2008 ). Alternative splicing can arise through normal, regulated processes; or through abnormalities such as mutation, disease, and environmental stress (Yeo  et al .,  2005 ). Before one can understand the abnormal conditions, it is valuable to understand the scope of normal alternative splicing by comparing splicing patterns between normal tissues. To facilitate this, we have provided the Affy Exon Tissues tracks in the UCSC Genome Browser (Kuhn  et al .,  2009 ), depicting exon probeset intensities in normal tissues in human, mouse, and rat. 2 DESCRIPTION The Affy Exon Tissues track consists of two parts: genomic coordinates of the exon array probesets; and a heat map indicating exon probeset intensities in normal tissues, based on data available from  http://www.affymetrix.com/support/technical/sample_data/exon_array_data.affx . Briefly, normal tissues were assayed in triplicate, and were analyzed with the Affymetrix Power Tools software ( http://www.affymetrix.com/partners_programs/programs/developer/tools/powertools.affx ) to produce normalized, background-corrected probeset intensities. For each probeset, we computed its median intensity for each tissue, and then the median of these median values. For each experiment, we calculated the log ratio between the probeset intensity and this median value. For numeric stability, we added a fixed, background-level pseudocount to each observation, which also renders probesets with no expression as constant-valued. The genome browser renders these log ratios as blue–white–red (shown), green–red, or yellow–blue heat maps, with the color selection controlled via the track's details page. Additional details are provided in the  Supplementary Material . Figure 1 a shows the Affy Exon Tissues track for TPM2 in mm9. The  constitutive  exons (those included in all transcripts) indicate that TPM2 is expressed most strongly in muscle and embryo, with some expression in ovary. TPM2 has a well-documented pattern of tissue-dependent splicing, with one isoform produced in skeletal muscle tissue and another in non-muscle tissue (Gooding and Smith,  2008 ). This pattern is apparent in the two mutually-exclusive exons (third and fourth from the left): one is highly-expressed (red) in muscle and embryo (a heterogeneous tissue), while the other is highly-expressed in ovary.
 Fig. 1. ( a ) The Affy Exon Tissues track demonstrates tissue-dependent splicing of two mutually exclusive exons (third and fourth from the left) in the mm9 TPM2 locus. Red indicates probesets that are up-regulated relative to median levels, while blue indicates down-regulated probesets. The  constitutive  exons (those included in all transcripts: first, second and fifth from the left) offer perspectives on overall gene expression, and indicate that the gene is up-regulated (red) in embryo, muscle and ovary. The leftmost mutually-exclusive exon appears upregulated in embryo and muscle, while the second appears upregulated in ovary. The remaining two probesets, which map to no known exons, are rendered mostly white. This indicates no variation in their expression levels, and suggests that they are not expressed in these tissues. ( b ) A probeset was designed for this unannotated region in mm9 based on more-speculative evidence such as genomic conservation or  ab initio  exon prediction. This region shows evidence of up-regulation (red) in brain, suggesting production of a brain-specific transcript. For contrast,  Figure 1 b shows an unannotated conserved region on chromosome 1 in mm9. While it does not overlap with any known gene, its red (up-regulated) log intensity in brain suggests brain-specific expression. This illustrates how this data can offer insights on regions with no annotation but strong conservation. 3 CONCLUSION The Affy Exon Tissues track displays exon probeset intensities in human, mouse, and rat tissues, including breast, cerebellum, heart, kidney, liver, muscle, pancreas, prostate, spleen, testes, and thyroid. In contrast to traditional microarray tracks such as the GNF Expression Atlas (Su  et al .,  2004 ), which provide one measure of overall expression per gene and cannot report any transcript variation, the Affy Exon Tissues track offers the ability to compare intensities of neighboring probesets and observe alternative promoter usage, polyadenylation, and splicing. Exon probeset intensities are rendered as heat maps to offer rapid visual identification of exons that vary under normal cellular conditions. Besides the Affy Exon Tissues track, the UCSC Genome Browser currently hosts the hg18 Sestan Brain exon expression track, which contrasts exon probeset intensities between sections of the brain (Johnson  et al .,  2009 ). This set of tracks may expand further as additional datasets become available, offering further insights into transcript variation in the mammalian genomes. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Memory-efficient dynamic programming backtrace and pairwise local sequence alignment</Title>
    <Doi>10.1093/bioinformatics/btn308</Doi>
    <Authors>Newberg Lee A.</Authors>
    <Abstract>Motivation: A backtrace through a dynamic programming algorithm's intermediate results in search of an optimal path, or to sample paths according to an implied probability distribution, or as the second stage of a forward–backward algorithm, is a task of fundamental importance in computational biology. When there is insufficient space to store all intermediate results in high-speed memory (e.g. cache) existing approaches store selected stages of the computation, and recompute missing values from these checkpoints on an as-needed basis.</Abstract>
    <Body>1 INTRODUCTION Dynamic programming algorithms are often used to find an optimal solution by backtracking through intermediate values of the computation. Typical examples are that of chain matrix multiplication, string algorithms such as longest common subsequence, the Viterbi ( 1967 ) algorithm for hidden Markov models, and sequence alignment algorithms such as those of Needleman and Wunsch ( 1970 ) and Smith and Waterman ( 1981 ). Dynamic programming algorithm backtraces are also used for random sampling, where the score for each possible backtrace path is deemed to be (proportional to) the probability of the path, and it is desired to choose a path according to that probability distribution. A typical example is the algorithm of Ding and Lawrence ( 1999 ) for the sampling of RNA secondary structure. A third use for dynamic programming backtraces is as the second step of a forward–backward algorithm, such as that of Baum–Welch (Baum  et al.  ( 1970 ) for finding the parameters of a hidden Markov model. Sometimes a trade-off with run time allows a problem to be solved without a backtrace through stored results, e.g. sequence alignment (Durbin  et al. ,  2006  Section 2.6; Myers and Miller,  1998 ; Waterman,  1995 , page 211) and Baum–Welch (Miklós and Meyer,  2005 ), but this is not always the case. When there is not enough space to store all intermediate results in high-speed memory, checkpointing strategies are employed, whereby selected stages of the computation are stored, and missing information is recomputed from these checkpoints on an as-needed basis. A stage of the computation, also known as a frontier, is a set of intermediate values that are sufficient for making subsequent computations. For instance, in a 2D dynamic programming algorithm that computes a small number of values for each ( i,j ) in a grid from the neighboring ‘earlier’ values associated with ( i −1, j ), ( i,j −1) and ( i −1, j −1), we could define a stage as a row of the computation grid. In this case, stage  k  would be the values associated with the cells {( k,j ) :  j = j min  …  j max }, and the stage  k  values would be sufficient for computing values for cell ( i,j ) for any  i &gt; k . Similarly one could use columns to define stages. In many cases it makes sense to have overlapping stages; in the above example stage  k  might be the  k -th diagonal frontier, i.e. the computation values associated with the cells {( i,j ) :  i + j ∈{ k −1, k }}. Herein we will describe an optimal checkpointing strategy that provably minimizes the number of stage re-computations necessary in performing a backtrace with limited high-speed memory. The algorithm is simple and efficient. Note that, because this limited-memory approach can be used to allow significant increases in locality of reference, it can provide more efficient computations even when the amount of high-speed memory might otherwise be considered sufficient. We build upon a previous approach that is fairly memory-efficient, which is described in  Bioinformatics  (Grice  et al. ,  1997 ; Wheeler and Hughey,  2000 ). With memory enough to store  M  stages, their ‘2-level’ algorithm uses the memory to compute the first  M  stages, but then retains only the  M -th stage as a checkpoint, discarding the previous ones. Using the remaining  M −1 memory locations, the algorithm computes stages  M +1, …, 2 M −1, and then uses the (2 M −1)th stage as the second checkpoint. It continues this process, using the ( M +( M −1)+( M −2))th stage as its third checkpoint, and so forth, up to and including  M +( M −1)+···+1= M ( M +1)/2 as its  M -th checkpoint. Thus, if  N = M ( M +1)/2 stages are needed in the backtrace, they can be achieved with   memory locations; in the backtrace, each missing stage is computed using the space freed by discarding the checkpoints that are no longer needed. Because the algorithm needs to compute each stage at most twice, once in the forward pass to create the checkpoints and once during the backtrace, the overall number of stage computations of the memory-reduced algorithm is at most double what it would have been. Wheeler and Hughey ( 2000 ) also generalize their 2-level algorithm to an ‘ L -level’ algorithm, where  L  is any positive integer. With  M  memory locations, the  L -level algorithm can compute
 (1) 
stages, where this formula works for any integers  L  and  M  so long as the binomial coefficient   is defined to be  n !/ d !( n − d )! when  d ≥0 and  n − d ≥0, and zero otherwise. The asymptotic limit is as  M →∞ for fixed  L . For the  M, L ≥1 algorithm, the  k -th stage to be checkpointed is
 (2) 
That is, the first stage to be a checkpoint is the last stage that would be a checkpoint under the ( L −1)-level algorithm. Generally, the  k -th stage to be checkpointed is beyond the ( k −1)th checkpoint by an amount that would be the last checkpoint for an ( L −1)-level algorithm that uses the remaining  M -( k −1) memory locations. Equation ( 2 ) solves to
 (3) The number of stage computations for the  L -level algorithm to compute  N WH ( M,L ) stages using  M  memory locations is given by the recursion
 (4) 
because the  L -level algorithm first computes all  N WH ( M,L ) stages, to get the  L -level checkpoints; it then provides access to the stages in reverse order by working with the  L -level checkpoints in reverse order; the  L -level algorithm uses the ( L −1)-level algorithm to generate the missing intervening stages. However, 1 is subtracted, because the last computation of each ( L −1)-level algorithm invocation produces an  L -level checkpoint that we already had available. Thus, this last computation for each ( L −1)-level algorithm invocation is not performed. Equation ( 4 ) solves to
 (5) 
where this formula works for any integers  L  and  M  so long as the trinomial coefficient   is defined to be ( a + b + c )!/ a ! b ! c !. when  a, b, c  ≥ 0, and zero otherwise. Thus we have a multiplier for the number of stage computations of approximately  L  for   memory locations. Computed values of  N WH ( M,L ) and  T WH ( M,L ) for small  M  and  L  are given in  Table 1  and plotted in  Figure 1 . Table 1. The number of stages and run time for both the algorithm of Wheeler and Hughey ( 2000 ) and the optimal checkpointing algorithm Alg. L T WH ( M,L )/ N WH ( M,L ) T opt ( M,L )/ N opt ( M,L ) 1 2 3 4 5 6 7 1 2 3 4 5 6 7 M =1 1/1 1/1 1/1 1/1 1/1 1/1 1/1 1/1 1/1 1/1 1/1 1/1 1/1 1/1 2 2/2 4/3 7/4 11/5 16/6 22/7 29/8 3/2 6/4 12/6 20/8 30/10 42/12 56/14 3 3/3 9/6 21/10 41/15 71/21 113/28 169/36 3/3 13/8 34/15 70/24 125/35 203/48 308/63 4 4/4 16/10 46/20 106/35 211/56 379/84 631/120 4/4 22/13 70/29 170/54 350/90 644/139 1092/203 5 5/5 25/15 85/35 225/70 505/126 1009/210 1849/330 5/5 33/19 123/49 343/104 798/195 1638/335 3066/539 6 6/6 36/21 141/56 421/126 1051/252 2311/462 4621/792 6/6 46/26 196/76 616/181 1596/377 3612/713 7392/1253 7 7/7 49/28 217/84 721/210 1981/462
 4753/924 10 297/1716 7/7 61/34 292/111 1020/293
 2910/671 7194/1385 15 972/2639 For the  L -level backtracking algorithm of Wheeler and Hughey ( 2000 ) with memory suitable for storage of  M  stages, the left side of this table shows both  N WH ( M,L ), the number of stages that can be produced in reverse order, and  T WH ( M,L ), the number of stage computations required for that backtrace. [See Equations( 1 ) and( 5 ).] For instance, to perform a backtrace on  N =36 stages with  M =3 memory locations requires the ( L =7)-level algorithm and requires  T =169 stage computations. It is not straightforward to predict the number of stage computations for other values of  N  ( Fig. 1 ). For the optimal checkpointing algorithm presented here, the right side of this table shows both  N opt ( M,L ), a number of stages that can be produced in reverse order, and  T ( M,N opt ( M,L )), the number of stage computations required for that backtrace. [See Equations( 7 ) and( 11 ).] When the number of stages is between  N opt ( M,L ) and  N opt ( M,L +1), the optimal number of stage computations  T opt ( M,N ) is computed via linear interpolation. For instance, to do the backtrace on  N =36 stages with  M =3 memory locations, we observe that  N  falls between  N opt ( M =3, L =5)=35 and  N opt ( M =3, L =6)=48. Thus, the algorithm requires  T ( M =3, N =36)= T ( M =3, N =35)+6(36−35)=131 stage computations. Thus, in this case, the number of stage computations for the  L -level algorithm of Wheeler and Hughey ( 2000 ) is 29% higher. Fig. 1. Low memory comparison of algorithms. This figure exhibits the effect on the run time at low memory levels. Clockwise from the top, the curves come in 12 pairs, one each for  M =2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 15 and 20 memory locations. Within each pair, the curve for the  L -level algorithm of (Wheeler and Hughey,  2000 ) is first; as  M  increases these curves become increasingly ‘fractal’, with jumps in the run time at several scales. The curve for the optimal checkpointing algorithm is second in each pair; these curves are piecewise linear. The main drawback to the  L -level algorithm is that it can perform badly for a value of  N  that falls between  N WH ( M,L −1) and  N WH ( M,L ), for some  L . Wheeler and Hughey, ( 2000 ) propose an ‘( L,L −1)-level’ algorithm that performs better for these intermediate values of  N , but it is not optimal. Wheeler and Hughey ( 2000 ) also discuss ( L, L −1, …)-level algorithms and an optimal checkpointing algorithm, but do not provide a quick computation for choosing optimal checkpoints, nor do they give formulae to compute the number of stage computations for general values of  M  and  N . 2 METHODS The choice of the stages to checkpoint can be framed as an optimization problem. We write  T ( M,N ) for the number of stage computations that are needed by the optimal checkpointing algorithm for a backtrace through  N  stages, using room for  M  stages. When  N ≤ M , there is ample memory, and  T ( M,N )= N . At the other extreme, if  M =1 there is room to compute only one stage, and  N &gt;1 stages cannot be computed even with an infinite amount of time available. [Following the lead of Wheeler and Hughey ( 2000 ) we bar in-place calculations.] If  N &gt; M &gt;1, and if we choose  C  as the first stage to checkpoint, then we begin by computing the first  C  stages, 1, …,  C , by alternating the use of two memory locations. We store stage  C , discarding the previous ones. Then, using the remaining  M −1 memory locations, we recursively perform an optimal backtrace on the final  N − C  stages,  C +1, …,  N . Next, we present the retained stage  C  to the user. Finally, we discard stage  C  and recursively perform an optimal backtrace on the initial  C −1 stages, using the full  M  memory locations. Thus, with an optimal choice for  C , we have the recursion for  T ( M,N ):
 (6) Note that Wheeler and Hughey ( 2000 ) give a different recursion for optimal checkpointing. Translated into our notation, their recursion is  T ( M,N )=min C { C + T ( M −1, N − C )+ T ( M,C )−1}. This error may have impeded their further progress. A straightforward computation of this recursion would require 𝒪( M N 2 ) time (Wheeler and Hughey,  2000 ). However, in the following we will show a mathematical solution to the recursion that permits the calculation of all the needed checkpoints in 𝒪( N ) time. As with the analysis of the  L -level algorithm of Wheeler and Hughey ( 2000 ), we find it easier to initially restrict our attention to special values of  N . The main contribution of this work is our subsequent generalization to arbitrary values of  N . 2.1 Special values for the number of stages With storage for  M ≥1 stages, and for any integer  L ≥1, we will define a special number of stages  N opt ( M,L ), and we will calculate  T opt ( M,L ), the number of stage computations required for a backtrace through  N opt ( M,L ) stages using  M  memory locations. Let
 (7) 
For  M,L &gt;1, with  N = N opt ( M,L ) stages, we set
 (8) 
the unique optimum checkpoint stage for the problem with  M  memory locations and  N = N opt ( M,L ) stages (see Appendix for proofs). Plugging  C ( M,N )= N opt ( M,L −1)+1 and  N −C( M,N )= N opt ( M −1, L ) into Equation ( 6 ), we obtain
 (9) 
This solves to
 (10) 
 (11) 
where Equation ( 11 ) is defined for  M ≥2 only. Computed values of  N opt ( M,L ) and  T opt ( M,L ) for small  M  and  L  are given in  Table 1  and plotted in  Figure 1 . As with the  L -level algorithm of Wheeler and Hughey ( 2000 ), with the optimal checkpointing algorithm, we have a multiplier for the number of stage computations of approximately  L  for   memory locations. However, we shall now see that, with the optimal checkpointing algorithm, we easily achieve this multiplier for the number of stage computations even when the number of stages  N  is arbitrary. Fig. 2. The optimal checkpointing algorithm in pseudo-C++, for a backtrace through  N  stages using memory sufficient for  M  stages. Using Equation ( 7 ), find the level  L =max{ L  :  N opt ( M,L )≤ N }. For the convention that the memory locations are labeled 0, …,  M −1 and the stages are labeled 0, …,  N −1, invoke backtrace(−1,  M , −1,  N, L, N opt ( M,L ),  advance, available, p ); where  advance  is a pointer to a callback function that computes stage  N to , to be stored in memory location  M to , from the immediately preceding stage, which is stored in memory location  M from  unless  N to  is the first stage; where  available  is a pointer to a callback function invoked during backtrace so that the user can make use of stage  N , stored in memory location  M ; and where  p  is a user-supplied pointer to applicable stage-independent information. BIGINT should be an integer type able to handle integers a little larger than  N M 2 . Note that, although the  backtrace  routine directs the callback routines on the use of the memory locations, the actual allocation and access of the memory is not handled by the  backtrace  routine. Further, note that if the generality is not required, the pointer parameters,  advance, available  and  p , can be eliminated, and their use in the body of the function can be replaced by ‘hard-wired’ calls to appropriate functions. See the Supplementary Materials for C++source code. 2.2 General values for the number of stages When  N  falls between two optimal values,  N opt ( M,L ) and  N opt ( M,L +1), we can compute the number of stage computations  T ( M,N ) by linear interpolation between  N opt ( M,L ) and  N opt ( M,L +1) (see Appendix for proofs). Noting that
 (12) 
we derive
 (13) 
for  N ≥ M &gt;1. Furthermore, for  N  between  N opt ( M,L ) and  N opt ( M,L +1), it is optimal to choose the initial checkpoint  C ( M,N ) so that  C ( M,N )−1 and  N − C ( M,N ) fall between the values that they would have had to equal, if  N  had equaled  N opt ( M,L ) or  N opt ( M,L +1). That is, we must choose  C ( M,N ) so as to simultaneously satisfy
 (14) 
and
 (15) 
In practice, we choose the largest legal value,
 (16) The optimal checkpointing algorithm is presented in  Figure 2 . Note that we include not just  M , and  N , but also a level  L  and a special stage  N opt ( M,L ) in the parameter list for the recursive subroutine, because the availability of values for  L  and  N opt ( M,L ) greatly speeds the calculations of  N opt ( M −1, L ) and  N opt ( M,L −1), which are needed in the calculations of optimal checkpoints:
 (17) 
 (18) It is imperative that the required calculations be impervious to integer overflows. We initially prevented overflow in integer calculations, such as  abc / de  for Equations ( 17 ) and ( 18 ), by canceling all common factors between each variable in the numerator and each variable in the denominator. This approach requires 3 × 2=6 invocations of Euclid's algorithm for finding a greatest common divisor. Such a procedure leaves the value of each denominator variable at 1 and, when  N opt ( M,L )+1 can be represented as an integer, the numerator variable values are sufficiently small enough to permit all the needed computations—so long as extra care is taken when verifying that the initial value of  N  is less than  N opt ( M,L +1). To handle computations where the number of stages exceeds the largest unsigned integer, often 2 32 −1≈4 × 10 9 , we modified our C++software implementation to use a C++class that manipulates integers of arbitrary size. 3 SOFTWARE Sample C++-code for optimal backtrace is available in the  Supplementary Materials . 4 RESULTS We applied the algorithm to pairwise local alignments (Smith and Waterman,  1981 ) of sequences of up to 3000 nucleotides of human DNA with sequences of up to 2864 nucleotides of rodent DNA. For the largest of the alignments, to keep within a 125 MB limit for total memory use, we restricted ourselves to  M =486 stages of storage for the  N =2864 stages. For these choices,  L =1,  N opt ( M =486,  L =1)=486, and  T ( M =486,  N =2864)=5242. Thus, the multiplier for the number of stage computations is  T / N =5242/2864≈1.83 for memory use  M / N =486/2864≈17%. The algorithm ran in 70 s, but would have run much more slowly if it had tried to use memory for all 2864 stages, because the resulting memory swapping would have been onerous. In contrast, the 2-level algorithm of (Wheeler and Hughey,  2000 ) computes checkpoints for stages 486, 971, 1455, 1938 and 2420, and requires two computations for all other stages with index under 2420. Thus its total number of stage computations is 2864+(2420−5)=5279, only slightly worse than 5242. The same calculation performed on a pair of sequences, each 10 000 nucleotides long, takes 12 min to run in 125 MB of memory, a memory size sufficient to store only 138 stages instead of the full 10 000 stages. For a problem of this size,  L =2,  N opt ( M =138,  L =2)=9728, and  T ( M =138,  N =10 000)=20 134. Thus, the multiplier for the number of stage computations is  T / N =20 134 / 10 000≈2.01 for memory use M / N =138/10 000≈1.4%. In contrast, the 3-level algorithm ofWheeler and Hughey ( 2000 ) is at a particular disadvantage in that it computes its only 3-level checkpoint at stage 9591, with subsequent 2-level checkpoints at 9728, 9864, and 9999. The algorithm requires 29 448 stage computations, significantly worse than 20 134. With 1 GB of memory, sufficient for storing 1104 stages for the pairwise alignment of sequences of length 10 000 nucleotides, the optimal checkpointing algorithm requires 18 896 stage computations, whereas the 2-level algorithm of Wheeler and Hughey ( 2000 ) requires 19 891 stage computations, almost 1000 more. On similar datasets, using a probabilistic model that defines a probability distribution on the set of possible alignments, we used the optimal backtrace algorithm to compute a centroid (Ding and Lawrence,  1999 ), also known as a posterior decoding (Miyazawa,  1995 ). This task requires a dynamic programming calculation during the backtrace that is comparable to the calculation performed during the forward pass. With sufficient memory, the total computation would require 2 N  stage computations, thus the multiplier for the number of stage computations with limited memory is
 (19) 
this value is better than  L , the multiplier of the number of stage computations for the cheaper backtrace tasks. We also can draw independent samples from the probability distribution on the set of possible alignments. Here, the run time is as slow as the centroid calculation only when the number of samples to be drawn is of the order of the smaller of the two sequence lengths. 5 DISCUSSION We have provided an algorithm for optimal backtrace through a dynamic programming algorithm when memory is limited. The algorithm improves upon previous work via the simplicity and speed of the calculation for the index of the optimal checkpoint, and via achievement of optimal performance for a problem of arbitrary size. A few variations are worthy of consideration. Generally, for backtrace computations, whether or not they are achieved with the optimal checkpointing algorithm described here, the first stage is computed from initial or boundary conditions, and each subsequent stage is computed from the immediately preceding stage. Thus, at least conceptually, the first stage requires special treatment. If this distinction makes implementation of the  advance  callback routine difficult, it may be prudent to compute and permanently store the first stage in the first memory location, and to run the optimal checkpointing algorithm so as to provide an optimally computed backtrace through the remaining  N −1 stages using  M −1 memory locations. The number of stage computations for this approach is 1+ T ( M −1, N −1). As already described for both optimal checkpointing and the  L -level algorithm of Wheeler and Hughey ( 2000 ), in the limit as the number of memory locations  M  goes to infinity with a fixed multiplier  L  for the number of stage computations, we can backtrace through  N  ∼  M L  /  L ! stages with  T  ∼  M L  / ( L −1)! stage computations. However, for the case when memory is severely limited, it is instructive to look at the asymptotics for a fixed value of  M , with  L  tending to infinity. For this situation we have
 (20) 
 (21) 
 (22) 
 (23) 
Thus, in these low-memory situations, the optimal algorithm is asymptotically faster than the  L -level algorithm of Wheeler and Hughey ( 2000 ), even when the latter is applied to its optimal problem sizes  N WH ( M,L ). The speed multiplier is  , which is approximately 1+(0.693}/( M −1.347)) for moderate values of  M . See  Table 1  and  Figure 1 . 6 CONCLUSION When high-speed memory is limited, dynamic programming algorithm backtraces make use of checkpoints for re-computing needed intermediate values. We have provided an easy-to-use algorithm for optimally selecting the checkpoints. 
          Supplementary Material 
          
             
                [Supplementary Data] 
             
             
             
          
       </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Hopper: a mathematically optimal algorithm for sketching biological data</Title>
    <Doi>10.1093/bioinformatics/btaa408</Doi>
    <Authors>DeMeo Benjamin, Berger Bonnie</Authors>
    <Abstract/>
    <Body>1 Introduction Recent improvements in single-cell technologies have enabled high-throughput profiling of individual cells, allowing fine-grained analyses of biological tissues. Droplet-based technologies have enabled profiling of millions of cells in a single experiment. Even larger datasets, containing tens or hundreds of millions or even billions of cells, are imminent ( Angerer  et al. , 2017 ). For example, the Human Cell Atlas project aims to characterize and classify all cells in the human body ( Rozenblatt-Rosen  et al. , 2017 ). While these large-scale assays have enormous scientific and therapeutic potential, they also present significant computational and analytic challenges. Even the most basic exploratory analyses—visualization, clustering and removal of batch effects—become intractable for more than tens of thousands of cells. Clinically or scientifically relevant cells are often far outnumbered by common cell types ( Hie  et al. , 2019 ). Thus, there is a pressing need to produce  sketches  that reduce the size of single-cell datasets while preserving their transcriptional diversity. There are several recent methods with this aim. Dropclust ( Sinha  et al. , 2018 ) performs Louvain clustering on an approximate nearest-neighbor network ( Blondel  et al. , 2008 ), and uses the resulting clusters as points of reference for downsampling. However, clustering itself is a very difficult and computationally expensive task, with the quality of the resulting sketches depending entirely on the clustering algorithm. The recently introduced Geometric Sketching ( Hie  et al. , 2019 ) samples evenly across transcriptional space by covering the Principal component-reduced dataset with a gapped grid of disjoint axis-aligned hypercubes of uniform size, and sampling a point at random from each. Geometric Sketching is very fast; yet, as we shall show, the fixed gridding axis can lead to artificial clusters near the grid intersections, potentially negatively affecting downstream analyses. Moreover, neither of these methods provides mathematical guarantees as to the approximation quality of the output sketches. To address these challenges, we introduce Hopper, a novel toolkit that produces sketches with mathematical optimality guarantees on the distance from a point in the original data to the nearest point in the sketch. It achieves this result by implementing  farthest-first traversal , a provably optimal polynomial-time approximation to the  k -center problem. Intuitively, this means that every point in the full dataset  X  is very close to some point in the sketch  S . Compared to Geometric Sketching, the current state of the art, Hopper dramatically improves the quality of sketches as measured by the Hausdorff distance, and better represents low-dimensional substructures without artifacts introduced by gridding. Unlike all prior methods, Hopper allows fast insertion and removal of cells from the sketch, whilst preserving strong mathematical guarantees. This enables fast multi-resolution analyses of large datasets. Hopper uses the triangle inequality to speed up farthest-first traversal, yielding feasible runtimes on today’s largest datasets. However, producing large sketches of large datasets is slow compared to Geometric Sketching ( Fig. 1 ). To address this issue, we introduce Treehopper, which leverages spatial partitioning to reduce the runtime by orders of magnitude without significant loss in performance. In particular, Treehopper yields lower Hausdorff distances than any prior approach, with speed comparable to or faster than Geometric Sketching ( Fig. 1 ). We thus recommend Treehopper as the state of the art for sketching large datasets.
 Fig. 1. Hausdorff distances and runtimes for various Hopper and Treehopper routines, with Geometric Sketching for comparison, on ∼1.3 million mouse neurons ( a ,  b ) and ∼2 million developing organ cells ( c ,  d ). For the Treehopper tests, the number of partitions  d  is indicated parenthetically in the legends. The basic Hopper routine produces the lowest Hausdorff distance obtainable in polynomial time, with our faster Treehopper routines nearly realizing the optimum. All significantly outperform Geometric Sketching, and show more consistent Hausdorff performance. Both Hopper and Treehopper take time linear in the sketch size, with slope depending on the overall dataset size and the degree of pre-partitioning. Geometric Sketching performs variably depending on the dataset’s geometry The code for Hopper and Treehopper is freely available at  https://github.com/bendemeo/hopper . In addition, we have provided pre-computed sketches of many of the largest single-cell datasets, available at  http://hopper.csail.mit.edu . 2 Algorithm 2.1 Overview of Hopper At the core of Hopper is the  farthest-first traversal , an elegant greedy approximation to the  k -center problem. Here the goal is to minimize, for some sample  S  of size  k  from a ground set  X , the  Hausdorff distance d H ( S , X ) = max ⁡ x ∈ X min ⁡ s ∈ S d ( x , s ) where  d  is a metric of choice (in our experiments, we use the Euclidean metric). The algorithm works by sampling an initial point from  X  at random, and repeatedly adding to the sample the point  p  that is furthest from any of the previously sampled points:
 (1) p = arg   max x ∈ X ( min ⁡ s ∈ S d ( x , s ) ) . Intuitively, we repeatedly add to  S  the point of  X  that is least well-represented by  S . We call this  hopping , and implement it in the "hop" function of the Hopper module. By design, this method is guaranteed to strictly decrease the Hausdorff distance  d H ( X , S )  after each step, assuming that the maximum is realized by only one point. In fact, one can show the following: Theorem  1.  Suppose that S is a k-step farthest traversal of X. Then ,
 d H ( X , S ) ≤ 2 d H opt ( X , k ) where   d H opt ( X , k ) is the optimal Hausdorff distance realized by any subset of size k. Thus, farthest-first traversal realizes a 2-approximation to the optimal Hausdorff distance. The proof of this Theorem is found in the study by  Gonzalez (1985) . The following theorem, due to  Hochba (1997) , shows that we cannot reasonably hope to do any better: Theorem  2.  Let   α &lt; 2 . Then, unless P = NP, there is no polynomial time algorithm for producing a set S satisfying d H ( X , S ) ≤ α · d H opt ( X , k ) . Thus, Hopper provides a gold-standard for sketching in the sense that no algorithm can reliably obtain a better Hausdorff distance, unless  P  =  NP . The output of Hopper is an ordered collection of  x 1 , x 2 , … , x k  of cells from  X , such that for any  ℓ ≤ k , the subset  x 1 , … , x ℓ  reaches within a factor of two of the lowest possible Hausdorff distance for any sketch of size  ℓ . 2.1.1 Geometric speedups The most computationally expensive aspect of farthest-first traversals is identifying the point  p  from  Equation 1 . To do so, one must maintain for each  x ∈ X , the distance to the nearest point in  S . Each time a point is added to  S , these distances must be updated. A naïve approach computes the distance from every  x ∈ X  to the newly added  p , and updates the minimum distances accordingly. This requires  O ( n ) time for each point addition, where  n  is the size of  X . Producing a sketch of size  k  thus takes  O ( nk ) time, which can be prohibitive for large sketches of large datasets. Various speedups have been proposed in the theoretical computer science community (e.g.  Har-Peled and Mendel, 2006 ), but all scale poorly with the dimensionality of the dataset. Instead, Hopper implements two simple geometric speedups using the triangle inequality. First, if the newly added point  p  has distance  r  to its nearest representative in  S , then by the triangle inequality,
 r ≤ d ( s , p ) ≤ d ( s , x ) + d ( x , p ) for any  s ∈ S  and  x ∈ X . In particular, if  d ( x , p ) ≤ d ( s , x ) , then we must have  d ( s , x ) ≥ r 2 . Thus, we need only examine those points in  X  with distance  ≥ r 2  to their nearest point in  S . To quickly find these points, the points  X  are sorted by their distance to the nearest point of  S . Second, for  s ∈ S , if  d ( s , p ) ≥ 2 r , then if  x ∈ X  is closest to  s , the triangle inequality gives:
 d ( s , x ) ≥ d ( s , p ) − d ( x , p ) ≥ r so there is no need to update any of the points associated to  s . These two observations often allow significantly fewer than  n  points to be examined at each iteration. The exact runtime depends on the dimensionality and geometry of the dataset ( Yu  et al. , 2015 ), but in practice the speedup is noticeable, especially for the first few thousand cells ( Fig. 1 ). 2.2 Overview of Treehopper In spite of the speedups discussed above, Hopper may still be prohibitively slow on very large datasets ( Fig. 1 ). We therefore introduce Treehopper, a derivative of Hopper which newly uses spatial partitioning to drastically speed up sketch generation, with little loss in performance. The dataset  X  is first divided into disjoint subsets  X 1 , … , X d  using Principal Component Trees (PC-trees), which hierarchically split the data into equal halves along the leading principal component ( Verma  et al. , 2009 ). A Hopper  H i  is instantiated in each partition  X i , beginning a farthest-first traversal  S i  of  X i . These Hoppers are sorted according to their Hausdorff distance  d H ( S i , X i ) . At each step, the Hopper with highest Hausdorff distance hops, adding a point to  S i , and adjusting its position in the sorted list of Hoppers. The final sketch is the union of all the sub-traversals  S i . Treehopper is inspired in part by Geometric Sketching, which demonstrates the utility of spatial partitioning for rapid sketch generation. However, in contrast with Geometric Sketching, where the partitions are all hypercubes of the same size and a point is drawn from each, Treehopper allows partitions to occupy variable-sized regions of transcriptional space, and draws variable numbers of points from the partitions according to their individual geometries. Thus, Treehopper bridges the gap between the fast partition-and-sample approach and the slower, but mathematically optimal, farthest-first traversal approach. We thus recommend Treehopper, and incorporate it into Hopper as the method of choice. Using a fast heap implementation, Treehopper achieves an average hop time of  O ( n / d )  instead of  O ( n ). Within each partition, the traversals  S i  realize the optimality bound of Theorem 1, but this bound may not be achieved globally. This tradeoff between time and performance is fully tunable. If  d  =   1, we achieve optimal polynomial-time performance in  O ( nk ) worst-case time. On the other extreme, if  d  =  n , a random subsample is produced in  O ( k ) time. For  d -values in the tens to hundreds, these methods produce drastic speedups with little loss in accuracy, sketching today’s largest datasets with very low Hausdorff distance in a matter of minutes ( Fig. 1 ). Thus, Treehopper improves the state of the art in both time and sketch quality. 3 Experimental results 3.1 Hopper better approximates biological datasets We assessed our method’s performance on two of the largest published single-cell RNA-seq experiments: A set of 1.3 million mouse neurons from 10X Genomics, and a set of ∼2 million mammalian organogenesis cells ( Cao  et al. , 2019 ). Each dataset underwent standard normalization and feature selection protocols, and was projected to its first 100 independent components. Consistent with our mathematical guarantees, Hopper obtained Hausdorff distances significantly lower than any prior sketching technique, showing empirically that all cells in the dataset are better-represented ( Fig. 1a, c ). These improvements remained significant even when Treehopper was used with as many as 256 pre-partitions, suggesting that pre-partitioning does not substantially reduce performance. In contrast with Geometric Sketching, Hausdorff distance decreases smoothly as the number of points increases, likely because Hopper and Treehopper are highly sensitive to individual outliers. Hopper, Treehopper and Geometric Sketching all require memory approximately equal to the size of the input dataset (data not shown). As expected, Hopper and Treehopper run approximately linearly in the dataset size, with slopes depending on the number of pre-partitions ( Fig. 1 ). Geometric Sketching shows variable time performance between the two tested datasets. We suspect that because Geometric Sketching relies on a binary search to select the correct grid size, runtime is heavily impacted by the number of search iterations needed, which depends rather unpredictably on the starting grid size and on the dataset’s geometry. Because even small sketch sizes may require several iterations, this leads to slower performance for small sketch sizes. On the other hand, the runtime may be faster for larger sketches ( Fig. 1b, d ). 3.2 Hopper reveals novel clusters of immune cells in mouse brain data Clustering is a key step in the analysis of single-cell data, allowing identification of known cell types, and discovery of new cell types, in a sample. Hopper facilitates better clustering by representing rare clusters even with small sketches. To demonstrate this, we used Hopper to order the first 5000 cells (about 0.4%) of the 1.3 million neuron dataset, and clustered the resulting cells using Louvain community detection ( Blondel  et al. , 2008 ). These cluster labels were then propagated to the full dataset via nearest-neighbor classification. The detected clusters, plotted and annotated in  Figure 2(a) , reveal several small but interesting cellular populations. For example one of the clusters, consisting of a mere 64 cells, showed elevated expression of the  Cd5l  gene, which is expressed by macrophages in inflamed tissues ( Sanjurjo  et al. , 2015 ) ( Fig. 2a ). Another cluster consisted of just 114 cells with elevated expression of  Pf4  and  F13a1 , marker genes for activated platelets ( Newman and Chong, 2000 ) ( Fig. 2a, c ). Another, consisting of just 221 cells, showed elevated expression the Interferon- β  gene  Ifnb1 , expressed in fibroblasts and monocytes in response to viral infection ( Hu  et al. , 2007 ). Clusters 2–4 express canonical microglial markers, highlighting the transcriptional diversity of this group ( Hammond  et al. , 2019 ;  Lee  et al. , 2008 ).  Figure 2(c)  shows expression heatmaps for each of these genes. Considering the role of the immune system in modulating disease states, these clusters are likely clinically important despite their small size.
 Fig. 2. ( a ) Louvain clustering on the 5000-cell Hopper sketch of the 1.3 million-cell mouse brain dataset. Each cluster is numbered, and biologically interesting clusters are annotated with their inferred identity. ( b ) Table showing the cell counts per cluster after nearest-neighbor classification on the whole dataset, and the top differentially expressed genes in each cluster. ( c ) Heat maps showing the expression of four different marker genes in a Hopper sketch of 5000 mouse brain cells out of 1.3 million. Elevated CD68 expression in the top half suggests a diverse population of immune cells. ( d ) Louvain clusters computed on the entire dataset fail to distinguish any of the cell subtypes identified by clustering on the sketch (see main text) 
 Figure 2(b)  lists all clusters, together with their sizes and differentially expressed genes relative to the total. Remarkably, almost all of the clusters computed from the Hopper sketch are extremely small relative to the full dataset size, indicating that miniscule populations can account for a large proportion of the dataset’s transcriptional diversity. These populations are completely invisible to any analysis of the full dataset. For example the Louvain clustering produced by scanpy ( Wolf  et al. , 2018 ) on the full dataset lumps all of the immune cell clusters into a single relatively small cluster of 8856 cells, obscuring their true diversity ( Fig. 2d ). This reflects a fundamental limitation of all modularity-based approaches, as documented in  Kumpula  et al.  (2007) : as the size of a dataset increases, so does the size of the smallest community that can be detected by modularity optimization. This has serious implications for single-cell pipelines: it is mathematically impossible to detect sufficiently small populations of cells via Louvain clustering, which remains the most popular method. Hopper and Treehopper circumvent this limitation by reducing the overall dataset size whilst retaining rare cell types, thus increasing the proportion of rare cells in the overall sample to the point where Louvain clustering can uncover them. Thus, our sketched datasets are not only more computationally manageable, but allow finer-grained detection of cellular populations as compared to the full data. 3.3 Hopper samples smoothly across low-dimensional substructures Geometric Sketching, the prior state of the art, covers the data with a gapped grid of axis-aligned boxes and samples a point from each box. This is a well-motivated approach that works well on many datasets. However, we have observed that axis-aligned grid hypercubes do not always represent the data evenly, especially where the local low-dimensional structure of the data aligns poorly with the gridding axis ( Yu  et al. , 2015 ). As demonstrated schematically in  Figure 3(a) , this results in more points near the grid square intersections. This effect is compounded as the ambient dimension  D  increases, since as many as  2 D  hypercubes may meet. As a result, we observe clumping even when the underlying data are Gaussian ( Fig. 3b ). On the mouse organogenesis dataset, this manifests as additional clusters not present in the Hopper sketches ( Fig. 3d ).
 Fig. 3. Grid-based sketches clump at grid intersections. ( a ) Schematic diagram, assuming the data lies near a one-dimensional line (red) in two-dimensional space. Where the line meets the grid intersection, four points are sampled, causing an artificial clump (circled). This effect is compounded in higher dimensions. ( b ) A sample geometric sketch on 2-D Gaussian data randomly embedded into 100-dimensional space. The 100 sampled points are shown in white, with the remaining points colored by grid cell. The grids partition the data erratically, and regions near grid intersections are preferentially sampled. ( c ) Hopper sketch of the same data, with 100 points colored according to their closest sampled point. The data are smoothly represented. ( d ) UMAP visualizations of sketches produced by Hopper, Geometric Sketching and by Treehopper with 32 partitions, coloured by cell type. Geometric Sketching generates additional clusters at grid intersections. Hopper and Treehopper avoid this issue Hopper avoids this issue entirely by not relying on any axis, ensuring that all low-dimensional substructures are smoothly represented regardless of spatial orientation ( Fig. 3c, d ). Sketches produced with Treehopper closely resemble those of Hopper, even with partition sizes less than 5% of the total sample size ( Fig. 3d ). We note that the pure-partitioning approach taken by Geometric Sketching does allow remarkably fast runtimes, and the artificial clumping effect does not occur on all datasets; indeed, geometric sketches of the 1.3 million mouse neuron dataset closely resemble Hopper sketches (data not shown). We suspect that the observed defect emerges primarily when the data have high intrinsic dimensionality, i.e. lie on a high-dimensional manifold, because this allows for more high-dimensional intersections between occupied grid hypercubes. 4 Discussion Hopper leverages the mathematical power of farthest-first traversal to produce sketches that preserve a sample’s transcriptional diversity and biological meaning. These sketches are mathematically guaranteed to represent the original data as well as any polynomial-time algorithm, thus providing a much-needed gold standard. By incorporating the powerful partition-and-sample approach implemented in Treehopper, we allow tunable scaling to massive-scale single-cell datasets without excessive computational burden. We have provided the first 50 000 cells in the far traversal of two super-massive single-cell RNA-seq datasets. This data require only a few megabytes of storage, but allow immediate production of mathematically optimal sketches of any size smaller than 50 000. This allows the researcher immediate access both to small sketches, which may isolate the rare cell types, and larger sketches, which may be more comprehensive at the expense of obscuring rare cell types. Indeed, the position of a cell in the far traversal produced by Hopper may prove a valuable input to other downstream analyses. For example, one could modify the Louvain community detection algorithm by weighting vertices according to their traversal positions, and modifying the modularity-detection step to ensure that both rare and common clusters are represented. The experiments in this article exclusively use Euclidean distance as a measure of dissimilarity, but Hopper also generalizes to arbitrary dissimilarity measures, provided that they satisfy the triangle inequality (e.g. Manhattan distance, Minkowski distance, etc.) Unlike other methods, an explicit embedding of the cells is not required—only a method of determining distance. As demonstrated by kernel support vector machines, this is a highly desirable property. There are several existing machine algorithms for learning discriminative metrics from single cell datasets, which can be directly fed into the Hopper framework. For example SIMLR ( Wang  et al. , 2017 ) uses machine learning to jointly predict the clustering and the distance measure. Other possibilities abound, from established kernels (e.g. polynomial kernels or radial basis functions) to custom-designed kernels which may incorporate prior knowledge about the relevant factors shaping a dataset’s diversity. Because the distance function can be user-specified, inputting such custom kernels into the Hopper framework is very straightforward. Hopper offers a flexible, scalable and mathematically principled workflow for distilling the essence of a single-cell dataset. As these datasets grow, such methods will become increasingly vital for enabling the advanced and computationally expensive downstream workflows that the future of single-cell data undoubtedly holds. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Significant speedup of database searches with HMMs by search space reduction with PSSM family models</Title>
    <Doi>10.1093/bioinformatics/btp593</Doi>
    <Authors>Beckstette Michael, Homann Robert, Giegerich Robert, Kurtz Stefan</Authors>
    <Abstract>Motivation: Profile hidden Markov models (pHMMs) are currently the most popular modeling concept for protein families. They provide sensitive family descriptors, and sequence database searching with pHMMs has become a standard task in today's genome annotation pipelines. On the downside, searching with pHMMs is computationally expensive.</Abstract>
    <Body>1 INTRODUCTION Profile hidden Markov models (pHMMs) are currently the most popular modeling concept for protein families. They provide very sensitive family descriptors, and sequence database searching with models from major pHMM collections (Finn  et al .,  2006 ; Haft  et al .,  2003 ) has become a standard task in sequence analysis. On the downside, database searching with pHMMs with well-known programs like  hmmsearch  or  hmmpfam  (Eddy,  1998 ) is computationally expensive. In particular, the long running times of pHMM-based methods and the time scaling behavior, which is linear in the length of the searched sequence, make them more and more demanding in today's sequence database search scenarios. This problem will become even more severe as the continuing exponential growth of sequence databases will certainly be amplified by the increasing dispersal of next-generation sequencing technologies (Shendure and Ji,  2008 ). Nevertheless, pHMM-based database searches are indispensable for today's genome annotation pipelines. For instance, the majority of member databases of the InterPro classification system (Hunter  et al .,  2009 ), a widely used system for protein annotation purposes, employ family information in form of pHMMs. The applied classification procedure  InterProScan  (Quevillon  et al .,  2005 ) includes searches with all pHMMs from the  Pfam  (Finn  et al .,  2006 ),  TIGRFAM  (Haft  et al .,  2003 ),  Superfamily  (Gough  et al .,  2001 ),  PIRSF  (Wu  et al .,  2004 ),  Gene3D  (Yeats  et al .,  2006 ),  Smart  (Letunic  et al .,  2006 ) and  Panther  (Mi  et al .,  2005 ) databases. These pHMM-based database searches render  InterProScan  a very compute intensive application whose employment on a large scale is challenging even on the largest cluster systems. To solve this dilemma, much effort has been spent on improving the running time of pHMM-based database search tools. Some approaches for improvement use parallelism techniques and/or fast, extended, CPU-specific instructions sets, like SSE/SSE2 (Streaming Single Instruction/Multiple Data Extensions) (Walters  et al .,  2006 ). Hardware solutions implementing proprietary variants of  hmmsearch  on special field- programmable gate array (FPGA) boards are also available. Moreover, the application of machine learning techniques has been suggested (Lingner and Meinicke,  2008a ,  b ). Very recently, Sun and Buhler ( 2009 ) described the design of patterns and profiles for speeding up  hmmsearch  using unordered sets of motifs in form of PROSITE-like patterns or position-specific scoring matrices (PSSMs) derived from a multiple alignment of a protein family. These motifs are then searched with standard regular expression matching and profile searching algorithms, respectively, to prefilter the search space for subsequent application of  hmmsearch . The reported speedups over unfiltered search are in the range of 20-fold with almost 100% sensitivity and 30- to 40-fold with 90% sensitivity. We propose a new software-based method well suited: (i) for efficient and reliable protein family classification, and (ii) to speedup database searches with  hmmsearch . Our approach employs a simpler model of protein families based on PSSMs in combination with exact  p -value computation using lazy evaluation and full text indexing with enhanced suffix arrays (Abouelhoda  et al .,  2004 ) to filter the search space for subsequent database searches with pHMMs corresponding to these families. The work is an extension of our PSSM search tool  PoSSuMsearch  (Beckstette  et al .,  2006 ), so we briefly describe previous work on index-based PSSM matching and efficient  p -value computation for PSSM matchscores ( Sections 2.2  and  2.3 ) before describing the new concepts and algorithms used in the new version of  PoSSuMsearch  ( Sections 2.4 – 2.6 ), herein after referred to as  PoSSuMsearch2 . 2 METHODS 2.1 Preliminaries Let  S  be a sequence of length  n  over finite alphabet 𝒜, and let  S [ i .. j ], 0≤ i ≤ j ≤ n  − 1, denote the substring of  S  of length  j  −  i  + 1 starting at position  i  and ending at (including) position  j . Let $ be a symbol in 𝒜, larger than all other symbols, which does not occur in  S . The suffix array  suf  is a table of integers in the range 0– n  that lists the starting positions of all  n  + 1 suffixes of  S $ in lexicographical order (symbol $ must be appended to  S  to obtain a well-defined order on suffixes). That is,  S suf [0] ,  S suf [1] ,…,  S suf [ n ]  is the sequence of suffixes of  S $ in ascending lexicographic order, where  S i  =  S [ i .. n  − 1]$ denotes the  i -th non-empty suffix of the string  S $, for  i  ∈[0,  n ].  lcp  is a table in the range 0– n  such that  lcp [0]≔0 and  lcp [ i ] is the length of the longest common prefix of  S suf [ i −1]  and  S suf [ i ] , for  i  ∈[1,  n ].  skp  is a table in the range 0– n  such that  skp [ i ]≔min({ n +1}∪{ j ∈[ i +1,  n ]∣ lcp [ i ]&gt; lcp [ j ]}). In terms of suffix trees,  skp [ i ] denotes the lexicographically next leaf that does not occur in the subtree below the branching node corresponding to the longest common prefix of  S suf [ i −1]  and  S suf [ i ] . Tables  lcp  and  skp  can be computed as a by-product during the construction of suffix array  suf , and enhance the basic suffix array. All three tables can be computed in linear time (Kärkkäinen and Sanders,  2003 ; Kasai  et al .,  2001 ). For a linear time construction algorithm for table  skp , see  Figure 1  in the  Supplementary Material . We refer to the troika of tables  suf ,  lcp  and  skp  as  enhanced suffix array . See  Figure 1  for an example.
 Fig. 1. Enhanced suffix array for  S  =  tccatcacct , consisting of the suffix array  suf , and additional tables  lcp  and  skp . The suffixes of  S  are sorted lexicographically (rightmost column). A PSSM is an abstraction of a multiple alignment and is defined as a function  M  : {0,…,  m  − 1} × 𝒜→ℝ, where  m  is the length of  M , also denoted with | M |, and 𝒜 is a finite alphabet. Usually function  M  is given by an  m  × |𝒜| matrix, where each row of the matrix reflects the frequency of occurrence of each amino acid or nucleotide at the corresponding position of the alignment. From now on, let  M  be a PSSM of length  m  and let  w [ i ] denote the character of  w  at position  i  for 0≤ i &lt; m . The  score range  of a PSSM is the interval [ sc min ( M ),  sc max ( M )] with  sc min ( M ) ≔ ∑ i =0 m −1  min{ M ( i ,  a )∣  a  ∈ 𝒜} and  sc max ( M ) ≔ ∑ i =0 m −1  max{ M ( i ,  a ) ∣  a  ∈ 𝒜}. We define the  match score  for a segment  w  ∈ 𝒜 m  of length  m  of the sequence w.r.t.  M  as  sc ( w , M )≔∑ i =0 m −1   M ( i ,  w [ i ]). We also define  pfxsc d ( w ,  M ) ≔ ∑ h =0 d   M ( h ,  w [ h ]), max d  ≔ max{ M ( d ,  a ) ∣  a  ∈ 𝒜}, σ d  ≔ ∑ h = d +1 m −1  max h  and θ d  ≔ θ−σ d  for any  d  ∈[0,  m −1]).  pfxs d ( w ,  M ) is the  prefix score of depth d . σ d  is the maximal remainder score that can be achieved in the last  m  −  d  −1 positions of the PSSM and θ d  the  intermediate threshold  at position  d . Given a score threshold θ, PSSM  M  is said to match string  w  with threshold θ if and only if  sc ( w ,  M )≥θ. Hence, the PSSM matching problem is to find all matching substrings of length  m  in some sequence  S  with their assigned match scores for a given threshold θ and PSSM  M . 2.2 Fast database searching with single PSSMs 2.2.1 Algorithms for finding PSSM matches A naive 𝒪( mn ) time algorithm solving the PSSM matching problem moves a sliding window of size  m  over the text to be searched of length  n  and is implemented in many programs facilitating PSSMs (Henikoff  et al .,  2000 ; Kel  et al .,  2003 ; Quandt  et al .,  1995 ; Scordis  et al .,  1999 ). Considerable practical speedups can be obtained with the lookahead scoring technique of Wu  et al . ( 2000 ). It uses the implication  pfxsc d ( w ,  M ) &lt; θ d ⇒ sc ( w ,  M ) &lt; θ as an early stop criterion for the calculation of  sc ( w ,  M ). However, lookahead scoring does not improve the theoretical worst case time complexity of the naive algorithm. 2.2.2 Index-based searching with PSSMs For fast database searching with PSSMs,  PoSSuMsearch2  employs the algorithm  ESAsearch  (Beckstette  et al .,  2006 ), which in turn makes use of enhanced suffix arrays. To use enhanced for fast database searching with PSSMs, one simulates a depth first traversal of the suffix tree (cf. Abouelhoda  et al .,  2004 ) by processing the arrays  suf  and  lcp  from left to right. To incorporate lookahead scoring, the search skips over certain ranges of suffixes in  suf  using the information from table  skp . Algorithmic details are given in Beckstette  et al . ( 2006 ). The practical speedup of  ESAsearch  over methods that operate on the plain text is influenced by the choice of threshold θ. The larger the value of θ, the more likely it is to fall short of an intermediate threshold θ d  on average. This in turn means that the computation of the scores can be stopped earlier and more suffixes can be skipped by utilizing the information stored in tables  lcp  and  skp . As shown in Beckstette  et al . ( 2006 ), the expected runtime of  ESAsearch  is sublinear in the text length, whereas its worst case runtime is 𝒪( n  +  m ) under the special condition that  n ≥|𝒜| m  +  m  − 1 holds, independent of the chosen threshold θ. The high speed of  ESAsearch  is the foundation for the speedup of database searches with pHMMs described in the sequel. 2.3 Efficient computation of score thresholds from  p -values To differentiate between match and mismatch,  ESAsearch  requires a score threshold parameter θ. However, PSSM scores are not equally distributed and thus scores of two different PSSMs are not comparable. This makes it difficult to choose a global score cutoff, meaningful for all PSSMs. Individual score cutoffs must be derived from  p -values. This can be computed by dynamic programming (Rahmann,  2003 ; Staden,  1990 ; Wu  et al .,  2000 ), but is expensive as the complexity depends on the range of possible score values. For arbitrary floating point scores this problem is NP-hard (Touzet and Varré,  2007 ; Zhang  et al .,  2007 ).  PoSSuMsearch2  uses the  LazyDistrib  algorithm (Beckstette  et al .,  2006 ) to speedup the computation of exact  p -values for given PSSM scores. By lazily computing only the tail of the distribution function,  LazyDistrib  obtains a speedup of more than 300, compared with previous methods based on dynamic programming. For the special case of PSSMs employing floating point scores of several decimal digits,  p -value computation could be further improved by more than a magnitude using the method of Touzet and Varré ( 2007 ), but this has not yet been integrated in the  PoSSuMsearch  software. Building on these techniques for fast searching of single PSSMs, we now proceed to their generalization to PSSM family models (PSSM-FMs). 2.4 PSSM-FMs Let  A = A 1 ,  A 2 ,…,  A L  be a sequence of non-overlapping alignment blocks. These alignment blocks are excised from a multiple alignment and the indexing from 1 to  L  reflects their order of occurrence in the alignment. See  Figure 2 A for an example. A PSSM-FM ℳ of length  L  is a sequence of  L  PSSMs ℳ= M 1 ,  M 2 , …,  M L  where  M i  denotes the PSSM derived from  A i ,  i ∈[1,  L ]. The order ≪ of the PSSMs occurring in ℳ is imposed by the order of the corresponding alignment blocks. In practice, ℳ can be obtained from multiple alignments of related protein sequences (i.e. of a protein family). PSSMs can be computed from the blocks by several well-known methods (Gribskov  et al .,  1987 ; Henikoff and Henikoff,  1996 ; Tatusov  et al .,  1994 ). A match to ℳ is a non-overlapping sequence of matches for some or all of the PSSMs in ℳ in their specified order. We will now make this more precise.
 Fig. 2. ( A ) Non-overlapping alignment blocks, excised from ungapped regions of a multiple alignment. Since  l i ≤ r i &lt; l j ≤ r j  for 1≤ i ≤ j ≤5,  A = A 1 ,  A 2 ,  A 3 ,  A 4 ,  A 5  is an ordered sequence of non-overlapping alignment blocks suitable to construct a PSSM-FM ℳ= M 1 ,  M 2 ,  M 3 ,  M 4 ,  M 5 . ( B ) Matches of  M i ,  i ∈[1, 5], on sequence  S , sorted in ascending order of their start position. ( C ) Graph-based representation of the matches of  M i ,  i  ∈ [1, 5]. An optimal chain of collinear non-overlapping matches is determined, by computing an optimal path in the directed, acyclic graph. Observe that not all edges in the graph are shown in this example and that the optimal chain (indicated here by their black marked members) is not necessarily the longest possible chain. Consider a PSSM-FM ℳ with total order ≪. Let  MS  be the set of all matches for all  M ∈ℳ in sequence  S  of length  n . A match is represented by a triple ( M ,  p ,  s ) such that  M  matches at position  p  in  S  and  s ≔ sc ( S [ p .. p + m −1],  M ) is the corresponding match score. We say that matches ( M ,  p ,  s ) and ( M ′,  p ′,  s ′) are collinear, written as ( M ,  p ,  s ) ≪ ( M ′,  p ′,  s ′) if  M ≪ M ′ and  p +| M |≤ p ′. A chain 𝒞 for family model ℳ is a sequence of matches
 
all from  MS , such that ( M i ,  p i ,  s i ≪( M i +1 ,  p i +1 ,  s i +1 ) for all  i , 1≤ i ≤ k −1. To incorporate a measure of match quality into PSSM-FMs, we associate with ( M ,  p ,  s ) a  p -value π( M ,  s ) and a weight α( M ,  s ) defined by
 (1) 
The chain score of a chain 𝒞 is defined by
 (2) The motivation for Equation ( 1 ) is as follows. π≔π( M ,  s ) is the probability for the event that  M  matches a random string  w  of length  m  = | M | for score threshold  s  by chance, i.e. π = ℙ[ sc  ( M ,  w )≥ s ]. Thus, (1 − π) is the probability for the complementary event that  M  does not match a random string of length  m , and (1 − π) n − m +1  is the probability that there is no match in  n  −  m  + 1 random strings. This corresponds to the number of different positions that  M  can actually match in a string of length  n . Conversely, 1 − (1 − π) n − m +1  is the probability for the event that there is at least one in  n  −  m  + 1 random strings that matches  M  with a score at least  s . We take the logarithm to obtain additive weights and divide by ln( n ) to account for sequence length. The smaller the  p -values of the matches in a chain (i.e. the more significant the matches of single PSSMs  M  are), the larger the fragment weights get, and hence the overall chain score. Consequently, chains that consist of a number of significant matches are assigned larger chain scores than those with fewer, or many less significant matches. Equation ( 2 ) implicitly assumes independence of random strings, which is certainly an invalid assumption in our case as the ‘random strings’ are overlapping substrings of some longer sequence. Yet, our experiments confirm our chain scoring to work well in practice; it is significantly better than a more straightforward strategy that simply computes the product of raw  p -values, i.e. one that sets α( M ,  s )=−ln(π( M ,  s )) (see  Fig. 2  in the  Supplementary Material ). 2.5 A specialized and improved PSSM chaining method Thus far our description was based on a single sequence. However, the results described below are based on a large set of sequences  S 1 ,…,  S k . To handle these, we concatenate the single sequences with separator symbols, and construct the enhanced suffix array for the concatenation. For a given PSSM-FM ℳ, all  M i , 1≤ i ≤ L , are matched one after the other against the enhanced suffix array. This gives match sets  MS ( M i ) for PSSM  M i . The PSSM chaining problem for a single sequence  S j  can be considered a chaining problem for pairwise matches between sequence  S j  and a virtual sequence  V [1.. L ] such that a match for PSSM  M i  is a match of length one at position  i  in  V . The pairwise chaining problem can be solved in  O ( b log b ) time using an algorithm described in Abouelhoda and Ohlebusch ( 2005 ), where  b =| MS ( S j )| and  MS ( S j ) is the set of PSSM matches in  S j . This algorithm is called the general chaining algorithm. For the special case of the PSSM chaining problem, we have specialized and improved the general chaining algorithm to obtain a method with the following advantages:
 While the general chaining algorithm requires a dictionary data structure with insert, delete, predecessor and successor operations running in logarithmic time (e.g. an AVL-tree or a red-black tree), our approach only needs a linear list, which is much easier to implement and requires less space. While the general chaining algorithm requires an initial sorting step using  O ( b *log b *) time, our method only needs  O ( b *+∑ j =1 k ∑ i =1 L   b j , i log b j , i ) time for this step. Here,  b * is the total size of all sets  MS ( M i ) and  b j , i =| MS ( S j ,  M i )|, where  MS ( S j ,  M i ) is the set of all PSSM matches of PSSM  M i  in sequence  S j . While the general chaining algorithm solves the chaining problem for  MS ( S j ) in  O ( b log b ) time, our method runs in  O ( b · L ) time. If  L  is considered to be a constant, the running time becomes linear in  b . 
The details of the improved chaining method are described in the  Supplementary Material . 2.6 Using PSSM-FMs for sequence classification To employ PSSM-FMs for protein family classification, we combine the three algorithms sketched in  Sections 2.2 – 2.5 . Namely (i)  ESAsearch  for fast searching with single PSSMs; (ii)  LazyDistrib  for exact and efficient  p -value computation; and (iii) chaining of single PSSM matches in the form of the chaining method sketched in  Section 2.5 . All three algorithms are implemented in  PoSSuMsearch2  and, in combination provide, an efficient solution to the problem of protein family classification. In the first phase,  PoSSuMsearch2  computes single PSSM matches for the PSSMs of a family model using algorithm  ESAsearch . In the second phase, PSSM matches obtained in phase one and their ordering information are used to compute optimal chains of PSSM matches according to the order given in the family model. When classifying an unknown protein sequence into a known family, a sequence is searched with several PSSM-FMs, representing different protein families. The classification into a certain family should be based on the quality of the  best  match of a sequence to the corresponding family model. Hence, the objective is to determine the  best  chain 𝒞 * ℳ, S  of PSSM matches in a sequence  S  for a given family model ℳ and their chain score
 (3) 
 (4) 
We call such a chain an  optimal  chain. With the definition of optimal chains and their chain scores, we introduce a quantifiable, rankable criterion of match quality to our PSSM-FM concept, making it applicable for sequence classification. More precisely, let  S  be a sequence and ℱ = {ℳ 1 , ℳ 2 ,…, ℳ T } be a collection of  T  PSSM-FMs, representing  T  distinct protein families. Further, let  csc * ℱ, S ≔max{ csc * ℳ i , S ∣ℳ i ∈ℱ} be the maximal score of all optimal chains in  S  over all family models in ℱ. We classify  S  into the family represented by ℳ∈ℱ if and only if  csc * ℱ, S = csc * ℳ, S . That is, we classify the sequence under consideration into the family whose family model generates the highest scoring optimal chain. In practice, it is often useful to employ a threshold constraint, like a minimal necessary chain length, as a lower boundary for classification. That is, sequences not satisfying this constraint are not classified. PoSSuMsearch2  can be used in two modes of operation:
 Mode  modsearch  allows sequence classification based on a, typically small, library of PSSM-FMs. For each sequence the best matching chains for (up to)  k  different family models are reported. Mode  seqclass  allows sequence classification based on a, typically large, library of PSSM-FMs. For each model, the best matching chains in (up to)  k  different sequences are reported. 
 Hence, mode  modsearch  mimics the  modus operandi  of program  hmmsearch , whereas mode  seqclass  is comparable with program  hmmpfam . 3 RESULTS 3.1 PSSM-FMs for protein classification Detection of protein families in large databases is one of the principal research objectives in structural and functional genomics. To evaluate the suitability of  PoSSuMsearch2  employing PSSM-FMs for fast and accurate protein family classification, we rigorously tested and validated our method using the evaluation system Phase4 (Rehmsmeier,  2002 ). We evaluated the sensitivity and specificity, addressing different database search scenarios at different levels of difficulty. That is, we measured our method's ability to detect (A) very close, (B) close and (C) distant sequence relationships and compared the obtained results with those gained by the HMM-based  hmmsearch  from the  HMMER  package, which marks the state-of-the-art in this field. For the evaluation, separated training and test sets (i.e. the sets that define the true positives) were created from the  SCOP  database (Andreeva  et al .,  2008 ).  SCOP  contains protein sequences classified into families, superfamilies, folds and classes, depending on their structural relationships. To minimize the influence of redundancies on the results of our experiments, we used the non-redundant  PDB90  subset of  SCOP  (Rel. 1.75), which contains sequences with pairwise homology of at most 90%. This subset consists of a total of 15 440 amino acid sequences classified into 3890 families and 1955 superfamilies. 3.1.1 Model construction and scoring The three scenarios used for our evaluations differ in how training and test sets are constructed from  SCOP  data.  Table 1  and  Figure 3  give more details on the three scenarios. The task of the searching program in each case is to find, preferably, only protein sequences from the test sets in the whole  SCOP  database, while only providing the corresponding training sets to the searching program. That is, a perfect searching method would always find exactly the set of true positives, which is the test set.
 Fig. 3. Construction of training and test sets for ( A ) very close, ( B ) close and ( C ) distant relationships. 
 Table 1. Evaluation scenarios and number of models used in the experiments to assess method sensitivity and specificity Scenario (#models) Description (A) Very close relationship (561) For each superfamily: for each family, half of its sequences are chosen as test sequences, and the remaining ones are chosen as training sequences. The sequences of the surrounding superfamily are ignored in the evaluation. (B) Close relationship (474) For each superfamily, half of the sequences of each of its families are chosen as training sequences and the remaining ones are chosen as test sequences. (C) Distant relationship (1221) From a superfamily, each family in turn is chosen to provide the test sequences. The remaining families within that superfamily provide the training sequences. 
 Since some superfamilies in  SCOP  contain only one family and some families are very small, we employed the following criteria to select superfamilies and families for evaluation. Only superfamilies comprised of at least two families were selected. From these superfamilies, families were chosen to be test families, if both, the family itself and the remainder of the superfamily contained at least five sequences. The resulting numbers of families employed in each evaluation scenario are given in  Table 1 . From each training set we constructed a PSSM-FM for use with  PoSSuMsearch2  and a pHMM for  hmmsearch , respectively. With these models, we subsequently searched the sequences in the corresponding test set. Both model types are derived from a multiple alignment, which we compute from each training set using  CLUSTAL W  (Higgins  et al .,  1994 ) with default parameters. To construct PSSM-FMs, we excised from the multiple alignments all blocks of width 6–12, favoring wider blocks and allowing at most 20% gaps per column inside a block. For this task, we used the  BLOCKMAKER  program from the  BLIMPS  distribution (Henikoff  et al .,  1995 ). We retained the block order and computed from the blocks log-odd score-based PSSMs with the method of Henikoff and Henikoff ( 1996 ). For this, we estimated residue probabilities of observing a certain residue in a column of the alignment block from relative frequencies. From the same multiple alignments, calibrated pHMMs for disposition with  hmmsearch  were computed using  hmmbuild  and  hmmcalibrate  ( HMMER  package version 2.3.1, using the programs' default parameters). Thus, the so computed PSSM-FMs and pHMMs are descriptors for their respective training sets and serve as input for a database search with  PoSSuMsearch2  and  hmmsearch , respectively. In these searches, thresholds were set in a very relaxed way (for  hmmsearch   E -value cutoff 10 and for  PoSSuMsearch2  single PSSM  p -value cutoff of 0.1) so that all sequences irrespective of their score are reported. Matches to a model were ranked by their method-specific scores in descending order, i.e. in case of  PoSSuMsearch2  the best chain score  csc * ℳ, S , and in case of  hmmsearch  the sequence classification score. 3.1.2 Assessment of sensitivity and specificity To assess the sensitivity and specificity of our PSSM-FM approach and to compare the classification accuracy with  hmmsearch  in the three different evaluation scenarios, we determined the percentage of true positives in all test sets (also called the  coverage ) that is achieved by the method when accepting different counts of false positives. We plotted the (accepted) false positive counts versus the average percent coverage. See  Figure 4 .
 Fig. 4. Classification performance of  PoSSuMsearch2  using PSSM-FMs versus  hmmsearch  using pHMMs in the three evaluation scenarios. Shown are percentage true positives values averaged over all test families, called the average percent coverage value or just coverage for short ( y -axis), for different numbers of accepted false positives ( x -axis). 3.1.3 Comparison of runtime and scalability All benchmark experiments described in this article were performed on a single Intel Xeon CPU running at 2.3 GHz. For runtime experiments, we took the first 100 protein families from the  Pfam  Rel. 23.0 database (Finn  et al .,  2008 ), and computed PSSM-FMs from the  Pfam  seed alignments by excising alignment blocks as described above, but of width 5–8. This resulted in 100 models, consisting of 2096 individual PSSMs. From the same alignments, we generated 100 calibrated pHMMs using  hmmbuild / hmmcalibrate . We searched with these family descriptors in the  UniProtKB/Swiss-Prot  Rel. 57.5 database (Wu  et al .,  2006 ), containing 470 369 protein sequences in 167 MB. It took  PoSSuMsearch2  ∼28.1 min to find all matches to the PSSM-FMs, using a  p -value threshold of 10 −4  for each PSSM. For  hmmsearch , we chose an  E -value of 10 −5  in order to find roughly the same set of matches. It took  hmmsearch  ∼30 h to find matches to the pHMMs. This makes for a speedup factor of more than 64.8 for  PoSSuMsearch2  over  hmmsearch . Along with these results,  PoSSuMsearch2  clearly showed sublinear time scaling when applied to subsets of  UniProtKB/Swiss-Prot  of different sizes, whereas  hmmsearch  showed linear scaling behavior due to the underlying Viterbi algorithm. For the results of this experiment, see  Figure 3  in the  Supplementary Material . 3.2 Acceleration of pHMM-based database searches Here, we evaluate the performance of  PoSSuMsearch2  when it is used as a filter to reduce the search space for  hmmsearch . The combination of  PoSSuMsearch2  and  hmmsearch  is called  PSfamSearch . The intention is to speedup the database search while keeping the sensitivity of  hmmsearch . As a prerequisite for reliable filtering, we have to calibrate  p -value cutoffs for the PSSM-FMs such that they match the corresponding pHMMs  trusted cutoff  ( tc ) and  noise cutoff  ( nc ). That is, our calibrated PSSM-FMs operate on the same level of sensitivity as  hmmsearch  employing the pHMM, but with possibly reduced specificity. Hence, the determination of a proper family-specific  p -value cutoff is crucial for the sensitivity as well as overall speedup of  PSfamSearch . A too stringent cutoff may reduce the search space too much and thus may have a negative effect on the sensitivity. On the other hand, a too relaxed cutoff may not sufficiently reduce the search space and lead to long running times. In the following, we evaluate two different strategies for cutoff calibration: cutoff calibration for lossless filtering and cutoff calibration based on training- and test-set separation. 3.2.1 Cutoff calibration for lossless filtering We start by searching with a pHMM representing a protein family in a large protein database like  UniProtKB/Swiss-Prot  using  hmmsearch  with the model's trusted and noise cutoffs, respectively, and tabulate all matching sequences. From the seed alignment of the employed pHMM, we construct a PSSM-FM with a block width of 6–12 and use this family model to iteratively search  UniProtKB/Swiss-Prot  using  PoSSuMsearch2 . In each iteration, we relax the  p -value cutoff until we find all the sequences also detected by  hmmsearch  using the model's trusted and noise cutoffs respectively. With this procedure, we determine  p -value cutoffs denoted by π tc  and π nc  corresponding to the pHMM's trusted and noise cutoffs in terms of sensitivity. Observe that the set of matching sequences detected by  PoSSuMsearch2  using cutoff π tc  or π nc  may be a super set of the set of sequences detected by  hmmsearch  employing the pHMM's trusted and noise cutoffs. However, since we are interested in using PSSM-FMs searched with  PoSSuMsearch2  as a prefilter for search space reduction for  hmmsearch , sensitivity is more important than specificity. Once π tc  and π nc  are computed on a large protein database, they are, together with the PSSM-FM, stored on file. That is, for further searches with  hmmsearch  using the model's trusted or noise cutoffs, we can use  PoSSuMsearch2  using cutoff π tc  or π nc  as a filter and apply the compute intensive  hmmsearch  only on sequences that contain chains matching to the PSSM-FM. Sequences that contain no such chains are filtered out. Since sequences containing sufficiently long chains constitute only a small fraction of all sequences to be searched and since  PoSSuMsearch2  is much faster than  hmmsearch , we expect a reduced overall runtime. We tested this hypothesis with the following experiment. We applied  PSfamSearch  to the first 200 out of 3603 pHMMs of the  TIGRFAM  database (Rel. 8.0) on the complete  UniProtKB/Swiss-Prot  database (Rel. 57.5, 471 472 sequences of total length ∼167 MB). We determined  PoSSuMsearch2   p -value cutoffs corresponding to  hmmsearch  trusted as well as noise cutoffs with the iterative procedure described above. We measured the search space reduction (see  Supplementary Fig. 4  for results for the first 20  TIGRFAM  families) and the total runtimes of  PSfamSearch  and compared them with  hmmsearch  operating on the unfiltered dataset. PSSM-FM-based filtering reduces the search space and hence the overall runtime considerably. For example, for family TIGR00011 only five sequences remain after the filtering step and are handed over to  hmmsearch . Filtering with  p -value cutoffs corresponding to the less-stringent noise cutoffs reduced, in the worst case (TIGR00001), the search space by ∼80%. For all 200 tested families, the overall runtime is reduced from 4233 (4234) to only 46 (117) min when using trusted (noise) cutoffs. This is a speedup of factor 92 (36). We emphasize that in this experimental setup,  PSfamSearch  and direct  hmmsearch  obtain exactly the same results on the full sequence set. Hence,  PoSSuMsearch2  works as a perfect, lossless filter. This is not too surprising, since thresholds were trained/adjusted on the same set of sequences as were searched afterwards employing these thresholds. This raises the question, how well the calibrated  p -value cutoffs generalize to sequences not included in the training set used for threshold determination. To test this, we performed additional experiments where  p -value cutoffs are calibrated based on training- and test-set separation. 3.2.2 Cutoff calibration based on training- and test-set separation For the first 200 families listed in  TIGRFAM , we built PSSM-FMs from the families' seed alignments with the procedure described in  Section 3.1 . We calibrated the  p -value cutoffs and minimal chain lengths to match all sequences of training sets of different sizes. Training sets contain every  k -th sequence returned by direct  hmmsearch  on  UniProtKB/TrEMBL  Rel. 40.3 (7 916 844 protein sequences with a total length of 2.58 GB) for  k ∈{2, 3, 4, 5} using the pHMMs' trusted and noise cutoffs (for  k  = 2 only), respectively. That is, we evaluated the classification performance of  PSfamSearch  using training sets that consist of 20%, 25%, 33% and 50% of the sequences matched by the pHMM. We employed these models and cutoffs in a database search with  PSfamSearch  on complete  UniProtKB/TrEMBL  and measured the overall running time and true positives coverage per family and compared the running time with the time needed by direct  hmmsearch  using trusted cutoffs. See  Figure 5  and  Supplementary Table 1  for the results of this experiment.
 Fig. 5. Comparison of running times ( x - and  y -axis) and achieved percentage true positive values (color coded) between  PSfamSearch  and  hmmsearch , when searching with PSSM-FMs (pHMMs) representing the first 200  TIGRFAM  families on  UniProtKB/TrEMBL  Rel. 40.3. Different values of  k  represent different training set sizes. For further details see text. PSfamSearch  returned &gt;99.54% of the original results determined by  hmmsearch , including their  E -values and scores, when using half of the sequences matched by  hmmsearch  as training sets. Of 150 851, 523 matches (0.34%) were missed. With  p -value cutoffs calibrated to match the sensitivity level of  hmmsearch  using noise cutoffs,  PSfamSearch  detected 99.4%, while missing 649 out of 180 263 sequences. See  Figure 6  and  Supplementary Tables 2 and 3  for more detailed results for the first 20  TIGRFAM  families.
 Fig. 6. Reduction of  UniProtKB/TrEMBL  achieved by PSSM-FM filtering for the first 20  TIGRFAM s families. Dark (light) bars indicate the effective number of sequences to be searched with  hmmsearch  ( x -axis) when using  P -value cutoffs π tc  (π nc ). The bar on the top shows the total number of sequences in  UniProtKB/TrEMBL  Rel. 40.3 (7 916 844 protein sequences with a total length of ∼2.58 GB) needed to be searched by direct  hmmsearch  without filtering. Note that the  x -axis has a logarithmic scale. It took  PSfamSearch  ∼24.8 h to search with the first 200  TIGRFAM  families, compared with &gt;45 days for direct  hmmsearch  using the models' trusted cutoffs. That is,  PSfamSearch  achieves a speedup of factor 43.8 over direct  hmmsearch , while retaining &gt;99.5% of the original results. In this experiment, the set of sequences to be searched with  hmmsearch  was reduced to only 0.80% of all sequences. Using the less-stringent noise cutoffs,  PSfamSearch  reduces the search space to only 3.83% of the original search space size also with a sensitivity of 99.5% (see  Supplementary Table 3  for more detailed results for the first 20  TIGRFAM  families) and a speedup of factor ∼14 over direct  hmmsearch . 3.2.3 Whole proteome annotation using PSfamSearch In an additional experiment we searched with pHMMs and PSSM-FMs representing the first 500 protein families in the  TIGRFAM  database in 26 publicly available  Escherichia coli  proteomes (see  Supplementary Table 4  for further details). These consist of 120 394 protein sequences with a total length of 37.3 MB. Cutoffs π tc  and π nc  for PSSM-FMs were adjusted based on  UniProtKB/TrEMBL  results as described above. In this experiment,  hmmsearch  detected 11 712 (12 516) matches to the 500 protein families when using trusted (noise) cutoffs and needed 2745 min for this task. Except for 1 (2) missed sequence(s),  PSfamSearch  returned exactly the same results at cutoff π tc (π nc ), but it took only 93.3 (171.2) min; this makes for a speedup of 29.52 (16.01). 3.2.4 Comparison of PSfamSearch with other hmmsearch acceleration solutions Another approach to accelerate  hmmsearch  is the  HMMERHEAD  program (Poster presentation at RECOMB2007). HMMERHEAD  uses a filtration approach that employs four filtering stages with increasing computational costs to reduce the search space for the subsequent application of the  hmmsearch  engine. We applied  HMMERHEAD  to the same experimental setup as described in the former paragraph. That is, we searched with pHMMs representing the first 200  TIGRFAM  families on the complete  UniProtKB/TrEMBL  database and measured the running time of  HMMERHEAD  and the coverage using the models' trusted (noise) cutoffs. In this experiment,  HMMERHEAD  was able to reduce the running time compared with direct  hmmsearch  from 1088.05 h to 626.08 h, while retaining 100% of the original results (for details, see  Supplementary Fig. 5 ). This makes for a speedup of factor ∼1.7, with per model speedups in the range of 1.4–1.9. rpsblast  (Marchler-Bauer and Bryant,  2004 ) may be seen as an alternative to  hmmsearch  employing pHMMs. It uses  psi-blast 's (Altschul  et al .,  1997 ) checkpoint files which can be seen as models for protein families, much like pHMMs and our PSSM-FMs.  rpsblast -compatible models representing  TIGRFAM  protein families are part of the CDD database (Marchler-Bauer  et al .,  2009 ). To test the ability of  rpsblast  to obtain the same results as  hmmsearch  and hence to offer an alternative to  hmmsearch  and  PSfamSearch , respectively, we compared the classification performance of  rpsblast  with that of  PSfamSearch  employing PSSM-FMs for the first 200  TIGRFAM  families with  p -value cutoffs π tc  and π nc , respectively. As a state of truth we use the results returned by  hmmsearch  using trusted (noise) cutoffs. In this experiment,  rpsblast  achieved an averaged coverage in the range of 85.6–98.6% (84.7–95.5%) compared with  hmmsearch  using trusted (noise) cutoffs. Using the same experimental setup,  PSfamSearch  achieved a uniform coverage of 99.54% (99.47%) when using cutoffs π tc  (π nc ). See  Supplementary Figure 6  for further details on this experiment. It took  PSfamSearch  1490 (4676) min using cutoffs π tc  (π nc ) to perform this task ( Supplementary Table 1 ), while  rpsblast  needed only 1341.96 min. Hence, by application of  rpsblast  one obtains an additional speedup of factor 1.1 (3.4) at the price of reduced sensitivity. Recently, Lingner and Meinicke ( 2008a ) described an approach for search space reduction applicable to speedup database searches with pHMMs based on machine learning techniques. Although the described method and presented results seem to be promising, up to now only a prototype implemented in  MATLAB  and a web server for interactive usage (Meinicke,  2009 ) are available. 4 DISCUSSION AND CONCLUDING REMARKS We have presented a new database search method based on PSSM-FMs. It is well suited for fast and reliable protein family classification. Moreover, it can serve as a filter to considerably speedup database searches with pHMMs, while retaining almost 100% sensitivity. Our method combines fast index-based searching of PSSMs, an efficient algorithm for exact  p -value computation for PSSM score thresholds, and a fast fragment chaining algorithm. The methods described here are implemented in the robust and well-documented software tool  PoSSuMsearch2 . We carefully evaluated the performance of PSSM-FMs in terms of sensitivity and specificity by using  PoSSuMsearch2  in two different modes of operation, i.e. for direct sequence classification, and as a prefilter for  hmmsearch . We have shown that PSSM-FMs are only marginally inferior to pHMMs when used for sequence classification. The FP50 value (the average coverage achieved when tolerating 50 false positives) for PSSM-FMs never dropped below the FP50 value for pHMMs by more than ∼6 percentage points in all of our three evaluation scenarios ( Fig. 4 ). This renders PSSM-FMs a fast alternative to pHMMs: for example, we have observed that  PoSSuMsearch2  is more than 64 times faster than  hmmsearch  for the same classification task. We also demonstrated that PSSM-FMs are well suited for prefiltering sequence databases to be searched by  hmmsearch . Using  PSfamSearch  (the combination of  PoSSuMsearch2  and  hmmsearch ), we observed dramatic search space reductions for  UniProtKB/TrEMBL  to 0.80% and 3.83%, respectively, when filtering with 200 PSSM-FMs built from the  TIGRFAM  database using the pHMMs' trusted and noise cutoffs, respectively. The reduction of the sequence database resulted in speedups of ∼43.8 and 14 over original, unfiltered  hmmsearch , respectively, while retaining 99.5% of the original results in both cases. Extrapolated to all 3603 families in  TIGRFAM  (Rel 8.0), we estimate a runtime of ∼18.6 days for  PSfamSearch , and 2.23 years for direct  hmmsearch  using the models' trusted cutoffs. Notably, the observed speedups come from an algorithmic as well as a conceptual advancement: the speed of index-based PSSM searching, and the astonishing fact that pHMMs can be approximated well by the much simpler PSSM-FMs. This is also consistent with the finding that protein classification works well with word correlation matrices (Lingner and Meinicke,  2008b ). In our experiments,  PSfamSearch  also showed a &gt;25-fold speedup over the program  HMMERHEAD . Compared with the well-known  rpsblast  tool,  PSfamSearch  is slightly slower.  PSfamSearch , however, achieved a significantly higher sensitivity in our experiment. In the experiment showing the ability of  PSfamSearch  for efficient annotation of  E.coli  proteomes,  PSfamSearch  returned &gt;99.99% of the original  hmmsearch  results and showed a speedup over direct  hmmsearch  of ∼30. PSfamSearch  is twice as fast as the previously fastest software-based acceleration method for  hmmsearch  (Sun and Buhler,  2009 ). Note that Sun and Buhler ( 2009 ) focus on the problem of designing unordered sets of motifs with good filtering characteristics while searching them with straightforward algorithms, whereas our work focuses on efficient index-based searching in sublinear expected time while keeping the derivation of motifs rather simple. This raises the question whether a future combination of both approaches might lead to further improvements in software-based pHMM database search methods. 
          Supplementary Material 
          
             
                [Supplementary Data] 
             
             
             
          
       </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Two C++ libraries for counting trees on a phylogenetic terrace</Title>
    <Doi>10.1093/bioinformatics/bty384</Doi>
    <Authors>Biczok R, Bozsoky P, Eisenmann P, Ernst J, Ribizel T, Scholz F, Trefzer A, Weber F, Hamann M, Stamatakis A, Schwartz Russell</Authors>
    <Abstract/>
    <Body>1 Introduction It is common practice to infer phylogenies on multi-gene datasets. One way to analyze these is to concatenate the data from several genes or entire genomes into one large super-matrix and infer a phylogeny on it via maximum likelihood (ML) or Bayesian inference methods. Typically, the sites of such a super-matrix are grouped into  p  disjoint partitions (e.g. genes)  P 1 , .... , P p . Each partition is assumed to evolve according to an independent model of evolution and has a separate set of likelihood model parameters (e.g. substitution rates, branch lengths etc.). Super-matrices often exhibit patches of missing data as sequence data for a specific taxon might not be available for  all  partitions  P i . Such patches occur because a specific taxon might simply not contain a gene/partition or because the gene has not been sequenced yet. In partitioned datasets, patches of missing data can induce an important effect on the likelihood scores of trees. Under specific partitioning schemes, model settings, and patterns of missing data, topologically distinct trees might have exactly the same analytical likelihood score if they reside on a terrace. Two distinct trees reside on a terrace if the sets of their induced partition trees are identical. Recognizing terraces, determining their size, and enumerating all trees on a terrace therefore constitutes an important step when searching tree space but also for post-processing the results of phylogenetic analyses. Final output trees of tree searches can reside on a terrace and thus, represent only  one  of many possible solutions. The presence of terraces in likelihood-based inferences was first used implicitly by  Stamatakis and Alachiotis (2010)  to accelerate ML calculations. One year later, the terrace phenomenon was explicitly named and mathematically characterized by  Sanderson  et al.  (2011) . Additional properties of terraces, in particular their impact on bootstrap and other support measures were discussed by  Sanderson  et al.  (2015) .  Chernomor  et al.  (2015 ,  2016 ) presented production-level implementations of topological moves that detect if consecutive trees reside on a terrace and thereby save computations in ML tree searches. Finally, D. Zwickl developed a python tool called terraphy for detecting terraces ( https://github.com/zwickl/terraphy ) based on the algorithm by  Constantinescu and Sankoff (1995) . 2 Implementation 2.1 Interface The C and C++ interfaces (see  https://github.com/terraphast ) take as input: a Newick tree string; a binary matrix  M  of size  n  ×  p , where  n  is the number of taxa and  p  the number of partitions and where every row is annotated by a corresponding taxon name, that denotes if data are available or not for species  i  at partition  j ; a bitmask specifying the computation mode (tree on a terrace; number of trees on terrace; enumeration of all trees on terrace); a destination file pointer to potentially print out all trees on the terrace; a pointer to a big integer library object for storing the number of terraces. For the latter we use the GNU multiple precision arithmetic library (GMP) by default to prevent integer overflow. The interface function returns an integer that either contains an error code or indicates a successful invocation. 2.2 Limitations As the library calculates the number of unrooted trees on a terrace given an unrooted, strictly bifurcating input tree, the following limitation applies: the binary input matrix must contain at least one row without any missing data, a so-called comprehensive taxon  tax C  such that all  p  induced per-partition trees  T | P i  can be consistently rooted on the branch leading to  tax C  (for an approach to relax the requirement for a comprehensive taxon, see  Supplementary Material ). By induced per-partition tree, we refer to the input tree pruned down to the taxa for which sequence data  are  available at a partition  i . This limitation allows to execute the SUPERB algorithm and, as we show in the supplement, guarantees that the number of rooted trees on the terrace calculated by SUPERB is identical to the number of unrooted trees on the terrace. This limitation can be circumvented by including an appropriate comprehensive outgroup sequence from a reference genome into the dataset. 3 Results We initially tested our implementations on several artificial small five-taxon datasets for which either all possible trees reside on a single terrace or no terrace exists. Subsequently, we tested both implementations on 26 empirical datasets from recently published biological studies (available at  https://github.com/BDobrin/data.sets ) and compared their performance to terraphy. For empirical datasets that did not contain a comprehensive taxon, we sub-sampled partitions such that the samples  did  contain a comprehensive taxon. For our tests we used a reference system with four physical Intel i7-2600 cores running at 3.40 GHz and with 16-GB main memory. We first verified that our two completely independent implementations (Terraphast I and II) yield exactly the same results and also compared their run-time performance to terraphy. Under identical settings (see  Supplementary Material  for details), all three codes yielded exactly the same number of unrooted trees on  all  datasets, provided that the input tree is rooted at the same comprehensive taxon  tax C . In  Table 1  we provide the average sequential execution times over 10 runs and number of trees on the respective terrace for Terraphast I and II and terraphy on the three empirical datasets with the largest terraces. All three codes were executed in tree counting mode, that is, enumeration and printout of all topologies on the terrace was disabled. Additional computational experiments under different modes, including memory utilization, parallel performance, and additional empirical datasets as well as a discussion of the reasons for the performance difference between Terraphasts I and II are provided in the  Supplementary Material . We recommend use of terraphast I as it is faster and also actively developed as well as maintained in a separate repository ( https://github.com/upsj/terraphast-one ).
 Table 1. Sequential execution times (seconds) for counting trees on a terrace with terraphy and Terraphast I/II Dataset Terraphy Terraphast I Terraphast II Terrace size Rosaceae 2.32 0.033 0.087 1.72 × 10 23 Shi.bats 6.34 0.015 0.081 2.42 × 10 35 Burleigh.small 4099.76 147.74 301.09 4.12 × 10 50 4 Conclusions We have provided two independent C++ implementations of the SUPERB algorithm for counting trees on a phylogenetic terrace. Because we developed two independent implementations that yield exactly identical results, we are confident that the implementations are correct. Furthermore, Terraphast I is 28 times faster than terraphy on the dataset containing the largest terrace (Burleigh.small) and requires one order of magnitude less RAM (see  Supplementary Table S2 ). As our experiments with empirical datasets show, a plethora of published phylogenetic trees  do  reside on a terrace. Although the phenomenon has been known since 2011, authors of empirical studies do not routinely assess if their tree resides on a terrace. We are optimistic that the availability of an efficient and easy-to-integrate library for this purpose will facilitate integration of this important phylogenetic post-processing step into popular phylogenetic inference tools that are predominantly written in C or C++. terraphast I has already been integrated into RAxML-NG ( https://github.com/amkozlov/raxml-ng ). The authors of GARLI (D. Zwickl, personal communication, October 2017) and IQ-Tree (B.Q. Minh, personal communication, October 2017) also intend to integrate it into their tools. Supplementary Material Supplementary Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>COVID-2019 associated overexpressed Prevotella proteins mediated host-pathogen interactions and their role in coronavirus outbreak</Title>
    <Doi>10.1093/bioinformatics/btaa285</Doi>
    <Authors>Khan Abdul Arif, Khan Zakir, Elofsson Arne</Authors>
    <Abstract/>
    <Body/>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>PWMScan: a fast tool for scanning entire genomes with a position-specific weight matrix</Title>
    <Doi>10.1093/bioinformatics/bty127</Doi>
    <Authors>Ambrosini Giovanna, Groux Romain, Bucher Philipp, Hancock John</Authors>
    <Abstract/>
    <Body>1 Introduction Knowing where transcription factors (TFs) bind to the genome is the key to understanding gene regulation. The binding specificity of a TF is commonly represented by a numerical matrix, either as a position weight matrix (PWM), a position frequency matrix (PFM) or a letter probability matrix (LPM). The three representations are information-wise equivalent and inter-convertible. A PWM contains weights for each base at each motif position. By summing up weights at corresponding positions, a binding score can be computed for any base sequence of the same length as the PWM. Large collections of TF specificity matrices are nowadays available from public libraries such as JASPAR ( Khan  et al. , 2017 ) or HOCOMOCO ( Kulakovskiy  et al. , 2017 ). PWMScan is a web-server for rapid scanning of large genomes for high-scoring matches to a user-supplied or server-resident PWM. Compared to other web-based PWM scanning tools, PWMScan is unique in that it scans server-resident whole genomes rather than user-uploaded DNA sequences. Other key features are: (i) menu-driven access to genomes of  &gt;30 model organisms; (ii) menu-driven access to  &gt;300 public PWM libraries; (iii) support of various PWM representations and formats; (iv) cut-off values can be specified as match scores or  P -values; (v) output in BEDdetail format with match scores and  P -values; (vi) links to UCSC genome browser for visualization of results; and (vii) action buttons to transfer match lists to analysis tools. A short description of the PWMScan server follows. Technical details about algorithms, programs and data are provided under  Supplementary Material . 2 Data and methods Genome sequences were downloaded from the NCBI in FASTA format. Indexed versions for rapid scanning were generated for  Bowtie  ( Langmead  et al. , 2009 ). The motif databases offered by PWMScan have been downloaded from the MEME Suite website ( Bailey  et al. , 2009 ). LPMs have been converted to integer PWMs (see  Supplementary Material ). The input form of the PWMScan server is shown in  Figure 1 . The user chooses a genome assembly from a menu. Optionally, a BED file may be uploaded to restrict the search to genomic regions of particular interest, e.g .  open chromatin regions. The right side of the form offers several ways to specify a DNA motif. PWMs from a server-resident database are chosen from a pull-down menu. Alternatively, matrices can be entered into a text area or uploaded. Accepted motif types are: PFMs, LPMs, real or integer PWMs and IUPAC consensus sequences. PFMs can be entered in several formats, including TRANSFAC and JASPAR.
 Fig. 1. Screen shot of the PWMScan graphical user interface All motif types have to be converted into integer PWMs for input to the genome search engines (see  Supplementary Material ). Default conversion parameters are proposed and can be changed by the user. For instance, real PWMs can be rescaled on input by a multiplication factor to ensure sufficient resolution after integer conversion. IUPAC consensus sequences are converted into binary matrices consisting of 0 and 1. For all matrix formats, the cut-off value can be specified as PWM score, as  P -value or as percentage of the score range (0% = minimal score and 100% = maximal score). For IUPAC consensus sequences, the cut-off value is specified as a maximal number of mismatches allowed. The  P -value of a PWM score  x  is defined as the probability that a random  k -mer sequence of the length of the PWM has a binding score ≥  x  given the base composition of the genome. The whole genome scan takes as input an integer PWM and a corresponding cut-off. The output is a list of sequence regions that match the PWM with a match score higher or equal to the cut-off value. Depending on the length of the PWM and the cut-off, one of the following search strategies is chosen: (i)  Bowtie , a fast memory-efficient short read aligner using indexed genomes and (ii)  matrix_scan , a C program developed by our group using a conventional search algorithm. The first strategy is more efficient for short PWMs and high cut-off values. It requires as a first step the generation of a list of all  k- mers that match the PWM with the given cut-off. The list of  k- mers is then mapped to the genome using  Bowtie . The second strategy takes genome sequences in FASTA format as input. Individual chromosomes are processed in parallel and distributed to multiple cores by a Python script. We empirically found that this approach becomes more efficient if the number of  k- mers exceeds 10 5  sequences.  matrix_scan  was benchmarked for speed together with five other matrix scanners and was found to be the fastest (see  Supplementary Material ). The basic search step outputs a list of PWM matches, including the genomic coordinates, the DNA sequence and the match score. Post-processing of this list involves computation of the corresponding  P -values, addition of the matrix name and, optionally, elimination of overlapping matches. The final match list is provided in BEDdetail format. The output page further shows the total number of PWM matches and a sequence logo reflecting the letter-probabilities of the input matrix. Action buttons are provided for: (i) sending the match lists to analysis tools of the ChIP-Seq and SSA servers ( Ambrosini  et al. , 2016 ), (ii) extracting DNA sequences around the matches, (iii) sending the output to the UCSC genome browser for visualization and (iv) liftover of the match coordinates to other assemblies of the same or related species. PWMScan is meant to support many types of genomic data analysis and designed to be interoperable with other tools from our group and elsewhere. An example of a typical workflow involving ChIP-seq data is presented in  Supplementary Material . PWMScan is also available as a command-line software package from SourceForge, including a master script scheduling all computational steps running during a web job. 3 Benchmark The runtime of PWMScan was measured by scanning the human genome (UCSC assembly hg19) with two different PWMs from JASPAR, STAT1 with length 11 bp and CTCF with length 19 bp, and different cut-off values expressed as  P -values. Results are shown in  Table 1 . Note that for longer motifs and higher  P -values, the  Bowtie -based approach becomes inefficient, whereas  matrix_scan  remains reasonably fast.
 Table 1. Benchmark results with different PWMs and  P- values PWM/ P -value Bowtie speed Matrix_scan speed STAT1(len = 11 bp)/10 −5 3 30/5 a STAT1(len = 11 bp)/10 −4 8 40/8 a STAT1(len = 11 bp)/10 −3 60 65/30 a CTCF(len = 19 bp)/10 −5 12 40/6 a CTCF(len = 19 bp)/10 −4 90 50/10 a CTCF(len = 19 bp)/10 −3 720 90/35 a Note:  Speed is expressed in seconds. The benchmarking tests have been run on a Linux/CentOS7/x86_64 workstation with 48 CPU-cores and 256 GB of DRAM. a Performance measurements using  matrix_scan  in parallel over 10 CPU-cores. Supplementary Material Supplementary Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Estimating large-scale signaling networks through nested effect models with intervention effects from microarray data</Title>
    <Doi>10.1093/bioinformatics/btm634</Doi>
    <Authors>Fröhlich Holger, Fellmann Mark, Sültmann Holger, Poustka Annemarie, Beissbarth Tim</Authors>
    <Abstract>Motivation: Targeted interventions using RNA interference in combination with the measurement of secondary effects with DNA microarrays can be used to computationally reverse engineer features of upstream non-transcriptional signaling cascades based on the nested structure of effects.</Abstract>
    <Body>1 INTRODUCTION The advent of RNA silencing enables researchers to selectively silence genes of interest on large scale. DNA microarrays allow to measure the effects of a perturbation on a genome-wide scale. This enables to reverse engineer interdependencies between gene products on a non-transcriptional level. The genes of interest are silenced individually, and the respective downstream effects on gene expression are measured by using genome-wide microarrays. By observing the nested structure of significant up- or down-regulations of affected genes, this allows to reconstruct features of the upstream signaling pathway (Boutros  et al. ,  2002 ). In a recent work, Markowetz  et al.  ( 2005 ) introduced  nested effect models  as a method to reverse engineer the signal flow between perturbed genes using the nested subset relationship of secondary downstream effects. They developed a Bayesian statistical framework, in which for a given network hypothesis one can calculate a score and thus can reduce the set of all possible networks to the most likely ones. A severe limitation of this method lies, however, in the restriction to small networks of up to five genes, because the method completely enumerates all possible network hypotheses. Furthermore, a difficulty in the practical use is the required binary discretization of the data (‘secondary effect present/not present’). In our work, we extend the framework by Markowetz  et al.  in several directions in order to overcome these restrictions: instead of the data discretization step needed in the original framework, we propose the usage of a beta-uniform mixture distribution on the  P -value profile, resulting from differential gene expression calculation, to quantify effects (Pounds and Morris,  2003 ). Moreover, we show how prior assumptions on the network structure can be incorporated into the network scoring scheme by defining appropriate prior distributions on the network structure as well as on its hyperparameter. Finally, and most important, we present our so-called  module networks  to scale up the original approach, which is limited to small pathways with around five genes, to the inference of large-scale networks (up to more than 30 genes). The idea is to build the complete network recursively from smaller pieces that are connected subsequently. In order to validate our approach, we conduct extensive simulations on artificially created networks and compare it to the triplets inference scheme described in Markowetz  et al.  ( 2007 ). We show that  module networks  offer a better performance in terms of reconstruction quality while being significant computationally faster at the same time. We also apply our  module networks  to infer the signaling network between 13 genes in the ER- α  pathway in human MCF-7 breast cancer cells. Using bootstrapping and the jackknife this reconstruction is found to be statistically stable. 2 METHODS 2.1 Original approach We start with a brief review of the framework by Markowetz  et al. : in general one distinguishes between silenced genes (S-genes) and genes showing a downstream effect (E-genes). It is assumed that each E-gene is attached to a single S-gene only ( Fig. 1 ). Knocking down a specific S-gene  S k  interrupts signal flow in the downstream pathway, and hence an effect on the E-genes attached to  S k  and all S-genes depending on  S k  is expected. Let us assume  n  knock-downs are performed and there exist  m  E-genes in total. The outcomes of these experiments are summarized in an  m  ×  n  data matrix  D . According to Bayes' formula, a specific network hypothesis Φ ∈ {0, 1} n  × {0, 1} n  can be scored as:
 (1) 
The position of the E-genes is introduced as a model parameter Θ = {θ i |θ i ∈ {1, …,  n },  i  = 1, …, m }, i.e. θ i  =  j , if E-gene  i  is attached to S-gene  j . Assuming independence of the observations (rows)  D i  in the data matrix  D  (given a fixed network hypothesis Φ and model parameters Θ) one can write down the conditional likelihood  P ( D |Φ, Θ) as:
 (2) 
It is furthermore assumed that all parameters θ i  are statistically independent, i.e.
 (3) 
The likelihood  P ( D |Φ) can then be written as:
 (4) 
 (5) 
We now suppose a decomposition of  P ( D i |Φ,θ i ) as follows:
 (6) 
This makes the assumption that knock-down experiments are statistically independent from each other. Hence, Equation ( 5 ) can be written down as
 (7) 
 Fig. 1. Main idea of the inference framework by Markowetz  et al. : a network hypothesis is a directed graph between S-genes. Attached to each S-gene are several E-genes. Knocking down S-gene  S 2  interrupts signal flow in the downstream pathway, and hence an effect of E-genes attached to  S 2  and to  S 1  is expected. 2.2 Extensions 2.2.1 Generalized inference framework Markowetz  et al.  suppose the data matrix  D  to consist of counts, how often a specific E-gene shows an effect in ℓ experiment repetitions. This requires a data discretization step, for which user-specified type-I and type-II error rates are assumed. The choice of these parameters is critical for the inference procedure, because it directly influences ( 6 ) and is difficult to estimate. Markowetz  et al.  suppose to have both, positive and negative controls (pathway stimulated/not stimulated) for this procedure, which for our data is not available ( Section 3.2 ). In our approach we only make the quite general assumption that  D  is an  m  ×  n  matrix of (raw)  P -values, which specify the likelihood of E-gene  i  being differentially expressed after knock-down of S-gene  k . The  P -values are calculated using an arbitrary method for detecting differential gene expression, e.g.  limma  (Smyth,  2004 ). They are supposed to be drawn from a mixture of a uniform [0, 1] distribution reflecting the null hypothesis and another distribution  f 1  reflecting the alternative hypothesis (Pounds and Morris,  2003 ):
 (8) 
Under the alternative hypothesis, there is a high density for small  P -values and a strong decrease for increasing  P -values. Both distributions overlap with mixing coefficient γ k  ·  P  ( D ik |Φ,θ i ) can therefore be decomposed as:
 (9) 
The density function  f 1  reflects the strength of the knock-down effect on E-gene  i  under the alternative hypothesis. If it is greater than 1 the alternative hypothesis would be accepted, and if it is smaller than 1 rejected. Still the problem remains, how to define  f 1  appropriately. For this purpose one may simply assume a single Beta (1, β k ) (β k  &gt; 2) distribution (c.f. Fröhlich  et al. ,  2007a ,  b ). However, a better fit can be obtained by modeling  P ( D ik ) : =  f ( D ik ) as a three component mixture of a uniform, a Beta(1, β k ) (β k  &gt; 2) and a Beta ( α k , 1) ( α k  &lt; 1) distribution:
 (10) 
with π 1 k  + π 2 k  + π 3 k  = 1 (π rk  ≥ 0,  r  = 1, 2, 3). This three component beta-uniform mixture model (BUM) can be fitted via an EM algorithm (Dempster  et al. ,  1977 ). The alternative distribution  f 1  can then be extracted as follows: Let   be the maximum uniform part of the BUM model. Then
 (11) 
 Figure 2  shows an example histogram of a  P -value distribution resulting from one of our real-life experiments, which are explained in detail in  Section 3.2 . As seen the model curve drawn in black fits the histogram perfectly. The extracted alternative distribution is shown in red.
 Fig. 2. Histogram of the  P -value distribution of AKT2 knock-down (see  Section 3.2 ). Black: mixture model curve; red: extracted alternative distribution. 2.2.2 A Bayesian prior on the network structure Equation ( 1 ) allows to specify a prior  P (Φ) on the network structure itself. This can be thought of biasing the score of possible network hypotheses towards prior knowledge or assumptions. At the same time, we have to take into account that our assumptions may only be true up to a certain degree. Hence, for each edge we should suppose a prior probability reflecting the degree of belief in its existence. In principle, this degree of belief can be very different for each edge. We summarize all prior edge probabilities in an  n  ×  n  matrix  . Making the assumption that all edge priors  P (Φ ij ) are independent, i.e.
 (12) 
allows us to define the connection between Φ ij  and   for each edge separately. Note that Φ ij  ∈ {0, 1} depending on whether we set the edge  i  →  j  or not. Hence, for each edge we have a certain difference   to our prior assumptions. The smaller this difference, the higher  P (Φ ij ) should be. We can therefore model  P (Φ ij ) as a Laplacian distribution with width parameter ν (cf. Imoto  et al. ,  2003 ):
 (13) 
The width parameter ν can scale the prior in an adjustable way. From a Bayesian perspective one should hence specify a prior on the parameter ν as well. A natural choice for this purpose is the inverse gamma distribution with hyperparameters 1 and 0.5:
 (14) 
The full edge prior  P (Φ ij ) can then be obtained via marginalization:
 (15) 
If the difference   to our prior assumptions is zero, then the prior is 1, whereas for   the prior superlinearly drops to 1/9. 2.2.3 Large-scale network inference The inference framework ( Sections 2.1  and  2.2.1 ), does not answer the question how to come up with a candidate network topology, which we would like to score. Markowetz  et al.  ( 2005 ) completely enumerate all possible topologies. This is, however, only suitable for small networks of up to 5 S-genes. For 5 S-genes there already exist more than 1 000 000 and for 10 genes more than 10 27  possible network topologies. In this context it should be noted that the scoring scheme ( Section 2.1 ) cannot distinguish between two network hypotheses, if they only differ in transitive edges. This issue is known as  prediction equivalence  and is due to the fact that subset relationships, which are represented by a nested effects model, are transitive in principle. Hence, it only makes sense to consider the set of all transitively closed network hypotheses. However, restricting ourselves to this limited class of network structures does not generally solve the problem, since even then the number of networks to consider scales in a similar way with the number of S-genes. Hence, we have to resort to heuristics. Module networks : The idea of the module network is to build up a graph from smaller subgraphs, called  modules  in the following. Here we present an updated version of the algorithm presented in our earlier publications (Fröhlich  et al. ,  2007a ,  b ). We begin with a hierarchical clustering of the preprocessed expression profiles of all S-genes, e.g. via average linkage. The idea is that S-genes with a similar E-gene response profile (here: with regard to the Pearson correlation similarity) should be close in the signaling path. We now successively move down the cluster tree hierarchy until we find a cluster with only 4 S-genes at most.  Figure 3  illustrates the idea with an assumed network of 10 S-genes. At the beginning we find  S 8  as a cluster singleton. Then by successively moving down the hierarchy we identify clusters  S 6 ,  S 7 ,  S 1 ,  S 10 ,  S 3 ,  S 2 ,  S 5  and  S 4 ,  S 9 . All these clusters (modules) contain 4 S-genes at most and can thus be estimated by taking the highest scoring of all possible network hypotheses.
 Fig. 3. Basic idea of module networks: by successively moving down the cluster hierarchy we identify the clusters (modules) of S-genes, which are marked in red. They contain 4 S-genes at most and can be estimated by exhaustively searching for the highest scoring model. Once all modules have been estimated their connections are constructed. This is done in a constraint greedy hill-climbing fashion: we successively add that edge between any pair of S-genes being contained in different modules, which increases the likelihood of the complete network most. This procedure is continued until no improvement can be gained any more, i.e. we have reached a local maximum of the likelihood function. 3 EXPERIMENTS 3.1 Large-scale inference: evaluation on artificial networks To test our methods and to get better insights into the performance of our large-scale inference methods, we generated data from artificial random networks. 3.1.1 Network topology creation Artificial random networks were generated as follows: For each node  S k  we randomly chose the number  o  of outgoing edges between 0 and 3. We then selected  o  nodes having at most 1 ingoing edge, connected  S k  to them and transitively closed the graph. Averaged over 100 random networks for  n  = 10, this procedure yielded an average of 3.5 ± 2.1 ingoing and 3.5 ± 3.6 outgoing edges per node (min. 0, max. 9 in both cases). After network topology construction, the  m  E-genes were attached uniform randomly over all S-genes. 3.1.2 Data sampling We then simulated knock-downs of the individual S-genes. For those E-genes, where no effects were expected, the ‘ P -values’ were drawn uniform randomly from [0, 1]. For the others there was an independent prior effect probability depending on the path distance  d  to the ‘knocked-down’ S-gene of  , i.e. at the maximal achievable path distance of  d  =  n  − 1 there was only a 50% chance to observe an effect. For each E-gene we threw a biased coin with the corresponding prior effect probability, and depending on the outcome the ‘ P -value’ was either again drawn uniform randomly from [0, 1] or sampled from the alternative distribution [Equation ( 8 )]. In order to do so we sampled random parameters  α k  ∈ (0, 1), β k  ∈ [5, 50] and π 2 k , π 3 k  ∈ (0, 0.5) (note that π 1 k  = 1 − π 2 k  − π 3 k ) of the three component BUM model [Equation ( 10 )] for each ‘knocked-down’ separately. That means for each S-gene the ‘ P -values’ could have a different distribution. To take into account the BUM model re-estimation error, we additionally blurred each parameter with normally distributed noise (SD β k : 10%; other parameters: 0.05). These ‘noisy’ parameters were then used to draw ‘ P -values’ from the alternative distribution. However, to quantify the effect strength according to Equation ( 11 ) the original parameters were used. Hence, we simulated a mismatch between the empirical and the modeled ‘ P -value’ distribution. 3.1.3 Simulation results We sampled networks with  n  = 10, 20, 30 S-genes. For each number of S-genes we varied the number  m  =  n , 2 n , 4 n , 8 n  of E-genes. We compared the module network with the triplets inference approach described in Markowetz  et al.  ( 2007 ). The idea of the latter is to decompose the complete network in all   possible combinations of three S-genes. For each triplet the highest scoring model can then be found among all 29 possible ones. No prior knowledge on the network structure was used. We evaluated both methods in terms of average sensitivity (i.e. ratio of correctly learned edges to total number of edges in the original network) and specificity (i.e. ratio of correctly unconnected genes to total number of unconnected genes in the original network over 10 n  generated networks for  n  S-genes. Moreover, the balanced accuracy, i.e. the average of sensitivity and specificity was computed. In  Figure 4 , we show the results for  n  = 10, 20 S-genes. While module networks and the triplets inference algorithm yield a comparable specificity, the sensitivity for module networks is much higher. As a result, the balanced accuracy for module networks differs from that of the triplets inference algorithm significantly for all numbers of E-genes. This conclusion was assessed by a pairwise  t -test at significance level 0.05. At the same time, the computation time for the triplets inference was significantly higher ( Fig. 5 ) than for the module networks. For  n  = 30 S-genes triplets inference already became impractically slow, so that we omit results here. In contrast, the module network only needed around 2 min for one network inference on average, which seems affordable. As indicated by the plots in  Figure 6 , the network reconstruction quality does not differ much from that for  n  = 10, 20.
 Fig. 4. Simulation results for artificial networks with  n  = 10, 20 S-genes and varying number of E-genes. 
 Fig. 5. Computation times (s) for module networks and triplets inference. For  n  = 30 triplets inference already becomes impractically slow. 
 Fig. 6. Simulation results for artificial networks with  n  = 30 S-genes and varying number of E-genes (module networks). Next we investigated the effect of the network prior [Equation ( 15 )]. For each network we randomly picked 25%, 50%, 75% of all edges in the original network (true positives) and included 5% false positives. For both, true and false positives, the prior edge probability was set to 100%.  Figure 7  summarizes the average improvement in terms of sensitivity, specificity and balanced accuracy, which is gained by our prior for the module network. As expected, the sensitivity is highly increased, especially for a lower number of E-genes. At the same time the specificity for  m  &gt; 10 remains almost constant. In conclusion, for all numbers of E-genes a significant improvement of the balanced accuracy can be gained ( P  &lt; 0.05).
 Fig. 7. Average improvement by prior knowledge of 25%, 50%, 75% true posititive and 5% false-positive edges with varying number of E-genes ( n  = 10 S-genes, module networks). 3.2 Application to RNAi data from human ER- α  pathway 3.2.1 Data We applied the module network to infer the complete topology for a network of  n  = 13 S-genes in the ER- α  pathway. The 13 genes were selected from previous microarray studies in our department to be influenced by ER status in breast cancer patients. Each of the 13 genes was silenced individually using two different siRNAs, respectively, and the effect on gene expression was studied on whole genome cDNA microarrays. The data were generated in our department. Details are omitted here due to restrictions of space, but can be obtained from the authors. 3.2.2 Preprocessing For each knock-down experiment after VSN normalization (Huber  et al. ,  2002 )  P -values for differential gene expression detection were calculated using  limma  (Smyth,  2004 ). Afterwards BUM models were fitted to quantify effects as described in Section 2.2.1. An a priori filtering among the joint set of the top 100 E-genes from each experiment was performed to select patterns of differentially expressed genes that can be expected to be non-randomly: supposed a gene is significantly non-differentially expressed in knock-down experiment  k . Nonetheless we can observe a (multiple testing corrected)  P -value &lt;  α  with false-positive rate  α . Let us encode with 1, if the  P -value is smaller than  α  and 0 otherwise. For  n  knock-down experiments, we can summarize the outcome for each E-gene in a binary vector  b  = ( b 1 , …,  b n ) T . Let  M  be the total number of E-genes and  s k  the number of significant genes in experiment  k . Then, under the null hypothesis the probability to observe  b  just by chance is
 (16) 
Among  M  E-genes we can thus expect to see  b  Pr( b | H 0 ) M  times just be chance. The statistical significance of observing  b  more often than can be expected by chance can therefore be assessed by a binomial test. The corresponding  P -value for each pattern is corrected for multiple testing using the Bonferroni method later. We then only choose those E-genes, which show a significant pattern. In conclusion this procedure eliminates false-positive patterns and thus reduces the noise in the data. Moreover, the dimensionality of the data is reduced efficiently. For our data we arrived at  m  = 621 E-genes this way. 3.2.3 Network inference We ran both, the triplets inference algorithm and the module network reconstruction on our dataset. We found the log marginal likelihood of the triplets inference algorithm network to be significantly lower than that of the module network (likelihood difference 142), thus supporting our conclusions drawn from the simulation studies. For our final network reconstruction we employed bootstrapping in order to ensure the statistical stability and robustness of the solution: we sampled  m  E-genes from the total set of E-genes 1000 times with replacement and each time ran the module network for topology induction. We did not use any of the literature knowledge for inference here in order to have an external source of validation later on. We only considered edges, which were found in more than 50% of all bootstrap trials. The average bootstrap probability for these edges was 90 ± 14%, i.e. most edges were inferred with high stability. Furthermore, we assessed the stability of the reconstructed network in a different way via jackknifing: Each S-gene was left out once and the network inferred on the present S-genes. We then counted the frequency of each edge among all  n  network reconstructions. Only edges with a jackknife probability of more than 50% were considered. The average probability of these edges was 86 ± 11%, i.e. again most edges were highly stable. The overlap of the results obtained from bootstrapping and from the jackknife is depicted in  Figure 8  as a transitively reduced graph. 1 
 Fig. 8. Left: Literature network obtained from Ingenuity™. Right: Consensus network induced by our method (transitively reduced graph). 3.2.4 Comparison with literature We performed a literature scan for known interdependencies between S-genes using the Ingenuity™ Software ( Fig. 8 , left). The edge  ESR 1 →  AKT 2 in our network reconstruction is reflected by the signaling cascade  ESR 1 →  Hsp 70 →  AKT 1 →  TCL 1 B  →  AKT 2. Likewise,  AKT 2 →  BCL 2 →  AKT 1 can be confirmed by the signaling cascade  AKT 2 →  ESR 1 →  BCL 2 →  Hsp 90 →  AKT 1. Furthermore, our network contains  BCL 2 →  STAT 5 B , which in the literature is  BCL 2 →  PPP 2 CA  →  PTPN 7 →  STAT 5 B , and  STAT 5 B  →  ERK 2 →  FOXA 1, which is reflected by  STAT 5 B  →  PTPN 7 →  ERK 2 →  TP 53 →  Hist 3 →  FOXA 1. At this point it should also be mentioned that due to experimental circumstances in RNAi knock-down experiments and due to the used cell line in principle there might be deviances of the literature knowledge to the measured data. 4 CONCLUSION We proposed a method for reconstructing signaling pathways from secondary effects, which were observed on microarray after silencing genes of interest via RNAi. Our approach systematically extends and generalizes previous work by Markowetz  et al.  instead of data discretization, a beta-uniform mixture distribution on the  P -value profile resulting from differential gene expression calculation was used, to quantify effects. A Bayesian prior on the network structure was developed to incorporate assumptions on the network structure. In our simulation studies, we could show that in principle this way the sensitivity of network reconstruction can be increased significantly. We developed an algorithm for large-scale inference of signaling pathways and evaluated in a systematic fashion on artificially created data. Our  module networks , which recursively build up the complete topology from smaller pieces, were found to have a significantly better network reconstruction quality than the previously proposed triplets inference algorithm (Markowetz  et al. ,  2007 ). At the same time, our  module networks  could be computed much faster and therefore allowed for the inference of large-scale networks of more than 30 genes. We used the module network to infer the signaling pathway for 13 genes in the ER- α  pathway in human MCF-7 breast cancer cells and used a bootstrapping as well as a jackknife approach to ensure the statistical stability of the result. The induced edges in our inferred network were found with high consistency and could partially be also confirmed by the literature. Future biological experiments are planned to validate our reconstructed network in a systematic way. In conclusion of our results we think that our approach offers a scalable, reliable and fairly general way for large-scale inference of signaling pathways from secondary effects and therefore provides researchers with a valuable tool to gain insight into complex cellular processes. The code for the module network inference method is available in the latest version of the  R -package  nem , which can be obtained from the Bioconductor homepage. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Automatic recognition of conceptualization zones in scientific articles and two life science applications</Title>
    <Doi>10.1093/bioinformatics/bts071</Doi>
    <Authors>Liakata Maria, Saha Shyamasree, Dobnik Simon, Batchelor Colin, Rebholz-Schuhmann Dietrich</Authors>
    <Abstract>Motivation: Scholarly biomedical publications report on the findings of a research investigation. Scientists use a well-established discourse structure to relate their work to the state of the art, express their own motivation and hypotheses and report on their methods, results and conclusions. In previous work, we have proposed ways to explicitly annotate the structure of scientific investigations in scholarly publications. Here we present the means to facilitate automatic access to the scientific discourse of articles by automating the recognition of 11 categories at the sentence level, which we call Core Scientific Concepts (CoreSCs). These include: Hypothesis, Motivation, Goal, Object, Background, Method, Experiment, Model, Observation, Result and Conclusion. CoreSCs provide the structure and context to all statements and relations within an article and their automatic recognition can greatly facilitate biomedical information extraction by characterizing the different types of facts, hypotheses and evidence available in a scientific publication.</Abstract>
    <Body>1 INTRODUCTION Since the launch of the first scientific journal in 1665,  Philosophical transactions of the Royal Society , the scientific literature has developed into the core medium for the exchange of ideas and findings across all scientific communities. In recent years, numerous initiatives have emerged to automatically process electronic documents in the life sciences, add semantic markup to them and facilitate access to scientific facts. Most work in biological text mining ( Ananiadou  et al. , 2010 ;  Cohen and Hersh, 2005 ) has concentrated on identifying biological entities and extracting the relations between these entities as facts or events appearing in article abstracts while recently, the focus has shifted towards full text articles ( Kim  et al. , 2011 ). While system performance on biomolecular event extraction is improving ( Kim  et al. , 2011 ), there is little progress in the analysis of the context of extracted events and relations which help to characterize the knowledge conveyed within the text and build the argumentation within the article discourse. The analysis of the scientific discourse plays a key role in differentiating between the nature of the knowledge encoded in relations and events, e.g. ‘AhR agonists suppress B lymphopoiesis’ in the fourth sentence of  Figure 1  is a known fact whereas ‘the potential of two AhR agonists to alter stromal cell cytokine responses’ in sentence 5 is a hypothesis to be investigated. Such a distinction between events or relations is currently ignored in standard biomedical information extraction. Discourse analysis of this type would improve the distinction between facts, speculative statements, pre-existing and new work. In  Figure 1 , factual sentences (denoted as ‘Background’, sentences 1, 2 and 4) are distinguished from a sentence containing information inferred from the ‘Background’, a hypothesis driving and justifying the work presented in the article (‘Hypothesis’, sentence 3). Sentence 5 which conveys the aim of the work as being that of evaluating a certain hypothesis, is annotated as both Goal and Hypothesis.
 Fig. 1. Example of discourse labelling using CoreSC. The categorization of sentences within scientific discourse has been studied in previous work and from a number of different angles. Simone Teufel ( Teufel  et al. , 1999 ;  Teufel, 2010 ) created argumentative zoning (AZ), an annotation scheme which models rhetorical and argumentational aspects of scientific writing and concentrates on author claims. AZ has been modified for the annotation of biology articles ( Mizuta  et al. , 2006 ) and chemistry articles ( Teufel  et al. , 2009 ). Other work has looked at the annotation of information structure in abstracts, based on abstract sections ( Hirohata  et al. , 2008 ;  Lin  et al. , 2006 ;  McKnight and Srinivasan, 2003 ;  Ruch  et al. , 2007 ). A separate line of work has looked at the characterization of scientific discourse in terms of modality and speculation ( Kilicoglu and Bergler, 2008 ;  Light  et al. , 2004 ;  Medlock and Briscoe, 2007 ) while  Shatkay  et al.  (2008 ) and  Wilbur  et al.  (2006 ) annotate sentences according to various dimensions such as focus, polarity and certainty. There is as yet no general consensus among researchers in scientific discourse regarding the optimal unit of annotation. Most of the previous research considers sentences as their basic unit while  de Waard  et al.  (2009 ) has proposed the annotation at the clause level and  Nawaz  et al.  (2010 ) and  Thompson  et al.  (2011 ) consider a multi-dimensional scheme for the annotation of biological events in texts (bio-events). Existing schemes vary in their scope and granularity, with ones designed for abstracts considering only four categories and schemes for full articles generally consisting of at most seven content-related categories. However, especially for the case of full articles, it is becoming apparent that more information is required to characterize statements and claims. Researchers are interested in identifying hypotheses and different types of evidence to support claims ( Ciccarese  et al. , 2008 ), which are not readily identifiable by current schemes. Our work fills the need for finer-grained annotation to capture the content and conceptual structure of a scientific article. Inspired by the definitions in the EXPO ontology for scientific experiments ( Soldatova and King, 2006 ) and the CISP meta-data ( Soldatova and Liakata, 2007 ), in  Liakata and Soldatova (2008 ) and  Liakata  et al.  (2010 ) we introduced a sentence-based, three layer scheme which recognizes the main components of scientific investigations as represented in articles (see  Fig. 2  and  Supplementary Material ). The first layer consists of 11 categories which describe the main components of a scientific investigation, the second layer is properties of those categories (e.g. Novelty, Advantage), and the third layer provides identifiers that link together instances of the same concept.
 Fig. 2. Hierarchical representation of concepts and properties in the CoreSC scheme. In comparison to closely related schemes ( de Waard, 2007 ;  Nawaz  et al. , 2010 ;  Teufel  et al. , 2009 ), none of which have been automated yet, the Core Scientific Concept (CoreSC) scheme makes finer grained distinctions between the different types of objective (Hypothesis–Goal–Motivation–Object), approach (Method–Model–Experiment) and outcome (Observation–Result–Conclusion) and constitutes the most fine grained analysis of knowledge types of any such scheme. The distinction between the above types of objective, approach and outcome are important to expert needs (For more details, see the definitions and explanations in the  Supplementary Material .). The CoreSC scheme has been applied to articles in biochemistry and chemistry to create a corpus of 265 annotated articles (ART/CoreSC corpus, 39 915 sentences + 265 titles, over 1 million words) ( Liakata and Soldatova, 2009 ;  Liakata  et al. , 2010 ).  Guo  et al.  (2011 ) showed that a finer level of annotation of cancer risk assessment (CRA) abstracts using CoreSC categories, increased experts' efficiency in extracting information from the text while  White  et al.  (2011 ) argue that the CoreSC scheme is ‘uniquely suited to recovering common types of scientific arguments about hypotheses, explanations, and evidence’. In this article, we automate the annotation of full scientific articles with categories from the first layer of the CoreSC scheme, provide intrinsic evaluation of the results and discuss existing and future applications of this work. The article is structured as follows: In  Section 2 , we describe how we trained and tested machine learning classifiers on automatic recognition of CoreSCs in full articles. In  Section 3 , we analyse the classifier performance and discuss the features used for building the classifiers and their contributions to each category. Finally in  Section 4 , we discuss existing and future applications of the work. Our system for the classification of CoreSCs, our guidelines and annotated articles are all available online for researchers in biology to use. To our knowledge this is the first time a discourse annotation scheme is being used to automatically annotate full articles in the biosciences on this scale. It is also the first such scheme for which machine learning classifiers have been trained and tested on chemistry articles. Both the resources and the tools for automatic annotation are available online. 2 METHODS The data the training and test data used as input to the machine learning classifiers consist of 265 articles from biochemistry and chemistry annotated at the sentence level by experts using the CoreSC annotation scheme. These articles constitute the ART/CoreSC corpus ( Liakata and Soldatova, 2009 ;  Liakata  et al. , 2010 ), which was developed in three phases (training, evaluation and expansion). During the first-phase 20 annotators, all chemistry experts at postdoc or PhD level, recruited from UK Universities, were trained on four full papers with the first version of the guidelines and detailed explanations resulting from error analysis. This data and individual comments from annotators were used to improve the annotation guidelines. The second phase was designed to evaluate both the guidelines and expert performance in terms of κ-inter-annotator agreement (κ-IAA). Our goal was to obtain IAA for a reasonable amount of papers, while ensuring at least three annotators per paper, so as to minimize the chance of random agreements. Thus, 16 annotators from the first phase were split into 5 groups of 3 annotators each, where each group annotated 8 different papers and 1 additional paper was common across all 5 groups. The 16th annotator annotated across groups to provide a normalizing factor. The κ-IAA for the 41 papers obtained in this manner, measured according to Cohen's κ ( Cohen, 1960 ), was κ=0.55 (median average for the 9 best annotators across all groups and the paper common to all annotators). The third and final phase of corpus development aimed at expanding the size of the corpus by selecting the nine best performing annotators (according to IAA) from the second phase to annotate 25 papers each. While no IAA could be obtained for the 225 papers 1  annotated in this way, the assumption is that it would be the same as the average of the agreement achieved by each of the nine annotators in the second phase of development. The 265 journal articles were chosen by a chemistry expert with extensive experience in publishing, so as to cover a wide range of topics and journals. The 265 articles cover 16 different chemistry journals and 25 topics, with the majority involving spectroscopy, biochemistry, kinetics and theoretical work. Article length ranges between 32 and 379 sentences and numbers of authors range between 1 and 11, with the majority attributed to 2–3 authors and being 150 sentences long. More details about the papers can be found in the  Supplementary Material . The corpus has therefore good coverage of the field and was designed in three phases with the contribution of multiple experts so as to minimize classifier bias. Statistics on the corpus are available in  Table 1 . The corpus consists of 39 915 sentences (&gt;1 million words) with the majority categories being Result (21%) and Background (19%). The next most populous category is Observation (14%), followed by Method (11%), Experiment (10%), Conclusion (9%) and Model (9%). Finally, the categories designating the Objectives (Hypothesis, Object, Motivation and Goal) altogether amount to 7 with Object and Hypothesis the most prominent at 3% and 2%, respectively.
 Table 1. Statistics on the training data (ART/CoreSC corpus) Measure Bac Con Exp Goa Met Mot Obs Res Mod Obj Hyp Total Number of sentences 7606 3636 3858 582 4281 541 5410 8404 3656 1161 780 39 915 Number of words 193 930 102 173 93 882 16 564 107 309 13 737 123 394 224 353 99 313 29 215 21 315 1 025 185 Percentage of sentences 19 9 10 1 11 1 14 21 9 3 2 Number of words p/s (mean) 25.5 28.1 24.33 28.46 25.07 25.39 22.81 26.7 27.16 25.16 27.33 Number of words p/s (SD) 12.32 12.49 20.6 12.69 11.4 10.34 11.44 12.65 14.76 11.16 12.01 κ-IAA 0.87 0.89 0.65 0.60 0.74 0.46 0.79 0.78 0.43 0.81 0.46 
 To segment sentences we used the XML aware sentence splitter SSSplit, described in  Liakata  et al. , 2009 . The choice of the sentence as our unit of annotation stems mainly from the fact that sentences are the most common unit of text selection for summaries ( Brandow  et al. , 1995 ;  Kupiec  et al. , 1995 ). We also regard the sentence as the most meaningful minimal unit for the analysis of scientific discourse, in agreement with earlier work ( Teufel, 2000 , Chapter 3). The methods we have used state of the art supervised machine learning algorithms to train classifiers on the automatic annotation of papers with the first layer of the CoreSC scheme, that is, the following 11 categories: Background (BAC), Hypothesis (HYP), Motivation (MOT), Goal (GOA), Object (OBJ), Method (MET), Model (MOD), Experiment (EXP), Observation (OBS), Result (RES) and Conclusion (CON) ( Liakata  et al. , 2010 ). From a machine learning perspective we treat the recognition of CoreSCs as: (i) text classification and (ii) sequence labelling. In text classification sentences are classified independently of each other and any dependencies between sentences need to be added explicitly. On the other hand, in sequence labelling the assignment of labels is such as to satisfy dependencies between sentences. The latter is a more natural approach when considering discourse annotation since the flow of the narrative is influenced by what has already been mentioned. For classification, we employed support vector machines (SVMs) and for sequence labelling conditional random fields (CRFs). Previous work on discovering information structure from papers and abstracts has made successful use of both of these methods ( Guo  et al. , 2010 ;  Hirohata  et al. , 2008 ;  Mullen  et al. , 2005 ). While experimental settings vary in each of the above cases, most notably in the number and type of classification categories, the amount of training data available and whether abstracts of full papers are used, the best performing algorithms were SVMs and CRFs. SVM and LibLinear we used the LibSVM (LibS) implementation of SVMs ( Chang and Lin, 2011 ) coded in C++. Our experiments were conducted using a linear kernel, known to perform well in document classification. We used the default values for the C, γ and ϵ parameters and concentrated on the input features. When we experimented with different types of cross-validation and feature configuration we used LibLinear (LibL) ( Fan  et al. , 2008 ) instead of LibS as the latter is costly timewise both in training and testing. LibL is a classifier for large scale data, which uses linear SVMs, splits data into blocks and considers one block at a time. To give an indication about the gain in speed using LibL as opposed to LibS, it takes 29 h 41 min to train one of our models with LibS and 8 h 15 min for testing a single fold versus 10 min and 4 h 36 min, 2  respectively, for LibL. Conditional random fields we chose CRFs because they do not assume independent features but do not suffer from the label bias problem, where preference is given to states with fewer transition possibilities. For our purposes we used CRFSuite ( Okazaki, 2007 ) an algorithm for linear-chain, first-order CRFs, optimized for speed and implemented in C. Stochastic Gradient Descent was employed for parameter estimation. Features for classification features are extracted from each sentence and are represented in a sparse binary matrix format. In selecting features our aim was to take into account different aspects of a sentence, ranging from its location within the paper and the document structure (global features), to its length and sentence-internal features such as the citations, verbs,  n -grams and grammatical triples (GRs) it may contain (local features). Below we describe all our features in detail. The following are all implemented as binary features:
 Absolute location  ( absloc ): we divide the document into 10 unequal segments (as in Loc of ( Teufel, 2000 )) and assign 1 of the 10 locations, A–J, to the sentences. Larger segments, containing more sentences, are designated to be in the middle of the paper. SectionId : a sequentially incremented section number (up to 10) is assigned to each section and inherited at sentence level. SectionId is assigned independently of the section heading, which is addressed by feature Struct-3 below. Struct-1 : the location of a sentence within seven unequal segments of a section. 3  Each section is first divided into three equally sized slices; the first and the last sentence of the section are considered separate segments (1 and 7) whereas the second and the third sentence of the section also form a segment (2). The rest of the first slice is segment 3 and the second slice is segment 4. Segment 6 consists of the second and third sentence from the end of the section and the rest of the third slice is segment 5 ( Teufel, 2000 ). Struct-2 : location within a paragraph split in five equal segments. ( Teufel, 2000 ) Struct-3 : one of 16 heading types assigned to a sentence by matching its section heading against a set of regular expressions (a variant on Struct-3 of  Teufel, 2000 ). SectionId and Struct-3 are complementary features since the first pertains to the absolute location of a section and is dependent on the length of the paper, while the other follows section structure irrespective of paper length. Details on header matching are available in the  Supplementary Material . Location in section  ( sectionloc ): like Struct-2 but at section level. Length : sentences are assigned to one of nine bins, representing a word count range. More details are available in the  Supplementary Material . Citation : we distinguish three cases: no citations, one citation, and two or more citations present. History : the CoreSC category of the previous sentence. Only used in LibS and LibL, implicit in first-order CRF. N-grams : binary values for significant unigrams (Uni), bigrams (Bi) and trigrams.  N -grams are lemmatized using morpha ( Minnen  et al. , 2001 ). Significant unigrams have frequency &gt;3. Bigrams and trigrams are filtered according to the measure of Fair Symmetrical Conditional Probability and the LocalMaxs algorithm, defined in  Silva  et al.  (1999 ). We considered filtering our n-grams by adapting an online stop word list. 4  However, classifier performance was better when we did not filter stop words. In this latter case, no trigrams exceeded the threshold. Examples of significant  n -grams are available in the  Supplementary Material . Verb POS  ( VPOS ): for each verb within the sentence we determine which of the six binary POS tags (VBD, VBN, VBG, VBZ, VBP and VB) representing the tense, aspect and person of a verb are present. Verbs : all verbs in our training data with frequency &gt;1. Verb Class : ten verb classes, obtained by clustering together all verbs with a frequency &gt;150 as in  Guo  et al.  (2010 ). The verb classes can be found in the  Supplementary Material . Grammatical triples  ( GRs ): dependency–head-dependent triples (Briscoe and Carroll format) generated using C&amp;C tools ( Curran  et al. , 2007 ). We used the model of the supertagger trained on biomedical abstracts ( Rimell and Clark, 2009 ) and applied self-training on our papers according to  Kummerfeld  et al.  (2010 ). We considered dependencies subj, dobj, iobj and obj2 with frequency &gt;3. Examples of significant GRs can be found in the  Supplementary Material . Other GR : subjects (Subj), direct objects (Dobj), indirect objects (Iobj) and second objects of ditransitive verbs (Obj2) with frequency &gt;1. Passive  ( P ): whether any verbs are in passive voice. 
 3 RESULTS AND DISCUSSION To test classification accuracy and establish feature contributions to CoreSC recognition we performed a number of runs, including multi-class (CRF, LibL and LibS) and binary classification using 9-fold cross-validation and a variety of feature configurations (All features, Leave-out-one-feature (LOOF) and Single feature with and without stop words). Our results ( Table 2 ) show we can achieve accuracy of &gt;50 in classifying the 11 CoreSCs in full papers. This is a promising result given the difficulty of the task. It is the first time the automatic recognition of such a fine grained set of categories is being attempted for full papers.  F -score for the categories ranges from 76% for EXP (Experiment) to 18% for the low frequency category MOT (Motivation). The distribution of categories in papers is shown in  Table 1 , with RES the most frequent category and MOT and GOA the least frequent. Our feature analysis shows that the most important role is played by  n -grams (primarily bigrams), GRs and verbs as well as global features such as history (sequence of labels) and section headings. It is important to note that particular features do not affect all categories in the same way. In the following, we present our results in detail.  Section 4  discusses various CoreSC-based applications already implemented on the basis of current results.
 Table 2. Micro precision, recall and  F -measure for different system configurations, with highest value for each measure per category in bold Acc BAC CON EXP GOA MET MOT OBS RES MOD OBJ HYP Features Classifier P R F P R F P R F P R F P R F P R F P R F P R F P R F P R F P R F B ase Multinomial 14 19 19 19 9 9 9 10 10 10 1 1 1 11 11 11 1 1 1 13 14 14 21 22 21 9 9 9 3 3 3 2 2 2 ngrams CRFS uite 45.3 45 60 51 35 28 31 72 74 73 42 17 24 29 28 28 24 11 15 49 49 49 43 43 43 49 47 48 42 26 32 24 12 16 LibLinear 39.9 41 47 44 26 23 25 61 67 64 27 18 22 24 24 24 15 12 14 43 45 44 38 37 38 39 39 39 31 25 27 17 13 15 L ib SVM 41.2 41 50 45 30 22 25 66 66 66 30 23 26 26 25 26 22 15 18 48 44 46 39 44 41 45 38 41 33 28 30 21 13 16 Hist+ngram LibLinear 41.2 44 52 47 29 27 28 68 70 69 32 19 24 26 26 26 15 12 13 44 45 45 40 38 39 43 42 42 31 23 26 17 12 14 L ib SVM 44.9 45 62 52 36 27 30 74 66 70 39 12 19 28 31 29 26 05 08 49 43 46 42 49 45 52 42 46 39 19 26 23 09 13 All Binary CRFS uite 34.7 60 51 55 51 32 39 78 72 75 39 13 19 33 17 22 28 10 15 53 40 46 46 31 37 58 37 45 42 18 25 28 06 10 LibLinear 34.6 53 60 56 41 39 40 69 73 71 32 21 25 27 25 26 23 18 20 45 47 46 44 43 43 45 45 45 35 26 30 18 12 14 All Multi CRFS uite 50.4 56 65 60 46 42 44 74 78 76 41 21 28 31 29 30 29 13 18 50 52 51 46 49 47 53 52 52 42 28 34 26 14 18 LibLinear 47.7 54 60 57 43 40 41 69 73 71 35 20 25 29 28 28 22 16 18 47 49 48 45 44 45 49 49 49 38 28 32 21 15 18 L ibsvm 51.6 56 68 62 50 41 45 72 78 75 37 20 26 33 25 29 25 06 10 53 47 50 46 57 51 54 52 53 43 29 34 32 13 19 
 Classifiers and categories Table 2  shows that LibS has the highest accuracy at 51.6%, closely followed by CRF at 50.4% with LibL at 47.7%. All three classifiers outperform the simple baseline (B ase ) by a large margin. The latter consists of multinomial trials, which randomly label new instances according to the percentage of each CoreSC in the training data. We have also considered an  n -gram baseline for both CRF and SVM and a history+ n -gram baseline for SVM (history is implicit for CRF), which are discussed in the section on Feature Contribution. The best results overall are obtained from multi-class classification using all the features we considered. Interestingly the combination of binary classifiers (one for each CoreSC category) gave the highest precision in most cases but recall was significantly lower than in the multi-class scenario. There is not a significant difference in performance between LibS+all features and CRF: five categories seem to be predicted better by LibS and for the other six CRF performs better. When the history feature is absent, LibS and LiBL perform much lower than CRF but hist+ n -gram for LibS is comparable to the n-gram performance of CRF. This highlights the importance of category sequence information for the task. The performance of LibL lags slightly behind both LiBS and CRF but this is to be expected since it is an approximation for linear SVMs. The highest performing categories for all three classifiers are EXP, BAC and MOD with an  F -score of 76%, 62% and 53%, respectively. BAC is the second most frequent category (19%) in the corpus after RES, so high recall is not surprising. EXP and MOD (experimental and theoretical methods) are more interesting, as they are moderately frequent (10 and 9%), respectively. Furthermore, EXP and MOD are the only categories which have a higher  F -score in automatic recognition compared with κ-IAA ( Liakata  et al. , 2010 ) as shown in  Figure 3 . On the other hand categories with high κ such as CON, MET and OBJ were more difficult to classify than expected. While κ was measured on only 41 papers (5022 sentences) ( Liakata  et al. , 2010 ), which may not be representative of the entire corpus, these results suggest that there is not necessarily a direct correlation between annotator agreement and classifier performance. This is in support of  Beigman Klebanov and Beigman, 2009 , which argues that IAA is neither sufficient nor necessary for obtaining reliable data from annotated material but rather it is important to focus on non-noisy, ‘easy’ instances.
 Fig. 3. F -score versus κ for CoreSCs. Beigman Klebanov and Beigman, 2009  suggest researchers should report the level of noise in a corpus and only use non-noisy (easy) instances for testing. They emphasize the importance of requiring the agreement between more than two annotators, which reduces both the chance of random agreements as well as hard case bias, whereby a classifier tends to model the pattern of bias of a particular annotator for instances which are hard to predict. By having different phases of corpus development, with a varied number of annotators for each phase and subset of the corpus as well as a large number of classification categories, we believe that we have minimized the chance of random agreements and hard case bias. Therefore, we can infer that when our machine learning annotations agree with manual annotations, noise levels will be usually low, instances will be easier to predict and thus classifier confidence will be higher. Indeed this is confirmed both by a Pearson moment correlation test between agreement and classifier confidence and a Welch  T -test for classifier confidence values in cases of agreement and disagreement, both of which gave a  p &lt;2.2e-16 at 99%. They showed a direct correlation between classifier confidence and agreement between manual annotation and classifiers. Details are in the  Supplementary Material . Classifier confidence for an instance is a probability, where a high value indicates high classifier confidence for the particular prediction. As an indication of the noise for different categories in the corpus, we show the confidence of the machine learning classifiers when both classifiers agree with the manual annotation and when there is no agreement between either the classifiers or the manual annotation (see  Figs 4  and  5 ). For the cases where LibSVM agrees with CRF and the manual annotation, confidence scores are high, with over 75% of the data having a confidence value of &gt;0.6, and over 50% of the data having a confidence score of over 0.7. This can be compared against the situation of disagreement where only 25% of the data have a confidence score of 0.6. For EXP, BAC and MOD the confidence scores are especially high in cases of agreement, with 50% of the data having a confidence score of over 0.87. Therefore, agreements for EXP and MOD consist mostly of non-noisy (easy) instances. Classifier confidence for the entire corpus is depicted in  Figure 6 . Assuming that lack of noise correlates with high classifier confidence, we can say that &gt;50% of data in each category (and in most well beyond 75%) is non-noisy.
 Fig. 4. Confidence value when LibS, CRFSuite and manual annotation agree. 
 Fig. 5. Confidence value when there is no agreement on annotation. 
 Fig. 6. Confidence value scores per category for the entire corpus. Classifier performance for the CoreSC categories can be ranked from highest to lowest  F -score as follows: EXP &gt; BAC &gt; MOD &gt; RES &gt; OBS &gt; CON &gt; OBJ &gt; MET &gt; GOA &gt; HYP &gt; MOT. OBJ performs well given its low frequency, suggesting that OBJ sentences contain distinct features. The low scores for MET may be due to noise introduced by our neglect of the distinction between MET-Old and MET-New ( Liakata  et al. , 2010 ). The low  F -score for MOT and HYP are due to their low frequency as the levels of noise are similar to those of OBJ. We intend to boost performance for the low frequency categories by using active learning. These are promising results given the complexity of the task, the number of the categories and their distribution in the corpus. A confusion matrix ( Fig. 7 ) gives an indication of which categories have consistent overlaps. There is bias in favour of the BAC category due to its high frequency and broad definition, which we will need to counterbalance in the future. CON is often taken as RES whereas RES is often confused with OBS and vice versa. GOA is often assigned to OBJ and MET, the latter presumably because goals and method are often expressed in the same sentence. MET is confused with EXP and BAC, the latter because we have not yet considered the second layer of CoreSC annotation at this stage, which caters for MET-Old, methods mentioned in previous work. OBJ is often confused with MET, since a method can be the object of an investigation. 5  Finally, HYP is often assigned to RES, CON and BAC. This can be explained by the fact that a weak result or conclusion is often expressed in the same language as a hypothesis, while a hypothesis may also be expressed as an assumption arising from background knowledge. For examples see the  Supplementary Material .
 Fig. 7. Confusion matrix for CoreSC categories according to LibS. If we merge CoreSC categories so that we consider a coarser grain layer of four categories, namely Prior (BAC), Approach (MET+MOD+EXP), Outcome (OBS+RES+CON) and Objective (MOT+GOA+HYP+OBJT) then our  F -measures respectively become: BAC: 59%, Approach: 72%, Outcome: 81%, Objective: 38%. A variant merge with seven categories, roughly corresponding to the scheme proposed by  de Waard  et al. , 2009 , which considers BAC, HYP, Problem(=MOT), GOA=(GOA+OBJT), MET= (MET+EXP+MOD), RES=(OBS+RES), Implication(=CON), gives us F1: BAC: 60%, CON: 44%, MET: 72%, GOA: 47%, MOT: 19%, HYP: 18% and RES: 72% This shows the flexibility of our scheme for different applications, which may require different levels of granularity. Feature contribution we examine feature contribution in LOOF cross-validation and single feature runs, using CRF and LibL.  Tables 3  and  4  show how  F -score is affected when each type of feature is omitted. For each CoreSC category we have highlighted the lowest scores (bold), corresponding to the most important features being left out, and the highest scores (italic), corresponding to features whose omission has less impact on classification. Performance for all categories drops when all  n -grams are removed. Since features are not independent, many of the important features of other categories are covered in  n -grams but this does not necessarily work in both directions. Primarily, bigrams are more important than unigrams, since many of the former contain the latter. Categories affected most by the omission of unigrams are the low frequency categories GOA, MOT and HYP for CRF and MOT, HYP for LibL. Bigrams are not as important for these categories and removing them improves performance in the case of MOT and HYP. This is probably because they are not frequent enough for association with bigrams. Removing the verb feature has a negative effect on MOT, HYP and GOA in CRF and GOA and HYP in LibL. This agrees with our observation of the importance of verbs in single feature classification ( Fig. 8 ). The high frequency categories are more robust to omission of features, whereas the lower frequency categories are dependent on all features.
 Fig. 8. Single feature classification with LibL, illustrating the contribution of 15 individual features. 
 Table 3. F -measures for CRFSuite LOOF, 9-fold cross-validation Feat BAC CON EXP GOA MET MOT OBS RES MOD OBJ HYP all 60 44 76 28 30 18 51 47 52 34 18 length 60 44 76 27 30 18 51 47 53 34 19 ref 58 44 76 26 30 18 51 47 52 34 17 absloc 60 44 76 28 29 18 51 47 52 34 18 struct1 60 43 76 27 30 18 51 47 52 33 17 secid 60 44 76 27 30 18 51 48 51 35 17 Struct-2 60 44 76 27 30 17 51 47 52 34 18 SecLoc 60 44 76 27 30 19 51 47 52 34 18 Struct-3 60 43 75 26 30 18 51 47 52 34 19 uni 60 44 76 24 29 17 50 47 51 33 15 bi 59 43 75 27 30 22 50 46 50 32 19 ngrams 58 42 74 25 29 17 47 45 48 29 14 gr 60 44 75 27 30 18 51 47 52 35 17 pos 60 44 76 26 29 18 51 47 53 34 16 subj 60 45 76 27 30 17 51 48 52 34 18 dobj 60 45 76 28 30 18 51 47 53 34 19 iobj 60 44 76 27 30 18 51 47 52 34 18 obj2 60 44 76 28 30 18 51 47 52 34 18 vclass 60 44 76 27 30 18 51 47 52 34 18 verb 60 44 76 27 30 17 51 47 52 34 17 
 Table 4. F -measures for LibLinear LOOF, 9-fold cross-validation Feat BAC CON EXP GOA MET MOT OBS RES MOD OBJ HYP all 57 41 71 25 28 18 48 45 49 32 18 history 55 41 71 28 27 20 48 43 46 32 17 length 57 42 71 24 28 19 48 45 48 31 17 ref 55 41 71 25 28 19 48 45 48 31 15 absloc 57 40 72 25 28 20 48 45 49 33 18 Struct-1 57 41 71 26 28 21 48 45 49 31 17 secid 57 41 71 26 28 19 48 44 48 32 17 Struct-2 57 41 71 25 28 19 48 45 49 32 17 SecLoc 57 41 71 26 28 18 48 45 48 32 18 Struct-3 56 40 70 25 27 19 48 44 47 30 19 uni 56 41 72 26 27 17 47 44 46 31 16 bi 54 40 70 25 27 20 46 43 45 27 17 ngrams 53 37 69 23 26 17 44 42 41 26 12 gr 56 40 71 23 29 19 47 44 49 31 17 pos 57 42 71 25 28 20 48 45 49 32 16 subj 57 41 71 25 29 21 47 45 48 31 17 dobj 57 42 71 25 28 20 48 45 49 32 19 iobj 57 41 71 26 28 19 48 45 48 32 17 obj2 57 41 71 25 28 18 48 45 49 32 18 vclass 57 41 71 26 28 19 48 45 49 33 17 verb 57 41 71 24 28 20 48 45 49 32 16 
 Single feature classification is more meaningful with respect to individual feature contributions and  Figure 8  paints a clear picture of which features are most important for which category. We believe this to be the most interesting finding of our analysis. While  Figure 8  shows the general trend whereby  n -grams (D) (bigrams and GRs are not actually shown in  Figure 8 , but they strongly correlate with unigrams) followed by direct object (E) and verb (F) as accounting for the overall F-measure of a category, this is not true for all categories. For EXP, BAC and CON section headings (C) matter more than  n -grams and for BAC, CON and RES absolute location (M) also plays a prominent role, meaning that the location of these three categories tends to be fixed in a paper (presumably in the beginning and the end). Citations (O) play an important role in discriminating BAC and are also prominent for CON, RES and MET to some extent. Verbs (F) are usually more important than subjects (G) but slightly less important than direct objects (E), however verbs (F) feature more prominently for categories such as RES, GOA, HYP and OBJ suggesting that particular verbs are used in the context of these categories. Perhaps more feature engineering involving semantic categories of verbs would benefit the low frequency categories. Verb tense (expressed by VPOS (I)) does not seem to play a major role, though its contribution is higher for OBS and RES. Looking at the feature profile of different categories, RES and MET show the least variation between individual feature contribution but it is clear that RES is more location specific than MET. Table 5  shows the number of individual features considered for each feature type. The vast majority of features are bigrams (42 438), unigrams (10 515) and GR triples (11 854), which also explains their importance for the classification. This makes the prominence of citations and global structural features such as section headings all the more important whenever we encounter them.
 Table 5. Numbers for each type of feature Feat Uni Bi GR VPOS Subj Dobj Iobj Obj2 Verb VC P H Gl L C No. 10 515 42 438 11 854 6 3843 7414 45 59 1543 10 1 12 53 9 3 Numbers for each type of feature were: L, length; H, history; C, citation; Gl, global features, including absloc, sectionid, struct1-3, sectionloc 
 Variants of some of the above features have been used by  Teufel and Moens (2002 ),  Mullen  et al.  (2005 ) and  Merity  et al.  (2009 ) to automate AZ.  Merity  et al. , 2009  found that  n -grams (unigrams and bigrams) in combination with knowledge of the label of previous sentences (history) constituted a very strong baseline for AZ. This agrees with our findings in general, where  n -grams are roughly responsible for 40% of the system accuracy, the history category contributes another 5% and a further 5–6% is due to all other features. In the future, we intend to consider more elaborate semantic classes for features and also consider training individual classifiers for each category which we would then combine using stacking or ensemble techniques. Comparison with related work a direct comparison between our results and earlier work is not possible, as the scope, schemes and experimental settings differ significantly. Earlier work on automating discourse schemes with four categories ( Hirohata  et al. , 2008 ;  Lin  et al. , 2006 ;  McKnight and Srinivasan, 2003 ) has reported  F -measures in the 80s or 90s. However, in addition to having a third of the number of categories, these schemes only concentrate on abstracts, which are shown to have a very different structure from full articles.  Shatkay  et al.  (2008 ) annotate sentences from full articles but they evaluate on a small scale and do not attempt to classify an entire article. Their scheme has only three to four categories per dimension, where each dimension is evaluated separately from the rest. Our results are more comparable to  Mullen  et al.  (2005 ) and  Teufel (2000 ), who have automated AZ for articles with six and seven categories, respectively, reporting respective  F -measures 0.44–0.87 and 0.26–0.86.  Merity  et al.  (2009 ) replicated and significantly improved on the results of  Teufel (2000 ), reporting an  F -measure in the 90s for the same categories. However,  Teufel  et al.  (2009 ) introduced a new scheme, designed specifically for chemistry papers (ChemAZ), containing 15 categories, which has not been yet automated. It has been shown that a small number of categories annotated by a small number of experts will result in a less challenging annotation task, leading to a higher  F -measure. However, a more expressive and thus more complex annotation scheme allows for better representation of the discourse structure of the articles so as to identify hypotheses and relevant evidence (see  Section 1 ). This will contribute to more advanced information extraction solutions in the future. 4 APPLICATIONS AND FUTURE WORK One of the applications stemming from our work is the use of automatically generated CoreSC annotations for the production of extractive summaries of full papers in chemistry and biochemistry. Such summaries are different from abstracts as they are longer (20 sentences) and represent the entire content of the paper, from Background and Hypotheses to Experiments performed, main Observations and Results obtained. The idea is that such summaries could be read much faster than the paper but convey a lot more of the key information than the abstract, which often acts as a selling point of the paper. We created summaries so that each contained 1–2 sentences from each CoreSC category (Hypothesis, Background, etc.), extracted from the original paper, following the distribution of categories in the paper. These summaries were given to 12 experts divided into 4 groups, along with summaries created using Microsoft Autosummarize and summaries written by humans. The automatically generated summaries performed significantly better than Miscrosoft autosummarize and achieved a 66% and 75% precision in answering complex content based questions. In some cases they outperformed human summaries. 6  Question-based extractive summaries created using CoreSCs could be used to help speed up curation and we plan to explore this in the future. A different user based study, involved collaboration with experts in CRA, who were presented with abstracts that contained CoreSC annotations and abstracts with no annotations, or annotations originating from simpler schemes (abstract sections or an AZ variant) ( Guo  et al. , 2011 ). Three experts were timed as they answered questions about the main objectives and methods described in abstracts and it was shown that experts responded consistently faster when given abstracts annotated with CoreSCs than in the rest of the cases, while no significant difference was observed pertaining to the quality of the responses. In the future, we plan to perform more question based user studies with CRA experts, using full papers. We also plan to use CoreSC annotated papers in biology to guide information extraction and retrieval, characterize extracted events and relations and also facilitate inference from hypotheses to conclusions in scientific papers. Our web-based tool for the automatic annotation of CoreSC categories in full biomedical papers from Pubmed Central is available for biologists to download and use. The ability to automatically identify and qualify discourse structure from the scientific literature has far-reaching implications. The original facts and results from a scientific publication form the key information to be extracted in order to curate biological resources and validate against resources such as UnitProtKb, EntrezGene, Reactome and others. The different types of conceptualization zones defined by CoreSCs (Background, Hypothesis, Method, etc.) so far have been used to create extractive summaries and more use cases of filtering text during information extraction are in progress. Work in progress also involves the application of CoreSC annotations to full papers involving CRA and drug–drug interactions and preliminary results show that the annotation scheme and categorization methods generalize well to these new domains. 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>REDEMPTION: reduced dimension ensemble modeling and parameter estimation</Title>
    <Doi>10.1093/bioinformatics/btv365</Doi>
    <Authors>Liu Yang, Manesso Erica, Gunawan Rudiyanto</Authors>
    <Abstract>Summary: Here, we present REDEMPTION (Reduced Dimension Ensemble Modeling and Parameter estimation), a toolbox for parameter estimation and ensemble modeling of ordinary differential equations (ODEs) using time-series data. For models with more reactions than measured species, a common scenario in biological modeling, the parameter estimation is formulated as a nested optimization problem based on incremental parameter estimation strategy. REDEMPTION also includes a tool for the identification of an ensemble of parameter combinations that provide satisfactory goodness-of-fit to the data. The functionalities of REDEMPTION are accessible through a MATLAB user interface (UI), as well as through programming script. For computational speed-up, REDEMPTION provides a numerical parallelization option using MATLAB Parallel Computing toolbox.</Abstract>
    <Body>1 Introduction Mathematical modeling plays an increasingly important role in understanding and predicting biological phenomena. In this regard, ordinary differential equation (ODE) models are often created to describe the dynamic behavior of biological systems based on mass or molar conservation, as follow:
 (1) d X ( t ) d t = S v ( X ( t ) , p ) ;     X ( 0 ) = X 0 , 
where  t  denotes time,  X ( t )  is the species concentration vector,  S  is the stoichiometric or connectivity matrix,  v ( X ( t ) , p )  is the reaction rate or flux equation vector,  p  denotes the kinetic parameter vector and  X 0  denotes the initial concentration vector. The estimation of unknown kinetic parameters from time-series data is the most challenging step in the creation of such biological models, due to: (i) high computational cost associated with global optimizations and with integrating ODEs ( Chou and Voit, 2009 ), and (ii) the lack of parameter identifiability ( Srinath and Gunawan, 2010 ). The high computational complexity could be addressed by performing the parameter estimation incrementally. Here, time-series data are first smoothened and differentiated, and subsequently the dynamic flux values are computed by viewing  Equation (1)  as an algebraic equation. The parameters are estimated one flux at a time while avoiding ODE integrations ( Voit and Almeida, 2004 ). However, such an approach requires that  S  has a full column rank, which is often not satisfied in the modeling of biological systems. We recently developed incremental parameter estimation (IPE) and integrated flux parameter estimation (IFPE) to address the limitation related to the matrix  S . In IPE (IFPE), we formulated the parameter estimation as a nested optimization, where the outer optimization was performed over a subset of parameters corresponding to the independent (integrated) fluxes, defined such that given their values, the remaining (integrated) fluxes could be estimated from the slope of time-series data ( Jia  et al. , 2012a ;  Liu and Gunawan, 2014 ). The inner optimizations involved estimating parameters associated with the dependent flux subset using their estimated (integrated) flux values, performed one flux at a time. Meanwhile, incomplete parameter identifiability implies that many parameter combinations could fit the data equally well ( Jia  et al. , 2012b ). In ensemble modeling, instead of generating one best-fit parameter estimate, one looks for a set of parameter combinations which fit the data within an acceptable prediction accuracy, e.g. an upper bound for the error function. We extended our incremental estimation strategy above such that the identification of parameter ensemble could be efficiently performed over smaller parameter dimensions ( Jia  et al. , 2012b ). Re duced  D imension  E nsemble  M odeling and  P arameter estima tion  (REDEMPTION) provides a UI for efficient parameter estimation and ensemble modeling based on IPE and IFPE in MATLAB. The functions within REDEMPTION were designed to address a common scenario in biological modeling, where the number of reactions exceeds that of the (measured) species. When this scenario does not apply, REDEMPTION automatically reverts to a standard IPE. For ensemble modeling, REDEMPTION employs an efficient parameter exploration algorithm HYPERSPACE, which combines an Adaptive Monte Carlo method and a multiple ellipsoids based sampling ( Zamora-Sillero  et al. , 2011 ). 2 Main features Model and data specifications:  REDEMPTION’s UI starts with the Main window ( Fig. 1 a), from which users can access all functionalities. The ODE model equations can be specified manually through the Model Editor using power-law or linear-logarithmic (lin-log) kinetics ( Fig. 1 c), or by importing an SBML file. In addition, REDEMPTION requires upper and lower bound values for the model parameters. A parameter will be estimated from data when the upper and lower bounds differ. Users also need to provide the time-series concentration data in Comma-Separated Values (CSV) format. For data pre-processing, REDEMPTION includes piecewise polynomial spline-fitting, where users can adjust the number of pieces and the order of polynomials ( Fig 1 d).
 Fig. 1. Workflow of parameter estimation and ensemble generation in REDEMPTION applied to branched pathway example 
 Parameter estimation: 
 Figure 1 e depicts REDEMPTION’s parameter estimation UI. For parameter estimation, REDEMPTION provides four variants of the IPE and IFPE methods and two error functions using sum of squares of the absolute or relative model prediction errors (User Guide for more details). Here, users can select MATLAB’s Genetic Algorithm (GA) or enhanced Scatter Search (eSS) ( Egea  et al. , 2010 ) as the global optimization algorithm. Ensemble modeling:  After a successful run of parameter estimation, users can generate parameter ensemble corresponding to parameter combinations whose error function values are smaller than a user-specified threshold ( Fig. 1 f). The optimal error function value obtained from the prior parameter estimation step is provided as a reference for setting this threshold. The appropriate threshold could for example be calculated by bootstrapping the original data ( Jia  et al. , 2012b ).  Figure 1 f shows a 2D parameter projection of the generated parameter ensemble, in which each blue dot represents a parameter combination in the ensemble and the largest dot corresponds to the optimal solution of the parameter estimation step. To include an additional dataset, users can perform parameter estimation and ensemble modeling again using the additional data and new parameter bounds based on the parameter ensemble (User Guide for more information). 3 Examples Figure 1  illustrates the workflow of REDEMPTION for parameter estimation and ensemble generation using a branched pathway example as shown in  Figure 1 b ( Voit and Almeida, 2004 ). REDEMPTION also includes the lin-log modeling of  Lactococcus lactis  glycolytic pathway as another example ( del Rosario  et al. , 2008 ). Details of these examples can be found in the User Guide. 4 Conclusions REDEMPTION provides an integrated, numerically efficient tool for estimating kinetic parameters and parameter ensemble of ODE models. The tools within REDEMPTION are accessible through a user-friendly MATLAB UI and applicable for ODE models with power-law or lin-log kinetics, and those in SBML format. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Hierarchical HotNet: identifying hierarchies of altered subnetworks</Title>
    <Doi>10.1093/bioinformatics/bty613</Doi>
    <Authors>Reyna Matthew A, Leiserson Mark D M, Raphael Benjamin J</Authors>
    <Abstract/>
    <Body>1 Introduction Many cellular processes involve interactions between different molecules. Therefore, the analysis and interpretation of large-scale ‘omics data is often informed by biological interaction networks. For example, the expression of genes in the same protein complex or pathway is often correlated ( Ge  et al. , 2001 ), so physical interaction networks can be used to study gene expression data ( Luscombe  et al. , 2004 ). Similarly, genetic variants associated with a disease often cluster in a small number of biological processes, and therefore networks can be used to analyze germline variants from genome-wide association studies (GWAS) ( Califano  et al. , 2012 ;  Lee  et al. , 2011 ;  Leiserson  et al. , 2013 ) or somatic mutations in cancer ( Leiserson  et al. , 2015 ;  Vandin  et al. , 2011 ). In these applications, one has a measurement, or score, on each of the vertices in a network, and the goal is to identify  altered subnetworks , or sets of vertices that are both close on the network and have high scores. A specific example of the general problem of identifying altered subnetworks arises in cancer genomics. In this case, one obtains measurements of somatic mutations from a cohort of cancer patients and aims to distinguish the small number of  driver  mutations that are typically responsible for tumor initiation and development from the much larger number of random  passenger  mutations. Tests for recurrence of individual mutations or genes ( Kandoth  et al. , 2013 ;  Lawrence  et al. , 2013 ;  Mularoni  et al. , 2016 ) are often challenged by the ‘long tail’ phenomenon, where most driver mutations are extremely rare in the cohort. By taking advantage of the observation that driver mutations tend to cluster in a few key biological processes (e.g. cell cycle or apoptosis) ( Hanahan and Weinberg, 2011 ;  Vogelstein  et al. , 2013 ), one can use protein–protein interaction (PPI) networks to identify significantly mutated sets of interacting genes ( Cowen  et al. , 2017 ;  Raphael  et al. , 2014 ). These gene sets may span multiple biological scales, from interacting gene pairs through protein complexes and pathways to entire biological systems ( Fig. 1 ). Hierarchies are commonly used to describe relationships between gene sets across different scales; gene ontologies (GO) ( Ashburner  et al. , 2000 ) are one such example. Fig. 1. Illustration of a hierarchy of gene sets, which vary in size across different biological scales A number of methods have been developed to address the problem of identifying altered subnetworks. We broadly classify these methods into three groups. The first group of methods defines a restricted class of candidate subnetworks and then select high-weight subnetworks from these candidates according to their vertex scores. Examples of these methods include jActiveModules ( Ideker  et al. , 2002 ) and heinz/BioNet ( Beisser  et al. , 2010 ;  Dittrich  et al. , 2008 ) which solve the maximum-weight connected subgraph (MWCS) problem. Other methods, such as MUFFINN ( Cho  et al. , 2016 ) and NetSig ( Horn  et al. , 2018 ) examine only the nearest neighbors of each vertex, thus assigning a score to ‘star subnetworks’ centered on every vertex. A related approach is used in Omics Integrator ( Tuncbag  et al. , 2013 ,  2016 ), which solves a variation of the prize-collecting Steiner forest (PCSF) problem. The second group of computational methods search for high-weight subnetworks by jointly examining vertex scores and network topology. These methods are able to identify subnetworks that would not be considered using vertex scores or topology alone. Many such methods use a diffusion process or random walk to measure the similarity of pairs of nodes using the paths between them ( Cowen  et al. , 2017 ). Heat diffusion and random walks are common tools for biological network analysis (e.g.  Cao  et al.,  2013 ;  Cho  et al.,  2015 ;  Chung and Zhao, 2010 ;  Hofree  et al.,  2013 ;  Komurov  et al.,  2012 ;  Paull  et al.,  2013 ;  Shnaps  et al.,  2016 ). PRINCE ( Vanunu  et al. , 2010 ), HotNet ( Vandin  et al. , 2011 ,  2012 ), HotNet2 ( Leiserson  et al. , 2015 ) and  Ruffalo  et al.  (2015)  are examples of this group of methods. The third group of computational methods is similar to the first and second groups, but these methods incorporate additional information, including predefined pathways [e.g. PARADIGM ( Vaske  et al. , 2010 ) and DEGraph ( Jacob  et al. , 2012 )], mutual exclusivity and/or co-occurrence of mutations among genes/proteins [e.g. MEMo ( Ciriello  et al. , 2012 ), MEMCover ( Kim  et al. , 2015 ) and BeWith ( Dao  et al. , 2017 )], or connections between mutations and expression (e.g.  Kim  et al.,  2011  and HIT’nDRIVE ( Shrestha  et al. , 2017 )). These methods leverage additional hypotheses and/or data in their application domains, often improving their predictions for specific applications but potentially limiting their extension to other areas. In this article, we introduce a novel computational method, Hierarchical HotNet, to identify altered subnetworks. Hierarchical HotNet addresses several limitations with existing approaches. Key features of Hierarchical HotNet include integration of both network topology and vertex scores; robustness to new and complex datasets; combating ascertainment bias in data, e.g. recurrently mutated genes tend to be better studied with more known interactions; and evaluating the statistical significance of results.  Table 1  compares features for several state-of-the-art approaches.
 Table 1. Features of several methods for identification of altered subnetworks Feature/Method heinz MUFFINN NetSig HotNet2 Hierarchical HotNet Candidate subnetworks Connected subnetworks Nearest neighbor subnetworks Nearest neighbor subnetworks Unrestricted Unrestricted Evaluates statistical significance No No Yes, network permutations Yes, multiple options Yes, multiple options Addresses ascertainment bias in network No No Degree-aware statistical test Penalizes high-degree nodes Penalizes high-degree nodes; degree-aware statistical test Model selection Yes, on data No Yes, on results No Yes, on results Consensus across datasets No No No Yes Yes Supported interaction types Unweighted and undirected Weighted and undirected Unweighted and undirected Unweighted and undirected Weighted and directed Hierarchical HotNet jointly considers network topology and vertex scores (i.e. it belongs to the second of the above groups of methods) to construct a hierarchy of topologically close and high-scoring subnetworks. It uses a rigorous approach to identify statistically significant subnetworks across different regions of its hierarchy, providing relationships between subnetworks. Applied to cancer genomics data, Hierarchical HotNet identifies significantly mutated subnetworks across biological scales. We evaluate the performance of Hierarchical HotNet on the problem of finding mutated subnetworks in cancer using two recent pan-cancer somatic mutation datasets and three interaction networks. We compare Hierarchical HotNet to heinz, MUFFINN, NetSig and HotNet2 as representative methods from the above groups. Hierarchical HotNet outperforms these other methods in identifying known and candidate cancer genes. Hierarchical HotNet also prioritizes many putative cancer genes that are not statistically significant by single-gene tests. 2 Materials and methods Given a network or graph  G = ( V , E , w )  with  n = | V |  vertices,  m = | E |  edges and scores/weights  w ( v 1 ) ,   … , w ( v n )  on the vertices, Hierarchical HotNet aims to hierarchically cluster high-weight vertices that are topologically close on the network and identify statistically significant subnetworks in the hierarchy. More specifically, Hierarchical HotNet (i) combines network topology and vertex scores, (ii) defines a similarity matrix  S  from  G  using a random walk-based approach as described in Section 2.1, (iii) constructs a hierarchy of clusters consisting of strongly connected components (SCCs) as described in Section 2.2, (iv) assesses the statistical significance of clusters in the hierarchy as described in Section 2.3, (v) identifies altered clusters from statistically significant regions of the hierarchy and (vi) combines these clusters from multiple networks and sets of vertex scores as described in Section 2.4.  Figure 2  shows an overview of the Hierarchical HotNet method in the context of biological data. Fig. 2. Overview of the Hierarchical HotNet method.  (1)  A biological interaction network and gene scores are paired to form a vertex-weighted graph  G = ( V , E , w ) .  (2)  A joint similarity matrix  S  is derived from both network topology and vertex weights using a random walk-based approach.  (3)  A dendrogram  T  of vertex sets is constructed from  S  using asymmetric hierarchical clustering.  (4)  The statistical significance  p  of  T  is evaluated (test statistic and null model for  G  not shown).  (5)  Representative altered subnetworks  C δ  are provided from the dendrogram  T 2.1 Similarity matrix  S  for vertex-weighted graph  G In order to characterize groups of related vertices in the vertex-weighted graph  G , we define a similarity measure between pairs of vertices. Any such measure yields a similarity matrix  S = [ s i j ] , where  s ij  measures how similar a vertex  v j  is to a vertex  v i  according to both network topology and the vertex scores. By incorporating both topology and scores, this measure allows us to quantify proximity in a vertex-weighted graph beyond the topological structure of the graph or the weights on the vertices. The network itself describes a particularly simple similarity measure that is given by the adjacency matrix of the graph, where two vertices are defined to be similar if and only if they share an edge. Beyond nearest neighbors ( Eppstein  et al. , 1997 ), diffusion kernels ( Kondor and Lafferty, 2002 ;  Schölkopf  et al. , 2004 ;  Vandin  et al. , 2011 ,  2012 ), random walks ( Chung, 2007 ;  Chung and Zhao, 2010 ;  Leiserson  et al. , 2015 ) and other measures ( Yip and Horvath, 2006 ) define similarity beyond the presence or absence of direct interactions. These similarity measures capture network topology or use network topology to ‘smooth’ or reprioritize the scores over the network ( Ruffalo  et al. , 2015 ;  Vanunu  et al. , 2010 ), resulting in network-adjusted vertex weights. In contrast, Hierarchical HotNet defines a similarity measure using both network topology and vertex scores. HotNet ( Vandin  et al. , 2011 ,  2012 ) uses a diffusion kernel to model heat diffusion over the edges of a graph, while HotNet2  Leiserson  et al.  (2015)  and Hierarchical HotNet use a random walk, which is a stochastic process describing evolving distributions on the vertices of a graph. In a common version of the random walk with restart, a random walker traverses the graph in a series of discrete steps, transitioning to one of its neighbors at each step with probability  β  and returning to its initial position with probability  1 − β , leading to a non-trivial stationary distribution. We use this stationary distribution to define the following similarity matrix. We now derive the similarity matrix for a simple vertex-weighted graph. Details for directed and weighted edges are in the  Supplementary Material . Let  A  be an adjacency matrix for a vertex-weighted graph  G  and let  D = diag ( deg ⁡ ( G ) ) = diag ( d 1 , … , d n )  be the corresponding diagonal degree matrix. We define a topological similarity matrix
 (1) P = β ( I − ( 1 − β ) A D − 1 ) − 1 
using the stationary distribution of the random walk with restart. This matrix is a stochastic matrix whose columns are probability vectors. We scale the columns of  P  with the vertex weights  w  to define a joint similarity matrix  S = [ s i j ] = P × diag ( w ( v 1 ) , … , w ( v n ) ) , where
 (2) s i j = w ( v j ) ( β ( 1 − β ) a i j d j + β ( 1 − β ) 2 ∑ k = 1 n a i k d k a k j d j + ⋯ ) . Note that (2) explicitly addresses network ascertainment bias by penalizing high-degree vertices. Note, too, that both  P  and  S  are asymmetric, allowing these matrices to capture potentially asymmetric relationships. See the  Supplementary Material  for a more detailed discussion of  S , including a procedure for choosing the restart probability  β  that preserves the locality of the random walk with restart. 2.2 Hierarchy  T  for similarity matrix  S We identify subnetworks in the graph  G  by finding clusters using the similarity matrix  S.  Most clustering algorithms rely, either explicitly or implicitly, on one or more parameters and a variety of clustering algorithms, including  k -means clustering, network modularity, density-based clustering and spectral clustering, give rise to parameterized families of vertex clusterings. Often, the resulting clusters are sensitive to the values of these parameters. Procedures for selecting parameter values may be sensitive to the chosen training datasets or computationally expensive and the recommended parameter values may reflect unstated assumptions about the data. Applying a clustering algorithm over all parameter values of potential interest may be computationally or statistically infeasible. Hierarchical clustering algorithms provide parameterized families of vertex clusterings in the form of a hierarchy or dendrogram, where the height of a dendrogram corresponds to the choice of a clustering parameter. Hierarchical clustering reduces the need for clustering parameter selection, providing clusters over all possible parameter values as well as relationships between the clusters themselves. Given a similarity matrix  S , a hierarchical clustering method can produce a parameterized family  { C δ } δ  of clusterings, where the relationships between the clusters can be described with a dendrogram  T.  The clustering parameter  δ  corresponds to a height of the dendrogram  T  and a cut of  T  at  δ  corresponds to a clustering, or partitioning,  C δ  of the vertices. Since  S  is a similarity matrix, smaller values of  δ  produce larger clusters  C δ  and larger values of produce smaller clusters. Each vertex of  T  is a cluster  C ⊆ V  and the height  δ  of a vertex in  T  is the largest value of  δ  for which  C ∈ C δ . There is a leaf vertex in  T  for each vertex  v ∈ V  and there is a single root vertex in  T  for the set  V.  The relationships between the leaf vertices, the inner vertices and the root vertex of  T  produce a substructure of clusters. Single-, average- and complete-linkage clustering are common examples of hierarchical clustering algorithms for symmetric similarity or dissimilarity matrices (see  Defays, 1977 ;  Sibson 1973 ). Hierarchical clustering algorithms for asymmetric matrices are less common ( Malliaros and Vazirgiannis, 2013 ). In particular, we perform the following hierarchical clustering procedure, which preserves asymmetric relationships between genes. For  δ ≥ 0 , let  H δ = ( V , E δ )  be a directed graph with  E δ = { ( v j , v i ) ∈ V × V : s i j ≥ δ }  and let  C δ  be the SCCs of  H δ . This parameterized family  { C δ } δ  induces a dendrogram  T , where smaller values of  δ  correspond to larger clusters  C δ  higher in  T. Hubert (1973)  first discussed using SCCs for hierarchical clustering with asymmetric similarity matrices, which is equivalent to single linkage clustering when  S  is symmetric.  Tarjan (1983)  described an  O ( n 2 log ⁡ n ) -time algorithm for this procedure, which we used for Hierarchical HotNet. To the authors’ knowledge, hierarchical clustering with SCCs has not been widely used; see  Slater (1984)  for one example. 2.3 Statistical significance for hierarchy  T Given a dendrogram  T  describing a parameterized family  { C δ } δ  of vertex clusters for a vertex-weighted graph  G , we want to evaluate whether there are clusters in  T  (corresponding to altered subnetworks in  G ) whose vertices have higher scores/weights than expected by chance. For such a test, we require a null hypothesis and a test statistic. A natural and commonly used null hypothesis is that the vertex weights are independent of network topology. However, this is not a well-formed null hypothesis as the notion of independence needs to be defined more precisely. We describe three null distributions that preserve different features of the vertex weights and/or network topology. 2.3.1 Random vertex-weighted graphs Graphs with prescribed vertex weights We define an ensemble of vertex-weighted graphs with the same vertices and edges as the observed graph  G.  Formally, we define
 (3) G w = { G ′ = ( V , E , w ′ ) : w ′ = π ( w )  for  π ∈ Π } , 
where Π is the collection of all permutations of vertex weights. We consider the uniform distribution on  G w , and we sample uniformly from  G w  by permuting the assignment of weights to vertices. Note that HotNet2 uses this permutation scheme as part of its statistical test. Graphs with prescribed vertex weights correlated with degree In many biological applications, vertex weights are correlated with vertex degree. This is often due to ascertainment bias, as high-scoring genes are frequently better studied and thus more of their interactions tend to be known and represented in networks. We define an ensemble of vertex-weighted graphs to preserve a correlation between vertex weights and degree. To preserve the exact correlation between the observed vertex weights and vertex degree, one might permute weights only among vertices with the same degree. However, in most biological networks, most degrees are unique or shared by few nodes. Thus, we partition vertices into bins according to their degree and preserve the vertex weights within each bin to define an ensemble  G w ℓ ⊆ G w . In particular, we partition  V  into bins  V 1 , …,  V k  such that (i) vertices with the same degree belong to the same bin, i.e. if  deg ⁡ ( u ) = deg ⁡ ( v ) , then  u , v ∈ V i  for some  i ; (ii) each bin has at least  ℓ  vertices; and (iii) the following quantity is minimized:
 (4) min ⁡ | V 1 | , … , | V k | ≥ ℓ ∑ i = 1 k ( max ⁡ v ∈ V i deg ⁡ ( v ) − min ⁡ v ∈ V i deg ⁡ ( v ) ) . This partition minimizes the differences in vertex degrees within bins while enforcing a minimum number  ℓ  of permuted vertex weights within each bin. Formally, we define
 (5) G w ℓ = { G ′ = ( V , E , w ′ ) : w ′ = π ( w )  for  π ∈ Π ℓ } , 
where  Π ℓ  is the collection of all degree-restricted permutations of vertex weights within the bins  V 1 , … , V k . We consider the uniform distribution on  G w ℓ , and we sample uniformly from  G w ℓ  by permuting the vertex weight assignments among the vertices in the same bin. Graphs with prescribed degree sequence We define an ensemble of vertex-weighted graphs with the same vertices, vertex weights, and vertex degree sequence  d = deg ⁡ ( G )  as the observed graph  G.  Formally, we define
 (6) G d = { G ′ = ( V , E ′ , w ) : deg ⁡ ( G ′ ) = d } . This ensemble preserves the exact correlation between vertex weight and vertex degree but alters the topology of the observed graph by permuting its edges. We consider the uniform distribution on  G d . There are various strategies for sampling uniformly from  G d , such as the double edge swap algorithm by  Milo  et al.  (2003)  and an importance sampling algorithm by  Blitzstein and Diaconis (2011) . Note that HotNet2 and NetSig use variations of this permutation scheme as parts of their statistical tests. Altogether, these random graphs models correspond to different null hypotheses regarding the relationships between vertex weights and edges. The second and third null models help to address issues of ascertainment bias; these null models may be especially important for methods that do not penalize or otherwise account for high-degree nodes.  Figure 3  illustrates an observed vertex-weighted graph and two instances of random vertex-weighted graphs. In practice, sampling from  G w  and  G w ℓ  is considerably faster than sampling from  G d . In Section 3, we use the null model  G w 1000  so that Hierarchical HotNet not only penalizes high-degree nodes in (2) but also conditions on degree in its statistical significance test. Fig. 3. Left: Observed vertex-weighted graph  G = ( V , E , w )  with vertex colors indicating vertex weights. Center: A random vertex-weighted graph  G w = ( V , E , w ′ )  with edges identical to  G  and permuted vertex weights. Right: A random vertex-weighted graph  G d = ( V , E ′ , w )  with vertex weights and degrees identical to  G  and permuted edges 2.3.2 Statistical test on dendrograms We evaluate the statistical significance of our observed dendrogram  T  by comparing it to dendrograms from a null distribution  T  of dendrograms that we obtain by sampling from  G w ,   G w ℓ  or  G d . Classic approaches to comparing dendrograms include the cophenetic distance ( Sokal and Rohlf, 1962 ) and the Fowlkes-Mallows index ( Fowlkes and Mallows, 1983 ). Both measures compare two clusterings by measuring how many objects are grouped together in both clusterings. However, in our application of altered subnetwork identification, we are typically interested in cuts of the dendrogram where most vertices are singletons, i.e. clusters of size 1. Neither of these two classic measures is appropriate in this case; therefore, we compare clusterings by comparing cluster sizes. For a dendrogram  T , we define
 (7) X δ = max ⁡ C ∈ C δ | C | 
as the size of the largest cluster  C ∈ C δ  at a similarity threshold  δ. We quantify the deviation of the observed dendrogram  T  from a null distribution  T  of dendrograms at a similarity threshold  δ  as the ratio of the observed size of the largest cluster to the expected size of the largest cluster at  δ.  Let
 (8) Y δ = X δ E [ X δ ] 
be a random variable for this ratio, where the expected value is computed over an appropriate null distribution  T  of dendrograms. Let  Y max = max δ Y δ  be the largest deviation, and let  y max  be the observed value of  Y max  on the observed dendrogram  T.  Note that while the maximum cluster size  X δ  occurs at the root of the dendrogram, the maximum ratio  Y max  typically occurs at an intermediate value of  δ  between the root and the leaves. We define
 (9) p = Pr ( Y max ≥ y max   |   T ) 
as the  P -value for the largest ratio of the observed and expected cluster sizes across all similarity thresholds  δ.  Furthermore, we specify a representative clustering  C max  for the similarity threshold
 (10) δ max = argmax δ y δ 
that achieves the maximum observed ratio  y max .  Figure 4  illustrates this statistical test. Fig. 4. Left: Dendrogram  T  with similarity threshold  δ  of maximum deviation  y max . Right: Maximum deviation  y max  between observed and expected largest cluster sizes 2.4 Consensus summarization for multiple datasets In many biological applications, one may want to combine information from multiple vertex scores and multiple interaction networks. Vertices may be weighted with different statistical measures on individual genes, and interaction networks may be defined by different interactions (e.g. physical versus genetic) with the cancer sequencing application in Section 3 as one such example. Although one may build a single consensus network and consensus scores in advance, an alternative and useful approach ( Leiserson  et al. , 2015 ) is to form a consensus of the resulting subnetworks, which can reduce network and score artifacts. We now define one such procedure. Let  G 1 , … , G t  be a collection of vertex-weighted graphs corresponding to different combinations of interaction networks and vertex scores. For each such vertex-weighted graph  G = ( V , E , w ) , we find the clusters  C δ  for a specific value of  δ , e.g.  C max , where  δ  may differ for each combination of network and scores. Let  λ G ( v ) = 1  if  v  belongs to one of the non-singleton clusters of  C δ  and  λ G ( v ) = 0  otherwise. Let  μ G ( u , v ) = 1  if both  u  and  v  belong to the same non-singleton cluster of  C δ  with  u , v ∈ E  and  μ G ( u , v ) = 0  otherwise. Given a threshold  ℓ , we define a consensus graph  G ¯ ℓ = ( V ¯ ℓ , E ¯ ℓ ) , where
 (11) V ¯ ℓ = { v ∈ V ^ : ∑ i = 1 t λ G i ( v ) ≥ ℓ } , (12) E ¯ ℓ = { ( u , v ) ∈ V ^ × V ^ : ∑ i = 1 t μ G i ( u , v ) ≥ ℓ } 
with  V ^ = ∪ i = 1 t V i . This procedure extends to directed and weighted interactions with minor changes. Many methods, including heinz, MUFFINN and NetSig, do not provide consensus procedures. HotNet2’s consensus procedure begins with ‘core’ sets of nodes found in the results for all networks and extends these sets iteratively with nodes found with fewer networks. In contrast, Hierarchical HotNet’s consensus procedure more simply considers the nodes and edges found in the results for multiple vertex-weighted graphs, further reducing artifacts by requiring more support for each prediction. 2.5 Implementation We provide an implementation of Hierarchical HotNet in Python, where parts of Hierarchical HotNet are also implemented in Fortran to provide optional but significant performance improvements. Hierarchical HotNet is highly parallelizable and can be used with single-core machines, many-core machines and compute clusters. Our code, along with examples and experiments from this article, is available online at  http://github.com/raphael-group/hierarchical-hotnet . 3 Results We apply Hierarchical HotNet and other state-of-the-art methods to the problem of identifying significantly mutated subnetworks in cancer. In this application, each gene is assigned a score according to the frequency, or statistical significance, of the somatic mutations in the gene across a cohort of tumors. Methods for identifying altered subnetworks exploit the observation that driver mutations alter a limited number of biological functions and aim to identify significantly mutated subnetworks that might include both frequently and rarely mutated genes. 3.1 Data We used two recent somatic mutation datasets and the most recent versions of several publicly available interaction networks. 3.1.1 Somatic mutation data We used gene mutation scores derived from two sources: (i) frequencies of non-synonymous somatic mutations in genes across 10 206 tumors from 33 tumor types in the The Cancer Genome Atlas (TCGA) PanCanAtlas project ( Ellrott  et al. , 2018 ;  Weinstein  et al. , 2013 ); (ii) MutSig  q -values ( Lawrence  et al. , 2014 ) from 4 742 tumors across 21 tumor types. MutSig, like other statistical driver gene prediction methods, corrects for biases in somatic mutation frequencies due to gene length, regional variation in background mutation rate etc. When computing mutation frequency scores, we removed hypermutated samples and genes such as  TTN  that are mutated in large numbers of samples but not identified as significantly mutated by MutSig.  Table 2  summarizes the resulting datasets, and the supplement describes the complete sources and processing steps for each dataset.
 Table 2. Summary of the gene scores used in the analysis Gene mutation scores Number of scored genes Number of samples Number of tumor types PanCanAtlas mutation frequency 19 057 9326 33 MutSig  q -value 18 388 4742 21 3.1.2 Interaction network data We created a HINT + HI interaction network by combining binary and co-complex interactions in HINT ( Das and Yu, 2012 ) with high-throughput derived interactions from the HI network ( Rolland  et al. , 2014 ). We also used the iRefIndex 15.0 ( Razick  et al. , 2008 ) and ReactomeFI 2016 ( Croft  et al. , 2014 ;  Fabregat  et al. , 2016 ) interaction networks. We treated each network as undirected and unweighted (only Hierarchical HotNet is able to consider both directed and weighted interactions) and analyzed the largest connected subgraph in each network.  Table 3  summarizes the resulting datasets, and the  Supplementary Material  describes the complete sources and processing steps for each dataset.
 Table 3. Summary of the interaction networks used in the analysis Interaction network Vertices Edges Density M.D. A.S.P. Diameter HINT + HI 15 074 182 088 0.00160 11 3.4 9 iRefIndex 17 136 408 688 0.00278 21 3.0 8 ReactomeFI 11 501 209 020 0.00316 13 3.4 13 M.D., median degree; A.S.P., average shortest path. The high overlap of high-degree and high-scoring genes in each combination of gene scores and interaction network (see  Table 4 ) reflects network ascertainment bias, where genes that are recurrently mutated in cancer are likely to have been studied more intensively, and thus have higher degree in interaction networks. This observation highlights the need to penalize or otherwise condition on degree for high-degree vertices.
 Table 4. Hypergeometric test  P -values for overlap between the top 1% of genes ranked by indicated gene score and the top 1% of genes ranked by degree in each interaction network Mutation frequency MutSig HINT + HI 2.8 × 10 –3 6.5 × 10 –5 iRefIndex 4.1 × 10 –6 2.3 × 10 –12 ReactomeFI 1.1 × 10 –5 9.1 × 10 –14 3.2 Comparison of network methods on mutation data We compared heinz, MUFFINN (DNmax and DNsum), NetSig, HotNet2 and Hierarchical HotNet using MutSig gene scores across three interaction networks: HINT+HI, iRefIndex and ReactomeFI interaction networks. When possible, we ran methods as prescribed in the published paper introducing the method; the  Supplementary Material  more fully describes these steps. For such a comparison, one needs ground truth or a gold standard, but there is no gold standard list of cancer genes. There are various lists of known cancer genes, such as the COSMIC Cancer Gene Census (CGC) ( Forbes  et al. , 2017 ;  Futreal  et al. , 2004 ), but these lists are incomplete. Also, while a method’s ability to recover known cancer genes is suggestive of its ability to recover novel cancer genes, it is not a guarantee. In some cases, high recall of known cancer genes may reflect a method’s preference for well-studied (high-degree) genes and provide no assurance that the method’s novel discoveries are interesting. To address these issues in our comparison, we defined two complementary lists of cancer genes. First, we defined a list of  known cancer genes  using 676 genes from Tiers 1 and 2 of the COSMIC CGC. Second, we defined a list of  candidate cancer genes  as the set of non-CGC genes with below median degree in a network and MutSig gene scores  q  &lt; 1. There are 519, 544 and 439 such candidate cancer genes in the HINT + HI, iRefIndex and ReactomeFI interaction networks, respectively. By definition, these candidate cancer genes are not known cancer genes, they are generally less studied because they have fewer known interactions, and they have some support as driver genes because they have MutSig gene scores  q  &lt; 1. We evaluated each method by computing its precision on the list of known cancer genes as well as its precision on the list of candidate cancer genes. Known and candidate cancer genes dominate the results of methods that score highly by both measures, and since these sets are necessarily disjoint, there is an inherent trade-off between them.  Figure 5  shows the performance of each method according to these complementary measures. As an additional benchmark, we also computed the measures for genes with significant ( q  &lt; 0.1) MutSig scores to show how network methods compare with gene scores alone. Fig. 5. Precision on known cancer genes ( x -axis) and on candidate cancer genes (defined in text;  y -axis) for eight different methods using MutSig gene scores across three interaction networks. Smaller markers show the precision of a method on each network, and larger markers indicate the average precision of a method across networks Hierarchical HotNet recovers larger proportions of both known and candidate cancer genes compared with other methods. All other network methods perform worse that the MutSig gene scores alone in identifying candidate cancer genes. HotNet2 identifies larger fractions of known cancer genes than MutSig but smaller fractions of candidate cancer genes. Like Hierarchical HotNet, HotNet2 also consider topology and scores jointly and penalizes high-degree genes. However, HotNet2 is less robust to complex network topology than Hierarchical HotNet and therefore less able to predict novel cancer genes, which we further describe in the following section. heinz finds a slightly larger proportion of known cancer genes than MutSig but a much smaller fraction of candidate cancer genes. Its connectivity constraint provides modest improvements for known cancer genes by removing a few high-scoring genes that do not interact with other high-scoring genes, but this constraint introduces a bias toward high-degree nodes (see  Table 5 ) that reduces its ability to identify less studied genes.
 Table 5 Median degrees of vertices identified by each method on different interaction networks, with methods sorted from smallest to largest degree Methods HINT +  HI degree iRefIndex degree ReactomeFI degree NetSig 27 54 46 Hierarchical HotNet 33.5 106 48 MutSig 29 95 90 HotNet2 32 92 95.5 heinz 40.5 118 104.5 DNmax 56 154 121 DNsum 53 170 124 Degree-biased MutSig 54 175 128 MUFFINN identifies similar fractions of known cancer genes as MutSig but smaller numbers of candidate cancer genes, demonstrating an even stronger preference for high-degree nodes. MUFFINN’s DNmax and DNsum scores take the maximum and sum, respectively, of the scores of their neighbors, favoring genes with large network neighborhoods. We created a degree-biased version of MutSig (see the  Supplementary Material  for details) that reprioritizes genes using a weighted combination of MutSig score and degree, which modeled network ascertainment bias to recover more known cancer genes at the expense of candidate cancer genes. As a result, it has similar performance to both heinz and MUFFINN. NetSig finds smaller proportions of known and candidate cancer genes than MutSig. The NetSig statistic intentionally omits a gene’s score when evaluating its network neighborhood, thus reducing NetSig’s recovery of known cancer genes with high MutSig scores. However, we found that most NetSig genes (380/463 distinct genes) have MutSig scores of  q  = 1, thus having little support for their recurrent mutation across the cohort as candidate cancer genes. Also, while NetSig’s statistical test attempts to condition on vertex degree, we find that NetSig reports several high-degree nodes, such as  UBC , that are less likely to be cancer genes. 3.3 Hierarchical HotNet hierarchies and consensus results Beyond predicting known and candidate cancer genes, Hierarchical HotNet also builds hierarchies of significantly mutated subnetworks and constructs consensus subnetworks across multiple network topologies and gene scores. This section describes its results on three distinct interaction networks (HINT + HI, iRefIndex and ReactomeFI) and two distinct sets of genes scores (PanCanAtlas mutation frequency scores and MutSig  q -value scores). Hierarchical HotNet’s results were statistically significant ( P  &lt; 0.01) on each of the six resulting vertex-weighted graphs. Hierarchical HotNet identified gene sets that had strong overlap with biological pathways that are implicated in cancer. For example,  Figure 6  shows part of the Hierarchical HotNet hierarchy for the ReactomeFI interaction network and TCGA PanCanAtlas mutation frequency scores. This part of the Hierarchical HotNet hierarchy overlaps well with the Notch signaling pathway, containing the Notch genes  NOTCH1 ,  NOTCH2 ,  NOTCH3 ,  NOTCH4 ,  JAG1 ,  JAG2  and  DLL3  as well as the COSMIC CGC genes  CPEB3  and  RB1  that interact with Notch pathway mutations. Fig. 6. Branches of the Hierarchical HotNet hierarchy for the ReactomeFI interaction network and TCGA PanCanAtlas mutation frequency gene scores, where Notch signaling pathway and COSMIC CGC genes are indicated (branch heights roughly to scale) Across these interaction networks and gene scores, Hierarchical HotNet identified the consensus subnetworks  G ¯ 2  (see Section 2.4) with 128 genes and 223 interactions supported by multiple significantly mutated subnetworks within their respective hierarchies. See the  Supplementary Material  for a full list of Hierarchical HotNet consensus genes, biological process and pathway annotations of the Hierarchical HotNet genes and a network view of the Hierarchical HotNet consensus subnetworks. Many of the Hierarchical HotNet consensus results are known cancer genes, and the Hierarchical HotNet consensus subnetworks contain parts of many canonical cancer pathways, including the Notch ( Rizzo  et al. , 2008 ), p53 ( Vazquez  et al. , 2008 ), PI(3)K ( Liu  et al. , 2009 ;  Yuan and Cantley, 2008 ), Ras/Raf ( Roberts and Der, 2007 ) and Rb ( Nevins, 2001 ) signaling pathways. They also contains gene sets with more recent implications in cancer, including parts of the BAP1 ( Carbone  et al. , 2013 ), CBFB ( Banerji  et al. , 2012 ) and SWI/SNF ( Wiegand  et al. , 2010 ) complexes. Moreover, the Hierarchical HotNet consensus results contain novel predictions that may be of interest for further study, including the possible tumor suppressors  RASA1  and  SERPINB5.  Hierarchical HotNet also identifies a subnetwork of protein kinase D1 genes ( PKD1  and  PKD2 ) with known roles in cell proliferation and other cancer hallmarks ( Sundram  et al. , 2011 ). An additional subnetwork includes several putative cancer genes that interact but do not otherwise have a clear relationship to one another:  MERTK  is a receptor tyrosine kinase that may activate several downstream oncogenic pathways ( Cummings  et al. , 2013 ),  VWF  has recently been implicated in angiogenesis and apoptosis ( Franchini  et al. , 2013 ) and  PROS1  may be a marker for aggressive prostate cancer ( Saraon  et al. , 2012 ). Since HotNet2 also provides consensus results, we compared Hierarchical HotNet’s consensus subnetworks with HotNet2’s consensus subnetworks on the same data. Although Hierarchical HotNet results were statistically significant ( P  &lt; 0.01) on each dataset, HotNet2 had statistically insignificant ( P  ≥ 0.05) results for mutation frequency scores on the iRefIndex or ReactomeFI networks. For the rest of the section, we compare the recall and precision of these methods for known cancer genes because a consensus comparison of candidate cancer genes is more complicated as these sets differ across networks and gene scores. 
 Figure 7  summarizes the overlap of the Hierarchical HotNet and HotNet2 consensus results with COSMIC CGC gene list. Hierarchical HotNet and HotNet2 recover 37 of the same genes, including 30 COSMIC CGC genes. However, Hierarchical HotNet identifies 91 genes that HotNet2 does not, including 27 CGC genes. These 27 genes include the well known cancer genes  APC ,  CTNBB1 ,  PTEN ,  TP53  and  VHL.  Each of these genes is significantly and recurrently mutated, but these well studied genes tend to have more reported interactions in more recent interaction networks, and the larger and denser networks in this analysis frustrates HotNet2’s less robust subnetwork selection procedure. Conversely, HotNet2 predicts 52 genes that Hierarchical HotNet does not, including three CGC genes:  KEAP1 ,  NFE2L2  and  RSPO3.  Hierarchical HotNet’s more aggressive consensus procedure eliminates the interacting partners  KEAP1  and  NFE2L2 , but  RSPO3  is absent from both the HINT + HI and ReactomeFI networks, so its loss in Hierarchical HotNet is more predictable. Fig. 7. Venn diagram summarizing the overlap of HotNet2 and Hierarchical HotNet consensus results with the COSMIC CGC genes Overall, Hierarchical HotNet identifies more COSMIC CGC genes than HotNet2 (57 versus 33, respectively), giving it nearly twice the recall (0.087 versus 0.051) with higher precision (0.445 versus 0.371, respectively) than HotNet2 (see  Fig. 7  and  Table 6 ). Hierarchical HotNet’s higher recall and precision are primarily attributable to Hierarchical HotNet’s ability to identify significantly mutated subnetworks across biological scales in its hierarchy of subnetworks and more aggressive consensus procedure.
 Table 6. Overlap of HotNet2 and Hierarchical HotNet consensus results with the COSMIC CGC genes Consensus results Total genes CGC genes CGC recall CGC precision HotNet2 89 33 0.051 0.371 Hierarchical HotNet 128 57 0.087 0.445 In practice, Hierarchical HotNet is more flexible, more robust and less computationally intensive (CPU time, memory usage and storage space) than HotNet2. For these results, the entire HotNet2 pipeline required a few days of compute time on a modern, many-core machine while the entire Hierarchical HotNet pipeline required a few hours. 4 Discussion In this article, we introduce a novel computational method, Hierarchical HotNet that simultaneously combines network interactions and vertex scores to construct a hierarchy of high-weight, topologically close subnetworks. Applied to cancer genomics data, Hierarchical HotNet builds a hierarchy of highly mutated subnetworks in a PPI network. Hierarchical HotNet outperforms several state-of-the-art computational methods on recent datasets in terms of recovering known and candidate cancer genes, addressing several important issues in current network-based methods for identifying altered subnetworks, including ascertainment bias (i.e. bias toward high-degree nodes) and statistical significance testing. In particular, Hierarchical HotNet is a simpler but more robust method than our earlier HotNet2 algorithm (see  Supplementary Material  for a more detailed comparison), removing parameters, reducing computational costs and improving predictions. Applied to multiple interaction networks and cancer mutation datasets, Hierarchical HotNet identifies many known and putative cancer genes. Further testing of these novel predictions may provide additional insight into cancer biology. There are multiple avenues for extending Hierarchical HotNet. Hierarchical HotNet can be understood as part of general framework for identifying clusters of high-weight, topologically close vertices. As such, Hierarchical HotNet is a modular method; different similarity measures, clustering algorithms or test statistics may be more appropriate for different datasets, and each of these parts can be changed as the application demands or curiosity dictates. This modularity is strength of Hierarchical HotNet that should allow it to be applied broadly. It is also possible to adapt Hierarchical HotNet to use categorical or vector-valued attributes instead of scalar-valued weights. For example, one could define a new mutation similarity matrix using the statistical significance of mutually exclusive or co-occurring mutations between pairs of genes.  Leiserson  et al. , (2016 ) use this similarity matrix either directly with Hierarchical HotNet or after combining it with the topological similarity matrix (1) to create a new joint similarity matrix. There is also a larger question about similarity measures on vertex-weighted graphs and what properties such measures should have. For the example, there is an interplay between the contributions from network topology and vertex weights, and an ideal method would attempt to quantify or balance these contributions to the discovery of a method’s results. Hierarchical HotNet acknowledges this interplay, and its framework provides opportunities to ascertain, e.g. if either network topology or vertex weights dominate the Hierarchical HotNet results. Additional study of similarity measures on vertex-weighted graphs would be useful in the design of new methods for altered subnetwork discovery. Funding This work is supported by a US National Science Foundation (NSF) CAREER Award [CCF-1053753] and US National Institutes of Health (NIH) grants [R01HG007069 and U24CA211000 to B.J.R.]. The results shown here are in part based upon data generated by the TCGA Research Network  http://cancergenome.nih.gov/  as outlined in the TCGA publications guidelines. 
 Conflict of Interest : B.J.R. is a co-founder and board member of Medley Genomics and M.R. is a contractor for Medley Genomics. Supplementary Material Supplementary Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Rhapsody: predicting the pathogenicity of human missense variants</Title>
    <Doi>10.1093/bioinformatics/btaa127</Doi>
    <Authors>Ponzoni Luca, Peñaherrera Daniel A, Oltvai Zoltán N, Bahar Ivet, Ponty Yann</Authors>
    <Abstract/>
    <Body>1 Introduction Single-nucleotide polymorphisms (SNPs) are single DNA base pair changes that are inherited (germline variants) or occur during the organism’s lifetime (somatic variants). A SNP located in a coding region of the DNA may lead to the translation of the gene codon into a different amino acid than the wild-type (non-synonymous SNPs), giving rise to a single amino acid variant (SAV or missense variant). Both synonymous and non-synonymous SNPs can perturb the normal activity of a cell. For example, synonymous SNPs can affect splicing, regulatory mechanisms and gene and/or protein expression levels although they do not affect the encoded protein’s sequence. SAVs can additionally have molecular effects, e.g. by altering a protein’s orthosteric or allosteric sites, its interaction with substrates or its stability. More than half of the mutations implicated in human inherited diseases are estimated to be associated with SAVs ( Stenson  et al. , 2017 ). As a result, devising analytical and computational approaches for predicting their effect has been of broad interest, but equally challenging due to complex effects in the cell. In recent years, it became evident that comprehensive approaches integrating multiple perspectives are the only viable solutions to achieve higher accuracy in pathogenicity predictions and to interpret experimental data at the molecular level. In the case of SAVs, this means understanding not only the significance of the mutated amino acid vis-à-vis the biological function of the protein, often captured by sequence-based conservation models, but also its importance for the fold stability and conformational mechanics and interactions, both intra- and intermolecular ( Ancien  et al. , 2018 ). Significant progress has been made in tools that focus on protein sequence conservation and residue coevolution, such as context-dependent modeling of sequence evolution ( Feinauer and Weigt, 2017 ;  Hopf  et al. , 2017 ) in recent years. In contrast, structure-based modeling approaches have been lagging behind compared to sequence-based approaches in evaluating the effect of SAVs, even though the first-generation classifiers that take account of 3D structures have shown considerable success ( Adzhubei  et al. , 2010 ;  Ancien  et al. , 2018 ;  Capriotti and Altman, 2011 ). The importance of considering structure, or solvent accessibility, especially when relatively few homologs are available, has been pointed out in early studies ( Saunders and Baker, 2002 ) and in more recent works based on residue network analysis ( Brown  et al. , 2017 ;  Brown and Tastan Bishop, 2017 ). This class of computations has been limited by two factors: first, they are possible only when the 3D structure of the protein is known, either from experiments or from comparative modeling. Second, even when a structure is available, the traditional methods to investigate the effect of missense variants such as molecular dynamics (MD) simulations require expensive computations which do not lend themselves to genome-scale analyses. While MD studies have shown success in predicting the impact of SAVs ( Abdul Samad  et al. , 2016 ;  Kumar and Purohit, 2014 ;  LaRusch  et al. , 2014 ;  Parveen  et al. , 2019 ;  Priya Doss  et al. , 2014 ), they are applicable on a case-by-case basis only and are limited by the time and space limitations of MD simulations. Yet, recent years have seen a rapid growth in the structural characterization of the proteome with advances in structure determination (e.g. cryo-EM) technologies. In parallel, computationally efficient methods such as those based on elastic network models (ENMs) have been developed, which efficiently provide insights into the intrinsic dynamics of proteins uniquely defined by their inter-residue contact topology ( Bahar  et al. , 2010 ;  Li  et al. , 2017 ). Many analytical tools have been developed within the framework of ENMs, which focus on different aspects of protein equilibrium dynamics, both on a local (e.g. fluctuations in residue positions) and a global (e.g. coupled domain movements and allosteric switches) scale. ENMs are broadly used for mechanistic studies, but their utility in genome-scale studies of the impact of mutations is becoming clear only in recent studies ( Ponzoni and Bahar, 2018 ;  Rodrigues  et al. , 2018 ). The rapidly growing experimental data on the functional impact of SAVs and on protein structures provide a unique opportunity for building upon that first generation of pathogenicity predictors to develop a machine learning approach trained not only on well-established sequence- and structure-dependent properties, but also on  intrinsic dynamics , derived from ENMs. A first attempt in that direction ( Ponzoni and Bahar, 2018 ) paved the way to the current development and implementation of  Rhapsody , an advanced tool and user-friendly server for  Rapid High-Accuracy Prediction of SAV Outcome based on DYnamics , accessible at  http://rhapsody.csb.pitt.edu . The inclusion of dynamics-based features distinguishes Rhapsody from tools broadly used in the field such as PolyPhen-2 ( Adzhubei  et al. , 2010 ), SIFT ( Ng and Henikoff, 2003 ), CADD ( Kircher  et al. , 2014 ) and others [see  Grimm  et al.  (2015)  for a critical review of some of these methods and  Hu  et al.  (2019)  for an updated list of tools]. We presently introduce a ‘full’ version of Rhapsody that incorporates coevolution features extracted from Pfam domains, inspired by the success of recent studies ( Feinauer and Weigt, 2017 ;  Hopf  et al. , 2017 ). We provide extensive comparisons of Rhapsody against PolyPhen-2 ( Adzhubei  et al. , 2010 ) and EVmutation ( Hopf  et al. , 2017 ), utilizing a refined dataset of about 20 000 human SAVs, built from consensual clinical interpretations between multiple databases (DBs). PolyPhen-2 is a broadly used tool for predicting the functional effects of human variants, which relies on a supervised naïve Bayes classifier trained on annotations, conservation scores and structural features that characterize the amino acid substitution. It is chosen here as a representative tool among several other publicly available methods because of its widespread use. EVmutation, on the other hand, emerges as one of the most accessible and powerful tools among the recent wave of tools that leverage coevolution analysis for predicting the fitness of mutants, going beyond the limitations of conservation analyses by taking account of the inter-dependencies between pairs of sequence positions. The change in ‘evolutionary statistical energy’ ΔE incurred upon mutation is directly interpreted as a proxy for the mutant fitness. However, a cutoff energy for binary classification of mutants as deleterious or neutral is not defined. Rhapsody is implemented as a standalone package, which may be used in conjunction with our ProDy API ( Bakan  et al. , 2011 ). The server offers the option of using as input customized Protein Data Bank (PDB) structures, such as those stabilized under different conformational and oligomerization states as well as those resolved for orthologues or generated by comparative modeling. We illustrate the utility of Rhapsody by way of applications to human H-Ras, a highly conserved G-protein belonging to Ras subfamily of small GTPases for which deep mutational scanning data have been recently reported ( Bandaru  et al. , 2017 ), and to two human proteins featured in a recent Critical Assessment of Genome Interpretation (CAGI) competition ( Andreoletti  et al. , 2019 ): PIP3 phosphatase, also called phosphatase and tensin homolog (PTEN) and thiopurine S-methyltransferase (TPMT). The new tool provides not only an efficient independent assessment of potential pathogenic effect of mutations, but also mechanistic insights into the molecular basis of the observed and/or predicted effects. 2 Materials and methods 2.1 Development of an upgraded dynamics-based pathogenicity predictor Three groups of features, sequence-based (SEQ), structure-based (STR) and dynamics-based (DYN), computed for each position along the sequence and/or specific amino acid substitution (e.g. ‘P01112 10 G A’ in UniProt coordinates, indicating variant G10A of GTPase H-Ras), are used for training a random forest classifier, following the approach described in our earlier work ( Ponzoni and Bahar, 2018 ). In the original version of the algorithm, SEQ features were computed by the PolyPhen-2 server ( Adzhubei  et al. , 2010 ), STR features by using structural data from the PDB and DYN features by the ProDy API ( Bakan  et al. , 2011 ). This classifier proved to achieve accuracy levels comparable to, if not better, than 11 existing tools ( Ponzoni and Bahar, 2018 ). In this study, we introduce two upgraded versions, referred to as ‘reduced’ and ‘full’ Rhapsody classifiers.  Supplementary Table S1  provides a detailed list of the features used in both versions along with their definition and interpretation. The reduced version includes BLOSUM62 amino acid substitution scores ( Henikoff and Henikoff, 1992 ) as an additional feature and upgraded DYN features calculations ( Fig. 1A and B ). The full Rhapsody classifier uses as additional features the mutation site entropy and coevolution properties deduced from Pfam domains ( El-Gebali  et al. , 2019 ). Fig. 1. Rhapsody features and prediction accuracy. ( A ) Random forest features used in Rhapsody classifiers. See  Supplementary Table S1  for detailed descriptions. ( B ) Comparison of the accuracy of three Rhapsody classification schemes of different complexities and coverage (full, reduced and combined with EVmutation, shown by the three sets of bars on the left) with that of two other tools, PolyPhen-2 and EVmutation, measured by area under the ROC plot (AUROC) values (and relative error bars from 10-fold cross-validation) obtained using OPTIDS ( Supplementary Table S2 ). As SAVs from the same residue or protein could be found in both training and testing subsets (gray bars), we repeated the Rhapsody computations for residue- (orange) and protein-stratified (blue) versions of our dataset to ensure unbiased evaluations. Light green bars in the background show the relative size of the datasets of variants (right ordinate) used for 10-fold cross-validations of Rhapsody and for testing the other two methods. See also  Supplementary Figures S1 and S2  for further comparison of these methods using additional metrics and for comparisons with outputs from other tools, and  Supplementary Materials and Methods  for more details. ( C ) Effect of excluding variants of various confidence levels (based on ClinVar DB review rates/stars) from the training dataset. Light green bars represent the numbers of SAVs (right ordinate) that could be processed by Rhapsody’s full classifier, for different subsets. The leftmost bar refers to the complete IDS (with PDB structures larger than 150 residues); the second bar excludes those with 0-star; the third excludes those with 0- and 1-stars and so on. The blue curve (left ordinate) displays the prediction accuracy levels with error bars computed through cross-validations We also designed a new interface ( http://rhapsody.csb.pitt.edu ) that enables efficient use of the algorithm and visualization of its output. A detailed description of random forest features and hyper-parameter optimization, Python package implementation and interface design is presented in  Supplementary Materials and Methods . 2.2 Construction of an integrated dataset of annotated human variants The dataset for training the algorithm has been generated by combining five publicly available datasets [HumVar ( Adzhubei  et al. , 2010 ), ExoVar ( Li  et al. , 2013 ), PredictSNP ( Bendl  et al. , 2014 ), VariBench ( Thusberg  et al. , 2011 ) and SwissVar ( Mottaz  et al. , 2010 )] with the Humsavar DB of all human missense variants annotated in the UniProtKB/Swiss-Prot DB and the ClinVar archive of reports on the level of concordance between human variations and phenotypes ( Landrum  et al. , 2016 ).  Supplementary Table S2  provides information on the content of these datasets and their level of agreement. After filtering out discordant labels, we obtained an ‘Integrated Dataset’ (IDS) of 87 726 SAVs, of which 27 655 could be mapped onto PDB structures, a prerequisite for computing STR/DYN features, and 23 085 had PDB structures with at least 150 residues. The ClinVar DB provides a reliability level for each variant, with the help of zero (weak) to four (best) ‘review stars’ assigned to each SAV, based on the number of, and consensus between, various sources. Variants with ‘no assertion’ or ‘no assertion criteria provided’ are assigned 0-star; those characterized by ‘single submitter’ or ‘conflicting interpretations’ are assigned 1-star; a 2-star assignment refers to ‘no conflicts and multiple submitters’; 3-star, to ‘reviewed by experts’ and 4-star, to ‘practice guideline’. As will be shown in Section 3, removal of the 0-star cases led to improved prediction accuracy. The final, optimized integrated dataset (OPTIDS) after eliminating these low-confidence cases contains 20 361 SAVs with at least 1 ClinVar review star, mapped onto 2828 unique chains in the PDB, each containing at least 150 residues. 3 Results 3.1 Cross-validation and comparison with other tools In a preliminary analysis ( Fig. 1C ), we monitored the average area under the ROC curve (AUROC) attained by the full classifier in a 10-fold cross-validation procedure while gradually excluding from the IDS those SAVs with lower ClinVar rating. The exclusion of SAVs with 0-stars helped improve the accuracy ( blue curve  in  Fig. 1C ). This was followed by a plateau or minimal decrease in accuracy when further excluding 1-, 2-, 3- and 4-star SAVs. These additional changes were within the error bars computed from 10 cross-validation iterations, so we opted to exclude SAVs with 0-stars only, which accounted for ∼12% of cases, from our training dataset in all subsequent analyses. This OPTIDS was used for evaluating the accuracy of the classifier through cross-validation. In  Figure 1B , we compare the performances of three variants of Rhapsody against PolyPhen-2 ( Adzhubei  et al. , 2010 ) and EVmutation ( Hopf  et al. , 2017 ). The  colored bars  represent accuracy measurements for each method’s predictions. For the three Rhapsody variants on the left, we calculated the average AUROC and associated SDs from a 10-fold cross-validation on OPTIDS, while for PolyPhen-2 and EVmutation, we plotted the AUROC values over the same dataset of variants. The  light green bars  in the background indicate the actual number of SAVs that could be evaluated by each approach. The cross-validation for Rhapsody classifiers has been carried out through random partitioning of OPTIDS, stratified by mutation classes to ensure equivalent bias in each fold ( gray bars  in  Fig. 1B ). Additional low-redundancy measurements have been performed as more stringent tests, by removing the variants of the same residue (‘residue-stratification’,  orange bars ) or within the same protein (‘protein-stratification’,  blue bars ) from the training subsets. Each of these steps resulted in lower estimates of accuracy, by up to ∼0.03. We notice that the full Rhapsody classifier outperforms both PolyPhen-2 and EVmutation, based on AUROC values. Similar conclusions could be drawn by evaluating the performance of these methods with other metrics, such as the Matthews correlation coefficient (MCC) and F1-score, as presented in  Supplementary Figure S1 . The latter are known to be less affected by high class imbalance (bias toward deleterious mutations in OPTIDS), and therefore may provide a better estimate of accuracy. Note that about 70% of our training dataset consists of deleterious variants while an opposite composition bias is observed in naturally-occurring human variants ( Lek  et al. , 2016 ). To mitigate the effect of such imbalances, the random forest models have been trained by assigning to training examples weights inversely proportional to class frequency. The full Rhapsody classifier is also seen to outperform the reduced version, although within the error margins defined by the metrics’ SD. Further comparison with the original version introduced in 2018 ( Ponzoni and Bahar, 2018 ), presented in  Figure 2C , shows the statistically significant improvement achieved in the full version, using two different ENMs, the Gaussian Network Model (GNM) ( Li  et al. , 2016 ) and the Anisotropic Network Model (ANM) ( Eyal  et al. , 2015 ), for evaluating DYN properties. However, the introduction of Pfam-derived features in the full classifier comes at the cost of a slight decrease in coverage, since Pfam domains often do not encompass the full span of a protein sequence, but only those portions that are preserved across species. In this regard, PolyPhen-2 has the widest coverage, being able to return a prediction even for variants without a PDB structure. Fig. 2. Analysis of the Rhapsody classifier. ( A ) Weights of features in the Rhapsody classifier integrated with EVmutation. See also  Supplementary Figure S4 . ( B ) Spearman’s correlations between all pairs of features. ( C ) Accuracy of the full Rhapsody algorithm (repeated using either GNM- or ANM-predicted DYN features, with and without environmental effects) on different subsets of the OPTIDS obtained by setting a minimum PDB structure size (i.e. number  N  of resolved residues). In yellow, we show the performance of the original algorithm ( Ponzoni and Bahar, 2018 ). Error bars represent the SD computed during cross-validations. See also  Supplementary Figure S5  for similar results with other accuracy metrics. ( D ) Training dataset size (SAVs successfully processed by ‘full’ classifier, in blue) and fraction of positive training examples (i.e. deleterious SAVs, in red) as a function of the minimum number of residues used to filter PDB structures based on their size In addition to the full and reduced versions of Rhapsody, we also considered a third option, designated as ‘Rhapsody + EVmut’, which incorporated the EVmutation ‘epistatic’ score ΔE within the feature set. This variant slightly improved upon the full classifier, but it also further reduced the coverage. Of note, the integration of EVmutation and Rhapsody leads to significantly more accurate predictions than EVmutation used alone. In the above comparative evaluations, we note that PolyPhen-2’s training dataset partially overlaps with OPTIDS, as discussed earlier ( Ponzoni and Bahar, 2018 ), which may lead to an overestimation of the accuracy of PolyPhen-2 ( Grimm  et al. , 2015 ). More generally, it is not always possible nor feasible to account for such ‘training biases’, unless a completely novel and independent testing dataset is designed. In order to facilitate future assessments, the output from our algorithm explicitly acknowledges whenever a tested variant is also listed in the training dataset. We presented in  Supplementary Figure S2  an additional comparison of the outputs from Rhapsody with those from 27 other tools currently compiled in dbNSFP, a DB of functional predictions and annotations for all potential non-synonymous single-nucleotide variants in the human genome ( Liu  et al. , 2011 ,  2016 ). Yet, the same type of training bias may also hold for the precomputed outputs in dbNSFP which may preclude an objective assessment, even though an exhaustive list of metrics has been considered therein. The large discrepancies in the accuracy levels for individual classes [neutral and deleterious SAVs, indicated by suffixes ‘(0)’ and ‘(1)’, respectively] observed for all methods reflects the imbalance of the dataset and the challenges associated with it. Finally, we carried out an additional benchmarking study against predictions from SNPs3D ( Yue  et al. , 2006 ). The latter is notable among pathogenicity prediction tools because it evaluates the functional consequences of a SAV by assessing its impact on structural  stability , in addition to identifying candidate genes for specific diseases and providing information on the relationships between these candidates. For this comparison, a new classifier was trained. A relatively small subset of variants in our OPTIDS was chosen as a test set, given the availability of precomputed predictions from SNPs3D, and the proteins containing those variants were excluded from the training set. The results presented in  Supplementary Figure S3  show equal or better performance of Rhapsody in general over SNPs3D using a broad range of metrics, even on this particularly challenging (imbalanced) test set that included a small proportion of deleterious SAVs, strongly departing from the composition of OPTIDS. Overall, these results confirm the usefulness of including intrinsic dynamics features in the context of functional assessment of variants, and further demonstrate the power of adopting an integrative approach that incorporates coevolution analysis into supervised learning approaches, thus taking advantage of its superior predictive power compared to single amino acid conservation properties. 3.2 Contribution of selected features 
 Figure 2A  illustrates the relative weights of the features used in the integrated classifier 'Rhapsody + EVmut'. The counterparts for the 'full' and 'reduced' Rhapsody classifiers can be seen in the  Supplementary Figure S4 . In parallel with previous observations ( Ponzoni and Bahar, 2018 ), sequence-based features (wtPSIC, ΔPSIC and entropy of Pfam domain) rank higher than dynamics-based (ENM-derived) features, since the latter lack residue specificity. Dynamics-based features, in turn, prove to be more informative than a widely used structural property, solvent accessibility. We note that these features are not necessarily independent. The heat map in  Figure 2B  provides a quantitative description of their similarities. Yet, their explicit inclusion in the training algorithm assists in increasing prediction accuracies. We note, in this context, the remarkable weight difference between two coevolution properties, the ‘ranked’ mutual information (MI) and EVmutation’s ΔE score. The former was chosen for its simplicity, which makes it orders of magnitude faster to evaluate computationally than EVmutation scores, for which a DB of precomputed values was used in practice ( Hopf  et al. , 2017 ). For real-time evaluation of coevolution properties, the integration of more efficient coevolution algorithms might be envisioned. 3.3 Higher accuracy achieved with larger structures 
 Figure 2C  illustrates the dependency of pathogenicity prediction accuracy on the minimum size of the PDB structure included in the evaluation of the STR and DYN features. More detailed results with different metrics are presented in the  Supplementary Figure S5 . A slight improvement in accuracy is observed when excluding structures with fewer than  N  =   150 residues, and again when limiting the analysis to structures with at least 500 residues. Examination of the dependency of feature weights on protein size illustrated in  Supplementary Figure S7  indicated that the observed pattern did not originate from differences in feature weights which remained relatively constant in the range  N  &lt;   300. The increased accuracy upon exclusion of small ( N  &lt;   150) structures could be attributed to the fact that sequence/structure data in this range might be incomplete and not representative of the intact protein. Conversely, the relatively high accuracy in the range  N  &gt;   500 could reflect the more complete inclusion of physical and evolutionary interactions between sequentially distal but spatially close neighbors in the multi-domain or multi-subunit proteins. The existence of a direct correlation between prediction accuracy and size of PDB structures, if any, is blurred by the concurrent changes in the training dataset size and composition (blue and red curves, respectively, in  Figure 2D ). The non-monotonic behavior of the AUROC plot in  Figure 2C  could thus be attributed to the changing imbalance between deleterious and neutral variants in the training dataset at different PDB size cutoffs. Such non-uniform distributions are also viewed in the breakdown of the IDS population and imbalance at various PDB chain length intervals in  Supplementary Figure S6 . However, the pattern observed in  Figure 2C  is robustly displayed by other metrics that are less susceptible to dataset imbalance, namely MCC and F1-score ( Supplementary Fig. S5 ). Thus, we deemed it safe to use the SAVs with  N  &gt;   150 for training purposes. 3.4 Application to H-Ras 3.4.1 Saturation mutagenesis analysis of human H-Ras protein Kuriyan and coworkers recently presented results from deep mutational scanning of human H-Ras ( Bandaru  et al. , 2017 ), a highly conserved signaling protein which transduces signals through a nucleotide-dependent switch between active (GTP-bound) and inactive (GDP-bound) conformations. The impact of a single mutation on the protein’s normal activity was experimentally linked to the survival of the hosting bacterial system and quantified by a ‘fitness score’ (ΔE), under different contexts. Here, we focus on the complete (‘regulated Ras’) experimental setup, designed to include regulatory factors that might constrain Ras sequence variability and that are necessary to obtain a realistic assessment of mutants’ fitness. 
 Figure 3  presents the results from our so-called ‘ in silico  saturation mutagenesis’ analysis. The results are presented in a 20 x  N  heat map ( Figure 3A ) where the entries are color-coded by pathogenicity probabilities ( Supplementary Materials and Methods ) predicted for all 19 possible substitutions at each of the  N  = 171 structurally resolved sequence positions of H-Ras (UniProt sequence ID: P01112). The entries corresponding to the wild-type amino acids are in white. The map structure mirrors that of analogous maps of experimental fitness measurements ( Bandaru  et al ., 2017 ). The structure-dependent (STR) and dynamics-based (DYN) features required by Rhapsody were computed on the active, GTP-bound conformation of H-Ras (PDB ID: 6Q21, chain A). Computations repeated for the inactive state (PDB ID: 4Q21, chain A) showed that the predictions were very similar ( Supplementary Figs. S8 and S9 ), with the main differences localized at the switches I and II ( Fig. 3B ). These results are consistent with the robustness of ENM results to structural details, i.e. H-Ras structural dynamics is predominantly defined by its 3D fold, which defines its inter-residue contact topology. The contact topology, in turn, determines the intrinsically accessible spectrum of motions. The impact of SAVs on collective mechanics can thus be inferred from either active or inactive state, provided that the overall fold remains unchanged. Fig. 3. 
 In silico  saturation mutagenesis results for human H-Ras. ( A ) The predicted pathogenicity probabilities for all possible SAVs in H-Ras computed by Rhapsody are shown as a heatmap with a color code ranging from red (deleterious) to blue (neutral); see  Supplementary Materials and Methods  for more details on the definition of pathogenicity probability. The corresponding residue-averaged pathogenicity profile is shown in red in the bottom panel, compared to analogous profiles from PolyPhen-2 (blue) and EVmutation (green) and from experimental fitness measures (grey). The two strips along the upper abscissa of the heatmaps display the secondary structure and solvent accessibility (SASA) along the sequence. The Rhapsody results are obtained for the structure in the active state. The counterpart for the inactive state is presented in  Supplementary Fig. S8 . ( B ) Residue pathogenicities displayed by color-coded ribbon diagrams for active (top) and inactive (middle) H-Ras. Red and blue colors indicate the regions with high and low propensities for pathogenicity, respectively. The difference is shown in the bottom panel. The respective purple and green regions refer to sites exhibiting increased and decreased pathogenicities in the active form. The purple regions include the two switches involved in activation At first glance, the heat maps in  Figure 3A  show an alternating pattern of  blue  (neutral) and  red  (pathogenic) vertical bands that loosely correlate with either secondary structure or surface exposure of residues ( top strips ). Such a pattern can also be discerned in the bottom panels of  Figure 3A . The  red curve  therein shows the  residue-based pathogenicity profile  predicted by Rhapsody upon averaging the entries in the corresponding column of the map. Analogous profiles obtained using PolyPhen-2 ( blue ), EVmutation ( green ) and experimental fitness scores for ‘regulated-Ras’ ( Bandaru  et al. , 2017 ) (–ΔE,  gray ) reveal an overall agreement between computations and experiments. Rhapsody performs better than EVmutation and PolyPhen-2 when comparing the predicted residue-averaged pathogenicities with experimental data, as can be seen in  Supplementary Table S3 . The table lists the Spearman’s rank-order correlations, |ρ|, between experimental and (different types of) computational data. For the ‘regulated’ case ( Fig. 4A  and  Supplementary Fig. S10 ), |ρ| = 0.60 and 0.57 for Rhapsody predictions based on the inactive and active states, respectively, as opposed to |ρ| = 0.52 and 0.51 for EVmutation and PolyPhen-2. Both Rhapsody and EVmutation outperform PolyPhen-2 in predicting individual fitness scores (|ρ| ≈ 0.42 versus 0.36). We also estimated the prediction accuracies using AUROC and AUPRC as metrics. These required a binary labeling of variants (neutral/pathogenic) that cannot be readily deduced from the distribution of experimental ΔE values, see  Supplementary Figure S11 . We arbitrarily set the median of the distribution as a cutoff, while the 40th and 60th percentiles have been used to compute an uncertainty interval (an alternative labeling scheme and relative metrics calculations are shown in  Supplementary Figs. S14 and S15 ). The resulting ROC curves ( Fig. 4B  and  Supplementary Fig. S12 ) confirm similar accuracy levels for Rhapsody and EVmutation, with respect to both individual (AUC) and residue-averaged («AUC» res ) experimental data, and slightly lower accuracies for PolyPhen-2. Analogous conclusions emerge from the analysis of Precision–Recall curves, presented in  Figure 5A  and  Supplementary Figure S13 . Fig. 4. Pathogenicity predictions of human Ras protein variants. ( A ) Scatter plots and Spearman’s ρ correlations between experimental fitness scores from ( Bandaru  et al. , 2017 ) and predictions from Rhapsody (based on inactive/active conformations), EVmutation and PolyPhen-2. Red circles correspond to residue-averaged values. See also  Supplementary Figure S10 . ( B ) ROC curves for substitution-specific (blue) and residue-averaged (red) predictions. The median of experimental ΔE values is used as cutoff to assign binary labels to variants ( Supplementary Fig. S11 ). The 40th and 60th percentiles have also been considered and used to compute uncertainty bands, represented in figure by semi-transparent blue/red shades. See also  Supplementary Figure S12 Fig. 5. Analysis of Ras predictions. ( A ) Precision-recall plot for individual (blue curve) and residue-averaged (red curve) Rhapsody predictions of experimental fitness values. Corresponding AUCs are 0.72 and 0.80, respectively. Analogous plots for EVmutation and PolyPhen-2 are reported in  Supplementary Figure S13 . ( B ) Scatter plot of Rhapsody predicted pathogenicity probabilities versus experimental measurements. See  Supplementary Figure S11  for the definition of the vertical boundary separating experimental fitness effects. ( C ) False positives (green) and False negatives (red) highlighted in panel (B) and displayed on the protein structure (active conformation) These results show that Rhapsody can be advantageously used for a first assessment of the regions that are sensitive to mutations. Moreover, the consideration of a more diverse set of properties, such as dynamics-based features on top of sequence- and structure-based ones, as in Rhapsody, provides the opportunity of interpreting the observations in the light of the protein’s structural and dynamic features. A visualization of Rhapsody incorrect predictions on Ras 3D structure ( Fig. 5B and C ) reveals that most False Negatives are localized on the protein’s surface, while False Positives are generally found in less exposed positions. A possible explanation is that the method is inherently biased toward the identification of residues important for the fold stability or internal dynamics, while locations subjected to other kinds of constraints, e.g. allostery and interactions with other proteins and small molecules, are more difficult to evaluate with the current set of features. 3.4.2 Analysis of H-Ras variants in gnomAD We tested our predictions on a set of human variants found in healthy individuals, as collected by the gnomAD DB ( Karczewski  et al. , 2019 ). The assumption is that those substitutions seen in the 140 000 people tested (mostly normal population) are somewhat permissive. We therefore compared the distribution of predictions obtained by Rhapsody on this set of gnomAD SAVs with the corresponding fitness scores from the experimental study considered above ( Bandaru  et al. , 2017 ). The results, illustrated in  Figure 6 , show that the predictions for the gnomAD SAVs are skewed toward ‘neutral’ classification in both distributions, with 49 out of 82 total variants classified as ‘neutral’ or ‘probably neutral’ by our algorithm. Of note, 3 out of 4 ‘high count’ SAVs (i.e. seen in 10 or more people) are interpreted as non-pathogenic by Rhapsody, while 2 out of 4 SAVs have a fitness score ΔE, as measured in the saturation mutagenesis study, significantly lower than the wild-type amino acid (when choosing the median of all values as cutoff). Fig. 6. Analysis of H-Ras SAVs from gnomAD DB. H-Ras SAVs (dark blue dots) collected from the gnomAD DB found in healthy population are shown along with the results for all SAVS (light blue dots) on the scatter plot between Rhapsody pathogenicity probabilities and experimental fitness scores ( Bandaru  et al. , 2017 ). ‘High-count’ SAVs (yellow stars) were seen in at least 10 individuals. The marginal plots show the corresponding distributions computed for all variants (light blue) and gnomAD variants (dark blue) 3.5 Application to PTEN and TPMT variants from CAGI competition As an additional test, we considered a dataset of over 7000 SAVs for the tumor suppressor protein PTEN and the enzyme TPMT. The pathogenicity of these proteins’ variants has been recently investigated by massively parallel sequencing (VAMP-seq), a functional assay that measures the steady-state abundance of variants in cultured human cells ( Matreyek  et al. , 2018 ). The results for PTEN/TPMT datasets were featured in the fifth edition of CAGI, a series of competitions that aim to objectively assess computational methods on blind prediction tasks ( Andreoletti  et al. , 2019 ). We performed a direct comparison of our predictor with other computational methods that, although adapted for the specific challenges proposed in the competition, were tasked with providing blind predictions, without having access to the experimental results ( Pejaver  et al. , 2019 ). To ensure as much as possible a similar unbiased evaluation, new Rhapsody classifiers were trained by excluding SAVs of PTEN (56 deleterious, 1 neutral) and TPMT (3 deleterious, 4 neutral) from our training dataset, as previously done for H-Ras. We first evaluated the predictions from Rhapsody, PolyPhen-2 and EVmutation by computing their Spearman’s correlation with experimental ‘protein-abundance’ scores from VAMP-seq data ( Supplementary Fig. S16 ). Low-abundance variants were found to be enriched in pathogenic variants and they correlated with low protein thermodynamic stability ( Matreyek  et al. , 2018 ), thus abundance score has been used as a proxy for variant impact on proteins ( Pejaver  et al. , 2019 ). A classification of variants into ‘abundancy’ classes (‘low-abundance’, ‘possibly low-abundance’, ‘possibly WT-like’ and ‘WT-like’) was also provided ( Matreyek  et al. , 2018 ), thus allowing the use of other class-based accuracy metrics, such as AUROC, MCC and F1 score. Based on these metrics, we see in  Figure 7  that Rhapsody and EVmutation are distinguished by their respective higher accuracy levels on TPMT and PTEN variants, and both consistently outperform PolyPhen-2. EVmutation, however, could only provide predictions for a small fraction (∼13%) of PTEN variants. Fig. 7. Assessment of pathogenicity predictions applied to PTEN and TPMT variants. The level of agreement between computationally predicted pathogenicity scores and experimental abundance scores is shown, based on four metrics (Spearman’s, AUROC, MCC and F1-score). Results are presented for Rhapsody, PolyPhen-2 and EVmutation, represented by bar plots (see also scatter plots in  Supplementary Fig. S16 ), and for CAGI predictors, represented by violin plots showing the distribution, median and range of values. The ‘low-abundance’ and ‘possibly low-abundance’ classes [as reported in ( Matreyek  et al. , 2018 )] were considered as ‘deleterious’, and ‘possibly WT-like’ and the ‘WT-like’ classes, as ‘neutral’. The rightmost bars in each plot represent the number of SAVs successfully predicted by each method (in red, the common value for CAGI predictors) Prediction accuracies from participants to the CAGI5 challenge, described in ( Pejaver  et al. , 2019 ) (data available from CAGI website to registered users only), are also shown in aggregated form as violin plots in  Figure 7 . In both cases, Rhapsody, EVmutation and Polyphen-2 all fall within the range of prediction accuracies measured for CAGI predictors. In the case of TPMT, we notice that Rhapsody consistently ranks between the median of the CAGI methods and the best-performing one. These results demonstrate the validity of Rhapsody predictions in tasks specifically designed for testing computational methods, and against tools specifically adapted for these tasks. The modest performances demonstrated by all methods, on the other hand, also highlight the need for more effective computational approaches. Systematic assessment campaigns such as CAGI constitute an invaluable platform for evaluating the progress in the field. 4 Discussion In the present study, we presented a novel machine learning approach for evaluating the functional impact of human SAVs, and illustrated its application to H-Ras, PTEN and TPMT. In a strict sense, Rhapsody, like many other tools in the field, predicts whether a given mutation is neutral or deleterious to protein activity, whereas pathogenicity entails many other factors, including inheritance pattern, penetrance, expressivity and environment. Thus, the outcome from the tool rather indicates a potential to be pathogenic. The newly introduced interface, Rhapsody, integrates dynamical features computed from the ENM-based analyses of protein structures and attains a state-of-the-art accuracy for predicting such a potential with a relatively simple design. We also highlighted how the method can be used not only for hypothesis generation (predictions for variants of unknown significance) but also for hypothesis testing, by providing a unified framework for comparing the predictive power of new as well as more established features. For instance, we demonstrated the utility of including in our machine-learning algorithm the ENM-derived dynamics-based features, in addition to more traditional features such as sequence conservation and structural accessibility, and emphasized the need for a better integration with coevolution analysis that recently showed significant success in evaluating the effect of SAVs. Through the analysis of saturation mutagenesis studies and other experimental and clinical data, we identified the strengths and limitations of our approach and compared it against other prediction tools. We observed a general robustness of computational predictions, especially in the identification of residue sites that are sensitive to  any  mutation, regardless of the specific amino acid substitution. This information can be invaluable for the study of the functional mechanisms of proteins, especially when projected on the 3D structures. The use of structure-based properties, in combination with sequence conservation properties (reduced classifier), can be used as an alternative approach to the more sophisticated coevolutionary analysis, whenever the latter cannot be applied due to lack of suitable multiple sequence alignments. The current algorithm has been designed to be easily expandable with new features and functionalities. Structural features such as those used in the FEATURE framework for protein annotation ( Halperin  et al. , 2008 ) could be incorporated in future versions for possibly enhancing the utility of Rhapsody. The comparison with clinical and experimental data also revealed a few issues that need to be resolved in order to advance the field. Apart from the obvious shortcomings such as the imbalance of available datasets toward pathogenic variants and the often-contradictory clinical interpretations in different DBs, we reported our difficulties in interpreting data from large-scale experimental studies. These studies provide a unique opportunity for dramatically increasing the size of training datasets. However, there is a need for a systematic definition of what is considered as a ‘pathogenic’ variant, that would account for both loss-of-function and gain-of-function effects in relation to the biological role of the affected protein. We expect future improvements to our method to address some of these shortcomings. A recent ENM study has demonstrated how the consideration of the intact structures of multimers, complexes or assemblies improves the accuracy of predicted fluctuation spectrum of residues, and predictions from that server ( DynOmics ) ( Li  et al. , 2017 ) could be used for evaluating context-dependent structural and dynamic properties. For example, a region that is deemed to be tolerant to mutations by virtue of its solvent-exposure in the PDB resolved structure, may become a buried site in a complex/assembly, and a substitution at that region could alter its binding properties. A recent study has demonstrated how disease-associated SAVs are likely to be located at singlet hot spots at protein–protein interfaces ( Ozdemir  et al. , 2018 ). Consideration of the involvement of residues in interfacial interactions is expected to improve the prediction accuracy of current algorithms. Another possible improvement would be the consideration of the signature dynamics of the protein family to which the investigated protein belongs, as opposed to the dynamics of the protein alone ( Zhang  et al. , 2019 ). In the same way as variations in sequence among family members point to sites that can, or cannot, tolerate mutations, family-based analyses can provide deeper insights into sites whose mechanistic properties are indispensable for function or for differentiation among subfamily members. Finally, a decomposition of the mode spectrum could help extract information on high-energy localization (hot) spots emerging as peaks in high frequency modes, as well as the hinge regions between domains, where substitutions may be detrimental ( Dorantes-Gilardi  et al. , 2018 ;  Rodrigues  et al. , 2018 ;  Sayılgan  et al. , 2019 ). The Rhapsody algorithm is provided both as an open-source Python package ( pip install prody-rhapsody ) and a web tool ( http://rhapsody.csb.pitt.edu ). The latter has been designed as a user-friendly service that requires minimal user input or computing skills, but also allows for some customization, such as selecting or uploading a specific PDB structure. The Rhapsody webserver can be used for both obtaining predictions on a list of human SAVs (batch query) and for visualizing a complete  in silico  saturation mutagenesis analysis of a human sequence, akin to those presented in  Figure 3  for H-Ras. Finally, the site offers tutorials, training data (OPTIDS) and precomputed features needed for reproducing all results presented here, or for analyzing new variants. The documentation also explains how to train a model on a completely different set of features and using a different training dataset, thus providing researchers with a flexible tool for analyzing personalized datasets and testing new predictors with the help of all the functionalities implemented in Rhapsody. Supplementary Material btaa127_Supplementary_Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Optimally discriminative subnetwork markers predict response to chemotherapy</Title>
    <Doi>10.1093/bioinformatics/btr245</Doi>
    <Authors>Dao Phuong, Wang Kendric, Collins Colin, Ester Martin, Lapuk Anna, Sahinalp S. Cenk</Authors>
    <Abstract>Motivation: Molecular profiles of tumour samples have been widely and successfully used for classification problems. A number of algorithms have been proposed to predict classes of tumor samples based on expression profiles with relatively high performance. However, prediction of response to cancer treatment has proved to be more challenging and novel approaches with improved generalizability are still highly needed. Recent studies have clearly demonstrated the advantages of integrating protein–protein interaction (PPI) data with gene expression profiles for the development of subnetwork markers in classification problems.</Abstract>
    <Body>1 INTRODUCTION In the treatment of cancers, patients presenting tumors with similar clinical characteristics will often respond differentially to the same chemotherapy ( van't Veer and Bernards, 2008 ). In fact, for many types of cancer, only a minority of treated patients will observe regression of tumor growth. This is the case for both conventional chemotherapeutic agents and newer targeted therapies that affect specific molecules. To achieve an effective cancer treatment, it is critical to identify the underlying mechanisms that confer chemoresistance in some tumors but not others. The advent of genome-wide expression profiling technologies has allowed the discovery of novel biomarkers for cancer diagnosis, prognosis and treatment ( van't Veer and Bernards, 2008 ). While some progress has been made toward identifying reliable prognostic markers for breast and other cancers, development of molecular markers predictive of response to chemotherapy has proved to be far more difficult ( van't Veer and Bernards, 2008 ). In recent years, a number of studies have used genome-wide expression profiling to identify genes that could be used as predictors of drug response in breast cancer ( Cleator  et al. , 2006 ;  Hess, 2006 ). In these studies, single gene marker methods were used, where each gene is individually ranked for differential expression and the top genes were selected as predictors known as single gene markers. Additional study ( Lee  et al. , 2007 ;  Liedtke  et al. , 2010 ) required single gene markers not only to be differentially expressed but also to have similar coexpression between the training and test cohorts. While some of these predictive markers have shown promising results in a limited number of patient cohorts, many of these signatures have failed to achieve similar performance in additional validation studies ( Bonnefoi  et al. , 2009 ). In addition, single gene markers developed from different cohorts have been shown to have very little overlap ( Ein-Dor  et al. , 2006 ). A further limitation of single gene markers is that they provide relatively limited insight into the biological mechanisms underlying response to drug response. Thus, predictive markers with robust performance, greater reproducibility and improved insights into drug action–which are critical for clinical application–still remains elusive. Previous studies have observed that gene products associated with cancer tend to be highly clustered in coexpression networks and have more ‘interactions’. Inspired by this observation,  Chuang  et al.  (2007 ) introduced the use of all members of a protein–protein interaction (PPI) subnetwork as a metagene marker for predicting metastasis in breast cancer.  Chuang  et al.  (2007 ) demonstrated that subnetwork markers are more robust, i.e. their results tend to provide more reproducible results across different cohorts of patients. Motivated by the limitations in predicting drug response using single gene markers and the better performance promised by subnetwork markers, this article aims to identify subnetwork markers to predict chemotherapeutic response, as detailed below. 1.1 Subnetwork markers in other applications Chuang  et al.  (2007 ) defined subnetwork activity as the aggregate expression of genes in a given subnetwork. The discriminative score of a subnetwork—which reflects how well the subnetwork discriminates samples of different phenotypes (or classes)–was derived from mutual information between subnetwork activity and the phenotype. The study presented greedy algorithms for identifying subnetworks with the highest discriminative scores and demonstrates significant improvement in classification performance over single gene marker approaches. Another approach introduced by  Chowdhury and Koyutürk (2010 ) used a binary representation of gene expression profiles to retrieve subnetwork markers. Binarized gene expression profiles were overlaid on PPI networks and subnetworks that contain genes differentially expressed in all the samples from a given class are chosen as markers. Using this approach, Chowhury  et al.  were able to predict colon cancer metastasis with high confidence. Recently, this group introduced an extension of their previous algorithm which takes into account patterns of differential expression for improved classification performance ( Chowdhury, 2010 ). A similar approach using binary representation of gene expression profiles was published by  Ulitsky  et al.  (2008 ), where subnetworks analysis was applied to the identification of dysregulated pathways in Huntington's disease. More recently,  Su  et al.  (2010 ) identified paths containing many differentially expressed and coexpressed genes from PPI networks and greedily combined these paths to obtain subnetwork markers for predicting breast cancer metastasis.  Wu  et al.  (2010 ) published a report on the application of a network-based approach to drug response data in Type 2 Diabetes. Samples were expression profiled upon treatment with individual drugs and affected subnetworks for these drugs were retrieved. These subnetworks were used to score the individual drug effect and further used to predict the effectiveness of the combination of two drugs. While the approach by  Wu  et al.  (2010 ) proved to be useful for the prediction of a drug effect on network activities, the study did not attempt to develop markers of response to any given therapy in any individual patient. Subnetworks from functional association networks can also be used for the development of markers. Edges among gene products in such networks [for example STRING database ( Jensen, 2009 )] are scored based on integration of different sources of information such as high-throughput experiments, physical binding extracted from literature and coexpression networks built from many microarray experiments.  Spirin and Mirny (2003 ) and  King  et al.  (2004 ) have observed that the constituent gene products of dense subnetworks contain many more edges than expected and usually participate in the same biological function/process or belong to the same protein complex. Dense functional association subnetworks have been used in two recent papers by  Dao  et al.  (2010 ) and  Fortney  et al.  (2010 ), which demonstrate that dense networks significantly improve the performance of subnetwork markers from PPI networks in the classification of colon cancer metastasis and prediction of chronological age in  Caenorhabditis elegans . Despite their improved performance, available approaches have a number of disadvantages. Network-based approaches introduced by  Chuang  et al.  (2007 ),  Fortney  et al.  (2010 ) and  Chowdhury and Koyutürk (2010 ) are heuristic methods and thus do not guarantee the optimality of the solution for marker selection—an optimal solution would presumably provide a better prediction performance. The branch and bound approach ( Chowdhury, 2010 ) or exhaustive enumeration ( Dao  et al. , 2010 ) can yield an optimal solution under some fixed set of parameters; however, their worst-case running time can be super-polynomial (and hence intractable). Therefore, there is a keen need for designing efficient algorithms to retrieve the optimal subnetwork markers that could successfully distinguish samples from different classes. 1.2 Our contributions In this article, we introduce a novel and efficient randomized algorithm to compute ‘optimally discriminative’ subnetworks for classification of samples from different classes. The discriminative score is calculated as the difference between the total distance between samples from different classes and the total distance between samples from the same class. Our algorithm is based on the color-coding paradigm ( Alon  et al. , 1995 ), which allows for identifying the optimally discriminative subnetwork markers for any given error probability. Since the running time of our algorithm is a logarithmic function of the error probability, we can set the error probability to a small value, close to zero, while the running time does not increase much. When the maximum size of a subnetwork is  k = O (log n ), where  n  is the size of the network, we have a polynomial time algorithm with a fixed error probability. The evaluation of our method on published patients' drug response data demonstrate that optimally discriminative subnetwork markers yield both greater and more robust classification performance compared with single gene markers and other subnetwork markers. Moreover, our algorithm provides classification results which are reproducible across independent cohorts, and provide greater biological interpretation of the underlying mechanisms of chemotherapy response. Since the discriminative score is additive, we can easily adapt our method to retrieve subnetwork markers to distinguish samples from more than two classes. This is very helpful, in particular, when there are more than three categories for responses to treatment: complete, partial and non-response. 2 METHODS In our methodology, each patient sample is represented as a point in high-dimensional space where each dimension represents one gene. We perform dimensionality reduction by projecting samples (points) into a subspace of at most  k  dimensions such that samples from different classes are well separated. The separation criteria is defined based on minimizing the distances of samples from the same class while maximizing the distances of samples from different classes.  Figure 1  sketches the idea behind our approach.
 Fig. 1. The main idea behind our approach: samples (denoted as points in a high-dimensional space) are projected into  k -dimensional space while ensuring that samples from the same class are clustered together, while samples from different classes stay separated. These  k  dimensions/genes have to form a connected subnetwork in a PPI network. The main difference between our approach and earlier ones is that we can identify the optimal subnetwork  S OPT  in polynomial time when  k = O (log n ); here  n  is the size of the network. This is done by minimizing the total distance of samples from same class while maximizing the total distance of samples from different classes. We formalize our problem as the Optimal Discriminating k-Subnetwork (ODkS) problem below. We then assess the complexity of the problem and finally give a randomized algorithm to solve it for any given error probability. 2.1 Problem definition and its complexity Before formally defining ODkS problem, we would like to introduce the notations used. Without loss of generality, we assume that we have only two classes of samples: positive and negative. Note that it is easy to generalize our approach for more than two classes. Let  A  and  A ′ denote the expression matrices for positive and negative samples, respectively. For each gene  g i , let  A i  and  A ′ i , respectively, denote the expression profiles of gene  g i  in positive class and negative class. For expression matrix  A ( A ′), let  A i ( j ) ( A ′ i ( j )) denote the expression of  g i  in sample  j . Given  n  genes, let  a  and  a ′ denote the number of samples in positive class and negative class, respectively. We denote the PPI network by  G =( V , E ), where | V |= n  and | E |= m . We define the weight function  w  on subnetwork  S  as the difference between the total distance between samples from different classes and the total distance between samples from the same class—under  L 1  distance:
 
The ODkS problem asks to compute the connected subnetwork  S OPT (| S OPT |≤ k ) from  G  such that  S OPT  ‘distinguishes’ samples from different classes ‘optimally’, i.e.  w ( S OPT ) is the maximum among  w ( S )'s for any connected subnetwork  S . We call  S OPT  the optimally discriminative subnetwork. For any connected subnetwork  S ,  w ( S ) could be rewritten as:
 
We will extend the discriminative score function  w  so that it can apply on a single gene. We assign each gene  g i  to a weight  w ( g i ):
 
Now we can rewrite the discriminative score of a connected subnetwork  S  as:
 
Thus, identifying the optimally discriminative connected subnetwork  S OPT (| S |≤ k ) is equivalent to finding the connected subnetwork for which the total weight of the vertices is maximum possible. A variant of this problem without any restriction on the size of the subnetworks ( k ≤ n ) was defined to extract dysregulated pathways in different cancer types by two independent studies ( Dittrich  et al. , 2008 ;  Qiu  et al. , 2009 ). Both studies provided integer linear programming formulations but rather than solving the IP formulation, ( Qiu  et al. , 2009 ) solved a relaxed version of the program, thus, did not give the optimal solution, and  Dittrich  et al.  (2008 ) tried to solve the integer linear program using a cutting plane method—however, this approach does not guarantee a worst-case running time. Another variant of the ODkS problem, the Connected k-Subgraph problem (where the weights of vertices are either 0 or 1), is proved to be NP-hard by (Hochbaum,D.S. and Pathria, A., in press). Here we prove that ODkS problem is also NP-hard: T heorem  2.1. The ODkS problem is NP-hard even when we have one sample for each class . P roof . The reduction is done from Connected k-Subgraph problem defined by (Hochbaum,D.S. and Pathria, A., in press). We are given an instance of Connected k-Subgraph problem where we have a graph  G =( V , E ), a weight function  h : V →{0,1} and positive integers  k  and  l . For a subnetwork  S , let  g ( S ) be the number of vertices with weight 1. The Connected k-Subgraph problem asks whether there exists a subgraph  S  in  G  with at most  k  vertices such that  g ( S )≥ l . We build an instance of the ODkS problem as follows. The network  G ′ for the instance of ODkS problem is the same as the given graph  G  i.e.  V ′= V  and  E ′= E . We only have one sample  a =1 for the positive class and another sample for the negative class  a ′=1. For each gene  g i  corresponding to a vertex  v i  in  G , set  A i (1)=0 and  A ′ i (1)= h ( v ). Now for every vertex  v i  such that  h ( v i )=1, we have the discriminative score  w ( v i )=1. By the construction, the discriminative score of any connected subnetwork  S ′ from  G ′ is equivalent to the number of vertices with weight 1 of the corresponding subgraph  S  in  G . Thus,  G  has a subgraph  S  with at most  k  vertices and  g ( S )≥ l  if and only if the network  G ′ has a subnetwork  S ′ also with at most  k  vertices and  w ( S )≥ l . 2.2 A randomized algorithm In this section, we give a randomized algorithm to solve the ODkS problem for any given error probability. This randomized algorithm is based on color-coding technique ( Alon  et al. , 1995 ). Color coding is an algorithmic technique that was first introduced by  Alon  et al.  (1995 ) to detect a simple path or a cycle of length  k  in a given graph. The algorithm consists of a predefined number of iterations. In each iteration, there are two main steps: assign each vertex uniformly at random with one of  k  colors and detect whether there is a ‘colorful’ path or cycle of length  k  in the given graph. A path or cycle is colorful if it is not the case that two vertices  u , v  in the path or cycle have the same color. The idea behind the algorithm is the clever use of colors to reduce the number of paths that need to consider in the detecting step. In the naive algorithm, we need to keep track of every vertices visited so far which uses  O ( n k ) time and space. Now we only keep track of all possible sets of vertices of distinct colors which only take  O ( n 2 k ) time and space. Color coding is widely applicable in the context of retrieving ‘homologous’ subnetworks from a PPI network given a particular query pathway or protein complex ( Bruckner  et al. , 2010 ;  Dost  et al. , 2008 ;  Scott  et al. , 2006 ;  Shlomi  et al. , 2006 ). Color coding has also been successfully applied to retrieve network motifs (subnetworks which are recurrent more than expected in a PPI network) and comparing PPI networks of different species ( Alon  et al. , 2008 ;  Dao  et al. , 2009 ). Similar to color-coding technique, our algorithm consists of a predefine number of iterations  n i  (we will show how to determine  n i  later). Each iteration consists of two main steps:
 Assign a vertex uniformly at random with one of  k  colors. Identifying the colorful connected subnetwork  S ′ OPT4 (| S ′ OPT |≤ k ) with the maximum discriminative score  w ( S ′ OPT ). 
We remind the readers that  S OPT  is the optimally discriminative connected subnetwork while  S ′ OPT  is the colorful optimally discriminative subnetwork after each iteration. After  n i  iterations, we return  S ′ OPT  of some iteration that has the the maximum  w ( S ′ OPT ). We will prove that we return  S OPT  with the given error probability δ by determining the number of iterations  n i  and identifying the colorful optimally discriminative subnetwork  S ′ OPT  in the second step efficiently. In the following, we describe how to estimate the number of iterations  n i . For each iteration, the probability that we could retrieve  S OPT  is the same as the probability that  S OPT  is colorful which is  k !/ k k ≥ e − k . In order to boost the success probability to at least 1−δ for a given error probability δ, we need
 
iterations to yield the  S OPT . In what follows, we describe an efficient dynamic programming algorithm to retrieve the  S ′ OPT . At each iteration, for any vertex  v ∈ V  let color( v ) denote the color of  v . By extending the notation of the discriminating function  w  defined earlier, we let  w ( u , T ) denote the colorful connected subnetwork  S ′ such that  S ′ contains  u , the color set of vertices in  S ′ is  T  and  S ′ has the maximum discriminative score compared with ones of the colorful connected subnetwork  S ″'s that contain  u . For the base case, for each vertex  u , we have:
 
In the general case, we can compute  w ( u , T ) as follows:
 
Here we assume that the addition of −∞ and any real number or −∞ is −∞. We first compute  w ( v , T 1 ) for each vertex  v  and each set  T 1  of one color and so on. In the final step, we compute  w ( v , T k ) for each vertex  v  and each set  T k  of  k  colors. Now we compute  w ( S ′ OPT ) as follows:
 Now we estimate the running time complexity of this randomized algorithm. Let deg( u ) be the degree of vertex  u . For any vertex  u  and a set of colors  T , in order to compute each  w ( u , T ), it takes  O (deg( u )2 | T | ) time. To retrieve  S ′ OPT  at each iteration, it takes  O ( mk 4 k ) time. Thus, the worst-case running time to retrieve  S OPT  is  O ( mk ln 1/δ(4 e ) k ). For our interests in subgraphs of small size  k = O (log n ) and for a fixed probability of error, it takes polynomial time to find the optimally discriminative subnetwork  S OPT . 2.3 Ranking subnetwork markers From now on, we fix the error probability δ=0.001 and the maximum size of a subnetwork  k =7 of for any experiment performed later. For each vertex  v ∈ V  and for each size  n ′ from 4 to  k , we compute the optimal discriminative subnetwork that contain  v  with  n ′ vertices. In total, we have at most  kn  subnetworks. For each subnetwork  S , we aggregate the expression profiles of genes in  S  into a metagene  s :
 
Now the normalized discriminative score of a subnetwork  S  is calculated in the same way as we calculate the discriminative score  w ( g ) for any gene  g  in  Section 2.1 . We rank all the extracted subnetworks by their normalized discriminative score. Then we select subnetwork markers from the top to the bottom of the list as follows. Suppose  L  is the number of genes in the selected subnetworks so far and  S  is the current considered subnetwork.  S  is only selected if we have at least | S |/2 genes that are not from  L . We finish the selection process with 50 subnetworks. 2.4 Classification process and performance assessment We always consider top 50 subnetworks for our method for any experiment performed after this point. For any  l (1≤ l ≤50), we represent a sample using top  l  subnetworks ( S 1 ,…, S l ) as follows. Each sample  j  is transformed into a  l -dimensional vector  V ( j )∈ℝ l  where the entries  V ( j ) l  for each marker  l  are
 
where  v  ranges over all genes  v  contained in the subnetwork marker  S l  and  E ( v , j ) is the expression of gene  v  in sample  j . In other words, each sample  j  becomes a point  V ( j ) in the  l -dimensional feature space ℝ l . Now, all the classification experiments were performed using three-nearest neighbor classifier under  L 1  distance. Since the tested datasets have an imbalanced ratio between number of samples in positive and negative class, accuracy is not a good measure for classification performance. We utilize Matthews Coefficient Correlation (MCC) as a measure to compare different classifiers ( BW, 1975 ). MCC is essentially the Pearson correlation between the vectors of predicted labels and true labels of a testing set. Suppose that TP is the number of true positives, TN the number of true negatives, FP the number of false positives and FN the number of false negatives. The MCC can be also calculated as follows:
 
If one of the four sums in the denominator is zero, the denominator is set to one. This results in a Matthews correlation coefficient of zero. MCC value of 1 indicates a perfect prediction, −1 an inverse prediction and 0 a completely random prediction. MCC is a recommended measure when compared with other measures for classification performance ( Baldi, 2000 ). We have chosen MCC over area under ROC curve (AUC) to facilitate comparison to competing models from the MAQC-II study ( Shi, 2010 ). 3 RESULTS AND DISCUSSION 3.1 Dataset and network We retrieved the human PPI data from the Human Protein Reference Database (HPRD) version April 2010 ( Prasad  et al. , 2009 ). By including both binary interactions and considering each protein complex as a clique of proteins, we obtained 46 370 protein interactions involving 9617 proteins. We assessed the performance of our method on a human breast cancer dataset contributed by the University of Texas M.D. Anderson Cancer Center (MDACC, Houston, TX, USA). The gene expression profiles were retrieved from NCBI Gene Expression Omnibus (GEO) with accession number GSE20194. Gene expression data from 230 Stage I–III breast cancers were generated from fine-needle aspiration specimens of newly diagnosed breast cancers before any therapy. Patients received 6 months of neoadjuvant chemotherapy comprising paclitaxel (T), 5-fluorouracil(F), doxorubicin (A) and cyclophosphamide (C) (and denoted as TFAC) followed by surgical resection of the cancer. Responders to chemotherapy was categorized as a pathological complete response i.e. no residual invasive cancer in the breast or lymph nodes or residual invasive cancer. RNA extraction and gene expression profiling were performed in multiple batches over time using Affymetrix U133A microarrays. This dataset was split into two different cohorts according to the time of collection. One cohort consists of 130 samples while the other one consists of 100 samples. The expression profiles were normalized with Robust-chip Median Average (RMA) algorithm ( Irizarry  et al. , 2003 ) and adjusted for batch effect using ComBat ( Johnson  et al. , 2007 ). Prior to model generation, the expression values of the two cohorts were normalized but not standardized. 3.2 Classification performance We evaluated the performance of our method (we denote as OptDis) against both single gene marker models and other subnetwork-based methods following the workflow presented by the MicroArray Quality Control (MAQC)-II studies ( Popovici, 2010 ;  Shi, 2010 ). In those studies, the MAQC project assessed the performance and limitations of various data analysis methods in developing and validating microarray-based predictive models with the ultimate goal of discovering best practices. Thirty-six groups participated in the project to develop classifiers for 13 large datasets, including the one used in our study. MAQC models (denoted as MAQC) were constructed by these groups using different methods for data processing (i.e. normalization), feature selection and classification. To assess the predictive performance, we performed two analyses. In the forward cross-dataset (FXD) analysis, we treated the 130 patient cohort as the training set used for deriving markers, and validated their performance on the 100 patient cohort. We also performed the complementary backward cross-dataset (BXD) analysis and swapped the cohorts used in training and validation. In  Figure 2 , we compare the performance of OptDis against single gene marker models. The single gene marker classifier constructed using  t -test is denoted by SGM and includes only genes that map to the PPI network. For each mappable gene, the corresponding probe with the lowest  P  value was used in the model. We also compared the performance of our method OptDis against implementations of existing subnetwork-based methods, one based on mutual information (GreedyMI) ( Chuang  et al. , 2007 ), and another based on dense subnetworks (we denote as Dense) using the STRING functional network ( Dao  et al. , 2010 ). The density threshold to extract all dense subnetworks is set at 0.7 as implemented in  Dao  et al.  (2010 ). Note that, top 50 subnetworks for GreedyMI and Dense are ranked based on their mutual information scores. Starting from around 20 features, the performance of OptDis is better than competing methods. While the maximum MCC value is not that high, it is still significant compared with the random classifier which has an MCC value of 0. Moreover, predicting response to chemotherapy has been shown as a difficult endpoint to predict in the recent MAQC publications ( Shi, 2010 ). The difficulties might be due to the known heterogeneity within tumors of the same cancer type, subtype-specific response, differences in drug metabolism between individuals and variations in chemotherapy schedules between patients ( Popovici, 2010 ).  Figure 3  shows the average performance of models in cross-dataset validation of FXD and BXD analyses. Here, the average performance for a model is the average MCC of 50 models generated using the top 1–50 features. The MAQC performance was derived from the average of top model from each participating group. As shown in  Figure 3 , OptDis outperforms all the other competitors on the average classification performance in FXD and BXD analyses.
 Fig. 2. Line graphs show the MCCs for different predictive models using the top 1–50 features. The compared approaches are single gene marker model based on  t -test (SGM) and subnetwork marker models include  Chuang  et al.  (2007 ) (GreedyMI), dense subgraphs from STRING functional network by  Dao  et al.  (2010 ) (Dense) and our approach (OptDis). 
 Fig. 3. Bar charts show the average MCCs of different predictive models. Single gene marker models include one based on  t -test (SGM) and models from MAQC project (MAQC). Subnetwork marker models include  Chuang  et al.  (2007 ) (GreedyMI),  Dao  et al.  (2010 ) (Dense), and our method (OptDis). The yellow bars and blue bars show the classification performance in FXD and BXD analyses respectively. The green bars show the overall average performance, calculated as the average of the yellow and blue bars. For further analyses, we compared the average best performance of different classifiers in  Figure 4 . The average best performance of a classifier is the average of its best model from FXD analysis and its best one from BXD analysis. Here, we compare against the top three MAQC models.  Figure 4  shows that our top OptDis model has consistent performance in cross-dataset validation experiments. In contrast, the top three MAQC models show discrepancy in performance when the datasets used for training and test were swapped—especially in the case of the MAQC_GeneGo model, which has the largest difference in performance (0.25) between the FXD and BXD analysis. The second and third best MAQC models also show similar discrepancy in performance.
 Fig. 4. The bar charts show the average best MCCs of different classifiers. The average best performance of a classifier is the average of its best model from FXD analysis and its best model from BXD analysis. Single gene marker models include one based on  t -test (SGM) and top 3 models from MAQC project (MAQC GeneGo, MAQC SAI, MAQC GSK). Subnetwork marker models include  Chuang  et al.  (2007 ) (GreedyMI),  Dao  et al.  (2010 ) (Dense), and our approach (OptDis). Green bars show the average best MCCs and red bars show the difference in MCC between a classifiers' best model from FXD analysis and its best model from BXD analysis. Figure 5  shows the performance of OptDis against one of the predictive model constructed, where the constituent genes were taken from the top  x (1≤ x ≤50) OptDis subnetworks (we denote as SGM_OptDis). We also compare our method against another single gene marker model that ranks all genes by  t -test and matches the number of genes in the top  x (1≤ x ≤50) subnetworks from OptDis (SGM_M). OptDis is consistently better than SGM_OptDis across different number of features. This suggests the importance of treating genes as functional modules. Moreover, on the average constituent genes taken from OptDis subnetworks tend to perform better than genes from simple predictive model using  t -test. Hence, OptDis subnetworks might capture genes more informative to predicting chemotherapy response.
 Fig. 5. Line graphs show the MCCs for different predictive models: our approach (OptDis), the model that constitutes genes from top  x  (1≤ x ≤50) subnetworks from OptDis (SGM_OptDis) and another model that ranks genes by  t -test and matches the number of genes from top  x  (1≤ x ≤50) subnetworks from OptDis (SGM_M). In summary, our subnetwork markers have the best combination of relatively high performance and greater stability between different cohorts of patients and thus could be more clinically applicable to other independent cohorts of patients. 3.3 Reproducibility of predictive markers We compared the reproducibility of predictive markers derived from subnetwork (SN) and single gene (SG) approaches by training OptDis and SGM on the two different cohorts of 130 and 100 breast patients and calculating the number of overlapping genes. For this comparison, we considered the top 50 SN markers (T50 SN), the top 50 SG markers (T50 SG) and the top X SG markers (Tx SG) that comprises a similar number genes as the T50 SNs. There is an overlap of 39 genes (27–30%) between the T50 SN, significantly more than the overlap of five genes (10%) between the T50 SG and 25 (17%) genes between T150 SG. Greater reproducibility may be contributing to the improved stability in predictive performance of our subnetwork markers over single gene markers. 3.4 Role of predictive markers in drug response Gene function analysis: we hypothesized that the set of 39 genes (O39) common between the two T50 SN signatures trained on different cohorts may be important to the activity of TFAC therapy. Some of their biological functions are listed in  Table 1 . About half are implicated in apoptosis, suggesting that changes in strengths of pro-apoptotic and anti-apoptotic signals can induce resistance to chemotherapy. There are also genes involved in DNA repair, which is expected given many of the anticancer drugs within TFAC therapy induce DNA damage (i.e. cyclophosphamide by cross-linking DNA strands).
 Table 1. Table of enriched molecular and cellular functions related to drug response of overlapping gene set (O39) Enriched terms Gene symbols P -value Apoptosis AR, EP300, ESR1, GADD45G, IGF2, 1.27E-06 IGF1R, IGFBP4, IL6ST, MAPK3, MDM2, MED1, NCOA3, PRKACA, RARA, RET, SHC1, SMAD3, SRC, TSC2 DNA synthesis AR, ESR1, IGF2, IGFBP4, IL6ST, 1.74E-06 MDM2, SHC1, SRC Actin filament organization EVL, CST3, RET, SRC, TSC2 7.16E-03 DNA repair GADD45G, MDM2, RARA, SMAD3 1.89E-02 The  P -values are adjusted using Benjamini–Hochberg method. 
 Some of the 39 genes have specific functions related to mechanism of individual TFAC drugs. Paclitaxel is a mitotic inhibitor that stabilizes microtubule activity during mitosis and induces cell death. While paclitaxel is known to act on beta-tubulin, some studies ( Kavallaris, 2010 ) have also shown association between the actin and tubulin cytoskeleton in drug response, and suggest that regulation of actin cytoskeleton can induce sensitivity to mitotic-inhibitors. From our O39 list, the EVL, RET and CST3 genes have regulatory roles in the organization and assembly of actin filaments. Fluouracil's primary anticancer activity blocks DNA replication by suppressing thymidyate synthetase activity and depleting thymidine ( Longley  et al. , 2003 ).  In vitro  studies have shown that AR and IGF2, from our O39 list, can increase incorporation of thymidine, which acts in antagonist to thymidyate synthetase suppression, to allow DNA synthesis through the actions of thymidine kinase ( Pedram  et al. , 2007 ;  Yang  et al. , 1996 ). Doxorubicin is an anthracycline antibiotic that intercalates with DNA and causes double-stranded breaks to induce cell apoptosis or disruption in mitosis ( Minotti  et al. , 2004 ;  Munro  et al. , 2010 ). SMAD3 from our list has been observed to affect BRCA1-dependent double-stranded DNA break repair in breast cancer cell lines and thus potentially may contribute to differential response to doxorubicin ( Dubrovska, 2005 ). Signalling pathway analysis: finally, we also compared subnetwork and single gene markers based on their insights into the mechanisms underlying drug response. We derived the T50 SN, T50 SG, and Tx SG from the combined cohort of 230 patients and used the Ingenuity Pathway Analysis software (IPA; Ingenuity ©  Systems,  www.ingenuity.com ) to identify significant pathway associations. Interestingly, several signaling pathways associated with chemotherapy response were identified for SN markers, whereas no significantly enriched pathways were found for the T50 and T111 SG markers ( Fig. 6 ). A closer examination of the top associated pathways suggests response to TFAC treatment is affected by the cross-talk between tumor subtype specific mechanisms and pathways regulating apoptosis. Chemotherapy response in breast cancer have been observed to be subtype-specific ( Sorlie, 2006 ), with ER+ tumors exhibiting much higher response rates to taxane-based therapies than ER− tumors ( Farmer, 2009 ;  Liedtke, 2008 ;  Popovici, 2010 ). Therefore, it was expected to find that the predictive subnetwork signature was strongly enriched for genes activating the estrogen receptor (ER) signaling pathway. For the same reason, we also observe an enrichment for the androgen receptor (AR) signaling pathway. With nearly all ER+ tumors and few ER− tumors showing AR expression ( Niemeier  et al. , 2010 ), it is likely that AR-based subnetworks serve as good predictive markers of TFAC treatment based on their association with ER status. Based on the enriched IPA pathways associated with response, we speculate that the differential response between subtypes may be attributed to differential regulation of apoptosis. Experimental studies have shown that expression of ERα selectively inhibits paclitaxel-induced apoptosis through modulation of glucocorticoid receptor activity ( Sui  et al. , 2007 ).
 Fig. 6. Signaling pathways associated with TFAC response, ranked by enrichmentin the T50 SN derived from our OptDis method. We also compare the enrichment of those pathways in the genes from T50 SN (dark blue), O39 (light blue), T50 SG (cyan), and T111 SG markers (black). Significantly enriched pathways have Benjamini-Hochberg corrected p-values above threshold of 0.05 (dotted line). Other response-associated pathways may also contribute to differential response to TFAC treatment. For example, signalling of insulin-like growth factor has known functions in cancer proliferation and inhibition of apoptosis, and has been experimentally implicated in chemotherapy resistance ( Benini, 2001 ;  Dunn  et al. , 1997 ;  Gooch  et al. , 1999 ). The PI3K/AKT pathway can also increase resistance to taxane-based therapies through downstream anti-apoptotic effectors BCL-2 and BCL-XL ( McGrogan  et al. , 2008 ). Experiments have shown that tumors with increased phosphorylated BCL-2 expression have increased sensitivity to paclitaxel compared with tumors with reduced expression ( Shitashige  et al. , 2001 ). We measured the reproducibility of these pathway enrichments by performing IPA pathway analysis on both O39 genes and the T50 SNs derived from the pooled 230 patients using another SN method (GreedyMI).  Figure 6  shows that both predictive SN signatures were significantly enriched with the same pathways, which may implicate a strong role for these pathways in response to TFAC treatment. 4 CONCLUSIONS From our analyses, we derived subnetwork markers from separate cohorts of patients and clearly demonstrated the advantages of using subnetwork markers over single gene markers for the prediction chemotherapy response. The improved reproducibility of subnetwork markers and its relevant insights into the underlying mechanisms of drug resistance or sensitivity suggest that they may serve as better clinical predictors of drug response. Funding :  IGTC Mathematical Biology Training Program and Bioinformatics for Combating Infectious Diseases fellowships  (to P.D.),  CIHR Bioinformatics Training Program  (to K.W.).
 Conflict of Interest : none declared. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>The locality dilemma of Sankoff-like RNA alignments</Title>
    <Doi>10.1093/bioinformatics/btaa431</Doi>
    <Authors>Müller Teresa, Miladi Milad, Hutter Frank, Hofacker Ivo, Will Sebastian, Backofen Rolf</Authors>
    <Abstract/>
    <Body>1 Introduction High-throughput sequencing experiments made blatantly evident that the majority of the eukaryotic genome is transcribed. Hundreds of thousands of non-coding RNAs (ncRNAs) and untranslated regions were reported. Revealing the subset of functional RNAs, and their specific functions, remains a major challenge. One important means to contribute to this task is to identify classes of homologous genes or at least conserved domains. However, aligning homologous RNAs to determine conservation is a notoriously hard problem if the function of these RNAs is carried out by their emergent structure. Such ‘structural’ RNAs often do not show clear sequence conservation, but have the potential to fold into conserved homologous structures. For this reason, sequence alignment tools fail to align RNAs once the sequence identity is below 60% ( Gardner  et al. , 2005 ), which is the case for many homologous functional RNAs ( Nawrocki and Eddy, 2013 ). This could explain, potentially many, cases of ncRNAs without any known homologues, not even in closely related species. Thus, with some plausibility, the observed absence of homologs could often be a pure artifact, caused by insufficiencies of the commonly used tools. To overcome such limitations, sequence–structure-based alignment methods like Dynalign, Foldalign and LocARNA ( Mathews and Turner, 2002 ;  Torarinsson  et al. , 2007 ;  Will  et al. , 2007 ) have been used routinely to align ncRNAs. Most of these tools are derived in one way or the other from Sankoff’s approach ( Sankoff, 1985 ), which simultaneously aligns and folds sequence and structure. Sankoff-based alignment approaches can be characterized by a scoring system that adds a dominantly positive structure contribution to a sequence alignment. It has been repeatedly shown that they produce high-quality alignments for RNAs that share a common conserved structure ( Gardner  et al. , 2005 ;  Puton  et al. , 2013 ). Moreover, they outperform pure sequence-based approaches, especially when aligning regions with lower sequence identity. Furthermore, it has been shown that the existing methods perform well for ‘globally’ aligning two or several RNAs that are  already known  to be homologous. However, the more important application of sequence–structure alignment is to reveal so far unknown ncRNA homologues. Since the exact genetic loci are generally unknown in, this scenario  global  alignment is of little use. One must rather apply (some form of)  local  alignment. We emphasize that the existing benchmarks do not assess the performance of the existing tools in this second scenario. As well note that clustering-based ncRNA annotation approaches ( Miladi  et al. , 2019 ,  2017 ) implicitly detect ncRNAs homologues and could directly profit from improved local alignment. There exists anecdotal evidence in the RNA community that the current pairwise sequence–structure alignment tools have difficulties to characterize homologous RNAs based on purely pairwise comparisons. For example, one study claimed that BLAST ( Altschul  et al. , 1990 ) still works better for this task than sequence–structure alignment ( Menzel  et al. , 2009 ). Another piece of evidence was provided by  Heyer (2000) , who showed that the Erdös-Rény law of a logarithmic growth of (local) alignment length (resp. score) for random sequences holds for many sequence analysis problems, except for sequence–structure alignment with a high structural scoring contribution. However, to the best of our knowledge, as of yet the relation of global Sankoff-like alignment to local Sankoff-like alignment has never been studied systematically. Consequently, it is insufficiently understood, how to ensure both correct alignments and a sub-linear growth with sequence length at the same time. Here, we approach this open question utilizing the popular sequence-alignment tool LocARNA ( Will  et al. , 2007 ). LocARNA inherits the scoring system of PMcomp ( Hofacker  et al. , 2004 ), which strongly reduces the computational load compared to the Sankoff’s original algorithm for simultaneous alignment and folding (SA&amp;F). Due to several algorithmic advancements, LocARNA moreover drastically reduces the run time over PMcomp (even reducing the theoretical complexity by a quadratic factor in sequence length over the original Sankoff algorithm). PMcomp’s scoring system itself turned out to be a highly successful advancement in RNA alignment, it is implemented e.g. in LocARNA, Sparse ( Will  et al. , 2015 ) and FoldalignM ( Torarinsson  et al. , 2007 ). LocARNA’s high performance and popularity makes it well suited to serve as representative of simultaneous folding and alignment algorithms in this work. Recall that, as we explained before the existing benchmarks are insufficient to study the different aspects of the local alignment problem. Therefore, as a prerequisite we design a novel  local  alignment benchmark. On this basis, we examine the effect of modifications to the LocARNA algorithm on local pairwise alignment. First, we simply tweak the balance factor (structure weight) between the sequence and the structure similarity component of LocARNA’s alignment evaluation (‘alignment score’). Here we are interested in the effect on the quality of the local alignment, the correct detection of motif boundaries and the length of the reported alignments for random sequences. In a second step, we investigated the interplay of different other parameters of LocARNA’s score. Performing parameter optimization, we showed that the optimal scoring parameters for local alignment drastically differ from the optimal parameters for global alignment. This is the effect of optimization attempting to compensate for the local alignment detrimental effects of the structure contribution bias. In this way, the strongly deviating trained parameter sets indicate deficiencies of the scoring scheme. As well, note that using different parameter sets for local and global alignment is not suitable to resolve the existing problems with local RNA alignment. Using different schemes would moreover be highly inconvenient: tools used in post-processing would have to be adapted to the different scoring schemes (often enough it is unclear how to achieve this). A popular example for such a tool is RNAz ( Gruber  et al. , 2010 ) [resp. RNAcode ( Washietl  et al. , 2011 )], which determines a conserved structure (resp. a conserved ncRNA) from calculated alignments. Finally, we go beyond studying different parametrizations of LocARNA by introducing a new position-wise penalty. This additional score component is designed to counteract the negative properties of a high structure score and allows for using the same scoring parameters. A graphical summary of our investigations and its results is given in  Figure 1 .
 Fig. 1. A schematic overview of the trouble with locality for SA&amp;F methods. The left side highlights the general issues of global and local alignments using default parameters. Without optimal parameters, global aligners (box  a ) will partial misalign structure (red bars). When predicting local alignments (box  b ), correctly aligning structured motifs gets even more difficult. Additionally, often local alignments are either extended over the motif’s boundaries or random structures are aligned. These issues can be improved only partially by using optimized alignment parameters for the global and local predictions (right of the figure). Box ( c ) demonstrates that the optimized parameter set for the global alignment improves the alignment quality, whereas simply training parameters is insufficient to produce accurate local alignments (box  d ). Only after introducing a position-wise penalty (box  e ), global optimized parameters improve the accuracy of the local alignment predictions Our results underline that the original scoring system  rewards  matching structures on top of the sequence score without being able to properly balance the two components. This compromises the ability to properly align RNAs locally. We show that the proposed position-wise penalty can alleviate these problems, which strongly supports our hypothesis. While we performed our deep analysis for one specific PMcomp-like system, the results indicate issues of any system for scoring RNA alignments that is composed from a sequence and structure component. This includes Sankoff’s original simultaneous folding and alignment algorithm. 2 Materials and methods 2.1 Local alignment scores and growth of random alignments The prerequisite for local alignment, as e.g. stated in the seminal work of  Karlin and Altschul (1990)  about the statistical significance of local sequence alignment, is that the expected  global  score for random sequences is negative. Otherwise, ‘the maximal segment would tend to be the whole sequence’ [see Karlin and  Altschul (1990) , p. 2265]. Log-odds scores, which are commonly used for sequence alignments, automatically satisfy this property. The reason is simply that the expected scores for independent sequences is the negative Kullbach–Leibler (KL) divergence between distribution for alignment edges in the case of homologous sequences versus the distribution of alignment edges for independent sequences ( Altschul, 1991 ). In more detail, the expected global score for independent sequences  E  is given by
 (1) E random = ∑ a , b q a q b   log   ( p a b q a q b ) = − ∑ a , b q a q b   log   ( q a q b p a b ) where  p ab  is the probability for seeing an alignment edge  a ,  b  in homologous sequences and  q a  (resp.  q b ) is the background probability of  a  (resp.  b ). This is equivalent to the negative KL divergence between the two distributions  ( p a b ) a , b  and  ( q a q b ) a , b . As the KL divergence is positive, the expected score should be negative. The side effect is that for homologous sequences, the expected global score is  E evol = ∑ a , b p a b   log ( p a b q a q b ) .  E evol  is the KL divergence between  ( q a q b ) a , b  and  ( p a b ) a , b , and therefore, positive. The relation between negative expected global score for random sequences and the length of maximal matched segments has not been assessed for the two-dimensional case of alignment, not to speak of sequence–structure alignment. However, for the one-dimensional case of maximal segments in a single sequence, it was shown ( Karlin  et al. , 1990 ) that the local scores for a single sequence (i.e. where a segment is defined by consecutive hits in a single sequence) grows with the logarithm of the sequence length  n  if the expected global score is negative, and with  n  if the expected score is 0. To profit from established thermodynamic energy models, e.g. with empirical or independently trained parameterization, common Sankoff-like sequence–structure scores consist of two components, namely a sequence alignment score and a structure contribution, which is positive for matched structures (see next section). In consequence, their score cannot be directly written as log-odds score as there is no background distribution for the structural which is used to weight this contribution.  Heyer (2000)  showed by experimental analysis that having  n  as a normalization factor, which would indicate a global expected score of 0, is probably too low for a scoring with a high structural contribution, and too high for a low structural contribution. This would indicate a logarithmic growth trend only for a low structure scoring. However, the effect on the actual alignments has not been investigated yet. Thus, we consider in the article the actual global score for random sequences, the effect on the alignment quality as well as the effect on the detection of alignment boundaries (i.e. the correct detection of the actual motif). 2.2 Details of the LocARNA alignment score Sankoff-style alignment scores Sankoff-style alignment algorithms optimize a score consisting of a sequence and structure score. The structure score depends on the structures of the RNAs, which are predicted simultaneously with the sequence alignment such that the combined score becomes optimal. In this way, Sankoff-style algorithms find and align the RNAs at the same time; the task is therefore called SA&amp;F. In Sankoff’s original algorithm, the score is composed of an edit distance and the energies of the predicted structures; for computing optimal alignments, the combined score is minimized (see  Supplementary Section S1 ). Since computing optimal local alignments typically requires similarity scores, which are able to distinguish similar subsequences from dissimilar ones, reformulations based on a similarity score have been suggested. For example, Foldalign introduced a similarity score based on sequence energies and negated energies; PMcomp suggested to compute the structure similarity from log odds of base pair probabilities. Alignment and scoring by LocARNA Following the idea of PMcomp, LocARNA scores structure based on precomputed base pair probability matrices. This allows LocARNA to compute alignments with a lot less overhead compared to the original Sankoff algorithm, while maintaining good accuracy. Moreover, by exploiting the sparsity of the structure space, LocARNA computes pairwise RNA alignment in only  O ( n 4 ) time—compared to the  O ( n 6 ) time complexity of the Sankoff algorithm. To determine an optimal pairwise alignment, it optimizes an RNA alignment score by dynamic programing. This score is the sum of two components [ Equations (2)  and  (4)] . The  sequence score component  evaluates the similarity at all sequence positions  A s  that are not involved in matching base pairs:
 (2) ∑ ( i , k ) ∈ A s σ ( i , k ) − γ N gap − β N gap o Here,  σ ( i ,  k ) denotes RIBOSUM base match similarity of  a i  and  b k , and  N gap o  and  N gap  respectively count gap openings and gap extensions—each respectively penalized by β or γ. For the second component, LocARNA scores each base pairs ( i ,  j ) of sequence  x  in the predicted consensus structure, by
 (3) Ψ i j x = log ( p i j / p 0 ) /   log ( 1 / p 0 ) , i.e. essentially by a log-odd score of the base pair’s probability  p ij  against a background probability  p 0  in  x . The complete  structure score component  is a sum of contributions for each match ( ij ;  kl ) of base pairs ( i ,  j ) and ( k ,  l ) in the consensus structure  S :
 (4) ∑ ( i j ; k l ) ∈ S ω ( Ψ i j a + Ψ k l b ) + τ σ ′ ( i , j , k , l ) , where  σ `  yields RIBOSUM base pair similarities. Observe how the parameter structure weight ω controls the contribution of the  structure score component   Ψ i j a + Ψ k l b  to the remaining  sequence score component  of the LocARNA score; the parameterτ, called  tau factor , controls the contribution of sequence similarity at the ends of predicted base pairs; as rationale of this parameter, there are contradicting motivations to penalize or even favor mutations at the ends of base pairs—compensatory mutations provide support for the conservation of the base pair, but discourage aligning its ends (see  Supplementary Section S1 ). Our extension of a  position-wise penalty  is achieved by subtracting a term λ for each scored position. Thus, the gap term  γ N gap  has to be replaced by  − ( γ + λ ) N gap . Each sequence match has now to be scored by  σ ( i , k ) − 2 λ . Finally, for each base pair match, a penalty of −4λ has to be added, as four positions are scored ( Supplementary Formula S7 ). Sequence-only score For computational efficiency, LocARNA computes the structure scoring part only if the base pair probability  p ij  of nucleotide  i  and nucleotide  j  is over a defined threshold  p . By setting the threshold  p  to its highest possible value (1), LocARNA’s objective function will only use its sequence part and not align any base pair for the alignment prediction. 2.3 Alignment quality measures Comparison to a reference alignment A standard approach to measure the alignment quality, given a reference alignment, is the s um-of-pairs score (SPS)  ( Thompson  et al. , 1999 ), which is defined as:  SPS = # correctly   predicted   columns | reference | . A perfect prediction has SPS of one, whereas an SPS of zero indicates a completely mispredicted alignment. To extend this measurement for local alignment, we have to take both the length of the reference alignment as well as the length of the predicted motif into account. Thus, we define a new measurement  maxSPS  as follows:  maxSPS = # correctly   predicted   columns max ( | reference | , | prediction | ) . A upper boundary for the maxSPS value is the SPS of the same local alignment. Therefore, a maxSPS value is always less than or equal to the correspond SPS value (see  Supplementary Section S2  and  Fig. S1 ). Structure prediction quality The  Matthews correlation coefficient  (MCC) evaluates the quality of the predicted structures from simultaneously aligned and folded alignments ( Gorodkin  et al. , 2001 ) (see  Supplementary Section S2  for details). Quality of alignment boundaries The alignment quality can be measured by computing how many nucleotides of the local motif are predicted and how many nucleotides of the context are not part of the predicted alignment. This allows us to introduce  sensitivity  as measure how well the structured RNA motif is found and  specificity  assesses how well the alignment avoids extensions into the context. True positive TP or true negative TN values are all nucleotides that are part or not part of the predicted and the reference alignment. False positive FP values are all nucleotides that are part of the predicted but not of the reference alignment and false negative FN values are all nucleotides that are not part of the predicted but part of the reference alignment. Sensitivity and specificity are defined as usually given TP, FP, TN and FN (see  Supplementary Fig. S2 ). 2.4 Data Artificial dataset A dataset of random sequences to investigate the expected alignment score and alignment length of sequence–structure algorithms was generated. We created an alignment input dataset of 7000 FASTA files each having two randomly generated RNA sequences. The random sequences were generated by controlling the features alignment length (100 nt) and GC content (average 50%). The  average pairwise sequence identity  (APSI) is computed using the ALISTAT tool from the HMMER package (Version 3.2.1) ( Wheeler and Eddy, 2013 ). The APSI distribution is on average 40% (see  Supplementary Fig. S7 ). The python function from  random  library was used to generate random RNA sequences. On average the dataset has a uniform distribution of the four bases with an average GC content of 50% (see  Supplementary Fig. S6 ). BRAliBase The BRAliBase 2.1 is a well-established alignment benchmark dataset ( Wilm  et al. , 2006 ). It contains in total 18 990 ncRNAs alignments from 36 different Rfam families. For each alignment a raw file, containing the unaligned input sequences and a reference file, providing the corresponding mostly hand-curated reference alignments, are provided. Each file is annotated with the (APSI) and the  structure conservation index  (SCI) information. We computed pairwise alignments and therefore only used the pairwise subset k2 of the BRAliBase. In total, k2 has 8976 entries from 36 different ncRNA families. For length distribution, see  Supplementary Figure S8 . The average SCI of BRAliBase is 0.93. All alignments below SCI 0.6 were excluded from the dataset. Therefore, BRAliBase fits well to our benchmark tasks. The shuffled ncRNAs dataset was produced by applying  FASTA-shuffle-letters  from the MEME suite ( Bailey  et al. , 2009 ) to all k2 sequences, to calculate the expected normalized scores. LocalBRAliBase A novel local alignment benchmark set is generated by placing all ncRNAs of the BRAliBase into its shuffled genomic context. The genomic context was derived from the European Nucleotide Archive (ENA) hosted by the European Molecular Biology Laboratory (EMBL) ( Hussein  et al. , 2019 ). Using the accession number, of each ncRNA, the according nucleotide sequence was downloaded in FASTA format from ENA. For every ncRNA, the start and end positions inside the downloaded nucleotide sequence are known. Using this position, the ncRNA and its genomic context can be located. A fixed size (100 nt) of the genomic context was extracted equally up- and downstream of the ncRNA. In the case of limited context on one side, the missing nucleotides were extracted from the other side (context). If a full extraction of the context failed or the nucleotide sequence could not be found by its accession number the entry is excluded from the final LocalBRAliBase. For the context elongation of 200 nt, 2750 ncRNAs had not enough context available or could not be found by their accession number in ENA and therefore were excluded from the final dataset. The flanking regions were dinucleotide shuffled using the tool uShuffle ( Jiang  et al. , 2008 ). 2.5 Optimization setup Sequential model-based algorithm configuration (SMAC) is a black box optimization tool that identifies (sub-)optimal parameter combinations for configuring arbitrary algorithms ( Hutter  et al. , 2011 ) (see  Supplementary Section S3 ). The relationship between parameters and the desired algorithm result (alignment) is learned by optimizing a cost function or quality score function. A python wrapper is handling the parameter settings and the input instances and applies them to our cost function (scoring function). The objective function is written in Perl and computes the LocARNA alignment for a given instance and parameter setting and the quality of this computed alignment. The alignment quality measures used for the optimization of LocARNA parameters in the global mode is the geometric mean of SPS and MCC. The MCC is in most cases between 0 and 1, but it can also take a value down to −1. If the MCC was negative, its value was set to zero. For the local alignment mode, we used the maxSPS value metric as described before. For details, please refer to the  Supplementary Section S2 . The BRAliBase dataset was used to infer the global alignment optimal parameter configuration and LocalBRAliBase, with a shuffled genomic context of length 200 nucleotides, was used for the local alignment mode. Both datasets are filtered so that the number of instances per families are comparable amongst families and the dataset is uniformly distributed, leaving the global dataset with 2090 instances and the local dataset with 1370 training instances. To identify the robustness of the parameter optimization and to exclude potential over fitting, a 10-fold cross validation was performed (see  Supplementary Fig. S3 ). The four LocARNA parameters gap extension (γ), gap opening (β), structure weight (ω) and a tau factor (τ) were subject to optimization within the relevant ranges (i.e.  γ : [ − 1000 , 0 ] ,   β : [ − 1500 , 0 ] ,   ω : [ 0 , 1000 ] ,   τ : [ 0 , 100 ] ) 3 Results and discussion We performed several experiments to elucidate our guiding question from different angles, namely whether there is a general difficulty of using Sankoff-like scores for the simultaneous  local  alignment and folding of RNAs. Since we are interested in shading light on the general phenomenon, we perform our experiments using the state-of-the-art RNA alignment tool LocARNA, as a representative of Sankoff-like approaches, or more specifically their PMcomp-like variety, which turned out particularly successful over the last decade. Since we conjectured a potential positive structure scoring bias, which results from the non-negative structure score contribution, we start by directly quantifying the dimension of this bias in our first experiment. In our second experiment, we show the direct effect on the length of local alignments. While, as our first two studies show, overestimation of the alignment boundaries can be avoided by reducing the structure weight, we anticipated a conflicting negative effect on the alignment accuracy. The counterplay of these effects was investigated in a third study. Furthermore, we were interested in optimal parameter settings for local and global alignment. Ideally, one could find a common optimal set of parameters for local and global alignment. As we discuss in more detail elsewhere, this has advantages in downstream analysis. Our final experiment attempts to directly rescue local alignment from the conjectured bias. A successful rescue could provide most direct evidence for the conjectured phenomenon; as well, such results could be directly useful to improve local RNA alignment. 3.1 Assessing the quality of local alignment requires a novel, specifically tailored benchmark In order to perform these studies, we designed a novel benchmark LocalBRAliBase and a local alignment quality measurement (maxSPS) for assessing local RNA alignment methods and their parametrization. To the best of our knowledge, there is not a comparably well-established benchmark set for  local  alignments evaluation, like BRAliBase for  global  alignments. Therefore, we generated a local alignment benchmark set by taking the ncRNA’s of the BRAliBase as local alignment motifs and embedded this motifs into their shuffled genomic contexts, also referenced as flanking regions. The flanking regions were shuffled, because the real genomic context of an ncRNA can be of high similarity. Specially for ncRNA inputs having lower sequence identity, it could be biologically meaningful to align the context. Hence, we clear possible similarity of the context but not its nucleotide frequency. To get insights on how sequences with lower sequence identity or high structure conservation behave differently in comparison to the overall set, we filtered the LocalBRAliBase. The set of sequences with lower sequence identity was constructed by filtering for sequences APSI smaller than 70 (APSI &lt; 70). It is a more challenging task to find a local motif of low sequence identity in its genomic context. On the other hand for local motifs that are highly structured, accounting for structure should help to find more exact alignments. To have a set of sequences that are highly structured, we filtered the LocalBRAliBase for sequences with a SCI over 100 (SCI &gt; 100). The LocalBRAliBase construction resulted in 6226 entries for k2. The filtering of this k2 instances resulted in 3019 instances for APSI smaller 70 and 2165 instances for SCI greater 100. The LocalBRAliBase dataset was used to investigate boundary detection and the local alignment quality. However, to evaluate the quality of a local alignment it is still important to validate the correctly predicted alignment edges. The maxSPS is therefore a novel way of scoring local alignment by penalizing context extensions but also local alignments which are to short (Section 2). It extends the idea of the SPS measure by penalizing extensions of local alignments into the context (in contrast to the SPS score of the local alignment). Thus, the measure properly assesses the quality of local alignments, taking into account that good local alignments are similar to the reference in terms of boundaries and alignment columns. Note that the latter implicitly penalizes too short predictions (see Section 2 and  Supplementary Fig. S1 ). 3.2 The expected background alignment score is shifted due to spurious structure contribution A general bias on unrelated sequences, as it could be caused by the structure component of the LocARNA score, should be visible by a shift of expected LocARNA scores toward positive scores, if one puts more weight on the structure component. Note that in LocARNA, this works directly by increasing the parameter ‘structure weight’. We quantified such effects based on the ncRNAs from BRAliBase. For each pair of ncRNAs, we shuffled both sequences using dinucleotide shuffling (see Section 2). Then, for a series of different structure weighs (ranging from 0 to 400, in increments of 50), we aligned the pairs of ncRNAs as well as the pairs of shuffled ncRNAs with LocARNA. Finally, we compared the distribution of the scores, normalized to the sum of sequence lengths ( Fig. 2 ). The normalized score reflects the average score contribution per nucleotide and allows comparing alignment scores from sequences of different lengths.
 Fig. 2. The structure contribution leads to a positive scoring bias. The average normalized LocARNA scores for increasing structure weights (on the x-axis) for the three date sets: Sequence–structure alignments of ncRNAs from k2-BRAliBase (green, X), sequence–structure alignments of the shuffled ncRNA from k2–BRAliBase (blue, box), and sequence-only alignments of shuffled ncRNA from k2–BRAliBase (blue, dashed line). Sequence-only alignments were obtained by turning off structure matching in LocARNA’s algorithm. The vertical bars represent the standard deviations. With increasing structure weights, the score value gap between sequence–structure and sequence-only alignments of the shuffled sequences increases; remarkably, even for the structure weight 0, sequence–structure alignments have a higher average score than the sequence-only alignments. Comparing alignments scores of true ncRNAs to the alignments scores of shuffled ncRNA shows that LocARNA is able to distinguish between shuffled and real ncRNAs We observe that the normalized score achieved when aligning ncRNAs is always positive and increases with an increased structure weight. Moreover, for the shuffled ncRNAs, the score increases with increasing structure weight, although these RNAs cannot contain any conserved structure. This supports the idea of a general positive shift due to the structure component even for unrelated RNAs. We also compared the score to a sequence-only score, which is [by  Equation (1)]  expected to be negative. Here, we completely turn off structure alignment (i.e. switch to pure sequence alignment) by setting LocARNA’s threshold for the base pair probability to 1; notably there is a subtle difference to setting the structure weight to zero. Only in the latter case, structure could still have an effect on the alignment: dissimilar nucleotides at the ends of two base pairs would rather be scored as structure match than two negative sequence matches, since the structure match has score zero (due to zero structure weight). Note also that a part of the observed shift for the shuffled ncRNA sequences could be explained by their higher energy compared to the unshuffled ncRNAs, since higher energy is correlated with lower scores (see  Supplementary Figs S11 and S12 ), which contributes to this gap. To make sure that not any database biases lead to wrong conclusions, we repeated the experiment with an artificial dataset (see  Supplementary Fig. S4 ). Having no bias like different GC content or different sequence length leads to an even slightly more positive score. For structure weight 0, we are more or less back to the sequence score, except that for base pair the alignment could use the Ribosum score ( Klein and Eddy, 2003 ) instead of the sequence score for both ends of the base pair. Nevertheless, even the Ribosum score is negative in expectation since it is a log-odds score as well. While this positive contribution fact of the structure score was already clear, no one until now investigated the relative contribution of structure and sequence part of the score. The main problem here is that one cannot make a simple theoretical analysis as given for sequence alignment by  Equation (1)  as the structure score in LocARNA (and many others) depends on base probabilities which use information stemming from the whole sequence. This way, the different alignment edges are not independent anymore. 3.3 The local alignment length of random sequences increases with growing structure weight The previous experiment ( Fig. 2 ) shows that the score clearly distinguishes positive data (pairs of ncRNAs) from negative data (pairs for shuffled ncRNAs); so far, this is in line with previous results that show the good quality of the LocARNA score for global alignment. Still, this leaves the questions, whether the observed bias has effects on the quality of local alignments and how strong such effects are. On the one hand, the required negative expected scores, to avoid linear growth of local alignment with its input sequence lengths, seem to be met for commonly used structure weights, e.g. the default weight 200. On the other hand, this does not rule out detrimental overestimation of boundaries due to a score bias. To quantify this, we first determined the expected length of local alignments between random RNA pairs; this was determined for LocARNA in default setting as well as for sequence-only setting. This length indicates the expected overestimation of real ncRNA alignments, even if their genomic context is completely unrelated. At the current default setting (structure weight of 200) almost half of the genomic context could be added to the aligned local motif. When we destroy sequence and structure homology in the instances of LocalBRAliBase by shuffling (Section 2), we still observe longer alignments for increasing structure weight ( Supplementary Fig. S5 ). In this benchmark, the input sequences have relatively large and diverse lengths of typically about 250–300 nt composed of 200 nt context and the ncRNA sequence. Thus, instead of directly comparing sequences from the shuffled benchmark, we generate an artificial dataset of RNAs of length 100, having the same nucleotide distribution (Section 2). As can be seen in  Figure 3 , sequence-only score does produce (on average) an alignment of length 10 or roughly 10% of the input. Given a typical length of 50–100 (see length distribution of BRAliBase ncRNAs in  Supplementary Fig. S8 ) for most of the ncRNAs, we would expect that the predicted boundaries are extended by 10–20 nts over the real boundaries. For high structure weight, however, more than 60% of the random sequences are covered by a local alignment, which implies that we do not have any chance to detect the real boundaries of ncRNAs even in shuffled genomic context.
 Fig. 3. The expected lengths of subsequences in local alignments strongly grow with increasing structure weight for artificially generated random sequence pairs (blue, box). In contrast, local sequence-only alignments produce much shorter alignments, without having any influence of the structure weight (dashed line). The expected lengths ( y -axis) are estimated as average lengths of the predicted local LocARNA alignments over different structure weights ( x -axis) using randomly generated RNA sequences of length 100 (see Section 2). The sequence-only scoring predicts a local average alignment of length around 15 nts, regardless of the structure weight. A structure weight of 300 already leads to local sequence–structure alignments covering about 60% of the input sequences 3.4 Emphasizing structure improves alignment accuracy but seriously overestimates boundaries of local alignments Sequence–structure alignment tools were introduced to overcome the limitations of sequence-based approaches in the detection of conserved structures ( Gardner  et al. , 2005 ). Thus, a high structure weight would be desirable for a correct detection of conserved structures. However, the length of local alignment of random sequences already suggest that we usually cannot find the true transcript boundaries using a high structure weight as it would likely extend a real ncRNA into the genomic context. While sequence alignment does not share these problems, resorting to sequence-only alignment (or setting LocARNA’s structure weight very low; see  Fig. 4 ) does not resolve the dilemma. Not picking up structure similarity can lead to bad alignment quality or even a complete miss of the local alignment, specially, for sequences with low sequence identity (see  Fig. 4 , bottom).
 Fig. 4. Using a higher structure weight improves the local alignment quality but also leads to an extension of the alignment into the context. The local alignment performance metrics (sensitivity and specificity) are shown for the LocalBRAliBase dataset (top) and the subset filtered for APSI less than 70% (bottom). There is a trade-off between covering the motif and restricting the alignment to the motif boundaries. A high structure weight helps to predict the complete local motif (top left), but it also leads to an extension into the context (top right). For a low structure weight the situation is vice versa. For sequences with lower APSI, the effect of not finding the complete or just parts of the motif is stronger (bottom left versus top left). However, the ability to detect the context for a respective structure weight is less different comparing lower APSI sequences to the complete k2 LocalBRAliBase (right) To investigate this further, we determined the agreement of the positions covered by the local alignment with the actual transcript boundaries using the LocalBRAliBase. Note that we do not investigate here how good the actual alignment is. Finding the correct transcript boundaries, however, is the necessary step to get a good alignment as we could re-align the detected transcripts using global alignment in a following step. For the purpose of measuring the agreement of transcript boundaries and regions detected by local alignment, we investigated the sensitivity and specificity of the predicted alignment areas. The sensitivity measures the fraction of positions that are correctly predicted according to a reference alignment (with optimal value 1), whereas the specificity measures how much of the context is not aligned (with optimal value 1). An example can be found in  Supplementary Fig. S2 . Taking sensitivity and specificity into account will reveal alignments which are extended into the context, but also too short alignments and is therefore a helpful boundary detection measurement. As  Figure 4  shows, a high structure weight does detect the full transcript (sensitivity close to 1) but tend to align into the context (indicated by a lower specificity). The initial arguments of this section, that a structure weight is needed to detect the ncRNA is clearly proved by looking at the sequences with lower sequence identity. With structure weight 100 on average less than half of the ncRNA is found. Sequence-based scoring or not adding a positive structure contribution, on the other hand, does fail to detect the correct transcript. See structure weight 0. 3.5 Global and local alignment require fundamentally different parameter sets In the previous experiment, we varied only the single parameter weighting the structure. While we see that this does not allow to simultaneously meet both objectives, accurate alignment and accurate boundaries in local alignment, we cannot rule out yet that such reconciliation is possible due to the complex interplay of the structure weight with the other essential parameters, which control the affine gap cost (gap opening and extension) and the importance of sequence similarity at structural matches (tau factor). For directly studying such potential effects, we apply machine learning techniques as a tool to illuminate this apparent contradiction from the perspective of optimal parameter settings. This allows us to simultaneously optimize these parameters for their suitability to reproduce reference alignments from a benchmark set. In this way, we determined independent optimal parameter sets for global alignment as well as for local alignment (Section 2). While we learn parameters for global alignment directly form the k2 dataset of BRAliBase, we employed the derived local benchmark set from LocalBRAliBase for optimizing the parameters for local alignment (Section 2). The optimized parameter sets are shown in  Table 1  together with LocARNA’s current default values for comparison. Interestingly, already our optimization results for global alignment on the k2 dataset differ quite substantially from LocARNA’s default values, since such parameters were never systematically optimized before. While the structure weight was apparently chosen well for aligning k2 sequences, the optimal gap opening cost is considerably higher, but compensated by much lower gap extension costs. We assume that such shift of gap extension costs to opening costs is also beneficial for aligning most other RNAs with LocARNA. Moreover, on k2 it turned out advantageous to more strongly consider sequence similarity at the ends of matched base pairs (higher tau factor), where originally, we had set the influence of sequence to zero to avoid penalizing compensatory mutations.
 Table 1. Optimized parameters differ strongly for global and local LocARNA alignments. Parameter set Gap extension (γ) Gap open (β) Structure weight (ω) Tau factor (τ) Default values 350 500 200 0 Global optimized 68 807 210 72 Local optimized with no penalty (λ = 0) 136 975 115 38 Local optimized with penalty (λ = 15) 82 883 176 71 
 Note :  However, after introducing a position-wise penalty in LocARNA, optimal parameters for local alignment (with penalty) turn out to be very close to the trained parameters for global alignment. Compared to LocARNA’s default values, the parameter optimization makes gap extensions cheaper and gap opening more expensive. To improve local alignments, the structure–weight is strongly decreased compared to the optimal weight for global alignment (which is nearly the same as the default weight). In the same way, the trained tau factor for local alignment is smaller than for global alignment. The other parameters are changed by the training for either local or global alignments in the same way (compared to default parameters). Using a position-wise penalty λ of 15 for the local alignment prediction, the optimized parameters are almost equal to the optimized parameters for global alignments. All parameter training was performed using the black box optimizer SMAC (see Section 2). Example LocARNA calls are given in the  Supplementary Material . More importantly for our study, comparing the optimized parameters for local and global alignments, we see most striking differences. Apparently, even allowing variations in the relevant other parameters, a common ideal parameter set that works equally well for local and global alignment cannot be found. As discussed before, a high structure contribution in local alignments leads to alignment extensions into the context and will produce a high number of wrong alignment edges. The structure weight is nearly halved, as is the tau factor. As the tau factor is another part of the structure scoring, it is conceivable the tau factor and structure weight are just correlated. Furthermore, the gap costs are significantly increased. Higher gap costs help to avoid aligning similar regions that are spread of a longer region and therefore most likely do not belong to the same local motif. Therefore, we hypothesize that the optimized parameters for local alignment limit the bias due to the structure contribution as to avoid strong overestimation of boundaries. Note that there is no corresponding pressure for global alignment. It seems that almost the only resolution is to lower structure weight (and tau factor). At the same time, as the optimization for global alignment (and the results from our previous experiment) indicate, this tends to lower the alignment accuracy of the locally aligned subsequences. 3.6 Position-wise penalty as an alternative to reduced structure weight Previous results suggest that even the optimized parameters for local alignment are forced into an unfortunate compromise between sensitivity and specificity that cannot be satisfyingly resolved. Moreover, the large deviation between optimized parameters for the local and global alignments do cause additional problems for tools like RNAz ( Gruber  et al. , 2010 ) that rely on statistics over local and global alignments, since for such tools, different parameterization lead to different statistics distributions. In this work, we are first of all interested to systematically study the potential issues of using Sankoff-like scoring in local alignment. However, we suggest a simple modification of the score, which has the potential to compensate such issues. This can yield final direct evidence, as well as suggests more sophisticated solutions to the problem. We start with hypothesizing that, on average, there is a length-dependent shift of the score due to the structure contribution. Already  Heyer (2000)  observed that local alignment cannot work properly, if the expected alignment score grows linearly with alignment length. While the expected length of proper local alignments should grow at most logarithmically with the input lengths, such a dependency would cause linear growth. This motivates us to subtract a position-wise penalty from the score; in order to compensate the hypothesized linear dependency of score’s on the input length due to the structure contribution. Concretely, we penalize each position in the locally aligned subsequences; to require only a single additional parameter, all positions are penalized equally. We first tested, using a simple grid search for the penalty, the effect of penalty on the alignment quality for structure weights 100 and 200. We have chosen these values as they were roughly the result of the parameter optimization for local and global alignment, respectively, see  Figure 5 . Our results in  Figure 6  support the expected effect of the penalty on the sensitivity and specificity, namely increasing specificity on the cost of sensitivity. However, as seen in  Figure 5 , in both cases the penalty is required to improve the overall alignment quality. The optimal penalty for structure weight 100 is 5–10, for 200 it is 15–20. Similar optimal and near-optimal performance was obtained for penalty 15 in our family-wise analysis of the six largest RNA families with structure weight 200 that is provided in  Supplementary Figure S13 . To get further insides on how the penalty is acting, we investigated the F1 distribution for the complete k2 LocalBRAliBase and two filtered sets of the LocalBRAliBase, see  Supplementary Figure S9 .
 Fig. 5. The compensating effect of position-wise penalties. The boundary detection (sensitivity + specificity) is shown for different position-wise penalties using global optimized parameters and structure weight 200. For higher penalties almost nothing of the context will be aligned (right, specificity). On the other hand, a high penalty leads partly to not finding the complete local motif (left, sensitivity). This result can be matched to the trade-off issue of finding the correct alignment boundaries, shown in  Figure 4 . Using high structure weight for alignment predictions will result in finding the complete motif but it also leads to an extension into the context. Now, for a high structure weight (200), already at penalty 15 most local motifs are still fully covered, but most of the predictions are not extended into the context. This shows how the bias due to the structure contribution can be compensated by using a position-wise penalty Fig. 6. Position-wise penalties can improve the average alignment quality of local alignments (as measured by maxSPS). For the entire k2 LocalBRAliBase (left) and a subset of low sequence similarity (APSI &lt; 70, right), we plot the average maxSPS for different position-wise penalties λ ( x -axis). We show our results for two different structure weights, the default value of ω = 200 and ω  = 100, which are close to our optimization results for global and local alignments. The plots demonstrate that the position-wise penalty can compensate for the bias of using a higher structure weight. A position-wise penalty of about 15 allows using the high structure weight of 200, an optimal value for the global alignment. Hence, the penalty allows applying a single set of optimal parameters for global and local alignment—as we argue in the text, this is beneficial on its own This shows that overall the penalty allows to improve the alignment quality, even more for higher structure weights (200). Notice that, if the dataset is filtered for a specific subset, e.g. sequences with APSI below 70 or with a SCI above 100 the optimal position-wise penalty value is shifting. For sequences with a lower sequence identity a structure weight around 5 seem to be optimal. However, looking at very structured sequences a penalty of 15 gives again an optimal result. Therefore we came to the conclusion that a penalty around 15 should be desirable, which is then also used for the second local parameter optimization (see  Table 1 , line 4). This allows the use of identical parameter set for global and local alignment, which is particularly beneficial for tools that rely on statistics over global and local alignments (like RNAz; as discussed before). Finally comparing the current default LocARNA parameter setting and our now improved, combination of global optimized parameters and the position-wise penalty, shows that we can drastically improve the local alignment prediction of LocARNA. Comparing the medians of optimized and default parameter settings ( Fig. 7 ), it is notable that using the default parameter setting for more than half of the sequences the alignment prediction failed completely. This is substantially improved when using the optimized parameter settings with penalty 15. If the alignment boundaries would be perfectly predicted we could achieve a median of 0.95 ( Supplementary Fig. S10 ), however the current improvement is already astonishing.
 Fig. 7. Improvement in local alignment quality by using the suggested parameter setting over the LocARNA default parameter settings. The suggested parameter set is the optimized parameters for global alignment ( Table 1 ) combined with a position-wise penalty of 15. The structure weight has been rounded to 200. We observe a considerable improvement, the median maxSPS is increased from 0.11 to 0.60. An example comparing the default and suggested scoring can be found in  Supplementary Figure S14 4 Conclusion In this work we systematically studied the comparison of ncRNAs by local (local SA&amp;F, which is highly demanded for the important challenge of identifying homologues from the plethora of identified RNA transcripts with unknown function. Over the years, several approaches for local SA&amp;F have been proposed and there have been attempts to improve local alignment prediction and to counteract alignment elongation beyond motif boundaries. A prominent example is Foldalign ( Havgaard  et al. , 2005 ), which limits the maximum length of motifs. However, the fundamental issues of local SA&amp;F as discussed in this work were neither investigated in depth before nor quantified systematically. In the first step, we designed a new local alignment benchmark set (LocalBRAliBase) together with an appropriate novel local alignment quality measure (maxSPS). This benchmark set is essential for studying local alignment and constitutes a valuable contribution by itself, since it enables proper assessment of local RNA alignment method performance for the first time. With this benchmark set at hand, we could empirically confirm our initial hypothesis that the structure prediction component in SA&amp;F introduces a bias to the total similarity score. The intuitive explanation for this bias is that the structure contribution of SA&amp;F is purely positive by definition. The bias was shown to be sufficiently high in current tools to compromise the prediction of correct alignment boundaries. Even worse, we showed that current scoring schemes do not allow to properly balance the overestimation of alignment length (i.e. misprediction of boundaries) against the sensitivity for detecting structural homology. By varying only the relative weight of the structure component in the scoring, we demonstrated this effect clearly: sufficient emphasis on structure is required to identify local motifs, however this compromises the accuracy of boundary prediction at the same time. As wrong parameterization of the scoring could still be the source of the problem, we subsequently used machine learning to optimize the alignment parameters, finding that the divergence between accuracy of boundary and structure detection cannot be resolved by re-parametrization. Surprisingly, we found that a position-wise penalty could completely resolve the problem, yielding the same set of parameters for global and local SA&amp;F alignment. This has the further benefit that downstream tools like RNAz for predicting ncRNAs do not have to differentiate between local and global scoring. In summary, we clearly showed that the current SA&amp;F scoring schemes are not directly applicable to local SA&amp;F. Moreover, we identified the positive structure contribution to the alignment score as the major bias source for overestimating the alignment boundaries. Finally, we presented a constructive solution of the observed issues through applying a position-wise penalty. By raising awareness in the community, precise identification of the issues, and pointing out viable solutions, this work constitutes an important step towards reliable local SA&amp;F approaches in future studies. Supplementary Material btaa431_Supplementary_Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Mixed-model coexpression: calculating gene coexpression while accounting for expression heterogeneity</Title>
    <Doi>10.1093/bioinformatics/btr221</Doi>
    <Authors>Furlotte Nicholas A., Kang Hyun Min, Ye Chun, Eskin Eleazar</Authors>
    <Abstract>Motivation: The analysis of gene coexpression is at the core of many types of genetic analysis. The coexpression between two genes can be calculated by using a traditional Pearson's correlation coefficient. However, unobserved confounding effects may cause inflation of the Pearson's correlation so that uncorrelated genes appear correlated. Many general methods have been suggested, which aim to remove the effects of confounding from gene expression data. However, the residual confounding which is not accounted for by these generic correction procedures has the potential to induce correlation between genes. Therefore, a method that specifically aims to calculate gene coexpression between gene expression arrays, while accounting for confounding effects, is desirable.</Abstract>
    <Body>1 INTRODUCTION The analysis of gene coexpression patterns has been of great interest in recent years due to the widespread availability of microarray datasets measuring thousands of genes. Gene coexpressions, evaluated by comparing the expression patterns of pairs of genes, have been utilized in order to identify loci responsible for regulating genes ( Ghanzalpour  et al. , 2006 ;  Lee  et al. , 2006 ), to evaluate the significance of known pathways ( Subramanian  et al. , 2005 ) and to identify functionally related genes whose relationships have been conserved through evolution ( Stuart  et al. , 2003 ). Unfortunately, gene expression data can be largely affected by technical bias such as a batch effects or plate effects ( Johnson  et al. , 2007 ). Such non-biological effects have been shown to induce correlations between genes. For example,  Balázsi  et al.  (2003 ) showed that spacial placement of microarray probes affected the correlation between gene expression patterns, causing genes to be more or less correlated depending on the proximity of their respective probes on the array. More generally, unobserved factors affecting gene expression have the potential to cause correlation between genes. When these factors are shared between gene expressions, they cause genes to have similar patterns of overall variation. Since these effects are not directly observed they are not incorporated into statistical models. The shared variation between genes is attributed to biological causes. This issue is referred to as expression heterogeneity and has been acknowledged as a general problem when analyzing expression datasets ( Leek and Storey, 2008 ). The detrimental effects of technical confounding on the results obtained from microarray analysis are well known.  Qiu  et al.  (2005 ), while examining the effects of stochastic dependence between arrays on the correlation of test statistics used in determining differential expression, noted that the correlation structure of arrays induced through non-biological effects can lead to spurious correlation between genes. They note that microarray normalization procedures mitigate such phenomenon, but are unable to completely negate them. In fact, the presence of spurious correlations is a general problem that arises when analyzing many types of noisy high dimensional biological datasets, and has been examined in many different contexts ( Clarke  et al. , 2008 ). In the context of gene coexpression, the cause of spurious correlations can be conceptualized by viewing a set of  n  microarrays measuring  m  genes as a  m × n  matrix. In this matrix, we expect that the microarrays represented by the columns are independent and that some of the rows, representing the genes will be correlated, indicating biological relationships. In the presence of technical confounding effects, such as batch effects, the columns will share characteristics that will cause the overall patterns of expression to be similar and thus the arrays will be statistically correlated. This increased correlation between columns induces correlation between rows, as it becomes more likely that two randomly selected rows will be correlated, given that the overall patterns of expression for each array are similar. In this way, the correlation between arrays, or inter-sample correlation, has the potential to induce correlations between genes. Many methods have been developed that aim to remove confounding effects from gene expression data. For example, in the case of known batch effects, a method such as ComBat ( Johnson  et al. , 2007 ) may be employed. ComBat ( Johnson  et al. , 2007 ) uses an empirical Bayes approach to estimate parameters associated with batch and produces corrected gene expression data. This corrected expression data can then be used in subsequent analysis. Unfortunately, technical confounding such as a batch effect may not be easily observable. In this case, a method that is able to identify possible confounding effects without prior information is of interest. For example, surrogate variable analysis (SVA) ( Leek and Storey, 2007 ) is a method for correcting gene expression data in the absence of known confounding effects. In SVA, a set of surrogate variables are estimated and regressed out of the expression data. These surrogate variables represent the unknown confounding effects that cause expression heterogeneity. Expression heterogeneity is expected to be encoded by the inter-sample correlation matrix, which is the matrix representing the correlation between all pairs of arrays. Surrogate variables are estimated by iteratively weighting a subset of the principal components of this matrix. The SVA method is aimed at the general problem of correcting gene expression data and does not specifically target the problem of calculating pairwise gene correlations. Furthermore, SVA only utilizes the principal axes of the inter-sample correlation matrix in order to correct expression. We can reason that the full inter-sample correlation matrix contains more information than its principal components and therefore SVA is only utilizing a subset of the available correlation information in its correction procedure. When the patterns of confounding are complex, the estimated surrogate variables may not capture all of the structure encoded in the inter-sample correlation matrix and as a result the corrected expression data may contain residual correlation. In this article, we propose a method for calculating pairwise gene correlations that utilizes the full inter-sample correlations matrix in order to correct for expression heterogeneity. Our proposed method, mixed-model coexpression (MMC), uses a linear mixed-model framework in order to adjust gene expression values and calculate pairwise gene correlations. Linear mixed-model frameworks have been successfully used in previous studies to remove confounding effects when performing eQTL analysis ( Kang  et al. , 2008 ). The MMC procedure represents confounding as a random effect in a statistical model for coexpression. This approach allows us to more accurately calculate coexpression while removing the effects due to confounding. Unlike ComBat, our method does not require previous knowledge about the batch effects. MMC is also able to calculate coexpression without assuming and estimating some finite number of confounding effects, such as with SVA. These two properties give our method the advantage of being able to represent a wide range of unknown effects. A caveat to our approach, as well as other approaches that utilize the inter-sample correlation matrix as a surrogate for confounding, is the potential to remove true biological signal, as expression heterogeneity may be caused by true biological effects. Consider one transcription factor whose activity marks the beginning of many possibly unrelated pathways. When this transcription factor exhibits high activity, the genes involved in the downstream pathways will appear to be highly differentially expressed. This high level of differential expression will cause the downstream genes to be statistically correlated. When this master regulator affects hundreds or even thousands of downstream genes, the global patterns of array variation become similar and thus arrays appear to be correlated. This correlation is represented in the inter-sample correlation matrix and is utilized in the correction procedure. Correlations between genes induced by such large-scale biological effects are not differentiable from correlations induced through large-scale technical confounding effects and therefore our method will ‘correct’ for both types of induced correlations. In this way, it is possible for our method or a method such as SVA that utilizes the inter-sample correlation matrix to remove a true biological signal. However, this caveat can also be a useful side effect, as the goal of many coexpression analyses is to find groups of genes that are tightly functionally related. Large-scale effects, whether true biological effects or technical confounding, may hinder the ability to find smaller gene modules. In this sense, our method can be seen as a complementary to current coexpression methods that identify large modules. In order to evaluate MMC, we take advantage of the fact that microarrays contain many more probes than measured genes and that expression patterns for probes measuring the same gene should be among the most highly correlated within the set of all probes. We compare methods for computing coexpression by comparing their ability to highly rank these probe pairs in terms of correlation. Our results show that MMC is able to rank these pairs more highly when compared to SVA and a traditional Pearson correlation. We evaluate our method further by utilizing replicate gene expression datasets. We utilized two yeast gene expression datasets ( Brem  et al. , 2002 ;  Smith and Kruglyak, 2008 ) produced by the same lab, covering the same strains and same genes but produced 5 years apart using different microarray platforms. We applied our method to both datasets and show that it is able to produce coexpression results which are more concordant when compared to both traditional Pearson and SVA corrected coexpressions. Finally, we consider how coexpressions may be used in order to identify biologically meaningful groups of genes. Under the assumption that genes working together in the same complex or pathway should be highly coexpressed, we examined coexpression values for sets of genes belonging to known functional categories. Given a set of known gene functional modules, we evaluated the ability of MMC coexpressions to identify these modules as biologically significant. Compared to both the traditional Pearson correlation and with SVA corrected coexpressions, we show that MMC has higher power to detect biologically meaningful gene sets. 2 METHODS We first highlight the relationship between a traditional Pearson's correlation coefficient and a basic linear model. We demonstrate the mathematical connection between the Pearson correlation and hypothesis testing under a linear model, and use this intuition when developing the MMC. 2.1 Pearson correlation as a linear model The coexpression between two genes is often estimated by using the traditional Pearson correlation coefficient. The Pearson correlation gives an absolute value ranging from 0 to 1. If the absolute value of the correlation is close to 1, then we say that the pair of genes is significantly coexpressed. The threshold for significance is usually domain dependent and set on a case by case basis. The Pearson correlation can be calculated for any two genes,  y 1  and  y 2 , by using Equation ( 1 ).
 (1) 
In this case,  y 1  and  y 2  are both gene expression vectors of size  n  and the Pearson correlation is the ratio of the their sample covariance to the product of their sample standard deviations. Here   and   are the sample means for each gene. The Pearson correlation can be represented concisely using matrix notation.
 (2) 
We use  1 n  to represent a  n ×1 vector of 1s. When the elements of  y 1  and  y 2  are sampled IID from a bi-variate normal distribution and are truly uncorrelated, the following relation holds ( Weatherburn, 1949 ).
 (3) 
where  t  has a Student  t -distribution with  n −2 degrees of freedom. In order to test the hypothesis that  r P =0, we test the equivalent hypothesis that  t =0, while evaluating  t  using the observed  r P . To understand how this relationship arises, let us consider the general purpose of the correlation coefficient. The correlation coefficient gives a measure of how linearly related one variable is to another. Another way to evaluate the linear dependence between two variables is by adopting a linear regression framework. Within this framework, the linear dependence between two variables is tested by first defining a linear model, in which one variable is used as a predictor of the other variable, which is called the response. We evaluate the magnitude of the linear dependence, by testing the hypothesis that the predictor variable has no effect on the response variable. With this in mind we define the two following linear models, in which we assume that each gene is a function of its mean, some random error and the observed expression value of another gene. We use ŷ i  to represent the observed gene expression vector for  y i .
 (4) 
 (5) In order to evaluate the significance of the effect that gene 1 has on gene 2, we test the hypothesis that β 2 =0 in the model in Equation ( 5 ). Under the null hypothesis that β 2 =0, we have that the computed  t -statistic follows a central Student  t -distribution with  n −2 degrees of freedom ( McCulloch and Searle, 2001 ). The computed  t -statistic is a function of both the estimate   and the sample variance for  y 1 . Through a series of algebraic manipulations we can show that the computed  t -statistic has the relationship observed in Equation ( 3 ) with the Pearson's correlation ( Rao, 1973 ). We briefly summarize this relationship here as follows.
 (6) 
 (7) 
 t 1  and  t 2  correspond to the  t -statistics computed for the estimates of β 1  and β 2 , respectively. Equation ( 7 ) shows that there is a direct relationship between the Pearson correlation and a linear model of the type in Equations ( 4 ) and ( 5 ). Under the null hypothesis, we assume that both  t 1  and  t 2  asymptotically follow the  t -distribution. Implicit in this assumption is the assumption that the variance of the residuals  e 1  and  e 2  is of the form σ e 2 I . More specifically, we assume that both  y 1  and  y 2  are normally distributed with means μ 1  and μ 2 , respectively, and variances σ 1 2 I  and σ 2 2 I , respectively. When these assumptions do not hold, such as when the residuals are not independent, we might experience overdispersion of the test statistics ( McCulloch and Searle, 2001 ). In other words, the variance of the test statistics will be greater than expected and thus our assumed null distribution will be incorrect. This phenomenon has been observed in many cases, for example, when the effects of population structure are not accounted for when computing association statistics ( Devlin  et al. , 2001 ). Since overdispersion leads to inflation of test statistics and the Pearson correlation coefficient is directly proportional to the  t -statistics for the models in Equations ( 4 ) and ( 5 ), this implies that overdispersion may lead to inflation of the Pearson correlation. 2.2 Coexpression as a linear mixed model In the previous section, we illustrated the relationship between a traditional Pearson's correlation and a linear model. We concluded from this relationship that when the variance of the residuals is misspecified, we have the potential to observe overdispersion, which leads to inflation of the test statistics and subsequently the Pearson's correlation. In the presence of expression heterogeneity, we expect that shared confounding between arrays will make them correlated. When this is the case, we no longer expect that the residuals for the models in Equations ( 4 ) and ( 5 ) will be independent. Therefore, the assumption of independent residuals is incorrect and this misspecification might lead to overdispersion and subsequently to inflation of the Pearson's correlation. One way to deal with overdispersion is to account for the source of overdispersion with a random variable ( McCulloch and Searle, 2001 ). Therefore, we propose the following two linear models that have an additional random variable, which accounts for confounding effects.
 (8) 
 (9) In these models, we assume that  var ( e 1 )= var ( e 2 )=σ e 2 I ,  var ( u 1 )= var ( u 2 )=σ u 2 K  and that  cov ( e i , u j )=0 ∀   i , j , where  K  represents the inter-sample correlation matrix. Given a set of  n  arrays each measuring  m  genes, we define the inter-sample correlation matrix as the  n × n  sample covariance matrix for the  m × n  matrix of the complete array data. In other words, the matrix  K  is a matrix containing all pairwise covariances for all pairs of arrays. The key assumption here is that the additional variance due to systematic confounding effects is proportional to the correlation between arrays. When gene 1 and gene 2 are truly uncorrelated (β 1 =β 2 =0), the Pearson's correlation should be zero. However, when the models in Equations ( 8 ) and ( 9 ) hold, the observed Pearson's correlation will be inflated due to correlation between the elements of  u 1  and  u 2 . Subtracting the true values of  u 1  and  u 2  from  y 1  and  y 2 , will produce corrected vectors for which the observed Pearson's correlation will not be inflated. However, the true values of these variables are unknown and in order to obtain estimates of them, we would have to make further assumptions and restrictions on the model. Instead, we only make an assumption about the distributions of both  u 1  and  u 2 . With knowledge of these distributions, we estimate the total variance of  y 1  and  y 2 . Under the null hypothesis, β 1 =β 2 =0, we have the following.
 (10) 
 (11) 
where
 When the gene expression vectors follow the distributions in Equations ( 10 ) and ( 11 ), the traditional Pearson's correlation will be inflated due to overdispersion. That is, when computing the Pearson's correlation, we assume that  Σ =σ c 2 I , for some σ c 2 . In order to remove the effects of overdispersion in each gene expression vector, we need to transform the gene expression vectors so that they have the same variance–covariance structure assumed when computing the Pearson's correlation. Then using these transformed vectors we apply the definition for a traditional correlation coefficient. This is accomplished by utilizing the following rule, which is applicable to random variables with a multivariate normal distribution with mean μ and positive semi-definite variance–covariance matrix Σ ( Kariya and Kurata, 2004 ).
 (12) 
 (13) 
Using this rule we obtain the distribution for  y 1 *  and  y 2 *  defined as follows.
 (14) 
 (15) 
When the Pearson's correlation is calculated in this transformed space (i.e. using the transformed vectors), we expect that the assumptions of independent residuals will hold and thus the correlation will not be subject to inflation. Given the true Σ and the observed gene expression vectors  y 1  and  y 2 , we transform the observed vectors and calculate a corrected Pearson's correlation.
 (16) 
We expect that this adjusted correlation coefficient will have a mean of zero when gene 1 and gene 2 are uncorrelated. Simplifying we obtain the following.
 (17) 
We are not given the true values of the means μ 1  and μ 2  or the true value of  Σ . In order to calculate  r MMC  between two given gene expression vectors, we must estimate these parameters from the data. Substituting the estimates for μ 1 , μ 2  and  Σ , we arrive at the final formula.
 (18) 
The  t -statistics corresponding to the βs from the models in Equations ( 8 ) and ( 9 ) maintain the relationship illustrated in Equation ( 6 ), while  r MMC  has been substituted for  r P . In order to determine the value of  r MMC , we must first determine the value of  . This means that we need to estimate the two variance components,   and  . When estimating only one variance component, the estimates are obtained analytically through a maximum likelihood (ML) or restricted maximum likelihood (REML) approach. However, there does not exist a general analytical method for estimated more than one variance component. Therefore, we must incorporate a numerical search strategy in order to obtain optimal estimates. Such solutions are computationally intensive. In order to estimate these variance components, we employ a method described by ( Kang  et al. , 2008 ). This method reduces the computational complexity at each search step from  O ( n 3 ), using the basic Newton–Raphson algorithm, to  O ( n ) by re-formulating the problem so that the singular value decomposition of  K  can be reused. The method combines grid search with the Newton–Raphson algorithm and can be applied, in order to find the optimal variance components. For each pair of genes,  i  and  j , we use the numerical search method to find the optimal estimates for the variance components  i σ e 2 ,  i σ u 2 ,  j σ e 2  and  j σ u 2 . We use the left subscript to identify the gene for which the component belongs to. For example,  i σ e 2  and  i σ u 2  are estimated using the model for gene  i  [refer to Equations ( 8 ) and ( 9 )]. Using the estimated variance components, we obtain   and  , the variance–covariance matrices for the models corresponding to  y i  and  y j . These variance–covariance matrices are used to obtain the observed MMC coexpression values corresponding to gene  i  and gene  j ,  i r MMC  and  j r MMC . Ideally, these MMC coexpressions are equal. However, in practice this is not the case, so we average them to define the final corrected correlation  r MMC , in order to ensure the symmetry of coexpression. It is also possible to calculate  r MMC  by using the corrected vectors   and   and then applying the definition of the Pearson's correlation from Equation ( 16 ). When  , we found the solution to be very concordant with that obtained by averaging  i r MMC  and  j r MMC . 3 RESULTS 3.1 Prioritizing probe pairs targeting the same gene In order to evaluate the ability of MMC to prioritize true coexpressions, we leverage the fact that microarrays typically contain many more probes than there are genes to measure, meaning that most genes are targeted by more than one probe. We assume that the expression levels of any two probes targeting the same gene should be highly correlated, and thus when ranked against all other pairwise coexpressions, these probe pairs should be among the most highly ranked. We compare the relative ranking of coexpressions for probes targeting the same gene between different methods, in order to determine which method is better able to prioritize strong coexpressions. It may be noted that when certain forms of alternative splicing occur or when some genes are simply not expressed, the results of this evaluation strategy may fail to differentiate the methods for calculating coexpression. We utilized a set of 732 probe pairs obtained from the Human HapMap gene expression arrays ( International HapMap Consortium, 2003 ). The gene expression data represents 60 unrelated individuals of European descent ( http://www.ncbi.nlm.nih.gov/projects/geo/query/acc.cgi?acc=GSE6536 ). The probe set corresponds to those probes that target known RefSeq genes and that could be coupled with at least one other probe targeting the same gene. For each probe pair, we calculate the MMC coexpression, the traditional Pearson correlation and an SVA corrected Pearson correlation. Each expression value is ranked with respect to all pairwise coexpressions for each method. Smaller ranks indicate higher coexpression. We expect that when examining the coexpression ranks for the set of 732 probes, the method that performs best should have an abundance of low ranks. Figure 1  shows the distribution of the coexpression ranks obtained with each method. The total number of genes considered was over 26 000, meaning that there were over 300 million pairwise coexpressions (26 000 choose 2) to consider. Subsequently, there are over 300 million possible rankings for each coexpression. Each method places ~96% of the 732 probe pairs within the top 1 million ranks. The figure shows that the MMC method consistently ranks these probe pairs higher than either of the other methods. For example, MMC places 79 of the 732 probe pairs within the top 100 ranks, while SVA and Pearson place only 63 and 76, respectively. In the top 10 000 ranks, MMC places 216 probe pairs, while Pearson and SVA place only 177 and 191. If we assume that each of the 732 probe pairs should have a correlation of 1, then their ranks should be in the top 732 choose 2. MMC places 415 of the 732 probe pairs within this range, while Pearson and SVA place only 370 and 366, respectively. These results suggest that MMC is more accurately calculating the coexpressions of these probe pairs, which we assumed to be truly coexpressed.
 Fig. 1. The distributions of coexpression ranks for a set of 732 probe pairs, for which both probes in a pair target the same gene. The coexpression values for each probe pair are ranked with respect to all other pairwise coexpression values. Smaller ranks indicate higher coexpression. We expect that probes targeting the same gene should be highly coexpressed and therefore should have very low rank. The MMC method consistently ranks these coexpressions lower when compared to the other two methods. 3.2 Concordance between replicated data sets Replicate datasets are great resources to use in order to validate experimental findings. When considering coexpression, we expect that genes found to be highly coexpressed in replicate dataset 1 would also be highly coexpressed in replicate dataset 2. However, due to confounding effects, we may observe a high level of discordance between coexpressions found using two separate replicate datasets. Methods that remove confounding may alleviate this problem and cause coexpressions to be more concordant between replicate datasets. In order to evaluate the performance of MMC in this respect, we obtain two yeast gene expression datasets produced by the same lab and measuring the same genes over the same strains of yeast but conducted 5 years apart ( Brem  et al. , 2002 ;  Smith and Kruglyak, 2008 ). For both datasets, we calculate coexpressions using MMC, traditional Pearson and SVA corrected Pearson. We then compare the concordance of coexpression values between the two datasets. In order to compare coexpression values between two replicate datasets, we compare their relative rankings and compute the proportion that are common. We are considering a total of 6143 genes, so there are over 18 million gene pairs and thus over 18 million coexpressions. We expect that the most highly coexpressed genes will be the same within both datasets. Given this, we define a measure of concordance between two datasets in which we calculate the proportion of genes that are common within the top  n  most highly ranked coexpressions. For example, consider the top 100 coexpressions from dataset one, we might see that of these coexpressions only half appear in the top 100 when considering dataset two. In this case, we determine that the proportion in common is 50% for  n =100. By calculating the proportion in common for every  n , we obtain a concordance at the top (CAT) plot, as shown in  Figure 2 .
 Fig. 2. Comparison of the concordance between two yeast datasets for both methods Concordance between two sets of coexpressions is compared by looking at the proportion of coexpressions in common for the top ranking coexpressions. The  x -axis represents the number of top ranked coexpressions considered, while the  y -axis represents the proportion of those coexpressions that are common between the new and old dataset. The CAT plot in  Figure 2 , illustrates the differences between concordance for each of the methods considered. Ideally, at each point on the  x -axis the  y -value would be 1, meaning that 100% of the coexpressions would be in common. Although, this is not the case, we do see that both MMC and SVA are concordant ~30–40% of the time when considering the top 200 ranks. However, when considering the ranks ranging from 300 to 50 000, our method out performs both methods by estimating coexpressions which are concordant 20–40% of the time. This result strongly suggests that MMC is more effective in removing confounding effects which may cause coexpressions to be discordant across datasets. 3.3 Gene module significance One intention behind the calculation of coexpression is to quantify the strength of the biological relatedness between genes. For example, if two genes code for signaling proteins that act together in one particular pathway, we expect that these genes will be expressed together and that their coexpression value will be quite high. If we assume that the coexpression between two genes reflects the strength of their biological relationship, it is possible to utilize coexpressions in order to predict how biologically relevant a group of genes may be. Consider a group of genes that all code for proteins that work together in a complex. It is reasonable to assume that each pair of these genes will be coexpressed. In this case, by comparing each of the pairwise coexpressions for the genes within this group we should see an abundance of significant coexpressions. In general, we can assume that a group of genes that are functionally related should all be significantly coexpressed. We can then use this assumption to test the significance of a group of genes in order to determine if it is biologically relevant. In practice, by using such an approach we will likely find many groups of genes which will appear to be biologically relevant, but in fact their high level of inter-coexpression is due to confounding. We tested our ability to detect biologically significant groups of genes using MMC coexpressions. We define the statistic found in Equation ( 19 ), which is simply the sum of the logged coexpression ranks.  rank ij  represents the relative ranking of the coexpression between gene  i  and gene  j , with respect to all other pairwise coexpressions. When genes within a group are highly coexpressed, the value of this statistic will be high and when they are not it will be very low. To obtain a set of gene groups which are known to be functionally related, we chose to use yeast, as it has some of the most well-characterized genes. The MIPS comprehensive yeast genome database contains detailed functional data for all yeast genes ( Mewes  et al. , 1999 ). We used this resource in order to construct 233 gene modules ranging in size from 2 to 20. Modules were chosen such that the number of modules was maximized while the modules in each size category did not overlap. We chose sizes of 2–20, assuming that smaller modules would represent more closely functionally related gene sets and thus the overall coexpressions within modules would be higher. For each of the 233 modules, we calculated the statistic  T  using coexpressions estimated with MMC, traditional Pearson and SVA corrected Pearson. We estimate the null distribution for  T  under each method and each module size  n , by repeatedly selecting  n  random coexpressions and calculating the statistic  T . Each null distribution was approximated with 1 million values. Using this null approximation we calculated  P -values for each known module.
 (19) Figure 3  shows the distribution of the  P -values for all modules. Module  P -values obtained when using our method tend to be smaller than the Pearson and SVA module  P -values. For example, ~40% of the tested gene modules were significant at a level of 0.05 when using MMC, while ~25% and 30% were significant when using Pearson and SVA, respectively. This result suggests that MMC is able to produce coexpression values which were better able to predict real biological relationships.
 Fig. 3. Distribution of gene-module  P -values for Pearson, SVA and MMC. We used a set of 233 known functional modules consisting of sets of genes of size 2 to 20. For each of these modules, a  P -value representing the biological significance is calculated. This figure plots the distributions of these  P -values. Since the  P -values were calculated for gene sets known to be functionally related, we expect that there should be an inflation of significant  P -values. It can be seen that the MMC method produces a larger number of significant  P -values when compared to both the traditional Pearson and SVA-corrected coexpressions. 4 DISCUSSION In this article, we present a statistical model for the calculation of gene coexpression called MMC. Our method calculates gene coexpressions that are robust to confounding effects. We calculate the coexpression between two genes by utilizing a mixed-model framework. Unknown confounding effects are represented as a random variable in a mixed-model formulation of coexpression. We use the inter-sample correlation to estimate the variance of the random variable representing unknown confounding and incorporate this variance into the model of coexpression. We compare the coexpressions obtained with our method with those obtained using the traditional Pearson correlation and those obtained using SVA corrected expression data. Although, rank based correlation methods, such as the Spearman correlation, have been used to reduce the prevalence of spurious correlations due to deviations from assumptions of normality in expression data, we have observed in practice that the Spearman correlation coefficient performs similarly to the Pearson when comparing with MMC (data not shown). When probe pairs target the same gene, we expect that their coexpressions will be highly ranked when compared with all other pairwise coexpressions. For probe pairs of this type, MMC is shown to produce coexpressions that are more highly ranked when compared with the other two methods. We also show that MMC produces coexpressions that are more concordant across replicate datasets generated by the same lab using the same strains but generated at different times. Operating under the assumption that biologically and functionally meaningful groups of genes will be highly coexpressed, we create a simple statistic which is used to assess the functional significance of groups of genes. Our method shows increased power to discover sets of genes which are known to be biologically significant. Although our method is able to calculate coexpression while removing the effects of confounding, it might also remove effects which are biologically meaningful. Technical confounding effects, such as a batch effect, typically have a global effect on the data. That is, these effects will increase the expression variation in a large number of genes. This shared variation within genes causes them to appear to be significantly coexpressed. Our method estimates global patterns of shared variation through the inter-sample correlation and effectively removes the effects causing the variation from the calculation of coexpression. A problem arises when we consider the case in which one gene has a large biological effect on hundreds of other genes. The effect that master regulators have on expression data as a whole is indistinguishable from the unwanted global confounding effects. That is, the variation in gene expression caused by master regulators quite closely resembles patterns of variation caused by confounding effects and will therefore be removed by our method. In this case, MMC may over-correct true biological signal and cause true coexpressions to be lost. The drawback to our method may also be seen as a beneficial side effect. When master regulators target many genes, traditional coexpression analyses employing clustering will yield many large sized gene modules. By removing the effects of master regulators, MMC essentially enables coexpression clustering analysis to produce smaller gene modules conditional on the large modules. Large gene modules discovered through the use of standard coexpression analysis may be seen as representative of large-scale cellular functionality. Small modules discovered through clustering with MMC will be subsets of these large modules. By intersecting results, it may be possible to more fully understand the detailed circuitry of the cell. Funding:   National Institute of Health  training grant  T32-HG002536  (to N.F.).  National Science Foundation  (No.  0513612 , No.  0731455  and No.  0729049 ) (to N.F., H.M.K., C.Y., E.E.); and  National Institutes of Health  ( 1K25HL080079  and  U01-DA024417 );  Samsung Scholarship, the National Human Genome Research Institute  (Grants No.  HG00521401  to H.M.K.);  National Institute for Mental Health NIMH  No.  NH084698 , and  GlaxoSmithKline  (in part). UCLA subcontract of contract  N01-ES-45530  from the  National Toxicology Program/National Institute of Environmental Health Sciences to Perlegen Sciences  (in part). Conflict of Interest : none declared. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Episo: quantitative estimation of RNA 5-methylcytosine at isoform level by high-throughput sequencing of RNA treated with bisulfite</Title>
    <Doi>10.1093/bioinformatics/btz900</Doi>
    <Authors>Liu Junfeng, An Ziyang, Luo Jianjun, Li Jing, Li Feifei, Zhang Zhihua, Gorodkin Jan</Authors>
    <Abstract/>
    <Body>1 Introduction Post-transcriptional modifications in RNAs have drawn much attention in recent literature ( Chi, 2017 ), as rapidly growing evidence has suggested that reversible RNA modifications may be a new layer of epigenetic regulation in gene expression ( Zhao  et al. , 2017 ), and its disruption may lead to life-threatening disease like cancer ( Frye  et al. , 2016 ;  Popis  et al. , 2016 ). 5-Methylcytosine is a type of chemical modification on the nucleotides that can be found in both DNA and RNA. To identify transcriptome-wide m 5 C, several advanced techniques have been employed, including high-throughput sequencing of RNA treated with bisulfite (RNA-BisSeq) ( Schaefer  et al. , 2010 ;  Squires  et al. , 2012 ), RNA m 5 C-RNA immunoprecipitation (RIP) ( Jayaseelan  et al. , 2011 ), 5-azacytidine-mediated RNA immunoprecipitation (Aza-IP) ( Khoddami and Cairns, 2013 ) and methylation individual-nucleotide-resolution crosslinking and immunoprecipitation (miCLIP) ( Hussain  et al. , 2013a ,  b ). Among these, RNA-BisSeq is considered the gold standard for RNA m 5 C ( Amort  et al. , 2017 ;  Hussain  et al. , 2013a , b ;  Squires  et al. , 2012 ;  Yang  et al. , 2017 ). Hereinafter in this article, m 5 C only refers to RNA m 5 C, unless otherwise indicated. The transcriptome-wide function of m 5 C is just starting to be investigated ( Edelheit  et al. , 2013 ;  Hussain  et al. , 2013a, b ;  Shelton  et al. , 2016 ). It is clear that m 5 C is distributed widely over protein coding and non-coding RNAs in human ( Squires  et al. , 2012 ) and mouse ( Amort  et al. , 2017 ), as well as enriched in CG-rich regions and the initiation site of coding regions ( Yang  et al. , 2017 ). RNA m 5 C is conserved from bacteria to mammals and plants, and its functions have been suggested to include structural and metabolic stabilization and translational regulation ( Blanco  et al. , 2014 ;  Burgess  et al. , 2015 ;  Chen  et al. , 2019 ;  Gabriel Torres  et al. , 2014 ;  Popis  et al. , 2016 ;  Schaefer  et al. , 2010 ;  Schwartz  et al. , 2013 ;  Squires  et al. , 2012 ;  Yang  et al. , 2019 ). At the transcription level, alternative splicing is a predominant contributor to the diversity of protein types in higher eukaryotic cells ( Black, 2003 ). Any changes in this regulated process could result in severe phenotypes ( Pan  et al. , 2008 ). However, both the distribution of m 5 C among the transcript isoforms and its functional relevance to the splicing programme have been barely investigated. In order to study the relationship between m 5 C and splicing, a tool that is able to quantify m 5 C at isoform level is necessary. However, to the best of our knowledge, such method has yet been found in literature. There are several tools for RNA-BisSeq or Aza-IP data analysis ( Bormann  et al. , 2018 ;  Legrand  et al. , 2017 ;  Liang  et al. , 2016 ;  Rieder  et al. , 2016 ). meRanTK is a toolkit composed of tools for RNA-BisSeq read mapping, methylation calling and differentially methylation identification ( Rieder  et al. , 2016 ). BS-RNA maps and annotates RNA-BisSeq data with more attention on the ‘dovetailing’ reads ( Liang  et al. , 2016 ). BisRNA considers the possible artifact that may be stochastically introduced by the experiment ( Legrand  et al. , 2017 ). BisAMP is more specific to targeted RNA m 5 C analysis ( Bormann  et al. , 2018 ). However, none of above tools supports the analysis of RNA-BisSeq data at isoform level. To address this issue, we developed a probabilistic model to quantify Epitranscriptomal RNA m 5 C at the transcript isoform level (named Episo), which utilizes single-nucleotide resolution m 5 C data from RNA-BisSeq. Episo consists of three tools:  mapper ,  quant  and  Bisulfitefq , for mapping, quantifying and simulating RNA-BisSeq data, respectively. Both  in silico  and wet experiments showed that the prediction of Episo is highly accurate. By applying Episo to recently published m 5 C data ( Yang  et al. , 2017 ), we generated the first transcript isoform level m 5 C profiles for HeLa cells and four mouse tissues. We found that the distribution of m 5 C in the transcript isoforms was remarkably dissimilar to random shuffled data, suggesting that there may exist a latent regulatory layer for m 5 C at isoform level. 2 Materials and methods Reference genome and transcriptome for human and mouse, version GRCh37 and GRCm38, respectively, were downloaded from the Ensembl database ( Yates  et al. , 2016 ). The RNA-BisSeq data were downloaded from the BIG Data Center under accession number PRJCA000315 ( Members, 2017 ). Episo consists of three tools:  mapper ,  quant  and  Bisulfitefq , for mapping, quantifying and simulating RNA-BisSeq data, respectively ( Fig. 1 ). Before reads mapping, Episo has all low-quality bases and adaptor sequences removed by Cutadapt ( Martin, 2011 ).
 Fig. 1. The Episo pipeline. ( A ) The mapping procedure. Incoming RNA-BisSeq reads are mapped to reference genome and transcriptome. The output methylation file contains two columns that represent mapped fragments and methylation pattern. The symbols Z, X and H represent cytosines in CpG, CHG and CHH, respectively, whereas H can be A, C or T. The upper- and lowercase letters represent methylated and unmethylated cytosines, respectively. ( B ) The quantification procedure. For any given cytosine site, the total reads that cover the site and the reads that carry methylated cytosine at the given cytosine site are denoted as R and R′, respectively The  mapper  maps RNA-BisSeq reads to the reference genome and reference transcriptome. We adopted the mapping strategy used in Bismark to map RNA-BisSeq reads with modifications ( Krueger and Andrews, 2011 ). First, all RNA-BisSeq reads were C-to-T and G-to-A transformed, and the resultant data were denoted as BSC-T and BSG-A, respectively. Second, the reference transcriptrome was also C-to-T and G-to-A transformed, and the transformed references were denoted as RefC-T and RefG-A, respectively. Last, the four types of mapping (BSC-T versus RefC-T, BSC-T versus RefG-A, BSG-A versus RefC-T and BSG-A versus RefG-A) were performed by Bowtie (version 1.1.2, see  Fig. 1 ). The uniquely mapped reads, i.e. those that were uniquely mapped to a genome locus in at least one of four above mappings, but not necessarily mapped to a unique transcript, were used in subsequent processes. The  quant  quantifies m 5 C level at transcription isoform level from RNA-BisSeq data. The  quant  consists of two steps. The first step estimates transcription level from RNA-BisSeq data. To accomplish this,  quant  constructs a virtual RNA-seq dataset, i.e. for all RNA-BisSeq reads that contain unmethylated cytosines,  quant  transforms them back to their native cytosine states. With such virtual RNA-seq data,  quant  estimates gene transcription level using third party tools, which has two choice in current implement, i.e. Tophat (version 2.1.0)/Cufflink(version 2.2.1) ( Trapnell  et al. , 2009 ,  2010 ) and Kallisto (version 0.44.0) ( Bray  et al. , 2016 ). The users can also replace them with any favorite tools easily by making input data format acceptable for Episo. For example, Episo takes fragments per kilobase of transcript per million mapped reads (FPKM) as the default input format, while the output of Kalliato is estimated transcripts per million (TPM). So, we need to convert TPM to FPKM using the following formula.
 (1) TPM i = FPKM i ∑ j FPKM j × 10 6 We present the performance and data analysis using the results from Tophat/Cufflink in this article; however, the results are essentially similar when using Kalliato. The second step of  quant  estimates the RNA m 5 C level at each putative methylation site in the isoforms. We define the methylation rate at global, isoform and single-nucleotide levels as follows. The global methylation rate is the proportion of cytosine sites that have been methylated in all examined RNAs. This rate is estimated by directly counting the unconverted cytosines in RNA-BisSeq data. The methylation rate at isoform level is defined as  R m, iso / R iso ,  R m, iso  ⊆ R iso , where  R m, iso  denotes the RNAs that carry at least one methylated cytosine site, and  R i s o  denotes all RNAs of the given isoform  iso . The methylation rate of a single-nucleotide at the level of a given set of isoform(s) is defined as  R m, c / R c ,  R m, c  ⊆ R c , where  R m, c  denotes the RNAs of the given isoform(s) from the methylated cytosine sites, and  R c  denotes all RNAs of the given isoform(s) that carry this cytosine site. To estimate the RNA m 5 C rate at isoform level, one needs to estimate the probability that a read  r ∈ R  was generated from a given isoform  t ∈ T , where  R  denote a set of RNA-BisSeq reads and  T  denote a set of isoform(s). Let’s denote this probability as  P(r, t).  One way to calculate this probability was showed by Trapnell  et al.  (2009),
 (2) P r ,   t = ρ t l ˜ t ∑ u ∈ T ρ u l ˜ u F l t r l t - l t r + 1 where  ρ t  denotes the proportion of reads that were generated from isoform  t ,  l t   denotes the length of an isoform,  l ˜ u   denotes the effective length of an isoform,  l t r   denotes the implied length of  r , assuming it originated from isoform  t  and  F  denotes the distribution function of read length .  The first term in the above formula (2) is the probability that a read selected at random originates from isoform  t  (denoted as  Pr  ( trans = t )), and the second term is the conditional probability that read  r  was observed when it originated from isoform  t . The effective length of an isoform is defined as
 (3) l ˜ t = ∑ l ( t ) i = 1 F i l t - i + 1 Therefore, the likelihood function, the maximum likelihood estimate and the 95% confidence interval of  ρ t  can be easily derived from formula (2). It is the  ρ t  and the 95% confidence interval needed for Episo; however, users can take these two values from any third party tools. In the current release of Episo, the users can either choose Kallisto (version 0.44.0) ( Bray  et al. , 2016 ) or Tophat (version 2.1.0)/Cufflink(version 2.2.1) ( Trapnell  et al. , 2010 ) for these two inputs. Now, we estimate the RNA m 5 C level for a given isoform  t  (denoted as  M t ). Let  R ′ denotes the reads that carry at least one methylated cytosine, i.e. unconverted cytosines after bisulfite treatment. We define the relative transcription level of methylated isoform, i.e. the proportion of reads from isoform  t in R ′ is denoted as  ρ ¯ t . Following logic identical to that in formula (2), we can calculate the maximum likelihood estimation and 95% confidence interval of  ρ ¯ ' t . To simplify the derivation, we assumed the independence between transcription and post-transcriptional modification, i.e.  ρ ¯ t ⊥ ρ ¯ ' t . M t  can be estimated according to the delta method, as
 (4) M t = m n ρ ¯ ' t ρ ¯ t + ρ ¯ ' t σ t 2 ρ ¯ t 3 where  m  and  n  denote the number of reads in  R ′ and  R , respectively,  ρ - t  and  ρ - ' t  denote the estimated  ρ t  and  ρ ′ t , respectively, and  σ t 2  is the variance of  ρ - t . Moreover,  quant  can estimate the m 5 C level for a subset of isoforms of a given gene. Let  B  be a subset containing  k  isoforms from gene  A ; then, the RNA m 5 C level in  B  can be estimated, as
 (5) m n ∑ k t = 1 ρ ¯ ' t ∑ k t = 1 ρ ¯ t + ∑ k t = 1 ρ ¯ ' t ∑ k t = 1 σ t 2 ∑ k t = 1 ρ ¯ t 3 Finally, we estimate the RNA m 5 C level for a given cytosine site. Let  R ¯  denote the reads that cover the given cytosine site, and let  R ¯ '  denote the reads that carry methylated cytosine at the given site. Following logic identical to that in formulas (2)–(5), the RNA m 5 C of a given cytosine site on an isoform or a subset of isoforms can be estimated. The  Bisulfitefq  simulates bisulfite treatments, and the sequencing process were simulated by the FluxSimulator with default parameters ( Montgomery  et al. , 2010 ).  Bisulfitefq  consists of  Bisulfitefq -reads,  Bisulfitefq -fragment and  Bisulfitefq -fragment-multirate. The  Bisulfitefq -reads component generates RNA-BisSeq data with a given global m 5 C level, i.e. it randomly transforms a given proportion of cytosines to thymines in the input reads. The  Bisulfitefq -fragment component transforms all cytosines in a given proportion of randomly selected reads into thymines. The  Bisulfitefq -fragment–multirate component can then simulate an epitranscriptome with multiple m 5 C levels between isoforms. It can take up to three m 5 C levels as input parameters. For each m 5 C level, it transforms all cytosines in the given proportion of randomly selected reads from one assigned isoform into thymines. For example, let a gene A have three isoforms, namely A1, A2 and A3, while P1, P2 and P3 are three m 5 C levels we want to simulate for the isoforms, respectively. The  Bisulfitefq -fragment–multirate transforms all cytosines with 1-P1, 1-P2 and 1-P3 of randomly selected reads from isoform A1, A2 and A3, respectively, into thymines. Gene ontology (GO) analysis of RNA m 5 C-containing genes was performed using DAVID ( Dennis  et al. , 2003 ), and the sequence motifs of the RNA m 5 C sites were discovered using MEME ( Bailey  et al. , 2009 ). The human cervical carcinoma cell line HeLa are cultured in DMEM high glucose medium (Hyclone, SH30243.01) with 10% FBS (Biowest, S1580-500) and 1% Penicillin–Streptomycin (Corning, 30002283). Cell identity was verified by STR analysis (DNA fingerprinting), and mycoplasma contamination was regularly tested for cell cultures. The regions spanning nt 914–1465 of  E.coli  16S rRNA sequence was amplified by PCR with a forward primer harboring a T7 promoter sequence . 0.5 µg production was used as template for in vitro transcription with MEGAScript Kit (Promega, P1440) according to the protocol. The transcribed RNA was treated 15 min at 37°C with 1 µl RQ1 RNase-free DNase and purified using RNA clean kit (ZYMO REASEARCH R1015) following the protocol. The spike-in RNA was subpackaged and stored in RNase-free water at -80°C. Following the manufacturer’s recommendations, about 3 × 10 7  HeLa cells were treated with 6 ml TRIzol Reagent (Invitrogen, 15596026) for total RNA isolation. Isolated RNA was then subjected to two rounds of poly(A) RNA enrichment using fresh Dynabeads (Ambion, 61006) and treated with 1U of DNase І (Thermo Scientific, 00383793) for 15 min at 37°C. The mRNA was cleaned using RNA clean kit (ZYMO REASEARCH R1015) and concentration was determined in nanodrop (Thermo Scientific, Nanodrop 1000) by measuring absorbance at 260 and 280 nm. Thirty microgram purified mRNA was mixed with 50 ng spike-in RNA. The mixture was divided equally into three portions. Four microgram anti-m 5 C antibody (1.36 µg/µl, Diagenode, C15200081), 2 µg random 25nt oligonucleotides and 50 µl Dynabeads Protein G (Novex, 10007D) was incubated in 300 µl IP buffer [10 mM Tris–HCl pH 7.5, 150 mM NaCl, 0.05%Triton-X(v/v)] at 4°C for 2 h on a rotating wheel. The same procedure was performed using Mouse IgG (1 µg/µl, Diagenode, C15400001) as control. We used 250 µl IP buffer to wash bead–antibody complexes three times, then added RNA mixture and finally brought to 250 µl with IP buffer. The mixtures were incubated at 4°C overnight (&gt;12 h) with 1 µl RNasin (Invitrogen, N8080119) on a rotating wheel. The RNA–antibody–beads complexes were gentle washed three times using IP buffer and incubated in 300 µl elution buffer (5 mM Tris–HCl pH 7.5, 1 mM EDTA, 0.05%SDS, 80 µg Proteinase K) for 1 h at 50°C. After removing beads complexes, the eluted production was cleaned with RNA clean kit (ZYMO REASEARCH R1015). We repeated the beads–antibody incubation, RNA-complexes incubation and RNA–antibody–beads complexes elution processes for five times. The supernatant was collected in a new tube and cleaned with RNA clean kit (ZYMO REASEARCH R1015). The supernatant concentration was determined in nanodrop by measuring absorbance at 260 and 280 nm. Five tubes of cleaned eluted MeRIP production and 300 ng supernatant RNA were reverse transcribed using SuperScript III reverse transcriptase (Invitrogen 18080-093). Each tube contains 11 µl RNA solution, 1 µl dNTPs and 1 µl random 6 bp Hexamers. The tubes were incubated at 65°C for 5 min and 0°C at least 1 min. Four microlitre 5× First strand buffer, 1 µl 0.1 M DTT, 1 µl RNasin and 1 µl SuperScript III reverse transcriptase were mixed and added the mixture into each tube. The tubes were incubated at 55°C for 60 min followed by 70°C for 15 min. The reverse transcribed products were stored at -20°C. The spike-in RNA was used as internal unspecific binding control to normalize binding of RNA between IgG control and m 5 C sample. Five tubes of reverse transcribed MeRIP products were mixed and measured for the enrichment using 2 - ΔΔCt  method by comparing the anti-m 5 C sample with the anti-IgG sample. Data were expressed as the expression of target genes relative to the spike-in control in the anti-m 5 C sample compared with the anti-IgG sample. PCR primers can be found in the  Supplementary Table S1 . If our model is correct, we should expect
 Predicted   MR   rate   ˜   Observed   MR   rate   where
 Predicted   MR   rate = m 5 c   level   from   isoform   A m 5 c   level   from   isoform   B Observed   MR   rate = observed   m 5 c   level   in   isoform   A observed   m 5 c   level   in   isoform   B = #   m 5 C   counts   from   isoform   A # IgG   counts   from   isoform   A # m 5 C   counts   from   isoform   B # IgG   counts   from   isoform   B = FCmeRIP ( isoform   A ) FCmeRIP ( isoform   B ) 3 Results We present an RNA-BisSeq data analysis package, named Episo, to quantify m 5 C at the transcript isoform level. Episo consists of three tools: named  mapper ,  quant  and  Bisulfitefq , for mapping, quantifying and simulating RNA-BisSeq data, respectively ( Fig. 1 ). The detailed algorithm can be found in the Section 2. 3.1  In silico  assessment of Episo To  in silico  assess the performance of Episo, we simulated paired-end RNA-BisSeq data with three global methylation rates, 0.1, 1 and 10% (about 23 million 101-bp length paired-end reads for each methylation rate) using  Bisulfitefq . Comparing to meRanTK, the mapping rates of  mapper  were consistently higher at all three methylation rates tested (86.6% versus 80.72%). Next, we assessed the accuracy of  quant  at global and single-nucleotide level. The  quant  accurately estimated the relative global RNA m 5 C rates at all nucleotide contexts, i.e. CpG, CHG and CHH, and at all the three methylation rates ( Supplementary Table S2 ). Because Episo, to the best of our knowledge, is the first computational tool that enables isoform level quantification of m 5 C, there are no existing methods to compare with. Thus, to further assess the accuracy of  quant  at isoform level, we simulated RNA-BisSeq data with 10, 5 and 1% m 5 C isoform level methylation ( Fig. 2A ). These three methylation levels were examined because most methylated transcript isoforms were estimated carrying 1–10 percent m 5 C in real samples ( Fig. 2B ). Because the m 5 C level is so low, when gene expression level is also low, the data would be too noisy to made meaningful predictions, e.g. the average differences are 0.2076, 0.4211 and 0.4765 compared with simulated levels of 10, 5, 1%, respectively ( Supplementary Fig. S1 ). We only considered the transcripts that have sufficient expression level, i.e. fragments per kilobase per million reads (FPKM) &gt;2 in our tests. We found that the average differences between estimated and simulated m 5 C levels were minor (the average differences are 0.0070, 0.0013 and 0.0006 compared with simulated levels of 10, 5, 1%, respectively,  Fig. 2A ). To assess the accuracy of  quant  at single-nucleotide level, we simulated RNA-BisSeq data with 60, 40 and 10% level single-nucleotide m 5 C methylation. These three methylation levels were tested because most methylated cytosines have an m 5 C level between 10 and 60% in real samples ( Fig. 2C ). In this dataset, we also found that the average differences between the estimated and simulated RNA m 5 C levels were nearly zero (the average differences are 0.0267, 0.0113 and 0.0013, when the simulated levels are 60, 40 and 10%, respectively,  Fig. 2A ). Last, Episo was found to predict isoform m 5 C levels specifically and sensitively in tested methylation level ( Supplementary Fig. S2 ).
 Fig. 2. The accuracy of Episo and the distribution of estimated m 5 C levels in human cells and mouse tissues. ( A ) The distribution of estimation errors of Episo. The difference of m 5 C levels between the simulated and Episo estimated data is shown at the resolution of RNA isoforms and single cytosine. At both resolutions, the comparisons were made at three m 5 C levels that covered the main range of estimations in real data from human cells and mouse tissues as shown in ( B ) and ( C ). (B) The distribution of estimated m 5 C levels at the resolution of RNA isoforms. In the mouse tissues tested, the m 5 C level is significantly higher in brain than that in the other three tissues. The ‘***’ indicates significant difference of  P -value &lt; 0.001. (C) The distribution of estimated m 5 C levels at the resolution of single cytosine. For both metrics, we found no significant different among mouse tissues tested. ( D ) Experiment design for PIK3R2. The unique exons for isoform PIK3R2-002 and PIK3R2-004 are marked in pink shadow and the primers design are indicated as red arrows. The unique methylated cytosine is marked with red flag. See  Supplementary Figure S3  for PRKCA and TUBGCP2. ( E ) The comparison of the predicted MR ratio with the observed MR ratio. The hollow points and horizontal whiskers represent the expectation and 95% confident interval of predicted MR rate, respectively. The Y coordinates for the hollow points are the mean of experimental data. The solid points and vertical boxplots represents the experimental measured MR rates and their distributions of six replicates (three independent experiments with two replicates each), respectively. The diagonal line is  Y  =  X . (Color version of this figure is available at  Bioinformatics  online.) 3.2 Experimental assessment of Episo To experimentally assess the accuracy of Episo’s prediction by MeRIP followed by qPCR ( Fig. 2D  and  Supplementary Fig. S3 ), we applied Episo to RNA-BisSeq data of HeLa cells and predicted m 5 C level of transcripts at isoform level. To distinguish the methylation levels between isoforms easily, we looked for genes satisfied all the following conditions for experimental validation. First, this gene should have and only have one Guanine site that its corresponding cytosine site in the RNA production be methylated. Second, the gene generates at least two isoforms that carrying the methylated site. Third, the predicted methylation level at the site is no less than 0.1 in tested isoforms, because the accuracy of current RNA m 5 C examination technology, i.e. MeRIP were limited when the methylation level is low. Fourth, there is at least one unique exon to enable proper primers design distinguishing the two tested isoforms ( Fig. 2D ). We identified seven sites after filtering ( Supplementary Table S3 ). Finally, we picked three genes (PIK3R2, TUBGCP2 and PRKCA). The rest four genes are excluded from experimental validation because of unsuccessful primer design (STK32C and COPS7A) or having too distinguished expression levels between the isoforms (fold change &gt; 30, RALY and GPAA1). The experimental observed m 5 C levels have consistent trends with the predictions ( Fig. 2E ). We measured the methylation ratio (MR) between the isoforms with three independent experiments (each independent experiment had two replications) using a modified methyl-RNA immunoprecipitation (meRIP) assay (Section 2). The order of average MR ratio in the experiments are PIK3R2 (1.195) &lt; TUBGCP2 (1.701) &lt; PRKCA (2.18), which was rather close to Episo’s predictions of PIK3R2 (1.507) &lt; TUBGCP2 (2.3865) &lt; PRKCA (3.3903) ( Fig. 2E ). However, the 95% confident interval of the predictions are large, particular for the PRKCA, which need to be improved in the further. Nevertheless, our experimental data suggested that Episo can finely predict m 5 C level at isoform level from BisSeq-seq data. Taken together, the assessment using both simulated and wet experimental data suggested that Episo is a fine RNA-BisSeq data analysis tool and it can estimate m 5 C level at global, isoform, exons and single-nucleotide level. 3.3 Uneven m 5 C distribution among tissues at isoform level To explore the distribution of m 5 C level in real samples, we applied  quant  to recently published RNA-BisSeq data in human (HeLa) and mouse (liver, kidney heart and brain). At the isoform level, although the overall m 5 C is low in all the samples tested, it is indeed higher in HeLa cells and the mouse brains than other tissues and the difference is moderate (The median m 5 C level in isoforms of HeLa cells, mouse liver, kidney, heat and brain were estimated as 0.035, 0.027, 0.025, 0.023 and 0.035, respectively) ( Fig. 2B ). Intriguingly, the distribution of m 5 C at single-nucleotide level is undistinguishable in all mouse tissues and HeLa cells ( Fig. 2C ), suggesting the presence of more methylated cytosine sites in HeLa cells and mouse brain compared to other mouse tissues. It is well known that the transcriptome pattern of brain is distinct from other tissues in mouse ( Zheng-Bradley  et al. , 2010 ). Therefore, we asked if the distinct m 5 C pattern in mouse brain stemmed from gene expression level per se. We found a weak negative correlation between RNA m 5 C at isoform level and its expression (Spearman’s R = -0.31,  P  &lt; 2.2e-16) in mouse brain data. However, this weak correlation may only hold for transcripts with relatively low m 5 C level because when we limit the analysis to the region of m 5 C larger than 0.3, the correlation disappeared (Spearman’s  R  = 0.15,  P  = 0.26). A similar pattern can be found in other mouse tissues and HeLa cells ( Supplementary Fig. S4 ). Therefore, the distinct m 5 C pattern in mouse brain and HeLa cells may be largely independent of gene transcription activity. 3.4 A fraction of cytosines are methylated specifically towards certain isoforms and CG dinucleotide Next, we asked whether m 5 C was evenly distributed in genome. If m 5 C, in general, is subjected to a certain regulation, we reasoned that the distribution of m 5 C over isoforms, or between genomic contexts, should deviate from random expectation. Thus, we compared the diversities of m 5 C as estimated by Episo in real data to the random controls, which were simulated with comparable total expression and methylation levels (about 0.1%) using  Bisulfitefq . First, we found that the RNA m 5 C in real data is less evenly distributed over isoforms than random expectation ( Fig. 3A ). We employed the coefficient of variation (CV) to index the diversity. The CV is a statistic measures the diversity of a distribution, and is defined as the ratio of the standard deviation σ to the mean μ of the distribution and a distribution with a larger CV indicates that it is more diverse than distributions with smaller CVs. We compared the distribution diversity of m 5 C over isoforms between real data and randomly simulations. The percentages of mRNAs that carrying m 5 C are comparable between real and simulated control (29, 27, 34, 31 and 40% of mRNAs in the HeLa, liver, kidney, heart and brain, respectively, compared to about 40% in the control,  Fig. 3B ). However, for those methylated mRNA in real data, their diversity is much higher than that in control ( Fig. 3B and C ). The average CV of m 5 C in HeLa, liver, kidney, heart and brain was 2.28, 2.06, 2.04, 2.08 and 1.94, respectively, higher than the expected CV in simulation data (1.34, KS test’s  P -value &lt;2.2e-16 for all the samples tested). The largest CV in simulation was less than three, while the distribution in real tissues was heavily tailed towards larger CVs. For example, 19, 9, 8, 9 and 7% genes for HeLa, liver, kidney, heart and brain, respectively, had CVs larger than 3 ( Fig. 3D ). This results suggest, at least, these fraction of cytosines are methylated specifically towards certain isoforms.
 Fig. 3. Diversity of m 5 C levels. ( A ) The distribution of expression and m 5 C levels between RNA isoforms of the M6PR gene in HeLa cells. The relative FPKMs were calculated as the absolute FPKMs of isoforms divided by the average FPKM of the isoforms of M6PR. ( B ) The proportion of m 5 C-containing and m 5 C-variable genes. The blue bars represent the proportion of genes that have at least one m 5 C- containing mRNA over all protein coding genes in the human HeLa cells, four mouse tissues and simulated data (sim). The red bars represent the proportion of genes with diversity of m 5 C CV &gt; 1 over that of all genes represented by blue bars. ( C ) The proportion of diversity singleton sites (CV &gt; 1) in the genomic context of CG, CHG and CHH. ( D ) The distribution of diversity of m 5 C at isoform and single-cytosine resolution. The orange curve represents the expected distribution of CV as obtained by random shuffling of real data. (Color version of this figure is available at  Bioinformatics  online.) Next, we wonder whether this larger than expected CVs were also true at single-nucleotide level. Because the CV will be zero if a m 5 C site was methylated in one isoform, we divided m 5 C sites into two classes. The two classes, named singletons and multitons, contain the m 5 C sites that be methylated in only one isoform and multiple isoforms, respectively. We noticed that the CVs in multitons are much smaller than control ( Fig. 3C ). The average CV of multitons in HeLa, liver, kidney, heart, brain and control was 0.87, 0.97, 0.99, 0.98, 1.02 and 1.42 (KS test,  P  &lt; 2.2e-16 for all tissues), respectively. This lower than expected CV implies that, if a cytosine is methylated in multiple isoforms, the methylation tends to be even. There are 991, 870, 872, 1261 and 2959 singletons in HeLa, mouse liver, kidney, heart and brain, respectively. We asked if they are biased to certain nucleotide type. By comparing the relative frequencies of m 5 C at three di-/tri-nucleotide (CG, CHH and CHG) contexts between these singletons and control, we detected a strong bias towards CG in all tissue types we tested ( Fig. 3C ). And m 5 C is also enriched at CHH in brain. GO analysis of genes with CV &gt; 0 showed that they were enriched for several post-translational modifications, while motif analysis on singletons showed that they were enriched in the simple repeats region ( Supplementary Figs S5 and S6 ). Together, at the whole gene level, the above analysis implies that the distribution of m 5 C may be isoform-specific, while at single-nucleotide level, it may be more CG specific. 4 Discussion In this work, we described a computational tool, Episo, to quantify the RNA m 5 C at isoform level and single cytosine sites. The ability to distinguish m 5 C level between isoforms of the same gene distinguishes Episo from existing tools, e.g. meRanTK. To the best of our knowledge, Episo is currently the only method with this feature. The accuracy of Episo mainly depends on the expression level of the host gene and the methylation level per se ( Fig. 2 ). Although the expected estimation error is almost zero, variation cannot be completely ignored. Given the low m 5 C level in most real samples, ultra-high sequencing depth would be recommended. A recent work by Zhang and colleagues showed that there might be technical biases in RNA-BisSeq data ( Huang  et al. , 2019 ). Thus, further development is needed, computationally and experimentally, to reduce the variations and the costs in the RNA-BisSeq analysis, respectively. Episo utilizes RNA-BisSeq data for m 5 C estimation, even though the most abundant modification type is N6-methyladenosine (m 6 A) ( Wang  et al. , 2016 ), not m 5 C, in mammals. Episo may not the best to handle m 6 A data, as current assays for m 6 A detection are mostly antibody-based and seldomly reach single-nucleotide resolution ( Zhou  et al. , 2016 ). However, we foresee the use of Episo for m 6 A in the future, most likely by taking advantage of rapidly evolving third-generation sequencing technologies. With the help of the third-generation sequencing technology, chemical modifications on DNA could now potentially be directly identified ( Shendure  et al. , 2017 ). However, the two most popular third-generation sequencing technologies, Pacbio ( Eid  et al. , 2009 ) and Nanopore ( Branton  et al. , 2008 ), suffer from severe sequencing errors. In real practice, data correction using second-generation data is a widely used strategy for both technologies ( Mahmoud  et al. , 2017 ). Thus, when the third-generation sequencing technology has evolved to the point where it can directly identify chemical modifications on RNAs with short reads, Episo might be a powerful tool for quantifying m 6 A and all detectable modifications at isoform level. We applied Episo to RNA-BisSeq data from human HeLa cells and mouse liver, kidney, heart and brain samples. We showed that RNA m 5 C at isoform level tends to be tissue-specific, particularly in brain sample and HeLa cells. Evidence suggested that not all m 5 Cs are randomly methylated, implying a potential layer of regulation in the m 5 C program. The biological significance of low-level chemical modifications detected in transcripts is always a concern ( Agris, 2015 ;  Song and Yi, 2017 ). In general, the present analysis showed that some m 5 C sites do not mimic the distribution from pure stochastic sampling. It is this portion of m 5 C sites that will draw much more attention in future functional investigation. The current version of Episo only takes into account the static m 5 C level in a given sample. If the m 5 C program is indeed involved in important biological processes, dynamic changes of m 5 C level between samples should be expected (Supplementary Text,  Fig. S7 and  Table S4 ). Thus, a quantitative model for differential methylations is needed in the future. Supplementary Material btz900_Supplementary_File Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>SimPhospho: a software tool enabling confident phosphosite assignment</Title>
    <Doi>10.1093/bioinformatics/bty151</Doi>
    <Authors>Suni Veronika, Suomi Tomi, Tsubosaka Tomoya, Imanishi Susumu Y, Elo Laura L, Corthals Garry L, Wren Jonathan</Authors>
    <Abstract/>
    <Body>1 Introduction Protein phosphorylation is a post-translation modification, which plays a vital role in the regulation of many cellular processes including cell cycle, growth, apoptosis and signal transduction pathways .  The leading technology to discover and confirm phosphorylation is tandem mass spectrometry. Due to ongoing developments in enrichment and separation techniques, faster scanning mass spectrometers and data analysis tools, it is now possible to identify tens of thousands of phosphopeptides. Despite its critical importance, phosphopeptide data analysis involves additional unmet challenges compared to analysis of unmodified peptides due to the more complex spectra that are harder to interpret ( Zhang  et al. , 2013 ). In addition to reliable identification of phosphorylated peptides and proteins, information on the exact phosphorylation sites is essential to understand the interaction and regulation of signaling pathways. Given that specific phosphorylation events function as molecular switches ( Vaga  et al. , 2014 ), accurate assignment of the precise site of phosphorylation is of utmost importance. One approach to identify phosphorylation sites involves searching a sequence database followed by analysis using designated localization tools ( Beausoleil  et al. , 2006 ;  Olsen  et al. , 2006 ;  Taus  et al. , 2011 ). Another employs searching a spectral library ( Bodenmiller  et al. , 2008 ;  Hummel  et al. , 2007 ;  Suni  et al. , 2015 ). While it offers better scoring compared to sequence database search scores ( Lam  et al. , 2008 ;  Suni  et al. , 2015 ), the downside of this approach has been the lack of readily-available and highly-accurate reference spectral libraries. Recently we reported a method for phosphosite validation that takes advantage of the sensitivity of spectral library searching by overcoming the lack of phosphopeptide libraries ( Suni  et al. , 2015 ). More specifically, our strategy builds libraries of simulated phosphopeptide spectra based on spectra of unmodified peptides. These simulated phosphopeptide spectra are then used as a reference in the spectral search of observed phosphopeptides. However, the prototype implementation of the simulation method was a command line application, available only for Windows, with no configuration option. Here, we present further development of our approach through a new tool called SimPhospho. It is the only publically available tool for simulation of higher-energy collisional dissociation (HCD) phosphopeptide spectra. Simulated phosphopeptide spectra by SimPhospho in combination with spectral library searching enable high accuracy and confident phosphosite validation in a comprehensive manner. It follows our previously described workflow but is superior to the prototype version in terms of usability, configuration options and performance. These improvements allow us to optimize various conditions of phosphopeptide simulation, as presented in this study. 2 Implementation The SimPhospho software ( Fig. 1A ) was implemented in C++ using components of the Proteowizard project ( Kessner  et al. , 2008 ) and an XML library (Thomason) and includes a user interface based on the Qt framework (The Qt Company). Two XML files ( Keller  et al. , 2005 ) serve as an input to SimPhospho: (i) a .pep.xml file that contains the sequence database search results [e.g. Mascot (Matrix Science), X! Tandem ( Craig and Beavis, 2004 ) or COMET ( Eng  et al. , 2013 )], validated by PeptideProphet ( Keller  et al. , 2002 ), and (ii) an .mzXML file that contains mass spectra. First, SimPhospho processes the .pep.xml file. For every peptide identification that contains serine, threonine or tyrosine residues in its sequence, singly phosphorylated peptide isoforms are created and theoretical masses of fragment ions to be phosphorylated are calculated. These masses are then searched in the corresponding spectrum in the .mzXML file and masses and intensities of the found ion peaks are modified for simulating a phosphorylation as described below. The program outputs two files: (i) a .pep.xml file that contains the sequences and modification sites of phosphopeptides, and (ii) an .mzXML file that contains the simulated spectra of the phosphopeptides.
 Fig. 1. ( A ) Screenshot of SimPhospho displaying the main features of software: simulation options, including ion intensity values, peak match precision, types of ions used for simulation, data filtering switch, as well as output statistics and progress bar. ( B ) Optimization of intensity values of simulated peaks. To determine the optimal default parameters for SimPhospho, we tested different ion intensity combinations for phosphoric acid neutral loss ions and for intact ions compared to original fragment ion intensities in spectra of nonphosphorylated peptides. We saw the largest number of correctly assigned phosphosites at &lt;1% FLR achieved with a combination of 50 and 50% intensities for intact ions and neutral loss ions for pS and pT, and 50% intensities for intact ions for pY. Data for 100 and 10% (pS, pT) and 100% (pY) are given for reference, as this combination was chosen for the prototype program ( Suni  et al. , 2015 ). See  Supplementary Figure S1  for other intensity combinations To demonstrate the performance of the SimPhospho program and to determine the optimal default parameters, we tested different ion intensity combinations for simulating phosphopeptide spectra. In the earlier paper we selected 100% of intensity for phosphoric acid neutral loss ions (e.g. y-H 3 PO 4 ) and 10% for intact ions (e.g. y-ion) compared to original fragment ions for pSer and pThr, and 100% of intensity for intact ions for pTyr. For more details on the simulation rules, refer to ( Imanishi  et al. , 2007 ;  Suni  et al. , 2015 ). When testing the simulation criteria using SpectraST 5.0 on an HCD dataset of synthetic phosphopeptides with known phosphosites ( Suni  et al. , 2015 ), we now observed that the largest number of correctly assigned phosphosites at 1% false localization rate (FLR) was achieved with a combination of 50% intensities ( Fig. 1B ). Other tested combinations are shown in  Supplementary Figure S1 . Users can select which proteins, peptides or scans are used for simulation by adding a .filter text file that lists protein names, peptide sequences or scan numbers of interest. For instance, filtering by scan numbers can be useful when applying identification score cutoffs to the input data. In addition to activating a filter, the other options users can specify are ion types used for simulation (a-, b-, y-ions, ammonia and water losses) and intensities of simulated peaks for intact ions and phosphoric acid neutral loss ions ( Fig. 1A ). The run time is 50 times faster than when using our prototype program ( Suni  et al. , 2015 ): simulating 13 000 phosphopeptides from 4000 peptides takes under one minute on a workstation equipped with an Intel Core i5 CPU, 2.30 GHz, 16 GB RAM, Windows 7, 64-bit. 3 Results SimPhospho is a fast and easy to use tool for simulation of phosphopeptide tandem mass spectra. The program output files can be used directly to build a spectral library using SpectraST ( Lam  et al. , 2008 ) as either stand-alone version or through Trans-Proteomic Pipeline (TPP) ( Keller  et al. , 2005 ). This simulated reference spectral library of phosphosites is suitable for phosphopeptide identification, and for validation of phosphopeptides and phosphosites identified by sequence database search programs. Simulated spectral libraries can be searched by stand-alone SpectraST, in TPP, or in Proteome Discoverer (Thermo Fisher Scientific) using SpectraST node. Visualization of the simulated spectra as well as the search results is possible via TPP viewer or in Proteome Discoverer. The updated version of the software has the following advantages compared to the prototype program: (i) graphical user interface in addition to command line options, (ii) faster data processing, (iii) improved simulation parameters, (iv) optional simulation features, (v) possibility to select a subset of input spectra or peptides to be used for simulation and (vi) cross-platform support. We anticipate that these improvements will facilitate further adoption of this phosphosite validation method, especially in the large scale studies, ultimately leading to fewer false-positive results in the public domain. SimPhospho is available for Windows, Linux and Mac operating systems at  https://sourceforge.net/projects/simphospho/ . The next version of the software is expected to support simulation of spectra of multiply phosphorylated peptides. Supplementary Material Supplementary Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Classification and feature selection algorithms for multi-class CGH data</Title>
    <Doi>10.1093/bioinformatics/btn145</Doi>
    <Authors>Liu Jun, Ranka Sanjay, Kahveci Tamer</Authors>
    <Abstract>Recurrent chromosomal alterations provide cytological and molecular positions for the diagnosis and prognosis of cancer. Comparative genomic hybridization (CGH) has been useful in understanding these alterations in cancerous cells. CGH datasets consist of samples that are represented by large dimensional arrays of intervals. Each sample consists of long runs of intervals with losses and gains.</Abstract>
    <Body>1 INTRODUCTION Numerical and structural chromosomal imbalances are one of the most prominent and pathogenetically relevant features of neoplastic cells (Mitelman  et al. ,  1972 ). One method for measuring genomic aberrations is comparative genomic hybridization (CGH) (Kallioniemi  et al. ,  1992 ). CGH is a molecular-cytogenetic analysis method for detecting regions with genomic imbalances (gains or losses of DNA segments). Applying microarray technology to CGH measures thousands of copy number information distributed throughout the genome simultaneously (Pinkel and Albertson,  2005 ). Raw data from array CGH experiments is expressed as the ratio of normalized fluorescence of tumor and reference DNA. Normalized CGH ratio data surpassing predefined thresholds is considered indicative for genomic gains or losses, respectively. Chromosomal and array CGH data has been an important resource for cancer cytogenetics (Bentz  et al. ,  1996 ; Desper  et al. ,  1999 ; Gray  et al. ,  1994 ; Hoglund  et al. ,  2005 ; Joos  et al. ,  2002 ; Jo Vandesompele  et al. ,  2005 ; Mattfeldt  et al. ,  2001 ). In contrast to the array CGH, the chromosomal CGH results (on which this article is based) are annotated in a reverse  in situ  karyotype format (Mitelman,  1995 ) describing imbalanced genomic regions with reference to their chromosomal location. CGH data of an individual tumor can be considered as an ordered list of status values, where each value corresponds to a  genomic interval  (e.g. a single chromosomal band). The terms  feature  and  dimension  are also used for genomic interval. The status can be expressed as a real number (positive, negative or zero for gain, loss or no aberration, respectively). We use this strategy and represent gain, loss and no change with +1, −1 and 0, respectively.  Figure 1  shows a plot of 120 CGH cases belonging to Retinoblastoma, NOS (ICD-O 9510/3).
 Fig. 1. Plot of 120 CGH cases belonging to Retinoblastoma, NOS (ICD-O 9510/3). The  X -axis and  Y -axis denote the genomic intervals and the samples, respectively. We plot the gain, loss and no-change status in green (light gray), red (dark gray) and white, respectively. An important task in cancer research is to separate healthy patients from cancer patients and to distinguish patients of different cancer subtypes, based on their cytogenetic profiles. This is also known as the classification problem. These tasks help successful cancer diagnosis and treatment. Cancer is currently responsible for about 25% of all deaths (Jemal  et al. ,  2005 ). Early identification of the cancer is often vital for the survival of the patients. For example, colon cancer is 90% curable when it is identified at the early age. Over 500 000 people die each year from colon cancer in the world (El-Deiry,  2006 ). Support vector machine (SVM) is one of the state-of-art kernel based machine learning techniques and has been widely used for the classification of microarray data (Li  et al. ,  2004 ). Choosing or developing an appropriate kernel function greatly improves the performance of SVM (Tan,  2005 ). The frequently used linear kernel function does not exploit the following properties of CGH data and can lead to sub par performance for classification:
 Features in CGH data represent ordered genomic intervals on chromosomes and their values are categorical. Neighboring features are often highly correlated as a point-like genomic aberration can expand to the neighboring intervals This results in a contiguous run of gain or loss status in CGH data (Liu  et al. ,  2006 ) ( Fig. 1 ). It is essential to develop a kernel that takes these properties into consideration. Another related task is  feature selection  that selects a small subset of discriminative features. Feature selection has several advantages for CGH data. First, it reduces the risk of over fitting by removing noisy features thereby improving the predictive accuracy. Second, the important features found can potentially reveal that specific chromosomal regions are consistently aberrant for particular cancers. There is biological support that a few key genetic alterations correspond to the malignant transformation of a cell (Renan,  1993 ). Determination of these regions from CGH datasets can allow for high-resolution global gene expression analysis to genes in these regions and thereby can help in focusing investigative efforts for understanding cancer on them. Existing feature selection methods broadly fall into two categories, wrapper and filter methods. Wrapper methods use the predictive accuracy of predetermined classification algorithms, such as SVM, as the criteria to determine the goodness of a subset of features (Duan  et al. ,  2005 ; Guyon  et al. ,  2002 ; Zhang  et al. ,  2006 ). Wrapper methods based on SVM mostly use the linear kernel that is not suitable for CGH data. Also, they select features in a backward elimination scheme, which is inefficient in determining highly discriminative features and leads to poor predictive performance when a small feature set is selected. Filter methods select features based on discriminant criteria that rely on the characteristics of data, independent of any classification algorithm (Ding and Peng,  2005 ; Yu and Liu,  2004 ). Filter methods are limited in scoring the predictive power of combined features, and thus have shown to be less powerful in predictive accuracy as compared to wrapper methods (Chai and Domeniconi,  2004 ). The classification problem of multiple classes is generally more difficult as compared to the classification of binary classes (Ding and Peng,  2005 ; Li  et al. ,  2004 ). It also gives a more realistic assessment of the proposed feature selection method (Ding and Peng,  2005 ). In this article, we consider the problem of classification and feature selection for CGH data with multiple cancer types. We address the above-mentioned problems and develop SVM-based methods. This article has two important contributions:
 We develop a novel kernel function called  Raw  for CGH data. This measure counts the number of common aberrations between any two samples. We show that this kernel measure is significantly better for CGH data than the standard linear kernel used in SVM-based methods. We develop an SVM-based feature selection method for CGH data called  Maximum Influence Feature Selection  (MIFS). It uses an iterative procedure to progressively select features. In each iteration, an SVM-based model on selected features is trained. This model is used to select one of the remaining features that provides the maximum benefit for classification. This process is repeated until the desired number of features is reached. We extend the MIFS feature selection method described above for multiclass CGH data. In each iteration, a one-versus-all strategy is used to train multiple SVMs with each SVM corresponding to the classification of one class from the others. A radix sort-based approach is used to combine the rankings of remaining features from each SVM into a global ranking. The best feature based on this ranking is added to the selected set. Our experimental results show that the Raw kernel improves the classification accuracy by 7.3% on average over twelve datasets. These datasets are systematically derived from the Progenetix database based on predefined similarity levels and sizes. These datasets will serve as benchmarks for future research on data mining methods for CGH data. We compared our MIFS method to well-known feature selection methods MRMR (Ding and Peng,  2005 ) (filter) and SVM-RFE (Guyon  et al. ,  2002 ) (wrapper) on twelve datasets. The results show that MIFS outperforms both MRMR and SVM-RFE in terms of classification accuracy. The results also show that our methods only need 5% of all features to provide a comparable classification accuracy as compared to all the features. Further, our methods can improve the accuracy by 3.1% using only 10% of the features as compared to using all features. The rest of this article is organized as follows.  Section 2  presents background.  Section 3  discusses the classification problem using SVM and introduces our new kernel function called Raw.  Section 4  proposes our MIFS method based on Raw kernel for multi-class CGH data.  Section 5  discusses our dataset resampling scheme for benchmarking purpose.  Section 6  presents the experimental results and related discussions. We conclude our work in  Section 7 . 2 BACKGROUND Classification aims to build an efficient and effective model for predicting class labels of unknown data. The model is built on the training data, which consists of data points chosen from input data space and their class labels. Classification techniques has been widely used in microarray analysis to predict sample phenotypes based on gene expression patterns. Li  et al.  have performed a comparative study of multiclass classification methods for tissue classification based on gene expression (Li  et al. ,  2004 ). They have conducted comprehensive experiments using various classification methods including SVM (Vapnik,  1998 ) with different multiclass decomposition techniques, Naive Bayes,  K -nearest neighbor and decision tree (Tan,  2005 ). They found SVM to be the best classifier for tissue classification based on gene expression. The problem of feature selection was first proposed in machine learning. A good review can be found at Guyon and Elisseeff ( 2003 ). Recently, feature selection methods have been widely studied in gene selection of microarray data. These methods can be decomposed into two broad classes
 Filter Methods : These methods select features based on discriminating criteria that are relatively independent of classification. Several methods use simple correlation coefficients similar to Fisher's discriminant criterion (Golub  et al. ,  1999 ; Pavlidis  et al. ,  2001 ). Others adopt mutual information (Ding and Peng,  2005 ) or statistical tests ( t -test,  F -test) (Ding,  2002 ; Model  et al. ,  2001 ). Earlier filter-based methods evaluated features in isolation and did not consider correlation between features. Recently, methods have been proposed to select features with minimum redundancy (Ding and Peng,  2005 ; Yu and Liu,  2004 ). The methods proposed by Ding and Peng, ( 2005 ), uses a minimum redundancy—maximum relevance (MRMR) feature selection framework. They supplement the maximum relevance criteria along with minimum redundancy criteria to choose additional features that are maximally dissimilar to already identified ones. By doing this, MRMR expands the representative power of the feature set and improves their generalization properties. Wrapper Methods : Wrapper methods utilize the classifier as a black box to score the subsets of features based on their predictive power. Wrapper methods based on SVM have been widely studied in machine-learning community (Guyon and Elisseeff,  2003 ; Rakotomamonjy,  2003 ; Weston,J.  et al. ,  2000 ). SVM-RFE (Support Vector Machine Recursive Feature Elimination) (Guyon  et al. ,  2002 ), a wrapper method applied to cancer research is called, uses a backward feature elimination scheme to recursively remove insignificant features from subsets of features. In each recursive step, it ranks the features based on the amount of reduction in the objective function. It then eliminates the bottom ranked feature from the results. A number of variants also use the same backward feature elimination scheme and linear kernel. The methods aimed for binary class data use a recursive support vector machine (R-SVM) algorithm to analyze noisy high-throughput proteomics and microarray data (Zhang  et al. ,  2006 ) and a method that computes the feature ranking score from statistical analysis of weight vectors of multiple linear SVMs trained on subsamples of the original training data (Duan  et al. ,  2005 ). For feature selection of multiclass data, Ramaswamy  et al.  used an one-versus-all strategy to convert the multiclass problem into a series of binary class problems and applied SVM-RFE to each binary class problem separately (Ramaswamy  et al. ,  2001 . Fu and Fu-Liu ( 2005 ) also proposed a method based on the one-versus-all strategy. For each binary class problem, they wrapped the feature selection into a 10-fold cross validation (CV) and selected features using SVM-RFE in each fold. They also developed a probabilistic model to select significant features from the 10-fold results. Filter methods are generally less-computationally intensive than wrapped methods. However, they tend to miss complementary features that individually do not separate the data well. A recent comparison of feature selection methods for multiclass microarray data classification (Chai and Domeniconi,  2004 ) shows that wrapper methods such as SVM-RFE lead to better classification accuracy for large number of features, but often gives lower accuracy than filter methods when the number of selected features is very small. 3 CLASSIFICATION WITH SVM Support vector machine is a state-of-art technique for classification (Vapnik,  1998 ). It has been shown to have better accuracy and computational advantages over their contenders (Guyon  et al. ,  2002 ). It has been successfully applied for many biological classification problems. The technique works as follows. Consider a set of points that are presented in a high-dimensional space such that each point belongs to one of two classes. An SVM computes a hyperplane that maximizes the margin separating the two classes of samples. The optimal hyperplane is called decision boundary. Formally, let  x 1 , x 2 ,…, x n  and  y 1 , y 2 ,…, y n  denote  n  training samples and their corresponding class labels respectively. Let  y i ∈{−1, 1} denote labels of two classes. The decision boundary of a linear classifier can be written as  w · x + b =0 where  w  and  b  are parameters of the model. By rescaling the parameters  w  and  b , the margin  d  can be written as  d =2/‖ w ‖ 2  (Tan,  2005 ). The learning task in SVM can be formalized as the following constrained optimization problem:
 
subject to  y i ( w · x i + b )≥1, i =1,2,…, n . The dual version of the above problem corresponds to finding a solution to the following quadratic program: Maximize  J  over α i :
 
subject to  , where α i  is a real number. The decision boundary can then be constructed from the solutions α i  to the quadratic program. The resulting decision function of a new sample  z  is
 
with  w =∑ i α i y i x i  and  b =&lt; y i − w · x i &gt;. Usually many of the α i  are zero. The training samples  x i  with non-zero α i  are called support vectors. The weight vector  w  is a linear combination of support vectors. The bias value  b  is an average over support vectors. The class label of  z  is obtained by considering the sign of  D ( z ). Standard SVM methods find a linear decision boundary based on the training examples. They compute the similarity between sample  x i  and  x j  using the inner product  . However, the simple inner product does not always measure the similarity effectively for all applications. For some applications, a non-linear decision boundary is more effective for classification. The basic SVM method can then be extended by transforming samples to a higher dimensional space via a mapping function Φ. By doing this, a linear decision boundary can be found in the transformed space if a proper function Φ is used. However, the mapping function Φ is often hard to construct. The computation in the transformed space can be expensive because of its high dimensionality. A kernel function can be used to overcome this limitation. A kernel function is defined as  K ( x i , x j )=Φ( x i ) T Φ( x j ), where  x i  and  x j  denote the  i -th and  j -th sample respectively. It really computes the similarity between  x i  and  x j . With the help of kernel function, an explicit form of the mapping function Φ is not required. In our preliminary work, we have introduced a new measure called Raw that captures the underlying categorical information in CGH data (Liu  et al. ,  2006 ). We will discuss how to incorporate it into the basic SVM method. CGH data consists of sparse categorical values (gain, loss and no change). Conceptually, the similarity between CGH samples depends on the number of aberrations (gains or losses) they both share. Raw calculates the number of common aberrations between a pair of samples. Given a pair of samples  a = a 1 , a 2 ,…, a m  and  b = b 1 , b 2 ,…, b m . The similarity between  a  and  b  is computed as  . Here  S ( a i , b i )=1 if  a i = b i  and  a i ≠0. Otherwise  S ( a i , b i )=0. The main difference between  Raw ( a , b ) and  a T · b  is the way they deal with different aberrations in the same interval. For example, if two samples  a  and  b  have different aberrations at the  i -th interval, i.e.  a i =1, b i =−1 or  a i =−1, b i =1, the inner product calculates this pair as  a i × b i =−1 while  Raw  calculates  S ( a i , b i )=0. The similarity value between  a  and  b  computed by Raw is always greater than or equal to the inner product of  a  and  b . We propose to use  Raw  function as the kernel function for the training as well as prediction. Using SVM with the Raw kernel amounts to solving the following quadratic program: Maximize  J  over α i :
 
subject to  . Accordingly, the resulting decision function of a new sample  z  is
 The main requirement for the kernel function used in non-linear SVM is that there exists a transformation function Φ() such that the kernel function computed for a pair of samples is equivalent to the inner product between the samples in the transformed space (Tan,  2005 ). In other words,  Raw ( x i , x j )=Φ( x i ) T Φ( x j ). This requires that the underlying kernel matrix is ‘semi-positive definite’. For given data points  , the kernel matrix can be defined as  . If for all  n , all sets of data points and all vectors  v ∈ R n  the inequality  v T Mv ≥0 holds, then  M  is called  semi-positive definite . We now prove that our Raw kernel satisfies this requirement. The function Φ():  a ∈{1,0,−1} m → b ∈{1,0} 2 m , is defined as follows;
 For example, given  a =[1,1,0,−1], Φ( a ) is computed as Φ( a )=[0,1,0,1,0,0,1,0]. With this transformation, it is easy to see that the Raw kernel can be written as the inner product of Φ( x ) and Φ( y ), i.e.  Raw ( x , y )=Φ( x ) T ·Φ( y ). This is because Raw only counts the number of common aberrations in computing the similarity between two samples (if both the values are 0, they are not counted). We define a 2 m  by  n  matrix  u  whose  j -th column vector corresponds to Φ( x j ), i.e.  u ≔[Φ( x 1 ) Φ( x 2 ) ···]. The Raw kernel matrix can be written as
 Now we have  v T Mv = v T ( u T u ) v =( uv ) T uv =‖ uv ‖ 2 ≥0,∀ v ∈ R n . Therefore, the Raw kernel is semi-positive definite. 4 MAXIMUM INFLUENCE FEATURE SELECTION An important characteristic of CGH data is that neighboring features are strongly correlated ( Fig. 1 ). Selecting these highly correlated features incurs ‘redundancy’ in the feature set. When the number of selected features is small, this ‘redundancy’ can lead to sub par performance for classification. For example, assume that we want to select two features for classification. If the  i th feature is ranked high for well separating samples of different classes, the ( i +1)-th or ( i −1)-th feature are likely ranked high too. However, selecting both  i -th and ( i +1)-th (or ( i −1)-th feature does not improve the classification accuracy significantly because they are redundant in discriminative power. On the other hand, if the  j -th feature improves the classification accuracy when combined with the  i -th feature but has a low ranking, the  i -th and  j -th feature should be selected instead. Wrapper methods based on backward feature elimination, such as SVM-RFE (Guyon  et al. ,  2002 ), are limited in choosing a small set of highly discriminative features. This is because they try to remove features that do not perform well with the remaining set of features. However, this does not imply that the eliminated feature would not work well for the final chosen set of features. Filter methods iteratively add features with the most discriminative power into an existing set. This easily causes redundancy in the selected features. The MRMR method (Ding and Peng,  2005 ) tries to address this limitation by adding features with maximum relevance and minimum redundancy. However, due to the difficulty in selecting complementary features, it often produces lower predictive accuracy as compared to wrapper method. We propose a novel non-linear SVM-based method called MIFS for the classification of multiclass CGH data that addresses the limitations of existing wrapper methods. A simple approach to feature selection is to perform an exhaustive search. Clearly, this is not computationally feasible but for a very small number of features. We use a greedy search strategy to iteratively add features to a feature subset in a similar vein as used by (Guyon  et al. ,  2002 ). The basic approach is to compute the change in the objective function caused by removing or adding a given feature. In our case, we select the feature that maximizes the variation on the objective function. The added feature is the one that has the most influence or gain on the objective function. The feature that has the most influence on the objective function is determined as follows. Let  S  denote the feature set selected at a given algorithm step and  J ( S ) denote the value of the objective function of the trained SVM using feature set  S . Let  k  denote a feature that is not contained in  S . The change in the objective function after adding a candidate feature is written as  DJ ( k )=| J ( S ∪{ k })− J ( S )|. In the case of SVM, the objective function that needs to be maximized (under the constraint 0≤α i  and ∑ i α i y i =0) is:
 For each feature  k  not in  S , the new objective function  J ( S ∪  k ) has to be computed. One option is to compute this gain or influence for each remaining feature  k , by retraining the SVM. However, the computational requirements can be significantly reduced by assuming that the value of α's do not change significantly after the feature  k  is added. Thus, the new objective function with feature  k  added can be defined as:
 
where  x i (+ k ) means training sample  i  with feature  k  added. Therefore, the estimated (this is because we are not retraining the classifier with the additional feature) change of objective function is:
 We add the feature that has the largest difference  DJ ( k ) to the feature set. The above method requires  S  to be non-empty. To jump start the method, the first feature has to be derived. One approach is to compute  J ({ k }) for every feature  k  by training a separate SVM for each feature  k . One can, then, select the feature with the largest value as the starting feature. However, this can be computationally very expensive. Another approach is to use the most discriminating feature (such as done by standard filter-based methods that rank features according to their individual predictive power). The mutual information  I  of two variables  r  and  s  is defined as
 
where  p ( r , s ) is their joint probabilities;  p ( r ) and  p ( s ) are the respective marginal probabilities. Assuming that the  k -th feature is a random variable, the mutual information  I ( k , y ) between class labels  y ={ y 1 , y 2 ,…, y n } and the feature variable  k  can be used to quantify the relevance of  k th feature for the classification task. The feature  k  with the maximum  I ( k , y ) is chosen as the starting feature. We have found that using such methods is satisfactory. Our preliminary experimental results showed that MIFS is not sensitive to the initial feature chosen. The feature selection method proposed above only works for two-class problems. We derive the multiclass version using a one-versus-all approach as follows.
 First step.  Let  C ≥3 denote the number of classes. For each  i , 1≤ i ≤ C , a binary SVM that separates the  i -th class from the rest is trained based on the selected feature set  S . Second step.  For each binary SVM,  DJ ( k ) is computed for every feature  k  not in  S . All the candidate features are ranked based on the value of  DJ . The larger value the value of  DJ ( k ), the smaller is its rank of  k  (smaller is better). As a result,  C  ranked lists of features are obtained. Each ranked list corresponds to one of the  C  SVMs. Equivalently, each candidate feature corresponds to a ranking vector containing its rankings in these  C  ranked lists. For example, a feature can be ranked as the first in the first list; third in the second list; 20th in the third list, 15th in the fourth list. The vector that is used for ranking this feature is [1, 3, 20, 15]. Third step.  A feature that ranks low in one list may rank high in another. Our goal is to determine features that are most informative in discriminating one class from the rest even if they are quite uninformative in other classifications. This is achieved as follows. The ranking vector of each candidate feature is sorted in an ascending order. If one regards each element of the ranking vector as a digit, each ranking vector could represent a  C  digit number. The smallest ranking (the first element) represents the most significant digit. A least significant digit radix sort algorithm can then be used to sort all the ranking vectors and, accordingly, a global ranking of features can be derived. For example, assume we have three features,  k 1 ,  k 2  and  k 3  whose rankings in four binary SVMs are [1, 3, 20, 15], [8, 4, 7, 6] and [5, 1, 30, 4], respectively. The vectors show that  k 1  ranks top in separating class one from others and ranks third in separating class two from others etc. Each ranking vector is sorted in an ascending order. The resulting vectors are [1, 3, 15, 20], [4, 6, 7, 8] and [1, 4, 5, 30], respectively. Next, a radix sort algorithm is applied over the three vectors. The resulting order of vectors changes to [1, 3, 15, 20], [1, 4, 5, 30], [4, 6, 7, 8], which corresponds to the order of features:  k 1 ,  k 3 ,  k 2 . This provides a global ranking of the three features. The lowest ranked feature is added into  S . The above three step process is used iteratively to determine the next feature. This process stops when a predetermined number of features are selected or  S  contains all the features. Also, with the set  S , the features are ranked based on the order of addition into this set. The iterative procedure for MIFS is formally defined as follows: Input:  Training samples { x 1 , x 2 ,…, x n } and class labels { y 1 , y 2 ,…, y n }, 1≤ y i ≤ C , initial feature set  S , predetermined number of features  r 
 Initialize:  Ranked feature list  RL = S , candidate feature set  L = D − S  ( D  is the set of all features) While  | S |&lt; r 
 For   i =1 to  C 
 Construct new class labels { y 1 ′, y 2 ′,…, y n ′},  y j ′=1 if  y j = i , otherwise  y j ′=−1; Train an SVM using training samples with features in  RL ; Compute the change of objective function  DJ ( k ) for each candidate feature  k ∈ L Sort the sequence of  DJ ( k ), k ∈ L  in descending order; create a corresponding ranked list of candidate features; Compute the ranking vectors for all the features in  L  from  C  ranked lists ; Sort the elements of each ranking vector in an ascending order; Perform a radix sort over all ranking vectors to produce a global ranking of features in  L ; Find the top ranked feature  e  and update  RL =[ RL , e ] and  L = L −{ e } Return:  Ranked feature list  RL This algorithm can be generalized to add more than one feature in Step 2.e to speed up computations when the number of features  r  is large. Time Complexity  The training time complexity for SVM is dominated by the time for solving the underlying quadratic program. The conventional approach for solving the quadratic program takes time cubic in the number of samples and linear in the number of features (Chapelle,  2007 ). (Some approximate solutions make the empirical complexity to be  O ( n 1.7 ) (Joachims,  1999 )). Based on this, the time complexity for this algorithm is  O ( n 3 r 2 C ) in the worst case. 5 DATASETS The Progenetix database (Michael Baudis and Michael,  2001 ) ( http://www.progenetix.net ) consists of more than 12 000 cases (Baudis,  2006 ). We use a dataset consisting of 5020 CGH samples (i.e. cytogenetic imbalance profiles of tumor samples) taken from Progenetix. These samples belong to 19 different histopathological cancer types that have been coded according to the ICD-O-3 system (Fritz  et al. ,  2000 ). The subset with the smallest number of samples, consists of 110 non-neoplastic cases, while the one with largest number of samples, Adenocarcinoma, NOS (ICD-O 8140/3), contains 1057 cases. Each CGH sample consists of 862 ordered genomic intervals extracted from 24 chromosomes. Testing the performance (predictive accuracy and run time) of the proposed methods, requires evaluating them over datasets with different properties such as (1) number of samples contained in the dataset, (2) number of cancer types contained in the dataset, and (3) the similarity level between samples from different cancer types, which indicating the difficulty of classification. Currently, there are no standard benchmarks for normalized CGH data that take the three properties into account. We propose a method to select subsets from the Progenetix database in a principled manner to create datasets with desired properties. The dataset sampler accepts the following three parameters as input: (1) Approximate number of samples (denoted as  N ) (2) Number of cancer types (denoted as  C ) (3) Similarity range (denoted as [δ min ,δ max ]) between samples belonging to different cancer types. An outline of the proposed dataset sampler is as follows:
 For each cancer type, partition all the samples belonging to this cancer type into several disjoint groups using clustering. Each cluster corresponds to the different aberration patterns for a given cancer type. Compute the pairwise similarity between pairs of groups obtained in the first step. Construct a complete weighted graph where each vertex denotes a group of samples and the weight of an edge equals to the similarity between two groups that are connected by this edge. One can use this graph to find a set of samples of a given size  N  (by choosing a subset of groups that sum to  N ), given number of cancer types, and based on level of similarity between groups (by only considering groups that have a similarity within the range of [δ min , δ max ]). The advantage of the above dataset sampler is that a large number of datasets can be created with variable number of samples and cancer types as well as variable level of similarities between the chosen cancer types. This allows for testing the accuracy and performance of a new method across a variety of potential scenarios. Figure 2  shows an example of how such a dataset sampler works. Consider a dataset containing 1000 CGH samples—400 samples belonging to cancer type  c 1  and the other 600 samples belonging to cancer type  c 2 . Assume that each cancer type is clustered into 2 clusters. This results in 4 groups of CGH samples, which are denoted as  g i ,1≤ i ≤4. Let the size of  g 1 ,  g 2 ,  g 3  and  g 4  be 150, 250, 450 and 150, respectively. The pairwise similarity between any two groups is shown in  Figure 2 . Using this, one can construct a weighted graph where each vertex denotes a group and the weight of each edge equals to the similarity between two groups that are connected by this edge. Suppose that a dataset needs to be sampled with  N =400,  C =2, δ min =0.025 and δ max =0.035. The graph can be parsed to find out that  g 2  and  g 4  satisfy the three conditions and a new dataset can be sampled by combining the samples in  g 2  and  g 4 .
 Fig. 2. A working example of dataset sampler.  c i  and  g j  denote the  i th cancer type and the  j th group of samples, respectively. In the first step, the samples are partitioned in each cancer type into two disjoint groups. In the second step, pairwise similarity metrics are computed. In the third step, a complete weighted graph is generated. The advantage of the above dataset sampler is that a large number of datasets can be created with variable number of samples and cancer types as well as variable level of similarities between the chosen cancer types. This allows for testing the accuracy and performance of a new method across a variety of potential scenarios. We used our dataset resampling scheme to select datasets at four different similarity levels from the Progenetix dataset. We denote the similarity levels as  Best ,  Good ,  Fair  and  Poor . The samples in Best has the highest similarity and those in Poor have the lowest similarity. For each similarity level, we created three datasets with four, six and eight cancer types respectively. Thus, in total, we have 12 datasets. For convenience, we use the similarity level followed by the number of cancer types to denote a dataset. For example,  best6  denotes the dataset with similarity level Best (i.e. homogeneous samples) and contains six cancer types. The number of samples in each dataset is around 1000. Note that there is no topological relations between different datasets because we generate all datasets in separate runs. For example, any sample in  best4  is not necessarily contained in  best6  or  best8 . Details of each dataset are listed in  Table 1  and  Table 2 .
 Table 1. Details of the cancers contained in the Progenetix dataset Code #cases Code translation A 310 Infiltrating duct mixed with carcinoma B 323 Diffuse large B-cell lymphoma, NOS C 346 B-cell chronic/small lymphocytic leukemia D 1057 Adenocarcinoma, NOS E 657 Squamous cell carcinoma, NOS F 209 Adenoma, NOS G 110 non-neoplastic or benign H 286 Hepatocellular carcinoma, NOS I 120 Retinoblastoma, NOS J 171 Mantle cell lymphoma K 180 Carcinoma, NOS L 190 Multiple myeloma M 141 Precursor B-cell lymphoblastic leukemia N 133 Osteosarcoma, NOS O 144 Adenocarcinoma, intestinal type P 118 Leiomyosarcoma, NOS Q 126 Ependymoma, NOS R 271 Neuroblastoma, NOS Term ‘#cases’ denote the number of cases in a cancer. 
 Table 2. The comparison of classification accuracy for three feature selection methods, MIFS, MRMR and SVM-RFE (denoted as RFE), on 12 datasets Dataset Cancer code N Method Number of Features 4 8 16 40 60 80 100 150 250 500 862 MIFS 0.696 0.765 0.811 0.819 0.814 0.819 0.821 0.824 0.814 0.815 poor4 A,B,C,D 803 MRMR 0.734 0.772 0.778 0.794 0.791 0.799 0.814 0.814 0.819 0.802 0.809 RFE 0.567 0.644 0.681 0.706 0.746 0.771 0.794 0.814 0.821 0.821 MIFS 0.527 0.59 0.615 0.622 0.64 0.654 0.659 0.645 0.649 0.633 poor6 A,B,C,D,E,F 815 MRMR 0.542 0.576 0.588 0.589 0.581 0.596 0.61 0.596 0.610 0.635 0.633 RFE 0.337 0.37 0.431 0.531 0.551 0.564 0.578 0.593 0.608 0.635 MIFS 0.338 0.394 0.433 0.469 0.470 0.488 0.496 0.513 0.53 0.486 poor8 A,B,C,D,E,F,G,H 764 MRMR 0.335 0.408 0.454 0.467 0.469 0.482 0.47 0.474 0.489 0.465 0.472 RFE 0.259 0.274 0.303 0.39 0.423 0.435 0.457 0.456 0.456 0.475 MIFS 0.621 0.687 0.755 0.784 0.802 0.816 0.816 0.809 0.808 0.806 fair4 B,D,I,J 812 MRMR 0.598 0.685 0.728 0.777 0.796 0.789 0.784 0.777 0.783 0.786 0.798 RFE 0.466 0.527 0.608 0.693 0.753 0.753 0.771 0.786 0.787 0.806 MIFS 0.587 0.698 0.754 0.814 0.822 0.825 0.827 0.82 0.82 0.807 fair6 B,C,D,E,F,I 880 MRMR 0.593 0.698 0.767 0.772 0.786 0.807 0.802 0.807 0.801 0.804 0.792 RFE 0.504 0.64 0.696 0.761 0.775 0.78 0.781 0.78 0.797 0.816 MIFS 0.536 0.641 0.684 0.7 0.736 0.733 0.727 0.735 0.732 0.713 fair8 B,C,D,E,H,I,K,L 767 MRMR 0.54 0.653 0.681 0.721 0.707 0.712 0.715 0.704 0.698 0.695 0.72 RFE 0.398 0.528 0.616 0.677 0.687 0.688 0.702 0.70 0.701 0.709 MIFS 0.586 0.673 0.763 0.773 0.782 0.78 0.783 0.774 0.778 0.767 good4 B,D,H,M 794 MRMR 0.609 0.681 0.755 0.761 0.779 0.78 0.78 0.77 0.772 0.761 0.755 RFE 0.543 0.61 0.656 0.711 0.718 0.74 0.732 0.735 0.767 0.749 MIFS 0.455 0.551 0.593 0.645 0.709 0.716 0.724 0.697 0.7 0.694 good6 D,J,K,L,N,O 867 MRMR 0.427 0.532 0.621 0.667 0.68 0.69 0.677 0.687 0.675 0.664 0.696 RFE 0.339 0.437 0.517 0.597 0.638 0.653 0.66 0.682 0.674 0.698 MIFS 0.373 0.477 0.567 0.659 0.674 0.676 0.665 0.673 0.666 0.655 good8 D,E,H,J,K,N,P,Q 827 MRMR 0.336 0.461 0.527 0.615 0.634 0.647 0.644 0.646 0.649 0.661 0.652 RFE 0.258 0.346 0.424 0.508 0.53 0.581 0.605 0.624 0.632 0.654 MIFS 0.650 0.754 0.763 0.817 0.829 0.832 0.829 0.821 0.838 0.82 best4 A,D,E,R 1158 MRMR 0.667 0.757 0.775 0.785 0.789 0.793 0.798 0.791 0.784 0.802 0.803 RFE 0.596 0.659 0.708 0.753 0.766 0.789 0.776 0.791 0.803 0.817 MIFS 0.497 0.568 0.699 0.731 0.767 0.765 0.763 0.77 0.75 0.755 best6 A,D,E,H,O,R 1095 MRMR 0.497 0.568 0.688 0.73 0.731 0.725 0.746 0.739 0.748 0.74 0.75 RFE 0.449 0.499 0.587 0.667 0.71 0.712 0.727 0.729 0.736 0.749 MIFS 0.427 0.543 0.635 0.726 0.737 0.733 0.735 0.732 0.735 0.727 best8 A,D,E,F,H,K,L,R 1016 MRMR 0.434 0.563 0.652 0.704 0.7 0.714 0.712 0.7 0.693 0.704 0.707 RFE 0.342 0.429 0.532 0.641 0.648 0.687 0.694 0.723 0.719 0.724 MIFS 0.524 0.612 0.673 0.713 0.732 0.736 0.737 0.734 0.735 0.723 Avg N/A N/A MRMR 0.518 0.606 0.664 0.696 0.702 0.709 0.71 0.707 0.707 0.706 0.716 RFE 0.422 0.497 0.563 0.636 0.662 0.679 0.69 0.7 0.708 0.721 The best accuracy obtained for each dataset is highlighted in  bold . Term  N  denotes the number of cases. The cancer codes are explained in  Table 1 . 6 EXPERIMENTAL RESULTS In this section, we describe the experimental comparison of our methods with SVM-RFE and MRMR. We developed our code using MATLAB and ran our experiment on a system with dual 2.59 GHz AMD Opteron Processors, 8 gigabytes of RAM, and a Linux operating system. 6.1 Comparison of linear and raw kernel In this section, we compare the Raw kernel to linear kernel for the classification of CGH data. We perform the experiments over the twelve datasets using a 5-fold cross validation (CV). For each dataset, we randomly divided the data set into five disjoint subsets about equal size. For each fold, we keep one subset as the test data set and the other four sets as the training examples. We train two SVMs over the training examples using linear and Raw kernel respectively. We then use each SVM to predict the class labels of the set aside examples respectively. We compute the predictive accuracy of each SVM as the ratio of number of correctly classified samples to the number of test dataset examples. Next, we choose another subset as set aside examples and the rest as training examples. We repeat this procedure until each subset has been chosen as set aside examples. As a result, we have five values of predictive accuracy corresponding to each kernel respectively. We compute the average of the five values as the average predictive accuracy for each kernel in 5-fold CV. We use the DAGSVM (Directed Acyclic Graph SVM) provided by MATLAB SVM Toolbox (Cawley,  2000 ) for the classification of multiclass data. All other parameters of SVM are set to the standard values that are part of the software package and existing literature. The results are presented in  Figure 3 .  X -axis lists the 12 different datasets.  Y -axis denotes the value of average predictive accuracy in 5-fold CV. For the 12 datasets, Raw kernel outperforms linear kernel in eleven datasets (except  best8 ). On average, Raw kernel improves the predictive accuracy by 7.3% over 12 datasets compared to linear kernel. For the  best8  dataset, the difference between Raw and Linear is less than 1%. These results demonstrate that SVM based on Raw kernel works better for the classification of CGH data as compared to linear SVM.
 Fig. 3. Comparison of predictive accuracies of SVM with linear and Raw kernels respectively.  X -axis denotes different datasets.  Y -axis denotes the predictive accuracy based on 5-fold CV. The remaining set of experimental results in this section are limited to the Raw kernel (unless stated explicitly). 6.2 Comparison of MIFS with other methods In this section, our method, MIFS, is compared against MRMR (a filter based approach) and SVM-RFE (a wrapper-based approach). MRMR is shown to be more effective than most filter methods, such as methods based on standard mutual information,  F -statistic or  t -statistic (Ding and Peng,  2005 ). The MIQ scheme of MRMR, i.e. the divisive combination of relevance and redundancy, is used because it outperforms MID scheme consistently. SVM-RFE is a popular wrapper method for gene selection and cancer classification. It is shown to be better than filter methods such as those based on ranking coefficients similar to Fisher's discriminant criterion. SVM-RFE is also shown to be more effective than wrapper methods using RFE and other multivariate linear discriminant functions, such as linear discriminant analysis and mean-squared error (Pseudo-inverse) (Guyon  et al. ,  2002 ). For each method, a 5-fold cross validation is used. In each fold, the feature selection method is applied over the training examples. Multiple sets of features with different sizes (4, 8, 16 features etc) are selected. For each set of features, a classifier is trained on the training examples with only the selected features. The predictive accuracy of this classifier is determined using the test (set aside) examples with the same set of features. These steps are repeated for each of the 5-folds to compute the average predictive accuracy. In the experiments, we use the DAGSVM with Raw kernel as the classifier for testing the predictive accuracy of features selected by different methods. Since the SVM-RFE presented in the literature only works for two-class data, we extended it to multiclass data using the same ‘ranking scheme’ that we use to extend MIFS (as described in  Section 4 ). The originally proposed SVM-RFE uses linear kernel for feature selection purpose. We stick to the same implementation of SVM-RFE in our experiments. We also implement a variant of SVM-RFE using Raw kernel. Based on our experimental results, the classification accuracy of Raw kernel-based SVM-RFE is roughly midway between the linear kernel based SVM-RFE and MIFS. Detailed results of Raw kernel-based SVM-RFE are not presented here due to space limitations. The experimental results are shown in  Table 2 . In  Table 2 , the predictive accuracy of features selected by three methods, MIFS, MRMR and SVM-RFE, over twelve datasets are compared. For each feature selection method, the results for 4, 8, 16, 40, 60, 80, 100, 150, 250 and 500 features over each dataset are presented. The results are averaged over the 5-folds and reported in columns 5 to 14. In the 15th column, the average predictive accuracies of SVM built upon 862 features, i.e. no feature selection, are reported. The average predictive accuracies of the 12 datasets are reported in the last three rows. The key findings are described as follows. Comparison between MIFS and MRMR  The results show that, when the number of features is ≤16, there is no clear winner between MIFS and MRMR. Although, MIFS is slightly better than MRMR based on the average results of the 12 datasets, neither of the two methods are predominantly better than other. However, when the number of features is &gt;16, MIFS outperforms MRMR in almost all cases. We believe that using SVM-based approach provides combination of features that have significantly better predictive power than MRMR for CGH datasets. Also, it is worth noting that if we compare the best predictive accuracy obtained for a given dataset (given in bold) by using MIFS to that of MRMR, we observe that MIFS always gives a better value. Comparison between MIFS and SVM-RFE  The results in  Table 2  show that MIFS outperforms SVM-RFE in almost all cases. Clearly, as the number of features increases, the gap between MIFS and SVM-RFE drops. They become comparable in terms of predictive accuracy only when the number of features reaches more than a few hundred (we do not report these results due to the space limitations). We believe that a forward scheme is better because it first adds the highest discriminating features followed by features that individually may not be discriminating, but improve the classification accuracy when used in combination with the discriminating features. A backward elimination scheme fails to achieve this. Using MIFS for feature selection  The results in  Table 2  shows that using only 40 features results in classification accuracy that is comparable to using all the features. Also, using 80 features derived from MIFS scheme results in comparable or better classification accuracy as compared to all the features. This is significant as beyond data reduction, the proposed scheme can lead to better classification. To support this hypothesis, we generated four new datasets using our dataset resampler. The resulting four datasets (newds1 to newds4) contain 4, 5, 6 and 8 classes, respectively. The number of samples in the four datasets are 508, 1021, 815 and 649. We applied the MIFS method over these datasets. We compare the classification accuracies obtained by using all 862 features to those using only 40 and 80 selected features. The results are shown in  Table 3 . These results substantiate our hypothesis that using around 40 features (roughly 5% of all features) can generate comparable accuracy to using all the features. Also, using around 80 features (roughly 10% of all the features) can result in comparable or better prediction than all the 862 features.
 Table 3. The comparison of classification accuracy using different number of features Dataset Number of features 40 80 862 newds1 0.801 0.792 0.799 newds2 0.803 0.819 0.8 newds3 0.629 0.67 0.637 newds4 0.706 0.748 0.719 Average 0.735 0.757 0.739 
 It is worth noting that the other two methods, typically have lower or comparable accuracy when the number of features used is less than all the features. 7 CONCLUSIONS Comparative Genomic Hybridization (CGH) is one of the important mapping techniques for cancerous cells. In this article, we develop novel SVM-based methods for classification and feature selection of CGH data. For SVM-based classification, we show that the kernel used by us is substantially better then the standard kernel for SVM. Our approach of greedily selecting features with the maximum influence on an objective function results in significantly better classification and feature selection. We compared our methods against SVM-RFE (wrapper) and MRMR (filter) approaches that have been used for classification and feature selection of large dimensional biological data. Our results on twelve datasets generated from the Progenetix database, suggests that our methods are considerably superior to existing methods. Further, unlike other methods proposed in the literature, our methods can improve the overall classification error by using a small fraction (around 10%) of all the features. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>A framework for scalable parameter estimation of gene circuit models using structural information</Title>
    <Doi>10.1093/bioinformatics/btt232</Doi>
    <Authors>Kuwahara Hiroyuki, Fan Ming, Wang Suojin, Gao Xin</Authors>
    <Abstract>Motivation: Systematic and scalable parameter estimation is a key to construct complex gene regulatory models and to ultimately facilitate an integrative systems biology approach to quantitatively understand the molecular mechanisms underpinning gene regulation.</Abstract>
    <Body>1 INTRODUCTION A quantitative understanding of how expression of genes is controlled in time and space through the integration of computational and experimental methods is a main goal of molecular systems biology ( Church, 2005 ;  Ideker  et al. , 2001 ;  Kitano, 2002 ). Among the major obstacles in such an integrative systems biology approach is the construction of kinetic models that quantitatively support the current knowledge of a given gene circuit. What makes the construction of gene circuit models especially difficult is the quantification of all reaction parameters, as direct measurements of gene regulation kinetics are seldom available. Thus, model parameters are often estimated indirectly using more readily available experimental data [e.g.  Schoeberl  et al.  (2002) ;  Zwolak  et al.  (2005) ]. Even in modeling of relatively well-known gene circuits, such as the phage-λ lysis–lysogeny developmental pathway ( Arkin  et al. , 1998 ), there are a number of unknown parameters, which are phenomenologically determined by fitting the model’s outputs to some experimental observations. The quality of time series gene expression data is crucial to the construction of phenomenological models that accurately capture the observed dynamical characteristics of a given gene circuit. With advances in the gene expression detection technologies, single-molecule level measurements of gene expression can now be obtained in a wide range of organisms ( Baugh  et al. , 2011 ;  Cai  et al. , 2006 ;  Golding  et al. , 2005 ;  Materna  et al. , 2010 ;  Newman  et al. , 2006 ;  Suter  et al. , 2011 ;  Taniguchi  et al. , 2010 ;  Zenklusen  et al. , 2008 ). In particular, recent advances in fluorescence imaging techniques ( Joo  et al. , 2008 ;  Raj and van Oudenaarden, 2009 ) facilitate real-time measurements of gene expression at the single-molecule level, making more accurate parameter estimation for quantitative modeling of gene circuits possible. Such single-cell gene expression data are, however, noisy because of intrinsic and extrinsic fluctuations ( Cai  et al. , 2006 ;  Elowitz  et al. , 2002 ;  Newman  et al. , 2006 ;  Golding  et al. , 2005 ;  Raj  et al. , 2006 ;  Raser and O’Shea, 2005 ;  Suter  et al. , 2011 ;  Taniguchi  et al. , 2010 ) and often limited to lower concentration molecular species such as mRNAs ( Kulkarni, 2011 ;  van Oijen, 2011 ). Because of such noisy gene expression and highly nonlinear dynamics involved in transcriptional regulations, manual parameter estimation in nontrivial gene circuit models is generally infeasible. To systematically estimate the parameters of a biochemical kinetic model, the parameter estimation problem is often treated as an optimization problem in which parameter values are selected to minimize a certain objective function ( Schwartz, 2008 ). Although several stochastic optimization and Bayesian-based methods were successfully applied to estimate parameters of biochemical models ( Baker  et al. , 2010 ;  Moles  et al. , 2003 ), they often suffer from scalability problems when there are a large number of unknown parameters. To make the estimation of parameters more efficient, several methods have been proposed to reduce the parameter search space by decomposing rate equations ( Jia  et al. , 2011 ;  Koh  et al. , 2006 ;  Zhan and Yeung, 2011 ). However, the quality of these methods strongly depends on interpolation and smoothing functions, which are often independent of the underlying model structure and can add strong artifacts. Recently, Kalman filter-based approaches, which can alleviate the scalability problem, were applied to efficiently estimate kinetic parameters ( Lillacci and Khammash, 2010 ;  Quach  et al. , 2007 ;  Sun  et al. , 2008 ). Although these approaches support parameter estimation of models with unobserved variables, a recent comparative study showed that their performance could be sensitive to the initial condition, and estimated parameters might be far from the true ones if the initial guess was not close to the solution ( Liu and Niranjan, 2012 ). Most of these existing methods are applicable to parameter estimation problems of generic dynamical models, as they do not demand any domain-specific knowledge. Although these general-purpose methods can easily be applied to modeling of any biological systems, it is clear that each of these methods has its advantages and disadvantages, and that no single method is versatile enough to efficiently give optimal parameter sets for all biological models. This observation has led us to develop a more tailored parameter estimation method that focuses on a specific yet important subclass of biological models, namely, gene circuit models. To facilitate the mechanistic construction of thermodynamics-based models ( Shea and Ackers, 1985 ;  Sherman and Cohen, 2012 ) that describe the quantitative behavior of gene regulation from time series mRNA data, we developed a novel parameter estimation framework called  Parameter Estimation by Decomposition and Integration  (PEDI) that specifically focuses on modeling of gene circuits. The main paradigm of PEDI is ‘divide’ and ‘conquer’; by using the given mRNA data and exploiting the structure of gene circuit models, our framework divides a high-dimensional parameter estimation problem into subproblems with a much smaller parameter space, each of which is, in turn, conquered (i.e. solved) by using any constrained optimization method. At the initial step, this problem reduction process leads to a crude linearization for numerical integrations, which often results in poor estimates especially for highly nonlinear systems. To improve the quality of the estimate with a basically negligible increase in computing time, PEDI places intermediate integration points using the underlying structural information of a given gene circuit model and iteratively increases the accuracy of these intermediate points to increase the accuracy of the numerical integration, which in turn improves the reconstructed dynamics. This article introduces PEDI and, through the use of simulated annealing (SA) as the optimization method, applies the framework to three-gene circuit models with complex dynamics based on synthetic time series mRNA datasets and one yeast gene circuit model based on time series microarray data. We compared PEDI with three state-of-the-art parameter estimation methods, namely, the  evolutionary strategy with stochastic ranking  (SRES) ( Runarsson and Yao, 2000 ), the  moment matching method coupled with hybrid extended Kalman filter  (HEKF + MM) ( Lillacci and Khammash, 2010 ) and the  two-phase dynamic decoupling method  (TDDM) ( Jia  et al. , 2011 ). Our results show that PEDI consistently produced the most accurate estimates efficiently in all the four parameter estimation experiments. This study, thus, demonstrated that PEDI could provide an effective approach to efficiently estimating kinetic parameters of gene circuit models. 2 METHODS 2.1 Problem setting We concern ourselves with time series gene expression data generated from an  N -gene network at equally spaced  M  + 1 time points,  . Gene  g i  is transcribed into mRNA  m i , which is then translated into protein  p i , which can then be used to regulate the transcription of genes in the network. We denote by  m ij  and  p ij  random variables representing the levels of the mRNA copy and the protein copy of gene  g i  at time  t j , respectively. We further assume that these random variables be expressed as follows:
 
where   and   are the true mean of  m ij  and  p ij , respectively, whereas each of  v ij  and  u ij  is a statistically independent random variable with mean 0. We consider that only the levels of mRNAs are observable from the experiments, but we assume that the true mean of each protein  p i  be known at time  t 0 . Here, we are interested in constructing a kinetic model that estimates the average trajectory of mRNAs given by  , and we do not focus on the time evolution of higher moments, as experimental time-series data often contain only few datasets. Our model describes the average time evolution of a gene circuit as a continuous-time deterministic process, which is governed by a system of ordinary differential equations (ODEs) as follows:
 (1) 
with the initial conditions:
 
Here,   and   are time-dependent variables that estimate the dynamics of   and  , respectively;   is a  K i  dimensional vector that represents the parameters used in the rate equation representing the regulation of mRNA  m i ;  h i  is the transcription rate function based on the equilibrium thermodynamics model of the  cis  regulation of gene  g i ;   is an  N  dimensional vector whose  i -th element is  ;   and   are the parameters used in the regulation of protein  p i , which we assume to be known; and   is the sample mean of  m i  at time  t j . The regulation of each gene is modeled using four reaction processes, transcription, mRNA degradation, translation and protein degradation, whereby transcription is considered to be the main regulatory step. Using this model, our objective is to search for values of the unknown parameters, which minimize a weighted sum of squared residuals of the sample means of mRNAs. As a system of coupled nonlinear rate equations can seldom be solved analytically, the numerical integration via simulation is usually used to estimate the levels of mRNAs for a given parameter value. When the dimension of the unknown parameters is high, however, finding practical solution for   based on simulation for each parameter change becomes computationally intensive, and such an approach eventually proves to be infeasible. 2.2 Overview of PEDI Figure 1  illustrates a high-level workflow of PEDI. The main idea of our framework is to optimize parameters separately by using what is available rather than dealing with exponentially larger parameter space involved in the optimization of θ. The framework takes advantage of the fact that the rate functions in our gene circuit models have the following structure:
 (2) 
whose definite time integral from time  t 0  to time  t j  has the following form:
 (3) 
where  . As the transcription rate functions depend on regulatory proteins, our parameter estimation framework based on the decomposition of a gene circuit model requires the estimate of the protein levels first. To this end, PEDI uses the time series sample average of the mRNA and makes a linear assumption to estimate the mean time evolution of each protein level. However, as gene circuits often involve highly nonlinear reactions, such a crude linear interpolation may result in an inadequate parameter estimation. To refine the quality of the parameter estimation, the framework enriches the number of the data points by estimating intermediate points of the observed data points using the output from a computational simulation. These intermediate data points are then used to make the interpolation of the observed data points and the numerical integration of the rate functions more accurate. The introduction of these intermediate data points does not increase the complexity of the parameter search space, as they are only used for numerical integrations. By repeating this process, PEDI attempts to increase the accuracy of the interpolation and the fitness of the parameter estimation. Thus, PEDI can efficiently perform parameter estimation by avoiding computationally intensive search in a high-dimensional parameter space while keeping the quality of the parameter estimation high.
 Fig. 1. An illustration of the workflow of PEDI. Briefly, given a model structure and time series mRNA data at   time points, it first makes a linear assumption and estimates the proteins data points, which are then used to estimate parameters for the mRNA regulations. These initial estimates are then iteratively refined by placing   mRNA integration points and   protein integration points and by increasing the accuracy of these integration points As PEDI decomposes a system of ODEs into individual ODEs, it has an objective function for each mRNA  m i . The form of  J i , the objective function of mRNA  m i  is a weighted sum of squared residuals. More detailed information on the objective functions in PEDI is described in  Supplementary Section S1 . 2.3 Initial optimization process PEDI decomposes a gene circuit model into individual rate equations. This process involves uncoupling of coupled rate equations. To estimate the time evolution of the mRNAs from the decomposed rate equations, we first need to estimate the time evolution of the transcription factors of each gene in the model. Thus, the first step of our framework is to generate the initial estimate of each protein copy at the  M  time points (i.e.  t 1  to  t M ). To this end, we estimate   by applying the time-integral form in  Equation (3) , using the time series sample average of  m i  and using the trapezoidal rule to approximate the numerical integration. This estimates the mean levels of the transcription factors of each gene  g i  at the  M  time points, making the evaluation of the transcriptional kinetic function of each mRNA  m i  at the  M  time points possible. Using the initial estimates of the protein levels at the  M  time points, PEDI sets out to estimate the mean time-course of  m i  by optimizing the value of  . To estimate the mean time evolution of  m i , we once again use the time-integral form in  Equation (3)  and apply the trapezoidal rule to approximate the integration of the rate equation of  . This approximate integration is used to compute   for each  i  with a given parameter combination, which is then used in a metaheuristic optimization—such as SA and genetic algorithms—to test the fitness of each parameter combination and to find the initial estimate of the optimal  . This parameter optimization process is largely independent of the values of the parameters in a model and remains efficient even when a combination of the parameter values makes the timescale of some rate equations widely different and the ODEs stiff. While facilitating an efficient and scalable parameter estimation, a model decomposition involving the linear approximation of the time integral of each rate equation may not result in a high-quality estimate, especially when the model of interest is highly nonlinear or the given time series data are sparse. In addition, such a linearization inevitably introduces integration errors, making the assessment of the prediction error for each parameter combination difficult. More detailed information on the initial optimization process is described in  Supplementary Section S1 . 2.4 Parameter estimation refinement To improve the accuracy of the numerical integration and the parameter estimation, PEDI next performs a simulation of the ODE model, given the current estimate of θ. This simulation-based numerical integration not only gives a much more accurate picture in terms of the performance of the current estimate but also generates an arbitrary number of data points for each   and  . From this simulation, we generate   equally spaced   data points between  t 0  and  t M  for each mRNA, where we set   for some integer  . The value of  L m  may come with additional constraints depending on the choice of a numerical integration method. By using the simulated mRNA data points, PEDI attempts to better estimate the time evolution of proteins than the simple linear interpolation that is used in the initial estimate. To this end, we first adjust the simulated data of   so that they can better reflect the time evolution of the sample mean of each  m i . Let   be the number of simulated mRNA data points in each time interval between time points  t j  and   (i.e.  ). Then, to estimate the   mRNA points in this time interval, we adjust every simulated data point between  t j  and   by considering the difference between the sample mean and the simulated data point of mRNA  m i . Specifically, by letting   Δ t  be the time interval between  t j  and  t j  + 1  and  m̌ i  be a time-dependent variable that represents the   adjusted   data points, we express   for all   as follows:
 (4) 
where  r j  is  . This definition makes sure that, at each time point  t j , we have  . This allows us to use the additional data points from   to make the interpolation of   and, in turn, the estimation of the time evolution of  p i  more accurate than the ones based on a simple linearization. By using the   data points of   between  t 0  and  t M , we generate equally spaced   data points for each  p i . Here, we require that   be smaller than  L m  where  d p  is a positive integer. We denote by   a time-dependent variable that represents the   data points of  p i . To compute the values of  at the   time points, we first set   to be  . Next, we iteratively compute the next data point of   for the other  L p  time points. To this end, we integrate the rate equation of  p i  between each time interval   using the   data points of   within this time interval (see  Supplementary Section S1  for details). Using the   time points of the newly generated protein variables,  , we can better interpolate the dynamics of the transcription factors of gene  g i  and estimate the dynamics of   from the decomposed rate equation of  m i  for a given parameter combination of  . Thus, applying this approach for the calculation of  J i  within an optimization method, we can search for a parameter combination   of   that increases the quality of the estimate (see  Supplementary Section S1  for details). By using   generated from this optimization, we simulate the model and calculate the sum of  J i . We repeat the parameter refinement process until a given termination condition is satisfied (e.g. until the value of the sum of  J i  stabilizes). For the next iteration of the refinement process, if the current error is smaller than the previous one, we use the current   as the seed parameter values for the next iteration. Otherwise, we select the current   over the previous one at probability of   where  p max  is the maximum probability of choosing the current estimate,   is the current sum of  J i  and   is the previous one. That is, if the error from the current   is worse than the previous one, the probability of accepting the current   for the next round becomes smaller. The detailed information of specific configurations of PEDI used in the  Section 3  of this article is described in  Supplementary Sections S1  and  S2 . 2.5 Prediction error Optimization-based parameter estimation methods may have different objective functions. To compare the accuracy of estimated parameters of various parameter estimation methods objectively and without depending on any specific objective functions, we define the prediction error of the  i -th mRNA as follows:
 (5) 
where  ϵ  is a small fixed value and the prediction error of the model as follows:
 (6) 
In other words, we defined the prediction error to be the sum of the difference between the sample mean and the estimate with respect to the difference between the sample mean and the true mean at each time point and for each mRNA. As this definition of prediction error depends on the true mean of mRNAs—whose values are hidden from objective functions—this prediction error can be more objective to compare parameter estimation methods than using a specific objective function (e.g. sum of mean squared error). However, this definition can only be used when mRNA data are synthesized from a model, as the true mean values are not available in real biological systems. In this study, we set  ϵ  to be 0.1. 3 RESULTS 3.1 Models To test the performance of our parameter estimation framework, we constructed three different gene circuit models,   and   (see  Fig. 2 ). The first system is a three-gene circuit ( Fig. 2 A). In this system, the transcription of gene  g 1  is upregulated by protein  p 2  and downregulated by protein  p 3 . The transcription of gene  g 2  is repressed by protein  p 1 , forming a negative feedback loop of gene  g 2 . Such a regulatory structure can be seen, for example, in the phage-λ lysis-lysogeny decision circuit in which  CII  upregulates synthesis of  CI ,  CI  in turn downregulates synthesis of  CII  and  Cro  downregulates synthesis of  CI  ( Arkin  et al. , 1998 ). Provided that the level of protein  p 2  is high and the level of protein  p 1  is low, this system exhibits a complex transient behavior. In this setting, protein  p 1  initially increases rapidly because of the upregulation facilitated by protein  p 2 , and this increase in protein  p 3  downregulates gene  g 2 , leading to a rapid decrease in protein  p 2 , which, in turn, downregulates gene  g 1  and so on. We modeled this gene circuit by the following system of ODEs, which we refer to as model  :
 
 Fig. 2. The schematics of the three gene circuits used in this study. ( A ) The gene circuit structure of model  . ( B ) The three-gene repressilator structure represented in model  . ( C ) The seven-gene repressilator structure represented in model  In this model, we treated the equilibrium rate constants and the maximum transcription rates:   as unknown and estimated them from synthetic time series mRNA data by adding a Gaussian noise to the simulated data. We simulated this model from time 0 to 1500 time units and sampled mRNA data at 31 time points. The initial condition for the simulation is   and  , and all protein molecules are initially set to be absent in the system. We note that this parameter estimation problem has an infinite number of suboptimal solutions. This is because the transcriptional regulation kinetic function of  p 1  can be simplified to   if   or   is assumed to be always much greater than 1. In such a case, there is an infinite number of combinations of  k 1  and  k 2  with the same   ratio that produce the same dynamics. As a result, we could obtain an infinite number of optimal solutions if this assumption were to be satisfied. However, as the initial amounts of proteins  p 2  and  p 3  are zero, those solutions may just be suboptimal and may not capture the initial transient behavior well. Thus, it is challenging to find an optimal solution that can capture the initial transient behavior without being stuck in one of those suboptimal solutions.  Supplementary Table S1  shows the values of the parameters used in the simulation. The second and third models are both based on a gene circuit structure, which has a potential to exhibit a sustained oscillation. This gene circuit is called  repressilator , which was synthetically constructed to exhibit an oscillatory behavior based on transcription regulation with cyclic repression ( Elowitz and Leibler, 2000 ). We model the mean time evolution of  n -gene repressilator by the following system of ODEs:
 
where  p 0  is equivalent to  p n . By having an odd number of interacting genes, the repressilator can exhibit an oscillation under specific parameter conditions. Here, we constructed two repressilator models with a different number of genes. One is a three-gene model, which we refer to as model  , and the other one is a seven-gene model, which we refer to as model   ( Fig. 2 B and C). Given the parameter combinations that we selected (see  Supplementary Tables S2  and  S3 ), these two models exhibit oscillatory dynamics as the eigenvalues of the Jacobian matrix at the fixed point contain complex numbers. We set the initial condition of model   to be  , and the other molecular species to be initially absent in the system. For the initial condition of model  , we set   and the other molecular species to be initially absent in the system. To generate the dataset for the true mean trajectory of the mRNAs in the two-gene circuit models, we simulated each deterministic model and sampled the mRNA levels at 31 equally spaced time points. Using these true mean trajectory datasets, we later added the noise term for each data point for each sample and generated the sample mean of each mRNA. 3.2 Comparison using synthetic data In this study, we used as the optimization component in PEDI an adaptive SA algorithm ( Kirkpatrick  et al. , 1983 ) in which the parameter to control the temperature schedule can be changed. To measure the improvement made by PEDI, we also used the SA algorithm—without the PEDI framework—for the parameter estimation of models   and   and compared the performance between PEDI and SA (see  Supplementary Section S3 ). Our results demonstrated that PEDI improved the consistency and the accuracy of parameter estimation while increasing the runtime efficiency. Next, we compared PEDI with three state-of-the-art parameter estimation methods, namely, the SRES ( Runarsson and Yao, 2000 ), the HEKF + MM ( Lillacci and Khammash, 2010 ) and the TDDM ( Jia  et al. , 2011 ). SRES is an evolutionary optimization algorithm, which was reported to be among the best candidates for parameter estimation of biological models in previous comparative studies ( Moles  et al. , 2003 ;  Sun  et al. , 2012 ). HEKF + MM is a hybrid parameter estimation method, which first applies the hybrid extended Kalman filter and then, if necessary, applies the moment-matching method as the refinement step. TDDM is another model decomposition-based method, which avoids costly ODE simulations by estimating the slopes of smooth piecewise polynomial functions that interpolate observed data. While PEDI, HEKF and TDDM were all implemented in Matlab, the moment-matching method was implemented in C. SRES was implemented in Matlab, but it calls a C library for stochastic ranking computations. Thus, we expected that the efficiency of HEKF + MM and SRES might be overestimated from the comparisons based on computational time. To generate a sample data point of a given mRNA at a given time point, we sampled a value by adding to the true mean of mRNA a Gaussian random variable with mean 0 and variance being the time average of the true means of this mRNA. For the experiments with models   and  , we generated four time series data samples and used the average of the four as the observed dataset. To analyze the performance of parameter estimation methods with a time series dataset at a higher noise level, we generated only a single time series data sample and used this as the observed dataset in the experiments with model  . For the parameter estimation of each of the three models, we ran each method 10 times. Detailed information about the specific settings used in the three parameter estimation methods in this comparison is described in  Supplementary Section S4 . To evaluate the performance of each method, we used four basic criteria, the computational efficiency, the quality of reconstructed dynamics, the accuracy of estimated parameters and the quality of predictability. The computational efficiency was measured by computing the average runtime of the 10 runs from each method. The quality of reconstructed dynamics was analyzed by measuring the average, the smallest and the largest prediction errors of each method, whereas the accuracy of estimated parameters was analyzed by measuring the average relative error of the estimated parameter set with the smallest prediction error for each method. Finally, the quality of predictability was analyzed by extrapolating mRNA data at the next  k  observed time points using the parameter set with the smallest prediction error of each method; we measured the mean squared distance between the estimated data points and 100 samples that are generated for each of the  k  observed data points where we set  k  to be 1, 3 and 5. The results from the comparison of the four methods using model   are summarized in  Table 1 . Here, PEDI outperformed the other methods in three of the four criteria. Both SRES (with 100 generations of evolution) and TDDM performed poorly in terms of efficiency and accuracy. Although HEKF + MM was the most efficient method in this experiment, PEDI was also relatively efficient (0.5 min versus 2.2 min). By comparing the best parameter solutions of these two methods, we found that PEDI generated the most accurate estimate ( Fig. 3 A and  Supplementary Fig. S9 ). HEKF + MM produced parameter sets with negative values in 7 of the 10 runs. This is due to the fact that the moment-matching algorithm used an unconstrained local optimization technique ( Lillacci and Khammash, 2010 ). To analyze the typical behavior of each method, we measured the average prediction error and the average relative parameter error ( Table 1  and  Supplementary Fig. S10 ). These show that PEDI generated not only the best parameter solution but also the highest quality parameter solutions on average. PEDI was also able to extrapolate the mRNA levels at next few time points more accurately than the other three. Taken together, we found that PEDI was able to generate the highest quality parameter solutions efficiently among the four methods.
 Fig. 3. Comparison of the four methods based on the reconstructed dynamics with the smallest prediction error from models   and  . ( A ) The results of  m 1  in  . ( B ) The results of  m 1  in  . Here, each value within parentheses next to each method indicates the prediction error for a given mRNA, the red square points indicate the observed data points, and the dotted red lines indicate the true average trajectories 
 Table 1. Comparison of the results from model  Method PEDI SRES a HEKF + MM TDDM Runtime 2.2 min 8.1 min 0.5 min 11.3 min Average PE 1584.0 N/A b 2145.0 Best PE 836.5 207.0 2144.9 Worst PE 2424.5 N/A b 2145.1 Best param c 8490.5 0.12 9382.6 Pred(1) d 378.8 40.0 3095.9 Pred(3) d 410.4 36.3 2831.9 Pred(5) d 392.8 38.6 2620.7 a With 100 generations.  b Because seven runs resulted in negative parameter values. c The average relative error of the best parameter solution.  d Pred(k) indicates the mean squared distance of the next  k  time points. The comparison criteria are as follows: the average runtime; the average, best and worst prediction errors; the average relative error of the best parameter set; and the quality of data extrapolation. Each bold face indicates the best among the four. Next, we analyzed the performance of the four methods using model  . These results are summarized in  Table 2 . In this experiment, PEDI outperformed the other methods in three of the four criteria. Although HEKF + MM was once again the most efficient method in this experiment, both PEDI and TDDM had comparable speed with HEKF + MM. Again, PEDI substantially outperformed the other methods in terms of the quality of the estimates in this experiment. By comparing the best parameter solution of each method, we found that PEDI generated high-quality estimates with the smallest prediction error ( Fig. 3 B and  Supplementary Fig. S11 ). Even the worst parameter solution of PEDI had a lower prediction error than the best solution of any of the other methods ( Table 2 ). To analyze the typical behavior of each method, we measured the average prediction error and the average relative parameter error of the best parameter solution ( Table 2  and  Supplementary Fig. S12 ). These show that PEDI consistently outperformed the other methods and produced much higher quality parameter solutions in a computationally efficient fashion. Furthermore, PEDI was also able to extrapolate the mRNA levels at next few time points substantially more accurately than the other three.
 Table 2. Comparison of the results from model  2 Method PEDI SRES a HEKF + MM TDDM Runtime 15.0 min 268.7 min 10.6 min 10.9 min Average PE 2119.9 N/A b 2197.1 Best PE 1854.0 2964.2 2196.8 Worst PE 2502.7 N/A b 2197.2 Best param c 1592.7 0.64 0.45 Pred(1) d 1139.2 1164.0 3021.6 Pred(3) d 893.0 1274.9 2501.2 Pred(5) d 767.7 1099.4 1765.0 a With 400 generations.  b Because nine runs resulted in negative parameter values. c The average relative error of the best parameter solution.  d Pred(k) indicates the mean square distance of the next  k  time points. The comparison criteria are as follows: the average runtime; the average, best and worst prediction errors; the average relative error of the best parameter set; and the quality of data extrapolation. Each bold face indicates the best among the four. We next applied the four methods to model  . The comparison of the four methods is summarized in  Table 3 . Among the four methods, the two model decomposition-based methods were much more efficient than the other two methods. TDDM was the fastest with its average runtime being 26 min, and PEDI was a close second with its average runtime being 33.5 min. Although HEKF + MM was the most efficient method for the 2 three-gene models ( Tables 1  and  2 ), it was more than four times slower than PEDI in this experiment. These results, coupled with the results from models   and  , show that model decomposition-based methods can scale better than typical parameter estimation methods. The least computational efficient method was SRES. We ran SRES with 2000 generations of evolution, which, on average, took more than eight times longer than PEDI did. However, the quality of the estimates from SRES was just on a par with that of TDDM.
 Table 3. Comparison of the results from model  Method PEDI SRES a HEKF + MM TDDM Runtime 33.5 min 279.5 min 143.5 min 26.0 min Average PE 2399.6 N/A b 2250.1 Best PE 2174.6 1477.2 2195.9 Worst PE 2735.5 N/A b 2587.5 Best param c 1.6 0.36 0.27 Pred(1) d 595.3 176.1 558.5 Pred(3) d 525.0 150.7 577.6 Pred(5) d 461.1 182.2 569.0 a With 2000 generations.  b Because four runs resulted in negative parameter values.  c The average relative error of the best parameter solution.  d Pred(k) indicates the mean squared distance of the next  k  time points. The comparison criteria are as follows: the average runtime; the average, best and worst prediction errors; the average relative error of the best parameter set; and the quality of data extrapolation. Each bold face indicates the best among the four. In the other three criteria, PEDI outperformed the other three methods substantially. In terms of the quality of estimated parameters, PEDI performed substantially better than the other three methods. By comparing the best parameter solution of each method, PEDI came out to be the most accurate one with its prediction error being at least twice as good as the other methods. PEDI outperformed the other methods in terms of the accuracy of both the reconstructed dynamics and the parameter values ( Fig. 4  and  Supplementary Fig. S13 ). In addition, the average parameter solution of PEDI had at least 30% lower prediction error than the best parameter solution of any other ( Table 3 ). Furthermore, the parameter sets from PEDI were the closest to the true parameter set on average, and the best parameter solution from PEDI had only 16% error to the true parameter set ( Supplementary Fig. S14  and  Table 3 ). PEDI was also able to extrapolate the data at the next time points much more accurately than the other three. As data extrapolation of biochemical dynamics—especially those with transient behaviors—is a challenging problem, this highlights the importance of high-throughput parameter estimation methods, which can generate high-quality parameter sets in a timely fashion to ultimately facilitate construction of a predictive model for given biological phenomena. To test whether our performance results remain intact in parameter estimation of a variant of seven-gene repressilator model, we modified   by adding a repression connection from  g 3  to  g 6 . Our results show that PEDI produced higher quality parameter solutions much more efficiently than the other methods (see  Supplementary Section S5 ). Taken together, we found that PEDI was able to consistently produce high-quality parameter estimates under various conditions in a computationally efficient matter.
 Fig. 4. Comparison of the four methods based on the estimated parameter set with the smallest prediction error from model  . This shows the results for mRNAs  m 1 ,  m 2  and  m 3 . The left-hand side panels show the comparison for the reconstructed trajectory. The numbers in the parentheses indicate the prediction error. The dotted red curve shows the true trajectory, whereas the red square points indicate the synthetic data points. The right-hand side panels show the comparison of the parameter combination generating the estimated trajectory for the four methods. Here, the red point in the middle of each side for the parameter comparison indicates the true value of each parameter 3.3 Parameter estimation with  yeast  microarray data To compare the performance of the parameter estimation methods using experimental data, we used time series microarray experiments of the genomic expression patterns in the yeast  Saccharomyces cerevisiae  responding to several environmental changes ( Gasch  et al. , 2000 ). We modeled a gene circuit involving genes GCN4, LEU3 and ILV5. GCN4 is a master regulator of many genes including those for the amino acid biosynthesis pathway ( Natarajan  et al. , 2001 ). LEU3 is a gene encoding a transcription factor that regulates genes involved in amino acid biosynthesis, whereas ILV5 encodes an enzyme that catalyzes amino acid biosynthesis ( Friden and Schimmel, 1988 ;  Zelenaya-Troitskaya  et al. , 1995 ). The network structure of these three genes is reported to follow the network motif called the coherent type 1 feed-forward loop ( Mangan and Alon, 2003 ). We described the model of this feed-forward gene circuit by the following system of ODEs:
 
where  f 1  is a piecewise polynomial function of  t  and  m 1 ,  m 2  and  m 3  are the mRNA copies of GCN4, LEU3 and ILV5, respectively, whereas  p 1 ,  p 2  and  p 3  are proteins Gcn4p, Leu3p and Ilv5p, respectively. As in  Cao and Zhao (2008) , we estimated the time evolution of  m 1  with a smoothing method based on spline. In this experiment, we set   and   to have protein stability higher assuming that each transcription rate is close to the maximum value when the corresponding expression profile is at the highest. With this setting, we estimated eight unknown parameters in the regulation of genes LEU3 and ILV5. In this experiment, we did not use HEKF + MM, and we just compared PEDI with SRES (with 400 generations of evolution) and TDDM. This is because the dataset contains only one time course microarray data sample with eight time points, with which we could not satisfactorily estimate the covariance matrix that HEKF + MM demanded. To quantify the performance, we measured the mean squared error of each estimate with respect to the observed mRNA data points, and we compared the best parameter solution from each method that gave the lowest error. We found that the error from the best solution of PEDI was 0.02, whereas the errors from the best solutions of SRES and TDDM were more than 0.2 and 0.3, respectively ( Supplementary Table S4 ). This shows that PEDI was able to approximate the dynamics of the RNA copy of LEU3 and ILV5 well. Indeed, the reconstructed dynamics from the best parameter solution of PEDI shows a close agreement between the results from PEDI and the time series microarray data ( Fig. 5 ). The average computational time of PEDI, SRES and TDDM is 3.3 min, 143.3 min and 1.7 min, respectively. These results once again show that PEDI was able to generate a high-quality estimate efficiently.
 Fig. 5. The reconstructed dynamics from the best parameter solution of PEDI for the yeast feed-forward loop model. The dynamics of mRNA GCN4 was estimated by a smooth piecewise polynomial function. PEDI predicted the dynamics of mRNAs, LEU3 and ILV5 By using the best parameter solution from PEDI, we next analyzed regulatory mechanisms of this gene circuit. By looking at the parameters controlling the binding affinity of Gcn4p to the  cis -regulatory elements of gene LEU3 and gene ILV5, we found that Gcn4p had close to 20% higher binding affinity to the binding site for gene ILV5. As the protein–DNA binding cooperativity estimated by PEDI was high ( Supplementary Table S4 ), we expect the change in transcription rates of ILV5 to be a switch-like, sigmoidal function of both Gcn4p and Leu3p. Thus, the differential binding affinity of Gcn4p allows for a delay in turning the ILV5 gene on after Gcn4p is turned on, as this AND-gate type transcription regulation of ILV5 in this model requires higher concentrations of both Gcn4p and Leu3p to turn ILV5 on. In the normal nonstarvation conditions, Gcn4p level is tightly controlled with a means of a rapid degradation through the ubiquitin pathway. On the other hand, Gcn4p level substantially increases in the amino acid starvation condition ( Kornitzer  et al. , 1994 ). Our results show that the delay caused by the differential binding affinity in the feed-forward loop may serve as an extra layer of protection to ensure that this amino acid biosynthesis pathway is only activated under the starvation condition. As our hypothesis is based on one type of time series expression dataset with only eight time points, however, it needs to be taken with caution and further analysis is required to contrast it with alternative explanations. Indeed, our hypothesis can be validated experimentally; by changing the Gcn4p binding site for gene ILV5 to have a lower binding affinity, it predicts that the regulation of the amino acid biosynthesis pathway will be disrupted more easily. 4 DISCUSSION The parameter estimation problem in modeling of biological systems is challenging, as it usually involves many (often infinite) suboptimal solutions. Efficient and scalable parameter estimation is crucial to the systematic construction of quantitative models that support existing knowledge of complex biological systems and, more broadly, to the success of integrative systems biology going forward. Here, we have introduced a novel computational framework, which, instead of considering general applicability, is customized especially for parameter estimation of gene circuit models. To see how PEDI performs in comparison with state-of-the-art parameter estimation methods, we applied SRES, HEKF + MM and TDDM to the parameter estimation of the same gene circuit models with the same datasets. We found that PEDI consistently gave the most accurate estimates in a computationally efficient matter. To test how PEDI performs given experimental gene expression data, we applied it to modeling of a yeast gene circuit from time series microarray data. We found that the reconstructed dynamics from PEDI closely agreed with the experimental data, and by analyzing the estimated parameter set, we were also able to make a testable hypothesis for an underlying regulatory mechanism of this gene circuit. Although PEDI can be applied to gene circuits with an arbitrary size and degree of transcriptional interaction connectivity, there are some limitations. For example, PEDI cannot directly support gene regulatory models including transcriptional elongation and posttranscriptional modifications. Although we can relax the conditions of PEDI so as to support more generic biological models by not requiring a model to have the form described by  Equation (3) , the efficiency and the accuracy of the model decomposition might decrease. While acknowledging the limited scope of the applicability, we believe that the value of a more tailored approach to the gene circuit domain far exceeds such potential drawbacks because of the importance of transcriptional regulation in quantitative understandings of cellular systems. By narrowing down our focus to gene circuit models, our customized approach was able to display two main advantages over the general parameter estimation methods: (i) it can make more appropriate assumptions about the kinds of gene expression data available for parameter estimation and (ii) it can exploit the structural information on gene circuit models—in particular, statistical thermodynamic-based gene circuit models. An additional practical benefit of PEDI is that it is relatively easy to implement in a lower level language such as C. 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>TE-greedy-nester: structure-based detection of LTR retrotransposons and their nesting</Title>
    <Doi>10.1093/bioinformatics/btaa632</Doi>
    <Authors>Lexa Matej, Jedlicka Pavel, Vanat Ivan, Cervenansky Michal, Kejnovsky Eduard, Luigi Martelli Pier</Authors>
    <Abstract/>
    <Body>1 Introduction Genomes of most eukaryotic organisms contain repetitive sequences present as dispersed repeats created by different classes of transposable elements (TEs) ( Kapitonov and Jurka, 1999 ;  Smit, 1999 ). The dispersed repeats are produced throughout evolution by the activity of TEs, often in transposition bursts of various intensities, with many transposition events happening in a short evolutionary time frame, such as those after polyploidization events ( Hirochika, 1997 ;  Vicient and Casacuberta, 2017 ). In genomes with high TE content, some insertions necessarily result in the fragmentation of another transposon already present at that particular insertion locus. This leads to nesting where only the youngest full-length copies can be recognized by software not accounting for fragmentation. Previous estimates of TE nesting in plant genomes ranged from no nesting detected in  Physcomitrella patens  to 14.3% of TEs fragmented by a TE insertion in  Oryza sativa  ( Gao  et al. , 2012 ). There are many tools and approaches searching for repeated sequences and their families ( Bergman and Quesneville, 2007 ;  Saha  et al. , 2008 ;  Valencia and Girgis, 2019 ). An exhaustive list has been published in a recent review ( Goerner-Potvin and Bourque, 2018 ). To discover nesting, people have come up with strategies to identify transposon fragments that may have originally been a part of a full-length element. Perhaps the most popular is RepeatMasker in its newer version  http://www.repeatmasker.org . It identifies fragments based on sequence similarity to a library of known repeats and stitches together nearby fragments that look like they are continuations of each other when mapped to a model element. LTRtype ( Zeng  et al. , 2017 ) identifies different types of structurally complex LTR retrotransposon elements as well as the nested configuration of these TEs. The system is capable of rapidly scanning large-scale genomic sequences characterizing eight complex types of LTR retrotransposon elements in addition to the common configuration of two LTR sequences positioned around an internal sequence with protein coding regions. The authors claim the program is able to correctly annotate a large number of structurally complex elements as well as nested insertions. It includes complex elements, e.g. those made of up to two internal sequences and three LTRs. REannotate ( Pereira, 2008 ) processes RepeatMasker annotation for: automated defragmentation of dispersed repetitive elements; resolution of the temporal order of insertions in clusters of nested elements; and the estimation of the age of elements with long terminal repeats. Another specialized software tool is TEnest, for untangling nested insertions of LTR retrotransposons, also using sequence similarity and classification of identified repeats into families ( Kronmiller and Wise, 2008 ,  2013 ). If nearby fragments belong to the same family, the software will recognize them as a valid power set and assign them to the same full-length element, thus establishing a nesting order. Greedier ( Li  et al. , 2008 ) is an alignment-based software tool also used to discover nested insertions of transposons.  Stitzer  et al.  (2019)  mentioned a recursive approach of annotating nesting of TEs, although no software is provided or mentioned in their paper. All available tools rely heavily on the evaluation of sequence similarity at some key step but it is evident that the structure of elements (order of domains and regulatory regions) can help to reconstruct fragmented elements. Structure-based tools are specifically available for certain classes of repetitive sequences, such as LTR retrotransposons ( Ellinghaus, 2008 ;  McCarthy and McDonald, 2003 ;  Xu and Wang, 2007 ), however, none are capable of recognizing element nesting. Therefore, here, we present an alternative approach to detect nesting, using structure-based recognition of repetitive sequences, relying primarily on identification of component features of a typical transposon and their relative position. 2 Algorithm and implementation We felt there is a possibility for improvement on TE detection by combining structure-based tools with a greedy algorithmic approach that would eliminate all detected TEs from analyzed sequences before going to the next round of detection. Initial rounds of analysis would help reconstruct many TEs that were fragmented by insertion of younger TEs but underwent little other change. Such an approach is inherently modular, it allows us to use an external, independently tested tool to detect LTR retrotransposons (or any other classes and tools in future modifications) and to separate full-length TE detection from their scoring and establishment of the most likely nesting order. Besides developing the TE-greedy-nester (labeled as TE-g-nester in all figures and tables), we also used the common code base for creating an application that runs in the opposite direction, to generate sequences containing nested TEs (‘TE-generator’). The TE-generator can be used for the limited testing of TE-greedy-nester. The TE-greedy-nester, TE-generator and other softwares are available from our GitLab project homepage at  https://gitlab.fi.muni.cz/lexa/nested . It is written in Python and will run on most Unix/Linux platforms. It requires prior installation of LTR FINDER ( Xu and Wang, 2007 ), BLAST ( Altschul  et al. , 1990 ) and GenomeTools ( Gremme  et al. , 2013 ). An installation script and a link to an Ubuntu virtual machine with the latest version installed are provided. We have also previously packaged our tools for integration and easier deployment. A Snakemake pipeline was created to run TE-greedy-nester on larger datasets and store results of the analysis in GFF files and also in a relational database  http://hedron.fi.muni.cz/TEDb/index.html . A Linux Mint virtual machine has been created to enable users not only to work with the above pipeline avoiding potential installation issues but also to modify it to meet their specific requirements  http://hedron.fi.muni.cz/TE-nester_Mint.zip . 2.1 TE-greedy-nester Our main goal was to design an application capable of processing sequences automatically and finding nested TEs in reasonable time. We needed to address specific problems related to the correct detection of element nesting. First, while sensitive enough, the procedure should be resistant to detecting false positives. To this end, we incorporate a greedy algorithm that evaluates several possible candidates for full-length TEs but ultimately picks only the best ones, based on the presence of typical full-length TE sequence features. As a result, false positives are quite rare initially and may become more frequent at later stages which, however, can be stopped at that point. To support precision, we chose to use LTR FINDER ( Xu and Wang, 2007 ), a TE detection tool that showed low false-positive results and high precision in our experience as well as recent tests ( Valencia and Girgis, 2019 ). Another requirement is the ability to detect deep nesting. In such cases, the oldest elements are barely recognizable because of ageing and the procedure must allow for imperfections without compromising the ability to detect the partly eroded elements. Several rounds of design produced a procedure according to the following pseudocode (see also Fig. 1A ) where capitalized terms in parentheses represent typical components of an LTR retrotransposon, non-coding sequences (LTR, PBS, PPT and TSD) detected by LTR Finder and conserved protein-coding sequences often called protein domains (GAG, PROT, RH, RT, INT and CHD) detected using BLASTX.
 Fig. 1. Nesting algorithm essentials. ( A ) Algorithm overview showing data processing in TE-greedy-nester; ( B ) Scoring graph structure used to evaluate structural completeness of LTR retrotransposon candidates (shown as SCORE CANDIDATES in panel A). Any deviation from prescribed order of structural components (full arrows) is penalized (dotted arrows) 
 
 algorithm TE-greedy-nester is 
   input: DNA sequences   while changed(export_TEs) = TRUE do    foreach DNA sequence do   with LTR Finder detect module = (LTRs|PBS|PPT|TSD)      save TE     with BLASTX get domain  =   (GAG|PROT|RH|RT|INTT|CHD)     foreach TE do     hsno_TE = calculate_score(TE, domain, module)     //using greedy algorithm, scoring graph   save hsno_TE//highest-score_non-overlapping TE   removed_positions = positions(hsno_TE)   move removed_positions to export_TEs   save removed_positions//to adjust export_GFF 
 export_GFF = adjust(export_GFF, removed_ positions) 
 
 output: export_TEs, export_GFF 
 
 Evaluation of full-length TE candidates is done by constructing a weighted directed graph, where nodes represent required sites in a full-length element (such as domains, PBS, PPT and TSD) ( Fig. 1B ). The program is designed to find a path from the left LTR to the right LTR, whilst visiting every required node in the correct order (domains are ordered differently in Gypsy and Copia families, some, like ENV are family-specific, all components are optional). By assigning weights to the edges, we prioritize a path that has as complete a structure as possible. At the same time, we allow alternative paths with respective penalties if there is a missing node, or an incorrect order of available nodes. The graph structure is quite universal and is open for future refinements. We also need a way to recover various subsequences of the analyzed sequence, such as the original unfragmented sequences of older TEs fragmented by nesting and the identified features annotated to the analyzed sequence. This is achieved by a procedure where the removed sequences are virtually returned to their positions in the genome and the coordinates of TEs and their features are adjusted for the inserted elements. Once all TEs that have been removed in the first phase are processed, we generate a GFF3 file with coordinates that map to the analyzed sequence. The final GFF output file can be used to visualize all the identified features with specialized software, such as Genome Tools Annotation Sketch ( Fig. 2 ) ( Gremme  et al. , 2013 ), a genome browser, such as IGV ( Robinson  et al. , 2011 ;  Thorvaldsdottir  et al. , 2013 ), or to extract sequences for certain features using e.g. bedtools ( Quinlan and Hall, 2010 ). In addition, the sequences of all TEs detected by TE-greedy-nester are clipped out of the genomic matrices and stored in FASTA format in a separate directory.
 Fig. 2. An example of TE-greedy-nester output visualized using Annotation Sketch from the Genome Tools ( Gremme  et al. , 2013 ) software suite (Command:  gt sketch output.png example.gff ) 2.2 TE-generator To carry out tests of the software, especially its ability to recover nested sequences, we used TE-generator, part of the code that is designed to carry out virtual insertions of TE sequences from a library into a background sequence. The precise position of each inserted sequence in the resulting test sequence is recorded, to compare the generated GFF3 file with results of analysis of the same sequence by TE-greedy-nester. 3 System and methods 3.1 Testing data 3.1.1 Randomly inserted sequences A first group of testing sequences was prepared as a random mixture of full-length sequences from maize. A databases of full-length TEs were downloaded from Maize TE Database at  http://people.oregonstate.edu/∼fowlerjo/MaizeRepeatDatabases/uniqueTEDB_1526.fasta.txt  in 2018 and Mips-REdat database at  ftp://ftpmips.helmholtz-muenchen.de/plants/REdat/mipsREdat_9.3p_ALL.fasta.gz  ( Nussbaumer  et al. , 2013 ) for maize and rice full-length TEs, respectively. The generator module of TE-greedy-nester was used to generate 10 Mbp sequences with 10 and 90% TE sequence content. Settings were calculated from average TE length (see commandsUsed.txt in  Supplementary Materials ). 3.1.2 Deeply nested sequences (Russian doll/Matryoshka) To test the depth of nesting discoverable by existing tools, as well as TE-greedy-nester, we wrote a short program (see file matryoshka.pl) which creates annotated sequence files with an arbitrary depth of mutually nested TE elements. It complements TE-generator, which also creates pockets of multiply nested TEs in the generated sequence, however, TE-generator is biased towards shallow nested sets. Matryoshka is written in Perl (code available in  Supplementary Materials ). The program takes a multifasta file of LTR retrotransposon sequences as input, samples it  i  times ( i  can be set on the command line), nesting each consecutive sequence into a random position of the previously inserted sequence. FASTA and BED files are written as output. Two sets of sequences containing 20 TEs taken from either  Zea mays  TE database (zea_matryoshka.fa) or  O.sativa  (oryza_matryoshka.fa) were generated. 3.1.3 Maize  adh1  neighborhood sequence To test TE-greedy-nester on biological sequence which is rich in nested LTR retrotransposons with known position and annotation, the  Z . mays  alcohol dehydrogenase 1 gene ( SanMiguel  et al. , 1998 ) ( adh1-F  gene accession number: AF050457.1) flanked with 150 kbp on both 5′ and 3′ regions, was analyzed. The maize genome was downloaded from Phytozome 12.0  https://phytozome.jgi.doe.gov/pz/portal.html  ( Goodstein  et al. , 2012 ). TE sequences obtained by TE-greedy-nester were annotated using the maize specific retrotransposon database multifasta  http://people.oregonstate.edu/∼fowlerjo/MaizeRepeatDatabases/uniqueTEDB_1526.fasta.txt  and BLASTN tool ( Altschul  et al. , 1990 ). Thereafter, the GT Annotation Sketch figure from TE-greedy-nester was rearranged following TE orientation given in the study by  SanMiguel  et al.  (1998)  as later adapted by  Fedoroff (2012) . 3.1.4 Maize sequence (first 2 MB from Chr10) The second biological sequence from the maize genome, Chromosome10: 0–2 Mb, was used previously to compare LTRtype, REannotate and TEnest tools ( Zeng  et al. , 2017 ). We used this sequence to maintain continuity and to obtain reliable data for users of TE-greedy-nester. 3.2 Testing software and data analysis Performance of TE-greedy-nester and other software able to detect TE nesting (TEnest, LTRtype and REannotate) were tested on three different types of data: (i) synthetic data with known randomly inserted sequences from maize and rice (TE-generator and matryoshka), (ii) thoroughly studied and annotated  adh1  locus from maize and (iii) biological data analyzed by other similar software (2 MB from maize Chr10). Testing on synthetic data was done by comparing GFF files produced by TE-greedy-nester with TE-generator and matryoshka GFF files describing the introduced nesting. We calculated the number of intervals representing TEs that overlap by a predefined percentage of length on both sides using the python script compGffs2Generator.py ( Supplementary Materials ). Testing on the  adh1  locus was done by visual inspection of published  adh1  annotations ( Fedoroff, 2012 ) and our own visualization with GT Annotation Sketch. Testing on biological data was carried out by counting the number of detected full-length TEs in assembled or partly assembled genomes of 18 species downloaded from Phytozome or other sources (where applicable):  Arabidopsis thaliana ,  Arabidopsis lyrata ,  Azolla filiculoides ,  Brachypodium distachion ,  Chlamydomonas reinhardtii ,  Dunaliella salina ,  Glycine max ,  Gossypium raimondii ,  Lotus japonicus ,  Medicago truncatula ,  Micromonas pusilla ,  Musa acuminata ,  O.sativa ,  P.patens ,  Populus trichocarpa ,  Pseudotsuga menziesii ,  Solanum lycopersicum  and  Sorghum bicolor . To confirm a quality of TEs retrieved by TE-greedy-nester, their long terminal repeat (LTR) sequences were cut with bedtools package ( Quinlan and Hall, 2010 ) and subsequently LTR identity was measured using global alignment by STRETCHER tool (Emboss 6.6.0;  Rice  et al. , 2000 ). 3.3 Performance measures Performance measures used here were calculated according to the formulas used in the study by  Ou and Jiang (2018) Sensitivity = TP / ( TP + FN ) Specificity = TN / ( FP + TN ) Accuracy = ( TP + TN ) / ( TP + TN + FP + FN ) Precision = TP / ( TP + FP ) where true positive (TP) stands for matches of coordinates of TE found by tested tools and corresponding element given by TE-generator within tolerance ±5% of reference TE length at the start and end positions. Correspondingly, false positive (FP) is a TE with no match with any TEs in the generated sequences, a false negative (FN) are TEs which were present in generated sequence and absent in output GFFs from given tools, true negative (TN) is estimated TE count from number of bases which are without TE in both GFF from TE-generator and also from tested tools. 3.4 Speed and RAM performance Resource utilization of the four different software tools was compared on all the datasets used in this article. For evaluations, the programs were run on a dedicated Linux machine running no other job, using the GNU time command to obtain ‘real time’ and ‘maximum resident set size’ as an average of three runs. The machine had 12 GB physical RAM, a 4 core Intel i5 CPU. The running process was monitored for signs of memory swapping and process pruning. 4 Results We developed the TE-greedy-nester software for finding nested LTR retrotransposons using a combination of a greedy algorithm and identification of full-length TEs. In contrast to comparable software relying mostly on sequence similarity, TE-greedy-nester is based on identification of structural features of LTR retrotransposons. We compared the performance of TE-greedy-nester with four other related software tools. The number of features detected by TE-greedy-nester on different types of data is reported in  Table 1 . We found that TE-greedy-nester detected the highest number of TEs in all tested sequences in comparison with other examined tools. Moreover, despite the higher rate of false-positive identification, TE-greedy-nester also retrieved the highest count of TEs matched with elements present in annotated sequences (number of TEs matched,  Table 1 ). To better evaluate the performance of TE-greedy-nester, we carried out a deeper analysis using both synthetic and biological data.
 
 Table 1 . Number of detected or expected full-length LTR retrotransposons by TE-greedy-nester, TEnest, LTRtype and REannotate on natural (rows 1–2) and artificial (rows 3–6) testing sequences Sequence name Seq. length (bp) No. of reference TEs No. of TEs found No. of TEs matched (tolerance = 0.01) TE-g-nester TEnest LTRtype REannotate TE-g-nester TEnest LTRtype REannotate Zm_adh1 302987 21 15 11 7 6 — — — — Zm_Chr10_2Mb 2000001 — 157 46 60 21 — — — — Zm_synth_10 10141285 260 78 77 42 30 35 36 24 14 Zm_synth_90 10271500 2329 774 177 250 193 72 15 36 16 Zm_matryoshka_20 120219 20 16 14 2 2 13 11 0 0 Os_synth_10 10190963 100 90 16 6 3 78 6 6 2 Os_synth_90 9562586 900 729 71 44 24 291 1 19 8 Os_matryoshka_20 198560 20 14 4 0 0 1 4 0 0 4.1 Annotated synthetic data (TE-generator and matryoshka) To test TE-greedy-nester on synthetic data, we generated artificial sequences using TE-generator. The TE-generator sequences had medium levels of nesting. We also prepared sequences with extreme depth of nesting, only inserting new sequences into previously inserted ones and call them ‘matryoshka’. After running TE-greedy-nester on these sequences, we evaluated sensitivity, selectivity, precision and accuracy (see Section 3). Sensitivity and precision measure the ability to detect sequences, selectivity measures the ability to reject false positives and accuracy is a combination of both. Calculated comparative performance values are shown in  Figure 3 . While TE-greedy-nester showed higher sensitivity for all available data, its comparative accuracy gave mixed results, with lower specificity (higher false-positive rate) on synthetic data with high TE density (90%) ( Fig. 3B ). On the other hand, TE-greedy-nester was superior to all existing software on synthetic data with deep nesting (matryoshka). While TEnest could compete with TE-greedy-nester when provided with the proper TE database (maize) ( Fig. 3C ), TE-greedy-nester was the only software that could detect deep nesting correctly on mixed-origin TE data ( Fig. 3D ), showing an even higher accuracy on mixed data than maize data. TE-greedy-nester was also one to two orders of magnitude faster than TEnest on all datasets.
 Fig. 3. Sensitivity, specificity, accuracy and precision of TE-greedy-nester and comparable software on synthetic and biological data; ( A ) zea_10%; ( B ) zea_90%; ( C ) zea_matryoshka; ( D ) oryza_10%, ( E ) oryza_90% and ( F ) oryza_matryoshka Because the above performance evaluations on annotated data partly depend on the definition of successful TE identification, we examined the effect of position tolerance on performance measures in the same four annotated synthetic datasets ( Fig. 4 ). It can be seen that low copy datasets, such as matryoshka, produce the same number of TEs at the lowest used tolerance of 1% ( Fig. 4C and D ). TE rich data from TE-generator show an increased TE discovery at higher tolerance level, most likely as a result of increased false-positive rate, this was more apparent in sequences with 90% TE coverage ( Fig. 4B ) than in those with 10% TE coverage ( Fig. 4A ).
 Fig. 4. Number of correctly identified TEs as a function of length tolerance by the four software tools; ( A ) zea_10%; ( B ) zea_90%; ( C ) zea_matryoshka; ( D ) oryza_10%, ( E ) oryza_90% and ( F ) oryza_matryoshka While  Figure 4  shows the overall performance values of the different tools tested, their abilities to discover nested TEs remain partly hidden in the lump sum numbers. We therefore divided the counts of identified TEs based on their nesting status, including their nesting level (how many successive nesting events occurred within the given TE internal region) ( Fig. 5 ). The most striking finding is that all tools overestimate the number of non-nested elements and underestimate the number of nested ones ( Fig. 5A and B ). TEnest was less prone to this error in high-density TE data (90% TEs;  Fig. 5B ), in accordance with its higher specificity on the same data ( Fig. 3B ). LTRtype and REannotate missed more TEs than the other two tools. Only TEnest and TE-greedy-nester were able to resolve the majority of the deeply nested matryoshka dataset ( Fig. 5C and D ). The same tendency could be seen at different nesting levels in 90% TE-generator data. TEnest and TE-greedy-nester always had much higher counts than the other tools at nesting levels II and deeper. TEnest gave higher counts than TE-greedy-nester at nesting levels IV and higher, however, most of those above nesting level VI were false positives ( Fig. 5B ). The best performance of TE-greedy-nester on mixed origin matryoshka data (generated with TE sequences from multiple plant species) observed in  Figure 5D  can be seen here as well.
 Fig. 5. Number of correctly identified TEs at different nesting levels by the four software tools; ( A ) zea_10%; ( B ) zea_90%; ( C ) zea_matryoshka; ( D ) oryza_10%, ( E ) oryza_90% and ( F ) oryza_matryoshka For a better perspective of individual tool performance, we also show the data from this analysis in the Integrated Genome Browser ( Robinson  et al. , 2011 ) ( Supplementary Fig. S1 ). While both TE-greedy-nester and to a limited extent also TEnest had a tendency to overestimate certain types of TEs (false positives), in the overall visualization, TE-greedy-nester results render best the overall density and nesting depth distribution of TEs along the sequence. 4.2 Biological data After testing on synthetic data, we applied the compared tools to biological data, namely (i) the well-studied  adh1  region from maize and (ii) a 2-MB region from maize chromosome 10 used in a previous comparative study by  Zeng  et al.  (2017) . 4.2.1 
 Adh1  region TE-greedy-nester as well as the other three compared tools was tested on the  adh1  region in which 21 full-length LTR retrotransposons (of which 12 are nested) were found by  SanMiguel  et al.  (1998) . TE-greedy-nester detected 15 (7 nested), TEnest detected 11 (1 nested), LTRtype 7 (0 nested) and REannotate 6 (1 nested) ( Fig. 6 ). Only four of these full-length LTR retrotransposons were identified by all tools, while six were common for TEnest and TE-greedy-nester results, as can be seen in  Supplementary Figure S2A .
 Fig. 6. Number of correctly identified TEs at different nesting levels by the four software tools in biological data; ( A ) zea_adh1 and ( B ) zea_2MB To compare  adh1  outputs from TE-greedy-nester with the original SanMiguel report on family level, TEs from TE-greedy-nester were additionally annotated using maize-specific TE database  http://people.oregonstate.edu/∼fowlerjo/MaizeRepeatDatabases/uniqueTEDB_1526.fasta.txt  and a locally installed BLASTN ( Altschul  et al. , 1990 ) tool ( Supplementary Fig. S3 ). Although the TE annotations from TE-greedy-nester and  Fedoroff (2012)  do not fully match, we counted 12 families that were correctly recognized and placed within the segment. 4.2.2 2 MB region of maize Chr10 We used all compared software tools for an analysis of LTR retrotransposons in the initial 2 MB region of maize chromosomes 10. The maize genome was chosen because of its high content of LTR retrotransposons that are often nested and that it was previously tested by other authors on the same sequence. The results for this maize segment ( Supplementary Fig. S2B ) were in line with results on 90% TE-generator data. TE-greedy-nester found the most TEs, TEnest was the most conservative in the number of non-nested TEs and LTRtype and REannotate were not able to find full-length TEs beyond nesting level I. 4.3 Performance tests To compare the four evaluated software tools also by computational performance and requirements, we recorded computation times and peak physical memory usage on the data described above ( Table 2 ). TEnest, which performed very well in nesting accuracy tests above, was the slowest and most memory hungry of the four (worst case 8719 s, 12GB RAM). With the largest datasets, it caused the system to swap memory and kill processes (shown as &gt;12GB RAM in 2) resulting also in a steep increase of computation time. Our TE-greedy-nester was comparable with LTR Type and REannotate in terms of speed and memory usage (worst case 886 s, 628 MB). While it performed better with RAM on extremely small datasets. LTR Type used slightly more RAM (752 MB versus 500–600 MB) and was also slower to compute the results on small datasets (36s versus 13–21 s). It should be noted that TEnest was set to use four processors, while TE-greedy-nester used multithread BLASTX searches (3 processors). LTR Type and REannotate relied on RepeatMasker output, however, the time to generate that output was included and tabulated as well. For further details, please, see Section 3.
 
 Table 2 . Time and memory requirements of tested software when analyzing input data of different types Species Sequence TE-g-nester TEnest LTRtype REannotate Process time (s) 
  Z.mays 
 synt10 174 83 297 338 synt90 650 8917 740 657 matryoshka 16 544 26 11 adh1 14 1052 36 21 Chr10_2Mbp 331 7866 136 122 
  O.sativa 
 synt10 187 6562 322 330 synt90 886 3072 145 525 matryoshka 13 57 36 15 Maximum memory usage (Mbytes) 
  Z.mays 
 synt10 591.5 146.4 740.5 489.0 synt90 628.1 &gt;12000 740.7 536.3 matryoshka 74.8 18.2 739.5 482.4 adh1 77.1 351.5 741.2 482.2 Chr10_2Mbp 152.7 11800.0 752.2 482.1 
  O.sativa 
 synt10 595.7 &gt;12000 740.4 488.0 synt90 603.7 &gt;12000 740.4 517.9 matryoshka 80.2 18.6 740.7 482.2 
 Note:  Time and memory usage were measured as ‘real time’ and ‘maximum resident set size’ using the Linux time command on a dedicated machine with 12 GB RAM, Intel i5 processor with four cores, Fedora Linux installed and no other jobs running. 4.4 Plant genomes Finally, we applied TE-greedy-nester to 18 plant genomes available mostly from Phytozome (see Section 3) and provide the results in GFF files ( http://hedron.fi.muni.cz/TEgnester/ ). In  Table 3 , we show the classification and the main characteristics of TE nesting in these 18 species. We demonstrated that 96.4% of detected TEs (i.e. 98 187 out of 10 1887), have LTR identity 80% or higher ( Supplementary Fig. S4 ). TE-greedy-nester also found at least one protein domain in 29.8–88.4% of identified LTR retrotransposons in vascular plants but only 0–1.9% in non-vascular plants. The percentage of TEs found in nested configuration was between 19.6 and 54% in vascular plants and 0 and 17.6% in non-vascular plants. The highest nesting rates were observed in  S.bicolor  and  G.max . The proportion of solo LTRs was higher in eudicots and algae (e.g.  Solanum ,  Gossypium  and  Dunaliella ) than in monocots.
 
 Table 3 . TE-greedy-nester result summary on genome sequences from organisms across the plant kingdom ordered by the detected rate of nesting Species Class Family Genome size (Mbp) All TEs TEs with domain a Annotation rate (%) Nesting rate (%) Solo LTRs Reported TEs Nested Isolated Sum Nested Isolated Sum 
 Selaginella moellendorffii 
 Isoetopsida Selaginellaceae 212.7 6704 3187 9891 2489 1756 4245 42.9 58.6 3247 ∼941 b 
 S.bicolor 
 Monocots Poaceae 732.2 19662 10128 29790 9947 8479 18426 61.9 54 8706 23915 c 
 G.max 
 Eudicots Fabaceae 978.5 19325 11203 30528 7128 8610 15738 51.6 45.3 20266 32370 d 
 M.acuminata 
 Monocots Musaceae 390.6 3304 3599 6903 1270 1842 3112 45.1 40.8 4917 ∼14400 e 
 P.trichocarpa 
 Eudicots Salicaceae 422.9 4455 3467 7922 1039 2106 3145 39.7 33 2747 1479 f 
 P.patens 
 Bryopsida Funariaceae 473.2 3353 6017 9370 2685 5598 8283 88.4 32.4 24643 ∼24261 g 
 Brachypodium  
 Monocots Poaceae 271.2 1668 2803 4471 848 1847 2695 60.3 31.5 5174 569 h 
 A.thaliana 
 Eudicots Brassicaceae 119.1 677 831 1508 196 429 625 41.4 31.4 422 376 i 
 S.lycopersicum 
 Eudicots Solanaceae 823.9 6895 9494 16389 2770 7234 10004 61 27.7 22591 2086 j 
 L.japonicus 
 Eudicots Fabaceae 462.5 1923 2706 4629 490 1534 2024 43.7 24.2 1974 ∼8930 k 
 A.lyrata 
 Eudicots Brassicaceae 206.7 1746 2820 4566 547 1713 2260 49.5 24.2 1318 ∼940 l 
 O.sativa 
 Monocots Poaceae 374.5 3361 5163 8524 929 3206 4135 48.5 22.5 6668 7043 c 
 M.truncatula 
 Eudicots Fabaceae 411.8 2685 4578 7263 462 1703 2165 29.8 21.3 4569 12250 m 
 G.raimondii 
 Eudicots Malvaceae 761.4 5064 9835 14899 1611 6620 8231 55.2 19.6 18393 13297 n 
 D.salina 
 Chlorophyceae Dunaliellaceae 343.7 20002 4314 24316 81 378 459 1.9 17.6 5586 ∼12572 o 
 C.reinhardtii 
 Chlorophyceae Chlamy domonadaceae 107.1 2360 1356 3716 0 47 47 1.3 0 181 ∼2230 o 
 A.filiculoides 
 Polypodiopsida Salviniaceae 750 238 270 508 0 0 0 0 0 459 17484 p 
 M.pusilla 
 Mamiello phyceae Mamiellaceae 22 20 41 61 0 0 0 0 0 116 ∼65 o 
 Note : ∼number of TEs was estimated from TE genome coverage data assuming 10kbp per TE. a Counted from TEs with annotated protein domain. b 
 Vanburen  et al.  (2018) . c 
 Jiang and Ramachandran (2013) . d 
 Du  et al.  (2010) . e 
 Hribova  et al.  (2010) . f 
 Cossu  et al.  (2012) . g 
 Lang  et al.  (2018) . h 
 Stritt  et al.  (2019) . i 
 Peterson-Burch  et al.  (2004) . j 
 Xu and Du (2014) . k 
 Holligan  et al.  (2006) . l 
 Civan  et al.  (2011) . m 
 Wang and Liu (2008) . n 
 Xu  et al.  (2017) . o 
 Goodstein  et al.  (2012) . p 
 Li  et al.  (2018) . 5 Discussion Here, we present a new bioinformatic tool TE-greedy-nester for the detection of nested LTR retrotransposons that is faster and more efficient in finding deeply nested elements than existing tools. These advantages are due to the combination of structure-based retrotransposon detection with recursive sequence removal. This results in comparatively low memory usage and high computation speed, as seen in performance tests presented herein. With the default settings our tool is competitively sensitive although can produce higher rates of false positives in some instances, as seen in both synthetic and real biological data. We are exploring ways to reduce the number of false-positive calls where several directions of action are possible, such as (i) optimizing the search parametrization and scoring of candidate TEs; (ii) improving TE annotation, especially by accounting for TSD sequences that should be present in bonafide full-length TEs and (iii) abandoning the greedy approach by introducing extra passes that would pre-score elements or explore multiple sequence fragmentation scenarios. Another advantage of TE-greedy-nester is its ability to identify nesting in different species without the need for species specific TE databases. This is in sharp contrast to the other three compared tools that lack performance in cross-species applications. Their results also differ significantly with the size and sparsity of the used TE database. While the focus now is on the nesting of LTR retrotransposons, the approach is modular and can be expanded to other classes of repetitive sequences by simply employing additional TE detection tools alongside LTR Finder. However, even without expansion to other TE classes, the algorithm in TE-greedy-nester can detect short foreign insertions in full-length LTR retrotransposons. This is the advantage of using structural information where the most important signal is the order of required TE components, while distance and sequence similarity is secondary. Both synthetic and biological data were chosen to represent different TE densities and levels of nesting to identify the strengths and weaknesses of the tested tools. In this respect, we found that TE-greedy-nester had the highest sensitivity across the board of different tests. As expected, high sensitivity typically comes with a higher proportion of false positives, and so it is important to look at accuracy and precision for an objective comparison of different tools. TE-greedy-nester showed markedly lower precision with 90% TE-generator data, suggesting that it could benefit from parameter fine-tuning depending on the TE density of the data being analyzed. Identifying the best parameter combinations for different types of data is beyond the scope of this article, where only fixed or default settings were used. TE-greedy-nester also found nested full-length TEs one to two orders of magnitude faster than TEnest, which gave the best results of the three compared tools across different tests. LTRtype was more conservative but still performed very well on TE-generator data of both densities. Compared to TEnest, it however failed to be competitive, together with REannotate, on simple data with deep nesting, such as the  adh1  locus or matryoshka data. It should be noted that LTRtype is a tool able to recognize composite LTR retroelements, something the other tools cannot do. Interestingly, running TE-greedy-nester on several species uncovered remarkable differences to previously published nesting estimates in certain species. For example, in  P.patens  no nesting was observed by  Gao  et al.  (2012) , while we saw 32% nested LTR retrotransposons. TE-greedy-nester development is a live ongoing project. While we were testing the software performance on nested full-length TEs, a new feature was added to the code base. TE-greedy-nester can now identify solo-LTRs in the analyzed sequence based on the sequences of the LTRs identified in all iterations. TE nesting reconstruction is important in genome evolution and TE life cycle studies. We hope it will help users excavate older full-length TE copies, differentiate between complex TEs transcribed as a single unit and nested TEs originating from many independent insertions. Based on the testing results, it may be useful in sequences with deeply nested structures, such as those in centromeric and pericentromeric regions of plant chromosomes. For such use, it might be useful to expand its abilities towards tandem repeat detection. In our observations, tandem repeats are one of the things that can confuse LTR Finder into interpreting some of their subsequences as pairs of LTRs. The situation becomes even more complicated when such tandem repeats originate from fragments of TE sequences containing fragments of canonical domain coding sequences ( Ahmed and Liang, 2012 ) or LTRs. TE-greedy-nester should also come handy in whole genome annotations where speed could be as important as precision. Our tool is similar to software mentioned by  Stitzer  et al.  (2019)  in two aspects. We also create a graph data structure to find the best TE candidates, the two structures, however, carry different types of data and are used for slightly different purposes. 6 Conclusion In this work, we present a new software tool for the recovery of full-length LTR retrotransposons fragmented by the nesting of other elements. We used a recursive approach in combination with structure-based detection of TEs with LTR Finder and implemented it in a Python tool called TE-greedy-nester. We tested this computational approach on synthetic and natural DNA sequences. Testing showed that TE-greedy-nester gave high-quality results faster than existing tools and is superior in selected parameters, especially in its ability to recover full-length LTR retrotransposons in deeply nested regions. Moreover, we analyzed 18 plant genomes and showed that TE-greedy-nester could be used in studies of TE life cycle and genome evolution, especially in areas where relative insertion times are important. Supplementary Material btaa632_Supplementary_Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>CRISPR-ERA: a comprehensive design tool for CRISPR-mediated gene editing, repression and activation</Title>
    <Doi>10.1093/bioinformatics/btv423</Doi>
    <Authors>Liu Honglei, Wei Zheng, Dominguez Antonia, Li Yanda, Wang Xiaowo, Qi Lei S.</Authors>
    <Abstract>Summary: The CRISPR/Cas9 system was recently developed as a powerful and flexible technology for targeted genome engineering, including genome editing (altering the genetic sequence) and gene regulation (without altering the genetic sequence). These applications require the design of single guide RNAs (sgRNAs) that are efficient and specific. However, this remains challenging, as it requires the consideration of many criteria. Several sgRNA design tools have been developed for gene editing, but currently there is no tool for the design of sgRNAs for gene regulation. With accumulating experimental data on the use of CRISPR/Cas9 for gene editing and regulation, we implement a comprehensive computational tool based on a set of sgRNA design rules summarized from these published reports. We report a genome-wide sgRNA design tool and provide an online website for predicting sgRNAs that are efficient and specific. We name the tool CRISPR-ERA, for clustered regularly interspaced short palindromic repeat-mediated editing, repression, and activation (ERA).</Abstract>
    <Body>1 Introduction The bacterial adaptive immune system, CRISPR (clustered regularly interspaced short palindromic repeats), was recently developed as a powerful and multi-purpose technology for genome engineering, including editing (modifying the genomic sequence) ( Cong  et al .,  2013 ;  Mali  et al. , 2013 ), and regulation (repressing or activating expression of genes) ( Gilbert  et al. , 2013 ,  2014 ;  Qi  et al. , 2013 ). The system is highly programmable, utilizing a single protein, the nuclease Cas9 for editing or the nuclease-deficient dCas9 for regulation. A single guide RNA (sgRNA) is required for precise and programmable DNA targeting ( Doudna and Charpentier, 2014 ). Effective and specific genome engineering requires careful design of sgRNAs, which remains a major challenge. Computational tools have been used to facilitate the design of sgRNAs for CRISPR editing but not for other applications such as transcriptional regulation. These computational tools should enable automated sgRNA design and off-target site validation ( Bae  et al. , 2014 ;  Doench  et al. , 2014 ;  Heigwer  et al. , 2014 ;  O'Brien and Bailey, 2014 ;  Xiao  et al. , 2014 ). A major goal of our designer tool is to address the discrepancy for designing sgRNAs that allow efficient and highly specific repression or activation of genes and for generating genome-wide sgRNA libraries for genetic screening in different organisms. Here, we describe CRISPR-ERA webserver, an automated and comprehensive sgRNA design tool for CRISPR-mediated  e diting,  r epression, and  a ctivation (ERA) ( Fig. 1 ). CRISPR-ERA utilizes a fast algorithm to search for genome-wide sgRNA binding sites and evaluates their efficiency and specificity using a set of rules summarized from published data for CRISPR editing, repression and activation ( Cong  et al . , 2013 ;  Doudna and Charpentier, 2014 ;  Gilbert  et al. , 2014 ;  Qi  et al. , 2013 ;  Ran  et al. , 2013 ). The design features are annotated and the target sites can be visualized in a genome browser. We also provide a local version for the generation of whole-genome sgRNA libraries.
 Fig. 1. CRISPR-ERA workflow and example. The CRISPR-ERA algorithm takes input information, including types of genome manipulation, organism, and gene name or genome location, and then computes and evaluates sgRNAs within the targeted genome region. By default, for editing, the algorithm chooses sgRNA sequences within coding region; for repression, the algorithm computes sgRNA binding sites within a 3 kb region centered at TSS (or the sense strand of the 5′ end of the gene for bacteria only); for activation, the algorithm computes sgRNA binding sites up to 1.5 kb upstream of TSS. In this figure, mouse gene  Sox2  is shown as an example. E, efficacy score; S, specificity penalty score ( Supplementary Methods ) 
 2 Methods For each target gene or genomic site, CRISPR-ERA first searches all targetable sites in that particular organism for patterns of N 20 NGG (N = any nucleotide). Each target sequence is then calculated for two scores ( Supplementary Methods ): (i) an efficacy score (E-score) based on the sequence features such as GC content (%GC), presence of poly-thymidine (which is a terminator for effective transcription of sgRNAs), and location information such as the distance from target gene transcriptional start sites (TSS); and (ii) a specificity score (S-score) based on the genome-wide off-target binding sites. For each sgRNA design, we compute the genome-wide sequences that contain an adjacent NRG (R = A or G) protospacer adjacent motif (PAM) site and zero, one, two, or three mismatches complementary to the sgRNA using Bowtie ( Langmead  et al. , 2009 ), which are regarded as off-target binding sites. The penalty score for NAG off-target is smaller than NGG off-target. The sgRNAs are finally ranked by the sum of E-score and S-score ( Fig. 1 ;  Supplementary Fig. S1 ). We implement a user-friendly web server ( http://CRISPR-ERA.stanford.edu ) that hosts the web application for the sgRNA designer tool. The webserver will host a broad category of sequenced organisms. Currently, it provides sgRNA design service for nine most commonly used prokaryotic and eukaryotic organisms including  Escherichia coli ,  Bacillus subtilis, Saccharomyces cerevisiae, Drosophila melanogaster, Caenorhabditis elegans, Danio rerio ,  Rattus norvegicus, Mus musculus,  and  Homo sapiens , etc .  ( Supplementary Table S1 .). The web application enables rapid searching in the pre-assembled sgRNA database using an indexing searching approach ( Supplementary Methods ). The program outputs the sequence, target location, off-target details of the possible sgRNAs, their E- and S-scores etc. Results can be visualized using the UCSC genome browser to highlight the custom tracks ( Supplementary Fig. S2  and  S3 ). 3 Conclusions CRISPR-ERA enables easy, fast, and predictive design of sgRNAs for broad applications of CRISPR in genome editing, gene repression and activation. The tool can be applied to other types of CRISPR applications such as genome imaging ( Chen  et al. , 2013 ) and CRISPR synthetic circuit design ( Kiani  et al. , 2014 ), and expanded to other organisms. We also provide the source code for the generation of whole-genome sgRNA libraries, useful for genome-wide screening based on CRISPR, CRISPRi or CRISPRa ( Gilbert  et al. , 2014 ,  Shalem  et al. , 2013 ;  Wang  et al. , 2013 ). 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Identification of topological features in renal tumor microenvironment associated with patient survival</Title>
    <Doi>10.1093/bioinformatics/btx723</Doi>
    <Authors>Cheng Jun, Mo Xiaokui, Wang Xusheng, Parwani Anil, Feng Qianjin, Huang Kun, Murphy Robert</Authors>
    <Abstract/>
    <Body>1 Introduction Cancers are highly heterogeneous diseases. Even for a specific type of cancer, there are often different subtypes conferring different clinical outcomes including different prognoses (i.e. survival times) and responses to treatments. For instance, according to the International Union Against Cancer and the American Joint Committee on Cancer, renal cell carcinoma (RCC, also called kidney cancer) histologic subtypes are categorized as clear cell, papillary, chromophobe, collecting duct and unclassified RCC types based on the Heidelberg classification system ( Kovacs et al. , 1997 ), and some types of RCC also have its own subtypes. Previous studies on many cancers have suggested that stratification by histologic subtype could provide prognostic value ( Patard et al. , 2005 ). Stratifying cancer patients into different subtypes with more accurate prediction of the clinical outcome will greatly enhance precision medicine practices. For example, patients with worse prognosis may benefit from closer follow-up, more aggressive treatment, and advance care planning. Currently, despite fast advances in genome medicine, patient stratification is still often carried out by pathologists by reviewing histopathology slides under a light microscopy. However, such review is often subjective, and both intra- and inter-operator variations are large, frequently leading to discrepancies in diagnosis and subtyping results and sometimes even misdiagnosis. Thus there is an urgent need for developing effective computer vision based algorithms and workflows to extract quantitative features that can effectively stratify patients with distinct prognosis. 1.1 Scope of study In this study, we focus on the second most common subtype of kidney cancer called papillary renal cell carcinoma (pRCC), accounting for 11% to 15% of all cases ( Hansel, 2010 ). pRCC is a less well understood cancer, and currently there are no effective morphological markers for pRCC that pathologists can use to effectively predict prognosis. We aim to improve the prognostic prediction of pRCC through objective features derived from histopathology images. Despite the specific application, our workflow is a general one focusing on an important aspect of the development of all solid tumors— the interaction between tumor and its surrounding stroma . With the development of cancer biology, scientists and clinicians have gained deep insights into the importance of the tumor microenvironment. As a highly heterogeneous organ-like structure, the development of tumor is not only achieved by unlimited growth of the tumor cells, but also supported, stimulated, and nurtured by the microenvironment around it ( Pietras and Östman, 2010 ; Trimboli et al. , 2009 ). The tumor microenvironment includes fibroblasts with large amount of extracellular matrices, blood vessels with endothelial cells, and various immune cells such as macrophages, T-cells, and B-cells, which form the stroma. In fact, the amount of stroma, its organization, gene expression profiles, and interaction with the tumor cells are sometimes more effective predictors for clinical outcomes than the tumor cells alone. For instance, Yuan et al.  (2012)  found that spatial distribution of stromal cells is critical for predicting patient survival in ER-breast cancers while Beck et al.  (2011)  found that most of the quantitative histopathological features predicting general breast cancer survival are stromal features. 1.2 Related work Commonly used clinical and pathological factors include patient age, gender, tumor multifocality, stage, grade, cancer specific subtyping and so on. Patard et al.  (2005)  conducted a retrospective study in a large cohort which contains 4063 patients from eight international centers. They concluded that the stratification in three main renal cell carcinoma histologic subtypes (clear cell, papillary, and chromophobe carcinoma) should not be considered as a major prognostic variable comparable to TNM stage, Furman grade, and eastern cooperative oncology group performance status. pRCC is usually classified into two subtypes based on specific histologic features, consisting of type 1 (or basophilic) and type 2 (or eosinophilic) ( Sukov et al. , 2012 ). Although some pRCCs can be easily split into two subtypes, pRCC is usually heterogeneous and can show both type 1 and type 2 features. For this reason, the prognostic value of pRCC type varies between studies. For example, several studies show type 2 pRCC has a significantly worse prognosis than type 1( Moch et al. , 2000 ; Pignot et al. , 2007 ). However, the prognostic utility of subtyping pRCC remains controversial. Ku et al.  (2009)  compared pRCC type 1 and type 2 in a small group of 70 patients, and did not observe a statistically significant difference in survival times. G ontero et al.  (2008)  confirmed that finding. With the recent availability of digital whole-slide images, we can perform systematic analysis of the diverse structures present in histopathology images and may find previously unrecognized image features that correlate with patient prognosis, while reducing the inconsistence arising from subjective interpretation. Quantitative analysis of biomedical images has been an area gaining increasing interest. Plenty of methods have been proposed for tasks like object detection and segmentation (nuclei, mitosis, and glands) ( Albarqouni et al. , 2016 ; Xu et al. , 2014 ), benign/malignant image classification ( Zhang et al. , 2015a ), and discovery of prognostic factors ( Beck et al. , 2011 ; Chen et al. , 2015 ; Veta et al. , 2012 ; Yuan et al. , 2012 ). For prognostic factor discovery, most studies focus on tumor morphology; however, we provide an effective workflow taking into account not only cell morphology but also the spatial arrangement of different cell patterns, with demonstrated advantages over existing subtyping or cell-based features in pRCC. 1.3 Technical challenges and overview of our contribution One big challenge for computerized histopathological image analysis is that each type of cancer, given the specific anatomical origin, will often have specific structures, calling for specific features. This is also reflected in the fact that pathologists are highly diversified in their expertise usually with focus on one or a couple of types of cancers instead of all cancers. From the computing point of view, it means that researchers need to strike a balance between general workflows and specific quantitative features tailored for specific cancers. In this paper, we aim at developing a bioimage informatics workflow to quantitatively characterize the interaction between different types of cells in the cancer tissue and examine if such interactions are associated with the outcomes of patients. The tumor microenvironment is a complex milieu, which includes not only the tumor cells but also the stromal cells, immune cells, and even normal, healthy cells. Biopsy tissue staining by such as hematoxylin and eosin (H&amp;E), can provide a spatial layout of heterogeneous tumor cellularity. Our contributions include: (i) an integrative pipeline to automatically learn different cell types and (ii) novel topological features characterizing the topological organization of different cell types including both tumor and stroma. This pipeline is universal for other solid tumors, and our application on pRCC will have a high impact on clinical practice. 2 Materials and methods 2.1 Dataset The dataset used in the experiments was downloaded (at May 4, 2016) from The Cancer Genome Atlas (TCGA) portal (project TCGA-KIRP). It consists of 190 patients with H&amp;E stained whole slide images (WSI) as well as corresponding clinical information (patients with survival times &lt;1 month have been excluded from the study). The typical resolution of a WSI is about 100 K by 90 K pixels. Due to the limited computational resource and the artifacts present in the image, in each WSI, 2–8 regions of interest (ROI) of size 3 K by 3 K are extracted, resulting in 856 ROIs in total. Regions having too much blood or artifacts are avoided. The patient demographics and tumor characteristics of our dataset are summarized in Table 1 . Note that the stage and subtype information of some patients is not available. In addition, since the prognosis prediction is more important for relatively early-stage patients, we focus our study on the patients with stages I, II and III and exclude the patients with stage IV (the latest stage).
 Table 1. Patient demographics and tumor characteristics Characteristics Summary Patient no. 190 Age (year)  Median 60.5  Range 28–85 Gender  Female 51  Male 139 Follow-up (month)  Median 16.3  Range 1–185.3 Number of Death 27 Subtype  Type 1 46  Type 2 60  Not available 84 TNM stage  I 110  II 10  III 39  IV 12  Not available 19 2.2 Overview of the workflow 
 Figure 1  shows the overview of our workflow to extract topological features, which consists of two modules. The first module ( Fig. 1A ) shows the process for learning nucleus patterns from the imaging dataset. The second one ( Fig. 1B ) demonstrates how to generate topological features (bag of edge histogram features) for an image using the learned nucleus patterns and Delaunay triangulation. In the following subsections we provide details of our workflow.
 Fig. 1. Overview of our workflow. ( A ) Learning nucleus patterns in an unsupervised manner. ( B ) Generating bag of edge histogram features and identifying survival-related edge patterns (Color version of this figure is available at Bioinformatics  online.) 2.3 Nucleus segmentation and patch extraction For nucleus segmentation, we employ a recently proposed approach by Phoulady et al.  ( Phoulady et al. , 2016 ), which is an unsupervised segmentation method for cell nuclei in histopathology images. More specifically, following an initial preprocessing step involving color deconvolution and image reconstruction, the segmentation step consists of multilevel thresholding and a series of morphological operations. Unlike the methods in ( Al-Kofahi et al. , 2010 ; Wienert et al. , 2012 ), which require many parameters, this method requires no parameter learning or training data because the parameters are set adaptively, making the approach insensitive to variations in staining intensity and appealing to our applications. After nucleus segmentation, we need to extract nucleus patches centered at the centroids of nuclei to train an autoencoder . In order to choose an appropriate patch size that can frame most nuclei and is not too large, for each nucleus we compute the size of the smallest square bounding box that exactly covers it. The patch size in our experiments is set to the 90th percentile of all the sizes, which is 41 by 41 pixels. 2.4 Nucleus subtyping using stacked sparse autoencoder Since there are a large number of cell types in the tumor microenvironment each calling for a different set of features for its recognition, it is difficult to design a repertoire of algorithms for this classification. Instead we take an unsupervised approach to categorize the cells based on their morphology without explicitly labeling their histological types using a stacked sparse autoencoder (SSAE) ( Fig. 1A ). A SSAE is a neural network that consists of multiple layers of sparse autoencoder (SAE) in which the outputs of each layer are wired to the inputs of each successive layer. In a recent work on cell nucleus detection ( Xu et al. , 2016 ), SSAE is utilized to learn high-level features from just pixel intensities of small patches. A slide window is applied to each image in order to represent image patches via high-level features obtained from SSAE. The features are subsequently fed into a classifier to determine whether an image patch contains nucleus. However in this paper, instead of putting the learned features into a classifier, we put them into the K-means clustering algorithm to learn K distinct nucleus patterns. Similar to Xu’s work ( Xu et al. , 2014 ), we also employ two SAE layers to form the SSAE whose architecture is shown in Figure 1A . The SSAE is trained by a greedy layer-wise approach, i.e. training each layer in turn. First, we train a SAE on the training nucleus patches x k . Next, we feed all the training nucleus patches into the first trained SAE, obtaining the primary features h k 1  for each x k . We then use h k 1  as input to another SAE to learn secondary features h k 2 . After that, we perform K-means algorithm on h k 2  to generate K  nucleus patterns. We randomly choose 50 000 nucleus patches to train the SSAE. The input to the first SAE layer is the vector of pixel intensities corresponding to the nucleus patch, which is 41 × 41 × 3 dimensional. The number of units of the first and second SAE is set to 400 and 200, respectively. 2.5 Topological features based on Delaunay triangulation For each image, we obtain the label of every nucleus therein by the following steps: nucleus segmentation, nucleus patch extraction, feeding them into the learned SSAE to generate high-level features, and quantizing these features to one of the K  clusters. Given an image I  consisting of its nuclei O ( I ) = { o i } , we construct a graph G = { V , E }  on the nuclei's centroids using Delaunay triangulation, where vertex set V  includes every nucleus o i ∈ O ( I ) , and edge set E  contains triangle edges e d g e ( o m , o n ) . Edges are labeled with regard to their end nodes, regardless of their order. Then, we characterize the image by the histogram of edge types, which we call bag of edge histogram  (BOEH for short) ( Fig. 1B ). If there are K  nucleus patterns, the dimensionality of BOEH would be K × ( K − 1 ) / 2 + K . The first term is the number of combinations when two nuclei come from different types, and the second term is the number of combinations when two nuclei are of the same type. Note that each patient has multiple images and the BOEH representations of these images for each patient are summed and then L1-nomalized. The BOEH representations encode the neighborhood information between adjacent nuclei. Figure 1B  shows a schematic diagram of constructing a Delaunay graph on eight nuclei. For the sake of simplicity, there are only three types of nuclei and therefore six edge types. Triangle edge types are indicated with different colors. 2.6 Morphological and intensity features In addition to the proposed BOEH representations, we also implement some morphological and intensity features ( Yang et al. , 2011 ) for comparison. Ten cell-level features are computed for each segmented nucleus: nuclear area, lengths of major and minor axes of cell nucleus and their ratio, mean pixel values of nucleus in RGB three channels, respectively, as well as mean, maximum and minimum neighbor distances of cell nuclei in Delaunay triangulation graph. Afterwards, for each type of cell-level features extracted from a patient, five statistical distribution parameters, including mean value, standard deviation, skewness, kurtosis and entropy, are calculated to characterize the distribution of cell-level features. Consequently, a total of 50 patient-level features are extracted to describe each patient. Skewness is a measure of the asymmetry of the data around the sample mean. Kurtosis is a measure of how outlier-prone a distribution is. And entropy is a statistical measure of randomness. 2.7 Machine-learning methods for prognosis prediction A lasso-regularized Cox regression model (lasso-Cox model) is built on image features to predict the risk indices of patients and divide them into a low-risk or high-risk group (R package ‘glmnet’). To validate our method, Leave-one-out cross validation (CV) strategy is used due to relatively small sample size and low death rate ( Table 1 ). More specifically, in each round of the leave-one-out CV process, a single patient is left out as test set with the rest as training set. In the training set, to reduce high dimensionality of BOEH features, we first perform univariate Cox regression to select features strongly related to survival by dichotomizing patients using median feature value and calculating the hazard ratio between the two groups. Features with hazard ratios &gt;4 or &lt;0.25 are selected. Next, principle component analysis (PCA) is used to reduce noise and decorrelate these features, and Cox regression model is trained using the top two principle components. To predict the risk index of the held-out patient, we select survival-related features based on the feature selection results in the training set, apply the PCA projection matrix learned from training set to the test data, and put the reduced features to the learned Cox regression model. Note that only the training set is used for feature selection and PCA while the test sample is excluded from training. After n rounds, each patient is assigned a predicted risk index. Finally, patients are divided into two groups (low-risk versus high risk) using the median risk index as cut-off point, and log-rank test is used to test if there are distinct survival outcomes between the two groups. In addition, we conduct receiver-operator characteristics (ROC) curve analysis for binary outcome of 5-year survival to determine the prognosis prediction capability for tumor stage, tumor subtype and the predicted risk index of Cox regression model by using nearest neighbor estimation method ( Heagerty and Zheng, 2005 ; Heagerty et al. , 2000 ). 3 Results 3.1 Nucleus segmentation, subtyping and BOEH representations For evaluating nucleus segmentation algorithm, we manually count the true number of nuclei, the number of nuclei identified by the segmentation algorithm, and the number of false positives in eight image patches of size 1500 × 1500 pixels; the three numbers are 4082, 4159 and 168, respectively. The recall is (4159 − 168)/4082 = 97.77%, and precision is (4159 − 168)/4159 = 95.96%. Thus this nucleus segmentation algorithm performs well enough for the subsequent steps in our pipeline. Examples of segmentation results are shown in Supplementary Figure S1 . Using a ROI as an example, Figure 2  shows the results of several steps involved in our computer-aided image analysis pipeline, including nucleus segmentation, nucleus subtyping by K-means clustering algorithm on the high-level features learned by SSAE, Delaunay triangulation on nucleus centroids, and construction of bag of edge histogram features. In this figure, we set the number of clusters to 8 in K-means algorithm, so we will have 8 distinct nucleus patterns and therefore 8 × (8 − 1)/2 + 8 = 36 edge types. We can see clearly that the proposed feature representations can capture the interaction between nuclei and their neighbors, which may provide useful information about patient prognosis.
 Fig. 2. Illustration of the three main steps involved in our feature extraction workflow. ( A ) Nucleus segmentation. ( B ) Nucleus pattern learning using stacked sparse autoencoder to learn high-level features followed by clustering. Nucleus patterns are indicated by different colors. There are eight nucleus patterns. ( C ) Delaunay triangle edge patterns showed in different colors. Edge patterns are defined in terms of their end nodes. There are 36 edge patterns since we have eight nucleus patterns. The H&amp;E image is converted to a grayscale image to highlight colors (Color version of this figure is available at Bioinformatics  online.) 3.2 Prognostic values of BOEH representations Since prognosis prediction for patients with terminal cancer is not crucial, instead we focus on survival analysis for relatively early to middle-stage (I, II, and III) patients, and patients with stage IV are excluded. By setting a large number of nucleus patterns, we can characterize nuclei more distinctively and discover the potential topological arrangements of nuclei that are related to patient prognosis. Several values of the number of nucleus patterns are tried, 8, 16, 32 and 64, which corresponds to 36, 136, 528 and 2080 dimensional BOEH representations (see Section 2.5). To conduct univariate survival analysis, patients are divided into two groups using the median of each feature. Results from log-rank test on each feature reveal that 1 out of 36, 1 out of 136, 16 out of 528 and 51 out of 2080 are significantly associated with patient survival. Besides, we also conduct univariate survival analysis on the 50 morphological features and two pathological variables, i.e. TNM stage and pRCC subtypes. We merge stage II and III into one group and compare it with stage I. Among the 50 morphological features, two features are identified to be related to survival. Table 2  shows the P  values of log-rank test for the pathological and image features. Due to limited space, only the two significant morphological features are listed, and only the top five significant BOEH features with the smallest P  values are listed. For extracting the BOEH features in Table 2 , the number of nucleus patterns is set to 64.
 Table 2. Univariate survival analysis results using log-rank test Feature P  value TNM stage (I versus II, III) 0.073 Subtype (type 1 versus type 2) 0.009 Skewness of length of major axis 0.044 Kurtosis of length of minor axis 0.034 Edge(14, 58) 0.005 Edge(58, 62) 0.007 Edge(16, 56) 0.008 Edge(21, 58) 0.009 Edge(15, 23) 0.010 Note : For morphological and intensity features only the significant features are listed, and for the proposed BOEH features only the top five features with smallest P  values are listed. The number of nucleus patterns is set to 64. Edge (14, 58) means the edge type with the 14th and 58th nucleus patterns as its end nodes, and the other pairs are listed in the same fashion. 
 Figure 3A–C  shows the Kaplan–Meier survival curves stratified by stage, subtype, and lasso-Cox on BOEH features, respectively. Early stage trends with better survival (log-rank test P  = 0.0726, Fig. 3A ). Patients with pRCC type 2 have worse prognosis than those with pRCC type 1 (log-rank test P  = 0.00946, Fig. 3B ), which is consistent with the conclusion by Pignot ( Pignot et al. , 2007 ). Patient stratification by the predicted risk inedx provides the best prognosis prediction (log-rank test P  = 1.46e-4, Fig. 3C;  see Section 2.7 for details of model training and classification). In addition, area under the curve (AUC) of ROC that predicts the binary outcome of 5-year survival for the stage and subtype is 0.63 and 0.66, respectively. The predicted risk index achieves an AUC of 0.78 ( Fig. 3D;  see Section 2.7 for the methods used to plot ROC curves). Therefore, the proposed BOEH features have a better predictive capability than stage or subtype.
 Fig. 3. The proposed BOEH features provide better prognosis prediction than clinical variables. ( A – C ) Kaplan–Meier curves stratified by tumor stage, tumor subtype, and predicted risk index of lasso-Cox model built on BOEH features, respectively. ( D ) ROC curves that predict the binary outcome of 5-year survival using predicted risk index of lasso-Cox model built on BOEH features, tumor stage, and tumor subtype, respectively. For extracting BOEH features, the number of nucleus patterns is set to 64 (Color version of this figure is available at Bioinformatics  online.) 
 Figure 4  shows some nucleus patch patterns that form the edge types that are strongly associated with survival, identified by univariate survival analysis ( Table 2 ). Although these nucleus clusters contain mostly tumor cells with diverse texture, there exist different levels of stromal deposition around the nucleus clusters (e.g. 1, 58, and 56). The clusters 16 and 14 also show strong interactions between tumor cells and lymphocytes (as shown in Supplementary Fig. S2 ). Therefore these four pairs of clusters with most significant difference between patient survival times are involved in tumor heterogeneity, implying the importance of stroma-tumor-lymphocyte interaction which is consistent with previous findings ( Beck et al. , 2011 ; Loi et al. , 2013 ; Yuan et al. , 2012 ) that stromal tissue and tumor-infiltrating lymphocytes also play an important role in predicting survival times.
 Fig. 4. Examples of the learned nucleus patterns forming edge types that are strongly associated with survival. The number of nucleus clusters is set to 64. The number in the upper-left corner of each image is the cluster index. Each image consists of 10 × 10 nucleus patches from the same cluster (Color version of this figure is available at Bioinformatics  online.) 4 Discussion Nuclei in histopathology images play a very important role in cancer diagnosis and prognosis prediction. Many studies have been focusing on nuclear morphological features. However, in this study we present a workflow to automatically extract image features which considers both nucleus morphology and topology of the distribution of different types of nuclei. Based on these features, we find that the co-occurrences of some nucleus patterns are potential biomarkers for pRCC with stronger prognostic power than clinical staging or existing subtyping in the TCGA-KIRP dataset. The interaction among different tumor cells, stromal cells and lymphocytes is known to play a major role in cancer growth and progression. Our method provides a way of quantitatively characterizing this interaction, and our results suggest that some connecting edges between tumor cells and stromal cells are related to patient survival. Many previous studies for discovering prognostic image features require laborious labeling by skilled pathologist, such as labeling various cell types ( Yuan et al. , 2012 ) and classifying tissues into epithelium or stoma ( Beck et al. , 2011 ; Wang et al. , 2013 ). In contrast, our approach is fully automated, which can learn potential nucleus patterns via an unsupervised feature learning algorithm (i.e. stacked sparse autoencoder) followed by clustering. Although the learnt nucleus patterns may not explicitly correspond to known cell types such as cancer cells, stromal cells or lymphocytes, due to this flexibility it enables us to thoroughly investigate nucleus morphology and their spatial arrangement which are proved by our experiments to be related to clinical outcomes. Our study has several limitations. First, although cross validation is used to validate our method, the proposed topological features need to be tested on other validation cohorts to fully validate its generalizability. This is an ongoing effort as we are collecting pRCC samples from multiple sites. Secondly, a common practice in survival analysis is to first conduct univariate survival analysis to identify significant variables and then to carry out multivariate survival analysis on these variables using Cox regression to determine independently significant variables. However, in order for results of the multivariate Cox regression to be reliable, there must be at least 10 events (deaths) for each investigated variable ( Peduzzi et al. , 1995 ). Due to the relatively few events in our dataset (16 in 159), we do not do testing for independence of features. Lastly, since the nucleus patterns are learnt directly from the images by the SSAE, they need to be further interpreted by pathologists. Future work of research includes automated detection of artifacts in histopathological images and application of our method to other cancers and tasks. Currently regions of interest are manually selected in our study, which is infeasible for very large dataset. Automated detection of artifacts can make our pipeline fully automated and therefore applicable to large clinical trials. The proposed features are the histogram of co-occurrence of nucleus patterns, so in order for the features to be robust it is important that the features are summarized over a much larger area of the tumor. Although our study focuses on predicting prognosis in patients with pRCC, our flexible workflow is not specific to this application and can be applied to other cancer types or even diseases other than cancers. In addition, we plan to investigate how the proposed bag of edge histogram representations perform in other applications in digital pathology and bioimage informatics such as the problem of histopathology image classification and retrieval ( Zhang et al. , 2015a , 2015b ). Furthermore, these features not only provide new insights into the topological organization of cancers, but also can be integrated with genomic data in future studies to develop new integrative biomarkers and to generate new insights regarding the genomic basis for tissue morphology and organization. Supplementary Material Supplementary Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>HiBrowse: multi-purpose statistical analysis of genome-wide chromatin 3D organization</Title>
    <Doi>10.1093/bioinformatics/btu082</Doi>
    <Authors>Paulsen Jonas, Sandve Geir Kjetil, Gundersen Sveinung, Lien Tonje G., Trengereid Kai, Hovig Eivind</Authors>
    <Abstract>Summary: Recently developed methods that couple next-generation sequencing with chromosome conformation capture-based techniques, such as Hi-C and ChIA-PET, allow for characterization of genome-wide chromatin 3D structure. Understanding the organization of chromatin in three dimensions is a crucial next step in the unraveling of global gene regulation, and methods for analyzing such data are needed. We have developed HiBrowse, a user-friendly web-tool consisting of a range of hypothesis-based and descriptive statistics, using realistic assumptions in null-models.</Abstract>
    <Body>1 INTRODUCTION Methods for detection of genome-wide chromatin 3D conformation, such as Hi-C ( Lieberman-Aiden  et al. , 2009 ) and ChIA-PET ( Fullwood  et al. , 2009 ), are drastically expanding our understanding of genome biology. However, statistical and computational methods to analyze chromatin conformation capture-based data are needed. Many of the available methods focus on data visualization, or are not suited for genome-wide statistical investigations ( Baù  et al. , 2010 ;  Servant  et al. , 2012 ;  Thongjuea  et al. , 2013 ;  Zhou  et al. , 2013 ). The structure of chromatin makes statistical analysis complicated, due to correlations between the interaction frequencies caused by both sequence-dependent and topological constraints ( Paulsen  et al. , 2013 ). A few statistical tests have been proposed, with varying possibilities to account for structural dependencies ( Botta  et al. , 2010 ;  Kruse  et al. , 2013 ;  Paulsen  et al. , 2013 ;  Wang  et al. , 2013 ;  Witten and Noble, 2012 ). Two useful command-line tools are the hiclib-package ( Imakaev  et al. , 2012 ), and the HOMER software suit ( Heinz  et al. , 2010 ), which both allow for noise-removal, outlier detection and compartment identification. The HOMER software additionally allows for identification of significant interactions in a given dataset, assuming a binomial distribution and a background model taking into account sequence-based and compartmental biases. The global nature of these data allow for other types of statistical investigations beyond detecting significance of individual interactions. A common type of analysis is to analyze a set of genomic elements (genes, regulatory elements, transcription factors, etc.), and ask how this subset, or ‘query track’, is spatially arranged in 3D space as represented by a Hi-C dataset, for example. Here we present HiBrowse, a web-based analysis server for performing statistical analysis of 3D genomes in a range of different settings. The available statistics provide a flexible and expandable catalog of tools based on state-of-the-art statistical methods utilizing Monte Carlo (MC) and analytic methods as suited, in addition to a range of tools for visualization and hypothesis-generating investigations. 2 FEATURES AND METHODS 2.1 Data representation and analysis framework We build on general software components of the Genomic HyperBrowser ( Sandve  et al. , 2010 ,  2013 ), a web-based analysis server for genome-scale data. The graphical user interface (GUI) is based on Galaxy ( Goecks  et al. , 2010 ), a user-friendly point-and-click environment familiar to many researchers. All tracks are based on a representation of elements as mathematical objects, consisting of points, segments, functions and variants of these [see  Gundersen  et al.  (2011)  for an in-depth discussion]. Any given analysis can be performed on all chromosomes, specific chromosomes or selected sub-parts of chromosomes, depending on the needs. In practice, an analysis is initiated by selecting one or more tracks either from the HyperBrowser repository, or from the user history. At least one of the selected tracks must be a Hi-C (3D) track, and the accompanying selected tracks (called ‘query tracks’) determine the types of statistical analyses that are possible, and therefore selectable in the system. A range of publicly available 3D-datasets have been installed in the repository. Since it has been shown that Hi-C and similar data can contain systematic biases, all the available Hi-C datasets have been corrected for such biases using the method of  Imakaev  et al.  (2012) . Furthermore, a specialized tool has been developed to allow users to upload their own Hi-C data (or similar) into the history, even if the dataset itself does not conform to well-known formats. See  Supplementary Table S1  for a list of already installed and pre-processed Hi-C datasets. 2.2 Overview of statistical methods Statistical tools are divided into two broad categories: hypothesis tests and descriptive statistics. Hypothesis tests are both MC based and analytical. Due to the complex structure of chromatin conformation capture data, finding suited explicit null distributions is generally not possible ( Paulsen  et al. , 2013 ;  Witten and Noble, 2012 ), and even randomization of the data through MC is difficult. Therefore, we consistently perform permutations on the query track only. The hypothesis tests can be divided into three types, defined by the query track type, as illustrated in  Figure 1 A. For example, Points (P) are used to analyze general (all-versus-all) 3D co-localization by specifying a set of genomic elements using the BED format, while Linked Points (LP) are used to analyze 3D co-localization between selected pairs of elements by providing additional information about which genomic elements that should be linked together.
 Fig. 1. ( A ) Overview of statistical hypothesis tests implemented in HiBrowse. See  Gundersen  et al.  (2011)  for an in-depth explanation of track types, and the  Supplementary Material  for details about each statistic. ( B ) Example of a HiBrowse analysis using the ‘Linked elements more/less co-localized in 3D?’ statistic, investigating whether fusion transcripts are co-localized in 3D. ( C ) Result page from the analysis, presenting the question asked by the user together with both a simplistic and a more detailed answer giving the  P -value and model assumption details. Links are provided to full details of the results at individual chromosome regions In the most basic case, if the user selects a set of points (genomic elements) in BED-format in addition to a Hi-C data track, one may ask whether all the genomic elements in the BED-file are more/less co-localized in 3D, in an all-versus-all fashion, than what would be expected by chance. In this case, the mean of the observed standardized interaction frequencies is compared to the expected value estimated from the permuted positions in representative regions of the rest of the Hi-C (3D) track. This analysis was introduced in  Paulsen  et al.  (2013) , and in this article we expand the methodologies by allowing a much wider variety of query tracks. For example, by specifying two point-tracks (two BED files), in addition to a Hi-C (or similar) track, the user can ask whether the points in track 1 are more/less co-localized with track 2, than expected by chance. In this type of statistical question, the permutations can be performed on both of the point-tracks, or by preserving one of the point-tracks completely. It is also possible to specify particular interactions between a set of genomic elements, and compare these interactions with randomly permuted interactions within the same set of elements. In HiBrowse, interactions between genomic elements are defined using LP, a format described in detail elsewhere ( Gundersen  et al. , 2011 ). Such linked track types can easily be created by using a dedicated tool that converts from a simple BED file format containing information about which elements that should be linked together (see  Supplementary Fig. S1 , for an example). Since this type of analysis only permutes interactions intrinsically with regards to the query track, the positions of all elements will be completely preserved. This type of analysis should be used whenever specific interactions between genomic elements are considered, and it would be natural to compare with random links between the same elements. Since regions of the genome can have varying properties (active/inactive genes, open/closed chromatin, etc.), global shuffling of links between all selected elements is not always preferable. To take such properties into account during the permutation, each of the points can be marked by a value, such that the link-permutations will be performed by preserving the value-combinations on both sides of the links. If the user wants full control over exactly what pairs of interactions that are allowed to take part in the link-permutations, it is possible to specify a case/control value on each of the links via a dedicated tool which accepts two BED files (‘case’ and ‘control’) of the same format as described above (see  Supplementary Fig. S2 , for an example). The case/control-linked elements can then be selected together with a Hi-C (3D) track, allowing the user to compare the interaction frequency of all the links marked as ‘case’ with the expected interaction frequency given by permuting the case/control labels. This type of statistic is optimal for data that is only sampled from a pre-defined set of elements of the genome, and where the user wants to find out whether a subset of these elements are co-localized in 3D. Finally, it is possible to find statistically significant differences between two Hi-C datasets, for example comparing treatments [as e.g. in  Rickman  et al.  (2012) ]. The statistical test implemented for this type of analysis is based on the edgeR-tool ( Robinson  et al. , 2010 ). Details about the mathematical formulation of the different types of statistics and their corresponding null-hypotheses are found in the  Supplementary Material . In addition to hypothesis tests, a range of descriptive statistics have been implemented. For example, each hypothesis test is accompanied by an enrichment score, giving the degree of over/under-representation of 3D co-localization, compared to the expected 3D co-localization (see  Supplementary Material  for details). Other types of available descriptive statistics are visualization of clustered Hi-C matrices as heatmaps or graphs, principal component analysis on Hi-C matrices and other summary statistics (see  Supplementary Table S2  for a comprehensive list). All available analyses are described thoroughly on the help pages linked from the main site, where example histories are provided such that users can explore each statistic in detail. Demo-buttons are provided for all tools, giving small example runs. See  Figure 1 B and C for an analysis example. Funding : This work was supported by the  Norwegian Cancer Society  [ PR-2006-0433 ]. Conflict of Interest : none declared. 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>TopoGSA: network topological gene set analysis</Title>
    <Doi>10.1093/bioinformatics/btq131</Doi>
    <Authors>Glaab Enrico, Baudot Anaïs, Krasnogor Natalio, Valencia Alfonso</Authors>
    <Abstract>Summary: TopoGSA (Topology-based Gene Set Analysis) is a web-application dedicated to the computation and visualization of network topological properties for gene and protein sets in molecular interaction networks. Different topological characteristics, such as the centrality of nodes in the network or their tendency to form clusters, can be computed and compared with those of known cellular pathways and processes.</Abstract>
    <Body>1 INTRODUCTION Functional genomic experiments provide researchers with a wealth of information delineating gene sets of biological interest. To interpret these lists of genes, common steps in a functional gene set analysis include the search for enrichment patterns [for a review, see (Abatangelo  et al. ,  2009 )], e.g. to identify significant signalling pathways or protein domains, and text mining in the literature [see review by (Krallinger  et al. ,  2008 )]. Another approach for the functional interpretation of gene sets is the analysis of molecular interactions in which the genes or their corresponding proteins are involved, in particular protein–protein interactions. In this context, some existing bioinformatics tools already allow users to map genes onto networks of interacting or functionally associated molecules to identify related genes and proteins (Jenssen  et al. ,  2001 ; Snel  et al. ,  2000 ). However, to the best of the authors' knowledge, so far these tools do not take into account topological properties in interaction networks to analyse and compare gene sets. In this article, we introduce TopoGSA (Topology-based Gene Set Analysis), a web-tool to visualize and compare network topological properties of gene or protein sets mapped onto interaction networks. 2 WORKFLOW AND METHODS 2.1 Analysis of network topological properties An analysis begins with the upload of a list of gene or protein identifiers (Ensembl IDs, HGNC symbols, etc.; see webpage for a complete list of supported formats). Alternatively, a microarray dataset can be used as input and differentially expressed genes will be extracted automatically using methods from a previously published online microarray analysis tool [arraymining.net (Glaab  et al. ,  2009 )]. Moreover, the user can add labels to the uploaded identifiers to compare different subsets of genes (e.g. ‘up- regulated’ versus ‘down-regulated’ genes). After submitting the list of identifiers, the application maps them onto an interaction network ( Section 4 ), and computes topological properties for the entire network, the uploaded gene/protein set and random sets of matched sizes. The available network topological properties are:
 The  degree  of a node (gene or protein) is the average number of edges (interactions) incident to this node. The  local clustering coefficient  quantifies the probability that the neighbours of a node are connected (Watts and Strogatz,  1998 ). The  shortest path length  (SPL) for two nodes  v i  and  v j  in an undirected, unweighted network is defined as the minimum number of edges that have to be traversed to reach  v j  from  v i . We use the SPL as a centrality measure, computing the average SPL from each node of interest to all other nodes in the network. The  node betweenness B ( v ) of a node  v  can be calculated from the number of shortest paths σ st  from nodes  s  to  t  going through  v :
 (1) The  eigenvector centrality  measures the importance of network nodes by applying a centrality definition, in which the score of each node reciprocally depends on the scores of its neighbours. More precisely, the centrality scores are given by the entries of the dominant eigenvector of the network adjacency matrix (see Bonacich  et al. ,  2001 , for a detailed discussion of this property). 
 Furthermore, user-defined 2D and 3D representations can be displayed for each individual gene/protein in the dataset and plotted data points are interlinked with relevant entries in an online annotation database. 2.2 Comparison with known gene sets The analysis of network topological properties of only a single gene/protein set does not lend itself to direct functional interpretation. However, TopoGSA enables the user to compare the properties of a dataset of interest with a multitude of predefined datasets corresponding to known functional processes from public databases. For the human species, these include signalling pathways [KEGG (Kanehisa  et al. ,  2006 ), BioCarta (Nishimura,  2001 )], Gene Ontology [Biological Process, Molecular Function and Cellular Component (Ashburner  et al. ,  2000 )] and InterPro protein domains (Apweiler  et al. ,  2001 ). Summaries of network topological properties are provided for all gene/protein sets, and in the 2D and 3D plots different colours distinguish different datasets. Users can identify pathways and processes similar to the uploaded dataset visually, based on the plots or based on a tabular ranking using a numerical score to quantify the similarity across all topological properties. The similarity score is obtained by computing five ranks for each pathway/process set according to the absolute differences between each of its five median topological properties and the corresponding value for the uploaded dataset. The sum of ranks across all topological properties is then computed and normalized to a range between 0 and 1. 3 EXAMPLE ANALYSIS It has been shown that proteins encoded by genes that are known to be mutated in cancer have a higher average node degree in interaction networks than other proteins (Jonsson  et al. ,  2006 ). This observation is confirmed by a TopoGSA analysis of the complete set of genes currently known to be mutated in cancer [Futreal  et al.  ( 2004 ),  http://www.sanger.ac.uk/genetics/CGP/Census ]. The cancer genes are involved in more than twice as many interactions, on average, than matched-size random subsets of network nodes (with a difference of &gt; 15 SDs for 10 random simulations). Furthermore, the analysis with TopoGSA reveals that the cancer genes are closer together in the network (in terms of their average pairwise shortest path distances) than random gene sets of matched sizes and occupy more central positions in the interaction network (see  Fig. 1 a for details). The 3D plot displaying node betweenness, degree and SPL reveals in particular the tumour suppressor p53's (TP53) outstanding network topological properties.
 Fig. 1. Example results produced with TopoGSA based on the cancer gene set by Futreal  et al.  ( 2004 ). (a) Topological properties can be computed and examined as visual (1) and tabular (2) outputs; (b) The gene set can be compared with a chosen reference database (here the KEGG database). When comparing the network topological properties of the cancer proteins with pathways from the KEGG database, considering each individual pathway as a gene set ( Fig. 1 b), the cancer proteins appear to have network properties comparable to several KEGG cellular processes and environmental information processing pathways [according to the KEGG-BRITE pathway hierarchy (Kanehisa  et al. ,  2006 ),  Fig. 1 b, purple and brown], whereas they clearly differ from metabolism-related pathways ( Fig. 1 b, yellow). Interestingly, although the network topological properties of cancer genes are in agreement with their role in promoting cell division and inhibiting cell death (Vogelstein and Kinzler,  2004 ), they differ from those of most disease-related KEGG pathways ( Fig. 1 b, green). 4 IMPLEMENTATION The network analysis and gene mapping was implemented in the programming language R and the web interface in PHP. To build a human protein interaction network, experimental data from five public databases [MIPS (Mewes  et al. ,  1999 ), DIP (Xenarios  et al. ,  2000 ), BIND (Bader  et al. ,  2001 ), HPRD (Peri  et al. ,  2004 ) and IntAct (Hermjakob  et al. ,  2004 )] were combined and filtered for binary interactions by removing entries with PSI-MI codes for detection methods that cannot verify direct binary interactions (these are evidence codes for co-immunoprecipitation or colocalization, for example; details on the used method definitions and PSI-MI codes can be found in the ‘Datasets’ section on the webpage). This filtering resulted in a network consisting of 9392 proteins and 38 857 interactions. Additionally, protein interaction networks for the model organisms yeast ( Saccharomyces cerevisiae ), fly ( Drosophila melanogaster ), worm ( Caenorhabditis elegans ) and plant ( Arabidopsis thaliana ) have been built using the same methodology as for the human network and the BioGRID database (Stark  et al. ,  2006 ) as additional data source (see the help sections on the webpage for additional details on these networks). TopoGSA will be updated periodically twice per year to integrate newly available protein interaction data and reference gene sets. Moreover, users can upload their own networks. A video tutorial and instructions on how to use the web tool are available in the ‘Tutorial’ section on the webpage. Funding : Marie-Curie EarlyStage-Training programme (MEST-CT-2004-007597); the Biotechnology and Biological Sciences Research Council (BB/F01855X/1); Spanish Ministry for Education and Science (BIO2007-66855); Juan de la Cierva post-doctoral fellowship (to A.B.). Conflict of Interest : none declared. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>BioMercator V3: an upgrade of genetic map compilation and quantitative trait loci meta-analysis algorithms</Title>
    <Doi>10.1093/bioinformatics/bts313</Doi>
    <Authors>Sosnowski Olivier, Charcosset Alain, Joets Johann</Authors>
    <Abstract>Summary: Compilation of genetic maps combined to quantitative trait loci (QTL) meta-analysis has proven to be a powerful approach contributing to the identification of candidate genes underlying quantitative traits. BioMercator was the first software offering a complete set of algorithms and visualization tool covering all steps required to perform QTL meta-analysis. Despite several limitations, the software is still widely used. We developed a new version proposing additional up to date methods and improving graphical representation and exploration of large datasets.</Abstract>
    <Body>1 INTRODUCTION Integration of multiple independent QTL mapping experiments is of major interest to unravel genetic factors underlying complex traits. As a first step, a consensus map is built from independent QTL maps and, if available, the organism reference map. QTLs are then projected onto the consensus map and subjected to meta-analysis ( Goffinet and Gerber, 2000 ). As a result, meta-QTLs and mapped genes lie on the same consensus map, greatly simplifying the inventory of candidate genes. In addition, the confidence interval (CI) of a meta-QTL is often shorter than CI of corresponding QTLs, decreasing the number of candidate genes to consider or the extent of fine mapping to conduct. In a survey of flowering-time traits in maize, 62 meta-QTLs were identified ( Chardon  et al. , 2004 ) including one corresponding to VGT1, which was subsequently cloned in the predicted region by fine mapping ( Salvi  et al. , 2007 ). As map compilation and QTL meta-analysis are not based on genotyping raw data or trait measure, they can be easily achieved from maps available from the literature or databases. The BioMercator software ( Arcade  et al. , 2004 ) offers a complete set of analysis and visualization tools dedicated to these approaches and is widely used ( Blackman  et al. , 2011 ;  Chang  et al. , 2011 ;  Quraishi  et al. , 2011 ) but suffers several limitations. We propose here a major upgrade of the software to overcome algorithm and graphical user interface limitations; map compilation can now be achieved in a single step whatever the number of maps, there is no limitation to the number of meta-QTL per chromosome and the code was rewritten to support high-density maps with no limitation of the number of loci. 2 COMPILATION OF GENETIC MAPS The previous versions of BioMercator proposed an iterative map compilation method, which required at least  n −1 steps to integrate  n  maps. The process could be fastidious for a large number of maps and in addition did not ensure optimality of consensus map. A one-step procedure to compile  n  maps based on weighted least squares and described in  Veyrieras  et al.  (2007 ) is now available in BioMercator. To assist user in examining the extent of shared loci between maps, we have implemented a graphical comparative map viewer, which highlights inverted loci. 3 QTL META-ANALYSIS The meta-analysis method implemented in BioMercator 2.1 suffered several limitations. Only models from 1 to 4 meta-QTLs per chromosome were considered, possibly forcing user to iterate meta-analysis process several times to fully cover a chromosome. In addition, classification of QTL into meta-QTL was discrete despite in some cases it would have been more realistic to take into account the uncertainty in assignation of a given QTL to two or more meta-QTL. Methods and code ( Veyrieras  et al. , 2007 ) addressing these issues have been included into BioMercator V3; among the new functionalities are full chromosome meta-analysis and probabilistic clusterization of QTLs. QTLs from related traits can be jointly subjected to a single meta-analysis by grouping corresponding traits into meta-traits. User may define meta-traits with any combination of traits looking relevant to him. As an alternative or complement to meta-analysis,  Chardon  et al.  (2004 ) have introduced the ‘QTL overview’ analysis, which summarizes QTL information by estimating the probability of identification of a QTL along the consensus map. BioMercator V3 automatically computes these probabilities, which are represented as a curve along the chromosomes. 4 DATA REPRESENTATION The datasets to be handled by BioMercator, if not carefully represented, can quickly overload the display as experienced in the previous version. New compact graphical representations have been developed; (i) a chromosome cascading zoom replaces the simple scale change of the previous version; this zoom allows the user to enlarge a region of chromosome as deep as he needs, while keeping an overview of the whole chromosome map. (ii) When compiling several maps into a reference one, QTLs may stack in some regions and occupy a really large space, impairing whole map representation ( Fig. 1 ). To address such cases, a new QTL track summarizing QTL data along the chromosomes has been developed. Basically, this track is a curve depicting QTL density that may be weighted or not by R2. (iii) For whole map representation, each chromosome can be enlarged independently at the user convenience in order to focus on regions of interest. By default, for a lighter display, only a randomly selected set of loci is represented at whole chromosome scale. By zooming in progressively, more loci are displayed. However, an option forcing the display of all loci is available. Datasets as well as analysis output may be exported in several formats (tabulation-delimited text file, xml). Either whole genome map or single chromosome can be exported in JPEG, PNG or the useful SVG vectorial format allowing image editing in dedicated third-party software.
 Fig. 1. Genetic maps graphical representation. ( A ) Dynamic maps comparison; shared loci are linked by blue line if collinear and red line otherwise. ( B ) Full representation of QTLs (left map), QTL density curves (one by trait) and QTL overview curve (all traits, right map). ( C ) Meta-analysis output; meta-QTL are drawn within chromosome and QTL-CI is colored according to the probability of its belonging to meta-QTLs; an enlarged region is displayed on the right along the full chromosome. The dataset used is maize maps collected by  Chardon  et al.  (2004 ). Traits are silking date (brown), days to pollen shed (black) and plant height (orange) 5 FUTURE DEVELOPMENTS Genetic maps and genome sequence are now integrated for many species. It would be a great enhancement for end-user if the structural and functional annotations of regions underlying QTL and meta-QTL could be reached directly from BioMercator. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>DExSI: a new tool for the rapid quantitation of 13C-labelled metabolites detected by GC-MS</Title>
    <Doi>10.1093/bioinformatics/bty025</Doi>
    <Authors>Dagley Michael J, McConville Malcolm J, Wren Jonathan</Authors>
    <Abstract/>
    <Body>1 Introduction Analysis of metabolic fluxes is important for understanding the metabolic and physiological state of microbes, plants and animals, as well as for understanding how complex metabolic networks are regulated and respond to different genetic, environmental and pharmacological perturbations. One of the most powerful approaches for measuring metabolic fluxes in intact cells involves the use of stable isotope (i.e.  13 C-,  15 N- or  2 H)-labelled tracers and the measurement of label into different metabolite pools using sensitive hyphenated mass spectrometry platforms, such as gas chromatography-mass spectrometry (GC-MS) ( Eylert  et al. , 2008 ;  Hartel  et al. , 2012 ;  Kloehn  et al. , 2015 ;  MacRae  et al. , 2012 ;  Obata  et al. , 2013 ). Quantitation of stable isotope enrichment at different time points or at isotopic steady state can be used to generate local or genome scale metabolite flux maps using metabolic flux analysis (MFA) or instationary MFA. GC-MS can be used to quantitate the steady-state levels of many intermediates in central carbon metabolism as well as stable isotope enrichment in these metabolites, and is amenable to high throughput analyses through automation of sample derivatization and analytical runs ( Buescher  et al. , 2015 ). However, there is currently no software that allows the automated detection of multiple metabolites and quantitation of corresponding mass isotopologues in GC-MS chromatograms with downstream correction for stable isotope abundance ( Supplementary Table S1 ;  Perez de Souza  et al. , 2017 ). In particular, the available vendor-supplied and open-source software pipelines require substantial manual processing during peak identification (as the mass spectra of unlabelled standards differ from those of heavily labelled metabolites) and/or quantitation of multiple isotopologues of the same metabolite. Corrections for the abundance of naturally occurring isotopes and calculation and visualization of isotopologue abundances are also typically done in separate packages ( Buescher  et al. , 2015 ). 2 Materials and methods DExSI ( D ata  Ex traction for  S table  I sotope-labelled metabolites) is a graphical and user-friendly software package for the automated identification annotation and quantitation of stable-isotope labelled metabolites detected by GC-MS. Data can be collected in either SIM or SCAN mode. In brief, DExSI utilizes a user-specified library comprising a list of ions (monoisotopic and related mass isotopologues) to identify a targeted list of metabolites. Using a scoring algorithm which accounts for retention time, the isotopic series of the metabolite, peak height and optionally, user-specified qualifying ions, the best matching peak for each metabolite is annotated independently of the extent of labelling ( Supplementary Table S3 ). Following automated peak annotation and integration, the graphical user interface ( Supplementary Fig. S1 ) allows for user curation and modification of integration boundaries, or the manual reassignment of the best peak, if required. Prior to exporting data, DExSI applies natural isotope abundance correction calculations to accurately determine the extent of labelling and the distribution of isotopologues using the approach developed by  Nanchen  et al.  (2007)  and  van Winden  et al.  (2002 ). This workflow is represented graphically in  Supplementary Figure S2  using data obtained from the unicellular eukaryotic pathogen,  Leishmania mexicana , cultivated in the presence of D-glucose- 13 C 6 . Here DExSI is used to identify 58 metabolites in GC-MS chromatograms. 3 Results Using DExSI, fractional labelling plots can be generated ( Supplementary Fig. S3A ) and absolute metabolite abundance can be determined through the calculation of a response factor for the known amount of a metabolite of interest in a reference sample, relative to the integrated area of an internal standard ( Supplementary Fig. S3B ). Data from the software can be exported in a range of ready-to-present formats: raw data tables or graphs of fractional labelling, isotopologue distribution or absolute abundance can be generated. For both absolute abundance and fractional labelling data ( Supplementary Fig. S2B ), heat maps can be prepared, and pre-formatted Excel tables can be generated for direct import into the VANTED metabolite pathway mapping and visualization software ( Supplementary Fig. S2C ) ( Junker  et al. , 2006 ). Data processing is performed using established methods for background correction and smoothing. First, GC-MS data is imported from vendor-neutral CDF-formatted files, with the entire data set subjected to both background correction, by applying top-hat background filtering, and noise reduction, using Savitzky–Golay smoothing, using the functions as implemented in the open-source SciPy toolkit ( Savitzky and Golay, 1964 ;  van der Walt  et al. , 2014 ). This approach to background correction has been previously applied in the PyMS and OpenMS software packages ( Lange  et al. , 2005 ;  O’Callaghan  et al. , 2012 ). Peaks are identified as local maxima in each extracted ion chromatogram, and metabolite searches are restricted to peaks which contain a minimum number of co-eluting ions ( Biller and Biemann, 1974 ). Following the above processing of the raw data, metabolite annotation is performed, as described earlier, and the best-matching metabolite peak is integrated. The background correction, peak smoothing and integration functions of DExSI were compared to Agilent MSD ChemStation using a timecourse series of D-glucose- 13 C 6  -labelled samples for 1035 peak areas ranging from 5 × 10 2  to 10 8  units, with least-squares linear regression yielding an  R 2  value of 0.9999817 ( Supplementary Fig. S4 ). Peak detection accuracy was compared between three biological data sets using a metabolite library of ∼60 compounds, yielding &gt;90% correct identifications ( Supplementary Table S2 ). 4 Conclusion DExSI provides the first integrated, graphical software package for the automated processing and visualization of GC-MS data from stable-isotope labelling experiments (features summarized in  Supplementary Table S1 ). It can be used to identify all isotopologues of multiple metabolites in complex biological mixtures irrespective of variation in the extent of labelling, greatly reducing processing time compared with current tools. It implements a range of calculations commonly used for flux analysis (natural isotope abundance correction and fractional labelling determination) and metabolite quantitation, as well as multiple graphing/heat map options and the ability to export data in a number of formats. DExSI can be used to create a highly automated workflow which will greatly improve the utility and throughput of stable isotope labelling experiments and the implementation of  13 C-MFA. Supplementary Material Supplementary Data Click here for additional data file. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Speeding up HMM algorithms for genetic linkage analysis via chain reductions of the state space</Title>
    <Doi>10.1093/bioinformatics/btp224</Doi>
    <Authors>Geiger Dan, Meek Christopher, Wexler Ydo</Authors>
    <Abstract>We develop an hidden Markov model (HMM)-based algorithm for computing exact parametric and non-parametric linkage scores in larger pedigrees than was possible before. The algorithm is applicable whenever there are chains of persons in the pedigree with no genetic measurements and with unknown affection status. The algorithm is based on shrinking the state space of the HMM considerably using such chains. In a two g-degree cousins pedigree the reduction drops the state space from being exponential in g to being linear in g. For a Finnish family in which two affected children suffer from a rare cold-inducing sweating syndrome, we were able to reduce the state space by more than five orders of magnitude from 250 to 232. In another pedigree of state-space size of 227, used for a study of pituitary adenoma, the state space reduced by a factor of 8.5 and consequently exact linkage scores can now be computed, rather than approximated.</Abstract>
    <Body>1 INTRODUCTION Genetic linkage analysis seeks to locate genomic regions that are likely to contain genes that increase the probability of traits such as heredity diseases. The input to such analysis are pedigrees of families that segregate a disease, marker information such as SNP readings, and affection status of some or all family members. The main idea is that markers which are found in the same vicinity on the chromosome are more likely to stay together during meiosis. Thus, based on the topology of the pedigree and the marker readings, it is possible to compute how likely it is for a predisposing gene to be located on the chromosome nearby each of the markers (Elston and Stewart,  1971 ; Lander and Green,  1987 ; Lange,  1997 ; Ott,  1999 ). There are several scoring methods commonly used for linkage analysis. They differ in how the scoring function depends on the probability of the possible inheritance patterns in the pedigree. Examples of such functions are  S all ,  S pairs  and log of odds (LOD) scores (Kruglyak  et al. ,  1996 ). As the number of such inheritance patterns grows exponentially in the number of markers and roughly in the number of persons in the pedigree, computationally sophisticated methods were proposed for this task. A common structure shared by most exact scoring methods is a hidden Markov model (HMM) (Rabiner and Juang,  1986 ) backbone, which is in fact a factored HMM with a state space defined by a set of variables called selectors that determine the inheritance pattern in the pedigree (Abecasis  et al. ,  2002 ; Gudbjartsson  et al. ,  2000 ,  2005 ; Ingolfsdottir and Gudbjartsson,  2005 ; Kruglyak and Lander,  1998 ; Kruglyak  et al. ,  1995 ,  1996 ; Lander and Green,  1987 ; Markianos  et al. ,  2001 ). Other techniques based, sometimes implicitly, on Bayesian networks (Lauritzen,  1996 ; Pearl,  1988 ) focus on larger pedigrees with fewer measurements (Cottingham  et al. ,  1993 ; Elston and Stewart,  1971 ; Fishelson and Geiger,  2002 ; O'Connell and Weeks,  1995 ; Silberstein  et al. ,  2006 ; Sobel and Lange,  1996 ; Thompson,  1994 ). Whenever the pedigree is too complex, and the number of selectors needed to determine the inheritance in the pedigree is too large, current methods can not utilize the HMM backbone, and exact computation of the linkage scores is not computationally feasible. In this article, we describe a method to reduce the state space for HMMs via a partition of the state space into equivalence classes, which consequently reduces the amount of computations needed in these models without sacrificing the exact solution. We specify two conditions a partition needs to satisfy in order to allow for such a reduction, and prove that whenever they hold, computations in the original and the reduced models yield the same results. A partition which is of particular interest for the HMM used for linkage analysis models is based on reducing chains of individuals in pedigrees where genetic information cannot distinguish among individuals in the chain. In such chains, it is possible to cluster together selector variables that control the inheritance from a founder allele in the pedigree, indicating only if the allele is transmitted to the individual at the end of the chain or, if not, what is the number of selectors that block its inheritance. For  r  selectors (meiosis), this clustering reduces the state space from 2 r  to  r  + 1 states. To demonstrate the usefulness of the state-space reduction, we provide several examples of pedigrees in which computations are significantly easier once the reduction is used. For a Finnish family in which two affected children suffer from a rare cold-inducing sweating syndrome (Knappskog  et al. ,  2003 ), we were able to reduce the state space of the internal inbreeding loop by over 9-folds, and to reduce the state space for the entire pedigree by more than five orders of magnitude from 2 50  to 2 32 . For another pedigree, recently used for the study of pituitary adenoma (Vierimaa  et al. ,  2006 ), we were able to reduce the state space from 2 27  to 60 · 2 18 , by a factor of more than 8.5. Previously, only approximate scores could be computed for this pedigree on a standard PC and software for exact linkage computations such as Merlin (Abecasis  et al. ,  2002 ) failed as reported by Albers  et al.  ( 2008 ). Our space reduction enables exact linkage computations and we demonstrate order of magnitude experimental speedups when performing computations across 6000 genomic locations, as used by standard SNP panels for linkage analysis. 2 STATE REDUCTION IN HMMS Consider an HMM with hidden variables  S i  and observed variables  X i ,  i  = 1,…,  L  (Rabiner and Juang,  1986 ). The ( hidden ) state space is the set  S  of possible values for  S i . The state space is identical for every slot  i . The likelihood of data ( x 1 ,…,  x L ) for  L  slots is specified via two main components. The single slot likelihood of data  P ( x i | s i ) at slot  i  given a state  s i  for  S i  and the transition probabilities  P ( S i  =  s i | S i −1  =  s i −1 ) from a state at slot  i  − 1 to slot  i .
 (1) 
 The time complexity of computing this sum grows quadratically with the size of the state space | S | and linearly in the number of slots  L . The time complexity is  O ( L | S | 2  +  cL | S |) where  c  is an upper bound for computing the single slot likelihood  P ( x i | s i ). We note that in many HMM applications, including linkage analysis, the goal is to compute the marginal probabilities  P ( S i | x 1 ,…,  x L ) for all  i  = 1,…,  L  rather than to compute the likelihood of data. This task can be completed using the junction-tree algorithm with only twice the computational cost (Lauritzen and Spiegelhalter,  1988 ). We show experimental results for both tasks, but restrict the discussion to computation of the likelihood to simplify the presentation. We focus on applications where  S  is possibly very large such as for linkage analysis where it grows exponentially in, roughly, the number of persons in the pedigree. In such cases, the dominating factor | S | 2  can be reduced substantially if the state space  S  can be partitioned into equivalence classes [ s ] for which the likelihood of data is constant. This effectively changes the sum over the state space at each slot to a sum over equivalence classes. The dominating complexity will now depend on the number of equivalence classes rather than on the number of states in  S . The likelihood is computed for one representative of each equivalence class via
 (2) 
where the prior for a class [ s ] is the sum over the priors of its constituent states, namely,  P ([ s ]) = ∑ s ∈[ s ] P ( s ). Note that [ s i ] is used to denote the class containing  s i , as in  s i ∈[ s i ], and also a representative from the class containing state  s i , as in  S i =[ s i ]. The equivalence of Equations ( 1 ) and ( 2 ) stems from two general conditions. Explicating these conditions facilitates the discovery of novel equivalence classes that reduce the computational cost of likelihood computations, as we show for genetic linkage analysis. Condition I: the single-slot likelihood given a hidden state  s  is equal for all states in the equivalence class [ s ], namely,  P ( x i | s ) =  P ( x i | s ′) for all  s  and  s ′ in the same equivalence class. Hence, we can safely define the single-slot likelihood given an equivalence class via  P ( x i |[ s ]) =  P ( x i | s ). Condition II: denote by  P ([ s ]| s ′) = ∑ s ∈[ s ] P ( s | s ′) the transition probability from state  s ′ to an equivalence class [ s ]. The condition is that this transition probability does not distinguish between two states in the same equivalence class, namely,  P ([ s ]| s ′) =  P ([ s ]| s ″) for all  s ′ and  s ″ in the same equivalence class. Hence, we can safely define the transition probabilities between equivalence classes via  P ([ s ]|[ s ′]) =  P ([ s ]| s ′). These two natural conditions are sufficient to ensure that Equations ( 1 ) and ( 2 ) are equivalent due to the following reasoning. We first rewrite the right most sum in Equation ( 1 ). The following equality is due to Condition I.
 
The latter sum is further rewritten,
 
where the final equality is due to Condition II. Proceeding with these steps over decreasing indices  L ,  L  − 1,…, 1 transforms Equation ( 1 ) to Equation ( 2 ) where in the last step  P ([ s 1 ]) is set to ∑ s 1 ∈[ s 1 ] P ( s 1 ). 3 STATE-SPACE REDUCTION IN FACTORED HMMS Factored HMMs  (Ghahramani and Jordan,  1997 ) are HMMs in which the hidden variable is a vector  S i  = ( S i 1 ,…,  S i k ) with values drawn from a Cartesian product  H 1  × ··· ×  H k  and with a transition probability defined component by component for  i  = 2,…,  L  via
 (3) 
and for the first slot,  P ( S 1  = ( s 1 1 ,…,  s 1 k )) = ∏ j =1 k   P j ( s 1 j ). When all the component transition probabilities  P j  are equal for all  j , we term the resulting HMM a  homogeneously factored HMM . Factored HMMs offer computational benefits when computing the likelihood of data. Ghahramani and Jordan ( 1997 ) show how specifying the probabilities  P ( S i | S i −1 ) via a product as in Equation ( 3 ) reduces the time complexity to  O ( L | S |log| S | +  cL | S |). Their algorithm is a special case of bucket elimination (Dechter,  1998 ). We note that computing the  L  marginal probabilities  P ( S i | x 1 ,…,  x L ) in a factored HMM can be performed with only twice the amount of computations using the junction-tree algorithm (Lauritzen and Spiegelhalter,  1988 ). We offer state-space reductions for homogeneously factored HMMs that maintain these benefits and further reduce the computational complexity. For simplicity of notation, we assume that  H i  = {0, 1} and so each  S i j  is a binary variable, which we call a  selector , and the state-space size is 2 k . The state-space reductions are formed by clustering the selectors and partitioning the states of each cluster so that Conditions I and II are satisfied. Assuming the choice of clusters is such that Conditions I and II are satisfied, we now explicate how the likelihood of data computations are carried out in the reduced state space, and examine the reduction in time and space complexity of the computation. In particular, starting with Equation ( 1 ), we need to show how the computation of each sum is done when the state space is in a factored form. Suppose ℬ = { B 1 ,…,  B m } is a set of disjoint clusters of all selectors for some slot  i  with  r 1 ,…,  r m  selectors, respectively, and suppose 𝒜 = { A 1 ,…,  A m } is a set of disjoint clusters of all selectors for the previous slot where  S i j  ∈  B l  iff  S i j −1  ∈  A l . Let  a l  and  b l  denote vectors of zeros and ones of length  r l . Then the final sum in Equation ( 1 ), denoted by Σ L , can be written as follows.
 
Due to Condition I, we get,
 
Due to Condition II, the last sum equals  P ([ b m ]|[ a m ]). Incorporating these two modifications sequentially for the indices  m ,  m  − 1,…, 1 yields
 (4) 
This sum is carried out right to left, summing over [ b m ], then over [ b m −1 ] and finally over [ b 1 ]. The result is a conditional probability table Σ L  = λ( x L |[ a 1 ],…, [ a m ]). This conditional probability table is carried to the  L  − 1's sum of Equation ( 2 ), and this process is repeated  L  times, once per slot. Hence, the likelihood depends on the cluster states and not on the states of individual selectors. We now define a specific choice of clusters and prove that it satisfies Condition II. In the next section, we specify additional domain-specific restrictions in order for this choice to also satisfy Condition I, as needed in order to achieve computational savings. For simplicity, we assume a symmetric transition probability table so that a transition from state 0 at slot  i  to state 1 and from state 1 to 0 are equal and are denoted by θ i . Note that the results can be easily extended beyond binary domains  H i  = {0, 1} and without assuming symmetric transition probability tables, but this extension is not needed for genetic linkage analysis. A selector can have two complement states:  on  and  off . For a cluster  C  with  r  selectors, a state [ j ] of the reduced state space of  C  is the equivalence class which contains all vectors of size  r  that have  j  entries being  on  and  r  −  j  being  off . So, we have  c ( j ,  r ) =  r !/ j !( r  −  j )! vectors in state [ j ] for  j  = 0,…,  r . This set of  r  + 1 equivalent classes is called the  counting partition . T heorem  1. Let S  = ( S 1 ,…,  S k )  be a vector of selectors and let  𝒞={ C 1 ,…,  C m }  be a set of disjoint clusters with r 1 ,…,  r m   selectors ,  respectively ,  in each cluster ,  where k  = ∑ j =1 m   r j .  Then a factored HMM in which the hidden variable has values drawn from the Cartesian product  [𝒞] = [ C 1 ] × ··· × [ C m ],  where  [ C l ]  is the set of equivalence classes of cluster C l   generated by using the counting partition ,  satisfies Condition II . PROOF. To prove that Condition II holds for the counting partition, we consider a single cluster  C  ∈ 𝒞 with  r  selectors. The transition probability   for switching from a state   of  C  with  j  positions  on  to any one state with  i  positions  on  is developed below. Let θ be the probability of switching from state  on  to state  off  and of switching from state  off  to state  on . The other two transitions have probability 1 − θ. The probability of switching from a state   where  j  selectors are  on  to the state [ i ] in which some arbitrary  i  selectors are  on  is given by
 
where  t  is the number of selectors that are  on  both in [ i ] and in the state  . Since this formula does not depend on which  j  selectors are  on , it follows that  . This is exactly Condition II for one cluster of  r  selectors. These definitions of the transition probability tables apply separately to each of the clusters  C 1 ,…,  C m . Consequently, the conditional probabilities  P ([ s ]| s ′) satisfy Condition II via
 
where  c l  is the component of state  s  for the selectors associated with  C l , and where  c l ′ and  c l ″ are the components for the selectors  C l  of the two equivalent states  s ′ and  s ″. ▪ The counting partition reduces the state space of each cluster with  r  selectors from 2 r  to  r  + 1 states. Thus, the complexity of computing Equation ( 4 ) for this partition is the following. Suppose the  k  selectors are divided into  m  equally sized clusters each corresponding to  k / m  selectors and having  d  = 1 +  k / m  states. Then summing over [ b m ] yields a probability table λ( x L |[ b 1 ],…, [ b m −1 ], [ a m ]). The next sum yields a table λ( x L |[ a 1 ],…, [ b m −2 ], [ a m −1 ], [ a m ]). Finally, the conditional probability table Σ L  = λ( x L |[ a 1 ], […, [ a m ]) is created. Since each λ table has  m  dimensions, each step involves  O ( d m +1 ) arithmetic operations. This step is repeated  m  times, and therefore  O ( md m +1 ) arithmetic operations are used. The entire process is repeated  L  times, once per slot, and so the overall complexity is  O ( Lm (1 +  k / m ) m ) where 1 ≤  m  ≤  k . For example, if  m  =  k , each cluster contains one selector, the complexity is  O ( Lk 2 k +1 ) as suggested by Ghahramani and Jordan ( 1997 ) and used in all the current HMM-based linkage analysis programs (Kruglyak and Lander,  1998 ; Markianos  et al. ,  2001 ). On the other extreme, when all selectors are clustered together, namely  m  = 1, then the complexity is merely  O ( L (1 +  k )). Examples of utilizing the counting partition and obtaining significant speed up in real genetic applications are discussed in the next two sections. 4 APPLICATION IN GENETIC LINKAGE ANALYSIS The purpose of genetic linkage analysis is to score the human genome in such a way that produces high scores for areas that harbor genes that predispose to a disease under study. The means are pedigrees of families that segregate the disease and genetic information such as SNP data measured on individual members of these families. There are various scoring methods for linkage analysis, some are called parametric and some non-parametric, but all scoring methods share the backbone of a common HMM. This HMM is in fact a homogeneously factored HMM and its state space is defined by a set of selectors precisely as discussed in the previous section. The data at slot  i  are the measurements of the individuals' genetic material at the  i -th location. In this section, we provide the needed background to describe the meaning of the transition probabilities, define precisely the likelihood  P ( x i | s i ) of data at slot  i  given a hidden state and provide clustering methods that generate the state-space reductions via the counting partition as studied in the previous section. 4.1 HMM for linkage analysis A  pedigree  is a directed acyclic graph ( V ,  E ) with a set  V  of  n  vertices of two possible types called male and female and a set of directed edges  E  ⊆  V  ×  V  such that for each vertex  v  there is at most one directed edge ( u ,  v ) for which the type of  u  is male and at most one edge for which the type of  u  is female. When a vertex has in-degree 0, it is called a  founder . A vertex that is not a founder is called a  non-founder . Each vertex in a pedigree is classified either as  typed  ( measured ) or  untyped  ( not measured ). Semantically, each vertex in a pedigree represents a person and each directed link represents a parent–child relationship. When a vertex has in-degree 1, it means that one parent is specified in the pedigree and the other is not. A typed vertex represents a person whose genetic material has been measured. Such person is also said to be  typed . D efinition . A  potential descent graph  for a pedigree ( V ,  E ) is a directed acyclic graph ( V ′,  E ′) such that for every vertex  v i  ∈  V , there are two vertices  m i  (termed the maternal vertex) and  p i  (termed the paternal vertex) in  V ′, and for every edge ( v i ,  v j ) ∈  E  there are two edges in  E ′ as follows: if the type of vertex  v i  is male, then the edges ( m i ,  p j ) and ( p i ,  p j ) are in  E ′ and if the type of vertex  v i  is female, then the edges ( m i ,  m j ) and ( p i ,  m j ) are in  E ′. If a vertex  v i  ∈  V  is typed, then both  m i  and  p i  are typed and if  v i  is not typed then both  m i  and  p i  are untyped. Semantically, the meaning of a pair of vertices ( m j ,  p j ) is the maternally inherited and paternally inherited genetic information of person  j  at some genomic location. A parent  i  contributes either  m i  or  p i  to each child  j . D efinition  (Sobel and Lange,  1996 ). A  descent graph D  = ( V ″,  E ″) of a potential descent graph ( V ′,  E ′) is a subgraph of ( V ′,  E ′) such that  V ″ =  V ′ and for every pair of edges {( m i ,  p j ), ( p i ,  p j )} in  E ′ exactly one is in  E ″ and for every pair of edges {( m i ,  m j ), ( p i ,  m j )} in  E ′ exactly one is in  E ″. Vertices that are classified as typed in  V ′ remain typed in  V ″ and the other remain untyped. Note that for a pedigree with  n  non-founders there are 2 2 n  descent graphs, since there is a binary choice of genetic material twice for every person that is not a founder. For each choice from a pair of edges, we assign a binary variable called a  selector  whose values are 0 if the first edge from a pair is chosen and 1 otherwise. The vector of selectors, which is called the  inheritance vector , can get 2 2 n  assignments and each assignment  s  defines a descent graph denoted by  D [ s ]. Each assignment  s  is called an  inheritance state . Each descent graph specifies how each of the founding alleles is inherited. That is, given a descent graph of a pedigree and an assignments of alleles  a  = ( a 1 ,…,  a 2 f ) to the maternal and paternal variables of its  f  founders, every maternal and paternal variable is assigned a specific founding allele. In other words, each descent graph consists of 2 f  directed trees, two for each founder, called  descent trees  and each descent tree specifies how one founder allele is assigned to the maternal and paternal vertices that constitute that tree. A  label  of a typed person  v i  is an unordered pair of letters { a i ,  a j } from a finite set  A . An element of  A  is called an  allele . The label is also termed the  genotype  of person  v i  (at some genomic location). The  marker data  at some genomic location are a vector of genotypes—one for each typed person. In the case of SNP marker data, there are only two letters in  A  and hence each label has three options: {{0, 0}, {0, 1}, {1, 1}}. In the case of simple tandem repeats (STR) markers, there are  r  letters in  A  and therefore   possible labels of the form { a i ,  a j }. If the genotype of a person is ( a ′,  a ″) then either its maternal allele is  a ′ and its paternal allele is  a ″ or the converse. The marker data are a vector of unordered pairs and does not distinguish which allele is the maternal and which is the paternal. Each typed person  v i  adds a constraint on the possible alleles for ( m i ,  p i ). D efinition . A vector of founder alleles  a  is said to be  consistent  with marker data  x i  and a descent graph  D [ s ] iff  x i  can be obtained by inheritance via  D [ s ] from  a . This consistency statement is denoted by  a  ↦  x i  ∧  s . The HMM for linkage analysis can now be defined as in Sobel and Lange ( 1996 ). The likelihood of a marker data vector  x i  given a state  s  of the inheritance vector is specified by
 (5) 
where  P ( a ) equals the probability of the founders having a vector of founder alleles  a  = ( a 1 ,…,  a 2 f ). The product form is justified by the common assumption that the founders are random persons from a population and their two alleles are randomly sampled as well (called Hardy–Weinberg equilibrium). As written, this sum is exponential in the number of founders. However, Sobel and Lange ( 1996 ) devised an efficient polynomial algorithm for this sum using founder graphs. The transition probabilities are given by
 
where  P j ( s i j | s i −1 j ) = θ i  if  s i j  ≠  s i −1 j . The biological meaning of the statement  s i j  ≠  s i −1 j  is that a  recombination  has occurred in the  j -th meiosis, namely, in one genomic location a maternal allele of a parent is transmitted to a child and in the next location the paternal allele of the same parent is transmitted to the same child. The specified model is a homogeneously factored HMM and therefore any clustering of the selectors using the counting partitioning satisfies Condition II, as shown in Theorem 1. The remaining challenge is to define clusters that also satisfy Condition I and for this the properties of the likelihood  P ( x i | s ) [Equation ( 5 )] must be studied in detail. 4.2 Chain reductions The main idea for identifying useful clusters is to find chains in the pedigree such that either the chain is  on , meaning that an allele is transmitted from the start of the chain to its end, or the chain is  off , in which case a random allele is transmitted to the end of the chain. Clusters of such chains only depend on the number of selectors that block the transmission of that allele. This idea is made precise as follows. D efinition . A  chain of length l in a pedigree  is a sequence of edges ( v i ,  v i +1 ),  i  = 1,…,  l , such that nodes  v 1 ,…,  v l  are each untyped and have one incoming edge and one outgoing edge in the pedigree, and node  v l +1  has one incoming edge (but may or may not be typed and may have any number of children). A chain of length  l  in a pedigree translates to a set of 2 l  edges in the potential descent graph of the pedigree connecting the maternal and paternal nodes of person  v i  to the paternal node of person  v i +1  when  v i  is a male, and connecting them to the maternal node of  v i +1  when  v i  is a female. In the potential descent graph, we define the maternal node  m 1  to be the  source  of the chain if the parent of person  v 1  in the pedigree is a female and define the paternal node  p 1  as the source, if the parent of person  v 1  is a male. Similarly, we define the  sink  of the chain to be node  m l +1  if person  v l  is a female, and define node  p l +1  as the sink, if person  v l  is a male. There is one descent graph in which the source of the chain is connected by a directed path to the sink of the chain. Let  S 1 ,…,  S l  be the selectors associated with the  l  choices of these edges and define an  on  state as the choice of an edge on the directed path from source to sink, and by  off  a choice that disconnects this path. In practice, before searching for chains in a given pedigree, we transform the pedigree to a  normalized form  by using the following two operations repeatedly until they no longer apply. First, remove an untyped person that has no children. Second, remove an untyped founder that has one child. It can be shown that the likelihood of marker data remains unchanged under these transformations. Furthermore, it can be assumed that the pedigree is specified in a normalized form. This normalization procedure is merely a rule that tells the geneticist when there is no need to add more persons to the pedigree specification, which, in principle, can expand endlessly to various directions. The normalization procedure can create more chains and consequently facilitates larger state-space reductions. For example, consider Pedigree I depicted in  Figure 1  with three typed children  D t ,  E u  and  F z  who are distant cousins. The parents of individuals along the chains  D ,  E  and  F  are not specified in the normalized pedigree.
 Fig. 1. A normalized pedigree with two founders ( A  and  B ) and three typed distant cousins ( D t ,  E u  and  F z , with genotypes  g 1 ,  g 2  and  g 3  respectively). The chains  D ,  E  and  F  can be reduced from a collection of selectors to one cluster each with number of states linear in the chain's length. In Pedigree II, the collection of the selectors in the two chains, can be reduced to a single cluster with number of states linear in the sum of lengths of the two chains. 
 For each chain  C j , we define a cluster which we denote also by  C j . The cluster  C j  consists of the selectors associated with the chain  C j . Such a chain cluster with  l  selectors can get one of  l  + 1 values, corresponding to the number of selectors that are  on . This reduction is the counting partition, defined in  Section 3 . The following theorem states that such chain reductions do not change the likelihood of data. The proof is given in the appendix of the  Supplementary Material . T heorem  2. Let S 1 ,…,  S n   be the selectors for a pedigree  ( V ,  E )  and let C be a chain in  ( V ,  E )  such that S 1 ,…,  S l   are the selectors associated with the edges of C .  Let S C   be a variable with a value equal to the number of selectors S j   that are   on   for  1 ≤  j  ≤  l .  Then the likelihood of data can be computed by summing over the states of S C ,  S l +1 ,…,  S n . This chain reduction can be repeated for every chain in the pedigree yielding considerable reduction in the state space. For example, the full-inheritance state space of Pedigree I in  Figure 1  corresponds to 4 +  t  +  u  +  z  informative meiosis yielding a state space of size 1   N  = 2 t + u + z +4 . The reduced state space uses the fact that the likelihood of data does not depend on the exact state of the selectors along the chains  D ,  E  and  F  that connect the typed cousins to the two founders, but only on whether they point toward a common ancestor, and if not, how many selectors do not point to the common ancestor. In other words, how many selectors are  off . Consequently, for each  W -chain, where  W  ∈ { D ,  E ,  F } of length  w , it is possible to cluster the  w  − 1 selectors from  W 2  along the chain into a single variable with  w  states. Thus, the total reduced state space is now  N ′ = 2 7  ·  t  ·  u  ·  z  yielding an exponential reduction of the state space. 5 EXPERIMENTAL RESULTS We demonstrate the power of the state space reduction in factored HMMs for genetic linkage analysis problems via the pedigree depicted in  Figure 2 . This pedigree was recently used for the study of pituitary adenoma (Vierimaa  et al. ,  2006 ). Since the state-space size of the pedigree is 2 27 , exact linkage scores could not be computed and heuristics were used (Albers  et al. ,  2008 ). Reducing the chains marked in the figure, of lengths 2, 3 and 4, reduces the state space to 2 18  × 3 × 4 × 5 by an overall factor of 8.5. Consequently, exact linkage scores can now be computed, rather than approximated.
 Fig. 2. A normalized pedigree used by Vierimaa  et al.  ( 2006 ) for the study of pituitary adenoma. There are six typed individuals in the pedigree that are marked in black stripes. We use chains 1, 2 and 3 to reduce the state space by a factor of 8.5 and speedup computations. 
 We implemented an algorithm that computes the probabilities  P ( S i | data ) for every location  i , which facilitates the computations of parametric and non-parametric linkage scores. Our implementation supports the factored HMM model for genetic linkage analysis with and without the state-space reduction. We used this software to compare the runtime of computations in the reduced model with the runtime of the original model across 6000 markers, for variable lengths of the chain marked as ‘Chain-1’.  Figure 3  shows the runtime in hours for the two models for lengths  m  = 0,…, 4. For  m  = 3 and  m  = 4 the time it takes to perform computations on the original model has been extrapolated from running the software for this model across 150 locations and multiplying by 40. It is evident from the figure that the runtime grows linearly in  m  in the new model, while it is exponential in  m  in the original model. The probabilities computed are the same in both models. In addition,  Figure 4  plots the runtime of the two models for  m  = 0 as a function of the number of markers for the computation of the likelihood of data and the inheritance probabilities at all loci given the data. As can be seen, the runtime is linear in the number of markers for both models and the runtime ratio is maintained regardless of the length of the model, as expected. In addition, as predicted from complexity analysis, the runtime of computing probabilities at all loci is twice that of computing the likelihood of data.
 Fig. 3. Runtime comparison for computations using the original model and the reduced state-space model for the pedigree in  Figure 2 , as a function of the length  m  of Chain 1. 
 Fig. 4. Runtime comparison for computations using the original model and the reduced state-space model for the pedigree in  Figure 2  across 6000 markers, where Chain 1 is of length 0. 
 6 DISCUSSION In this article, we described two general conditions which, when satisfied, allows one to reduce the state space of HMMs and factored HMMs. We also described when these conditions can be applied to linkage analysis problems which yields a new method for performing exact linkage computations at a potentially reduced cost. In general, when our method reduces the size of the state space it yields a computational savings and, for linkage problems, these savings are exponential. The computation of exact linkage scores (LOD or non-parametric) in linkage analysis is of great importance. Having an exact linkage scores provides researchers confidence to proceed with the often expensive and time-consuming fine-mapping process. The use of approximate linkage score methods typically yields no guarantees or loose bounds that do not enable a researcher to draw conclusions regarding the linkage score. In addition, for stochastic-based Monte Carlo techniques, researcher must rely on approximate tests of convergence. Our state-space reduction method does not yield a computational benefit for all pedigrees, but, as we describe herein, it can yield a significant reduction of the computational cost. This reduction can be crucial for some pedigrees, turning a previously intractable computation into one that is tractable, and beneficial for others. In addition, the identification of the potential cost savings of our method is easy and can even be done by manual inspection of the pedigree. The general idea of collapsing states into equivalence classes is a natural one, yet its realization in genetic linkage analysis is far less obvious because the standard way to represent linkage problems uses redundant selector variables. Identifying these redundancies and using them to speed computations is one of our novel contributions. In our method, we focus on reducing the state space by the quotient of the subspace that arise from chains in a pedigree, where no genetic information is available for individuals on the chain. For a chain that consists of  r  selectors the state space reduces from 2 r  to  r  + 1. Although such chains are the most common structure that enables the space reduction, there are cases when more reductions are possible such as combing two chains together. Consider Pedigree II depicted in  Figure 1  within the dashed rectangle with two affected distant cousins  D t  and  E u . Here, the full-inheritance state-space size equals  N  = 2 t + u +2 . In the reduced state space, the likelihood of data depends only on the combined number of selectors that are  off  in chains  D  and  E  combined. Consequently, it is possible to cluster the  t  +  u  selectors into a single counting variable with  t  +  u  + 1 states. The total reduced state space is now  N ′ = 2 2  · ( t  +  u  + 1) yielding an exponential reduction of the state space and, for this example, an algorithm that grows quadratically in the number of persons in the chains and linearly in the number of markers. Without combining the two chains, the reduction would be smaller yielding a total reduced state space of  N ″ = 2 2  · ( t  + 1)( u  + 1). As another example consider the internal inbreeding loop of the family shown in  Figure 5  which connects the two parents of the affected individuals. We retain four selectors for the two affected children and one selector for a child of the common founder. All other 13 selectors for the two chains are replaced with a single counting variable with 14 states that replaces both chains, rather than having one cluster per chain. The state space reduces from 2 18  = 252, 144 to merely 14 × 2 5  = 448, by a factor of 64/7. When considering contracting chains in the entire pedigree, the total state space dropped from 2 50 , which is completely infeasible, to a state space of 2 32 , a reduction of more than five orders of magnitude.
 Fig. 5. The Finnish family studied by Knappskog  et al.  ( 2003 ), which includes two affected individuals that suffer from cold-inducing sweating syndrome (marked in black). 
 HMMs and factored HMMs are widely used in various applications, thus speeding up common algorithms in these models, as done via the state-space reduction, can be proved useful for other domains as well. We note that the challenge in applying the reduction to other domains lies in finding a suitable partition of the state space, which satisfies Conditions I and II. Once these conditions are satisfied, the computational savings are automatic and do not require a special analysis. Finally, we note that our state-space reductions are also immediately applicable to other methods for linkage analysis such as Silberstein  et al.  ( 2006 ), Sobel and Lange ( 1996 ) and Thompson ( 1994 ). </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>Annotation-based inference of transporter function</Title>
    <Doi>10.1093/bioinformatics/btn180</Doi>
    <Authors>Lee Thomas J., Paulsen Ian, Karp Peter</Authors>
    <Abstract>Motivation: We present a method for inferring and constructing transport reactions for transporter proteins based primarily on the analysis of the names of individual proteins in the genome annotation of an organism. Transport reactions are declarative descriptions of transporter activities, and thus can be manipulated computationally, unlike free-text protein names. Once transporter activities are encoded as transport reactions, a number of computational analyses are possible including database queries by transporter activity; inclusion of transporters into an automatically generated metabolic-map diagram that can be painted with omics data to aid in their interpretation; detection of anomalies in the metabolic and transport networks, such as substrates that are transported into the cell but are not inputs to any metabolic reaction or pathway; and comparative analyses of the transport capabilities of different organisms.</Abstract>
    <Body>1 INTRODUCTION The importance of membrane transport proteins (transporters) to cells is illustrated by the fact that transporters typically make up 5–15% of the total gene content of sequenced organisms. Transporters bring essential nutrients into the cell, and therefore partially determine the environments in which cell growth is possible. They also provide pathogenic bacteria with resistance to antibiotics, and provide cancer cells with resistance to chemo-therapies. This research is motivated by the need to perform symbolic systems biology (Karp,  2001 ) analyses involving cellular transport systems, such as to compute the answers to the following queries for a given organism: (1) What chemical compounds can the organism import or export? (2) For which cellular metabolic pathways can the organism neither import, nor produce via a metabolic reaction, the precursors required by that pathway? This query identifies incompleteness in our knowledge of the metabolic and transport networks. (3) Which molecules can enter a given cellular compartment based on its known transporter complement? (4) How do the complements of transporter functions differ among two or more organisms? These applications are discussed in more detail in  Section 5 . Such analyses demand a computable (ontology based) representation of transporter function. We developed such a representation several years ago as part of the Pathway Tools project (Karp  et al. ,  2002 ; Krummenacker  et al. ,  2005 ; Paley and Karp,  2006 ). In Pathway Tools, transporter functions are represented as  reactions  in which the substrates of the reactions are labeled with the cellular compartments in which those substrates reside. The curators of the EcoCyc database (DB) have manually populated EcoCyc with 238 transport reactions describing the functions of 214 transporters (Keseler  et al. ,  2007 ) using the interactive editors within Pathway Tools. However, manually curating transport reactions is very time-consuming. Therefore, the problem addressed in this article is to develop an algorithm that will automatically infer the correct transport reaction for a transport protein given the English functional annotation assigned to the transporter. Our work follows in a long line of research involving the application of natural-language processing techniques to bioinformatics [see, for example, Rzhetsky  et al.  ( 2004 ) and the biomedical text mining tracks in the Pacific Symposium on Biocomputing proceedings from 2006–2008] to extract ontology-based descriptions of biological knowledge from natural-language text. However, our work does not involve processing of long texts, but rather of short textual descriptions of transporter functions. This article describes an algorithm called the Transport Inference Parser (TIP) that identifies the transporter proteins within a genome, that infers the transport reaction(s) catalyzed by these proteins, and that constructs a full ontology-based representation for each transport reaction and protein within a Pathway/Genome DB (PGDB). In addition, TIP infers multimeric transporter complexes, and constructs PGDB objects describing the inferred complexes. TIP is a component of Pathway Tools, which is a comprehensive symbolic-systems biology software system that supports several use cases in bioinformatics and systems biology. It supports development of organism-specific DBs [also called model-organism DBs (MODs)] that integrate many bioinformatics datatypes, from genomes to pathways, and is in use by many MOD projects including EcoCyc (Keseler  et al. ,  2007 ), SGD (Nash  et al. ,  2007 ), Mouse Genome Informatics (Bult  et al. ,  2008 ), dictyBase (Chisholm  et al. ,  2006 ) and the BioCyc collection of 370 PGDBs (Caspi  et al. . Pathway Tools provides several other computational inferences including prediction of metabolic pathways, prediction of metabolic pathway hole fillers and prediction of operons. It provides scientific visualization services including automatic display of metabolic pathways and full metabolic networks; a genome browser; and display of operons, regulons and full transcriptional regulatory networks. It supports visual analysis of omics datasets, such as painting omics data onto diagrams of the full metabolic network, full regulatory network and full genome. It supports comparative analyses of PGDBs and analysis of biological networks such as identifying choke points (potential anti-microbial drug targets) in metabolic networks. Although the annotation of membrane transporters from genome sequence has been extensively studied, we are not aware of any past work to generate transport reaction equations from free-text transporter annotations. Our work fills this fundamental gap and will be of interest to researchers in genomics; metabolic modeling; and development of flux-balance models (Feist  et al. ,  2007 ; Mo  et al. ,  2007 ), where transporters are represented by reactions, and it is critical that all transport reactions be included in the model. 2 APPROACH The algorithm described in this article addresses several related problems:
 Given the full set of monomeric proteins P of an organism, identify the subset P T  of P that are transport proteins. For each monomeric transporter in  P T , infer the one or more multimeric complexes that the transporter is involved in. For each monomeric transporter or transporter complex in  P T , infer the one or more transport reactions that the transporter facilitates. Populate a PGDB with objects describing the inferred transport reactions and multimeric complexes. We consider all information provided as part of an annotated genome sequence to be fair game toward solving these problems, such as the protein sequences, and the functional annotations of the transporters, which are typically provided in the Genbank / product  field. Our method relies principally on textual analysis of the natural-language descriptions of transporter functions that are found in annotated genomes. Why do we perform this textual analysis rather than simply inferring the transport reaction directly from the protein sequence? The reasons are that inferring every aspect of a transporter function from its sequence in a completely automated fashion is a very hard problem. Many genome centers annotate transporter functions with oversight from a person who is skilled in sequence analysis. We believe that replacing those expert annotations with transporter functions that are inferred automatically would reduce their quality; therefore, our approach is to build on the existing genome annotation. Table 1  shows example inputs and outputs for TIP. In  Table 1 , we write transport reaction substrate compartments as bracketed subscripts. Omission of the compartment both in this table and in a PGDB, implies the default compartment: the cytoplasm. Within a PGDB, transport reactions are represented as a single PGDB object. One attribute of the object stores the reactants; a second attribute stores the products. Each substrate (reactant or product) can be labeled with the identities of the one or more compartments in which the substrate occurs using a PGDB construct called an  annotation , which is simply a way of attaching labeled information to an attribute value.
 Table 1. Example inputs and outputs to TIP Input: protein function Output: inferred transport reaction predicted ATP transporter of cyanate cyanate [ extracellular ] +H 2 O + ATP = cyanate + phosphate + ADP putative phosphate ABC transporter ,  ATP binding subunit phosphate [ extracellular ] + H 2 O + ATP =2 phosphate + ADP putative potassium channel sodium/proline symporter - proline [ extracellular ] = Na + + L - proline lactose transport system permease protein The inputs to TIP consist of transporter function names present in protein objects in PGDBs, plus other information available in the genome and its annotation. The outputs of TIP consist of structured descriptions of transporter function in the form of transport reactions. Our approach to transport function prediction for a PGDB consists of solving the following sequence of subproblems:
 Starting with the full set of monomeric proteins of an organism as defined in the PGDB, identify the transport proteins  P T . For each transport protein T:
 identify the reaction substrates of T; determine an energy coupling for T (e.g. is T a passive channel, or an ATP-driven transporter, or a sodium-driven symporter?) assign a compartment to each substrate of T; identify and construct transporter complexes for T; and construct a full PGDB reaction for T. 
The TIP algorithm is discussed in the following section. See  Supplementary Material  for data supporting the algorithm. 3 METHODS Development of the rules underlying TIP was guided by Paulsen's expert knowledge of transporters in general, and of the key pieces of knowledge one must have to characterize the function of a transporter: its primary substrate, carrier substrate(s) (if any), energy coupling mechanism and directionality. Rule development was further guided by a review of hundreds of example transporter function descriptions from many genomes. TIP has undergone an iterative development process whereby we have run it on many genomes, and manually adjusted the algorithm to improve its performance on transporter functions that it did not properly interpret, such as by extending the lists of indicator and counterindicator keywords. One of the most challenging aspects of this project was the fact that different genome annotation centers use different styles and conventions in the phrasing of transporter annotations. That is, the same transporter function is expressed in many different ways in different genomes. Therefore, our development of TIP involved running it on genomes from multiple genome centers, and from a taxonomically diverse set of organisms. Overall, the rules used by TIP are specific to transporter functions, but it may be possible to use a similar strategy for converting free-text descriptions of other types of proteins into ontology-based descriptions. 3.1 Identify transport proteins To identify transporters, all proteins of the PGDB are filtered based on their annotation. Each protein whose annotation contains an  indicator keyword or phrase  indicative of transport function (e.g. ‘transport’, ‘channel’, ‘permease’) is designated as a transporter, unless it contains a  counterindicator keyword or phrase  (e.g. ‘regulator’). A counterindicator indicates that the annotation is likely to be difficult to analyze without sophisticated parsing techniques. For example, such annotations sometimes refer to a transport function of another protein, rather than describing the function of its protein. Each transporter is classified as either a  high-quality  or a  low quality  prediction. A low-quality prediction is one in which there is incomplete or ambiguous information in the annotation. Proteins whose annotations contain an  ambiguity keyword or phrase  (e.g. ‘resistance’) are considered low quality, as are those with annotations that exceed a threshold number of words (12), because we observed that textual analysis of long annotations is error prone. One type of error is improperly identifying non-transporters as transporters because their annotation happens to contain transporter indicator keywords. 3.2 Identify reaction substrate(s) The protein annotation text is parsed and analyzed to identify one or more compounds that are the substrates of the transport reaction. The set of small-molecular weight chemical compounds and compound classes within the MetaCyc DB (Caspi  et al. ,  2006 ) is used as a dictionary for compound identification. MetaCyc contains an extensive set of synonyms for the names of metabolites. All transporters have at least one  primary  substrate that crosses a cell membrane. Most transporters have a single primary substrate (for example, ‘probable phosphate transporter’). Other transporters have multiple primary substrates, due to loose substrate specificity (for example, ‘cytosine/purine/uracil/thiamine/allantoin permease family protein’ or ‘magnesium and cobalt transport protein corA, putative‘). If no primary substrate can be found, the transporter is considered a low-quality prediction. Secondary transporters have an additional  carrier  substrate, for example, for many transporters the transport of the primary substrate is driven by the proton gradient between the interior and exterior of the cell. The carrier substrate defines the energy-coupling mechanism of the transport process. The carrier substrate may be explicitly named in the annotation (for example, ‘sodium:sulfate symporter transmembrane domain protein’) or it may be unspecified. Finally, a transport reaction may have one or more  auxiliary  substrates. For example, ATP-driven transporters include the compounds ATP, ADP and H 2 O as auxiliary substrates; these are not determined by parsing the annotation for substrate names, but are implied by the energy-coupling mechanism. A simple search of a compound DB for words that occur in the annotation will yield many false positives (for example, ‘as’), which are filtered using an exception list of compounds that do not occur biologically, or do not typically occur in transport reactions. A substrate name in the annotation may not correspond exactly to the chemical names in MetaCyc. Substrate names are canonicalized for DB matching by removing spaces, hyphens and other punctuation; by converting to lower case; by converting plurals to singular form; and by stripping off affixes (for example, ‘- specific’, ‘- transporting’). Certain elemental forms are converted to their ionic form (for example, ‘hydrogen’ to ‘H+’). Some substrates are specified as classes of compounds, rather than individual compounds (for example, ‘amino acid transporter’). This is not a difficult complication, because MetaCyc includes both compound classes and individual compounds. However, since many classes have multi-word names, word sequences as well as individual words must be considered. It is assumed that the first substrate or a group of textually contiguous substrates found in the annotation are the substrates in the transport reaction; subsequent substrates are ignored. For example, in the annotation ‘Ca+2 transporter; possible Mg+2 transporter’ only ‘Ca+2’ would be detected as a substrate. In practice, this simplifying assumption leads to few incorrect substrate predictions. 3.3 Determine energy coupling mechanism In a PGDB, the energy coupling of a transport reaction determines the class of reaction within the Pathway Tools reaction ontology under which that reaction object is created. The couplings currently handled by TIP are
 Ion channel: ions passively diffuse through the transporter. The reaction equation is  ion [ extracellular ] = ion [ cytosol ] . Secondary transporters: a secondary (carrier) substrate is co-transported with the primary substrate. If substrates are transported in the same direction the reaction equation is  primary [ extracellular ] + carrier [ extracellular ] = primary [ cytosol ] + carrier [ cytosol ] . If substrates are transported in the opposite direction the reaction equation is  primary [ extracellular ] +carrier [ cytosol ] = primary [ cytosol ] +carrier [ extracellular ] . ATP-dependent (ATP): the hydrolysis of ATP provides energy to transport a primary substrate that is a solute. The reaction equation is  H 2 O+ ATP + solute [ extracellular ] = phosphate + ADP + solute [ cytosol ] . Phosphenolpyruvate-dependent phosphotransferase system (PTS): phosphenolpyruvate is used as an energy source and phosphate donor to transport and phosphorylate a primary substrate that is a sugar. The reaction equation is  phosphoenolpyruvate + sugar [ extracellular ] = pyruvate + phosphorylated - sugar [ cytosol ]. Unknown: a catch-all class used if a more specific determination cannot be made. The reaction equation is  substrate [ extracellular ] = substrate [ cytosol ] . To identify the energy coupling, the following rules are applied to the predicted primary substrate(s) and annotation:
 TIP is configured to treat a few primary substrates as being indicative of a particular coupling. For example, the transport of ‘protoheme’ indicates an ATP transporter because the only transporters known for protoheme are ATP driven. 1 If no coupling is indicated by the primary substrate, the annotation is searched for the presence of keywords indicating an energy coupling. For example, ‘channel’ indicates an ion channel transporter, ‘carrier’ indicates a secondary transporter and ‘atp-binding’ indicates an ATP transporter. If no coupling clues are present, the reaction is put into a default class indicating that the coupling mechanism is unknown. For secondary transporters, the carrier substrate must be determined. The following rules are applied in order:
 If one of the substrates identified is either a proton or a sodium ion, it is designated as the carrier, and the remaining substrates become primary substrates (for example, ‘sodium:glutamate symporter’). If there are exactly two substrates, one is chosen arbitrarily as the carrier, and the other becomes the primary substrate. The carrier is assumed to be a proton; all other substrates become primary substrates (for example, ‘amino acid transport system carrier protein’ implies a proton carrier and ‘amino acid’ as the one primary substrate). 3.4 Assign a compartment to each substrate A complete transport reaction includes the designation of the initial and final cell compartments of each primary substrate. Our method searches the annotation for keywords indicating the initial or final compartment of the primary substrate(s). For example, ‘uptake’ indicates transport into the cell, and ‘export’ or ‘efflux’ indicates transport out of the cell. For transporters with a carrier substrate, certain keywords indicate transport direction relative to the carrier. For example, ‘symport’ indicates that both the carrier and the primary substrate start and end in the same compartment. The keyword ‘antiport’ indicates an exchange between two compartments. In the absence of more specific information, it is assumed that transport of the primary substrate (and carrier substrate if present) is into the cytoplasm. Our current implementation of TIP is limited in that it is oriented toward bacteria, and does not perform well on eukaryotic compartments. In particular, it does not detect eukaryotic compartment names, nor does it attempt to assign eukaryotic compartments to substrates by other means. In addition, even for bacteria the implementation currently assumes a Gram-positive cell type, meaning it does not distinguish the inner membrane from the outer membrane. 3.5 Identify and construct transporter complexes Many transporters are multimeric systems of several protein monomers. In these systems, the proteins comprise a transport complex that catalyzes the transport reaction. TIP infers the grouping of a set of individual transport monomer proteins into a complex (and constructs the representation of the complex in the PGDB) if the monomers satisfy all the following rules:
 The predicted substrate (or set of substrates) for each monomer in a complex must be identical. The predicted energy coupling must be ATP or PTS. The genes of all proteins within a multimeric complex must share a common operon. Rule (1) enforces a requirement of consistency among the monomeric annotations with respect to substrates. Rule (2) is in place because other types of transporters are less commonly found as multimers. Rule (3) reflects that the co-occurrence of transporter monomers within a single operon is a strong indicator of multimers. Note that this implies that a genome with no operons will have no predicted multimers. Thus, no multimers are inferred in eukaryotes, and multimer inference is attempted only for PGDBs that contain operons. Operons can be predicted by the Pathway Tools operon predictor (Romero and Karp,  2004 ) or defined manually using Pathway Tools editors. We believe these rules will result in few false positive predictions, but acknowledge they will fail to identify a significant number of multimeric transporters. 3.6 Construct full compartmentalized reaction At this point in the method, all predictions have been made. Each predicted transport reaction is added to the PGDB. One reaction is created for each predicted primary substrate. The primary substrate is added to the list of reactants of the reaction, and is annotated with its initial compartment; it is then added to the list of products of the reaction, annotated with its final compartment. If present, the carrier substrate is added as a reactant and a product, annotated with the appropriate compartment depending on whether it is a symport or an antiport reaction. Each auxiliary substrate implied by the coupling (for example, ATP, ADP and water for ATP reactions) is added to the appropriate side of the reaction. If the coupling mechanism is PEP, the primary substrate is phosphorylated during the transport. To reflect this, MetaCyc is searched for a phosphorylated variant of the primary substrate. If found, it replaces the primary substrate as a product of the transport reaction. In the atypical case in which it is not found, the unmodified primary substrate remains both a reactant and a product in the reaction. To maximize the notion of MetaCyc reactions as a controlled vocabulary, we prefer to maintain a one-to-one mapping between reaction identifiers and reaction equations. That is, we do not want MetaCyc or other PGDBs to define the same reaction under different identifiers. Thus, if a reaction with the same substrates as those in the transport reaction just inferred are found in MetaCyc, the MetaCyc reaction object is imported into the PGDB where TIP is being run. Otherwise, a new reaction object is created. 3.7 Examples We present an extended example of the operation of the TIP algorithm for a representative protein, followed by examples of erroneous predictions and successive refinement to TIP's behavior. 3.7.1 Extended example Consider a protein with the annotation  sodium/proline symporter . First, the protein is identified as a transporter by the presence of the keyword ‘symporter’. Next, the annotation is scanned for substrates; TIP has a rule that considers words separated by slashes as possible multiple substrates. TIP queries MetaCyc with the word ‘sodium’, that matches the compound object  Na + , which has a synonym ‘sodium’. Similarly, MetaCyc is queried with ‘proline’, that matches the compound object  L -proline. The energy coupling mechanism is determined to be SECONDARY by the presence of the keyword ‘symporter’. A secondary transporter has a carrier substrate; TIP applies the rule that if either a proton or a sodium ion is present, it is the carrier. So  N a +  is made the carrier substrate, and  L -proline remains the primary substrate. The presence of ‘symporter’ indicates that the primary and carrier substrates start and end in the same compartments. Absent other indicators, TIP assumes transport is into the cytosol. This leads to the compartment assignments of [ extracellular ] to both substrates on the left side of the reaction, and of [ cytosol ] to both substrates on the right side. Since the coupling is not ATP or PTS, no attempt to identify transport complexes is made. Finally, the full transport reaction is constructed:  . Provenance data is attached indicating that the reaction was predicted by TIP, the date and time of the creation of the reaction, the name of the user operating TIP and an evidence code indicating that the supporting evidence for the prediction is computational in nature. 3.7.2 Examples of errors TIP fails to identify a protein with the annotation ‘arsenical pump-driving ATPase’ as a transporter because neither ‘pump-driving’ nor ‘ATPase’ had been identified as transport indicator keywords. For a protein with the annotation ‘magnesium-exporting atpase’, TIP detects the substrate correctly, as it recognizes that ‘exporting’ is an acceptable suffix for a substrate. However, TIP did not, at the time this protein was encountered, include this suffix as a clue for compartment assignments; this resulted in the incorrect assignment of the cytosol as the ending compartment, rather than the starting compartment, of magnesium. Upon encountering this example, TIP is easily modified to recognize that this suffix implies transport out of the cytoplasm. For a protein with the annotation ‘high-affinity iron permease’, TIP fails to detect a substrate because, although iron is clearly mentioned, iron has multiple valences and there is no mention of which form of iron is transported. TIP currently has no rules to disambiguate such references. 3.8 TIP Implementation TIP can be run in both an interactive and a batch mode. Interactive mode permits review and modification of all inferences made by TIP. Interactive TIP is available as part of the PathoLogic program, as a step in construction of a PGDB. Results are presented in a columnar GUI ( Fig. 1 ). Transporters are sortable by gene name, substrate or coupling, facilitating systematic review and comparison of predictions. The GUI permits display of the set of either high-confidence, low-confidence or all transporters predicted. Each transport reaction is individually selectable, and may be rejected, accepted as is, or modified. Permitted modifications consist of changing the energy coupling, editing the protein annotation and editing the reaction (including the primary substrate and compartment assignments). Typical modifications include changing a predicted coupling of UNKNOWN to a more specific coupling, and providing a reaction for a low-confidence transporter in which the primary substrate was not detected.
 Fig. 1 PathoLogic GUI for TIP. Predicted transport reactions may be individually accepted or rejected (when rejected, the reaction is deleted from the PGDB). Attributes of transporters, including the compartmentalized reaction, may be edited. 
 Fig. 2 A portion of the Comparative Genomics display available at  www.biocyc.org.  Supplementing BioCyc PGDBs with transport reactions enhances metabolic analysis of a single organism and of multiple organisms. In batch mode, TIP is applied to a set of organisms. The batch mode of TIP performs the same predictions as interactive mode except that low-confidence predictions are discarded. Results are not presented for review, but are incorporated immediately into the PGDB. Batch mode permits automation of transporter predictions, such as for the large-scale PGDB creation effort within the BioCyc project (Karp  et al. ,  2005 ). In BioCyc version 11.5, TIP was used to predict transport functions for its 349 Tier 3 PGDBs. In both interactive and batch modes, provenance information is attached to predictions. This information includes a timestamp, identification of the software making the prediction (TIP), the user that is operating TIP and an indication that the evidence supporting the prediction is computational in nature. 4 EVALUATION We evaluated TIP on three genomes chosen randomly from the set of BioCyc Tier 3 (version 11.5) organisms (Caspi  et al. ,  2008 ):  Helicobacter hepaticus ATCC 51449  (sequenced by MWG Biotech, University of Wuerzburg, Massachusetts Institute of Technology, and GeneData),  Fusobacterium nucleatum ATCC 25586  (sequenced by Integrated Genomics Inc., Chicago, Illinois, USA) and  Leifsonia xyli CTCB07  (sequenced by University of Campinas, Campinas, Brazil). Neither these genomes nor genomes from these centers were used to formulate or to tune the TIP heuristics. We partitioned the evaluation into two parts: identification of transporters and prediction of attributes of transport reactions. We do not present an evaluation of transport complex prediction. Since no gold standard exists for evaluation of transporter function predictions, we generated a gold standard for this work. We used TransportDB ( http://www.membranetransport.org/ , (Ren  et al. ,  2007 ) as a basis for this standard. TransportDB is an extensive DB of transporters, supported by evidence of varying type and strength. Like most gold standards in bioinformatics, TransportDB has high-quality data, but it is probably not perfect. We did not take into account the type or strength of evidence in our scoring. We adapted the data from TransportDB in cases of clear annotation discrepancies between it and the PGDB being evaluated. That is, when a PGDB gene function differed from a TransportDB gene function, we used the PGDB gene function in our gold standard since TIP would be run on the gene function present in the PGDB. 4.1 Evaluation of transporter identification Our standard for evaluation of TIP transporter identification was formed by comparing the set of predictions made by TIP with the set of transporters in TransportDB, and reviewing each discrepancy. If there is a transporter in TransportDB that is not predicted by TIP, it is excluded from the standard if the protein function in the PGDB was different for that protein in TransportDB and was non-specific with respect to transport function (for example, a protein with no annotation at all in PGDB would be excluded). Recall that TIP does not perform functional annotation; its goal is to interpret a functional prediction that is in natural-language form. If there is no transporter in TransportDB corresponding to a TIP prediction, the TIP prediction is added to the gold standard if there is a clear indication of transport function in the PGDB annotation. Transporter identification results are shown in  Table 2 .  TransportDB size  is the total number of monomeric transporters in TransportDB.  Standard size  is the number of monomeric transporters in the evaluation standard.  False negatives  is the number of transporters in the standard that are not predicted to be transporters.  True positives  is the number of transporters in the standard that are correctly predicted to be transporters.  False positives  is the number of transporters not in the standard that are predicted to be transporters. True negatives—proteins that are not transporters and not identified as such—are not shown, and typically number in the thousands for an organism.  Precision  is the ratio of true positives to all positives predicted.  Recall  is the ratio of true positives to the size of the standard. Precision and recall are shown for each organism and in aggregate.
 Table 2. Transport identification results for transport reactions TransportDB Standard False True False All Organism size size negative positive positive positive Precision Recall H.hepaticus 117 48 1 47 0 47 1.0 0.98 F.nucleatum 261 263 32 231 32 263 0.88 0.88 L.xyli 179 165 15 150 0 150 1.0 0.91 Total 557 476 48 428 32 460 0.93 0.90 4.2 Evaluation of transporter attribute prediction The standard for evaluation of transport reaction attribute prediction was formed by using the attributes identified in TransportDB, adjusted for any clear discrepancies with the annotation of the PGDB protein. The attributes included are substrates, substrate compartments and energy coupling. TransportDB does not include reactions, but lists substrates and couplings, as well as character-izing transporters as symporters, antiporters, exporters and so on, implying a compartment. In the absence of an explicit compartment, we assume that the primary substrate is imported into the cytoplasm. If either TransportDB or a PGDB annotation is non-specific with respect to substrates, the transporter is excluded from the standard. For example, the annotation ‘Probable transporter’ contains no clue as to the substrate because the annotator could not confidently infer the substrate from the protein sequence. If there is a conflict between the substrates in TransportDB and those apparent in the PGDB annotation, the annotation substrates are used, thereby overriding TransportDB. For example, the protein for gene FN1747 in  F.nucleatum  is annotated as ‘cysteine permease’ but its primary substrate is identified as  alanine  in TransportDB;  cysteine  is used in the standard. In the absence of conflicting or non-specific substrates, the TransportDB reaction attributes are included in the standard. Conflicts between TransportDB compartments or couplings and PGDB annotations are possible, but did not occur for the PGDBs evaluated. Attribute prediction results are shown in  Table 3 . To evaluate prediction of transport reaction attributes, we score all correctly identified transporters (that is, all true positives) in comparison with the standard. A substrate is scored as  Too general  if the prediction is a superclass of the actual substrate (for example, predicting a substrate ‘amino acid’ when the actual substrate is ‘a branched-chain amino acid’). Any other substrate that is not an exact match with the standard is scored as an  Error . A prediction is scored as a  Compartment Error  if the transport direction of the primary substrate does not match the standard (for example, if the ending compartment of the primary substrate is the cytosol in an export reaction). The predicted energy coupling is scored as an  Error  if it does not match the standard, except in the case where the prediction is UNKNOWN; in this case it is scored as  Too general .
 Table 3. Attribute prediction results Substrates Coupling Standard Too Compartment Too Organism size Error general Correct error Error general % Perfect % General % Error H.hepaticus 39 2 0 37 0 2 5 76.9 12.8 10.3 F.nucleatum 147 29 1 117 0 8 20 60.5 14.3 25.2 L.xyli 88 6 1 81 0 1 14 75.0 17.0 8.0 Total 274 37 2 235 0 11 39 67.5 15.0 17.5 Each reaction is attributed to exactly one category in the results. If a reaction's substrate is not predicted correctly, then neither its compartment nor its coupling is scored; if a reaction's compartment is not predicted correctly, its coupling is not scored. The  % Perfect  column shows the percentage of reactions whose predicted attributes match the standard exactly. The  % General  column shows the percentage of reactions that match the standard except for a substrate or coupling that is more general than in the standard; that is, those reactions that are accurate but not precise. The  % Error  column shows the percentage of reactions that are neither  Perfect  nor  General , that is, the reactions containing an attribute that is inconsistent with the standard. It is possible to supplement our method with rules that increase the prediction accuracy for various attributes. However, depending on how the predictions are used, this may not be desirable. For example, many energy couplings that are predicted as being too general (typically UNKNOWN) are in fact channel transporters. Predicting CHANNEL in these cases would increase accuracy, but would also increase errors. In addition, if the prediction results are being reviewed by a curator, it is a natural workflow to examine generic predictions for possible refinements; hence the less specific predictions may lead to a higher quality final result. 5 APPLICATIONS Once transporter activities are encoded in a declarative (computable) form, they can be exploited in several ways. 5.1 Database queries Transport reactions can be searched through DB queries such as ‘Enumerate the set of all influx substrates for this organism’ (Keseler  et al. ,  2007 , or ‘Find all transporters of organic anions for this organism.’ Simple queries such as these require painstaking manual analyses for most genome DBs, but are trivial queries within the Pathway Tools ontology. 5.2 Cellular Overview and Cellular Omics Viewer Once transporters have corresponding transport reactions, transporters can be automatically added to the Pathway Tools Cellular Overview diagram (see  http://biocyc.org/ECOLI/NEW-IMAGE?type=OVERVIEW . This diagram is automatically generated by Pathway Tools, but generation of transporters in the diagram requires that the transporters have transport reactions. Once the diagram is generated, it can be used for visual interrogation of cellular networks (e.g., the software will draw connections between a transported metabolite and all metabolic pathways that involve the metabolite). A large version of the diagram can be generated and printed to provide a metabolic/transport wall chart for the organism. Furthermore, the diagram can be painted with omics data (e.g. gene expression, proteomics or metabolomics data), allowing visual analysis of the data in that experiment, e.g. what transporter genes are upregulated in a given experiment (Paley and Karp,  2006 )? 5.3 Anomaly detection in metabolic/transport networks Existence of transport reactions enables detection of anomalies within the metabolic/transport networks. One type of anomaly is a dead-end metabolite (Keseler  et al. ,  2007 ), which is a compound that is only produced by or consumed by the metabolic network, and has no associated transporter. Most dead-end metabolites are due to errors or incompleteness in the metabolic network. Even the  Escherichia coli  metabolic network, which is probably the best studied and curated network of any free-living organism (Feist  et al. ,  2007 ; Keseler  et al. ,  2007 ) contains 169 dead ends (Keseler  et al. ,  2007 ). Another type of potential anomaly between metabolism and transport is the case of metabolites other than inorganic ions for which a transporter exists, but no metabolic pathway or reaction exists that consumes the metabolite.  Escherichia coli  has a number of such metabolites, which could be due to genome annotation errors in transporters or enzymes, or unknown metabolic transformations. 5.4 Comparative genomics Declarative representation of transport activities also enables comparative analyses of the transport capabilities of an organism. Pathway Tools implements several comparative analyses, such as those shown at  http://biocyc.org/comp-genomics . These comparative analyses are based on functional capabilities, not on sequence. They provide the user with the following comparisons:
 Number of genes coding for transporter proteins. All transporters; and those that catalyze efflux versus influx transport. All compounds transported into the cell; and then a breakdown of those influx substrates that are also pathway inputs, pathway intermediates or enzyme cofactors, and those that fall into none of the preceding categories. All compounds transported out of the cell; and then a breakdown of those efflux substrates that are pathway inputs and those that are not pathway outputs Lists of transporters with multiple substrates; and of substrates with multiple transporters Operon analysis of transporters, listing transporters that are in the same operon as an enzyme operating on the same substrate; and transporters of unknown function in the same operon as an enzyme, which may yield clues to the transported substrate 6 RELATED WORK Many genome DBs are unable to compute with transporter functions, such as answering the questions given in Section 1, because they lack an ontology-based representation of transporter function. Even genome DBs based on Gene Ontology (GO; Consortium,  2008 ) would find such queries difficult to answer. GO does provide controlled vocabulary terms for specific transporter functions. However, at best those GO terms contain transport reactions within their comments that would have to be parsed (e.g. GO:0005330, ‘dopamine:sodium symporter activity’), but those reactions do not refer to a controlled vocabulary of chemical compound names such as that present in MetaCyc (Caspi  et al. ,  2008 ). At worst, many GO terms do not contain transport reactions. The Transporter Classification (TC) system and associated Transporter Classification DB Saier  et al.  (Saier  et al. ,  2006 ) ( http://www.tcdb.org ) provides a different taxonomic classification of transporter than does GO. TCDB does not provide transport reactions, nor does it employ a controlled vocabulary of chemical compounds; therefore, it could not provide an ontological foundation for answering the questions in  Section 1 . We are not aware of previous approaches to converting natural language descriptions of transporter functions. The closest work would probably be programs that automatically assign GO terms to transporters through sequence analysis. However, the outputs of those programs would be subject to the limitations discussed in the first paragraph. Similarly, Lin  et al.  ( 2006 ) developed a method for assigning a transporter to a TC class based on its amino acid sequence. 7 FUTURE WORK TIP is the result of an ongoing process of successive refinement. As new organisms annotated by different genome centers are studied, TIP's rules are enhanced. To prevent regressions and to evaluate cases in which there are tradeoffs in prediction accuracy among various rules, we maintain a testbed of representative proteins. TIP is currently oriented toward bacteria. We plan to enhance TIP to cover eukaryotes. We expect this will include parsing for eukaryotic compartments, associating particular substrates with their most likely origin or destination compartment, and exploiting other knowledge sources besides annotation. Furthermore, we plan to extend the rules for transporter complex prediction for cases in which operons are absent or unknown. 8 CONCLUSIONS We have presented an approach to prediction of transporters and transport reactions based primarily on the textual analysis of the functional annotations of the proteins of an organism. We have discussed its implementation in the Pathway Tools software, which enhances its PGDB construction capabilities by supporting predictions for both curated and non-curated DBs. We have evaluated the performance of TIP on several randomly selected organisms versus TransportDB, a high-quality standard for transporter knowledge. TIP achieves precision and recall rates of 0.93 and 0.90 respectively in identifying transporter proteins, and 67.5% accuracy in predicting complete transport reactions; if allowance is made for predictions that are overly general yet not incorrect, reaction prediction accuracy is 82.5%. Once transporter activities are encoded as transport reactions, a number of computational analyses are possible including DB queries by transporter activity: inclusion of transporters into an automatically generated metabolic-map diagram that can be painted with omics data to aid in their interpretation, detection of anomalies in the metabolic and transport networks, and comparative analyses of the transport capabilities of different organisms. 
          Supplementary Material 
          
             
                [Supplementary Data] 
             
             
             
          
       </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>CAMPways: constrained alignment framework for the comparative analysis of a pair of metabolic pathways</Title>
    <Doi>10.1093/bioinformatics/btt235</Doi>
    <Authors>Abaka Gamze, Bıyıkoğlu Türker, Erten Cesim</Authors>
    <Abstract>Motivation: Given a pair of metabolic pathways, an alignment of the pathways corresponds to a mapping between similar substructures of the pair. Successful alignments may provide useful applications in phylogenetic tree reconstruction, drug design and overall may enhance our understanding of cellular metabolism.</Abstract>
    <Body>1 INTRODUCTION Metabolic pathways consisting of metabolites, biochemical reactions transforming a set of metabolites to others and enzymes catalyzing these reactions provide valuable information regarding material processing centers of a functioning cell and cellular metabolism in general. Several online databases including KEGG ( Kanehisa  et al. , 2012 ) and BioCyc ( Caspi  et al. , 2008 ) provide access to metabolic pathways of various organisms. A comparative analysis of pathways from different organisms provides insights for understanding evolution, speciation, phylogenic reconstruction ( Mithani  et al. , 2011 ;  Heymans and Singh, 2003 ) and drug target discovery ( Guimerà  et al. , 2007 ). Pharmaceutical drug testing is usually implemented on animals, most of the time on mice, before human testing. In such an application, it is usually crucial to know whether specific pathway components of the two species exhibit similar properties ( Caglic  et al. , 2009 ). A successful pathway alignment would prove useful for determining whether test results on one species could be transferred to another without incurring complications. Furthermore, such an analysis is not limited to that between different organisms. It may also be applied between pathways of cancer types and those of healthy cell types to enhance our understanding of cancer-specific metabolic features ( Agren  et al. , 2012 ). A common method for comparative analysis of pathways and biological networks in general is through network alignment. Given a pair of biological networks either from different species or from different tissues within the same species, the goal of network alignment is to map components in one of the networks to their similar counterparts in the other. With regard to alignments targeting specifically metabolic pathways, several methods have been suggested. In  Tohsato  et al.  (2000) , an alignment method based on enzyme hierarchies and enzyme EC number similarity was suggested for the alignment of possibly more than two pathways. Path matching and graph matching to query certain metabolic pathways in an input graph was provided by  Yang and Sze (2007) . Sets of reactions in multiple pathways were compared, omitting the connectivity between the reactions in  Clemente  et al.  (2007). Heymans and Singh (2003)  created an enzyme graph and obtained a one-to-one mapping between the enzymes of two input pathways via maximum weight bipartite matching. Similar enzyme graph construction was used in  Pinter  et al.  (2005) . An integer quadratic programming-based method was suggested by  Zhenping  et al.  (2007) . Similar to metabolic pathway alignment is the problem of protein–protein interaction (PPI) network alignment. The graph models used in the latter are undirected, whereas the former usually aligns directed graphs. However, as far as general graph matching and alignment is concerned, most of the time, the techniques can be extended in both directions, and mainly similar approaches are proposed. Two versions of network alignment have been suggested in related work. In local network alignment, the goal is to identify from the input networks, subnetworks that closely match in terms of network topology and/or sequence similarities. Approaches proposed for this version of the problem include PathBLAST ( Kelley  et al. , 2004 ), NetworkBLAST ( Sharan  et al. , 2005 ), MaWISh ( Koyutürk  et al. , 2006 ) and Graemlin ( Flannick  et al. , 2006 ). In global network alignment on the other hand, the goal is to align the networks as a whole, providing unambiguous mappings between the nodes of different networks. Starting with IsoRank ( Singh  et al. , 2008 ), several global network algorithms using similar definitions have been suggested ( Aladağ and Erten, 2013 ;  Chindelevitch  et al. , 2010 ;  Kuchaiev and Pržulj, 2011 ;  Zaslavskiy  et al. , 2009 ). We provide a constrained alignment framework and a metabolic pathway alignment algorithm, CAMPways. Our algorithm is inspired by the model suggested in  Ay  et al.  (2011) . Within this general model, the goal is to find a global one-to-many alignment of the pathways such that a node may be mapped to a connected subgraph of many nodes. The model is justified by the fact that biologically meaningful mappings may exist when different organisms perform the same function through varying number of steps. Therefore, it appropriately handles the gaps/mismatches inherent in alignment problems, an issue arising in both sequence-related and network-wise alignment. Such is the motivation behind the PPI network alignment approach of  Liao  et al.  (2009)  as well. Although this general model of one-to-many alignments is the same, our method diverges from that of  Ay  et al.  (2011)  after this point. The novelties of the current work are 3-fold. First of all we provide a novel  constrained alignment framework  appropriate for the one-to-many alignments model. This framework has not been used in biological network alignment previously. Second, we show that even the simplest version of the alignment problem within this framework is computationally hard. Based on this computational intractability result, we finally provide a novel algorithm, CAMPways, which appropriately and efficiently implements this framework. Through experimental evaluations based on reverse engineering pathways and biochemical significance measured through functional group conversion hierarchy of KEGG ( Kanehisa  et al. , 2012 ), we demonstrate that the CAMPways algorithm provides higher quality alignments than the state-of-the-art approaches. Furthermore, a second major advantage of the CAMPways algorithm is in terms of its much faster execution speeds as compared with the alternatives. 2 METHODS AND ALGORITHMS 2.1 Problem definition The metabolic pathway alignment problem definition we consider is based on the one-to-many alignments of the  reaction-based  pathway representations used in ( Ay  et al. , 2011 ). Given a metabolic pathway  , we assume a  reaction-based  representation   of  .  G P  is a directed graph where each node   corresponds to a reaction  r i  in  . There exists a directed edge   if an output compound of  r i  is an input compound of  r j . If  r i  is reversible, the edge existence condition is extended by considering the case where an input compound of  r i  becomes an input compound of  r j . Similar extension applies to  r j  as well. Thus, if both reactions are reversible, there are in total four cases for the existence of an edge. Given two pathway representations  , we need to formalize the types of mappings that are allowed under the one-to-many mapping restrictions. Let  R x  indicate a subset of  V P  such that the induced subgraph of the nodes in  R x  is connected in the underlying undirected graph. Denote the set of all such subsets of size greater than zero and less than or equal to  k  with  . Let   denote the analogous set for  . A  legal alignment 
  between   is a set of mappings   for   such that the following are satisfied:
 For   or   is 1. For   and  ,   and  . 
 The first condition implies that all mappings in the alignment are one-to-many mappings, whereas the second implies that all mappings are pairwise compatible in the sense that no reaction from a given pathway may belong to more than one mapping. The quality of an alignment is usually defined in terms of two possibly conflicting measures;  homological similarity  and  topological similarity . The former can be defined as a sum of homology scores of all mappings in the alignment. The homology score of a given mapping   can be defined in terms of the similarities of input compounds, output compounds and enzymes of  R x  and  . Such similarity scores are usually determined as a result of sequential similarity analysis of the molecules under consideration (enzymes or input/output compounds). For the current study, we use the homological similarity scores produced by  Ay  et al.  (2011) . For a given mapping  , first  , which correspond to the unions of all enzymes involved in the reactions subsets  R x  and  , respectively, are produced. An enzymatic homological similarity between   can be computed by creating a bipartite graph where a partition corresponds to the enzymes of  E x  and the other to those of  . A similarity score between every pair of enzymes from  E x  and   is assigned as the weight of the corresponding edge in the bipartite graph. The homology score between   then corresponds to the maximum weight bipartite matching of the produced graph. Similar constructions can be carried out for the unions of input compounds,   and the unions of output compounds  . The homology score of   is then defined as a convex combination of the scores attained from the scores calculated independently for the enzymes, input compounds and output compounds. Topological similarity on the other hand is a measure of the conservation of network topologies with respect to the given set of mappings in the alignment. Given a pair of mappings   and  , a  conserved edge  is induced by this pair if there exists an edge from a reaction in  R x  to a reaction in  R y  and an edge from a reaction in   to a reaction in  , or vice versa. Topological similarity is then defined as a score proportional to the number of conserved edges induced by the pairs of mappings in the alignment. Once both types of similarity scores are resolved, the network alignment problem is usually posed as that of maximizing a convex combination of these two scores. 2.2 Constrained alignment framework We provide a formal description of our constrained alignment framework within the provided one-to-many pathway alignments model. Rather than posing the problem as one of a simultaneous optimization of two possibly conflicting goals, that is, that of homological similarity and of topological similarity, we propose a framework where the only goal is to maximize topological similarity while satisfying some constraints on homological similarity. Given a pathway representation   let   denote the  kth extension  of  G p . It is a directed, edge-weighted graph. Each node   in   corresponds to a reaction subset  . There exists a directed edge   in   if there exists a directed edge from   to   in  G p , where   and  . Let   denote the total number of such edges. We note that   can be defined analogously. The set of constraints of node   in  , denoted with  , is defined as the subset of nodes of   that   can be mapped to. The definition can be extended to the nodes of   analogously. Note that this definition is symmetrical in the sense that   if and only if  . Assume   for any node   in   and   for any node   in  , for fixed constants  k 1  and  k 2 . All constraints can be represented as a bipartite  similarity  graph where the nodes of   form one partition and those of   form the other, and each constraint is represented with an edge in the bipartite graph. The  constrained alignment  problem is that of finding a subset of constraints, that is, a subset of edges from the bipartite similarity graph, such that the subset of edges define a legal alignment and the number of conserved edges induced by the alignment is maximum. It is worth noting that the concept of constrained alignments has appeared in biological network alignment literature before.  Zaslavskiy  et al.  (2009)  provide a definition of constrained alignments applicable to global one-to-one alignments of PPI networks. We note that our constrained alignment framework may trivially be generalized to undirected PPI networks. Moreover, our framework is more general; it strictly includes the model of  Zaslavskiy  et al.  (2009) . There are instances that can not be defined using their model, whereas the opposite is never the case. Using our notation, given   from one of the networks, if  , their model imposes the condition that  . Considering the case where the  Cons  definition reflects high-homological similarity, this is restrictive; either long homologically similar chains of nodes are to be created incorrectly or some homologically similar pairs missed completely. We first state that the constrained alignment problem defined herein is computationally intractable even in a restricted case.
 P roposition  2.1 The constrained alignment problem where  
 and   is NP-complete. 
 P roof Because of space considerations, the proof is provided in the  Supplementary Document . We simply state that as the proof works for the undirected graphs as well, the same theorem can immediately be applied to the constrained pairwise alignment of PPI networks. ▪ To provide further depth to our understanding of the problem within the constrained alignment framework, we next state the following proposition, which may suggest a clue as to the point the computational intractability starts dissolving. 
 P roposition  2.2 The constrained alignment problem where   and k 2  any positive integer constant is polynomially solvable if one of the directed graphs G p  or   is acyclic. 
 P roof Because of space considerations, the proof is left to the  Supplementary Document . ▪ 
 2.3 The CAMPways alignment algorithm Although Proposition 2.2 provides a positive result, it is restrictive to be useful in practice. We provide a more general algorithm that although may not find the optimum in all cases, will in general produce high-quality alignments. Assuming  , the constants  , and a homological similarity value between the pair   for any node   in   and any node   in  , the algorithm consists mainly of three steps. These major steps are depicted in  Figure 1  on a sample input pathway pair.
 Fig. 1. CAMPways algorithm depicted on a sample input for  k  = 2; the final alignment includes 1-to-1 and 1-to-2 mappings of reactions. First step involves  b-matching ; degrees of nodes are bounded by  k 1  or  k 2  depending on the partition they belong to in the similarity graph. Only a small representative portion of the extended similarity graph is shown. The conflict graph arising from this portion is shown exactly. All the alignments in the  MWIS  boxes of the loops in  Steps 1  and  2  and in the  MWIS  box of the final expansion step are included in the output alignment. Note that the conflict graph definitions within the loops and that of the final expansion phase are different Step1-Constructing the bipartite Similarity Graph : This step involves the construction of   for every node   in   such that   and   for any node   in   such that  . Assuming an edge-weighted bipartite graph on the set of nodes of   in one partition and those of   in the other, where each weight represents the homological similarity of the pair of nodes, a reasonable goal is to find out a subset of edges that satisfies the degree constraints   and that maximizes the sum of edge weights in the output subset; see  Figure 1  where the weight is depicted through the thickness of bipartite graph edges in the similarity graph. The problem then turns into that of  b-matching  (or the  degree constrained subgraph problem ), which has been studied fairly well starting with the pioneering work of  Edmonds (1965) . Polynomial time solutions, including appropriate modifications of the network flow algorithms ( Gabow, 1983 ) and belief propagation methods ( Bayati  et al. , 2011 ), have been suggested. For efficiency considerations, we choose to use a simple greedy algorithm for this step. Each time the algorithm selects the heaviest edge that does not violate the degree constraints   for neither of the end points and extends the output set with the edge. The algorithm stops when there are no more edges to consider, and the bipartite graph resulting from the output set of edges is the similarity graph,  S . Step2-Conflict Graph Generation and Conflict Resolution : Assume the bipartite similarity graph  S  is extended with the directed edges of  , that is, directed edge   is inserted in  S  for   and   in  , if   is an edge in  . Analogous extensions apply to edges of  . We construct an undirected node-weighted  conflict  graph  , where each node corresponds to a set of four nodes providing a conserved edge in the extended graph  S . More precisely, in the conflict graph, there is a node corresponding to  tuple   if and only if all of the following hold:
  and  . Either   are in  , respectively, or   are in  , respectively.  are undirected edges in  S . 
Denote such a  tuple with a  c 4 , as the underlying undirected subgraph induced on the four nodes gives rise to a  cycle. A weight of 1 is assigned to the  c 4 s satisfying only one part of condition  ii,  and a weight of 2 is assigned to those satisfying both parts of  ii . It should be clear that each  c 4  node in the conflict graph represents a pair of reaction subset mappings that gives rise to at least one conserved edge. Furthermore, the weight of the node provides the number of edges conserved as a result of the pair of mappings. The conflict graph depicted in  Figure 1  is the exact conflict graph corresponding to the partially depicted extended similarity graph in the figure. Note that although the structure of the  tuple   resembles that of a  c 4 , that is, conditions  ii  and  iii  defined earlier in the text are valid, it does not correspond to a node in the conflict graph, as condition  i  is not satisfied. Regarding the weights, it should be noted that the node   has weight two, and the rest has weight one in the conflict graph depicted in the figure. Let  C 1  =  ,  C 2  =   and let   and  S ′ 1 . For a  c 4 
 C i , let   indicate the neighbor of  u  in  C i  from the opposite network. There exists an edge between the nodes corresponding to the two  c 4 s in the conflict graph if and only if at least one of the following holds:
  such that   and  .  such that   and  .  such that   and  .  such that   and  . 
This construction implies that an edge exists between a pair of  c 4 s if and only if the pair of conserved edges represented by the  c 4 s can not coexist in any legal alignment. For the conflict graph of  Figure 1  for instance, the edge between the  c 4 s   and   is due to condition  i ; reaction subsets  R 9  and  R 2  share a reaction. Therefore, no legal alignment can include both of the corresponding conserved edges. On the other hand, the edge between   and   is due to  iii . Simultaneously conserving both edges corresponding to both  c 4 s,  R 4  would have to be mapped to two different reaction subsets, which is not possible in any legal alignment by definition. The discussion regarding the conflict graph construction leads to the following proposition:
 P roposition  2.3 The maximum weight independent set (MWIS) of   provides an optimum solution to the constrained alignment problem. 
 However, some modifications are necessary to make our conflict graph model more useful in practical applications of the constrained alignment framework. First, each node in the conflict graph may not necessarily have an exact binary contribution, that is, 1 or 2 to the quality of the final alignment. Therefore, we propose appropriate generalizations for the weights of conflict graph nodes. We provide two alternative weighting schemes. For a given edge  e  in the similarity graph  S , let   denote the weight of  e , which reflects the homological similarity of the reaction subsets corresponding to the end points of  e . For  , the first scheme, denoted with  , assigns a weight of  , where
 
For the computation of  , the total number of directed edges between   and between   is normalized with the maximum number of possible directed edges   in any  c 4 . The parameter α is a balancing parameter between the weight of homological similarity and that of conserved interactions. Our second weighting scheme does not check the number of conserved edges; as long as there is at least one conserved edge, the contribution of edge conservation remains the same. On the other hand, depending on the evolutionary distance of the organisms providing the input pathways, it might be more meaningful do differentiate between the alignments yielding  one-to-many  mappings as opposed to those providing  one-to-few  mappings. Therefore, for the second scheme, denoted with  , we introduce additional input parameters   such that  . Each   reflects the relative importance of the  one-to-i  mappings in the complete alignment. Without loss of generality, let   and  . The weight of  C 1  =   is defined as  . A second issue is related to resolving conflicts, that is, the computation of the MWIS of the conflict graph. The problem is NP-complete in general ( Garey and Johnson, 1979 ). Several greedy heuristics have been investigated in  Sakai  et al.  (2003) . We implemented each and applied extensive tests to determine their performances. The  GWMIN2  heuristic, which selects the node  u  in the conflict graph   that maximizes  , where   denotes the neighborhood of  u  in   together with the node  u  itself, provided better results than the rest. Furthermore, it provides a theoretical guarantee that the weight of the output independent set is at least  , where   denotes the vertex set of the conflict graph  . Therefore, we chose to implement this part of our algorithm using this heuristic. Finally, we note that the resulting mappings are those limited to the edges of the bipartite similarity graph  S  constructed after  Step1 . To enlarge the alignment, we remove all mapped nodes from   after the execution of  Step1  and  Step2 , restore all the homological similarity edges and repeat both steps. This whole process is iterated until convergence, that is, the conflict graph   generated after  Step2  becomes empty. For the example pathway alignment of  Figure 1 , the loop iterates only once; the remaining extended similarity graph contains nodes defined on reaction subsets   and  , which gives rise to an empty conflict graph. Step3-Final Alignment Expansion : The iterative process involving the first two steps aforementioned produces mappings based on  tuples because of the conserved interaction maximization goal of the constrained alignment framework. The convergence of the process implies that no more conserved interactions can be attained. However, there may still exist potential mappings with high-homological similarity that might be added to the alignment. To implement such an expansion, we first remove all the mapped nodes from   and restore all homological similarity edges. Considering the resulting similarity graph  S , we create a new type of a conflict graph, called the  expansion conflict graph . Each node in the expansion conflict graph corresponds to a  tuple   such that   is an edge in  S . There is an edge between two nodes of this conflict graph if and only if the intersection of their reaction subsets coming from the same pathway is non-empty; see  Figure 1  for the expansion conflict graph generation on the sample pathways. Note that the conflict graph defined in  Step 2  is conceptually different from the expansion conflict graph of this step. We finally apply the  GWMIN2  heuristic to resolve the conflicts in the expansion conflict graph, and the alignment is expanded with the mappings corresponding to the resulting nodes. 3 DISCUSSION OF RESULTS The CAMPways implementation is in C++ using the LEDA library ( Mehlhorn and Naher, 1999 ). Source code, useful scripts for testing and evaluations, all the data and output results are available as part of the  Supplementary Material . We experimented on data from the KEGG database ( Kanehisa  et al. , 2012 ) as retrieved and reformatted by  Ay  et al.  (2011) . Our comparative performance evaluations presented in this section are with regards to those achieved in SubMAP ( Ay  et al. , 2011 ), as the used problem definitions are the same; the goal being one-to-many mappings for an input pair of pathways. We note that although a version of SubMAP using network compression to speed-up the original algorithm has appeared recently ( Ay  et al. , 2012 ), lack of publicly available implementation made further extensive comparisons with the new version impossible. Nevertheless, it is suggested that the compression-based version is provided mainly for execution performance at the expense of output alignment qualities. Therefore, in terms of alignment qualities, it is sensible to compare CAMPways with SubMAP. According to the reported results of  Ay  et al.  (2012) , attaining considerable runtime efficiency could cost an accuracy loss of almost 50%, where accuracy is measured in terms of the Pearson’s correlation coefficient between the alignment outputs of the compressed version and the original version of SubMAP. Our experimental results on the other hand indicate that not only does our algorithm provide superior runtime efficiency but also achieves this without incurring any cost on accuracy; to the contrary, the alignment outputs provided by CAMPways provide better accuracies than those of the original SubMAP algorithm. Although the KEGG database provides pathways under detailed metabolism categories, such as  Glycerolipid metabolism  and  Tryptophan metabolism  among many others, directly using these pathways in a network alignment study does not reveal enough information. The most important reason is the lack of a gold standard to be the basis of an objective evaluation of the alignment qualities. Although less serious, the small pathway sizes constitute yet another problem. Predicting the behavior of a possible alignment method at this scale may not lead to reliable conclusions. A mechanism to handle both of these issues is to merge all pathways from detailed metabolism categories that are categorized under the same more general metabolism categories provided in KEGG. Considering the first 11 of the listed high-level categories, we merged all pathways specified under each into a larger metabolic network. This way we obtained 11 metabolic networks in total, each corresponding to one of the following metabolisms: 1.1 carbohydrate metabolism, 1.2 energy metabolism, 1.3 lipid metabolism, 1.4 nucleotide metabolism, 1.5 amino acid metabolism, 1.6 metabolism of other amino acids, 1.7 glycan biosynthesis and metabolism, 1.8 metabolism of cofactors and vitamins, 1.9 metabolism of terpenoids and polyketides, 1.10 biosynthesis of other secondary metabolites and 1.11 xenobiotics biodegradation and metabolism. The number of pathways contained in each larger metabolic network changes between 2 and 15. The subjects of all experimental evaluations of this section are these metabolic networks from pairs of different species. The next two subsections provide our comparative experimental evaluations with regards to the accuracies of output alignments produced by CAMPways and SubMAP. We used two types of accuracy parameters for this purpose. The first one is based on reverse engineering successes of the output alignments, whereas the second one is based on their biochemical significances in terms of coherence with regards to the functional group conversion categorizations as provided by KEGG. We finally conclude our evaluations by providing a running time analysis of CAMPways and a discussion of experimental results on observed execution speeds of both algorithms running on networks under consideration. 3.1 Reverse engineering metabolic pathways The large metabolic networks under consideration can be regarded as networks  engineered  out of small pathways on detailed metabolism categories. A natural accuracy measure is then the reverse engineering capabilities of the provided output alignments; intuitively an alignment mapping reactions that belong back to the same original KEGG pathway is considered to be of high quality. Thus, the pathways on detailed metabolism categories provided by KEGG become our gold standard. Note that this approach assumes the retrieved pathways are noise-free, that is, all pathways in KEGG are considered perfectly correct without any missing data or incorrect pathway associations. Let   denote two species and   be their metabolic networks corresponding to some metabolism  , listed earlier in the text. Let   be a mapping from an alignment of  , where   is a subset of reactions from  X  and   is a subset of reactions from  . Without loss of generality, let  , that is, it is the subset containing a single reaction in the one-to-many mapping. Let   be the pathways that include reaction  r x  in the set of pathways associated with metabolism   in the species  X . We call the mapping  correct  if every reaction in the subset   is included in at least one of the pathways   where each   is a pathway in metabolism   of species  , corresponding to  P i  of  X . We divide the experimental evaluations into two; those regarding the alignments between species within the same domain and those between species from different domains. We pick  Homo sapiens (hsa)  and  Mus musculus (mmu)  as the two representative species from the eukaryota domain, and the  Escherichia coli (eco)  and  Agrobacterium tumefaciens (atc)  from bacteria. The value of  k  = 3 is fixed, that is, each reaction from one of the networks may be mapped to at most three reactions from the other. For the CAMPways alignments, we pick  . 3.1.1 Same-domain alignments The evaluations of the output alignments of  hsa  versus  mmu  and  atc  versus  eco  with regards to all 11 high-level metabolism categories are presented in  Table 1 . Each multi-row in the table provides the results for the alignments of two pairs of networks for metabolisms 1.1 through 1.11 from top to bottom; the top row at the  mth  multi-row lists the alignment results of the hsa-mmu network pair pertaining to metabolism category  , and the bottom row lists those of the atc-eco network pair for the same metabolism category. The  TR  column in the table provides the number of total reactions of the network pair. The  coverage  column provides the total number of reactions covered by the mappings in the alignment. The  correct mappings  column provides the number of correct mappings in the alignment, whereas the  ratio  column provides the ratio of the number of correct mappings to the total number of mappings produced by the alignment. In each subcolumn, we indicate the name of the algorithm providing the alignment scores with respect to the parameter provided in the column including it. The subcolumn marked with  S  provides the corresponding column scores of the alignments produced by SubMAP and the one marked with  C 1  provides those of the alignments produced by CAMPways with weighting scheme   and α = 0.3. Alignments obtained for other settings of α provide almost the same results as this setting. The subcolumn marked with  C 2  provides the corresponding column scores of CAMPways with weighting scheme   and   =   = 0.5 and   = 0.1. The coverages of both algorithms are similar; in some instances, coverages of SubMAP are better, whereas in others, both versions of CAMPways provide higher coverage, although in neither case the differences are large. With regard to the number of correct mappings, CAMPways results are overwhelmingly superior to those of SubMAP. For the  atc-eco  alignment of 1.11 xenobiotics biodegradation and metabolism for instance, even though SubMAP provides a much larger coverage than CAMPways (153 versus 134), the number of correct mappings of CAMPways is still better (60 versus 53). This implies that although in some cases SubMAP aggressively creates mappings in favor of covering many reactions, in a lot of the mappings, it provides the mapped reactions that do not share the same pathway. Over all 22 instances, in five instances, SubMAP does not execute until completion because of excessive memory consumption; shown with empty entries in  Table 1 . For 16 instances, CAMPways provides a larger number of correct mappings, whereas only in one instance, both algorithms provide equal number of correct mappings. The provided ratios also confirm the superiority of CAMPways over SubMAP. Note that the ratio does not normalize the number of correct of mappings with coverage but rather with the total number of output mappings. Thus, it is a measure of the percentage of the correct mappings in the alignment.
 Table 1. Same-domain reverse engineering experiment TR Coverage Correct mappings Ratio S C1 C2 S C1 C2 S C1 C2 437 — 435 435 — 211 213 — 0.99 0.98 458 — 416 416 — 166 171 — 0.82 0.83 62 62 62 62 29 31 31 0.96 1 1 116 105 110 110 45 51 51 0.93 0.94 0.94 745 — 726 726 — 361 361 — 0.99 0.99 264 244 254 254 96 105 103 0.82 0.82 0.83 320 — 320 320 — 159 159 — 0.99 0.99 296 280 262 262 110 128 128 0.90 0.98 0.98 496 491 481 481 221 239 239 0.96 0.99 0.99 369 352 340 339 122 143 143 0.79 0.86 0.86 134 128 130 130 59 64 64 0.96 0.98 0.98 108 102 97 97 37 39 39 0.78 0.82 0.82 168 148 168 168 73 76 76 1 0.90 0.90 73 69 64 64 31 31 31 0.96 0.96 0.96 307 — 306 307 — 150 151 — 0.98 0.98 334 325 324 326 129 143 144 0.87 0.89 0.90 31 28 28 28 12 14 14 1 1 1 51 43 43 44 15 17 17 0.78 0.80 0.77 35 34 34 34 16 17 17 1 1 1 23 21 20 20 8 9 9 0.8 0.9 0.9 207 201 200 200 87 100 100 0.92 1 1 175 153 134 134 53 60 60 0.81 0.89 0.89 Note : In each multi-row, the top row lists the hsa-mmu alignment results and the bottom row lists the atc-eco results. The entries of the rows corresponding to the hsa-mmu network pair are italicized for readability purposes. Each multi-row itself provides the results for the alignments of networks for metabolisms 1.1 through 1.11 from top to bottom. 3.1.2 Across-domains alignments We repeated the same tests for every pair of species under consideration such that members in the pair belong to different domains giving rise to four pairwise alignment instances per metabolism. Two noteworthy observations arise. First, both the number of correct mappings and the correctness ratios decrease for all alignments as compared with those presented in  Table 1 . This is in accordance with the intuition that as the divergence of the pair of species increase, any global alignment starts providing more dissimilar mappings, that is, mappings that match reactions from different pathways of the given species. Second, comparing the alignment qualities of the algorithms, the trend is the same as with the same-domain experiments; in almost all cases, CAMPways provides more correct mappings and better correctness ratios. Over all 44 instances, SubMAP is unable to produce results in 20 of them. In seven instances, both algorithms provide equal number of correct mappings. For 16 instances, CAMPways alignments induce more correct mappings, whereas only for a single instance, the correct mapping count of SubMAP is better. The complete table with detailed results of the across-domains setting can be found in the  Supplementary Document . We note that we implemented several tests to determine how the correctness values and the number of 1-to-i mappings for each   in the output alignments of CAMPways change with respect to various   settings in the   version of the algorithm. Because of space constraints, we provide a detailed discussion regarding these results in the  Supplementary Document . 3.2 Biochemical significance of the alignments To compare the alignment qualities of both algorithms in terms of biochemical significance, we use the functional group conversion (FGC) hierarchy data provided as part of the RCLASS database of KEGG ( Kanehisa  et al. , 2012 ). The reactions in the database are classified into hierarchically organized functional group categories. The same functional group undergoes the same or similar chemical reaction(s) regardless of the size of the molecule it is a part of ( March, 1985 ). Thus, an inter-species alignment of a pair of pathways is considered biochemically validated if the alignment maps reaction subsets classified under the same FGC category. There are five levels of the KEGG hierarchy where the initial root level consists of eight high-level FGC categorizations: carbon-related, hydrogen-related, isomerization-related, nitrogen-related, oxygen-related, phosphorus-related, sulfur-related and halogen-related. The correctness measure is defined analogous to that used in the previous section; for a fixed level  i  of the hierarchy, a mapping is called  correct  if there exists at least one category at the  ith  level of the FGC hierarchy that includes all the reactions involved in the mapping. We compare and evaluate the correctness values provided by the alignments of CAMPways and SubMAP algorithms for the first five levels of the hierarchy starting with the root level at  i  = 1. As with the experiments of the previous section, we use two types of evaluations; those pertaining to the same-domain alignments and those of the across-domains alignments. The results of the former are presented in  Table 2 . The used network pairs and the correspondence of rows, multi-rows are the same as in  Table 1 . The subcolumns marked with S indicate the results of SubMAP alignments and those marked with C indicate results of CAMPways’   version. The   version provides results similar to those of  ; therefore, they are not included in the table. The main column titles indicate all five levels of the FGC hierarchy that provide the categories relevant for the correctness definition of a mapping. Each table entry in these columns corresponds to the number of correct mappings. It can easily be verified that in all the experimental instances, the CAMPways alignments are superior to those of the SubMAP. As the network pairs under consideration are those of the same-domain species, going from more abstract categorizations of the root level 1 to the less abstract levels deeper in the FGC hierarchy, the number of correct mappings does not decrease significantly. We also note that for the 1.7 glycan biosynthesis and metabolism, although there are an average of 80 mappings for the  hsa-mmu  pair, both algorithms produce few correct mappings. The ratio of the correct mappings to the total number of mappings of the alignment is almost 6%. This is in contrast with the 90% correctness ratio of the same pair under the reverse engineering results of the previous section presented in  Table 1 . The prime reason for the low correctness values is the lack of FGC categorizations for most of the reactions involved in the mentioned network. This in turn provides a potential application for the network alignment; the FGC category of a reaction can be transferred to those with unknown categorizations if they belong to the same mapping in the alignment. With regards to the results of the across-domains setting, it can be stated that similar to the results of  Table 2 , the alignment outputs of the CAMPways algorithm provide more correct mappings than those of the SubMAP in almost all network instances under all hierarchy levels; the only exception is the  hsa-atc  metabolism 1.10, in which case the correctness values of both algorithms are already low to bear any significance. The complete table providing results under the across-domains setting is provided in the  Supplementary Document .
 Table 2. Same-domain biochemical significance experiments Level 1 Level 2 Level 3 Level 4 Level 5 S C S C S C S C S C — 193 — 193 — 193 — 192 — 192 — 154 — 154 — 151 — 144 — 138 23 23 22 23 22 23 21 23 21 22 32 41 32 41 32 39 32 39 32 39 323 343 323 343 323 343 318 340 316 338 97 105 97 105 97 104 93 103 92 102 — 103 — 103 — 101 — 101 — 101 66 84 66 84 64 80 64 80 63 80 209 229 209 229 208 229 205 227 205 227 117 143 110 139 104 132 97 130 93 127 53 57 53 57 52 57 52 57 52 56 37 35 37 35 34 33 33 33 33 32 5 6 5 6 5 6 5 6 5 6 20 21 20 21 20 21 20 21 19 21 — 123 — 123 — 123 — 123 — 123 96 115 94 114 93 111 93 110 90 109 9 13 9 13 9 13 9 13 9 13 16 17 16 16 16 16 15 15 14 15 14 16 14 16 13 16 13 16 13 16 7 9 7 9 7 9 6 8 6 8 79 97 78 97 76 97 76 97 76 97 44 59 44 58 42 55 42 55 42 54 Note : The correspondence of the rows and multi-rows are the same as in  Table 1 . The aforementioned analysis based on functional group conversion hierarchies is extended to include the RPAIR data provided by KEGG on a sample mapping pair provided by both algorithms executed on the amino acid metabolism networks of the  atc-eco  pair. A  reactant pair  is defined as a pair of a substrate and a product that preserve chemical substructures through enzymatic reactions. In fact, the RCLASS database classification also provides information regarding reactant pairs. The difference is that the classifications of RCLASS are produced by computerized methods based on chemical structure comparison or molecular alignment, whereas those of RPAIR are produced by manually compiled reactant pairs and molecular alignments incorporating biochemical knowledge. The sample mapping pair provided by the CAMPways alignment is depicted in  Figure 2 . The  atc  reactions R01374 [ d -phenylalanine: acceptor oxidoreductase (deaminating)] and R01582 ( d -phenylalanine: 2-oxoglutarate aminotransferase) are together mapped to reaction R01374 of  eco . Additionally, reactions R00694 ( l -phenylalanine: 2-oxoglutarate aminotransferase) and R01372 [phenylpyruvate: oxygen oxidoreductase (hydroxylating,decarboxylating)] of  atc  are together mapped to the reaction R00694 of  eco . The output compound C00166 (phenylpyruvate) of the reactions R01374 and R01582 is an input compound of the reactions R00694 and R01372. As a result, there is a directed edge from the node corresponding to the subset of reactions R01374, R01582 to the node corresponding to the subset of reactions R00694, R01372 in the  atc  pathway. Similarly, a directed edge exists from the node of reaction R01374 to the node of R00694 in the  eco  pathway. This implies a conserved edge resulting from the provided mappings. With regards to the classifications, it is worth noting that the FGC categories of the reactions R01374 and R01582 are the same for all five levels of the hierarchy, which further strongly validates the mapping involving these reactions based on the RCLASS classification. Both reactions are co-categorized even at the furthest level, which signifies identical RCLASS entry, RC00006. Further validation is observed when the manually compiled and biochemically more reliable RPATH data are examined; both reactions correspond to the identical reactant pair, RP00289 within RPATH. In contrast, the SubMAP mapping, including R01582, maps this reaction and the reaction R01373 [prephenate hydro-lyase (decarboxylating; phenylpyruvate-forming)] of  atc  to the single reaction R01373 of  eco . The FGC categories of reactions R01373 and R01582 separate starting with the second level of the hierarchy and thus belong to separate RCLASS entries. Furthermore, there are no connections between the two as far as the RPAIR database is of concern.
 Fig. 2. Sample mapping from the CAMPways alignment of the amino acid metabolism networks. The reactions at the top are part of the  atc  network, whereas those at the bottom are part of the  eco  network. The mapped reactions (reaction subsets) are shown with the vertical edge. Enzymes are shown using EC numbers. The compounds are depicted within small rectangles 3.3 Execution speed and memory requirements Assuming the degree of every node in   is bounded by a constant, the running time of CAMPways is  , where   is assumed without loss of generality to be larger than  . We provide a detailed analysis of this running time bound in the  Supplementary Document . In comparison, no explicit running time analysis of the SubMAP algorithm is provided. All experimental results in this section are obtained by running the algorithms on an Intel(R) Xeon(R) CPU 2.67 GHz with 24 GB of memory. The required CPU times for all the tested networks are listed in  Table 3 . The first three rows correspond to the experiments within the same-domain setting and the rest to those within the across-domains setting. The total number of reactions for each instance is listed at the subcolumns marked with  TR . The columns provide the abbreviations of algorithm names as in  Table 2 . An important limitation of the SubMAP algorithm is its excessive memory consumption; the SubMAP code could not be executed until completion for some network pairs. For the hsa-mmu alignment of the 1.1 carbohydrate metabolism for instance, the CAMPways algorithm completed in &lt;3 min, whereas the SubMAP code after 2 h of execution consumed all memory resources before crashing. In 15 of the 17 instances within the same-domain setting, CAMPways runs faster than SubMAP. For the across-domains setting in 14 of 28 instances, CAMPways provides better execution time. An important point worth emphasizing is that for the instances where CAMPways run faster, the differences between the execution times of CAMPways and SubMAP are large, whereas for the instances favoring SubMAP, both algorithms provide more or less similar execution times. The difference between the computational efficiency trends of the algorithms under the same-domain and the across-domains settings is interesting. It actually pinpoints the main reason behind the computational efficiency differences of the two algorithms. Within the same-domain setting, the pair of species that the metabolic networks belong to are evolutionarily close. Therefore, the aligned networks induce many conserved edges. In fact, these are the instances for which application of network alignment is sensible; simultaneous nature of the problem in terms of optimizing both homological (high-sequence alignment scores) and topological similarity (high-edge conservation) is most apparent in this setting. Most of the reactions in the pair of networks are aligned throughout the main loop of the CAMPways algorithm, as the generated conflict graphs are large because of high-edge conservation. When the pair of species is evolutionarily apart, the edge conservation is naturally low in which case the main task of both algorithms reduces to that of producing alignments that achieve only high-homological similarity.
 Table 3. The TR subcolumns provide the number of reactions in the network pair TR S C TR S C TR S C TR S C TR S C TR S C 62 3.04 0.30 116 62.81 2.26 264 454.21 13.39 296 1620 15.73 496 975.31 39.87 369 121.43 25.23 134 48.09 1.42 108 17.99 0.94 168 0.32 2.94 73 0.50 0.28 334 1788.84 25.17 31 0.06 0.04 51 0.15 0.09 35 0.09 0.04 23 0.04 0.02 207 3.25 1.00 175 0.67 5.39 93 33.16 2.79 85 6.64 0.82 85 6.51 0.72 93 34.68 2.72 128 40.46 1.67 114 21.52 1.17 118 20.7 1.13 124 42.0 1.45 125 0.44 10.25 116 0.3 6.64 116 0.38 6.08 125 0.41 10.19 39 0.07 0.09 43 0.09 0.05 46 0.10 0.11 36 0.08 0.07 30 0.04 0.03 28 0.05 0.02 27 0.06 0.03 31 0.05 0.03 174 1.26 10.95 208 1.85 20.03 215 1.77 13.24 167 1.27 9.56 Note : CPU times in seconds are provided under the S and C subcolumns. 
 Supplementary Material 
 
 
 Supplementary Data 
 
 
 
 
 </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>BioContext: an integrated text mining system for large-scale extraction and contextualization of biomolecular events</Title>
    <Doi>10.1093/bioinformatics/bts332</Doi>
    <Authors>Gerner Martin, Sarafraz Farzaneh, Bergman Casey M., Nenadic Goran</Authors>
    <Abstract>Motivation: Although the amount of data in biology is rapidly increasing, critical information for understanding biological events like phosphorylation or gene expression remains locked in the biomedical literature. Most current text mining (TM) approaches to extract information about biological events are focused on either limited-scale studies and/or abstracts, with data extracted lacking context and rarely available to support further research.</Abstract>
    <Body>1 INTRODUCTION The amount of information available in the biomedical literature is increasing rapidly, with over 2000 articles published daily ( http://www.nlm.nih.gov/bsd/index_stats_comp.html ). While the information available in these articles (now exceeding 18 million in number) represents a vast source of knowledge, its sheer size also presents challenges to researchers in terms of discovering relevant information. Efforts in biomedical text mining (TM) seek to mitigate this problem through systematic extraction of structured data from literature ( Lu, 2011 ). To date, progress in biomedical TM research has primarily focused on tools for entity recognition (locating mentions of species, genes, diseases, etc.) and the extraction of gene/protein relationships ( Krallinger  et al. , 2008a ,b). Recently, there has been increasing interest to develop TM tools for the extraction of information about a wider array of biological and molecular processes (often referred to as ‘events’), such as expression, phosphorylation, binding and regulation of genes and proteins. Community challenges ( Kim  et al. , 2009, 2011 ) have shown that extracting such events is often difficult because of the complex and inconsistent ways in which such processes are reported in the literature ( Zhou and He, 2008 ). In addition, most efforts to extract events have been restricted to limited-scale studies or abstracts. Although some event extraction tools are now publicly available, their usefulness for supporting biological discovery is still unknown given the difficulties in applying and integrating data from these systems on a large scale. In this article we present BioContext, an integrated TM system which extracts, extends and integrates results from a number of TM tools for entity recognition and event extraction. The system also provides contextual information about extracted events including anatomical association and whether extracted processes have been reported as speculative or negated (i.e. not taking place). In addition to making the integration platform available under an open-source license, we also provide the data resulting from processing the whole MEDLINE and the open-access subset of PubMed Central (PMC) for batch download and online browsing. 2 BACKGROUND Biomolecular events are frequently reported and discussed in the literature, and are critical for understanding a diversity of biological processes and functions. Although some databases exist that contain information about certain types of molecular events (e.g. protein–protein interactions, PPIs;  Ceol  et al. , 2009 ;  Szklarczyk  et al. , 2011 ), extraction and contextualization of a more general set of events using TM systems will present a valuable addition to manually curated data and enable focused navigation of the literature through a variety of biological processes. Identification of molecular events in the literature has been the topic of several recent text mining challenges ( Kim  et al. , 2009 ;  Krallinger  et al. , 2008a ,b). The shared task 1 of BioNLP'09 ( Kim  et al. , 2009 ), for example, aimed to identify and characterize nine types of molecular events:  gene expression, transcription, protein catabolism, localization, phosphorylation, binding, regulation, positive regulation  and  negative regulation . Depending on the event type, the task included the identification of either one (for the first five event types mentioned above) or more (for  binding ) participating proteins/genes (sometimes referred to as themes). Regulatory events could also have a cause (which could be a protein/gene or another event) in addition to one theme/target of regulation (also a protein/gene or another event). The task also included the identification of a textual span (called ‘trigger’) that indicated the occurrence of an event. For example, the sentence ‘MDM2 acts as a negative regulator of p53 expression’ contains two events: (i) a ‘gene expression’ event, with the theme p53, and (ii) a ‘negative regulation’ event, where the theme is the gene expression event in (i) and the cause is MDM2. The trigger for the first event is the word ‘expression’ and the trigger of the regulatory event is ‘negative regulator’. Named entity recognition (NER, locating entities in text) is typically performed before information extraction. Entity classes that have received attention vary widely and include genes/proteins ( Leaman and Gonzales, 2008 ;  Settles, 2005 ), species ( Gerner  et al. , 2010a ) and chemical molecules ( Hawizy  et al. , 2011 ). Recognized entities may also be normalized (i.e. linked to standard database identifiers) in order to enable integration of extracted information with other biological data [e.g. linking gene and protein mentions in the literature to Entrez Gene using GNAT ( Hakenberg  et al. , 2011 ) or GeneTUKit ( Huang  et al. , 2011 )]. Despite increased interest and efforts, only a few general biomolecular event extraction tools are publicly available. The Turku event extraction system (TEES;  Björne  et al. , 2009 ) combines a machine learning approach (relying on dependency parse graph features) with a rule-based post-processing step to identify complex, nested events. EventMine ( http://www.nactem.ac.uk/EventMine/ ), based on the work of  Miwa  et al.  (2010)  also uses machine-learning methods and a set of rich features. The Stanford Biomedical Event Parser ( McClosky  et al. , 2011 ), which uses dependency parses to extract events, has also been made available very recently. Finally, a recent publication by  Kano  et al.  (2011)  describes the creation of a bio-event meta-service, which would make nine different event extractors (including EventMine and TEES) available through U-compare. However, this system is currently not yet available, and it is not clear if sufficient computational resources would be available for it to perform large-scale document processing. 3 MATERIALS AND METHODS We designed and implemented an integrated TM system, called BioContext, which extracts, extends and integrates mentions of molecular events in the biomedical literature. The following sections describe the system architecture and components ( Section 3.1 ), integration and event expansion methods ( Section 3.2 ) and the evaluation approaches ( Section 3.3 ). 3.1 System overview, architecture and components Figure 1  shows an overview of our TM system for large-scale integrated extraction of biomolecular events. Processing is performed in four stages: NER, grammatical parsing, event extraction and contextualization. Each stage is composed of several components, which are described in detail in the following sections. In some cases, outputs from multiple components are merged prior to use by other components. To illustrate the main stages, consider the example sentence ‘Interleukin 6 is probably not expressed in the spleen’. In this example, ‘Interleukin 6’ and ‘spleen’ are first recognized by the gene/protein and anatomical NER components, respectively (Stage 1). The event extractors then use grammatical processing (Stage 2) to identify the internal structure of the input sentence, and to recognize that it discusses the gene expression event involving Interleukin 6 (Stage 3). Finally, in Stage 4, the extracted event is placed into context: the anatomical association component recognizes that the Interleukin 6 expression relates to the spleen, and the negation/speculation component identifies this event as both negated and speculative.
 Fig. 1 System architecture. Each box represents the application of one tool (see main text for description), and each arrow represents the transfer of data from one tool to another. Circles represent data merging and post-processing. Entity recognition is performed by GeneTUKit (genes), GNAT (genes), LINNAEUS (species) and GETM's anatomical NER component. Parsing is performed by the McClosky–Charniak, Enju and Genia dependency parsers. Event extraction is performed by TEES and EventMine. In the final contextualization step, Negmole detects whether events are negated and/or speculative, and events are associated to anatomical entity mentions. Additional document parsing functions provide input to the system and database storage functions handle outputs (but are not shown here) To facilitate the efficient execution of tools and merging of data, we constructed a lightweight TM integration framework, which we call TextPipe. Although other integrative TM frameworks are available [e.g. UIMA and GATE ( Cunningham  et al. , 2011 )], we developed TextPipe since we needed a system which was both more lightweight than what was already available and, more importantly, could be easily modified and optimized for any stability or performance problems we encountered (see  Section 4.1 ). TextPipe makes extensive use of modularization, parallel processing, database optimization and error handling/recovery to address various practical challenges when applying many TM tools to large datasets of abstracts and full-text articles. Tools are wrapped as TextPipe components (treated as black boxes internally) by implementing two simple functions: one to specify the output fields, and another to call the main method of the tool. Data are communicated in the form of lists of key-value pairs. TextPipe components are either applied directly to documents or run as services on demand. They do not need to provide a list of dependencies. Instead, during run-time, they connect directly to other components, providing the document(s) that need to be processed, and fetching the output of those components to use as their input. Computed results can be stored in a database for later re-use to avoid processing of the same document multiple times. The components that have been implemented and integrated in BioContext are explained as follows. Stage 1. NER: identification of genes, species and anatomy In the first stage, identification of gene and protein mentions is performed by GeneTUKit ( Huang  et al. , 2011 ) and GNAT ( Hakenberg  et al. , 2011 ;  Solt  et al. , 2010 ). To the best of our knowledge, these tools are the only tools available that are capable of high-accuracy gene/protein normalization and are practically applicable to large-scale datasets. Whereas GeneTUKit performs normalization for any species, GNAT was limited to 30 of the most commonly studied organisms. Both tools were configured/adapted to utilize BANNER ( Leaman and Gonzales, 2008 ) for gene/protein name recognition in order to reduce the number of false positive (FP) results. In addition, GNAT was modified to also return any entities recognized by BANNER that could not be normalized. While data extracted using non-normalized entities will have more limited use, they should reduce the number of errors that the event-extraction systems make due to incomplete gene/protein information. The outputs from both gene/protein NER systems are merged. We used a confidence level cut-off of 0.01 for GeneTUKit, and all results from GNAT. If the two tools have identified overlapping spans, then we create a new span with the union of their coordinates. If the tools have assigned different Entrez Gene identifiers in the original overlapped spans, then priority is given to the GeneTUKit normalization, as it was ranked higher in BioCreative III gene/protein normalization challenge than GNAT. Identification of species mentioned was performed using LINNAEUS ( Gerner  et al.  2010a ), and recognition of anatomical locations (e.g. brain, T cells) and cell-lines (acting as proxies for anatomical locations, e.g. HeLa for cervical cells) was performed by the anatomical NER system from GETM ( Gerner  et al. , 2010b ). Stage 2. Grammatical parsing In the second stage, a number of grammatical parsers process the text to determine the structure of the processed sentences. Parsing was done by the McClosky–Charniak constituency parser ( McClosky  et al. , 2006 ), the Enju constituency parser ( Sagae  et al. , 2008 ) and the Gdep dependency parser ( Sagae and Tsujii, 2007 ), as these are requested by the down-stream modules (Stages 3 and 4). To increase the accuracy of the parsers when applied to sentences with long and complex entity names, we performed ‘semantic tokenization’ by ensuring that multi-word phrases that are identified as entity names (e.g. ‘acid phosphatase’ or ‘T cells’) were treated as single tokens. Stage 3. Event extraction For identification of event mentions, we used TEES and EventMine. Similarly to gene NER tools, these were chosen since they were the only tools available at the time with large-scale processing capabilities and reasonable performance. Both systems recognize events from the nine BioNLP types (gene expression, transcription, protein catabolism, localization, phosphorylation, binding, regulation, positive regulation and negative regulation) and use gene/protein NER results from the first stage. The output from each system consists of information about the event type, trigger and participants. TEES relies on output from the McClosky–Charniak parser, whereas EventMine uses results from the Enju and Gdep parsers. The data extracted by TEES have previously been released for the 2009 MEDLINE baseline release ( Björne  et al. , 2010a ,b). We utilized this data for any documents that were in the 2009 MEDLINE baseline release by mapping the entities in the TEES data to those extracted by us in Stage 1 using positional overlaps. Events that referred to entities that could not be mapped to our entities were not included. Further, additional TEES extractions were performed for the remaining documents (the full-text PMC documents and 1 412 095 additional abstracts that only are available in the 2011 MEDLINE baseline release). We performed event extraction with EventMine for all documents. Before contextualization (Stage 4), the outputs of the two event extraction systems are post-processed, expanded and integrated as described in  Section 3.2 . Stage 4. Contextualization Events can occur in different places and under different conditions, and this information is key for the comprehensive understanding of the biomedical processes. In the fourth stage, extracted events are enriched with contextual information, including species involved, anatomical locations associated with the event and whether extracted events have been reported as speculative or negated. Anatomical locations are linked to events using an expanded version of the GETM method described in  Gerner  et al.  (2010b) , which relies on Gdep dependency trees to link events and associated anatomical entities. Speculative and negated events are recognized by an extended version of the negation-detection system Negmole ( Sarafraz and Nenadic, 2010 ). Negmole uses machine learning and relies on constituency parse trees from the McClosky–Charniak parser. 3.2 Event integration and post-processing Events extracted from TEES and EventMine are compared to determine if they refer to the same mention. Two events extracted from a given sentence are considered to be the same if their type and participants match (using approximate span matching, allowing overlap). If the event is ‘nested’ (e.g. regulates another event), the comparison is performed recursively. Note that we do not require the triggers to match, as they do not convey any additional biological information. The results of event extraction are additionally post-processed to improve precision (by eliminating likely FPs) and coverage (by inferring additional events), as follows:
 Removing probable FPs.  After studying a sample of the merged output from the large-scale MEDLINE processing, we noted certain patterns that contributed towards clearly incorrectly extracted events. We therefore designed post-processing methods that discriminated against likely FPs, similarly to our previous study ( Sarafraz  et al. , 2009 ). We have not observed the patterns removing any events that should not have been removed. The rules were based on (a) event chains (sequences of ‘nested’ events linked through regulation); and (b) event triggers (keywords indicating the presence of an event) as follows.
 Negative discrimination based on the event chains : For nested regulation events, one or more of the participants can be other events. Common likely FPs were events that are circularly nested (e.g. E1 causes E2, and E2 causes E1) or where there is a long, potentially indefinite chain of events (i.e. E1 causing E2 causing E3 and so on). For example, in one instance, TEES found a chain of 211 769 connected events. We noticed that there were very few instances in the BioNLP'09 training data where events are nested further than two levels, and there are no circularly nested events. Therefore, all events with a ‘nestedness’ level above 2 were removed. Negative discrimination based on the event trigger : Events characterized with unlikely triggers are also removed, for example, events with very short triggers (one or two characters, mostly consisting of punctuation, single letters or abbreviations). We compiled a whitelist of 11 short words (e.g. ‘by’) that  could  be triggers, and a blacklist of 15 longer words (common English stop words) that were often recognized incorrectly as event triggers. Events that had a trigger from the whitelist were not removed, whereas events that had a trigger from the blacklist were removed. In addition, capitalized triggers that did not occur in the very beginning of the sentence were also removed as they were likely to be proper nouns. For example, an event with the (incorrect) trigger ‘Region’ from the sentence ‘The prevalence of urinary lithiasis in children in Van Region, Turkey.’ (PMID 20027811) would be removed (here, the incorrect event was ‘expression of Van’, also representing a gene/protein NER FP). Inferring additional events from enumerated entity mentions.  A number of gene/protein and anatomical entity mentions in MEDLINE (see  Section 4 ) are part of entity ‘enumerations’, i.e. lists of more than one entity connected within a conjunctive phrase. Event extractors, however, typically ignore enumerations. We hypothesized that, where an event is associated with a gene/protein or anatomical entity that is part of such an enumeration, we could infer additional events by substituting the original entity with each of the other entities in the enumeration. For example, consider the sentence ‘In the present study, we describe three novel genes, Dorsocross1, Dorsocross2 and Dorsocross3, which are expressed downstream of Dpp in the presumptive and definitive amnioserosa, dorsal ectoderm and dorsal mesoderm.’ (PMID 12783790). Here, gene expression events should be extracted for all three Dorsocross genes, and each of those events should be associated with each of the three anatomical locations mentioned. If any of these nine events are not extracted directly, the enumeration processing would allow them to be inferred indirectly. To implement this inference, we used regular expression patterns to detect groups of enumerated entities. Where at least one of these entities (e.g. T1) were part of an event (e.g. E1), we constructed a new event E2 with the entity T2, where T2 was mentioned in the same enumeration group as T1. Except for T1, all other properties of E1 were duplicated in E2. 3.3 Evaluation approaches To measure the impact that different processing steps have on the data as it moves through the pipeline, the performance of different components was evaluated individually, with evaluations of the final components also showing the accuracy of the system as a whole. The gene/protein NER, event extraction and negation/speculation detection components were evaluated against a corpus based on the BioNLP'09 and GENIA corpora (described in the following paragraph). The anatomical associations and the event inference components were evaluated by manual inspection. The public portion of the BioNLP'09 corpus ( Kim  et al. , 2009 ), consisting of 800 training documents and 150 development test documents was created from a subset of the GENIA event corpus ( Kim  et al. , 2008 ) with extensive manual modifications ( Kim  et al. , 2009 ;  Ohta  et al. , 2009 ). Only the GENIA entities considered to be genes or gene products were included in the BioNLP'09 corpus. However, many events in the GENIA corpus contain links to protein complexes, which were not included in the BioNLP corpus. For example, many mentions of NF-kappa B that refer to protein complexes were removed from the BioNLP'09 corpus. Because of the importance of protein complexes for biomolecular processes, we decided to expand our definition of event participants to also include protein complexes. Therefore, the BioNLP'09 and GENIA corpora were merged into a new corpus, which we refer to as the B+G corpus. More specifically, the BioNLP'09 corpus was expanded with mentions of entities from the Protein_complex and Protein_molecule GENIA classes. Protein_molecule entities were added since protein complexes often were annotated as Protein_molecule entities in the GENIA corpus (for example, we estimate that NF-kappa B was annotated as Protein_molecule in 38% of cases). We also included any events involving these entities. By merging the two corpora, we could retain the modifications made to the BioNLP'09 corpus but gain additional events that involve protein complexes from GENIA. The merged corpus is used as a gold standard for evaluation of both NER and event extraction, and is available in  Supplementary Material  S1. In the following, we include protein complexes together with genes and proteins when we refer to gene(s) or protein(s). When evaluating event extraction components, an extracted event was considered to be a true positive (TP) if all of the following criteria hold: (i) the extracted event type is the same as the event type annotated in the gold standard; (ii) the entity participants are all TPs, and approximately match boundaries with the participants in the gold standard; (iii) the participant types match (theme or cause); and (iv) if any of the participants is an event, it is also a TP, defined recursively. To the best of our knowledge, no corpus currently exists that provides cross-species normalized gene/protein mentions—all available corpora provide gene/protein annotations at the recognition level (i.e. gene/protein mentions are not normalized to database identifiers). Therefore we can only evaluate the combined data of GNAT and GeneTUKit on the recognition level, and refer to the original papers ( Hakenberg  et al. , 2008, 2011 ;  Huang  et al. , 2011 ) for evaluation in terms of normalization. Likewise, no gold-standard data exist that could be used to evaluate anatomical associations or event inference, so we manually inspected a randomly selected set of 100 extracted events for each component to estimate their levels of precision. 4 RESULTS AND DISCUSSION We performed two types of experiments to assess the benefits of the data produced by our system: a large-scale data generation experiment to quantify and characterize application of the system on MEDLINE and PMC, and a smaller-scale evaluation of the quality of the data. 4.1 Large-scale application to MEDLINE and PMC We applied our system to MEDLINE (2011 baseline files, containing 10 946 774 abstracts) and to the open-access subset of PMC (downloaded May 2011, containing 234 117 full-text articles.) Table 1  shows the number of gene/protein entities (both mentions and distinct entities) extracted from MEDLINE and PMC with the two gene normalization tools. Of the 80 003 072 extracted gene mentions in the union set, 10 261 208 (12.8%) were not normalized, all coming from GNAT. The GeneTUKit and intersection data contain only normalized entities linked to Entrez Gene. We note that only 43% (34 479 547/80 003 072) of all mentions in the MEDLINE+PMC union set were recognized by both NER tools. This is even more extreme in the case of full-text articles (only 37% of all mentions recognized by both tools).
 Table 1. The total number of gene mentions and the number of normalized, distinct genes recognized by GNAT and GeneTUKit in MEDLINE and PMC Source Entity mentions Distinct entities MEDLINE PMC MEDLINE + PMC MEDLINE PMC MEDLINE + PMC GNAT 35 910 779 12 729 471 48 050 830 227 809 129 244 253 929 GeneTUKit 47 989 353 19 217 778 66 431 789 258 765 143 706 287 218 Intersection 26 281 266  8 638 823 34 479 547 224 604 125 763 249 932 Union 57 618 866 23 308 426 80 003 072 261 412 146 552 290 557 Of the 80 003 072 gene/protein mentions in the MEDLINE and PMC union sets, 11 317 242 (14%) were part of enumerated groups as detected by our patterns. Likewise, of the 56 659 248 anatomical mentions found, 3 489 723 (6.2%) were found to be enumerated. These results suggest that authors study multiple genes/proteins more frequently than they study multiple anatomical locations. Table 2  presents the number of events extracted from MEDLINE and PMC. The relative volumes of different event types are available in  Supplementary Material S2 . To estimate the number of distinct events reported, we define two events to be the same if the following are true: (i) they are of the same type; (ii) they involve the same normalized gene entities; or, if non-normalized genes are involved, the gene mention strings match; or, if more than one entity is involved, all pairs match; (iii) either no anatomical entity is associated with neither of the two events; or, if one event is associated with an anatomical entity, the other event should also be associated with an entity normalized to the same anatomical location; (iv) they are both affirmative or both negated; (v) they are both certain or both speculative; (vi) if any of the participants of the events is another event, those nested events also match recursively. We note that in total, almost 11.5 million distinct events could be extracted from the MEDLINE+PMC union set. Of the union of distinct events, only 32% and 21% were recognized by both tools in abstracts and full-text articles, respectively. Similar observations hold for event mentions (32% abstracts, 18% full text), demonstrating complementarity of the event extractors.
 Table 2. The total number of event mentions and the number of distinct events extracted by TEES and EventMine from MEDLINE and PMC Source Event mentions Distinct events MEDLINE PMC MEDLINE + PMC MEDLINE PMC MEDLINE + PMC TEES 19 406 453 4 719 648 23 856 554 6 570 824 1 804 846 7 797 604 EventMine 18 988 271 4 010 945 22 737 258 6 502 371 1 588 178 7 539 364 Intersection 9 243 903 1 331 456 10 455 678 3 080 900 573 903 3 424 372 Union 29 150 821 7 399 137 36 138 134 9 635 566 2 676 257 11 442 462 Of the 36 138 134 event mentions in the MEDLINE+PMC union set, 1 052 541 (2.9%) were created through the event inference method. Although the percentage of events inferred is low, the absolute number is still large enough to demonstrate its utility. In terms of contextualization, 13 564 939 events (37.5%) could be associated with an anatomical entity, 1 487 502 (4.1%) were negated and 1 253 133 (3.5%) were speculative. We note that the negation/speculation ratios are slightly lower than those of the combined BioNLP'09 training and development sets (at 6.8% and 5.3%, respectively). Compared with the previously released dataset of 19.2 million total event mentions extracted from the 2009 release MEDLINE by TEES ( Björne  et al. , 2010a ,b), the dataset described here provides additional value in a number of ways, including the addition of nearly 1.5 million MEDLINE abstracts and more than 234 000 full-text PMC articles, normalization of genes and proteins to species-specific identifiers, and association to anatomical locations. In addition, the use of multiple tools for the more challenging aspects (gene/protein NER and event extraction), allows users to query and interpret data depending on whether it was extracted by one or more components of the system. When scaled to the total number of documents processed, we find an average of 2.7 extracted event mentions per abstract and 31.6 event mentions per full-text article in the union data. Thus, only ∼ 8.4% of the events stated by authors are in the abstracts. This is similar to results from a previous study, which after manual annotation of 29 full-text articles reported that only 7.8% of claims were made in the abstracts ( Blake, 2010 ). Although events stated in the abstracts can be expected to be more important in general than those stated elsewhere in the article, it still highlights the importance of processing full-text documents. It is therefore unfortunate that only ∼ 2% of MEDLINE entries have open-access full-text articles that are available for text mining. If the open-access subset of PMC is an indicator of the richness of full-text articles in general, we extrapolate that roughly 300–400 million (31.6 × 10.9 million) further event mentions are described in the full text of articles in MEDLINE. In addition to the 80.0 million gene/protein mentions and the 36.1 million event mentions, the process of extracting events from MEDLINE and PMC also produced large volumes of other intermediary data that are available and should prove useful to the text mining and bioinformatics communities. This data include 70.9 million species entity mentions, 56.6 million anatomical entity mentions and 133 million parsed sentences from each of the Gdep, Enju and McClosky–Charniak parsers. We note that processing on the scale reported here presents several challenges. The large number of documents resulted in large computational time requirements. Even using 100 processor cores (in a cluster), a full run of the system required 2–3 months. Testing requirements and tool crashes resulted in the total processing requirements of roughly double that. For example, although the documents in MEDLINE and PMC are generally well-structured, there were outliers that introduced significant problems. Examples we have found included documents over 300 pages long (causing some tools to crash when running out of memory, and others never to finish) and documents that typically confuse TM tools by containing non-ASCII characters or programming or TeX source code (causing every single grammatical parser to crash). We have, therefore, implemented robust general error detection and recovery methods within our framework (TextPipe) to help with unusual processing time, frequent crashes and other external problems, such as network connection timeouts or machine failures. 4.2 Gold-standard evaluation Evaluation results for the gene/protein NER systems on the 3 000 annotated gene/protein mentions in the B+G corpus are shown in  Table 3 . Both precision (at 72–80% for the individual systems, with a maximum of 83% for the intersection set) and recall (at 79–84%, with a maximum of 92% for the union set) are similar to what has previously been reported for common recognition tools [BANNER: 85% precision, 79% recall; ABNER: 83% precision, 74% recall; ( Leaman and Gonzales, 2008 )]. A brief manual inspection of FP and false negative (FN) errors indicate that some of the more common categories of errors include incorrect dictionary matches (e.g. non-gene acronyms matching synonym entries in gene dictionaries), the use of terms by authors that are not in dictionaries, and incomplete manual annotations of the corpora.
 Table 3. Gene/protein NER evaluation results Precision (%) Recall (%) F-score (%) GNAT 79.8 83.7 81.7 GeneTUKit 72.2 79.1 75.5 Intersection 82.8 70.4 76.1 Union 71.4 92.0 80.4 Evaluation of the two event extractors on the 2 607 annotated events in the B+G corpus ( Table 4 ) shows the best precision of 66% (for intersection) and the best recall of 62% (for union). Event type-specific evaluation results are available in  Supplementary Material  S3. TEES alone provides the best balance between precision and recall (52%). Manual inspection reveals that many FP and FN errors by the event extractors were due to incorrect entity recognition that propagated to the event extraction stage, sentences that were particularly linguistically or semantically complex, and incomplete manual annotation of the corpora.
 Table 4. Event extraction evaluation results on the B+G corpus Precision (%) Recall (%) F-score (%) TEES 50.4 53.6 51.9 EventMine 45.7 45.5 45.6 Intersection 66.2 36.6 47.1 Union 41.3 62.0 49.6 We note that our evaluation results differ from the previously reported level of 64% precision in ( Björne  et al. , 2010a ,b); recall was not reported. However, the evaluation methods between these two studies are different, hindering direct comparisons. In the evaluation of ( Björne  et al. , 2010a ,b), 100 events were selected randomly for manual verification, rather than being compared with an already annotated gold-standard corpus. Furthermore, a more inclusive definition of ‘entity’ was also used, with ‘cells, cellular components or molecules involved in biochemical interactions’ considered as TPs if recognized. Evaluation of the event extraction results after performing negation and speculation detection by Negmole is shown in  Table 5 . In addition to having a correctly extracted event (evaluated in  Table 4 ), events were also required to have both their negation and speculation status correctly identified to be classified as a TP. Only relatively small differences in data quality can be observed before and after the application of Negmole (relative to the results in  Table 4 ). This is expected since only a small subset of events are negated and/or speculative (3.3% and 4.3% of events in the test corpus were found to be negated and speculative, respectively).
 Table 5. Event extraction results on the B+G corpus, including negation/speculation detection as processed by Negmole Precision (%) Recall (%) F-score (%) Intersection 62.6 34.6 44.6 Union 38.8 58.3 46.6 To evaluate the performance of the anatomical association and event inference, we randomly selected 100 events associated with anatomical entities and 100 events produced by the event-inference rules. The events were otherwise selected randomly from the complete union of events in MEDLINE and PMC. Manual inspection of the 100 events associated to anatomical entities showed a precision of 34%. The lack of a gold-standard corpus of anatomical associations prevented estimation of recall for anatomical association. The 100 inferred events showed a precision level of 44%. Overall, we observed that many FPs and FNs that occurred in the NER stage are propagated to the event extraction stage, and additional FPs and FNs introduced there are in turn propagated to the context association stage. This means that even relatively small error rates can have a large impact on the final results, especially if they occur early in the pipeline. Finally, we note that evaluations performed using the B+G corpus are limited by the fact that it was drawn by the corpus creators from the set of MEDLINE abstracts containing the MeSH terms ‘humans’, ‘blood cells’ and ‘transcription factors’ and therefore may not be completely representative for MEDLINE as a whole. 5 CONCLUSION In this article we present an integrated TM system, BioContext and the data produced by it when applied to 10.9 million abstracts in MEDLINE and 234 000 full-text articles in the open-access subset of PMC. The data contain 36.1 million event mentions, representing 11.4 million distinct events describing processes such as gene expression, transcription, catabolism, localization, phosphorylation, binding and regulation of genes and proteins. Over a million of additional event mentions were created through enumerated entity mentions. Event participants are whenever possible linked to Entrez Gene identifiers. The data contain contextual information regarding the associated anatomical locations and whether events are reported as negated or speculative. The process of generating and integrating this huge volume of data proved challenging and differences observed between the output of individual tools indicate that the identification of events in text is not always easily reproducible on a large scale. Nevertheless, differences among tools in recognized entity and event mentions can be useful when deciding the balance between precision and recall for different applications. As expected, we find that for both gene/protein NER and event extraction, the intersection of multiple tools shifts the balance towards increased precision while the union favours increased recall. We have made the data available at www.biocontext.org both for batch download and for browsing through a web search interface, giving biologists not only more comprehensive access to data in the literature, but allowing bioinformaticians to run more powerful integrative analyses using information extracted from literature. We also provide the entire set of intermediary data files as well as the integration framework, TextPipe, which can be used either for completely new TM projects or to construct and deploy modified versions of the system described here. Several tasks remain for future work. Data interfaces could be enhanced by allowing restrictions on which documents results are returned for the search interface, or by importing and integrating the BioContext data into a pubmed2ensembl-style biological data mining portal ( Baran  et al.  2011 ). We would also like to improve the accuracy through improved filtering techniques. Further, as protein complexes currently are not linked to any reference source, we would like to enable normalization of these entities, either to their constitutive genes or directly to protein complex databases. Finally, a more detailed analysis of the extracted data is also warranted, which will hopefully shed new light on biomolecular events at an unprecedented scale of global understanding. </Body>
  </Article>
  <Article>
    <Journal>Bioinformatics</Journal>
    <Title>PathVar: analysis of gene and protein expression variance in cellular pathways using microarray data</Title>
    <Doi>10.1093/bioinformatics/btr656</Doi>
    <Authors>Glaab Enrico, Schneider Reinhard</Authors>
    <Abstract>Summary: Finding significant differences between the expression levels of genes or proteins across diverse biological conditions is one of the primary goals in the analysis of functional genomics data. However, existing methods for identifying differentially expressed genes or sets of genes by comparing measures of the average expression across predefined sample groups do not detect differential variance in the expression levels across genes in cellular pathways. Since corresponding pathway deregulations occur frequently in microarray gene or protein expression data, we present a new dedicated web application, PathVar, to analyze these data sources. The software ranks pathway-representing gene/protein sets in terms of the differences of the variance in the within-pathway expression levels across different biological conditions. Apart from identifying new pathway deregulation patterns, the tool exploits these patterns by combining different machine learning methods to find clusters of similar samples and build sample classification models.</Abstract>
    <Body>1 INTRODUCTION In the search for new diagnostic biomarkers, one of the first steps is often the identification of significant differences in the expression levels of genes or proteins across different biological conditions. Commonly used statistical methods for this purpose quantify the extent and significance of changes in measures of the average expression levels of single genes/proteins [see for example  Smyth (2004) ;  Tusher  et al. , (2001 )] or analyze aggregated data for gene/protein sets representing entire cellular pathways and processes ( Glaab  et al. , 2010 ;  Guo  et al.  2005 ;  Lee  et al. , 2008 ). However, since these approaches compare measures of averaged expression levels, they cannot study how the variance of expression levels across the genes/proteins of a cellular pathway (termed ‘pathway expression variance’ here) changes under different biological conditions. In this article, we present a web application for microarray data analysis to identify and prioritize pathways with changes in the pathway expression variance across samples (unsupervised setting) or predefined sample groups (supervised setting). In particular, we show example cases on cancer data in which significant pathway deregulations manifest themselves in terms of changes in the variance of gene/protein expression levels in pathways, while no significant changes can be detected in the median pathway expression levels (see section ‘Results on Cancer Microarray Data’ and  Fig. 1 ). Finally, we discuss how the software enables automated sample clustering and classification using the extracted pathway expression variances.
 Fig. 1. Left: box plot comparing the median expression levels in the KEGG Urea cycle pathway (hsa00220) for the prostate cancer dataset by ( Singh  et al. , 2002 ) across 50 healthy individuals (green) and 52 tumor patients (red); right: box plot comparing the variance of expression levels in the same pathway and microarray dataset (see also  Supplementary Material ). 2 WORKFLOW AND METHODS PathVar identifies and analyzes deregulation patterns in pathway expression using two possible analysis modes, a supervised and an unsupervised mode, chosen automatically depending on the availability of sample class labels. In the first step, the user uploads a pre-normalized, tab-delimited microarray dataset and chooses an annotation database to map genes/proteins onto cellular pathways and processes (see  Section 4 ). Next, in the supervised analysis mode, the software computes two gene/protein set rankings in terms of differential pathway expression variance using a parametric  T -test and a non-parametric Mann–Whitney  U -test (or respectively, an  F -test and Kruskal–Wallis test for multi-class data). Alternatively, in the unsupervised analysis mode, three feature rankings are obtained from the pathway expression variance matrix (rows = pathways, columns = samples) by computing the absolute variances across the columns/samples, the magnitude of the loadings in a sparse principal component analysis ( Zou and Hastie, 2008 ) and a recently proposed entropy score ( Varshavsky  et al. , 2006 ). These rankings are combined by computing the sum of ranks across the three methods and normalizing the sum-of-ranks scores by dividing by the maximum possible score. The resulting sortable ranking table of pathways contains the test statistics and significance scores, the number and identifiers of the mapped genes/proteins, and buttons to generate box plots for each pathway and forward the genes/proteins to other bioscientific web services for further analysis. Moreover, a heat-map visualization of the expression level variances is provided as output. In the next step, the user can forward the extracted pathway variance data to a clustering module, for identifying sample groups with similar expression variance across multiple pathways, or to a classification module (for labelled data), to build models for sample classification. The clustering module provides a selection of four hierarchical clustering algorithms, three partition-based approaches and one consensus clustering approach to combine the results of the individual methods see  Glaab  et al.  (2009)  and  Supplementary Material . In order to compare the outcome for different clustering approaches and identify a number of clusters that is optimal in terms of cluster compactness and separation between the clusters, five validity indices are computed and aggregated by computing the sum of validity score ranks across all methods and numbers of clusters. Moreover, the clustering results are visualized using both 2D plots (cluster validity score plots, principal component plots, dendrograms and silhouette plots) and interactive 3D visualizations using dimensionality reduction methods ( Supplementary Material ). For a supervised analysis of the data, the classification module contains six diverse feature selection methods and six prediction algorithms, which can be combined freely by the user [see  Glaab  et al.  (2009)  and  Supplementary Material ]. To estimate the accuracy of the generated classification models, the available evaluation schemes include an external  n -fold cross-validation as well as user-defined training/test set partitions. In addition to the average prediction accuracy and SD obtained from these evaluation methods, several other performance statistics like the sensitivity and specificity, and Cohen's Kappa statistic are computed. Additionally, a  Z -score estimate of each gene set's utility for sample classification is determined from the frequency of its selection across different cross-validation cycles, and a heat map is generated to visualize the expression variance for the most informative gene sets. All machine learning technique implementations stem from a fully automated data analysis framework  Glaab  et al.  (2009) , which has previously been employed in variety of bioscientific studies ( Bassel  et al. , 2011 ;  Glaab  et al. , 2010 ;  Habashy  et al. , 2011 ). To alleviate statistical limitations resulting from incomplete mappings of genes/proteins onto pathways and from multiple hypothesis testing, only pathways with a minimum of 10 mapped identifiers are considered in all analyses and  p -values are adjusted according to  Benjamini and Hochberg (1995)  (see section on limitations in the  Supplementary Material  for details and advice). 3 RESULTS ON CANCER MICROARRAY DATA The microarray prostate cancer dataset by  Singh  et al.  (2002) , containing 52 tumor samples and 50 healthy control samples, is a typical example for a cancer-related high-throughput dataset with gene expression deregulations across many cellular pathways. When analyzing this data using both a comparison of median gene expression levels in KEGG pathways across the sample classes, and a comparison of the expression level variances with PathVar, the top-ranked pathway in terms of differential expression variance,  Urea cycle and metabolism of amino groups (hsa00220) , showed a significant increase of the variance in the tumor samples (see  Fig. 1 , right; adjusted  P -value: 2.2e-06). Interestingly, a conventional comparison of the corresponding median gene expression levels does not identify statistically significant differences between the sample groups ( Fig. 1 , left). Similar results were obtained for other cancer-associated KEGG pathways, including the angiogenesis-related  VEGF signaling pathway (hsa04370)  and the inflammation-related  Natural killer cell mediated cytotoxicity (hsa04650)  process. Corresponding statistics and box plots are provided in the  Supplementary Material , which also contains results from the clustering module and the classification module, similar outputs for a further microarray study, as well as details on the used data and normalization procedures. In summary, PathVar identifies statistically significant pathway deregulations, different from those detected by methods for comparing averaged expression levels, and provides pathway-based clustering and classification models that enable a new interpretation of microarray data. 4 IMPLEMENTATION All data analysis procedures were implemented in the R statistical programming language and made accessible via a web interface written in PHP on an Apache web server. Gene and protein sets representing cellular pathways and processes were retrieved from the databases KEGG ( Kanehisa  et al. , 2008 ), BioCarta ( Nishimura, 2001 ), Reactome ( Joshi-Tope  et al. , 2005 ), NCI Pathway Interaction Database ( Schaefer  et al. , 2009 ), WikiPathways ( Pico  et al. , 2008 ), InterPro ( Apweiler  et al. , 2001 ) and Gene Ontology [GOSlim,  Ashburner  et al.  (2000) ] and will be updated on a regular basis. A detailed tutorial for the software is provided on the web page. Funding :  German Academic Exchange Service (DAAD)  short-term fellowship (to E.G.). Conflict of Interest : none declared. 
          Supplementary Material 
          
             
                Supplementary Data 
             
             
             
          
       </Body>
  </Article>
</Articles>
